Docker and devops collaborate for
efficient software development Docker
ensures consistency and portability
while devops focuses on teamwork
Automation and cicd resulting in Faster
development and deployment this Docker
complete course we will cover Docker
Basics installation on ubu and Windows
10 advanced concepts containerization
and more we will compare Docker with
kubernetes and virtual machines and end
with Docker interview questions so if
you are looking to become a Docker
expert consider the postgraduate program
in devops by simply learn to gain
expertise in Docker this program offers
Blended learning that combines live
online devop certification classes with
interactive Labs that will give you
hands-on experience you will gain
expertise in the tools like terraform
Maven an Jenkins Docker junit G and more
we prefer one year of Prior experience
to enroll in this course so why wait
enroll the course from the link in the
description box so before we begin
consider subscribing to Simply learn and
hit the Bell icon to never miss any
updates from us so without any further
delay over to our
tutors doops Engineers are one of the
highest paid Professionals in the tech
industry today and if you are watching
this video most likely you are someone
who wants to get into devops or want to
learn devops but with so many tools and
Technologies like dform Anil Linux AWS
genkin stalker kubernetes and so much
more learning devops can be very time
consuming and confusing and that is why
in this video I'm going to share with
you a complete devops road map that you
can follow to learn devops from scratch
and also an excellent resource you can
use to learn all these devops tools at
one place so watch this video till the
end before we start with the video and
look into the devops road map let me
introduce myself my name is NASA Chri
also goes by Cloud champ on YouTube and
I work as a freelance devops engineer
for multiple companies the road map
shared in this video is the exact road
map I followed to learn devops from
scratch and I'm pretty sure if you watch
this video till the end you will have a
clear idea on what things to learn and
what not to learn and get jobs faster a
doops engineer is responsible for
deploying application and automating the
manual process but how can you automate
a manual process if you don't know how
is an application created so first thing
you need to know is to understand the
concepts of software development what is
a build what is software deployment so
generally try to understand the whole
software developer life cycle from idea
to code and to releasing the application
to end users after you understand the
software developer life cycle and now
you have an idea of how is an
application created second thing you
need to know is Linux Linux is very very
very important a de Ops engine should
have a good Hands-On Lage with Linux
need to know all the important commands
because every Dev tools you look at it's
a Anil terraform kubernetes stalker all
of them work on commands and you can
only manage to work with them when you
have good hands on knowledge with Linux
so learn Linux in Linux you can learn
things like shell commands Linux file
systems and permissions SSH Key
Management virtualization some part of
networking like Lo bances how to set up
firewalls how IP addresses work and much
more Linux is very important so it's
also called as operating system of the
cloud so spend time learning leadex now
most of you might be confused and you
might ask let's see I want to learn all
this tools but where should I find the
resources you can learn this from
YouTube some of these from blog some
from documentation but they are all
scattered which will waste your time so
rather than that I will suggest you
checking out an exellent devops program
by simply learn this postgraduate devops
program by simply learn is in
collaboration with IBM which will teach
you all the important devops tools that
you require in order to become a devops
engineer like terraform M an Jenkins
stalker kubernetes kit and a lot more
along with industry projects that will
provide you hands on experience and can
help you become devops engineer faster
and also a certification by Caltech
which will validate your learning in
devops so you can check out the learning
part here and reviews by previous
learner wow all of them are five star
reviews so click on the link in the
description and click on apply now for
this program you don't require any PRI
work experience you require a bachor
degree with an average of 50% or higher
marks you can be from programming or
non-programming background which makes
this program for everyone so click on
the link in the description click on
apply now fill in all your details and
proceed to start learning devops with
simply learn next important thing after
learning leadex is going to be git
because every company is going to store
their code online on git repositories
like GitHub gitlab or bit bucket so
doops Engineers need to know how to work
with Git to close and push the code from
local machine to G repositories or from
G repositories to local machine also
understand what is branching how does
branching books what is M request what
is pull request and a lot more so DS
engineer should have a good
understanding of G and also know all the
most used G commands once you have
cleared your Basics now is the time you
can learn Cloud because D Engineers need
to know how to create servers databases
stage virtual VCS and lot more on the
cloud you can choose any cloud like AWS
azure gcp IBM Oracle or anything but Ops
Engineers need to know how to create
infrastructure to deploy the
applications and software on the cloud
once you have learned how to create
infrastructure and deployer applications
on the cloud manually you need to
automate this because Toops is all about
automation so you can automate this
using infrastructure as code tools like
terraform anible chef perpet and a lot
more but I would suggest you learning
only these two tools which are terraform
and anible that are highly used in the
industry today learning anible and
terraform can provide you with so many
job opportunities so start with learning
terraform and anible after you have
mastered creating an application on the
cloud manually due to rise in demand and
application stability companies are
shifting from servers to Containers
which is why you need to learn Docker
and kubernetes for Docker you need to
understand the concepts of
virtualization and concept of
containerization also you need to know
how to containerize an application and
draw it on a server or a kubernetes
class
you need to know the commands on how to
create Docker file how to create Docker
image how to run containers networking
in Docker and some of the parts for
kubernetes you need to understand the
kubernetes architecture what is
deployment what is replica set what is
POD what is node and how to properly
manage your cized application using kues
cluster either on eks AKs or gke which
is going kues engine so it is very
important for you to understand and know
the commands to properly work through
containerization which is a leading and
a popular tool right now in the market
next very important thing for a doops
engineer to learn is going to be cicd or
continuous integration and continuous
deployment because every company wants
to deploy their application
automatically in devops all code changes
like new features or buck fixes should
be integrated in the existing
application and provided to the end user
in an automated fashion and you can only
do this by using cicd so ad op you
should know how to set up cicd server
how to integrate code repositories to
trigger pipeline automatically when
there is a change and to fix B faster
and to provide better quality software
to the end user very fast in automated
fashion some popular C tool includes
genkins gitlab Circle CI travisci so
learning cicd will help you land your
job very very fast and it's very
important also known as heart of Dem Ops
so you should master
cicd all the tools that we have
mentioned till now will help you deploy
an applic ation on the cloud or on
containers anywhere but once the
software or your application is in
production it is very important for you
to monitor it to track performance to
see if there are any issues check system
resources like CPU or Ram or anything so
one of the responsibilities of Dos
engineer is to set up software
monitoring infrastructure monitoring
collect logs and to visualize data to
check if there are any issues or if
system has less resources or not so you
need to learn tools like Prometheus
also loging tools like Cloud watch or
elk stack which is very important to
make sure that our application is
running without any issues and without
any problems congratulations now you
know all the tools and Technologies you
require to become a devops engineer but
devops is all about Automation and there
is always going to be something that you
can automate so to automate this you
will require scripting language and some
of you might argue that scripting is not
required or you don't need any
programming language in devops coding is
not a thing in devops but that's not
actually true you will require knowledge
of one programming language or a
scripting language because you need to
automate the manual process so some
popular scripting language can be bash
Powershell which are all OS related so
you can use bash in Linux in Powershell
and windows and non OS related are
python go Ruby uh which is what I would
suggest I would suggest you trying to
Learn Python which will help you
automate all the manual process like uh
rotating passwords of your databases
like starting a deployment starting a
build or a clearing C anything that you
are doing manually can be automated
through scripting languages using this
python or go or bash my suggestion would
be to start with python and if you love
python you will be unstoppable and you
will have more value as a devops
engineer and you don't need to learn any
programming language at a software
engineer level you just need to know
enough python or enough go just to write
scripts which can automate your things
you don't need to go deep in uh DSA and
all those things which are very common
questions that I get so just focus on
learning python in a way where you can
automate things by writing scripts so
just the basics not to advance only to
automate stuff so there you have it a
complete devop road map to learn things
from scratch along with this make sure
you have your LinkedIn perfect you
attend meetups and also enroll to the
simply learn devops program the field of
devops has grown exponentially over the
past few years and it's no surprise why
devops is a set of practices and tools
that aims to break down the SOS between
development and operations team and
streamline the software delivery process
by doing so devops enables organizations
to deliver high quality software faster
and more efficiently than ever before so
if you're interested in pursuing a
career in devops then this video on how
to choose devops as a career is for you
with the proper road map on how to get
started with it also do not forget to
subscribe to our YouTube channel and hit
the Bell icon to never miss an update
from Simply learn so without any further
Ado let's get started first is to
understand the role of a devops
professional before diving into the
specifics of how to become a devops
professional it's essential to
understand the roles and
responsibilities of a devops
professional a devops professional is
typically responsible for Designing
implementing and maintaining the
infrastructure automation tools and
processes required for the efficient
delivery of software this includes
collaborating with development teams to
integrate automation into the software
development process setting up and
maintaining the infrastructure required
for the software delivery pipeline such
as servers databases and network systems
developing and implementing automation
scripts to improve the efficiency of
software delivery process continuously
monitoring and analyzing the software
delivery process to identify and resolve
bottlenecks and improve
efficiency ensuring that the software
delivery process aderes to Industry best
practices and regulatory requirements so
now that you understand the roles and
responsibilities of a devops
professional let's take a look at the
road map to become a devops professional
so first is to learn the basics of
software development to Be an Effective
devops professional it's essential to
have a good understanding of software
development processes including
programming language ages software
development methodologies and Version
Control tools following that learn about
infrastructure and operations as a
devops professional you will be
responsible for managing infrastructure
including servers databases and network
system so it's essential to have a good
understanding of infrastructure and
operations this include learning about
servers operating systems Network
protocols and storage systems then
explore about automation tools and cloud
services automation is a critical part
of devops learning automation tools such
as anible puppet or shift can help you
streamline the software delivery process
and improve efficiency most organization
today use cloud services to deliver
software learning cloud services such as
AWS Azure or gcp can help you design and
Implement Cloud architecture that are
efficient and scalable well to help you
with learning devops and cloud services
simply learn has to offer as your
develop solution expert master's program
to help you become an industry ready
professional in this course you will
learn to plan smarter collaborate better
and ship faster with a set of modern
development services the link for this
course is mentioned in the description
do check them out once done with
Skilling you need to gain experience and
get certified as discussed before the
best way to gain experience in devops is
to work on real world projects this can
be achieved through internships
volunteering for open-source projects or
even taking on small projects on your
own this will allow you to put your
theoretical knowledge into practice and
gain hands-on experience along with it
there are various certification programs
available such as the devops Institute
certification program AWS certification
devops engineer and Microsoft Asha devop
solution certification once you have
gained experience and acquire the
necessary skills it's time to start
looking for job opportunities there are
very I job titles in devops such as
devops engineer site reliability
engineer automation engineer and release
engineer in top hiring companies in the
field include Amazon Google Microsoft
IBM and many others in today's
fast-paced and ever evolving world of
software development devops has emerged
as a transformative approach Bridging
the Gap between development and
operations team devops tools are portal
in streamlining the software delivery
process foring collaboration and
empowering organizations to achieve
continuous integration deployment and
delivery here we present a curated list
of top devops tools that have
revolutionized the industry the power of
these D tools lies in their ability to
enhance productivity scalability and
reliability enabling organization to
deliver high quality software faster and
with confidence from Version Control
Systems like git and subversion to
containerization platforms like doer and
kubernetes from configuration management
tools like anible and perpet to
monitoring and logging tools like
promeets every tool in every step
Fosters seamless software development
and deployment ensures system health and
provides valuable insights for
troubleshooting in this devop tools
tutorial we will learn the top devops
tools you must focus on to become a
successful devops professional if these
are the types of videos you would like
to watch then hit the Subscribe button
like and press on the Bell icon to never
miss any further content now over to our
training experts hello everyone without
further Ado let's get started before
diving into the tools let's Briefly
summarize the devops phases devops
phases are a series of interconnected
steps that streamline software
development and deployment here's a
crisp explanation of each stage first
plan Define project goals requirements
and Milestones plan takes prioritize
work and establish a road map for
development next is code developers
write and review code collaboratively
using Version Control Systems to manage
changes and ensure code quality the
third one is build convert the code into
executable software components including
compiling linking and packaging the code
base fourth step testing automated
testing is performed to validate the
software's functionality performance and
security and this includes unit testing
integration tests and system tests the
fifth stage is the release stage prepare
the software for deployment by
finalizing features documentation ation
and versioning ensure all necessary
approvals and compliance requirements
are met sixth one deploy move these
software to Target environment whether
it is on premises or the cloud this
involves setting up infrastructure
configuring resources and installing the
software now the seventh one operate
manage the software and infrastructure
in a production environment monitor
system helps address operational issues
and ensure smooth functioning the eth
one is monitoring phase continuously
monitor the software's performance
availability and security collect
metrics analyze logs and identify and
resolve any anomalies or bottlenecks
these stages form a continuous loop
allowing for iterative development rapid
feedback and continuous Improvement
devops enhances collaboration Automation
and efficiency throughout the software
development life cycle by integrating
development and operations team now
let's discuss the tools the primarily
used and prominent ones in each phase
starting with planning jir and Trello
are the most popular tools jira is a
popular project management tool that
helps teams and planning phase of devops
it enables teams to create and manage
tasks track progress and allocate
resources Trello Trello is a visual
collaboration tool that simplifies
project planning and organization it
provides a flexible and intuitive
interface where teams can create votes
list and cards to represent task
workflows and Milestones these tools
provide teams with the necessary
functionalities to plan organize and
track tasks ensuring Clarity
transparency and alignment of planning
phase of devops projects now proceeding
ahead we will discuss about the next
phase which is the coding phase the
important tools here are get Visual
Studio firstly git git is a widely used
distributed version control system that
plays a crucial role in coding phase of
devops it allows developers to attack
changes to their database collaborate on
a code with teammates and easily manage
different versions of their software now
the visual studio code Visual Studio
code is a lightweight versatile code
editor that provides a rich set of
features for developers it supports
various programming languages synag
highlighting debugging and integrated
terminal making it a popular choice for
coding in the devops context these tools
get vs code Pro provide developers with
the necessary infrastructure and
capabilities to manage code efficiently
collaborate effectively and ensure code
quality during the coding phase of
develops projects now let's proceed to
the next phase which is the build phase
we have genen gradal and Maven in this
particular segment firstly Jenkins
Jenkins is a widely used open source
automation server that plays a crucial
role in the built phase of devops it
enables teams to automate the build test
and deployment process facilitating
continuous integrity and delivery now
the gradal gradal is a powerful build
automation tool that provides a flexible
and scalable framework for building and
managing software projects it supports
multiple programming languages and
offers Advanced features like dependency
management incremental builds and
parallel execution and lastly Maven
Maven is a popular build management tool
that simplifies project setup dependency
management and build processes it uses a
declarative XML based configuration and
follows conventions over configuration
principles these tools Empower teams in
the Bel phase of devops by automating
the compilation packaging and
distribution of software the Streamline
the build process ensure Cod quality and
facilitate efficient integration with
other stages of devops pipeline now we
will discuss the testing phase we have
three most popular testing tools
selenium junit and postmen first
selenium selenium is a widely used open
source framework for automating web
browser testing it allows teams to write
and execute automated test across
different browsers and platforms
ensuring consistent functionality and
user experience next we have junit junit
is a popular testing framework for Java
applications it provides a simple and
standardized way to write and execute a
unit tests allowing developers to verify
and the correctness of individual code
units and lastly Postman Postman is a
powerful API testing and collaboration
platform it allows teams to design build
and automate test for restful API
Services ensuring their functionality
performance and security are integrated
Postman provides an intuitive interface
for creating and organizing test cases
making API requests and validating
responses now these tools Aid in the
test phase of devop by automating and
simplifying the testing process they
facilitate efficient testing of web
applications unit testing of code units
and comprehensive testing of apis
ensuring software quality and reducing
the risk of bugs and issues in
production environment proceeding ahead
we have the release phas anible AWS code
deploy are the most popular release
tools anible is a powerful open source
automation tool that helps teams
automate the release phase of devops it
allows for easy configuration management
deployment and orchestration of software
releases next AWS code deploy AWS code
deploy is a fully managed deployment
service provided by Amazon web services
it simplifies the relase fees by
automating application deployments to a
variety of computer environments these
tools assist teams and release phase of
develops by automating deployment
process ensuring consistent and reliable
releases and simplifying configuration
management now we are in the deployment
phase DOA cubes and AWS elastic bean
stack are the popular deployment tools
firstly Docker Docker is a popular
containerization platform that
simplifies the deployment phase and
devops it allows teams to package
applications and their dependencies into
lightweight portable containers Docker
provides a consistent and isolated
environment for running applications
enabling seamless deployment across
different infrastructures next we have
kubernetes kubernetes is an open source
container orchestration platform that
automates the deployment scaling and
management of containerized applications
it provides a robust framework for
managing clusters of containers across
multiple nodes followed by that we have
the elastic Bean stag AWS elastic Bean
stack is a fully managed platform as
service provided by Amazon web services
it simplifies the deployment phase by
abstracting the underlying
infrastructure and providing the easy to
ous platform for deploying and managing
applications these tools play a vital
role in deployment phase of devops by
automating application deployment
ensuring scalability and simplifying
infrastructure management next we will
be taking care of operate phase
promethus elastic stack P Duty are the
popular operate tools firstly Prometheus
Prometheus is an open- Source monitoring
and alerting Tool widely used in operate
phase it collects metrics from VAR
systems services and applications
allowing tools to monitor performance
availability and resource utilization
next we have elastic stack or elk stack
elk stack is also known as elastic
search long stash and kibana which is a
powerful combination of Open Source
tools used for log management and
Analysis in the operate phase elastic
search stores the indexes log L data
logs stet and processes logs and kabana
provides a userfriendly interface for
searching analyzing and visualizing log
data now the pag of Duty pag of Duty is
an incident management platform that
helps teams in the operate phase of
devops to respond and resolve incidents
quickly it provides a real-time alerting
on call schedules and escalation
policies ensuring that the right team
members are noticed and engaged during
incidents now these tools are
instrumental in the operate phase of
devop by providing monitoring log
management and incident response
capabilities and finally we have the
monitoring phase naos and grafana are
the most popular monitoring tools
firstly we will have it discussed about
the naos naos is a widely used open
source monitoring tool that plays a
crucial role in monitoring phase of
devops it enables teams to monitor the
health and performance of their systems
applications and network infrastructure
next we have grafana grafana is also one
of the popular data visualization and
Analytics tool called commonly used in
monitor phase of devops it integrates
with various data sources including
monitor systems and databases to create
visually appealing and interactive
dashboards these tools are essential in
monitoring phase of devops as they
provide the necessary capabilities and
monitoring visualization and Analysis of
system performance metrics and logs they
enables teams to proactively Monitor and
optimize their applications and
infrastructure ensuring Optimal
Performance availability and user
experience and with that we have
finished the the top devops Tools in
2023 so let's take a little scenario of
a developer and a tester before you had
the world of Docker a developer would
actually build their code and then they
send it to the tester but then the code
wouldn't work on their system the code
doesn't work on the other system due to
the differences in computer environments
so what could be the solution to this
well you could go ahead and create a
virtual machine to be the same of the
solution in both areas
we think Docker is an even better
solution so let's kind of break out what
the main big differences are between
Docker and virtual machines as you can
see between the left and the right hand
side both look to be very similar what
you'll see however is that on the docker
side what you'll see as a big difference
is that the guest OS for each container
has been illuminated Docker is
inherently more lightweight but provides
the same functionality it as a virtual
machine so let's step through some of
the pros and cons of a virtual machine
versus Docker so first of all a virtual
machine occupies a lot more memory space
on the host machine in contrast Docker
occupies significantly less memory space
the boot up time between both is very
different Docker just boots up faster
the performance of the docker
environment is actually better and more
consistent than the virtual machine
Docker is also very easy to set up and
very easy to scale the efficiencies
therefore are much higher with a Docker
environment versus a virtual machine
environment and you'll find it is easier
to Port Docker across multiple platforms
than a virtual machine finally the space
allocation between Docker and a virtual
machine is significant when you don't
have to include the guest OS you're
eliminating a significant amount of
space and the docker environment is just
inherently smaller so after Docker as a
developer you can build out your
solution and send it to a tester and as
long as we're all running in the docker
environment everything will work just
great so let's step through what we're
going to cover in this presentation
we're going to look at the devops tools
and where Docker fits within that space
we'll examp what Docker actually is and
how Docker works and then finally we'll
step through the different components of
the docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delight and improve the
efficiency of your customers if you look
at the vend diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate and monitor
when you can have both groups
interacting with each other on these
seven key elements then you can have the
efficiencies of an excellent devops team
so planning and c-base we use tools like
jit and geara for building we use Gradle
and Maven testing we use selenium the
integration between Dev and Ops is
through tools such as Jenkins and then
the deployment operation is done with
tools such as Docker and Chef finally
nagas is used to monitor the entire
environment so let's step deeper into
what Docker actually is so Docker is a
tool which is used to automate the
deployment of applic appliations in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what doger is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different frame Frameworks so you may
have a framework such as rails perfect
and flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our ABNB
example so we have three rooms and a
kitchen and cupboard how do we resolve
this issue well we put a kitchen and
cupboard in each room we can do the same
thing for computers Docker provides the
suitable Frameworks for each different
application and since every application
has a framework with a suitable version
this space could also then be utilized
for putting in software and applications
that alone and since every application
has its own framework and suitable
version the area that we had previously
stored for a framework can be used for
something else now we can create a new
application in this instance a fourth
application that uses its own resources
you know what with these kinds of
abilities to be able to free up space on
the computer it's no wonder Docker is
the right choice so let's take a closer
look to how Docker actually works so
when we look at Docker and we call
something Docker we're actually
referring to the base engine which
actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
and this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker demon which runs
on the server itself will then check out
the interaction and the necessary
operating system pieces needed to be
able to run the container okay so that's
just an overview of the docker engine
which is probably where you're going to
spend most of your time but there are
some other components that form the
infrastructure for Dockers let's dig
into those a little bit deeper as well
so what we're going to do now is break
out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Damon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows it's going to be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the dock demon and the registry service
when you have your terminal window open
so you can actually use your terminal
window to create instructions on how to
build and run your Docker images and
containers if we look at the images part
of our registry here we actually see
that the image is really just a template
with the instructions used for creating
the containers which you use within
Docker the docker image is built using a
file called the docker file and and then
once you've created that Docker file
you'll store that image in the docker
Hub or R Street and that allows other
people to be able to access the same
structure of a Docker environment that
you've created the syntax of creating
the image is fairly simple it's
something that you'll be able to get
your arms around very quickly and
essentially what you're doing is you're
creating the option of a new container
you're identifying what the image will
look like what are the commands that are
needed and the arguments for within
those commands and once you've done that
you have a definition for what your
image will look like so if we look here
at what the container itself looks like
is that the container is a standalone
executable package which includes
applications and their dependencies it's
the instructions for what your
environment will look like so you can be
consistent in how that environment is
shared between multiple developers
testing units and other people within
your devops team now the thing that's
great about working with Docker is that
it's so lightweight that you can
actually run multiple Docker containers
in the same infrastructure and share the
same operating system this is its
strength it allows you to be able to
create those multiple environments that
you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use
the docker registry which is an OP
Source serviz service for hosting and
distributing the images that you have
defined you can also use Docker itself
as its own default registry and Docker
Hub now something it has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public report repositories
can be used to host the docker images
which can be accessed by anyone and I
really encourage you to go out to Docker
and see the other docket images that
have been created because there may be
tools there that you can use to speed up
your own development environments now
you will also get to a point where you
start creating environments that are
very specific to the solutions that you
are building and when you get to that
point you'll likely want to create a
private repository so you're not sharing
that knowledge with the world in general
now now the way in which you connect
with the docker registry is through
simple pull and push commands that you
run through terminal window to be able
to get the latest information so if you
want to be able to build your own
container what you'll start doing is
using the pull commands to actually pull
the image from the docker repository and
the command line for that is fairly
simple in terminal window you would
write Docker pull and then you put in
the image name and any tags associated
with that image and use the command
pools so in your terminal window you
would actually use a simple line of
command once you've actually connected
to your Docker environment and that
command will be Docker pull with the
image name and any Associated tags
around that image what that will then do
is pull the image from the docker
repository whether that's a public
repository or a private one now in
Reverse if you want to be able to update
the docker image with a new information
you do a push command where you take
take the script that you've written
about the docker container that youve
defined and push it to the repository
and as you can imagine the commands for
that are also fairly simple in terminal
window you would write Docker push the
image name any Associated tags and then
that would then push that image to the
docker repository again either a public
or a private repository so if we recap
the docker file creates a Docker image
that's using the build commands a Docker
image then contains all the information
necessary for you to be able to execute
the project using the Dogg image any
user can run the code in order to create
a Docker container and once a Docker
image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so the five key takeaways
here so with a virtual machine you're
able to create a virtualized environment
to run an application on an operating
system with Docker it allows you to
focus on just running the application
and doing it consistently it improves
the ability for teams to be able to
share environments that are consistent
from Team to team it's highly productive
and it's really quick and easy to
configure the architecture of Docker is
really primarily built out of four
components of which the one that you'll
use the most is the client server
environment where as a developer you
have a client application running on
your local machine and then you connect
with a server environment where you're
getting the latest information about
that container that you're building a
solution for and then finally what we
see with the workflow improvements with
Docker is that the goal is to be able to
be more efficient to be able to be more
consistent with your development
environments and be able to push out
those environments whether it goes to a
test person to a business analyst or
anybody else on your devops team so they
have a consistent environment that looks
and acts exactly like your production
environment and can be eventually pushed
out to a production environment using
tools such as puppet or Chef so you're
creating a consistent operations
environment so before we begin let me
give you an intro to Docker so what is
Docker Docker is a tool which is used to
automate the deployment of applications
in lightweight containers so when I say
containers I mean a software package
that cons of all the dependencies
required to run the particular
application and when you deploy
containers you're basically providing
the capability for the application to
run in any kind of environment so
Dockers have multiple features some of
which are that multiple containers can
run on the same Hardware it has very
high productivity when compared to
Virtual machines it maintains isolated
applications and has a very quick and
easy configuration process so now let's
begin with the demo we'll be installing
Docker on an Ubuntu system so this is my
system I just open the terminal so the
first thing you can start with is
removing any Docker installation that
you probably already have present in
your system if you want to start from
scratch so this is the command to do so
P sudo app get remove
Docker Docker engine docker.io enter
your
password and Docker is removed so now
we'll start from scratch and we'll
install Docker once again before that
I'll just clear my
screen okay so before I install Docker
let me just ensure that all these
softwares on my system currently is in
its latest
date so sudo app get
update
great so that's done next thing we'll
actually install our
Docker so type in pseudo
apt
get
install
Docker now as you can see here there's
an error that's occurred so sometimes
it's possible that due to the
environment of the machine that you're
working in this particular command is
does not work in which case there's
always another command that you can
start
with just type Docker
install and that by itself will give you
the command you can use to install
Docker so as it says here Pudo appt
install docker.io is a command that we
will need to execute to install docker
and after that we'll execute the sudo
snap install Docker so sudo apt install
docker.io
first and this will install your
Docker after that done we will have suro
snap install Docker so snap install
Docker installs a newly created snap
package they are basically some other
dependencies for Docker that you'll have
to install
of course since this is the installation
process for the entire doer IO it will
take some
time
great so our Docker is installed the
next thing we do as I mentioned earlier
is that we need to install all the
dependency packages so the command for
that
is sud sudo snap install
talker enter your
password
so with that we have completed the
installation process for Docker but
we'll perform a few more stages where we
will test if the installation has been
done
right so before we move on with the
testing for Docker let's once again just
check the version that we have
installed so for that the command is
docker
version and as you can see doer version
17.12.19
doer image hello world has been pulled
onto your
system so let's see if it's actually
present on your system now the command
to check this is pseudo Docker
images and as you can see here hello
world repository this is present on our
system currently so the image has been
success y pulled onto the system and
this means that our Docker is working
now we'll try out another
command suro Docker PS minus a this
displays all the containers that you
have pulled so far so as you can see
here there are three hello world images
displayed and all of them are in exited
state so I did this demo previously too
which is why the two hello worlds which
is created 2 minutes ago is also
displayed here and the first hello world
which has been created a minute ago is
the one we just did for this demo now as
you have probably noticed that all the
hello world images over here all these
containers are in the exited state so
when you give the option for dock a PS
minus a where minus a stands for all it
displays all the containers whether they
are in exited or running state if you
want to see only those containers which
are in their running State you can
simply execute pseudo Docker
PS sudo
Docker
yes and as you can see no container is
visible here because none of them are in
running State and with that we come to
an end of our Docker installation now
Docker is something which is available
for most of the operating systems
different different platforms so it
supports both the Unix and the windows
platform as such so um Linux through
various commands we can do the
installation but in the case of Windows
you have to download the exe file and a
particular installer from the dockerhub
websites you can simply Google it and
you know will get a kind of link from
where you will be able to download the
package so let's go to the Chrome and uh
try to search on for the windows stren
uh particular installer you will get a
link from dockerhub you download it you
get the stable version you get the edge
version whichever version you want you
wish to download you can download it so
let's go back to the Chrome so here you
have the docker desktop for Windows so
you can go for the stable or you can go
for the edge right so you also have the
comparison that what is the difference
between these two versions right so um
the particular Edge version is something
which is getting releases every month
and uh the um stable version is getting
the releases every qu so they are not
doing much of the changes to the stable
version as compared to the edge there so
you just have to double click on the
installer and that will help you to do
the installation of the process so let's
get started so you just click on the get
instable version so when you do that the
uh particular installer is going to
install now it's going to take like
around 300 MB there so that's the kind
of installer which is available so uh
once the installer is downloaded so what
you can do is that you can actually go
ahead and you can uh proceed with uh
doing the double click on this installer
when you double click on that you have
to proceed with some of the steps like
you know from the GUI itself you are
going to proceed with these steps so
we'll wait for 10 to 20 seconds more and
then the installer will be done and then
we can do the double click and the
installation will proceed so another
thing is that uh there is a huge
difference between the installer like
for example in case of Unix the
installer is a little bit less but in
case of windows it's a gy is also
involved and there are a lot of bandies
which is available there so that's the
reason why you know the huge size is
there now it's available for free that's
for sure and it also requires the
Windows 10 professional or Enterprise
64bit there so um if you are working on
some previous uh version of operating
systems like Windows 7 and all you have
the older version called Docker toolbox
so they used to call it as like Docker
toolbox earlier but now they are calling
it as an Docker desktop with the new
Docker Windows 10 support as such here
so another couple of seconds and then
the installer will be done and then we
will be able to proceed with the
installation so let's see that how much
progress is there to the download so
we'll click on the downloads and here
still we have some particular
installations or some download going on
so we'll wait for some time and uh once
the installation is done then we'll go
back and uh we'll proceed with
installation so couple of seconds so
it's almost done so I'll just click on
this one you can go to the directory to
the downloads and you can double click
on that also but if you want to do the
installation you can click on this one
also and it will ask for the approval
yes or no you have to provide now once
that is done so um a desktop kind of a
GUI component will open there so it will
start proceeding with installation so
it's asking whether you want to add a
desktop the shortcut to desktop so you
can say okay I'm going to click on okay
so it will unpack the
files all the files uh which is required
for Docker to successfully install that
is getting unpacked over here so it will
take some time to do the installation
because it's doing a lot of work here so
you can just wait for till the execution
of the installer to be completed and
once the installer is done you can open
your command line and start working on
the
docker so taking some time to extract
the
files now it's asking us to you know do
the close and uh do the restart so once
that is done you will be able to proceed
further and you can just you know run
the command line and uh any Docker
command if you can run so that will give
you the response whether the docker is
installed or not so you can see here
that Docker is you know something which
is installed so you can run like Docker
version you will be able to get a
version of the client when you do the
restart of the machine then at that
moment of time the docker server will
also be started and then this particular
error message will go off right now the
docker demon is not up and running
because the installation requires a
restart and when you close on this one
and go for the restart the machine will
be restarted here so this is the way
that how exactly we can go for a Docker
installation and we can go on that part
so if you are looking to become a Docker
expert consider the post-graduate
program in devops by simply learn to
gain expertise in Docker this program
offers Blended learning that combines
live online devop certification classes
with interactive Labs that will give you
handson on experience you will gain
expertise in the tools like terraform
Maven anible Jenkins doer junit G and
more we prefer one year of Prior
experience to enroll in this course so
why wait enroll the course from the link
in the description box so let's look at
some of the more advanced concepts
within the docker environment and we're
going to look at two Advanced components
one is Docker compose and the second is
Docker swamp so let's look at Docker
compose Docker compose is is really
designed for running multiple containers
as a single service and it does this by
running each container in isolation but
allowing the containers to interact with
each other as was stated earlier on you
would actually write the composed
environment using yaml as the language
in the files that you would create so
where would you use something like
Docker compose so an example would be if
you are running an Apache server with my
SQL database and you need to create
additional containers to run additional
services without the need to start each
one separately and this is where you
would write a set of files using doer
compos to be able to help balance out
that demand so let's now look at Docker
swarm so Docker swarm is a service that
allows you to be able to control
multiple Docker environments within a
single platform so what you actually are
looking at doing is within your Docker
swarm is we're treating each node as a
Docker Damon and we're actually having
an API that's interacting with each of
those nodes there are two types of node
that you're going to be getting
comfortable working with one is the
manager node and the second is the
worker node and as you'd expect the
manager node is the one sending out the
instructions to all of the worker nodes
but there is a two-way communication
that is happening the communication
allows for the manager node to be able
to manage the instructions and then
listen to and receive updates from the
working node so if anything happens
within this environment the magic node
can react and adjust the architecture of
the worker node so it's always in sync
was really great for large scaled
environments so finally let's go through
what are some of the basic commands you
would with use within Docker and once
we've gone through all these basic
commands we'll actually show you a demo
of how you'd actually use them as well
so if we're going to go in probably the
first command is to install Docker and
so if you have yum installed you just do
yum install Docker and you'll install
Docker onto your computer to start the
docker Damon is you want to do system
CTL start Docker the command to remove a
Docker image is Docker RMI and then the
image ID itself and that's not the image
name that's the actual alpha numeric ID
number that you want to uh grab the
command line to download download a new
image is Docker pull and then the name
of the image you'd want to pull and by
default you're going to be pulling from
the docker default registry that will
then connect to your dock Damon and
download the images from that registry
CL the command line to run an image is
Docker run and then the image ID and
then we have the if we want to pull
specifically from dockerhub then we
would have uh Docker pull and then the
image name and colon its tag
to pull build an image from a Docker
file you would do Docker build- T and
then the image name and colon tag to
shut down the container you do Docker
stop container ID the access for running
a container is Docker exact it container
ID bash so we've gone through all the
different commands but let's actually
see how they would actually look and
we're going to go ahead and do a demo so
welcome to this demo where we're going
to go ahead and put together all of the
different commands that we outlined in
the presentation for Docker uh first is
just to list all of the docker images
that we have so we do pseudo Docker
images and we enter in our
password and this will Now list out the
images that we've created already and we
have three images
there so let's go ahead and pull a
Docker image so to do that we'll we'll
go ahead and type pseudo
Docker and actually we don't want to do
image we want to select pull and then
the name of the image that we want to
pull which is going to be my
SQL and by default this is actually
going to go ahead and use the latest
MySQL command MySQL image that we have
so it's now going ahead and pull this
image it's going to take a few minutes
depending on your internet connection
speed it's kind of a large file that has
to be downloaded so we'll just wait for
that to
download
you see the others have completed just
waiting for this last file to download
almost there once that's done we're
going to go ahead and do is we'll
actually uh run the dock container and
create the new container using the image
that we just downloaded but we have to
wait for this to download
first all right so the image has been
pulled from Docker Hub and let's go
ahead and create the new Docker
container so we're going to do pseudo
docka run
dashd
dasp
0.0.0.0
colon
80 colon
80 and then we put in my SQL colon
latest so we have the latest
version and we have our new token and
that shows our new docket container has
been
created and let's go ahead and see if
the container is running and we'll do
pseudo
Docker
PS to uh list all the running containers
and what we see is that the container is
not listed there which means it's
probably not running so let's go ahead
and list out all of the images that we
have within docco so we can see whether
it's actually listed there so we'll do
ps- a and yes there we are we can see
that we do have our new container my SQL
latest and it was created 36 seconds ago
but it's in the exited mode so what we
have to do is we have to change that
status so it's actually
running so let's change that to running
State we'll do
pseudo
Docker
run
Dash it dash
dash
name and we can name
it
SL
SQL my
SQL slash bin
slash
Dash and that's now going to be in the
root and we'll exit out of that and now
if we list out the do containers we
should see it is now an active
container so Docker
start and then we'll start the say and
then and we should now see
it there we are it's now in the running
State
excellent and we can see that it was
updated 6 seconds
ago we're going to go ahead and we're
going to clear the
screen okay now what we want to do is
remove the docker container so we're
going to do is checklist of images that
we
have and so pseudo Docker images here
are the images that we have and we have
my SQL is listed and what we want to do
is delete my SQL and to do that we're
going to type in in PSE sudo doer rm- F
image my
SQL run that command and what we'll find
is the image uh there's no such image oh
okay so what we actually have to do is
we have to go and see that the image is
now gone it's uh being removed excellent
it's exactly what we wanted to
see and we can also delete an image by
its image ID as
well however if an image is running and
active we have to kill that image first
so we're going to go ahead and we're
going to select the image ID we'll copy
that and it's going
to we paste that it won't be able to
actually run correctly because the image
is active so what we have to do now is
stop the image and then we can kill
it so it's in the running state so we
have to
do so we do pseudo darker kill and just
kill SL and that will kill the container
and now we'll see that the container has
gone and now we can delete the image and
that's going to be be the image gone
with the image ID b boom easy
peasy okay let's go ahead on to the next
exercise which is
to so here we are we've listed all of
the uh containers and they're all gone
so let's go to the next exercise final
exercise which is to actually create a
batch image and we going do a batch HTTP
image so let's go ahead and write that
out so it's going to be Docker
[Music]
run
dashd dash dash
name white is that's going to be the
name of this HTTP service-
P 8080 colon 80-
V open
quotes dollar sign
PWD close quotes
colon SL
USR SL
looc slash Apache 2 slht
dos slash
httpd semicolon
2.4 run
that our password
again
so what we see is the port is already
been used so let's go ahead and see
which ports let's go see if we can
change the port or see what ports are
running so let's do pseudo images and
see which ports are being used because
it's either the the port or the name
hasn't been put in correctly so pseudo
docket
images
PS sud sudo Docker ps- a
and yep there's Port 80
there so we'll clear the
screen so we're going to change the
container name CU I think we actually
have the wrong container name here so
let's go in and change that and we'll
paste that in and voila here we go now
working and we'll just double check and
make sure everything's working correctly
so to do that we'll go into our web
browser and we'll type in soon as
Firefox opens
up type in Local
Host colon
8080 which was the the port that we
created and there we are it's a list of
all the files which shows that the
server is up and running so let's have a
look at what we have in our current
environment so today when you actually
have your standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce
significantly the different elements
that you would normally have within a
virtualized environment so we're able to
get rid of the the bins and the so we're
able to get rid of the guest OS and
we're able to eliminate the hypervisor
environment and this is really important
as we actually start working and
creating environments that are
consistent because we want to be able to
make it so it's really easy and stable
for the environment that you have within
your uh Dev and op environment now
critical is getting rid of that
hypervisor element it's just a a lot of
overhead so that's let's have a look at
a container as an example so here we
actually have a couple of examples on
the right hand side we have different
containers we have one container is
running Apache Tomcat in a with Java a
second container is running SQL server
and a microsoft.net environment the
third container is running python with
mySQL these are all running just fine
within the docker engine and sitting on
top of a host OS which could be Linux it
really could be any host OS within a
consistent infrastructure and you're
able to have a solution that can be
shared easily amongst your teams so
let's have a look at an example that
you'd have today if a company is doing a
traditional Java application so you have
your developers working in JBoss on his
system and he's coding away and he has
to get that code over to a tester now
what will happen is that tester will
then typically in your traditional
environment then have to install J boss
on their machine and get everything
running and and hopefully set up
identically to the developer chances are
they probably won't have it exactly the
same but they'll try to get it as close
as possible and then at some point you
want to be able to test this within your
production environment so you send it
over to a system administrator who would
then also have to install JBoss on their
environment as well yeah this just seems
to be a whole lot of duplication so why
go through the problem of installing
JBoss three times and this is where it
things get really interesting because
the challenge you have today today is
that it's very difficult to almost
impossible to have identical environment
if you're just installing software
locally on devices the developer
probably got a whole bunch of
development software that could be
conflicting with the Jos environment the
tester has similar testing software but
probably doesn't have all the
development software and certainly the
system administrator won't have all the
tools that the developer and test to
have their own tools and so what you
want to be able to do is kind of get
away from The Challenge you have of
having to do local installation on three
different computers and in addition what
you see is that this uses up a lot of
effort because when you're having to
install software over and over again you
just keep repeating doing really basic
foundational challenges so this is where
Docker comes in and Docker is the tool
that allows you to be able to share
environments from one group to another
group without having to install software
locally on a device you install all of
the code into your Docker container and
simply share the container so in this
presentation we're going to go through a
few things we're going to cover what
Docker actually is and then we're going
to dig into the actual architecture of
Docker and kind of go through what
Docker container is and how to create a
Docker container and then we'll go at
through the benefits of using Docker
containers and then the commands and
finalize everything out with a brief
demo so what is darker so Docker is as
you'd expect because all the software
that we cover in this series is an open
source solution and it is a container
solution that allows you to be able to
containerize all of the necessary files
and applications needed to run the
solution that you're building so you can
share it from different people in your
team whether it's a developer tester or
system administrator and this allows you
to have a consistent environment from
one group to the next so let's kind of
dig into the architecture so you
understand why docker runs effectively
so the docker architecture itself is
built up of uh two key elements there is
the docker client and then there is a
rest API connection to a Docker Damon
which actually hosts the entire
environment within the docker host and
the docker demon you have your different
containers and each one has a link to a
Docker registry the docker client itself
is a rest service so as you'd expect
rest API that sends command line to the
docker Damon through a a terminal window
or command line interface window and
we'll go through some of these demos
later on so you can actually see how you
can actually interact with Docker the
docker Damon then checks the request
against and the docker components and
then performs the service that you're
requesting now the docket image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the actual container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of all of your Solutions the actual
Docker registry itself is an environment
for being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so the
important part of a duckin container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and your libraries and it
allows the teams to be able to build out
and run exactly the right environment
that the developer intended what's
interesting though is the natural
applications then will run in isolation
so they're not impacting other
applications that's using dependencies
on other libraries or files outside of
the container because of the
architecture it really uses a lot less
space and because it's using less space
it's a much more lightweight
architecture so the files and the actual
folder itself is much smaller it's very
secure highly portable and the boot up
time is incredibly fast so let's
actually get into how you would actually
create a Docker container so the docker
container itself is actually built
through command line and it's built of a
file and Docker image so the actual um
Docker file is a text file that contains
all the instructions that you would need
to be able to create that Docker image
and then we'll actually then create all
of the project code with inside of that
image then the image becomes the item
that you would share through the docker
registry you would then use the command
line and we'll do this later on select
docket run and then the name of the
image to be able to easily and
effectively run that image locally and
again once you've created the dock image
you can store that in the docker
registry making it available to anybody
within your network so something to bear
in mind is that Docker itself has its
own registry called dockerhub and that
is a public registry so you can actually
go out and see other dock images that
have been created and access those
images as your own comp you may want to
have your own private um repository so
you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hub where you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a Docker image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is a build command and another
one is to actually put it into your
registry which is a push command so if
you want to get a image from a Docker
registry then you want to use the pull
command and a pull command simply pulls
the image from the registry um in this
example using NG as our registry and we
can actually then pull the image down to
our test environment on our local
machine so we're actually running the
container within our Docker application
on um local machine we're able to then
have the image run exactly as it would
in production and then you can actually
use the Run command to actually use the
docker image on your local machine so
just a you know a few interesting
tidbits about the docking container once
the container is created a new layer is
formed on top of the dock image layer
called the container layer each
container has a separate read right
container layer and any changes made in
that docking container is then reflected
upon that particular container layer and
if you want to delete the container
layer the container layer also gets
deleted as well so you know why would
using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external dependencies
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so there's
no having to install additional pieces
of software such as the example we gave
with Jake boss at the beginning of the
presentation now the containers are
really lightweight so it makes it very
easy to share the containers amongst
your teams whether it's a developer
whether it's a tester whether it's
somebody on your operations environment
it's really easy to share those
container containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation um which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
is you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
Docker compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
Within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands in the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docket image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line depending whether
you're running Linux PC or Mac and we're
going to go ahead and the first thing we
want to do is see what our Docker image
lists are so we can go pseudo Docker
images and this will give us well first
we're entering our password so let's go
enter that in and this one now giv us a
list of our Docker images and here are
the docker images have already been
created in the system and we can
actually go ahead and actually see the
processes that are actually running so
I'm going to go ahead and open up this
window a little bit more but this will
show you the actual processes and the
containers that we actually have and so
on the far left hand side you see under
names we have learn simply learn bore
cool these are all just different ones
that we've been working on so let's go
ahead and create a docket image so we're
going to do
pseudo
Docker
run-
d-p
0.0.0.0
colon 0 colon
80 obuntu and this will allow us to go
ahead and run an obuntu image and this
will R the latest image and what we have
here is a hash number and this hash
number is a un unique name that defines
the container that we've just created
and we can go ahead and we can check to
make sure that the container actually is
present so we're going to do so pseudo
doer. PS and this actually show us on
there so it's not in a running state
right now but that doesn't mean that we
don't have it so let's list out all the
containers that are both running and in
the exit SE so let's do sucker ps- a and
this lists all the containers that I
have running on my machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called
learn and these are all running Ubuntu
and this is the one that we had created
just a few seconds
ago let's open it up and there we go so
let's change that to that new Doc
container to a running state so scroll
down and and we're going to type
pseudo
[Music]
Docker
run
dashit Das
Das name
my um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment and we'll put in the image
name which is
BTU and dash bin Dash
bash and it's now in our rout and we'll
exit out of
that so now we're going to go ahead and
start the new my Docker container so PSE
sudo
Docker
start
my and we'll get the container image
which will be my Docker my
docker return and that started that
Docker image and let's go ahead and
check against the other running Docker
images to make sure it's running
correctly so pseudo Docker
PS and there we are underneath name on
the right hand side you have to see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds quite fast so we
want to rename the container let's use
the command pseudo
docker rename we can take another Docker
image this grab this one and we'll put
it in rename and we'll rename and we'll
put in the old container name which is
image and then we'll put in the new
container name and let's call
it
purple so now the container image that
had previously been called image is now
called Purple so we do pseudo Docker
PS to list all of our Docker
images and if we scroll up and there
there we go purple how easy is that to
rename an
image and we can go ahead and use this
command if we want to stop container so
we're going to write pseudo
Docker
stop and then we have to put it in the
container
name and we'll put in my Docker the
container that we originally
created and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docket images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side but if
we go ahead and we can list out all of
the dock images so you actually see it's
still there as an image it's just not in
an active stage it's what's known as in
an exit state so here we
go and there's my Docker it's in an
exited state so that happened 27 seconds
ago so if we want to to remove a
container we can use the following
command so pseudo
Docker
RM we
remove my Docker and that will remove it
from the exited
state
and we're going to go ahead and we're
going to double check
that and
yep yep it's not not listed there under
exit State
anymore it's
gone and there we go
that's where it used to be all right
let's go
back so if we want to exit a container
in the running state so we do pseudo
kill and then the name of the
container I think one of them is called
yellow let's just check and see if
that's going to kill
it oops no I guess we don't have one
called yellow so that's find out the
name of container that we actually have
so PSE sudo Docker kill oh we're going
to list out the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let write pseudo Docker
kill simply learn and that will actually
kill an active Docker
container boom there we
go and we list out all all the active
containers you can actually see now
that's the simply learn container is not
active
anymore and these are all the basic
commands for Docker container so let's
dig into what a Docker swarm is so a
Docker swarm is essentially a tool that
allows you to very easily create and
schedule multiple Docker nodes so really
two or more Docker nodes and you can
have quite a large number of Docker
nodes in a single swarm uh each node
itself is actually a Docker demon and
that demon is able to interact with the
docker API and have all the benefits of
being a full Docker environment one of
the other advantages you have is that
the each dock container within the swamp
can then be deployed and managed but as
a node in that entire clustered
environment so what we have here is a
breakdown of the five key elements
within a do ER environment you have the
docker container you have the demon the
docker images and the docker client and
Docker registry so the docker Damon
itself is what does all the work
interacting with the actual host
operating system to be able to manage
the docker containers so if we look here
here we have set up an environment where
we have three Docker container being run
with Docker and what we want to be able
to do is be able to interact with the
environment because what would happen if
we actually have something change in our
environment so we have the environment
set up as a Docker swarm and one of our
containers fails what we're able to do
is use the Swarm to be able to correct
that failure so the docker swarm manager
is able to come in and reschedule
containers and as you would imagine the
actual swarm note has full backups and
full redundancy for any kind of failures
that would happen and we would do all of
this work work through command line
interface so let's go through some of
the features that you have within the
actual Docker swarm itself so a key
feature for Docker swarm is that it is
fully decentralized which means that it
makes it very easy for teams to be able
to access and manage the environment as
you would expect as well is that the
communication that happens between the
manager and client nodes within the
swamp is highly secure and of course
this is really just a fundamental that
you should have for any kind of solution
but it is good to know that Dr swarm has
that buil in there is also autoload
balancing within your environment you
can actually script that into how you
write out and structure your swarm
environment and that load balancing then
also allows you to then convert that
swarm environment into a highly scalable
infrastructure and then rollback task
allows you to be able to roll back
environments to previous safe
environments so say something that does
get pushed out and something breaks
you're able to immediately roll back
into a safe environment so each of the
containers are pushed out and and are
controlled using services and they
actually happen to be rest Services
which make it very easy for you to be
able to integrate within your
environment and each of the different
Services contains a group of containers
of the same image Now by having this
structure it allows you to be able to
scale your application appropriate to
the demands on your service so if you
have a service that needs to have a
significantly larger number of services
for it to run you can actually scale
that appropriately and that can be
either Geo or demand based one of the
requirements for setting up a Docker
swarm is you must have at least one node
deployed so the way that the
architecture is set up is that you have
a manager node and a client node and
there must be one of each for the entire
environment to be able to work effective
ly so you know here we are just jumping
ahead of sales a little bit we have two
types of nodes in a Docker swarm we have
the manager node and we have the worker
node which is the client that does the
actual execution of the tasks the
manager note very similar to other
systems that we've talked about on
simply Lear allows you to actually
control and manage the actual tasks that
are being executed by the worker nodes
and the working noes as you can imagine
and then actually execute the
instructions that the manager are
sending out to it so here we have a
situation where we can illustrate what
would happen with a manager node sending
out commands to different worker nodes
the manager is fully aware of the status
of the entire swarm environment at all
times this is because of the two-way
secure communication that's going from
the manager to the worker environment
the workers as you'd expect are
accepting tasks that are being sent from
the manager so if the manager sends out
a task saying that you need to be
running as a MySQL environment then the
worker node will then convert to a MySQL
environment and all of these
environments are scripted and controlled
by you as the manager the actual worker
nodes themselves actually have a client
agent and that client will then
communicate all different states of the
infrastructure back to the manager so
that any time the manager node is in
full control of the entire swarm the
manager is the controller of this
environment and so you always want to be
able to ensure that the manager has full
access to all of the work that's
happening within the Swarm and this
allows you as the manager to be able to
control your swarm and very quickly be
able to react to any changes without
having to rely on manual installations
of software and hardware and as we
covered earlier on there is a rest API
that uses the communication over HTTP
from the manager to the worker node it's
interesting to note that it is a rest
API because if you wanted to you could
actually integrate that API into custom
applications and even then create
automated docket images to be created on
demand from third party solutions that
you may want to create so one of the
things that's a really a big Advantage
with having a swarm is that once you've
actually created the Swarm any of the
services um that you create can be
accessed by any node of the same cluster
one of the things you do have to do
though is you have to specify what
container image that you're going to use
when you're creating a new service and
you can do that either through a
centralized Docker Hub environment or
through your own private Docker Hub
environment one of the things that's
interesting is that you can set up uh
commands and services to be either
Global or replicated a global Service
will allow you to run a service
consistently on every node within the
Swarm whereas a replicated service will
only push out functionality and tasks to
specific worker nodes within a swarm so
you may be asking yourself hey isn't a
service and task the same thing no
they're they're kind of not in the
docker world and the difference is is a
service is a description of a task or
the state whereas the actual task is the
work that needs to get done and that's
the differentiator here so what you can
do as a Docker user is you actually
create services and then you can then
Define when you want them to start as
tasks now what's interesting is that
when you do assign a task to a node that
same task cannot be assigned to another
node also what's interesting is that you
can actually have multiple managers
within a Docker swarm environment if you
do however go down this path you have to
elect one manager to be the primary
manager and the other managers to be
secondary managers in many ways those
secondary managers are really similar in
concept to worker nodes in which they
have the capability of a man manager but
they are dependent on that single
primary manager to be able to provide
the right instructions and for services
and tasks to the entire swarm
environment so if we kind of recap some
of these we have a command line
interface which allows us to create and
connect via apis and that those apis
that we connect to in our swarm
environment allows us to do
orchestration via tasks and services the
task allocation allows us to allocate
work to tasks via their IP address which
allows them to execute them on the work
and then the worker nodes themselves
have to connect to the manager node to
be able to check for when tasks come in
so that they are keeping a consistent
communication back and forth across the
entire swarm and then the final stage is
to actually execute the tasks that have
been assigned from the manager note to
the work note so that you have a
successful execution of the solution
you're looking to build right so with
that said let's go ahead and we're going
to do a demo so we're going to go ahead
and going to do a demo and showing how
you can run Docker swarm for this you'll
need to have both a virtual environment
running your manager and your working
environment so here we have our worker
and uh here we have our manager so we're
just going to open up terminal window
and we're going to go into the manager
for terminal window so the following
command here is used to initialize
Docker swam I'm just going to type this
in so PSE sudo Docker swam in it and we
put the IP address for the network we're
connecting
to and we put in our
password and here we have now
connected and this is our manager node
and what we want to be able to highlight
is the specific token which identifies
the docker swarm environment and this is
our token right here so we're going to
copy
that because we'll need to use that for
our worker environment so here we are in
the worker environment and we'll use the
token that we just copied and um as a
way to be able to connect to the manager
and connect to the docker swarm
environment and so PSE
sudo we'll paste in the docker swarm and
we'll join that and now here we shows
that we have joined the Swarm
environment as a
worker and now we're in the manager we
can actually show that we actually have
that worker in the environment so
there's Docker node LS to out all of the
items in that
environment sorry we actually did that
the wrong area so we on the right pseudo
doer node LS to list out all the nodes
in the Swarm and there you are you see
we've actually connected the worker to
the manager node and both are active and
the manager is the leader and the work
of virtual box is in the
swamp Okay we're going to go ahead and
create a new service so Docker swarm
create and we're going to just change
the name to hello world for the new
service and we're going to use the
Alpine image from docket.com takes just
a moment for it to
run there we
go and there we go Services
converged so what we have now is a new
service that has been created let's go
ahead and we'll list out the services
and here we have our new Hello World
Service and we go ahead and check the
dock and containers and we're going to
do Docker PS and here's our Alpine it's
running the latest image and that's the
docker container
ID
and we can see that it was just
updated and so what we're going to do is
use the same command in the worker node
and make sure everything is working over
there so go to our worker paste that
in so what we see here is there is no
image or container created because it's
not in the worker node it would be in
the manager node so we can go ahead and
create the new service
and we're going to make the Mode Global
and that way the service is available
across all of the
Swarm so we're just going to execute
that
work takes a
moment
and there we are done work has been
completed and we have the two different
IDs which shows that two different
services are running and if we go back
to the worker mode and we run pseudo
doer PS again and there we are now we
see that we have the hello world new
virtual environment has been added with
the image of Alpine and we also have the
new token ID for
that if we want to kill a node from the
swamp we can use the following command
which is sud doco swamp leave-- force
and that will force the nodes to leave
the Swarm so one of the questions that
will come up though is what if you want
to use two containers in a single
service and an example of this might be
like a web server that is using a
database but the database may be in one
container and the web server may be in
another container which is very typical
for your own web and database
infrastructure so very similar scenario
but how would you do this with Docker so
by using just Docker itself it's
actually very difficult to do this you
can do it but it's just very time
consuming and just not a very good use
of your productivity but with Docker
composed you can actually do the whole
process quite quickly and easily and
hence the reason why we have Docker
compos to allow you to have two or more
Docker containers running in the same
environment being able to communicate as
if they were running in a production
environment so let's dig into what
Docker compose is so let's consider a
scenario that you'd have today so mintra
is a fashion website similar to Amazon
and you go to mtro with your web browser
and on that website you would go through
a number of activities such as logging
into your account browsing a catalog uh
you'd have your checkout application
server and you go through the typle
process of um building out an
application behind each of these
services are different products such as
you know you have an account database
you'd have a product database different
checkout processes and these would all
be run behind the scenes and each of
these can be considered a micros service
so the more microservices you build into
your environment the more complexity
you're adding and the more of value it
would be to have each of these services
in their own container but as a
developer you want to be able to jump
from one container to another container
so your login account can then be passed
onto your product catalog and then be
able to move through the whole process
so the environment that you would see
today would be small like what was
showing right now where you'd have your
server and your database and you may
have a server running on Apache tom cat
or a SQL Server you'd have different
databases and you'd want to be able to
have each of those in their own
containerized environment so the way
that Docker compose runs is that it
works as a single service so you have
the docker compos running multiple
containers but the perception is of a
single service run the thing that's
great though is that each of those
containers will run in their in
isolation and but they can can interact
with each other so unlike a normal
Docker environment where you have
multiple Docker images running
completely in isolation of each other
these are images that will run in
isolation but can interact with each
other the docker composed files are very
easy to write they're all written with a
scripting language called yaml and yaml
is an XML based language it actually
stands for yet another markup language
but it's an XML based language and it's
very easy to use the great thing about
learning yaml is that a lot of
open-source tools in the devops
environment use yaml as the scripting L
and then finally in docket compose as a
user you can actually go in and Trigger
all your services within the containers
to start with a single command as you
can imagine this dramatically reduces
the amount of work that you as a
developer have to do so let's take an
example of if you're running a container
one with a Nix um server and one with a
reddish database uh you can create a
yaml docket compos file that actually
has the instructions on the containers
needed to build out both environments
and then you can run each of those
environments separately but have them
connected as a single service so the
benefits of dock and compose is that you
have a single host um deployment
environment you can run this all on one
piece of Hardware you can have a quick
and easy configuration using yaml
Scripts the productivity that you get as
a developer significantly increases
because you're not having to have time
wasted trying to configure just
traditional Docker containers by
themselves you now have a way of being
able to interact with those containers
and then finally security is at the
center of all of these containers so
each of those containers are completely
isolated from each other and they are
controlled with the same level security
that you would have with a traditional
Docker image but there is a question
question here around Docker compose
isn't it similar in concept to Docker
swarm because at a high level it kind of
seems like both do the same thing and
the answer is well actually no they're
different there are similarities but
there are also differences and
particularly for a developer these
differences will really start coming out
in scale as you start working on your
Solutions so let's look at Docker
compose so Docker compose will allow you
to create multiple containers on a
single host and that's the important
part having a single host which is maybe
your development PC with Docker swarm it
also allows you to create multiple
containers however you have to manage
those multiple containers on multiple
hosts which makes a lot of sense if
you're running an operations environment
but not so much if you're doing a
development environment and darker
compose is scripted with yaml which is
very easy for you to be able to control
your scripts Dr swarm doesn't have a
scripting technology like yo so it's a
little bit harder to work with so what
we have here is just some basic commands
that allow you to um get up and started
working with dock compose in our next
demo we're actually going to take you
through how to use all of those commands
so we're going to go ahead and do a demo
on how to use Docker cose um the first
thing you want to do is check that you
have Docker installed so what we're
going to do is we're going to open up
the command window and we're going to
type in Docker you'll be able to see now
that we have Docker installed uh if you
don't have Docker installed go ahead and
install Docker you can go to Google
Search to install Docker and they'll
give you the instructions for that we
also have videos that we've already done
if once you have dock installed then you
want to do a Google search on Docker
compose or go to the docket.com website
and select for the compose section and
look for the install Docker compose
instructions so we already have another
tab which has uh the instructions
already installed you'll see on the left
hand side there are a number of Docker
tools you want to expand the docker
compose section select install compose
and then on the right hand side you'll
have different install options for Mac
windows and Linux uh if installing on
those platforms make sure you're copying
uh the correct command lines uh there's
a couple command lines you want to copy
copy the first one over and we paste
that in and it's going to go ahead and
download all of the files for compar it
takes a while but uh we have to get
another file so while it's downloading
we'll go get the other command line just
go back and grab it real quick that was
a little fast there but we'll copy that
over there was a second command
instruction and post that command yep
there's the command right there and that
will apply the appropriate binary and
once you've done that you can actually
go ahead and validate that you have the
correct version of Docker compos
installed and for that you want to write
the command Docker Das compos POS
space-- version and that will then give
you a version build once you know that
you have a version build that means that
you have everything installed correctly
on your machine so you can see here that
we have a version number which means we
have everything installed you might see
a different version number depending on
how old the video is when you're
watching it but we're using very content
at the moment and just go ahead and
clear the screen and now what we're
going to do is create a folder where
we're actually going to install the file
that will have the instructions for our
Dock and compos environment so we're
moving the cursor to the desktop and on
the desktop we're going to create a new
folder and uh using the MK D um command
and we'll call this one dock compose f
for file and then we'll move the cursor
into that new folder by using CD command
and now we are in the folder called do
compose f um and you can go look at your
desktop and you'll see it's there and
now you want want to select um pound
touch
docker-compose.yml and that will
actually create the new yaml file for
you and you can use the ls list command
LS do- composed. yaml and they allow you
actually now to step in and edit that
file so let's go ahead and uh hit return
and that allow us to go in and edit the
file so now I'm in the editor and I'm
going to write some instructions um so
I'm I'm going to put a couple mistakes
in these instructions so we can see how
Dr pose catches those mistakes um so but
most of this is going to be fairly
correct so you can just type this out so
we're going to start with Services colon
then we'll do web colon and then we
create image colon and we'll do Nix and
then we'll do a database colon and we'll
call the image redis and so what you can
see because we have two reference images
we're actually calling two different
images two different containers that
will be created by dockerhub uh so what
we're going to do is uh where we can
actually show you um how that's pulling
in you can go to dockerhub and you can
actually see the references to the
images that we've just created so go to
Docker Hub which is H hub. do.com and
you'll see that uh Nix uh reddis those
are the default names for the images so
you could do uh other images like HTTP
for Apache web server if you wanted to
there a lot of images in Docker Hub so
certainly have a lot of fun building uh
tools from that environment so we're
going to save those files um in the dock
image and we're just going to do colon
WQ to um exit this screen save the text
and let's go and run our uh yaml file
and you'll see the two different error
messages that will come up so we're
going to do cat do- compose yml and here
we have a display of the text that we
just just wrote and now let's go check
the validity of the file that we just
created so we'll do do- compose config
and we should get an error message and
there we are error message and the
reason why we have an error message is
because the spacing is not correct in
the yamamo file itself so let's go to
the actual yamamo file that we created
let's go through and we're going to use
uh just the file finder and you'll
actually open up the yaml file in your
text editor your favorite text editor
and here we have um the code that
created it and let's use the correct um
spacing so that everything is space out
appropriately in the image and so this
is the spacing that you'd expect to see
a space a line between surfaces and the
first image and the first image is a tab
in and then it reference what the image
is and let's go ahead and clear the
screen and we'll see whether or not
we've got that all correct clear um
let's clear the screen and we're going
to check that everything is running
correctly and we're seeing we're still
getting an error message and the reason
why we're getting another error is that
there's still a config error uh in this
the actual yamamo file so one of the
things you have to do when you're
creating a yamamo file for dock compos
is you actually have to have the
appropriate version uh number in there
so what we have to do is we have to go
and find which is the appropriate
version and there are three major
versions 1 2 and three depending on the
age of docker that you have installed on
your computer uh so if you have the very
latest version of docko 18 and newer uh
you would put 3.7 as the version number
and what we're going to do is we're
going to just check on the version that
we have installed and so we just clear
the screen and then we'll just do Docker
version and what we have is our version
of Docker is
17.12 now this is the version of Docker
not Docker compos uh it's your main
Docker environment and so if we go back
to our web browser we'll actually see
that version 17 should align with
version number 3.5 yep there we are
version 3.5 so what we want to do is go
back into our yo file the very first
line that we want to put in above the
services is the version number and that
version number we're going to put in is
3.5 so we type in version version and we
put colon and then version number is in
in quotes and you save that and now what
we can actually do is we can actually go
ahead and use the single line
instruction to run both of these uh uh
two images that we've created because
everything is configured correctly but
just one note you'll see that the
version number is actually at the bottom
of the list and that's just the way the
dock compos works it pulls out the
services and the images that you'd
create and then put the image the
version number at the bottom of the list
so in the presentation we talked about
how you can use one line to trigger two
or more images and so to do that we're
going to do pseudo
docker-compose up- D and it's going to
create the two environments we have the
web environment and the database
environment and let's use pseudo Docker
PS to actually list out that all the
processes that are running right now and
there we are we actually see all of the
images that running we have the names
the commands the images is and
everything's up and running and if we
want to close both of those images we
can actually use a single line of
command as well and that command will be
pseudo Docker Das compose down and that
closes both of the images and we can see
that all the databases and the web
services images have been closed and
pseudo Docker PS which she list that
there is nothing running and that's how
to use Docker compose so if you are
looking to become a docker expert
consider the postgraduate program in
devops by simply learn to gain expertise
in Docker this program offers Blended
learning that combines live online devop
certification classes with interactive
Labs that will give you hands-on
experience you will gain expertise in
the tools like terraform Maven anible
Jenkins doer junit G and more we prefer
one year of Prior experience to enroll
in this course so why wait enroll the
course from the link in the description
box so uh we are going to talk about
what is a Docker what is a Docker file
what is a syntax of Docker file and how
to build a custom Docker image with the
help of Docker file and in the end we
are going to see a particular demo
related to the docker files over here
now let's talk about what is a Docker
now Docker is in configuration
management tool which we are pretty much
using to prepare the automation of the
deployment of the uh software of the
applications now in this case we make
use of the docker containers using which
we can can host our applications and
within this Docker containers we are
typically hosting both the application
source code and the defenses all
together so both the dinks are getting
packaged all together into a single unit
called docka containers and getting
hosted onto the
server these containers are very
efficient in work because they are not
taking that much utilization and
resources and at the same time they can
be shared across different environments
also so the same container can be easily
deployed across different different uh
environments as
such now it helps us to maintain the
isolated application it uh helps in
having a high productivity there and it
also you know helps us to see to resolve
all the problems which is related to the
uh dependencies for your software
because we are packaging up both the
source code and the dependencies all
together in the single unit in the
single entity and that's where you will
be able to get a particular final
solution called as in Docker container
over here so Docker container is in
complete package using which you can
have like both the applications and the
dependencies bundled up together into a
single entity and that's where we call
it as a Docker
container now what exactly is an a
Docker that uh you know let's see a
particular difference between the
virtual machine and the docker here now
if you see on the left hand side we have
the host operating system uh which is
available there let's say I have a
laptop in this one we have a Windows 10
now we have a hypervisor on top of that
like hyper is an software which is used
to manage the virtual machines it's a
software which is available there which
takes up all the requests from the
virtual machines process it and then uh
give the response back so hypervisor
acts like a medium between the virtual
machine and the host operating system
resources so uh it's going to give that
a particular virtual environment to the
virtual machines and then we have a
virtual machines where in fact a guest
operating system is there then
application boundaries then the
application is hosted so this guest oper
in system actually takes up a lot of
utilization and these was the old
mechanism where we used to you uh follow
this mechanism for hosting our
applications so all the applications are
pretty much getting deployed in the same
manner on the form of the virtual
machines here right now if you talk
about the new method which we are
talking about right now is in Docker so
here we have host operating system but
instead of hypervisor we have now the
docker engine which is available there
now this Docker engine is something
which is making making sure working in
the same way that how the hypervisor is
behaving but it's much lighter as
compared to the hypervisor so Docker is
very lightweight as compared to the
hypervisor here and here we don't have
the concept of uh the heavy duty guest
operating system over here so the guest
operating systems which we are using in
case of Docker containers are very small
in size the size of guest operating
systems uh is is in GBS in case of
virtual machine but in case of Docker
containers it's pretty much available in
the m B size which is there so the size
of the guest operating system in case of
the virtual machine is very small as
compared to the virtual machine and
that's the biggest uh benefit because
again it will help the containers to
have a low utilization low consumptions
and uh that improves the overall
performance of the uh overall mechanisms
and how exactly the um New method of
application deployment really helps us
to go for an a particular advanced
mechan mechanism so that brings on a
very good and the efficient uh way of
doing the deployment of the
applications now let's talk about what
exactly is in Docker file here now uh
Docker file is a concept which is being
used as in build script so it's being
used so that we can have a build uh
script there so this will definitely
helps uh to see that how we can go for a
build of a Docker image a custom Docker
image now when we are using using the
docker file uh concept so it really
helps to uh prepare a kind of Docker
image so dock image is the end result
when you are going for the mechanism
when you're going for the docker file
buildup ho so Docker file is a build
script which is available for preparing
the custom Docker containers because not
every Docker image is available on
Docker Hub so some containers you have
to use some uh images you have to use to
work on that part now Docker file is a
very simple straightforward text file
which is label there and it's basically
used to build up custom Docker images so
Docker file is basically used to prepare
a once so when you running the docker
build so it's actually going to build up
a particular custom Docker image and
that will be given to you so Docker uh
image is the whole build process which
is being done so it's a custom process
which is available and being done over
here in this
case so you got a Docker file you run
the docker build command and that's how
you will be able to get a Docker image
so whatever the steps you are having in
the docker file that will be processed
one by one and as a end result you will
be able to get a full-fledged Docker
image over here in this case right now
let's talk about the syntax of the
docker file so Docker file is equal to
commands commands and the arguments so
whatever the things you want to put up
so in the docker file you can do that
and as a part of the execution so you
will be able to get an particular end
result now it's a kind of a mechanism
where we feel that how execution and the
modifications can be done so uh the uh
comments which we are trying to do over
here is that you know we can put up the
whatever the comments we want to put in
Thea case of Docker files we can do that
comments is a standard process in uh
preparing any particular file or any
kind of code because it helps us to
understand that for what reason we are
preparing that particular line then uh
we have the uh particular run statements
the commands are there so you can see
that we are running a particular command
called Eco so using the Run attribute we
are basically going ahead and running a
particular command into our system as
part of our Docker file over here now
how to build up the docker images using
the docker file so um you know Docker
file is uh just a template which is
available there where we are putting up
that what are the different steps we
want to follow and these steps when we
follow one by one step by step so that's
where we will be able to get a
particular final Docker container over
here so Docker image is getting
converted as a Docker container and we
are getting a docker container as an end
result now um Docker file is being built
up and uh stored in the version control
system and uh this particular Docker
files uh when we are processing So It
prepares a respective uh a specific uh
image layer over here and this image
layer is something which is uh
benefiting us that how the executions
can be done and how we can manage on
that part so that really helps us to see
that how the executions and the things
can be managed and that can help us to
see that how the management of the
things can be performed as such so uh
Docker file is a very good concept which
is there using which you can prepare any
kind of custom Docker image and it can
be very easily handled also and it's an
fully automated mechanism you can use it
into inside the cicd pipelines for
automating the docker images right now
it's uh Docker image is a layer by layer
mechanism which is there in which we
have multiple layers each and every
layer we are doing some specific uh
entry some specific task or some you
know changes we are trying to deploy in
part of our these layers right so uh you
can use the uh particular Docker build
command to perform the executions and
you will be able to get an particular
response and output will be given back
to you so Docker file is basically
helping you to see that how the
executions can be done and you will be
able to get an end result over here so
the docker image is the final result
which we are getting when we are talking
about the docker file here right so
these are all read format which is
available you cannot modify these
directly but yes with the help of Docker
file you can do whatever the changes you
want to deploy on these Docker
images right so uh this is a small
example where we are using a base image
as an 1 12 18.04 so we are using the
from attribute to specify that yes we
are going to use the 21 18.04 here then
we are uh pulling a the file into the uh
directory we are transferring a
particular file inside the docker
container and then we are running a make
Command and then CMD attribute is there
using which we can run the command on
the runtime on the docker container so
these are a very basic example but
eventually when we are done with this
Docker image we will be able to get a
four layered uh Docker image over here
because there are four steps and then
hence it will be providing you a
particular four layers in your final
Docker image over here so layer by layer
these changes will be done during the
build process and in fact using the
docker history command you can see the
layers these layers information
also right so layer one is 1 to 18.04
Layer Two is pull and uh the third layer
is there where we can run the uh make
command to you know run this file which
we have pulled on and then the finally
the last one is there where we are
running like using the CMD P so the
difference between run and CMD is that
CMD will run only when you run a Docker
container so run attribute will be
executed during the docker build part so
this is uh pretty much is an way that
how we can go for the execution of a
Docker file and how you know we can go
for the uh buildup of a custom Docker
image over here so these are the some
couple of attributes which we are using
in the docker file here right now let's
talk about the entry point over here now
entry point allows you to specify a
command along with the parameters right
so uh let's say that you want to run a
particular command uh over here in the
docka containers so entry point is
something which you can use to refer to
that so application is the command which
you're trying to run and then you can
give some arguments to that and argument
one is what you're trying to run over
here so entry point is very important
because that eventually helps to see
that how the executions can be done and
the implementations uh will be done over
here in this
case right so you can see like you can
run a command called Eco and then you
can use the parameters also it's not
like only the commands can be used but
you can in fact use the arguments also
now add is an attribute which is
available in the docker file which is
primly used to see to uh transfer the
files from the host machine inside the
docker image now let's see that you are
trying to prepare your own custom Docker
image apart from the dependencies you
need the source code also right so the
source code needs to be transferred
inside the docker image and for that
reason we can use the add attribute so
add attribute requires a source and
destination Source will always be the
path on which uh on the Destin on the
host machine the files are present so we
can give that and destination is the
folder which is available inside the uh
particular Docker container where we
want to store the files so you can use
the example over here just like this you
can use and you can give two entries one
is the source one is the destination and
once that is done so you will be able to
see your files being uh stored up in the
destination directory once the final
Docker image is prepared then we have em
attribute uh now en is something which
is used to set up some environment
variables which is required of course
for the docker image uh for the
application which is running inside the
docker image now environment variables
are something which you can hardcore
during the docker build one but using
the when you are running like when
you're creating the docker container at
that time also you can override the
these environment variables so
environment variables are very important
because uh first during the build
process you can hardcode some values but
if you you really want to change them
during the runtime that also is being
supported by the environment variables
so most of the times we use it as an
kind of a key value pair just like in in
Linux we use uh we use to set up a
particular environment variable so the
key value pair is being used in case of
em attribute maintainer is there where
we can describe that what is the details
about the author these are the fields
which is available in the top so where
we can give the the author name and the
author you know details like email ID
and all so that we know that who is the
person who is maintaining this
particular file and we can reach out to
them in case any issues are there for
the docker file CM right so uh that is
what we have here in this one so now we
are going to see that how we can go for
a small demo on Docker file and we can
go for the implementation of this uh
particular Docker file and prepare a
custom Docker image so let's just wait
on that part so let's see some uh demo
here now in this one we have got two
examples one is the python and one is
the nochas one so first of all what we
need to do is that we need to process
the python one here so this is the
sample Docker file which we have got
here for the python now all this one
requires a python file which is already
present on the server and we are going
to transfer we are going to copy this
content over here that this is the
specific Docker file content which is
available to us and all we need to do is
that we need to put this content across
the server so that we will be able to
build our custom Docker image so let's
go back to the server so here we have
already got the python file which is
present so we have to create a Docker
file here which will have the steps
related to the docker image preparation
over here so we will be including some
uh steps over here that these are the
different steps we want to configure we
want to set up over here in this one so
python 3.6.4 is the version of the
Python which we want to configure and
then we are going to transfer some files
into slash app we are also setting up
the working directory as SL app and
ultimately we are running a command
called pip install flask to do the
installation of flask package and and
then we are starting the uh specific
applications on that part so I'm going
to save this file and uh once I save
this file so all we need to do is that
we need to create a custom Docker image
right so right now there is no image
which is available over here and it's in
complete MD instance so I'm going to run
a command called Docker build hyph T
python hyphen m dot so Docker build is
the typical command but hyphen T is for
the tag that what kind of Docker image
you want to build and then we giving a
image name and then we are specifying
dot over there now once we are done with
that so it will pick up all the things
one by one the basic Docker image is
being pulled down then some sequence of
steps is being executed so it's a
step-by-step process which is happening
over here in this case and we are then
transferring some files in the SL app
and then ultimately we are going for the
PIP install flask it's installing those
packages and at the end the final Docker
image will be prepared and will be given
back to us so we will be able to get
this uh Docker images over here when we
run that command we will be able to find
out that we got our custom Docker image
also available and as you can see that
the base doer image is also available in
this one but yes we have added up like
around 10 uh MB of work we have added on
top of this python uh base Docker image
here so that's how we are able to do the
processing over here in this case in
this setup so that's what we uh can do
to prepare a custom environment of
python with the help of the docker file
here now let's see that uh we will be uh
proceeding on with the nodejs to see
that if the nodejs uh kind of setup also
can be done now for nodejs again of
course we got uh the uh specific Docker
file which is present so we can process
that but let's create a new folder over
there so I'm going to create for a new
nodejs folder so that we can create the
work all together in this one so I'm
going to run a command called npm Unit
which is the command to initialize any
empty project in case of node now you
can typically bring on some uh real code
also or you can say that I want to
process an empty uh project so you can
give the configurations you can give the
custom names but uh I want to process
with these uh default name itself so
ultimately it's going to uh create an
entries like uh you will be able to see
like all these uh values which we are
not providing these standards value will
be picked up for the uh specific code
and at the end you will be able to get a
package.json file created over here in
this one now you can actually bring down
the steps that how you can perform or
you can build up a custom nodejs based
Docker image now these Docker files is
giving you the capability that you can
convert a full-fledged application which
is Deployable on the virtual machine you
can actually convert them in the format
of the docker containers over here in
this one so that's what we are are going
to get over here with this uh particular
setup or these mechanisms so let's go
back to the document and see that what
exactly we can copy the docker file so
we have got this Docker file over here
so all we can do is that we can copy
this one so once we copy this uh
specific file here so all we can do is
that we can transfer uh these files into
the onto the server we can convert it
into the ad doer file there so that we
will be able to set up the
configurations and we can perform the
setup that so Docker file is again we
want to create and we want to set up on
that here so let's go back to the server
so here we want to create a Docker
file and uh we want to set up the files
inside that these are the different
files which is being available which is
being configured over here in this one
so we are running like node server.js
which is the master file so we are doing
an expose of 880 then of course we have
uh copy commands which is there then npm
install command is there and then
package of Json file is also there now
what you need to really do is that you
need to go for a server.js file which is
available in which you can configure
that how the setup needs to be done and
how the configurations should be
performed there because that's a command
that's a script which is going to be
executed within the contain so let's
copy the content now I'm going to copy
this content over here which is like a
sample uh nodejs code which is available
so I'm just copy this one and transfer
it to the server so
server.js file we have to create here
here just like we did in case of the
Python because this is the source we
want to transfer inside the docker
container so this is what we have got
now I'm going to uh transfer like uh I
want to change this port value from
5,000 to 8080 because that's what it's
configured in the docker file of course
you can configure 5,000 also depending
on your requirement you can do that
configurations now you have got uh these
two files over here in this one so all
you need to do is that you need to run
like Docker
build hyphen T no nodejs hyphen app and
Dot so it will automatically pick up
like what exactly it will uh you know go
for the nodejs basic Docker image over
here so that will be processed over here
in this one so step by step it will be
able to do the
configurations so first of all it will
download the basic Docker image which is
um quite uh more in size here in this
one once it's downloaded onto your
system then it will proceed further with
the build activity on processing the
docker file performing the execution so
all that stuff will be performed with
this help of this Docker build process
over here so it's going to process it
one by one in this
one and it's going to the next step
where it's transferring some of the
files like creating some working
directory transferring the files uh
putting up there and running the npm
install command and uh then ultimately
it will expose the port and then it will
have the uh CMD attribute configured
over there so this is basically
typically trying to portray that how a
particular Docker image can be built up
and of course once the uh build is
Success you can validate it with the
help of Docker images here so this is
the app Docker image which is been
prepared of 9911 MP and uh we have got
it like a final package where the
runtime is also there and the code base
is also available now you can pretty
much experience it like with other
actual Source sces like you can generate
the download some preconfigured nodejs
websites or the uh Source codes here and
you can try to run them on 3D Docker
container so that you can get the full
uh the complete idea about that how it
goes on but right now the uh these are
the basic Docker image which you can
prepare and you can build up over here
in this one so that's how it really
works on for you now let's see that what
are the different commands we have for
the docker here now the very first one
is that what are the different images
related to commands which is available
there the very first one is the command
which we typically use to prepare a
custom Docker image now I can uh easily
go to the docker Hub and I can pull down
a Docker image which is available there
but let's assume that you are working on
application source code so uh you want
to prepare a Docker container for that
now that Docker container you will not
be able to have it on the docker Hub you
have to prepare on your
own right so in order to prepare your
own Docker image all you have to do is
that you have to run Docker build
command now in this case we have to prep
prep a Docker file which will act like a
Docker a kind of a Docker build script
just like in case of MAV we have pound
XML file and in case of and we have
build.xml file so here we have the
docker file which is available there and
this Docker file makes sure that
whatever the steps are mentioned in this
Docker file in the build script it's
going to perform a series of steps and
the build will be done so Docker build
is the command hyphen T is the tag that
what exactly name uh you know whatever
the image you are building so what
exactly tag name you're trying to
provide over here so here we are using a
tag name called my image and then after
the column we are providing that what
version we want to go here 1.0 2.0 5.0
so whatever the version you feel that
you want to um you know refer you can do
that now this is the exact way that how
in the actual world the docker images
are getting built up by the uh
respective vendar let's say that Jenkins
is want Jenkins wants to prepare a
Docker image so they will be preparing
the docker file in the same manner and
prepare these Docker images and publish
it to the docker Hub so all the docker
images are built up the custom Docker
images are built up with the help of
Docker file so Docker build is the
command which will process the docker
file and give you a particular Docker
image in the
end now how to list down all the docker
images so Docker list uh LS is something
which is there so this will eventually
help you to list down all the docker
images which is listed on or which is
available into your system so that will
be uh listed out over there for you if
you feel that you want to remove a
particular image in that case you can go
for Docker image RM command and uh
whatever the image you want to remove
you can give the respective image name
and the tag name there so Docker image
arum command is there using which you
can actually see that how the image can
be deleted from the host over here in
this one right so whatever the ways
whatever the you know mechanism is there
so we are making sure that how the image
can be built up and how we can go for
the uh builtup and a particular image
can be deployed over here in this case
so in order to delete an Docker image so
you can go for Docker image armm command
to remove a Docker image completely from
the docker host so these are some couple
of commands which is available there
using which you can actually interact or
manage the docker image because in order
to prepare or in order to run a Docker
container we require ire a source and
that Source we are going to get it from
the docker image so now we are going to
see that how we can execute these
commands so first of all what we need to
do is that we need to see that how
Docker build works for that we require a
Docker file so I'm going to have a
simple Docker file over here in which we
are going to have a basic Docker image
selected as an open two after that we
are going to go for the Run attribute
where we are going to do the
installation of a specific package like
let's say G and accordingly you can
whatever software you want to prepare
you want to set up you can do that and
put up the commands over here in this
Docker file so I'm going to save this
file and after that what I need to do is
that I need to run like Docker images to
find out that how many images are there
there is no images present so I'm going
to build a Docker image over here with
the name of Simply Lear or whatever the
uh name you want to give you can uh give
it over here in this case it's not any
kind of restricted word so you can give
the actual Docker image name also that
what for what purpose you are preparing
this image and with that what will
happen happen that it will take up the
values whatever attributes you have put
up into the docker file so that will be
processed and that will be picked up and
once that uh Docker file has been
processed has been uh you know prepared
you will be able to see that yes uh a
final product or a final Docker image is
finalized and prepared over here in this
one so we just have to wait for the
whole process to complete and once my
Docker image is prepared I will be able
to have a particular setup done or
achieved over here so that's what we
require over here in this one to be
performed now once the installation is
done so you will be able to see like the
final Docker image is being prepared
over here so it says successfully built
which means that the docker image is
prepared now if you run like Docker
images you will be able to find out that
this image is available over here in
this one now you are seeing like two
images because one is like the basic
image which is being pulled on because
we are using a Docker file to do that to
process that one
there now in case you want to remove it
so all you need to do is that you need
to run like
Docker image RM and simply learn so if
you do that what will happen that your
Docker image which is related to Simply
learn will be wiped off from your system
and in case you want to do like removal
of the open to also so that also you can
do over here with the help of Docker
image
RM open to and that will be removed from
your system so your Docker images will
be removed from your system using the uh
particular command called Docker um
image and then RM command to remove the
docker image from your system from your
host here so that's how you deal with
the different Docker images uh commands
over here in this one now let's go back
to the document now the next section
over here is the docker container
commands over here so all the commands
related to Docker containers is what we
are going to talk about over here now
the very first one is the uh particular
container so in order to create a
container so you can run the command
called Docker create and the image
because for a container you require a
Docker image that's a prequest so once
the docker image is prepared you can
just simply run Docker create command
and that will help you to create a
container onto your system now if you
want to run a particular um you want to
create a container and you want to uh
run it also at the same time so in that
situation you can go for Docker run
command the main difference between the
docker create in Docker run command is
that the docker run command is going to
uh run a container and uh it's going to
give you a response from the command but
the docker create command is going to
create only the container so once the
container is up and running so that's
only being done by the docker create
command but the docker run command is
going to actually do the uh preparation
of the container also and at the same
time you are being given as an
particular response also like you know
what a command you want to run so that
response also you will be able to get
here and if you feel that you want to
rename an existing container so for that
you can go for the docker rename command
so existing container you can rename it
with the new container and that's how
the uh particular rename of an existing
container will be done with onto the
docker host here so Docker rename is the
command using which you can rename an
existing container and you can get a new
container over here in this case so
these are some of the options some of
the ways that how you can interact with
the containers how you can manage them
onto your Docker host over here so you
can run it you can create it and you can
rename the containers also so now what
we're going to do is that we going to
see that how we can uh work on these
particular container commands here so
first of all I'm going to quickly pull
down a Jenkins image so that I can
create a container out of that so it's
in kind of 600 around M Docker image
which is available there now you can
also have a custom Docker image prepared
using the docker file or you you can
have a rade Docker image pulled down
from the docker Hub also so this is
going to pull the latest version of the
chenkin from the docker Hub so that we
will be able to proceed further on that
and we will be able to continue with the
hosting so we will be converting or
running this uh specific Docker images
in the form of Docker container so that
we will be able to have a instance of
Docker container over here running in
this one so that's what we are trying to
do here so we will just wait for this
pool activity to complete because it's
an heavy Docker image so it may take
some time to download and once it's
download you will be able to see that
Docker images is showing a specific
Docker image of chenkin here now you're
going to create a container out of that
so you're going to run a command called
Docker create and Jenkins over here so
that will help us to do uh the creation
of a Docker container and once that is
done so you will be able to see like
using Docker PS command that if any
container is up and running and if you
go for Docker PS hyphen a you will be
able to see that yes a particular Docker
container is in created mode now this is
not in running mode because you see that
when I ran the docker PS command it was
not showing it in the running mode
because I just created a Docker
container here and the created setus
also shows that yes the docker container
is created but it did not ran over here
so if uh you want to run it so
definitely you can say like Docker start
and and
uh you can give the container name and
uh that will help me to uh start my
container over here but of course you if
you feel that you want to directly
create a container and uh you know you
don't want to create the container only
you want to start it also so in that
case you can go for the docker run
command so Docker run command is
something which can be used instead of
uh doing the execution so you can say
like docker
run Jenkins and what it will do it will
definitely help you to get the uh
container created onto of the Jenkins
over here in this case it's not like
only creating a container but in fact
it's actually starting the container
also so you can see that it started over
here into your screen itself that whole
Jenkins gets initialized over here in
this one so this is the way that how you
can initialize or you can create a
container now I can just do a control C
over here because says that and Jenkins
is up and running so you can start using
this one according to your requirement
over here right so I'm just doing
control C so that it can be stopped over
there right now so this is the way that
how uh typically we can create the
containers now we got like two
containers over here and uh one is up
and running and one is something which I
just closed just now over here now if
you feel that you want to rename a
container let's say like this is a
container which is available but it's
not having a meaningful name now I want
to convert into a meaningful name like
Devore chins so that you can easily do
with the help of Docker rename command
because that will rename the docker
container name so that you will be able
to connect onto that and you can go for
the processing so that's what we can get
over here in this one so this is how you
can do the various management of the
resources over here so let's go back to
the main content now if you feel that
you want to remove all the stopped
containers because what happens that
sometimes we simply stop the container
but we do not remove it from the uh
Docker host because um we are not going
for the cleanup now clean cleanup is
very required because these containers
are actually taking up the dis space
also so we need to make sure that we
should be using we should be having like
minimum amount of dis space utilization
so for that what we need really need to
do is that we need to run Docker
container prune command
so this will basically help you to uh
prune or do the cleanup uh Purge of all
the containers which is running on the
host machine here and that will
definitely help us to remove all the
containers which is in stop status so
that will be simply removed over here in
this one so uh this really completely
removes all the stopped containers onto
the docker host over here in this
scenario now second one is that it
creates a new image from a container's
file changes now the way what happen
happens that normally what uh we follow
is that we use the docker build command
to do the build process right so Docker
build command is being used in that
particular way now if you really want to
go for uh preparation of a new you know
image so all you need to do is that you
need to create a new image from a
container file so uh you can run the
command called Docker container commit
command and uh you know whatever
container is there so that will be saved
or preserved as a Docker image because
sometimes we also do some ad hoc changes
or modifications on the Fly inside the
docker container by going inside the
docker container and doing the steps so
we really need to preserve those steps
and that is the reason why we are
converting a Docker container in the
form of a Docker image here so that's
what we are trying to perform here so
Docker container is something which is
uh being uh done over here in such a way
that whatever the steps whatever the
changes we are trying to do so that
needs to be performed over here in this
case so Docker container uh commit
command is being used so that we can get
a particular you know final image as
part of the container so the container
will be converted in the form of Docker
image over here and the last one is that
how we can remove an existing container
so we can go for the docker armm command
which is available there using which you
will be able to go ahead and you can get
a particular container removed from your
existing system so Docker RM command is
pretty much there using which we can see
that how we can remove an existing
container from your system system as
such what exactly now we are going to
see that how we can perform the various
commands on the containers
here so the very first command which we
want to do here is that uh you know in
order to see that how we can actually
remove all these top containers but for
this one what we really need to do is
that we need to first of all run some
couple of containers so I'm going to
quickly run a container called
Docker run hyphen D
httpd so I'm going to run a particular
web server over here and uh it should be
able to give me a proper container up
and running so I can just double check
or valid that it as with the help of the
docker PS command so Docker PS command
should give me the proper confirmation
that whether the container is in up and
running over here or not so you can see
that it's up and running from last 11
seconds here now we got a command where
you know for example we have got some
stoed containers but again you have to
manually remove them one by one now if
you don't want to do that stuff if you
feel that whatever the stock containers
which is available that should be
automatically removed from your system
then we have an command called Docker
container PR now for this what we really
need to do is that we need to actually
stop this container because again it
will be only picked up like whatever the
containers which is available in these
stop Setters only those will be picked
up so I have copied the uh container
name here in this case and uh I need to
stop it so that I will be able to uh
remove it with the pr command over here
in this one and then I can run like
Docker container pront command which
will basically remove all the containers
which is in the stop status and once
that is done you will be able to see
like that is something which is
completely gone from your system so this
is a way that how it can not only take
care of uh these top containers but it's
actually being done so that you can
remove the multiple containers with a
single command itself over here so this
is not something which will be uh
executed one by one on a different
container but yes it will definitely
take care of doing the uh changes and
Performing the
activities now another thing is that how
we can actually do the uh commit of a
particular Docker image let's say that
we got a Docker container which is uh
running and there are some changes like
uh some Dynamic changes which is
available there so you want to save this
uh uh particular container as a Docker
image so that whatever the changes
temporary changes are there so that will
be also saved as an Docker image so all
you can do is that let's say let again
run a particular
container of htpd here so this will help
me to get a particular container from
the HTTP perspective available of to us
so Docker PS command will give me the
option now here I have got a container
in which it's running now if I want to
save this container let's say that I
went inside it did some modification did
some temporary changes I want to refer
it I want to use it so how you can do
that so for that what you really need to
do
is you need to run like Docker commit
that's a command uh or you can say
Docker because you're trying to do a
Docker container commit over here so the
commit of the container is what we are
going to do so Docker container commit
and then we going to see like what is
the container so for container we will
be yes again copying uh that uh
particular container ID and we'll say
like
Custom Image colum version one so we
will be giving a custom name or a name
of a Docker image and a tag over here in
this one so this will basically save my
Docker container which is available in
this one in the format of a particular
Docker image in a format of a Docker
image over here in this case so as you
can see that almost both of them are
having the same change but the container
which which we got uh over here in this
one so this whatever the changes uh you
want to wish you want to do you can do
that and you can then put it up into the
uh form of a Docker image a final Docker
image and at last if you want to remove
it so Docker armm and container ID now
this again Docker Aram will come remove
only those containers which is in stop
status so if you want to forcefully uh
remove it so you will be able to use it
with the help of hyphen f which is the
forceful remove which will remove the
container completely from your system
and once it's removed from your system
you will be able to see like there is no
container running up and running over
here in this one so Docker PS hyphen a
is not something which is up and running
over here in this one so they completely
completely got uh wiped out from your
system so that's how you basically work
on these commands on this uh particular
uh container related commands here so
let's go back to the main
content all right so these are the
commands where we can actually do the
management like how to stop the
containers how to remove the containers
and how to get a new you know darker
image out of the container here now
let's see that how we can uh list down
all the running containers if you feel
that you want to list down all the
running containers then you can go for
the docker containers LS command and
that will eventually help you to list
down all the containers which is
available there which is uh being there
into your system then Docker stop
command is there to stop a
container so whatever container which is
there in the running status so that will
be stared with the help of the docker
stop command and then the restart
command is there using which you can do
a restart of a container into your
system right so once the container is
restart so whatever the application
running inside the container so that
will also be restarted over here in this
one so these are the different options
which is available using which we can
explore over here in this one some what
uh more commands about the container so
again I'm going to run a container over
here of H htpd here so that I can show
you some couple of more commands over
here now we can run like Docker PS or
you can say like Docker container LS
will be list down all all the containers
which is available to you which is being
uh you know present in this case now if
you want to do any kind of operations
like let's say if I want to stop it now
the moment you do the stop over here so
you will be able to uh stop the
container which is up and running for
you and uh you can come uh you know
whatever the application is being
deployed that will also not be available
to you because that's simply uh
completely gone for you now you can see
that Docker container list is not able
to show you that particular option that
whether it's uh being shown or not
because it's in stop status so in fact
you can do a you know start or you can
do a restart so both the operations you
can pretty much do over here so that you
will be able to have a complete activity
implemented so restart is also something
which is available which can definitely
help you to see that how the things
really work and how you can go for the
container U you know uh the management
over here in this case so you can do
container do container list command to
list down what are the containers which
is available then you can do a restart
you can do a stop also in fact start is
also there which can help you to see
that how the containers is are really
available so this is the way that how
you can manage like uh you know even the
restarts and all because this really
helps you to even do the restart or
bounces to the application which is
present inside the container so that
also you will be able to see with this
particular component here so that let's
go back to the main contain and then um
you know killing a running container so
if you really want to kill a container
so for that we can go for the docker
kill because Docker stop is also going
to stop the container but if you want to
forcefully kill it so in that case you
can go for the docker kill command to do
that particular kill on the containers
and then if you really want to go inside
the container and you want to attach
your terminal so so for that you can go
for the docker attach command and with
this what will happen that you can go
inside the container you can do whatever
the changes you want to perform you can
do that and you can again come back
outside that also so normally typically
we use it so that we can see some errors
of uh in the logs which is coming up in
the containers and if any kind of issues
are coming up for the applications we
should be able to know that part now the
last one is the uh particular weight
Docker weight is there so that will
block a container and uh it will go in
the weight status so Docker weight and
the container ID or the container name
you can give so with that the particular
container will go into the block status
let's see somewhat more uh commands over
here so we can run like Docker container
LS again to find out like what container
is available so you can see that this
particular container is available over
here in this one now if you see like if
I want to get uh attached to this
container if I want to attach my input
and output uh stream with this container
so you can run like Docker attach and
the container ID
so your terminal will be stuck over here
because it's taking up whatever the
progress which is going to be done in
the container that will be in you know a
kind of attached to your uh terminal
over here and you will be getting the
output but the moment you come out of
this like you say control C so you will
see that you know it's basically sending
the request to the container that it's
getting stopped so you will be able to
see like the doer container list it's
not going to show you because the
container is in stop status so again you
have to do the start of the container to
make sure that the container is up and
running all right so the attach one will
help you to go uh and uh attach your
standard input and output with your
container with the running container but
when the moment you comes out you will
be able to uh disconnect or shut down
the container and again I had to restart
that container over here in this one
then again of course we have like Docker
wait then container is there so this
will basically put the U container into
a block status because all the request
and all will be not sent to the uh
container and uh it will be blocked and
uh if you really want to like uh consume
or if you want to make use of the
container you will not be able to do
that so Docker weight command will kind
of block the container and you know if
you again uh try to come out of that so
again the requests uh to The Container
will be uh pres resumed and they will be
able to use on that part so Docker
weight command is being used to block
the container and uh then we have Docker
attached to attach to the container the
standard input and output and lastly is
the docker kill which is going to kill
the container and you will be able to uh
see that the container is in simply
killed status so it's uh you know it's
not like a stopped over here if it's
gracefully stopped then the exit code
definitely will be zero but in this case
since it's being killed the particular
status is 137 over here in this one so
that's a nonzero status code so that's
how where you will be able to manage the
containers on different aspects let's go
back to the main container so these were
some of the commands which was there
from the docker uh containers
perspective let's talk about some of the
commands from the share command
perspective so how we can share the
docker images so we have a typical pull
push mechanism which is there using
which we can actually interact with the
docker Hub or a docker regist
so uh if you really want to pull a
Docker image so in that case you can go
for the docker pull command if you want
to push an image to the registry so you
can use for the docker push command if
you want to execute a command inside a
particular running container or you want
to uh execute a command or a scate or
something like that so you can go for
the docker is a command and with that
the execution will be done and you will
be able to get a response so that will
make sure that yes whatever the
execution you're trying to do so that
will be done over here in this this
mechanism now let's see that how we can
do a pull and push activity to a Docker
image so first of all the pull command
is pretty simple all you need to do is
that you need to say that okay I want to
pull a Docker image uh if let's say that
a Docker image is already present then
it's just going to say that it's already
up to date it's already available there
but if uh in some cases you feel that
the docker image is not available and
you want to pull it so all you can do is
that you can give that Docker image name
and uh that you can give the the tag
also but in this case we are using the
tag as an
latest so that's the reason why even if
you provide or you don't provide it
doesn't really matter so I'm going to
give like a Docker pull myol over here
and the moment I hit on this one what
will happen that it says this a
particular Docker image is not available
into my system it's going to download it
and once it's going to be downloaded you
will be able to see that into your local
system so Docker images is something
which is going to help you with the
commands and it will give you the uh
particular ways that how the docker
image is available so I can run like
Docker images over here to see that what
are the different Docker images which is
available in this manner now if you
really want to move any of the docker
image then of course you have to do a
Docker login and uh you will be doing
the uh you know the connectivity to the
docker Hub and then establish the uh
connectivity and all so what you need to
do is that you can run the docker login
command we will also see this command in
the next uh particular slide but right
now we are just doing it because we want
to showcase that how the push really
works so uh you can actually give
your username that uh what username you
are holding up so the password does not
shows any activity but yes still it will
let you know that whether the login is
successful or not so in this case it
says login is successful so all you need
to do is that let's say like I got this
Custom Image and I want to push it to
the docker Hub so you need to rename or
you need to do a tag over here because
the same if you
run Docker push
Custom Image colum version one so if you
do that it will not be post into this uh
particular Docker Hub so it's going to
throw you error over here that the
access is denied so all you need to do
is that you need to do a tag
here so you have to rename it in such a
way that you should be able to uh use uh
add uh your username over there because
dockerhub will identify that if it's
there with your username then only it
will be pushed otherwise it will be
simply rejected over there so that's
what you need to do you need to rename
your Docker image uh the same Docker
image which you have you have to append
your username of the docker Hub and once
that is done so all you need to do is
that you need to just run like
Docker then push is
there then this image name and then
version one so the moment you you do
this what will happen that it will start
transferring those Docker image to the
docker Hub you see this uh specifies
that yes this is going to the docker Hub
docker.io where my username is there and
this is the Custom Image which is
available there so if I refresh my page
on the dockerhub I will be able to see
like this public repo which is available
there but again this one does requires
the credentials and the login which we
did in just uh last command where we ran
the docker login command to do the login
to the portal and after that only we are
able to do the setup and the
modifications over here so that's how
you do the login and uh you are able to
connect on that now these are some of
the pull and push operations but let's
say that if you got a container which is
up and running like this is uh right now
we are not getting any container which
is available over here so you can say
like Docker
run hyph D htpd like I'm just running a
dummy container over here so that I will
be able to show you this this command
now if you for some reason want to go
inside the container and want to see
some processing and want to see some
content so you can run the command
called Docker isik hyphen ID and there
you can give the container ID this is
nothing but the full format of the
container ID which is given to you so
you can use this one also or you can use
the shorten also shorten value also you
can use and in the last we can say like
bash the moment you do this you will be
going you will be going inside the
container you see that this is the host
name which is being shown known as the
container ID you can do a like psph EF
command and it will uh throw an error
that the PS command is not available
because you are running this command
into the docker into the docker
container so using these kind of
commands which is not available in
container will help you to identify that
yes you are not on the host machine you
are in fact into the docker container so
that's the reason why you can do it
because for you the parts and all
everything is same the terminal is also
same so you may not be able to
differentiate in that part so that's the
reason why it's preferred that we use
that uh commands which is going to fail
but that's okay it will give you that
okay this command is failing because I'm
into the docker container right now so
that's what we are performing over
here right and you can come out of that
but still the got Docker container is
going to be still up and running over
here in this case so the container is
not going to be down over here in this
one so that's how we manage the
container we can go inside and we can
check some informations and get some
files there
so these are some of the commands where
we can inact now in order to log to the
docker Hub because before doing the
docker post you have to log in you have
to get a credentials and the
connectivity with Docker Hub so you can
run the docker login command where you
can see that how the execution can be
done now using the docker login command
you can interact uh with the uh
particular Docker Hub you can set up the
credentials once the uh credentials is
being set up so what you can do is that
you can go ahead and say that okay the
uh configurations is done and and uh we
can go ahead and we can perform the
execution so Docker login is a command
where you can give the credentials
Docker info is the command using which
you can get the uh information about the
docker tool so whatever the docker info
is available there so those things you
can actually get it over here in the
case of the docker info so Docker info
is going to give you complete
information that what exactly you have
the information available over here on
this one and then we have the docker
history so using the docker history you
can take history of of the docker image
that what exactly uh the mechanisms or
what are the different image IDs are
available there so uh this is basically
giving you the complete history on how
the uh Docker image is prepared all the
commands is available there and that
will help you to eventually go and
understand that how the images can be
going on and you can manage on that part
so uh these are some of the couple of
options which is available there where
we can see that how the executions can
be done and we can go ahead on on that
particular part so Docker history is the
command again which will help you to see
that how you can go for the uh
particular mechanism and you can list on
on the things over there so Docker login
was already done previously when we
tried to push the image but again if you
try to run Docker login you will be able
to do that now Docker info is something
which will give some additional
information about the docker and uh all
the informations about the docker will
be given back to you over here in this
one so the complete Clarity uh then
let's say that you got got a Docker
image called uh HPD now you want to
check the history of this image that how
this image is prepared what are the
different layers which is available how
many layers are there how much size is
being modified in each and every layer
so you can see that we got a layer where
we have a particular changes of 7.8 38
MB but pretty much the uh from the
complete size of this httpd The major
portion is actually of the uh this one
the basic operating system which is
being implemented and then of course
there are some packages which got
deployed due to which this uh 60 MB and
7 MB size is coming up there so this
actually gives you the complete
layerwise mechanism that what are the
informations which is available that how
the modifications actually got modified
and got changed over here in this one so
this is the additional uh help which you
can get from the docker to manage the
containers here so these are the ways
where you can interact with the docker
Hub if you really want to create a
volumes for the container so you can go
for the docker volume create command so
Docker volume create command is there
using which you can create the docker
volumes and uh you can pretty much do
that uh setup so Docker volume create is
the command which is available there for
that particular part it will create a
volume for you now let's see that how we
can actually work with the volume so you
can run the command called Docker volume
LS to find out that what are the
different volume switches available
there now if you really want to create a
volume over here in this one so you can
say like Docker volume create and uh you
can go for the uh volume name here now
the moment you do that what will happen
that a volume will be created and you
can validate it with the help of Docker
volume LS command that whether the
volume is being created or not over here
so this is the way that how you can
manage the volumes onto the docker host
now let's talk about some of the docker
swam commands here now Docker swam
commands is there to set up a particular
mechanism where you can see that how you
can perform a setup you can uh actually
go here and see that how the setup and
modifications can be done so Docker swam
is there where we can see that how the
execution needs to be done so uh we can
go for the command called Docker swam
init command this will actually go for
an initialization of a Docker swam
cluster and once the docker swam cluster
is initialized then you can pretty much
go there and perform whatever the
executions you want to perform you can
do that and you can execute as such over
there so Docker swam in it is the
command using which you can actually see
that how the execution name it's be done
and how we can perform the uh you know
mechanism and the changes over here so
Docker swam in it uh hyph advice uh ADR
address is there which helps you to see
that how the executions can be done now
joints you have the docker swam join
command which is available there now if
you go for Docker swam join command so
uh you can have a particular host so
doer swam join command is there which
you can run on the Node machine and in
that case what will happen that node
will be a part of the docker swam
cluster and then uh if you want to leave
uh from the swam cluster then you can go
for doer swam leave command and it will
be simply removed from the docker swam
cluster there some of the couple of
commands which using which we can
initialize or we can uh remove from the
docker swam cluster here so Docker swam
is already being initialized or it's
being created when the moment you do the
installation of talker so talker swam in
it is the command using which you can
inal inaly a Docker container now once
you inaly over here so init after the
initialization you got a command you
which you want to add it like you want
to run it onto the docker famam uh
cluster it will act like a worker node
so that you will be able to deploy the
containers on top of that also if you
want to join it as a manager so you can
also get the join token again so it will
give you the proper command now if you
run like Docker node LS so you will be
able to see like what are the different
nodes which which is available there and
for any reason if you're not comfortable
with that and you want to reinitialize
your Docker swam cluster so all you can
do is that you can say like Docker swam
leave hyphen f and you will be left out
of the docker swam and now if you want
to run like Docker node LS you will not
be able to get that output because it's
saying that there is no uh particular
swam manager which is available so you
can run Docker spam in it or you can run
the join command to connect to that
specific node but we have already left
it and nothing is available over here so
that's a way that how you can manage
like what are the different Docker swam
commands we can typically run to manage
the docker spam cluster now let's talk
about the docker compost commands now
Docker compost commands is actually
there using which we can create multiple
containers it's not only one container
which can we can create we can create go
for multiple creations of the containers
with the help of Docker compost file so
Docker compost hyphen version is the U
command which we can give you the
version of Docker compose which is being
used so here we use a particular wml
format file which will help us to create
the multiple containers we Define the
services that what are different
containers we want to create here and
then with the help of Docker hyphen
compos WL file we can create or we can
have that particular Docker compost file
set uped over here in this case so
Docker hyen compost. yml file is used to
do the setup and the modification so
that you will be able to have the
modifications done and uh you will be
able to find out a particular uh
multiple containers gets created all
together so Docker compose is the is the
way that how you can do the setup how
you are going to run the docker compose
file so in that case you're going to run
the docker hyphen compose up command
which will help you to set up like how
the compose can be done and uh once you
are done with the docker hyphen compose
up command so using this the compose
will be done and the containers will be
up and running so these are the commands
using which we can manage multiple
containers using the docker compose one
so let's try to run some commands
related to Docker compose so Docker
iPhone compose iPhone version will let
you know that what exactly version of
Docker compose is installed into your
system so you can run typical apt
command to do the installation so Docker
hyphen compos the package name which you
can install so that the executables and
the package can be installed into your
system onto the server now all we need
to do is that we need to create a Docker
compos file so what we need to do is
that we need to create a Docker hyphen
compos yml file is the default name of
the yml file which we want want to
create here that's a standard name so
once you create this all you need to do
is like you need to put some uh values
attributes the very first one is the
version value that what version of uh
this uh particular uh attribute you want
to create here let's say that I want to
create a version two so I will configure
here that yes the version two is what I
require I want to set up over here in
this one now again if I want to
configure that what are the different
Services we want to run because Services
is the main thing which we will be
running on this machine on This Server
here so that's what we require so then
we need to put for a particular service
called Web so web service we want to
search and we want to run and then again
we will need to take care about the
spacing because that's very important
part and uh we need to provide the
values of the image and uh all these
configurations here so that you should
be able to set up like how the uh images
and how the docker images can be built
up over here so uh web is the service
which we want to deploy and then of
course the image is the attribute which
is there for the web that yes engine is
the one which we are trying to build up
now you can give the ports configuration
also so you can say like
ports and inside that you can say like
what port mapping you want to perform so
you can say like um 80 80 or sorry 80 is
what you can attach for the engine X
because it runs on 0 Port but now I have
to run like 0 Port I want to attach that
Port over here in this one so I can
specify that at Port should be available
in this one so I'm going to save this
one here and I'm going to run a command
called docker iph compose up now this
will run the containers in the
interactive mode but if you feel that I
want to run in the detach mode so you
can say like hyph d also which will of
course run these ones into the detach
mode and it will be executed so it will
process your Docker compos file and
since you have used a standard name
called hyphen Docker hyphen compose ml
so it will automatically pick that
Docker uh details and start creating the
container as angular is a front end
framework we'll be using a web server
called enginex to to serve the
application HTML files after this
session you will understand three things
first what is Docker and what is it used
for you will understand the basics of
Docker and what problem Docker
fixes next you'll learn how to write the
docker file which you can think of as
the recipe Docker uses to build your
image in this session we'll be using a
multi-stage Docker file to build our
application third you will learn how to
use an engine X server to serve an
angular application inside a Docker
container to follow through the examples
in this session you should have an
angular application ready and the latest
version of Docker installed on your
workstation you can install Docker by
following instructions on
doer. first let's look at what Docker is
Docker is the most popular
containerization technology and it has
quickly become the def facto standard
when talking about
containers containerizing an application
means packaging it and all its
dependencies into a single easily
transportable and isolated container
this container will run in exactly the
same fashion regardless of the computer
it is run on by providing this layer of
consistency Docker fixes the traditional
but it works on my machine
problem instead of Distributing just our
application we're Distributing a full
runtime environment along with
application while not exactly correct it
might help you to think of a Docker
container as a lightweight virtual
machine inside your computer your
computer can run multiple Docker
containers at the same time stopping and
starting them individually as
required a common problem in software
delivery is dependency
management when one application is run
on multiple development machines and
multiple server environments a small
difference in the version of an external
Library can change the functionality of
your application making it behave
differently on different
environments the beauty of Docker is
that if you build your application into
a container image and transfer the same
container image to your colleague's
computer you can be sure that the
application will function identically on
both
computers this is because the container
includes all dependencies for the
application inside it on the other hand
and a Docker container should not have
any dependencies to the host it's
running on apart from Docker
itself it's important for you to
understand what images and containers
are and what's the difference between
them for the purposes of this session
you can think of a Docker image as
something that holds a file system and
some metadata in it the metadata
includes things like the name or tag of
the image and instructions for Docker
like like what command to run by default
when the image is
started while the official Docker
documentation is a bit vague about the
meaning of names and tags you should
learn the basics of it every Docker
image has at least one tag and in fact
the same Docker image can have multiple
tags pointing to
it every image tag has a name part and a
version part separated by a column for
example the enginex image has multiple
tags representing different versions and
different flavors of the
image there's a special version tag
called latest which points the latest
version of the image all Docker commands
default to using the latest version tag
if no other version tag is explicitly
defined when you build an image on your
local machine or for example a build
server you can upload the image into a
Docker
registry when you configure other
machines they use the same docker
registry you can download the same image
to as many other machines as you like
there is an official Docker registry
called the docker Hub and the model is
the same as for example GitHub you can
store public images free of charge under
your account but you will need to pay to
host private
images having your images public is
great for hosting open source projects
but for any commercial proprietary
applications you will most likely want
to opt for hosting the images
privately if you don't want to use a
thirdparty service you can always run
your own dog
registry my advice is that unless you're
planning on hosting a very large number
of images it's absolutely worth the cost
to spend a few dollars per month on a
hosted Docker registry such as Docker
Hub once your application EOS system
starts to build around Docker the
registry becomes a critical piece of
infrastructure and keep keeping it up
and running can prove to be a
substantial piece of
work when you build or download a Docker
image to a computer and run it it
becomes a running
container if you don't tell Docker
otherwise Docker will run a
pre-specified command inside the
container as far as this command is
concerned it is running inside a Linux
server and it has access to other
commands and resources that are
available inside the same container
you can think of a container as a
Sandbox where processes running inside
the container do not have access to the
host system or other containers unless
you explicitly specify otherwise note
that the complete container runs around
a single process created by the initial
command used to start the container if
this process terminates for any reason
the container will
stop an example for our container would
be the engine X server process when you
start a container the engine ex server
process starts in it and when the engine
ex server process dies or is killed the
container will stop as
well it is possible to run multiple
processes inside a single container by
for example using an init system to
spawn the processes you need but for the
purposes of this session we'll use the
concept of just a single process per
container the basic workflow with do is
as
follows first you will build a Docker
image either on your workstation or on a
continuous integration
server second you will push this Docker
image into a Docker registry to make it
available for other
computers third you will most likely
want to run the image on a server that
is accessible via the Internet so
clients can access whatever server
software is running inside your
container most often this is an HTTP
server and in this session we'll be
using the engine X HTTP
server in software delivery environments
that Embrace continuous delivery this
workflow is often part of a fully
automated
pipeline a pipeline like this can
trigger from committing a piece of code
into a version control system and just
automatically deploy a new version of
your application into a testing or even
production environment whenever you
change your application
code as we discussed earlier a container
is isolated from the machine it's
running on as well as any other
containers running on the same machine
unless you explicitly Define connections
between
them this is out of the scope of this
session but I recommend you spend some
time after the session to learn about
Docker networking and links between
containers in this session we'll be
running a single container that can
connect to the internet through the host
machine and expose a single TCP port for
the enginex server
process building Docker images is done
with Docker files you can think of a
Docker file as a text file containing a
recipe to tell Docker how to build an
image the basic function of a Docker
file is to be a list of instructions
that incrementally manipulate an
existing Docker image the word EX
assisting here is important while you
can theoretically build a Docker image
from scratch in most use cases you will
actually be using another Docker image
as the base for your new
image many Frameworks and open source
projects publish their own Docker images
which can be used by developers to build
images on top
of in this session we'll be using a new
Docker feature called multistage
builds this feature is available from
docker version 17.05 forward and it
allows you to use one base image for
building our application and another
base image for serving it this works
brilliantly for our use
case while angular is often used as part
of the full mean stack in this session
we're only concentrating on
angular I'm assuming you know angular
and how the angular command line
interface works and that you have a
package.json in your application
directory if you're not quite there yet
check out the excellent getting started
guide on the angular
website please note that the
functionality of angular varies wildly
between major versions however the
fundamental idea behind containerizing
an angular application is the
same our Docker file is going to have
two
stages first we'll need to build our
angular application using the NG build
command this will output the package
application in the disc directory under
our application
route in the second stage we will place
the files from the dis directory into a
directory that served by a web server
and finally run the web server
itself this is where the multi-stage
build feature comes in handy as you
remember a Docker container only has the
software available which we explicitly
added into
it this means that for the NG build
command to work we need to have the
angular command line interface installed
inside our Ducker
container this is something we wouldn't
want on our final Ducker image because
it's just extra weight and on the other
hand we wouldn't want the web server
inside the image we're building our
application
in because the angular CLI is
distributed as a node package we can use
the official node Docker image as the
base of our first Builder
stage because angular is a front-end
framework it doesn't come with a server
side component other than the test
server shipped with the angular
CLI unfortunately this test server isn't
fit to be run on production so we'll
want to use something else to serve the
angular HTML
files in this example we will use a
robust web server called engine X to
serve the angular
application please note you could just
as well be using a server application
like Apache or liy instead I'm not going
to be diving into the specifics of
configuring engine X engine X does come
with good documentation but more
importantly it also comes with an
official Docker image we can use the
official engine X Docker image as the
base for our angular application image
the the good thing is that this makes
our Docker file very short and simple
let's have a
look this is the docker file and it's
divided into two
stages the way the multi-stage build
Works in Docker is that we're actually
creating multiple Docker images with a
single Docker file but we're only
keeping the last image we've
defined this allows us to build the
application first and then copy the
resulting build artifacts into the next
Docker image
the idea behind this is that our final
Docker image won't have all the build
time dependencies in it which makes the
resulting image nice and
small in the first stage we'll use the
from directive to instruct Docker that
we want to use the Noe 8 image as our
base
image we're also using the as Builder
keyword so we'll be able to reference
this stage in our second stage
later the next directive is the copy
directive this command is for copying
files and directories from our local
machine to the docker
image we're giving this directive two
arguments the location of our
application code directory and the
target path within the docker
container in this example our
application code sits in test app and we
want to place it in the directory test
app inside the container
the next directive is work dear this
tells Docker that all following commands
should be run within the specified
directory which in this example is the
application directory within the
container the next directives are the
Run directives as you might guess these
directives instruct Docker to run
commands inside the
container first we're running npm
install to install all node packages
defined in the package.json file of our
application then we're running NG build
to package our angular application in
the dis
directory now we'll get to the second
stage which is copying the contents of
the dis directory into a directory
that's served by default by
enginex we're using the from directive
again this tells Docker to start
building another image we're defining
the from image to be an image called
engine X which is the official enginex
Docker image this image is preconfigured
to run the enginex server and serve HTML
files that are stored in a predefined
directory next we'll use the copy
directive but this time with the from
argument this tells Docker to not look
for the source path in our workstation
but instead the previous stage in this
Docker
file essentially we're copying the
contents of the dis directory in the
previous Docker image into a directory
called user share enginex HTML inside
our new
image the enginex docker image will then
serve the contents of this
directory finally we're using a
directive called expose to tell Docker
that the port 80 has a server running in
this image the port 80 is the default
HTTP Port as you might
know note that the exposed directive
alone isn't enough to actually expose
the port when the image is run you can
merely consider this as a documentation
feature and we'll still need to
explicitly map Port 80 from the
container into a port on the host
machine this enables you to connect to
the mapped port on the host machine
which will then forward the connection
into the
container to follow this session for
your angular application create a file
with these contents and place it in The
Parent Directory of your application
code remember to customize the paths in
the file
accordingly make sure the name of the
file is Docker file written as one word
with the capital
D now that you've learned what the
docker file is and what one looks like
for an angular application let's learn
how to use it to containerize an example
angular
application for this we'll be using the
docker command line
interface after you've installed Docker
on your workstation you can use the
docker command line interface to build
and run
images to find the command line shell on
a Mac click on the spotlight search icon
on the top right corner of your screen
and write terminal followed by by
enter while the docker command line
provides many features in this session
we'll concentrate on two the docker
build and the docker run commands first
let's navigate to the directory with our
Docker
file the command we'll be using is
Docker
build we'll be giving the command the
minus t argument which defines the tag
for the image that will be built
for this we'll be using the tag test
app the second argument for the docker
build command is the build
context this is where Docker expects to
find the docker file we'll use a single
dot to denote the current directory and
press enter to start
building you can see Docker runs through
every directive in the docker file as a
step outputting the directive and the
output of running it
finally you'll see that Docker has
successfully built the image and taged
it as test app latest as you might
remember from before since we didn't
Define the version tag the tag latest is
used automatically great work now let's
run the image we just
built for this we'll be using the docker
run command with we'll be giving the
docker run command a few arguments first
minus I and minus t to denote that we
want to run the image in an interactive
terminal that is running the container
attached to our current terminal session
so we can see the output and interact
with the
process we'll also be using the minus P
argument to map the port 8080 on our
workstation to Port 80 inside the
container as you remember the engine X
server is listening on Port 80 with this
mapping anyone connecting to port 8080
on our workstation will get forwarded to
Port 80 inside the
container finally the last argument is
the tag of the image we want to run
we're using test app colon latest now
that the container is running let's use
our web browser to navigate to port 8080
on our local machine and we can see our
angular application being served well
done you can see that engine X by
default outputs all the requests in the
standard output of the
container after you've finished testing
press contrl C in your terminal to
terminate the engine X process and tus
kill the container you were running and
that's how you dockerize an angular
application before you go three
takeaways one using Docker makes it easy
to ensure that you're application runs
consistently on different workstations
and
servers secondly angular is a front- end
framework angular needs a separate web
server software for serving it enginex
is a fast and robust Choice finally
using multi-stage Docker files enables
you to streamline your build process and
create slim Docker images so if you are
looking to become a Docker expert
consider the postgraduate program in
devops by simply learn to gain expertise
in Docker this program offers Blended
learning that combines live online devop
certification classes with interactive
Labs that will give you hands-on
experience you will gain expertise in
the tools like terraform Maven anible
genkins Docker junit G and more we
prefer one year of Prior experience to
enroll in this course so why wait enroll
the course from the link in the
description box why exactly the docker
networking is important let's let's talk
about some scenario over here so my
application is something which totally
works perfectly fine into my system and
there is no issues on there I ran it
perfectly fine and I got the response
I'm able to interact everything I'm able
to do but the same application does not
works on the other person's system now
what could be the issue because it's
working on one machine and it's not
working on another machine now the
application does not work on another
system because there is a difference in
the computer systems maybe one system is
having a machine and another system is
having a different machine and you may
be having some of the differences maybe
in the uh term of Hardware maybe in the
terms of software so the differences are
possible in two different machines so
how we can get a solution that uh we
don't get this kind of problems and once
the container is prepared it should be
able to run it across different systems
so doer networking is the ideal solution
for this uh problem over here with
Docker networking the application works
totally fine and uh it's uh something
which can uh work on any system you
don't have any dependency that it will
run only on a specific machine or a
particular system so what exactly is in
Docker networking all about so Docker
networking enables a user to link a
Docker container to as many as networks
you know he or she requires so uh you
can actually connect a Docker container
to a specific Network the docker
networks are used to provide complete
isolation to the docker containers so
you can have it like okay I want to run
some couple of containers in a different
network and another containers I want to
run it into a different networks so you
can do that isolations and the whole
purpose of doing that isolation is so
that we will be able to have a proper
networking and proper executions and all
that stuff so Docker networking is a
very important concept when we talk
about or when we come up with the
interaction on having the docker
containers isolated so it resolves most
of the problems okay so um you can have
like uh multiple containers and running
in same n Network you can have uh
different uh particular networks also
there so uh a specific Network have its
own uh attributes and then you can use
those attributes to connect to that
containers and to access that containers
but the whole idea is that the
containers are something which is uh
specifically running in a set of uh
servers or in a set of uh platform so
whenever I'm trying to look forward to
that particular container uh it has to
be there into a network if you don't run
it into your custom Network then
whatever the default Network present it
uh will be hosted in that Network there
so how does the docker networking
actually works so you got a Decor file
so using the docker file you got up a
custom Docker image and from this Docker
image you have to actually go there and
uh you know put up like the docker
container so um you can also store this
Docker image to the docker Hub so that
the same Docker image can be shared
across different users and they can also
pull down their Docker image and they
can have the container running in their
systems so for the collaborating the
docker image you require the docker Hub
and the container is the ultimate thing
or object which we need to create out of
the docker image when we go with the uh
particular process when the whole
process goes on over there so what
happens that uh the instead of the
docker Hub you can use your own private
repositories also that's something which
totally relies on to you that how
exactly those things works on so a
Docker file creates a Docker image using
the build command uh a Docker image
contains all the project source code all
the dependencies U using this Docker
image you can actually run the source
code in order to get a Docker container
the moment you initialize or run an
instance of Docker image you will be
able to get a Docker container you can
actually use the dockerhub for
collaborating or for sharing the docker
image so you can store the docker image
on Docker Hub and then different people
can actually go ahead and download or
pull this Docker image onto their system
and they can also run their specific
containers so what are the different
advantages we get with the docka
networking very first one is the rapid
uh deployment portability better
efficiency uh faster configuration
scalability and the security over here
so these are some of the advantages
which we get when we talk about the
docker networking here now let's talk
about the container Network model here
so what is the network model we are
looking forward from the containers
perspective now this is a kind of a
architecture here now in this one let's
see that how it actually works on now
you have a Docker engine which is a main
component which takes up all the things
and all the important components are
actually interated with each other now
the network sandbox is the one in which
you will be having your container
running up and running now every network
is having its own attributes the uh
particular different Gateway IPS the IP
addresses the range of IP address the
cidr blocks also everything can be
different to for two different networks
so the whole point is that you will be
defining okay if you don't Define that I
want to run this uh particular container
into a specific Network then definitely
that's going to run into default Network
which is which gets created the moment
you do the installation of Dockers but
anyhow any network is required for
running a Docker container so there are
different network drivers which is
available there so if you feel that you
want to create a network so you have to
choose a particular uh networking or the
connectivity over there and uh it really
helps us to understand that how the
configurations and uh how the different
modification and those changes can be
really done here so a lot of changes are
actually going on and uh you know
performing there and this really helps
us to understand that how the things are
being done as such over there so network
is something which is very important
from the containers perspective so the
network sandbox is an kind of a isolated
sandbox that holds the all the Network
configurations of the containers so you
will be requiring a at least one Docker
container network if you want to run a
container inide in that particular
sandbox so sandbox is created when a
user requests to generate an endpoint on
the network so that's where the sandbox
gets created uh it can have several
endpoints in a network as it represents
a container's network uh configurations
like IP address Mac address DNS name so
every container will be having its own
unique IP address right so that kind can
be a particular endpoint to connect to
that container and it can have it like
different uh endpoints and different
different networks so that's always the
possibilities which is available there
now the endpoints uh uh establishes the
connectivity for the container services
with other services also so the moment
you work on the uh particular uh end
points so that can be used for having
okay there is a service running in a
container so how you can do that so if
in order to connect that you require
these endpoints so that you can
establish the connective it with the
container services whatever running
inside these Docker
containers right now the network is
something which provides the specific
underlining Network attributes like uh
the networking support to the docker
containers because these uh Network
sandbox uh boxes will definitely have a
requirement of the network so how you're
going to get that that you will be only
able to get when you go okay I will be
looking forward for the connectivity and
I can go for the communications and I
can give the hardware because although
it's a OS level virtualization but still
you require some interaction with the
hardware from the networking perspective
and then Docker engine is uh the base
engine uh which is installed on the host
machine and it's actually the main
component which makes sure that the
docker containers and the corresponding
Docker services are always up and
running and the uh using the network
drivers you can actually create multiple
networks onto your system and you can
have those uh configurations now the
network architecture is something which
helps us to provide the connectivity
among the endpoints that Bel probably
belongs to the same network and uh it's
something which is a kind of isolated
Network so different different networks
can be isolated uh networks and we can
have the end points to them so uh there
are different ways in which we can
establish the connectivity and this is a
complete uh Network architecture which
is there so you have a network drivers
you have the networks you have the
network sandboxes in which internally
the docker containers are running and
then these uh networkand boxes are
having their respective end points there
to access them to establish the
connectivity now let's talk about the
network drivers now there are five type
of uh network drivers which is available
first one is the bridge host nun overlay
and Mac vand so these are the different
uh particular network drivers which is
available over here uh first one is a
bridge it's a private default Network
created on the host if you go there if
you go to the install ation of a Docker
this is the default driver which gets
installed onto the system onto the host
the containers link to this network will
have an internal IP address through
which we they can actually communicate
with each other on the same machine also
you can establish the connectivity you
can connect to the containers using that
private IP address but outside the
server you will not be able to have the
communication but internally you can
connect to the containers and containers
can also interact with each other they
can also communicate with each other
with this one uh the docker demon
creates the uh Docker zero so if you run
the IP config config or if config on the
system you will be able to see a dock
zero kind of ethernet Bridge uh Network
which is created using which you can see
that okay this is the network which is
created by default it's a default
installation or default configuration
which automatically happens the moment
you set up a Docker instance so you
don't have to go into the configurations
or in the setup of there it will be
already available on that particular
installation mode there there and next
one is the host one so it's a public
network so when we go with the public
network it's actually uses the host IP
address and a DCP port in order to uh
interact with the service running inside
the docker container so if you are
looking forward that I want to reach out
or I want to connect to the container
but I want to use an uh IP address which
can be accessible from outside world
probably some other server should be
able to resolve that in that situation
the host Network driver comes into the
picture so it uh effectively disables
the network isolation between the docker
host and the docker containers which
means that using this driver a user can
actually run multiple containers you
know it's it's not possible for the user
to run multiple containers on the same
host because of the fact that you are
using uh the host IP address now when we
were talking about the bridge connection
at that moment of time we were getting
the private IP addresses which was
totally isolated and totally unique so
there were actually if you create uh
using the bridge connection multiple
containers so you will see that if there
are 53 containers everyone will be
having a different P private IP address
but the moment you go for the host one
so in that situation the IP address is
not going to change so you get some
restrictions okay because the
combination is of IP address and Port so
uh IP address and at8 can happen only
once it cannot happen multiple times so
that's a limitation that you cannot run
multiple containers on the same host due
to this restriction and uh next one is
the none so in this network driver the
docker containers will uh neither have
any access to the external network or it
will not be able to communicate with the
other containers also total isolation
and total private containers are being
done when we use this driver so if we
these options are usually used when you
want to uh disable so this is not doing
nothing but disabling the total
networking functionality for a specific
container that's what it happening over
here next one one is the overlay so this
one is utilized for creating an internal
private Network to the docker nodes in
the docker swam cluster so overlay does
not comes up in case of a standalone
Docker installation the moment you go
with the uh particular Docker swam
cluster at that moment of time the
overlay type of network is created so
Docker uh swam is an ustation tool so uh
that requires a overlay kind of uh
driver which can um you know help them
to achieve the networking in case of
cluster because in case of Docker swam
you have Master node manager node worker
nodes different different machines are
there so in that situation overlay is
the main component or is the main type
of uh driver which is used and then we
have the macvan so it simplifies the
communication processes between the
containers this network assigns a MAC
address to the docker containers with
this Mac address the docker you know
servers will route the network traffic
to a router so it's it's not on the
basis of IP address it's actually on the
basis of of Mac address so uh on the
specific Mac address the network this
Docker server will redirect the network
to a specific
router right it's usable when a user
wants to directly connect uh the
containers to the physical Network
rather than the docker host so that's
where this communication works on so
it's a direct connectivity between the
host Network and uh the docker container
so there is no virtualization which is
happening over here so uh it's something
which is you know giving a access to the
networks but it's definitely on the
basis of Mac address which on which the
communication is happening now let's
talk about the networking implementation
and we'll see that how we can go for a
particular Bridge Network implementation
there so let's go back to the virtual
machine so I am connected to a
particular system here so I can check
that if the docker is available to me
and I can just run the docker version so
it says that okay the uh client and the
server component both is available there
now we are going to see that what are
the the different uh containers which is
what are the different Docker containers
Network which is available there so I'm
going to run a command called Docker
Network LS now the moment you do it over
here you will see that none host and
Bridge these are the different
containers uh Network which is available
there and uh you can see their Network
IDs the names the drivers the drivers is
also very important because that's what
we are going to use here so it's a very
good one here so the default one you can
see that the bridge is uh available and
uh it's a default one whenever you
create any container by default whether
you give the container attribute or not
it's something which gets created into
the uh terms or in the uh basis of uh
the container only in the default
Network only so uh how we can get that
we can always get that part by um you
know having the understanding that what
network we are using so I'm going to run
a couple of containers or small
container and we can see that how
exactly uh the containers can be
executed and uh can be done over there
right so uh let's see so I'm going to
run a
command
Docker run hphone D
ID hph iPhone
name Aline
one
Alpine image is what I'm going to use
and
so it uh pulls down the Alpine image
that's what it happened and the same
thing I'm going to run but uh this time
I'm going to run uh Alpine 2 because uh
I want to create two
containers right so with this one what
will happen that I will be having a like
two containers available here so I can
say like Docker PS so I can see that two
containers are ring one is the Alpine
and one is Alpine 2 right so now what
happens that we will see that how the
connectivity happens and and uh what are
the network details so what I'm going to
do is that first of all I'm going to do
an uh inspect over there onto the uh
Bridge connectivity so that I can get
the detailed information so um this is
the name of uh the particular ones of
the network so this is the network which
is available and this is the default one
so if I scroll up you can see that this
is the name of the bridge connectivity
and uh even if you run the docker
Network LS so uh you will be able to see
that the ID also so ID also if you
compare so uh e65 that's the one which
is available now this one is something
which is using a driver called bridge
and here you can see the subnet
information now if you scroll down you
can see that two containers are
available here so I did not provide any
kind of network details but uh
definitely what happens that uh the two
containers which I deployed are
basically running inside this network
okay and you can see that uh the bridge
name Docker zero is the internet Network
which gets created uh which during the
installation of Docker gets uh
configured now you can see that uh this
is the IP address this is the IP address
here this is the MAC address and this is
also a MAC address here so uh the IP
address you can see this is a 3.0 uh3 in
the last and this is two over here so
you can see that increase the sequence
is there so Alpine 1 was having an IP
1721 17.0.2 and uh Alpine 2 is having an
incremented IP address like a different
one as in three so you can see that
these different IP addresses are there
and uh represents if you want to access
them if you want to connect on them you
will be able to have the connectivity
accordingly so uh even if you go inside
these containers you can actually see
that what exactly uh the connectivity
you want to establish or the
connectivity you want to perform
here right so this is the way that how
the network uh usually works and uh the
networks Bridge you are using over here
is the bridge once which definitely
helps us to understand that how the
configurations can be done now so uh
what we are going to do over here is
that we will try to see that how exactly
uh we can uh go ahead and uh we can
connect on there and uh we can also see
that if we are able to communicate with
each other because probably uh if I run
like Ping command over here and I'll put
the containers like
two and I'm able to reach out there is
no problem in reaching out to these IPS
because these are the private IP
addresses and I will be able to reach
out to them without any problem and
without any concerns so from the current
uh connectivity perspective there is no
problem as such and I have a connection
uh available over here but let's see uh
we'll go uh back to the containers we'll
see that how exactly we can talk about
we can reach out the one container with
each other so I want to connect Alpine
one with alpine 2 so how we can do that
so I'm going to use like Docker
l
Pine
one okay now I got a terminal so what I
can do is that I can go for the IP
address show so with this one what will
happen that I will be able to know the
IP address so do two is the one which is
available over here okay so this is the
IP address now I can uh simply do the uh
small ping command so I can say like
Ping google.com so that will show me
that if I have an internet connectivity
so that is great so I got a connectivity
with the internet uh over there so I
will be able to reach out any websites
or any kind of that order now uh I have
another IP so dot three is remember we
were using different IP so I can
actually do a ping also here so pink
hyphen C5 five request will put and uh
three we'll do here now we are trying to
reach out to the Alpine 2 over there so
although we are connecting to the Alpine
1 but we have the connectivity with
alpine 2 also so from Alpine 1 you can
connect to the Alpine 2 containers so
that shows that since both of them are
running in the same container so they
are having the automatic uh uh
connectivity and you can easily reach
out to them and without any issues you
can do that and similarly the same thing
you can do it uh in case of uh uh Alpine
2 also so you get it connected to the
Alpine 2 and uh you can uh just follow
the same thing so you can connect it uh
with the if you go to the Alpine 2 you
can connect to the Alpine 1 also so if
you come out the containers will be
always up and running and uh you will be
able to have the uh mechanisms there so
the whole idea about these things is
that it will be basically helping you to
understand that how the executions can
be done and how we can perform the
activities as such over here so this is
the way that how we deal with the
networking and how the containers gets
created into a network and then they can
interact with each other darker but
before we jump into that I want you to
hit the Subscribe button so you get
notified about new content as it gets
made available and If you hit hit the
notification button that notification
will then pop up on your desktop as a
video is published from Simply learn in
addition if you have any questions on
the topic please post them in the
comments below we read them and we do
reply to them as often as we can so with
that said let's jump into kubernetes
versus Docker so let's go through a
couple of scenarios let's do one for
kubernetes and then one for Docker and
we can actually go through and
understand what the problem specific
companies have actually had and how
they're able to use the two different
tools to solve them so our first one is
with Bose and Bose um had a large
catalog of products that kept growing
and their infrastructure had to change
so the way that they looked at that was
actually establishing two primary goals
uh to be able to allow their product
groups to be able to easier more easily
catch up to the scale of their business
so after going through um a number of
solutions they ended up coming up with a
solution of having kubernetes running
their iot platform as service inside of
Amazon's AWS cloud service and what
you'll see with both these products is
they're very Cloud friendly but here we
have um Bose and kubernetes working
together with AWS to be able to scale up
and meet the demands of their product
catalog and so the result is that we
were able to increase the number of non
production deployments significantly by
taking the number of services from being
large bulky Services down to small
microservices being able to handle as
many as
1250 plus deployments every year an
incredible amount of time and value has
been opened through the use of
kubernetes now let's have a look at doer
and see what a similar problem that
people would have so uh the problem is
with PayPal and PayPal um processes
something in the region of over 200
payments per second across all of their
products and PayPal doesn't just have
PayPal they have rry and venmo so the
challenge um that uh PayPal was uh
really being given is that they had
different architectures which resulted
in different maintenance cycles and
different deployment times and an
overall complexity from having a decades
old architecture with PayPal through to
a modern architecture with venmo through
the use of docka PayPal was able to
unify the application delivery and be
able to centralize the management of all
of the containers uh with one existing
group the net net result is that PayPal
was able to migrate over 700
applications into doer Enterprise which
consists of over 200,000 containers this
ultimately opened up a 50% increase in
availability for being able to um add in
additional time for building testing and
deploying of applications just a huge
win for PayPal now let's dig into
kubernetes and ER and so kubernetes is
an open source platform and it's
designed for being able to maintain a
large number of containers and what
you're going to find is that your
argument for kubernetes versus Docker
isn't a real argument it's kubernetes
and Docker working together so
kubernetes is able to manage the
infrastructure of a containerized
environment and Docker is the number one
container management solution and so
with Docker you're able to automate the
Dey M of your applications being able to
keep them in a very lightweight
environment and being able to uh create
a nice consistent experience so that
your developers are working in the same
containers that are then also pushed out
to production so with darker you're able
to manage multiple containers running on
the same Hardware much more efficiently
than you are with a VM environment the
productivity around Docker is extremely
high you're able to keep your
applications very isolated uh the
configuration for docka is really quick
and easy you can be up and running in
minutes with docka once you have it
installed and running on your
development machine or inside of your
devops environment so we look at the
deployment between the two um and the
differences kubernetes is really
designed for a combination of PODS and
services in its deployment whereas with
Docker it's around about deploying
services in containers uh so the the
difference um here is that KU etes is
going to manage the entire environment
and then and that environment consisting
of PODS and inside of a pod you're going
to have all of your containers that
you're working on and those containers
are going control the services that
actually power the applications that are
being deployed cuetes is by default an
autoscaling solution it has it turned on
and is always available whereas Docker
does not and that's not surprising
because Docker is a tool for building
out Solutions whereas kubernetes is
about managing your infrastructure
kubernetes is going to run health checks
on the liveness and Readiness of your
entire environment so not just one
container but tens of thousands of
containers whereas Docker is going to
limit the health check to the services
that it's managing within its own
containers now I'm not going to kid you
kubernetes is quite hard to set up it's
it's if all the tools that you're going
to be using in your devop environment
it's it's not an easy set up for you to
use um and for this reason you want to
really take advantage of the surfaces
within Azure and other similar Cloud
environments where they actually will do
the setup for you Docker in contrast is
really easy to set up you as I mentioned
earlier you can be up and running in a
few minutes as you would expect the
fault tolerance within kubernetes is
very high and this is by Design because
the architecture of kubernetes is built
on the same architecture that Google
uses for managing its entire Cloud
infrastructure in contrast Docker has
lower fault tolerance but that's because
it's just managing the the services
within its own containers what you'll
find is that most public Cloud providers
will provide support for both kubernetes
and Docker here we've highlighted
Microsoft Azure because they were very
quick uh to jump on and support
kubernetes but the reality is is that
today Google Amazon and many other
providers are having first level support
for kubernetes has just become extremely
popular in a very very short time frame
the company's using both kubernetes and
Docker is vast and every single day
there are more and more companies using
it and you should be able to look and
see whether or not you can add your own
company to this list did you know the
famous Global researcher Gartner
predicts by 2023 more than 50% of the
companies will be adopting Docker
containers however a serverless
container like dock Brer will have a
rise in the revenue from a small base of
465.16
to
944 million in the year 2024 in this
video we will be doing a side by-side
comparison of the docker containers and
the virtual machines but before that
let's have a look at the objectives of
this lesson in this session you will
have a good understanding of what a
virtual machine is and what a Docker is
followed by that we will have a major
difference between Docker and the
virtual machines also we will discuss
some of the key differences between a
Docker and virtual machines and finally
a use case on what made BBC to use
Docker so the first one what exactly is
a virtual machine it is an isolated
Computing envirment that enables a
person to use an operating system via a
physical machine virtual machines
provide the functionality of a physical
computer as you can see on my screen
there is a virtual machine and this is
how a typical virtual machine looks like
now let's look at what a Docker is
Docker is an operating system level
virtualization software platform that
enables software developers and it
administrators to create deploy and run
applications in a Docker container with
all their dependencies and a Docker
container is a lightweight software
package that includes all the
dependencies like Frameworks libraries
and many more which are essentially
required to execute an
application this is the architecture of
Docker tool now let's move on to our
next main topic for the discussion that
is Docker versus virtual machine when it
comes to comparing the two we could say
the docker containers have much more
potential than virtual machines as you
can see on the left and right hand side
both the images look similar now let's
define the layers of virtual machine
from the bottom up let's begin with the
infrastructure infrastructure could be a
computer system a laptop or a virtual
private server such as an Amazon ec2
instance or a your virtual machine or
gcp virtual machine instance on the top
of the infrastructure runs the operating
system an operating system is a software
that manages hardware and software
resources for the computer programs on
your laptop this will be likely Windows
Mac OS or Linux flavored operating
system such as a bu2 next comes the
hypervisor a hypervisor is a firmware
that builds and runs virtual machines
there are two types of hypervisors type
one hypervisors are hyperkit for Mac OS
hyperv for Windows and KVM for Linux
operating systems and type two
hypervisors are virtual box and VMware
then we have the guest operating systems
consider an example where you want to
run two apps on your server in total
isolation this process would require two
guest operating systems which will be
controlled by a hypervisor virtual
machines come with many dependencies
where each guest operating system will
at least occupy 5 12 Mb of your RAM this
is worse because each guest operating
system needs its own CPU and memory
resources eventually this will be
expensive then on top of that each guest
operating system requires its own
dependencies such as binaries and
libraries since every application has
different dependencies it requires its
own set of libraries finally it's the
application this layer consists of the
source code for the application you have
built this was all about the virtual
machine on a server here's what the same
setup looks like when you're using
Docker containers here you'll notice
that there are a lot fewer layers Docker
doesn't require a bunch of massive guest
operating systems let me break it down
from the bottom up again infrastructure
and host operating systems are the same
as we discussed for the virtual machine
coming into the third layer instead of a
Hy supervisor Docker uses Docker engine
Docker engine or Docker is a client
server application that builds and
executes containers using Docker
components next we have our
dependencies just like on the virtual
machines here dependencies are built
into a template called Docker images as
you can see on the screen each
application is still isolated and
occupies less space now let's have a
look at the sign significant differences
between Docker and virtual machine let's
start with the operating system first
Docker is a container based model where
containers are software packages used
for executing an application on any
operating
system on the other side virtual machine
is a container-based model it utilizes
user space along with the kernel space
of the operating system a kernel space
is where the core of the operating
system runs and provides services to the
user a user cannot modify the space and
user space is the portion of the system
memory in which the users processes run
Docker containers share operating system
kernels with other applications and
result in a higher server efficiency
hence Docker provides the most
substantial default isolation
capabilities among the other
configuration tools on the other hand
virtual machines do not share the
operating system also it does not
provide isolation in the host kernel in
Docker multiple workloads can run on a
single operating system but in virtual
machine each workload needs a complete
operating system or a
hypervisor the next significant
difference between the two is the
performance in the case of Docker they
use the same operating system without
any additional software like hypervisor
in case of virtual machines performance
is isues are a major problem it can be
due to the several reasons like CP
constraints memory allocation Network
latency and many more running multiple
virtual machines lead to an unstable
performance Docker containers can start
up quickly and result in less pop time
whereas virtual machines do not start
quickly and lead to a PO performance now
let's talk about the next difference
which is portability with Docker
containers at a developer can build an
application and store it into a Docker
image later he or she can run it across
any host environment but when it comes
to Virtual machines it has portability
issues virtual machines do not have a
central Hub like Docker does and it
requires more memory space to store data
Docker containers are smaller than
virtual machines due to which the
process of transferring files on the
host file system is easier on the other
hand dependency on the host operating
system and Hardware makes virtual
machines less portable while
transferring files virtual machines
should have a copy of the operating
system and its dependencies due to which
image size is increased and becomes a
tedious process to share
data the boot of time between the both
is very different the application in
Docker containers start without any
delay since the operating system is
already up and running on the other hand
virtual machines take much longer time
than it takes for a container to run
applications these containers were
basically designed to save time in the
deployment process of an application
whereas in Virtual machines to deploy a
single application virtual machine needs
to start an entire operating system
which would result in a full board
process so those were the major
differences between Docker and the
virtual machine
now let's discuss the minor differences
between
them Docker is not always in a running
state it stops when the stop command is
executed whereas the virtual machines
are always running in the background
which will result in huge RAM
consumption talking about snapshots
Docker has a lot of snapshots a snapshot
is an image that you can upload on a
private repository to access it on
another host but virtual machines do not
consist of any snapshots in Docker
images can be version controlled just
like git they have local registry called
Docker Hub where uses store and
distribute container images on the other
hand virtual machine does not have a
central Hub they are not version
controlled Docker can run multiple
containers on a system users can connect
multiple containers using userdefined
networks and shared volumes multiple
containers can be accessed on the same
machine and share the operating system
kernel with other containers each
container is isolated in user space Also
Docker containers occupy less space than
virtual machines and start
instantly virtual machine can run only a
limited number of virtual machines on a
system since each virtual machine
requires a certain amount of CPU RAM
memory and other resources your physical
systems will have less space multiple
containers can be started at a time on
the doer engine the isolation
environment allows a user to run and
deploy several containers simultaneously
on a given host virtual machines can run
only a limited number of virtual
machines on a system since each virtual
machine require certain amount of CPU
RAM memory and other resources your
physical systems will have less space
now that I have told you the differences
between Docker containers and virtual
machines let me show you a real life
case study of how BCC uses Docker BCC
news is a British News Channel over 500
developers working across the globe BCC
news delivers broadcast in almost 30
different languages and with over
880,000 Daily News in English alone the
news channel ran more than 26,000 jobs
with more than 10 continuous
Integrations with sequential scheduling
the company had issues with identifying
a way to unify the coding process and
monitor The Continuous integration
consistently also the existing jobs took
up to more time that is up to 60 Minutes
to schedule and perform its task Docker
allowed BCC news to eliminate job wait
times and run jobs in parallel it also
gave the users the ability to work in a
more flexible CI environment where
entire code processes were unified and
stored in a single place however Docker
succeeded and spreading up the whole
continuous integration process so if you
are looking to become a Docker expert
consider the postgraduate program in
devops by simply learn to gain expertise
in Docker this program offers Blended
learning that combines live online devop
certification classes with interactive
Labs that will give you hands-on
experience you will gain expertise in
the tools like terraform Maven anible
Jenkins doer junit G and more we prefer
one year of prior experience to enroll
in this course so why wait enroll the
course from the link in the description
box so let's go through and you're going
to be asked to explain what the
architecture of Docker is and Docker
really is the most popular
containerization environment so Docker
uses a client server architecture and
the docker client is a service which
runs in a command line and and then the
docker demon which is run as a rest API
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an executable package of
applications and its dependencies
together and then finally the docker
registry is a service to host and
distribute Docker images among other
users so you'll also be asked to provide
what are the advantages of Docker over
virtual machine and and this is
something that comes up very
consistently in fact um you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a Docker
or Docker like environment and really
the the arguments for Docker are just
absolutely fantastic you know first of
all Docker does contain and occupy
Docker containers occupy significantly
less space than a virtual machine or a
dedicated machine the boot up time on
Docker is significantly faster than a VM
containers have a much better
performance as they hosted in a single
Docker image Docker is highly efficient
and very easy to scale particularly when
you start working with kubernetes easily
portable across multiple platforms and
then finally for space allocation docko
data volumes can be shared and reused
among multiple containers the argument
against virtual machines is significant
and particularly if you're going into an
older environment where a company is
still using actual dedicated hardware
and haven't moved to a cloud or
cloudlike environment your Arguments for
Docker are going to be very very
persuasive be very clear on what the
advantages are for Docker over a virtual
machine because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to work with the tools
like Docker you need to be able to
effectively share with that team what
the benefits are so how do we share dock
containers with different nodes and in
this instance what you want to be able
to do is Leverage The Power of Docker
swarm so Docker swarm is a tool which
allows the it administrators and
developers to create and manage clusters
of swarm nodes within the docker
platform and there are two elements to
the node there's the manager node and
then there's the the worker node the
manager node as you would assume manages
the entire infrastructure and the worker
node is actually the work of the agent
as it gets executed so what are the
commands to create create a Docker swarm
and so here we have an example of what a
manager node would look like and once
you've created a swarm on your manager
node you can now add worker nodes to
that swarm and again when you're
stepping through this process be very
precise in the execution part that needs
to be taken to be able to effectively
create a swarm so start with manager
node and then you create a worker node
and then finally when a node is
initialized as a manag node it can
immediately create a token and that
token is used for the worker nodes and
associating the IP address with the
worker nodes question 17 how to run
multiple containers using a single
service it is possible to run multiple
containers of single service by using
Docker compose and Docker compose will
actually run each of the services in
isolation so that they can interact with
each other the language used to write
out the composed files that allow you to
run the service is called yaml and
stands for yet another markup language
so what is the use of a Docker file so
Docker file actually is used for
creating docket images using the build
command so let's go through and show on
the screen what that would look like and
this would be an opportunity where if
you're actually in a technical interview
you could potentially even ask hey can I
draw on a whiteboard and show you what
the architecture for using the build
command would look like and what the
process would look like um again when
you're going through interview process
as someone who interviews a lot of
people one of the things I really like
is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to the
Whiteboard and actually show what can
actually be done through actually
creating images on the Whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as in architect this
should be something that you are
comfortable doing and by doing it in the
interview and suddenly you want to ask
ask permission before you actually do it
but doing this in the interview really
helps demonstrate your comfortable
feelings of working with these kind of
architecture drawings so back to the
question of creating a Docker file so we
go through and that we have a Docker
file that actually then goes ahead and
creates the docker image which then in
turns creates the docker container and
then we are able to push that out up to
a Docker Hub and then share that Docker
Hub with everybody else as part of the
docker registry with the whole network
so what are the differences between
Docker image and Docker container so
let's go through the docker image so the
docker images are templates of a Docker
container an image is built using a
Docker file and it stores that Docker
file in a Docker repository or a Docker
Hub um and you can use Docker Hub as an
example and the image layer is a readon
file system the docker container is a
collection of the runtime instances of a
Docker image and the containers are
created using Docker images and they are
stored in the docker Damon and every
container is a layer is a read write
file system so you can't replace the
information you can only append to it so
while you can actually use yaml for
writing your so a question you can to be
asked is instead of yaml what can be an
alternate file to build Docker compose
so yaml is the one that is the defold
but you can also use Json so if you are
comfortable working with Json and my is
something that you should be get
comfortable with is you want to be able
to use that to name your files and as a
frame of reference Jason is a logical
way of being able to do value paired
matching using a JavaScript like syntax
so you're going to be asked to how to
create a Docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a MySQL Docker
container so to do that you want to be
able to build a Docker image or pull
from a an existing Docker image from a
Docker repository or Hub and then you
want to be able to then use docket to
create a new container which has myql
from the existing Docker image
simultaneously the layer of readwrite
file system is also created on top of
that image and Below at the bottom of
the screen we have what the commands
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have Docker registry
is an open Source server siiz service
used for hosting and distributing docket
images whereas in contrast for
repositories to collection of multiple
versions of a Docker image in a registry
a user can distinguish between Docker
images with their tag names and then
finally on the registry Docker also has
its own default registry called Docker
har for the repository it is a
collection of multiple versions of
Docker images it is stored in a Docker
registry and it has two tubs a public
and private registry so you can create
your own Enterprise registry so you're
going to be asked you know what other
Cloud platforms that support Docker
really you know list them all and we
have listed here Amazon web services
Microsoft Azure Google Cloud Rackspace
but you could add in their IBM blue mix
you could put in Red Hat really any of
the uh cloud service providers out there
today do support Docker it's just become
an industry standard so what is the
purpose of Expose and publish commands
in docka so if we go through expose is
an instruction used in docka file
whereas publish is used in Docker run
command for expose it is used to expose
ports within a Docker Network whereas
with publish it can be used outside of a
Docker environment for expose it is a
documenting instruction used at the time
of building an image and running a
container whereas with publish is used
as to map a host port to a running
container port for expose is the command
used in Docker whereas for publish we
use the command- P for when we're doing
our command line used in Docker and
examples of these are exposeed 8080 or
with Docker we would put in or for
publish we would do the example Docker
run- d-p and then
0.0.0 80 80 colon 80 as our command line
so here we wrap up doger complete course
so if you like this video consider
subscribing the simply learn to stay Ed
with devops technology thanks for
watching staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get CER if I click
here