1
00:00:01,020 --> 00:00:05,650
[Music]

2
00:00:13,820 --> 00:00:16,320
so blow there and welcome to another

3
00:00:16,320 --> 00:00:19,080
tutorial my name is Tammy Bakshi and

4
00:00:19,080 --> 00:00:20,490
this time we're going to be going over

5
00:00:20,490 --> 00:00:22,230
how you can use PI torch and

6
00:00:22,230 --> 00:00:25,769
specifically open an MTP why in order to

7
00:00:25,769 --> 00:00:28,590
run numeral machine translation tasks

8
00:00:28,590 --> 00:00:30,179
now essentially what I'm going to show

9
00:00:30,179 --> 00:00:32,669
you today is how you can use a sequence

10
00:00:32,669 --> 00:00:35,160
to sequence LS TM long short-term memory

11
00:00:35,160 --> 00:00:37,620
network in order to convert from English

12
00:00:37,620 --> 00:00:40,050
to French sentences in fact today we've

13
00:00:40,050 --> 00:00:42,450
also got a special guest on the show who

14
00:00:42,450 --> 00:00:43,860
is proficient at French and will be

15
00:00:43,860 --> 00:00:46,050
helping us out when testing out the

16
00:00:46,050 --> 00:00:48,120
entire system we'll be getting to that

17
00:00:48,120 --> 00:00:49,920
in just a little bit but just before we

18
00:00:49,920 --> 00:00:51,510
get to that though I have to tell you

19
00:00:51,510 --> 00:00:54,239
that if you watched my autoencoder video

20
00:00:54,239 --> 00:00:56,550
before that will really help you out in

21
00:00:56,550 --> 00:00:58,500
understanding how the system actually

22
00:00:58,500 --> 00:01:00,600
works there will be a link down in the

23
00:01:00,600 --> 00:01:02,609
description to that video and that will

24
00:01:02,609 --> 00:01:05,040
give you a good base to start off of but

25
00:01:05,040 --> 00:01:06,930
for now though let's start with this

26
00:01:06,930 --> 00:01:08,880
video now first of all how will this

27
00:01:08,880 --> 00:01:10,860
system work well if you've watched the

28
00:01:10,860 --> 00:01:13,140
autoencoder video you know that neural

29
00:01:13,140 --> 00:01:15,990
networks can take information really

30
00:01:15,990 --> 00:01:18,540
condense that information down into just

31
00:01:18,540 --> 00:01:21,000
a few points a few few numbers into a

32
00:01:21,000 --> 00:01:23,040
small vector and then decode it and

33
00:01:23,040 --> 00:01:25,110
really expand it back up to its original

34
00:01:25,110 --> 00:01:28,080
form or at least really really close to

35
00:01:28,080 --> 00:01:30,360
his original form now in your own

36
00:01:30,360 --> 00:01:32,520
networks are great at that but what if

37
00:01:32,520 --> 00:01:34,799
you could take information decode it

38
00:01:34,799 --> 00:01:37,049
down into a really small vector then or

39
00:01:37,049 --> 00:01:38,430
encode it down into a really small

40
00:01:38,430 --> 00:01:40,680
vector and then decode it into another

41
00:01:40,680 --> 00:01:43,049
really big vector that has a slightly

42
00:01:43,049 --> 00:01:45,390
different way of representing the same

43
00:01:45,390 --> 00:01:47,700
information for example if you were

44
00:01:47,700 --> 00:01:49,860
using a regular MN ist auto encoder

45
00:01:49,860 --> 00:01:51,750
which I talked about in the last video

46
00:01:51,750 --> 00:01:54,299
you could having your ol network that

47
00:01:54,299 --> 00:01:56,399
took in a regular Animus digit and

48
00:01:56,399 --> 00:01:58,680
outputted an inverted color version of

49
00:01:58,680 --> 00:02:00,630
that image it would be representing the

50
00:02:00,630 --> 00:02:03,750
same information in a different way what

51
00:02:03,750 --> 00:02:05,460
if you can apply the exact same

52
00:02:05,460 --> 00:02:08,340
technique to language that's right so

53
00:02:08,340 --> 00:02:10,349
now I'm going to show you how you can

54
00:02:10,349 --> 00:02:10,830
take

55
00:02:10,830 --> 00:02:12,960
English words an entire english sentence

56
00:02:12,960 --> 00:02:15,240
fit into a neural network have it

57
00:02:15,240 --> 00:02:17,460
condense that down and pound it into a

58
00:02:17,460 --> 00:02:20,010
small vector and then decode it into a

59
00:02:20,010 --> 00:02:22,410
bigger vector but this time instead of

60
00:02:22,410 --> 00:02:24,630
representing the input as English word

61
00:02:24,630 --> 00:02:25,230
vectors

62
00:02:25,230 --> 00:02:27,870
it'll represent them as French word

63
00:02:27,870 --> 00:02:29,760
vectors in the right order and the right

64
00:02:29,760 --> 00:02:32,370
grammar this is really really

65
00:02:32,370 --> 00:02:33,840
interesting and there's actually an

66
00:02:33,840 --> 00:02:36,840
entire new approach this is in fact an

67
00:02:36,840 --> 00:02:38,730
entire new approach to the machine

68
00:02:38,730 --> 00:02:40,650
translation problem instead of

69
00:02:40,650 --> 00:02:42,390
converting from words to words which

70
00:02:42,390 --> 00:02:44,370
oftentimes makes mistakes because of

71
00:02:44,370 --> 00:02:45,720
course in languages you don't always

72
00:02:45,720 --> 00:02:48,450
have the same word order I want to order

73
00:02:48,450 --> 00:02:50,340
an extra large pizza in English gonna

74
00:02:50,340 --> 00:02:52,350
have an entirely different word order in

75
00:02:52,350 --> 00:02:55,140
french however with the neural machine

76
00:02:55,140 --> 00:02:57,420
translation system it will learn how to

77
00:02:57,420 --> 00:02:59,190
change the word order and it will learn

78
00:02:59,190 --> 00:03:02,010
which use to which words to use in other

79
00:03:02,010 --> 00:03:04,620
words places depending on the context as

80
00:03:04,620 --> 00:03:06,570
well and the data that it was trained on

81
00:03:06,570 --> 00:03:09,480
so now let's get into an explanation of

82
00:03:09,480 --> 00:03:11,940
how the entire system works now the

83
00:03:11,940 --> 00:03:13,770
neural machine translation bit has

84
00:03:13,770 --> 00:03:17,160
already been implemented by on github

85
00:03:17,160 --> 00:03:19,200
and there will be a link to that game

86
00:03:19,200 --> 00:03:20,670
hub repository down in the description

87
00:03:20,670 --> 00:03:22,440
below and we showing you that github

88
00:03:22,440 --> 00:03:24,840
repository in just a moment as well the

89
00:03:24,840 --> 00:03:26,519
original implementation for this was an

90
00:03:26,519 --> 00:03:29,280
irregular torch power by luma but now

91
00:03:29,280 --> 00:03:32,010
it's also been translated into Python

92
00:03:32,010 --> 00:03:35,190
YouTube PI torch and it's release so now

93
00:03:35,190 --> 00:03:36,660
I'm going to show you how you can

94
00:03:36,660 --> 00:03:38,340
actually implement a neural machine

95
00:03:38,340 --> 00:03:40,800
translation system but just before we

96
00:03:40,800 --> 00:03:43,350
get to that I'd like to explain how the

97
00:03:43,350 --> 00:03:45,570
entire system works let's get started

98
00:03:45,570 --> 00:03:47,910
now let's take a really simple example

99
00:03:47,910 --> 00:03:52,080
sentence like for example you say I want

100
00:03:52,080 --> 00:03:55,800
an extra large pizza all right now you

101
00:03:55,800 --> 00:04:00,650
have this English sentence

102
00:04:07,459 --> 00:04:10,680
repeat some and let's just say the

103
00:04:10,680 --> 00:04:12,959
person's name that you're asking his

104
00:04:12,959 --> 00:04:14,939
name is James

105
00:04:14,939 --> 00:04:21,570
okay uh and so now what you're gonna do

106
00:04:21,570 --> 00:04:22,560
is of course this is this is completely

107
00:04:22,560 --> 00:04:25,350
valid in English but let's just say you

108
00:04:25,350 --> 00:04:27,060
wanted to convert this to French now

109
00:04:27,060 --> 00:04:28,979
what you could do which is the naive way

110
00:04:28,979 --> 00:04:30,930
of doing this is you could just convert

111
00:04:30,930 --> 00:04:32,940
every single individual word to their

112
00:04:32,940 --> 00:04:35,760
French counterpart which wouldn't be

113
00:04:35,760 --> 00:04:38,070
very nice of course in this case if you

114
00:04:38,070 --> 00:04:39,810
were to do that that would be very very

115
00:04:39,810 --> 00:04:42,419
grammatically incorrect uh and no one

116
00:04:42,419 --> 00:04:44,400
would be able to understand that however

117
00:04:44,400 --> 00:04:46,440
there are some smarter ways of doing it

118
00:04:46,440 --> 00:04:48,270
for example the older older method of

119
00:04:48,270 --> 00:04:50,820
Google Translate which tries to do a

120
00:04:50,820 --> 00:04:52,260
little bit of a smarter machine

121
00:04:52,260 --> 00:04:53,970
translation but I won't be getting into

122
00:04:53,970 --> 00:04:56,490
that algorithm just yet however with

123
00:04:56,490 --> 00:04:58,919
neural networks all you have to do is

124
00:04:58,919 --> 00:05:02,039
take this sentence send it to a

125
00:05:02,039 --> 00:05:07,530
tokenizer okay so now you're going

126
00:05:07,530 --> 00:05:09,419
tokenize this what that means is they're

127
00:05:09,419 --> 00:05:11,610
going to split every individual part of

128
00:05:11,610 --> 00:05:13,590
sentence into of course as I mentioned

129
00:05:13,590 --> 00:05:16,440
individual parts so for example you put

130
00:05:16,440 --> 00:05:17,910
a space before the comma space before

131
00:05:17,910 --> 00:05:19,650
the punctuation and what that would

132
00:05:19,650 --> 00:05:25,979
allow you to do have individual

133
00:05:25,979 --> 00:05:28,740
tokenized element of the sentence into

134
00:05:28,740 --> 00:05:30,449
individual vectors that can go into the

135
00:05:30,449 --> 00:05:32,849
neural network at the end after that

136
00:05:32,849 --> 00:05:34,770
though once that wants the network or

137
00:05:34,770 --> 00:05:37,080
once you have tried your sentence you

138
00:05:37,080 --> 00:05:39,419
can vectorize the sentence and what the

139
00:05:39,419 --> 00:05:41,849
vectorization does of course allows the

140
00:05:41,849 --> 00:05:45,270
neural network to understand the input

141
00:05:45,270 --> 00:05:47,250
now the factorization of course you can

142
00:05:47,250 --> 00:05:48,900
do this by Ward vector is really however

143
00:05:48,900 --> 00:05:50,370
else you don't want to in this case

144
00:05:50,370 --> 00:05:52,190
we're going to use love word vectors

145
00:05:52,190 --> 00:05:55,260
which I will also be talking about in

146
00:05:55,260 --> 00:05:56,370
the rest of the video

147
00:05:56,370 --> 00:05:58,740
but once you have finally vectorize it

148
00:05:58,740 --> 00:06:01,260
you can use a recurrent neural network

149
00:06:01,260 --> 00:06:04,320
to understand that vectorization or that

150
00:06:04,320 --> 00:06:06,960
vectorized version of the sentence and

151
00:06:06,960 --> 00:06:08,430
so in this case you could just use any

152
00:06:08,430 --> 00:06:10,590
recurrent neural network but I'm going

153
00:06:10,590 --> 00:06:12,870
to be using LST ends you could also use

154
00:06:12,870 --> 00:06:15,659
gated recurrent units GRE use but I'm

155
00:06:15,659 --> 00:06:16,250
not going

156
00:06:16,250 --> 00:06:18,020
sacrificing really any accuracy

157
00:06:18,020 --> 00:06:19,760
whatsoever we're going to be going for

158
00:06:19,760 --> 00:06:22,490
LST m/s now the RM what it's going to do

159
00:06:22,490 --> 00:06:24,710
is it's going to take the entire set of

160
00:06:24,710 --> 00:06:27,140
huge vectors and convert it to one

161
00:06:27,140 --> 00:06:29,690
individual set of smaller vectors are

162
00:06:29,690 --> 00:06:31,910
really really tiny vector compared to

163
00:06:31,910 --> 00:06:33,650
what the input is and that can that

164
00:06:33,650 --> 00:06:35,660
actually describes the entire input

165
00:06:35,660 --> 00:06:38,450
sequence in just one small vector its

166
00:06:38,450 --> 00:06:40,730
amount it describes all the semantics of

167
00:06:40,730 --> 00:06:43,010
the input without being very specific on

168
00:06:43,010 --> 00:06:44,950
what the actual awards themselves were

169
00:06:44,950 --> 00:06:47,330
however there are a few problems with

170
00:06:47,330 --> 00:06:49,640
this like for example James isn't an

171
00:06:49,640 --> 00:06:52,100
English word that's just some random

172
00:06:52,100 --> 00:06:53,420
word we came up with in this case a

173
00:06:53,420 --> 00:06:55,370
person's name so how do we deal with

174
00:06:55,370 --> 00:06:56,990
that well what happens is that

175
00:06:56,990 --> 00:06:58,970
vectorization process you could just

176
00:06:58,970 --> 00:07:00,860
input all zeroes as a factor you could

177
00:07:00,860 --> 00:07:03,590
end put a random vector but in this case

178
00:07:03,590 --> 00:07:05,480
what I'm going to do is input an all

179
00:07:05,480 --> 00:07:07,520
zero vector and what that'll do is tell

180
00:07:07,520 --> 00:07:08,840
the recurrent neural network that this

181
00:07:08,840 --> 00:07:11,090
is an out of vocabulary word it is an

182
00:07:11,090 --> 00:07:13,370
unknown word that it's never seen before

183
00:07:13,370 --> 00:07:16,220
and in theory the neural network should

184
00:07:16,220 --> 00:07:18,860
be able to replace this with a token in

185
00:07:18,860 --> 00:07:24,169
this case angular bracket U and K and

186
00:07:24,169 --> 00:07:26,660
then at the very end once you are done

187
00:07:26,660 --> 00:07:28,669
converting it you can put you can

188
00:07:28,669 --> 00:07:31,190
actually replace all the unknown unknown

189
00:07:31,190 --> 00:07:34,040
unknown vectors with the actual words

190
00:07:34,040 --> 00:07:36,080
from English because of course it

191
00:07:36,080 --> 00:07:37,310
doesn't know how to convert them to

192
00:07:37,310 --> 00:07:39,200
French in the first place but then again

193
00:07:39,200 --> 00:07:41,090
once you're done with that recurrent

194
00:07:41,090 --> 00:07:43,010
neural network it'll feed into one more

195
00:07:43,010 --> 00:07:44,900
recurrent neural network that will then

196
00:07:44,900 --> 00:07:47,930
take the encode a vector and then decode

197
00:07:47,930 --> 00:07:49,970
it back into a huge vector that

198
00:07:49,970 --> 00:07:51,880
represents this exact sentence in French

199
00:07:51,880 --> 00:07:55,010
and then in theory you should be able to

200
00:07:55,010 --> 00:07:57,590
reverse the factorization process and

201
00:07:57,590 --> 00:07:59,870
then you can do you can basically take

202
00:07:59,870 --> 00:08:06,950
the vectors word vectors to words run

203
00:08:06,950 --> 00:08:09,800
this process in Reverse and then you

204
00:08:09,800 --> 00:08:12,470
should finally have your output which is

205
00:08:12,470 --> 00:08:18,550
your French sentence

206
00:08:18,699 --> 00:08:20,959
which is what I'm going to be showing

207
00:08:20,959 --> 00:08:24,559
you how to build right now but now

208
00:08:24,559 --> 00:08:26,209
they'll before I tell you how you can

209
00:08:26,209 --> 00:08:28,159
actually implement the system I think it

210
00:08:28,159 --> 00:08:29,539
would be really interesting to see a

211
00:08:29,539 --> 00:08:31,789
demo of this in action in fact this is

212
00:08:31,789 --> 00:08:33,259
actually the exact system that Google

213
00:08:33,259 --> 00:08:35,449
Translate now uses for many different

214
00:08:35,449 --> 00:08:38,180
languages as it creates much much more

215
00:08:38,180 --> 00:08:40,250
accurate results but now let's get over

216
00:08:40,250 --> 00:08:42,829
to a demo of the system itself where

217
00:08:42,829 --> 00:08:45,470
I'll show you a demo of around seven

218
00:08:45,470 --> 00:08:47,839
different sentences and how they are

219
00:08:47,839 --> 00:08:50,269
converted to French via this entire

220
00:08:50,269 --> 00:08:52,399
system now let's get over to how you can

221
00:08:52,399 --> 00:08:55,490
implement this with PI torch alright so

222
00:08:55,490 --> 00:08:57,019
welcome back to the demonstration now

223
00:08:57,019 --> 00:08:58,850
I'm going to show you how this system

224
00:08:58,850 --> 00:09:01,009
actually works now before we actually

225
00:09:01,009 --> 00:09:02,600
take a look at the system we have to

226
00:09:02,600 --> 00:09:05,120
have to we have to find a way to see if

227
00:09:05,120 --> 00:09:07,490
the system actually works and while I

228
00:09:07,490 --> 00:09:10,220
don't know French there is someone who's

229
00:09:10,220 --> 00:09:12,680
joining me today special guest Ari who

230
00:09:12,680 --> 00:09:14,149
is actually very proficient in French

231
00:09:14,149 --> 00:09:15,949
would you like to introduce yourself hi

232
00:09:15,949 --> 00:09:17,899
my name is Abby I'll be helping tad make

233
00:09:17,899 --> 00:09:19,939
with the French translation let's get

234
00:09:19,939 --> 00:09:22,009
into the video perfect all right let's

235
00:09:22,009 --> 00:09:24,379
start now you can actually see on screen

236
00:09:24,379 --> 00:09:26,569
right now all we've got 8 different

237
00:09:26,569 --> 00:09:29,480
sentences and so these are straight from

238
00:09:29,480 --> 00:09:31,459
the French translation system not been

239
00:09:31,459 --> 00:09:33,529
modified whatsoever these are tokenized

240
00:09:33,529 --> 00:09:35,509
versions so before punctuation like

241
00:09:35,509 --> 00:09:37,970
apostrophes commas question marks you've

242
00:09:37,970 --> 00:09:40,220
got little spaces here however those can

243
00:09:40,220 --> 00:09:42,949
be done tokenize just as easily but for

244
00:09:42,949 --> 00:09:44,720
now let's actually begin and see if

245
00:09:44,720 --> 00:09:46,459
these translation system works with a

246
00:09:46,459 --> 00:09:49,129
few examples so now Abby what do you

247
00:09:49,129 --> 00:09:50,750
think the first sentence here actually

248
00:09:50,750 --> 00:09:52,430
says in English so this is the French

249
00:09:52,430 --> 00:09:54,410
version if you want you can actually say

250
00:09:54,410 --> 00:09:55,610
out the French version then we'll talk

251
00:09:55,610 --> 00:09:58,189
about the English a su la and I manifest

252
00:09:58,189 --> 00:10:01,279
yawn xenophobia sandy Emma okay so what

253
00:10:01,279 --> 00:10:02,660
do you think that means in English okay

254
00:10:02,660 --> 00:10:05,589
so are you going to the xenophobia

255
00:10:05,589 --> 00:10:10,250
protests Saturday Emma yes that is in

256
00:10:10,250 --> 00:10:11,689
fact what it means if we were to go to

257
00:10:11,689 --> 00:10:13,699
the English version over here you can

258
00:10:13,699 --> 00:10:15,920
take a look and the original English is

259
00:10:15,920 --> 00:10:18,019
are you going to the xenophobia protest

260
00:10:18,019 --> 00:10:20,809
on Saturday Emma so yes that is correct

261
00:10:20,809 --> 00:10:22,910
and apparently the translation system

262
00:10:22,910 --> 00:10:26,000
has worked in this case so that was a

263
00:10:26,000 --> 00:10:28,029
demo of the translation system

264
00:10:28,029 --> 00:10:30,590
converting one simple English sentence

265
00:10:30,590 --> 00:10:33,620
ah - French let's try it out a few more

266
00:10:33,620 --> 00:10:35,390
so let's just try out number two now so

267
00:10:35,390 --> 00:10:37,850
do you think this is from ink from

268
00:10:37,850 --> 00:10:41,660
French to English la pluie tada they

269
00:10:41,660 --> 00:10:45,560
informed them a class of alright okay so

270
00:10:45,560 --> 00:10:47,720
what would this be in English so like

271
00:10:47,720 --> 00:10:48,920
most of the kids in my class are going

272
00:10:48,920 --> 00:10:51,710
yes exactly that is in fact exactly what

273
00:10:51,710 --> 00:10:53,810
it is in English most of the kids in my

274
00:10:53,810 --> 00:10:57,140
class are going now let's try out one

275
00:10:57,140 --> 00:10:59,600
more so what does this say so this is

276
00:10:59,600 --> 00:11:03,860
French first of all so the program the

277
00:11:03,860 --> 00:11:06,680
program judogi de la ville is sandy

278
00:11:06,680 --> 00:11:10,610
Adisa it's a zoo okay so how what would

279
00:11:10,610 --> 00:11:12,170
that be in English so like the large

280
00:11:12,170 --> 00:11:14,750
like the biggest or large hockey hockey

281
00:11:14,750 --> 00:11:17,900
game yeah a hockey game in the town is

282
00:11:17,900 --> 00:11:20,600
this Saturday at 10 o'clock time p.m.

283
00:11:20,600 --> 00:11:25,520
already great are you ok so does it um

284
00:11:25,520 --> 00:11:27,890
ok so the actual sentence from English

285
00:11:27,890 --> 00:11:30,440
is the town's largest hockey came is

286
00:11:30,440 --> 00:11:33,140
this Saturday at 10 a.m. are you going

287
00:11:33,140 --> 00:11:35,450
so is it does it doesn't mostly match up

288
00:11:35,450 --> 00:11:38,120
or yes emotionally it mostly matches up

289
00:11:38,120 --> 00:11:41,330
but here at the end it's where it says a

290
00:11:41,330 --> 00:11:43,580
tegu it just says like are you there's

291
00:11:43,580 --> 00:11:45,230
no other variable asking like are you

292
00:11:45,230 --> 00:11:49,070
going like that yes so in this case the

293
00:11:49,070 --> 00:11:50,360
reason for that would be that their

294
00:11:50,360 --> 00:11:52,640
perplexity of this model is actually 6.0

295
00:11:52,640 --> 00:11:54,950
7 which is relatively high and this was

296
00:11:54,950 --> 00:11:57,770
only trained for around 24 hours on a

297
00:11:57,770 --> 00:12:00,020
you know mid-sized GPS so if we were to

298
00:12:00,020 --> 00:12:02,240
scale that up to more GP was in much

299
00:12:02,240 --> 00:12:04,250
more training time then we should get a

300
00:12:04,250 --> 00:12:06,290
much lower perplexity in this case

301
00:12:06,290 --> 00:12:08,780
basically meaning error rate and we

302
00:12:08,780 --> 00:12:10,040
should be good to go and that should be

303
00:12:10,040 --> 00:12:11,780
able to translate that nicely and Plus

304
00:12:11,780 --> 00:12:13,220
this was trained on much much more

305
00:12:13,220 --> 00:12:14,959
formal sentences not in this context

306
00:12:14,959 --> 00:12:18,170
however it is again completely possible

307
00:12:18,170 --> 00:12:19,700
to fix that error with more training

308
00:12:19,700 --> 00:12:22,160
data and more training time as well

309
00:12:22,160 --> 00:12:24,110
let's try out number four now ah so this

310
00:12:24,110 --> 00:12:28,130
is French first of all label a so a

311
00:12:28,130 --> 00:12:33,160
Levante about either deep diesel so uh

312
00:12:33,160 --> 00:12:40,339
Judy okay okay so um it says that the

313
00:12:40,339 --> 00:12:42,650
tickets are going on sale at twelve

314
00:12:42,650 --> 00:12:44,940
twelve o'clock

315
00:12:44,940 --> 00:12:47,160
okay so was there an error in the

316
00:12:47,160 --> 00:12:49,050
sausage yes it didn't translate the

317
00:12:49,050 --> 00:12:49,590
Thursday

318
00:12:49,590 --> 00:12:51,360
okay I just left it is Thursday okay so

319
00:12:51,360 --> 00:12:53,820
again that is because we're using very

320
00:12:53,820 --> 00:12:55,470
little training data meaning that

321
00:12:55,470 --> 00:12:57,570
Thursday's not in the vocabulary this is

322
00:12:57,570 --> 00:13:00,390
again an issue with the training data

323
00:13:00,390 --> 00:13:02,100
that we have if I were to have more

324
00:13:02,100 --> 00:13:04,590
training data for example from some

325
00:13:04,590 --> 00:13:05,820
corpuses that I'm going to be showing

326
00:13:05,820 --> 00:13:08,280
you at the end of this video we'd be

327
00:13:08,280 --> 00:13:10,770
able to get much higher accuracy but

328
00:13:10,770 --> 00:13:12,300
then again I'll get into that in second

329
00:13:12,300 --> 00:13:13,650
part of this video for now let's just

330
00:13:13,650 --> 00:13:15,450
continue with simple sentences number

331
00:13:15,450 --> 00:13:17,700
five now so I'm also for the sentence

332
00:13:17,700 --> 00:13:19,320
number four the one on Thurs the one

333
00:13:19,320 --> 00:13:19,920
about Thursday

334
00:13:19,920 --> 00:13:21,600
it shouldn't say sua is you just say

335
00:13:21,600 --> 00:13:23,490
Thursday well because like you're saying

336
00:13:23,490 --> 00:13:27,330
it's like on Thursday it's like it

337
00:13:27,330 --> 00:13:28,920
sounds weird because you're saying that

338
00:13:28,920 --> 00:13:31,080
it's like on top of like Thursday so you

339
00:13:31,080 --> 00:13:33,330
have to say Thursday ok so again this is

340
00:13:33,330 --> 00:13:36,030
one of the examples were less perplexity

341
00:13:36,030 --> 00:13:37,440
it should understand that grammar and

342
00:13:37,440 --> 00:13:39,210
she'll be able to understand that on in

343
00:13:39,210 --> 00:13:42,180
some contexts means you know without sir

344
00:13:42,180 --> 00:13:44,880
or some without so with sir in this case

345
00:13:44,880 --> 00:13:47,400
since it didn't understand Thursday that

346
00:13:47,400 --> 00:13:48,930
wasn't in its vocabulary didn't

347
00:13:48,930 --> 00:13:50,160
understand Thursday and I thought it

348
00:13:50,160 --> 00:13:51,840
might literally be on top of something

349
00:13:51,840 --> 00:13:55,230
and so it was able to infer from that

350
00:13:55,230 --> 00:13:56,730
context but if that were in the

351
00:13:56,730 --> 00:13:58,170
vocabulary you would understand from the

352
00:13:58,170 --> 00:14:01,440
context and not insert sir ah so next

353
00:14:01,440 --> 00:14:03,990
number five so this is French first of

354
00:14:03,990 --> 00:14:06,450
all so la francais a long time for

355
00:14:06,450 --> 00:14:07,320
talking to hed

356
00:14:07,320 --> 00:14:09,240
I turned to pay a dollar a month I lost

357
00:14:09,240 --> 00:14:11,760
that long alright so what do you think

358
00:14:11,760 --> 00:14:13,680
of me in English so French is a very

359
00:14:13,680 --> 00:14:15,630
important language to study other

360
00:14:15,630 --> 00:14:17,460
countries in the world speak this

361
00:14:17,460 --> 00:14:20,010
language okay sure so now that's

362
00:14:20,010 --> 00:14:21,590
actually very very close to what it was

363
00:14:21,590 --> 00:14:23,880
French is a very important language to

364
00:14:23,880 --> 00:14:26,040
study as many countries around the world

365
00:14:26,040 --> 00:14:29,520
speak this language is exactly what the

366
00:14:29,520 --> 00:14:32,250
English version was however it seems as

367
00:14:32,250 --> 00:14:35,310
if as many may have been left out by the

368
00:14:35,310 --> 00:14:38,610
translation system was it so um we

369
00:14:38,610 --> 00:14:39,840
learned to throw in Canada we learn

370
00:14:39,840 --> 00:14:42,240
Quebec French and this is translating

371
00:14:42,240 --> 00:14:45,180
French French so some of these words may

372
00:14:45,180 --> 00:14:47,430
be different from what what you may say

373
00:14:47,430 --> 00:14:49,610
in Canada versus what we say in France

374
00:14:49,610 --> 00:14:51,120
yes

375
00:14:51,120 --> 00:14:53,580
so that might be one of the reasons that

376
00:14:53,580 --> 00:14:56,040
it's a little bit more confusing however

377
00:14:56,040 --> 00:14:57,510
let's go over a number

378
00:14:57,510 --> 00:15:00,329
so which is actually relevant so number

379
00:15:00,329 --> 00:15:02,100
six this is the French version sentence

380
00:15:02,100 --> 00:15:03,630
what do you think it would mean English

381
00:15:03,630 --> 00:15:06,930
okay so example it's a G the dizzy I'm

382
00:15:06,930 --> 00:15:09,779
long appreciate the Canada so what that

383
00:15:09,779 --> 00:15:11,970
mean in English it so for example it's

384
00:15:11,970 --> 00:15:14,880
for example sorry it's a second it's

385
00:15:14,880 --> 00:15:16,170
official second language of Canada

386
00:15:16,170 --> 00:15:18,180
perfect that is exactly what it is in

387
00:15:18,180 --> 00:15:20,579
English for example is the official

388
00:15:20,579 --> 00:15:23,220
second language of Canada now let's try

389
00:15:23,220 --> 00:15:25,860
just two more and see how exactly we're

390
00:15:25,860 --> 00:15:28,740
able to use the system so now this is of

391
00:15:28,740 --> 00:15:30,089
course prediction number seven let's

392
00:15:30,089 --> 00:15:32,730
talk about this well deck eBay in person

393
00:15:32,730 --> 00:15:35,839
asset class evoke a Mahad the class demo

394
00:15:35,839 --> 00:15:39,480
davinia key la okay so what do you think

395
00:15:39,480 --> 00:15:41,339
this would be so describe a person in

396
00:15:41,339 --> 00:15:43,490
this class and your classmates or

397
00:15:43,490 --> 00:15:45,690
students other students in the class

398
00:15:45,690 --> 00:15:47,120
will have to guess who it is

399
00:15:47,120 --> 00:15:49,769
exactly again semantically the exact

400
00:15:49,769 --> 00:15:52,260
same as what was entered into the system

401
00:15:52,260 --> 00:15:54,510
describe a person in this class and your

402
00:15:54,510 --> 00:15:56,339
classmates will have to guess who it is

403
00:15:56,339 --> 00:15:57,810
now one more thing I'd like to point out

404
00:15:57,810 --> 00:15:59,250
here is that if I were to take the

405
00:15:59,250 --> 00:16:01,860
original English sentence here and if I

406
00:16:01,860 --> 00:16:03,209
were to copy that back into this

407
00:16:03,209 --> 00:16:05,610
document take a look at these two days

408
00:16:05,610 --> 00:16:07,050
it copying the punctuation like

409
00:16:07,050 --> 00:16:08,639
correctly where it should be like the

410
00:16:08,639 --> 00:16:10,740
commas and and of course the end is

411
00:16:10,740 --> 00:16:13,050
correct what is the Komets correct place

412
00:16:13,050 --> 00:16:14,100
where it should be is all the

413
00:16:14,100 --> 00:16:16,050
punctuation taking care of nicely yes

414
00:16:16,050 --> 00:16:16,850
thank you

415
00:16:16,850 --> 00:16:19,680
perfect that's great so system again is

416
00:16:19,680 --> 00:16:21,060
learning exactly how to use that

417
00:16:21,060 --> 00:16:22,589
punctuation as well can textually with

418
00:16:22,589 --> 00:16:23,970
the rest of the sentence even though

419
00:16:23,970 --> 00:16:25,709
it's scaling all that information down

420
00:16:25,709 --> 00:16:28,230
to a really small vector however let's

421
00:16:28,230 --> 00:16:30,990
try out one more over here prediction

422
00:16:30,990 --> 00:16:34,290
number eight okay so you know let's see

423
00:16:34,290 --> 00:16:37,170
what entry so the winner will receive a

424
00:16:37,170 --> 00:16:40,769
prize okay again exactly what it was in

425
00:16:40,769 --> 00:16:42,600
English this one was actually completely

426
00:16:42,600 --> 00:16:45,829
correct the winner will receive a prize

427
00:16:45,829 --> 00:16:49,649
exclamation mark and so that was a demo

428
00:16:49,649 --> 00:16:51,449
of the system in action so how many

429
00:16:51,449 --> 00:16:54,649
write the system I would rate it like

430
00:16:54,649 --> 00:16:57,329
like in being eight out of ten

431
00:16:57,329 --> 00:16:59,399
eight out of ten perfect there was just

432
00:16:59,399 --> 00:17:01,110
like some areas where it wouldn't pick

433
00:17:01,110 --> 00:17:04,020
up on some yards yeah and also it would

434
00:17:04,020 --> 00:17:06,270
bring in like the English word into like

435
00:17:06,270 --> 00:17:08,309
it wouldn't travel exactly exactly

436
00:17:08,309 --> 00:17:09,689
it wouldn't translate the English word

437
00:17:09,689 --> 00:17:09,939
in

438
00:17:09,939 --> 00:17:12,579
a French word again all solved with more

439
00:17:12,579 --> 00:17:15,069
vocabulary more training data higher

440
00:17:15,069 --> 00:17:18,250
vocabulary size and of course more

441
00:17:18,250 --> 00:17:20,470
training time as well a hotter just for

442
00:17:20,470 --> 00:17:21,850
24 hours of training this is a

443
00:17:21,850 --> 00:17:24,879
relatively good system an open-end MT py

444
00:17:24,879 --> 00:17:27,309
allows me to build this really really

445
00:17:27,309 --> 00:17:29,559
easily so now I'm going to be getting

446
00:17:29,559 --> 00:17:32,080
over to the actual coding part where I'm

447
00:17:32,080 --> 00:17:33,490
going to show you how to implement this

448
00:17:33,490 --> 00:17:35,139
entire system how to get all the

449
00:17:35,139 --> 00:17:37,059
training data and then how to calculate

450
00:17:37,059 --> 00:17:40,539
calculate a BL you score to find out how

451
00:17:40,539 --> 00:17:42,820
good your translation system actually is

452
00:17:42,820 --> 00:17:44,919
now let's get over to the coding part

453
00:17:44,919 --> 00:17:47,710
Thank You Abby alright so welcome back

454
00:17:47,710 --> 00:17:50,049
to the coding part and now just before I

455
00:17:50,049 --> 00:17:52,570
begin I would like to say a big thank

456
00:17:52,570 --> 00:17:55,389
you to the guys at hard for actually

457
00:17:55,389 --> 00:17:58,120
making this so so easy if you actually

458
00:17:58,120 --> 00:18:00,700
take a look open nmt itself as a project

459
00:18:00,700 --> 00:18:03,429
was actually created by the folks over

460
00:18:03,429 --> 00:18:06,220
at Harvard which is simply amazing makes

461
00:18:06,220 --> 00:18:08,769
it so so easy to train on neural machine

462
00:18:08,769 --> 00:18:11,350
translation systems with both torch and

463
00:18:11,350 --> 00:18:14,620
PI torch which surprisingly of course I

464
00:18:14,620 --> 00:18:16,360
mean I mean of course it works with PI

465
00:18:16,360 --> 00:18:18,580
torch due to the fact that it does work

466
00:18:18,580 --> 00:18:20,740
with torch Python or PI towards being a

467
00:18:20,740 --> 00:18:23,529
port of torch to the Python world of

468
00:18:23,529 --> 00:18:26,080
course really really popular relatively

469
00:18:26,080 --> 00:18:28,899
new as well and it's an amazing library

470
00:18:28,899 --> 00:18:30,700
I which I'll be adding a lot deeper into

471
00:18:30,700 --> 00:18:32,649
in future tutorials because we're going

472
00:18:32,649 --> 00:18:35,590
instead of relearning a language and

473
00:18:35,590 --> 00:18:37,779
like Lua you can just use these sorts of

474
00:18:37,779 --> 00:18:41,139
torch library in the Python language

475
00:18:41,139 --> 00:18:43,210
however now though I'm going to be

476
00:18:43,210 --> 00:18:44,889
bringing you in to actually how you can

477
00:18:44,889 --> 00:18:48,610
use open nmt on some simple data and by

478
00:18:48,610 --> 00:18:51,009
simple though I mean French to English

479
00:18:51,009 --> 00:18:53,019
in English to French shada in this case

480
00:18:53,019 --> 00:18:54,399
I'll show you adjust the English to

481
00:18:54,399 --> 00:18:56,200
French version and then I'll talk about

482
00:18:56,200 --> 00:19:00,190
why it's so so interesting now let's

483
00:19:00,190 --> 00:19:02,110
start off with the code itself how do

484
00:19:02,110 --> 00:19:04,210
you train the system it's really easy in

485
00:19:04,210 --> 00:19:06,190
fact I just created one simple shell

486
00:19:06,190 --> 00:19:08,590
file that automatically trains it for me

487
00:19:08,590 --> 00:19:10,990
first thing it does of course is in the

488
00:19:10,990 --> 00:19:12,940
shell fell in the shell file they get

489
00:19:12,940 --> 00:19:15,929
some tools it'll help it to tokenize and

490
00:19:15,929 --> 00:19:20,139
and find a new score etc in this case

491
00:19:20,139 --> 00:19:21,909
though these tools I'm only using the

492
00:19:21,909 --> 00:19:23,020
tokenizer for now

493
00:19:23,020 --> 00:19:26,140
and a few other tools you blue score is

494
00:19:26,140 --> 00:19:27,730
something that will come in a separate

495
00:19:27,730 --> 00:19:31,510
part of this series but after that once

496
00:19:31,510 --> 00:19:32,559
we've gotten the tools that will

497
00:19:32,559 --> 00:19:33,970
actually help us pre-process our data

498
00:19:33,970 --> 00:19:36,490
we've got to pre-process the data itself

499
00:19:36,490 --> 00:19:38,559
now in order to pre-process don't data

500
00:19:38,559 --> 00:19:40,630
we've got some really simple commands

501
00:19:40,630 --> 00:19:43,330
that run these Perl scripts that I

502
00:19:43,330 --> 00:19:46,360
actually just downloaded and so as you

503
00:19:46,360 --> 00:19:48,250
can see or in fact we actually wouldn't

504
00:19:48,250 --> 00:19:51,100
necessarily need these two lines of code

505
00:19:51,100 --> 00:19:52,720
here so we can remove those those aren't

506
00:19:52,720 --> 00:19:54,400
actually being used in this circumstance

507
00:19:54,400 --> 00:19:56,470
although for other German to English

508
00:19:56,470 --> 00:19:59,050
ones they will be used but we don't need

509
00:19:59,050 --> 00:20:00,910
those tools in this case so I'm only

510
00:20:00,910 --> 00:20:03,160
going to be downloading these tools here

511
00:20:03,160 --> 00:20:04,780
but after that of course as I mentioned

512
00:20:04,780 --> 00:20:06,250
getting back to the point we've got to

513
00:20:06,250 --> 00:20:08,830
pre-process our data now in order to

514
00:20:08,830 --> 00:20:10,270
pre-process data we've got a really

515
00:20:10,270 --> 00:20:12,640
simple script that's running here first

516
00:20:12,640 --> 00:20:15,010
of all it tries to actually remove the

517
00:20:15,010 --> 00:20:18,700
very first and last lines from the from

518
00:20:18,700 --> 00:20:20,679
the files themselves just in case

519
00:20:20,679 --> 00:20:23,080
there's extra extra space and then

520
00:20:23,080 --> 00:20:25,120
finally you're going to organize all the

521
00:20:25,120 --> 00:20:27,490
training data and testing data to make

522
00:20:27,490 --> 00:20:28,929
sure that the model actually understands

523
00:20:28,929 --> 00:20:32,260
the data from there though we've got a

524
00:20:32,260 --> 00:20:34,030
Python that actually really pre process

525
00:20:34,030 --> 00:20:36,460
all the data and takes all of these you

526
00:20:36,460 --> 00:20:37,750
know token ate it

527
00:20:37,750 --> 00:20:40,840
token native files and actually converts

528
00:20:40,840 --> 00:20:42,610
them to data that the neural machine

529
00:20:42,610 --> 00:20:44,470
translation system can understand and

530
00:20:44,470 --> 00:20:47,440
work with in terms of training after

531
00:20:47,440 --> 00:20:49,840
that though the actual training of a

532
00:20:49,840 --> 00:20:52,870
model comes in and it's as simple as one

533
00:20:52,870 --> 00:20:55,570
line of code that's right a simple

534
00:20:55,570 --> 00:20:58,300
Python 3.5 call to train button dot py

535
00:20:58,300 --> 00:21:00,790
pass it the data tell it where it's Oh

536
00:21:00,790 --> 00:21:02,950
save the model in this case multi 30k

537
00:21:02,950 --> 00:21:06,370
model and finally give it your GPU ID in

538
00:21:06,370 --> 00:21:08,350
this case I'm relieved one Scene one GPU

539
00:21:08,350 --> 00:21:12,910
so GPU 0 that's right it's that simple

540
00:21:12,910 --> 00:21:15,340
to train the system however you've got

541
00:21:15,340 --> 00:21:17,710
to have your data now in this case

542
00:21:17,710 --> 00:21:20,260
people with my daya slash multi 30k

543
00:21:20,260 --> 00:21:22,240
folder you can see that I've got quite a

544
00:21:22,240 --> 00:21:23,830
few different files here and if you were

545
00:21:23,830 --> 00:21:25,420
to remove the clean process versions of

546
00:21:25,420 --> 00:21:30,070
these files you can see that I got

547
00:21:30,070 --> 00:21:32,919
trained at de train de en and valid at

548
00:21:32,919 --> 00:21:35,650
the en value now the train files are

549
00:21:35,650 --> 00:21:36,659
what's going to tween

550
00:21:36,659 --> 00:21:38,220
system and the vowel files are what's

551
00:21:38,220 --> 00:21:39,869
going to validate the system to find out

552
00:21:39,869 --> 00:21:42,059
what types of accuracy were getting now

553
00:21:42,059 --> 00:21:44,249
in this case I'm using English and

554
00:21:44,249 --> 00:21:47,789
French however it is annotated as de and

555
00:21:47,789 --> 00:21:50,849
en you can do that for now let's just to

556
00:21:50,849 --> 00:21:52,889
make it compatible in general like for

557
00:21:52,889 --> 00:21:56,489
example target is de and we're coming

558
00:21:56,489 --> 00:21:58,830
from is en so you can actually paste in

559
00:21:58,830 --> 00:22:01,320
any information that you want to within

560
00:22:01,320 --> 00:22:03,179
these two files and learn eventually

561
00:22:03,179 --> 00:22:05,369
using sequence to sequence how to

562
00:22:05,369 --> 00:22:07,529
convert from one language to another in

563
00:22:07,529 --> 00:22:09,090
fact you'd apply this to really anything

564
00:22:09,090 --> 00:22:12,629
like for example IBM Watson has aid to

565
00:22:12,629 --> 00:22:14,879
speech service and what it allows you to

566
00:22:14,879 --> 00:22:18,090
do used to say like for example instead

567
00:22:18,090 --> 00:22:19,559
of just getting a text and hoping that

568
00:22:19,559 --> 00:22:21,119
it says it correctly you can actually

569
00:22:21,119 --> 00:22:24,179
give it certain expression on the text

570
00:22:24,179 --> 00:22:25,799
so what you could do is you could build

571
00:22:25,799 --> 00:22:26,970
one of these sequence the sequence

572
00:22:26,970 --> 00:22:29,190
systems to actually automatically

573
00:22:29,190 --> 00:22:32,519
convert just regular text to text with

574
00:22:32,519 --> 00:22:34,649
expression to tell Watson exactly what

575
00:22:34,649 --> 00:22:36,749
expression so Watson does the main text

576
00:22:36,749 --> 00:22:39,029
speech for you but you do the expression

577
00:22:39,029 --> 00:22:40,979
handling for the voice itself just in

578
00:22:40,979 --> 00:22:42,929
case that you want a more personalized

579
00:22:42,929 --> 00:22:44,879
or accurate version of that expression

580
00:22:44,879 --> 00:22:47,039
handling and so there are all sorts of

581
00:22:47,039 --> 00:22:48,419
things that you could possibly do with

582
00:22:48,419 --> 00:22:51,419
this um but for now all you will need to

583
00:22:51,419 --> 00:22:53,460
do is actually go back to this directory

584
00:22:53,460 --> 00:22:57,359
run back between dot SH and that's what

585
00:22:57,359 --> 00:22:59,220
you need to do now it's only gonna work

586
00:22:59,220 --> 00:23:00,749
with bash because a few specific

587
00:23:00,749 --> 00:23:03,359
commands that this shell file has all

588
00:23:03,359 --> 00:23:05,460
but try that out as you can see W gets

589
00:23:05,460 --> 00:23:07,470
gonna do some really interesting stuff

590
00:23:07,470 --> 00:23:09,450
it's gonna start dying I'm gathering

591
00:23:09,450 --> 00:23:11,940
some of the tools and at this point it's

592
00:23:11,940 --> 00:23:14,519
pre processing the data once it's done

593
00:23:14,519 --> 00:23:16,320
pre-pro is a pre processing the data

594
00:23:16,320 --> 00:23:17,849
it's going to actually step training

595
00:23:17,849 --> 00:23:20,460
model through the trained up UI file now

596
00:23:20,460 --> 00:23:22,109
instead of me actually showing you this

597
00:23:22,109 --> 00:23:24,289
model training which would I assume be

598
00:23:24,289 --> 00:23:26,759
really really repetitive and it would

599
00:23:26,759 --> 00:23:28,710
take a long time instead of me showing

600
00:23:28,710 --> 00:23:30,809
you how the model is trained let's just

601
00:23:30,809 --> 00:23:32,970
skip over to the evaluation of the model

602
00:23:32,970 --> 00:23:35,369
itself so I remember I have already

603
00:23:35,369 --> 00:23:38,940
trained this model and now we're ready

604
00:23:38,940 --> 00:23:40,889
to actually take a look at how it

605
00:23:40,889 --> 00:23:41,549
performs

606
00:23:41,549 --> 00:23:43,289
you already saw some simple

607
00:23:43,289 --> 00:23:45,359
classifications and translations are not

608
00:23:45,359 --> 00:23:47,070
classifications those translations that

609
00:23:47,070 --> 00:23:49,979
I did before with Abby's help oh but now

610
00:23:49,979 --> 00:23:50,460
I think

611
00:23:50,460 --> 00:23:52,080
show you how exactly you run those

612
00:23:52,080 --> 00:23:54,390
translations in the first place let's

613
00:23:54,390 --> 00:23:56,100
just kind of control see this script

614
00:23:56,100 --> 00:23:58,500
over here I'm let's go over to the

615
00:23:58,500 --> 00:24:00,150
folder that contains my pre trained

616
00:24:00,150 --> 00:24:02,940
model and then what I'll do is I'll get

617
00:24:02,940 --> 00:24:06,060
out this command and this command will

618
00:24:06,060 --> 00:24:07,590
allow me to actually translate from

619
00:24:07,590 --> 00:24:09,210
English to French how does it do this

620
00:24:09,210 --> 00:24:11,820
well if you take a look over here I said

621
00:24:11,820 --> 00:24:14,010
I defined a specific model for the

622
00:24:14,010 --> 00:24:16,860
program to use now every epoch it will

623
00:24:16,860 --> 00:24:19,170
actually log down a little checkpoint a

624
00:24:19,170 --> 00:24:21,360
model it tells you the accuracy

625
00:24:21,360 --> 00:24:22,800
challenge and it gives you the

626
00:24:22,800 --> 00:24:24,840
perplexity as well in this case I

627
00:24:24,840 --> 00:24:28,110
reached a perfect perplexity of 6.07 at

628
00:24:28,110 --> 00:24:31,260
the 13th epoch which is you know it's

629
00:24:31,260 --> 00:24:33,180
it's okay it's not the best you could

630
00:24:33,180 --> 00:24:34,530
get and of course you can get much much

631
00:24:34,530 --> 00:24:35,390
better than that

632
00:24:35,390 --> 00:24:39,600
however it's X is relatively low of

633
00:24:39,600 --> 00:24:41,310
course I would not use the translation

634
00:24:41,310 --> 00:24:43,620
system until it's less than say like

635
00:24:43,620 --> 00:24:47,070
eight or at least in the single digits

636
00:24:47,070 --> 00:24:49,460
is minimum to actually start using it

637
00:24:49,460 --> 00:24:52,650
six is still a little bit high if I were

638
00:24:52,650 --> 00:24:54,570
to use this my everyday life I want to

639
00:24:54,570 --> 00:24:58,140
train this even more however apart from

640
00:24:58,140 --> 00:24:59,310
that though again you're gonna tell it

641
00:24:59,310 --> 00:25:01,590
in which GP you use to translate you can

642
00:25:01,590 --> 00:25:03,840
also do this with CPU but GPU is better

643
00:25:03,840 --> 00:25:06,240
of course I'm hoping that we can we tell

644
00:25:06,240 --> 00:25:07,980
it where to translate from in this case

645
00:25:07,980 --> 00:25:10,590
a text file oh and of course we're gonna

646
00:25:10,590 --> 00:25:13,140
tell to replace unknowns and what that

647
00:25:13,140 --> 00:25:14,520
means is that if for example if there's

648
00:25:14,520 --> 00:25:17,430
an unknown input in the original text

649
00:25:17,430 --> 00:25:20,100
file date or can vary from try and find

650
00:25:20,100 --> 00:25:22,290
out using attention mechanisms where

651
00:25:22,290 --> 00:25:23,880
that would be in the output and replace

652
00:25:23,880 --> 00:25:25,410
that in the output so you have final

653
00:25:25,410 --> 00:25:27,570
translations and do it no verbose way so

654
00:25:27,570 --> 00:25:29,190
we don't need to save the output to a

655
00:25:29,190 --> 00:25:31,170
file we can just see it in the terminal

656
00:25:31,170 --> 00:25:33,330
before I run that command though I'd

657
00:25:33,330 --> 00:25:36,120
like to actually create the Committee of

658
00:25:36,120 --> 00:25:39,240
course the the sentence for us to

659
00:25:39,240 --> 00:25:41,700
translate pasal is just open up a new

660
00:25:41,700 --> 00:25:45,960
file here called say YT transpose dot

661
00:25:45,960 --> 00:25:51,500
txt now what I'm going to say here is

662
00:25:53,480 --> 00:25:55,560
[Applause]

663
00:25:55,560 --> 00:25:58,780
[Music]

664
00:26:00,530 --> 00:26:24,590
I don't I won't be sure alright so there

665
00:26:24,590 --> 00:26:27,170
we go now we've got two sentences on now

666
00:26:27,170 --> 00:26:29,900
in theory like and I manually tokenize

667
00:26:29,900 --> 00:26:31,910
this for example before and after the

668
00:26:31,910 --> 00:26:34,640
apostrophe there have to be spaces same

669
00:26:34,640 --> 00:26:36,590
thing would be period at the end and the

670
00:26:36,590 --> 00:26:39,470
commas in the middle and so as you can

671
00:26:39,470 --> 00:26:40,940
see once you've got this all tokenized

672
00:26:40,940 --> 00:26:43,940
and nice I'm going to save the file I'm

673
00:26:43,940 --> 00:26:46,970
going to run the same script Python 3.5

674
00:26:46,970 --> 00:26:49,250
or we could just copy and paste this

675
00:26:49,250 --> 00:26:52,550
command here and what I will do is pass

676
00:26:52,550 --> 00:26:56,060
it the YouTube translate txt file we're

677
00:26:56,060 --> 00:26:58,310
gonna use the same model and let's look

678
00:26:58,310 --> 00:26:58,910
for the best

679
00:26:58,910 --> 00:27:01,070
now again unfortunately I don't have any

680
00:27:01,070 --> 00:27:05,330
right now however what happens is I mean

681
00:27:05,330 --> 00:27:08,180
we've got this translation ready as you

682
00:27:08,180 --> 00:27:09,980
can see right now see this is where the

683
00:27:09,980 --> 00:27:12,920
perplexity comes in it has not been able

684
00:27:12,920 --> 00:27:16,370
to translate this sentence here why not

685
00:27:16,370 --> 00:27:18,620
sure but how can you fix it well by

686
00:27:18,620 --> 00:27:20,540
using much more data than I currently am

687
00:27:20,540 --> 00:27:22,550
because I'm using very very small data

688
00:27:22,550 --> 00:27:25,790
set and you're training for much much

689
00:27:25,790 --> 00:27:28,070
more than 24 hours even if you have a

690
00:27:28,070 --> 00:27:31,310
much stronger GPU with for example if

691
00:27:31,310 --> 00:27:33,440
you take seven ten seventy this would

692
00:27:33,440 --> 00:27:35,450
not be it's nearly fast enough to be

693
00:27:35,450 --> 00:27:38,270
finished in 24 hours so again more

694
00:27:38,270 --> 00:27:41,390
epochs better model better data and you

695
00:27:41,390 --> 00:27:42,770
would get much much higher accuracy

696
00:27:42,770 --> 00:27:45,200
however I do actually hope that this

697
00:27:45,200 --> 00:27:47,180
very first translation here was correct

698
00:27:47,180 --> 00:27:49,220
in fact if anyone here in the comments

699
00:27:49,220 --> 00:27:50,690
knows French I'd love to hear from you

700
00:27:50,690 --> 00:27:53,620
and see if this translation is correct

701
00:27:53,620 --> 00:27:55,790
alright so that's what we have to cover

702
00:27:55,790 --> 00:27:57,380
in terms of the actual implementation

703
00:27:57,380 --> 00:28:00,500
that's right it's that simple to train

704
00:28:00,500 --> 00:28:02,660
and test a neural machine translation

705
00:28:02,660 --> 00:28:05,480
system through open and NTP why in fact

706
00:28:05,480 --> 00:28:07,460
in later tutorials obviously you had

707
00:28:07,460 --> 00:28:09,620
actually build this type of system from

708
00:28:09,620 --> 00:28:12,050
scratch in terrassa that actually takes

709
00:28:12,050 --> 00:28:14,150
in its own input data out an output

710
00:28:14,150 --> 00:28:16,250
you know and does everything manually

711
00:28:16,250 --> 00:28:18,110
but for now I thought it would be more

712
00:28:18,110 --> 00:28:19,400
interesting to actually take a look at

713
00:28:19,400 --> 00:28:20,990
the concept itself and an implementation

714
00:28:20,990 --> 00:28:25,610
by folks at Harvard next now let's talk

715
00:28:25,610 --> 00:28:27,410
about the errors that the model made

716
00:28:27,410 --> 00:28:30,260
last time while I was with happy now the

717
00:28:30,260 --> 00:28:32,780
first one you may have realized here was

718
00:28:32,780 --> 00:28:36,830
the phobia one now of course it says are

719
00:28:36,830 --> 00:28:38,540
you going to the xenophobia protest on

720
00:28:38,540 --> 00:28:40,880
Sunday Emma and I may have noticed that

721
00:28:40,880 --> 00:28:42,680
it did not translate gene phone yet to

722
00:28:42,680 --> 00:28:44,900
it's French counterpart here why didn't

723
00:28:44,900 --> 00:28:47,060
do that because xenophobia was

724
00:28:47,060 --> 00:28:50,300
misspelled in fact if you see over here

725
00:28:50,300 --> 00:28:52,730
I've actually correctly spelled

726
00:28:52,730 --> 00:28:55,910
xenophobia over here in are you going to

727
00:28:55,910 --> 00:28:57,470
the xenophobia protest on Saturdays to

728
00:28:57,470 --> 00:29:00,200
Emma and what happened is a model

729
00:29:00,200 --> 00:29:03,800
actually changed the grammar not only

730
00:29:03,800 --> 00:29:06,260
did it translate xenophobia to

731
00:29:06,260 --> 00:29:09,260
xenophobia in the in the French sentence

732
00:29:09,260 --> 00:29:12,530
it change the beginning grammar of the

733
00:29:12,530 --> 00:29:15,260
sentence itself look at this we've got a

734
00:29:15,260 --> 00:29:17,900
tofu all around a read a lot over here

735
00:29:17,900 --> 00:29:19,760
like I'm sorry not parenting that

736
00:29:19,760 --> 00:29:21,860
correctly but we've got something

737
00:29:21,860 --> 00:29:25,130
entirely different over here so remember

738
00:29:25,130 --> 00:29:28,490
even that small the change the fact that

739
00:29:28,490 --> 00:29:31,220
it did not understand xenophobia made it

740
00:29:31,220 --> 00:29:33,770
then it had to infer that it's referring

741
00:29:33,770 --> 00:29:36,020
to xenophobia in a different context

742
00:29:36,020 --> 00:29:38,510
when it understood xenophobia and was

743
00:29:38,510 --> 00:29:40,310
able to translate it correctly then

744
00:29:40,310 --> 00:29:42,890
understood that context and was able to

745
00:29:42,890 --> 00:29:45,620
create the correct I was able to create

746
00:29:45,620 --> 00:29:50,080
the correct grammar before xenophobia

747
00:29:50,080 --> 00:29:52,520
but an even more interesting example

748
00:29:52,520 --> 00:29:55,010
though which really proves the context

749
00:29:55,010 --> 00:29:57,080
appendix of the context dependence is

750
00:29:57,080 --> 00:29:59,990
this one now as you can see there's an

751
00:29:59,990 --> 00:30:02,390
error here tickets will be sale at 12

752
00:30:02,390 --> 00:30:04,640
p.m. with only be for sale at 12 p.m. on

753
00:30:04,640 --> 00:30:07,280
Thursday now Thursday is not a word in

754
00:30:07,280 --> 00:30:11,140
the vocabulary but Thursday itself is

755
00:30:11,140 --> 00:30:14,990
Thursday exclamation mark isn't a word

756
00:30:14,990 --> 00:30:19,190
in the vocabulary itself that means that

757
00:30:19,190 --> 00:30:21,500
the sentence wasn't tokenized correctly

758
00:30:21,500 --> 00:30:23,960
which is why it did not translate

759
00:30:23,960 --> 00:30:25,910
Thursday to whatever it's French

760
00:30:25,910 --> 00:30:27,720
counterpart is correctly

761
00:30:27,720 --> 00:30:29,429
it didn't use correct grammar in the

762
00:30:29,429 --> 00:30:32,130
rest of the sentence however what's

763
00:30:32,130 --> 00:30:34,200
really interesting is once i tokenized

764
00:30:34,200 --> 00:30:36,210
it correctly over here and set it back

765
00:30:36,210 --> 00:30:38,370
into the model as you can see again

766
00:30:38,370 --> 00:30:40,919
almost entirely different grammar it's

767
00:30:40,919 --> 00:30:43,110
not literally putting twelve hours on

768
00:30:43,110 --> 00:30:44,220
Thursday

769
00:30:44,220 --> 00:30:45,780
instead what it's doing is it's saying

770
00:30:45,780 --> 00:30:47,370
twelve on Thursday

771
00:30:47,370 --> 00:30:51,120
it's contextually very correct which as

772
00:30:51,120 --> 00:30:53,580
I always as Abby was pointing out sir

773
00:30:53,580 --> 00:30:55,289
makes it sound as if something is

774
00:30:55,289 --> 00:30:56,789
literally on top of something and

775
00:30:56,789 --> 00:30:58,530
because the fact of the model never knew

776
00:30:58,530 --> 00:31:00,330
that Thursday wasn't referring to a day

777
00:31:00,330 --> 00:31:02,460
it thought it that was the object or

778
00:31:02,460 --> 00:31:05,400
something of that sort and it thought it

779
00:31:05,400 --> 00:31:08,909
was literally on it and it used sue so

780
00:31:08,909 --> 00:31:11,789
as you can see it is very very context

781
00:31:11,789 --> 00:31:15,030
dependent however once it gets the

782
00:31:15,030 --> 00:31:17,760
context that it needs it makes extremely

783
00:31:17,760 --> 00:31:20,789
accurate human-like natural translations

784
00:31:20,789 --> 00:31:23,580
that sound good the people who actually

785
00:31:23,580 --> 00:31:25,679
know the language and don't seem awkward

786
00:31:25,679 --> 00:31:28,740
or weird in terms of word order or

787
00:31:28,740 --> 00:31:31,799
Grammatik choice like for example if you

788
00:31:31,799 --> 00:31:33,630
have an English sentence like for

789
00:31:33,630 --> 00:31:36,059
example or you know you have an English

790
00:31:36,059 --> 00:31:37,679
sentence like for example I want a large

791
00:31:37,679 --> 00:31:40,110
pizza you wouldn't want to form that in

792
00:31:40,110 --> 00:31:43,049
French in a formal way and so that's why

793
00:31:43,049 --> 00:31:45,330
the Watson language translation service

794
00:31:45,330 --> 00:31:47,580
allows you to actually choose the

795
00:31:47,580 --> 00:31:49,679
context in which your language

796
00:31:49,679 --> 00:31:51,630
translation should be done and so

797
00:31:51,630 --> 00:31:53,250
depending on the day that you train the

798
00:31:53,250 --> 00:31:54,960
neural network wins you can actually

799
00:31:54,960 --> 00:31:58,650
choose the context or the formality in

800
00:31:58,650 --> 00:32:00,450
which this conversation is taking place

801
00:32:00,450 --> 00:32:02,549
and so for example if I'm at a pizza

802
00:32:02,549 --> 00:32:04,530
place I might not want to sound really

803
00:32:04,530 --> 00:32:07,289
formal with my friendship I'd like a

804
00:32:07,289 --> 00:32:08,730
large pizza pie I mean you don't want to

805
00:32:08,730 --> 00:32:10,770
sound really formal however if you're

806
00:32:10,770 --> 00:32:14,460
speaking to for example you know you

807
00:32:14,460 --> 00:32:16,260
making an automatic translation system

808
00:32:16,260 --> 00:32:17,940
for the UN in fact there's actually a

809
00:32:17,940 --> 00:32:21,020
data set from the UN their speeches

810
00:32:21,020 --> 00:32:23,309
English to French French to English lots

811
00:32:23,309 --> 00:32:25,350
of other languages as well which is

812
00:32:25,350 --> 00:32:26,580
something that I'm going to be working

813
00:32:26,580 --> 00:32:28,860
on in another part of this video and

814
00:32:28,860 --> 00:32:32,010
that actually holds really formal French

815
00:32:32,010 --> 00:32:34,169
well it's so depending on what you train

816
00:32:34,169 --> 00:32:36,330
the model on that's a knowledgeable game

817
00:32:36,330 --> 00:32:38,970
and that's the type of language it'll

818
00:32:38,970 --> 00:32:41,610
convert to and from and so again

819
00:32:41,610 --> 00:32:43,260
so why these models are so great you can

820
00:32:43,260 --> 00:32:45,390
personalize them to specific contacts

821
00:32:45,390 --> 00:32:47,760
you can personalize them to specific

822
00:32:47,760 --> 00:32:49,740
occasions or where you'd like to use

823
00:32:49,740 --> 00:32:52,559
them and that was what I had to go over

824
00:32:52,559 --> 00:32:55,799
in this video thank you very much for

825
00:32:55,799 --> 00:32:58,080
joining everyone I really joined making

826
00:32:58,080 --> 00:32:59,220
this video I hope you enjoyed watching

827
00:32:59,220 --> 00:33:01,170
and you'll learn something along the way

828
00:33:01,170 --> 00:33:03,090
thank you very much for joining in today

829
00:33:03,090 --> 00:33:04,950
that's my goal for this video of course

830
00:33:04,950 --> 00:33:06,990
if you liked the video and you want to

831
00:33:06,990 --> 00:33:09,059
see more of it please do consider liking

832
00:33:09,059 --> 00:33:10,950
the video share it with your friends or

833
00:33:10,950 --> 00:33:12,299
family or whoever else you think may

834
00:33:12,299 --> 00:33:14,520
benefit from it and of course from there

835
00:33:14,520 --> 00:33:15,630
if you have any more questions or

836
00:33:15,630 --> 00:33:17,400
suggestions or feedback please do leave

837
00:33:17,400 --> 00:33:18,900
it down in the comment section below you

838
00:33:18,900 --> 00:33:20,370
know it to me at any age you know that

839
00:33:20,370 --> 00:33:21,450
come where tweeted to be attached you

840
00:33:21,450 --> 00:33:21,750
Manny

841
00:33:21,750 --> 00:33:24,330
apart from that thank you very much

842
00:33:24,330 --> 00:33:25,950
everyone for joining in today if you

843
00:33:25,950 --> 00:33:27,390
really like my content you want to see

844
00:33:27,390 --> 00:33:28,740
more of it please do consider

845
00:33:28,740 --> 00:33:30,390
subscribing to my youtube channel as it

846
00:33:30,390 --> 00:33:33,360
really does help out a lot and thank you

847
00:33:33,360 --> 00:33:35,309
very much everyone for joining that's

848
00:33:35,309 --> 00:33:37,620
really to beat all of course do turn on

849
00:33:37,620 --> 00:33:38,910
those occasions if you'd like to be

850
00:33:38,910 --> 00:33:40,590
notified of whenever I release new

851
00:33:40,590 --> 00:33:45,230
content thank you very much goodbye
