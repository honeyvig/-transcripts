hello Tech Enthusiast and Welcome to our
Channel today we are diving into the
vast and Powerful realm of Big Data with
a comprehensive Hardo full course in
this digital age data is generated at an
unprecedented pace and managing
processing and analyzing this massive
volume of information requires robust
Solutions that's where Hardo comes into
play revolutionizing the way we handle
big data whether you are a beginner
eager to grasp the fundamentals or a
Season Pro looking to enhance your
skills this full course is designed to C
to all the levels of expertise so buckle
up as we embark on a knowledge pack
Journey Through the entry cases of big
data and Hado and just a quick info for
you if you want to up skill yourself
master big data and data engineering
skills and land your dream job or grow
into your career then you must explore
simply learns cohort of various big data
and data engineering programs simply
line offers various certification and
postgraduate programs in collaboration
with some of the world's leading
universities like Purdue and companies
like IBM through our courses you will
gain knowledge and work ready expertise
in skills like Apaches spark haduk
python Kafka Flume and over a dozen
others and that's not all you also get
the opportunity to work on multiple
projects and learn from industry experts
working in Top Tire product companies
and academicians from top universities
after complet these courses thousands of
Learners have transitioned into a big
data engineer or a data engineer role as
a fresher or moved on to a higher paying
job and profile if you are passionate
about making your career in this field
then make sure to check out the link in
the pin commment and description box to
find a data engineering program that
fits your experience and areas of
Interest what is Big Data big data is
extremely large or complex set of data
and it's so large that it's difficult to
process it using traditional datab base
and software
techniques every day we are creating
approximately 2.5 quintilian bytes of
data so where is this huge amount of
data getting generated
from earlier we had mobile phones with
the functionality of calling and text
messages or clicking some pictures maybe
but with the new technologies like
smartphones we have a lot of
applications for music Sports social
media like Facebook Twitter LinkedIn and
many more
also data is getting generated when we
shop online so why does it need
attention as the data is growing
companies are capturing the data that
streams into their
businesses they can apply analytics and
get significant value from it with
better speed and
efficiency companies are leveraging the
benefits of big data by analyzing the
patterns and Trends and predicting
something useful out of it for example
companies like Amazon and Netflix use
big data to improve customer
experience as we see here from the
statistics shown by 2020 1.7 megabytes
of data will be created every second for
each human this needs immediate
attention because this data can't be
just thrown away it's going to give
profit to the businesses Big Data
challenges big data is not just about
the volume of data it poses other
challenges as well like velocity and
variety as a volume 40 zettabytes of
data will be created by
2020 this huge volume of data is either
human generated like from social media
YouTube or can be machine generated like
through sensors and personal health
trackers and can also be generated with
organizations like credit card details
commercial transactions and medical
records another challenge is
velocity the speed at which data is
coming into the system the data needs to
be processed with faster
speed and then there is variety of data
data is not only structured but
unstructured and semi-structured data
like images videos and
tweets so how are Enterprises using this
big data today let us see big data
popular use cases Internet of Things
these are numerous ways in which
analytics can be applied to internet of
things for example sensors are used to
collect data that can be analyzed to
achieve actionable insights tracking
customer or product movement
Etc many Enterprises are creating a
dashboard application that provides a
360 degree view of the customers that
pulls data from a variety of sources
analyzes it and presents it to customer
service so that allows them to gather
the rich insights about
businesses Big Data popular use cases
are related information security and
data warehouse
optimizations Big Data tools are being
used to remove some of the burdens from
the data
warehouses even the healthc care
industry is looking for patterns and
treatment that lead to the best outcomes
for
patients the main challenge of big data
is storing and processing the data at a
specified time span the traditional
approach is not efficient in doing that
so Hadoop technology and various Big
Data tools have emerged to solve the
challenges faced in the Big Data
environment so there are a lot of Big
Data tools and all of them help the user
in some or another way in Saving Time
money and uncovering business
insights these can be divided into the
following cat categories like data
storage and management for example the
nosql databases such as mongod DB
Cassandra neo4j and hbase are popular N
nosql
databases the talent Hadoop Microsoft HD
inside and zookeeper are popular for
data storage and management
tools next broad category is data
cleaning data needs to be cleaned up and
well structured examples of such tools
tools which help in defining and
reshaping the data into usable data sets
are Microsoft Excel and open
refine data mining is a process of
discovery insights within a database
some of the popular tools used for data
mining are Terra dat and Rapid
minor data visualization tools are a
useful way of conveying the complex Data
Insights in a pictorial way that is easy
to understand for example Tableau and
IBM M Watson analytics and plotly are
the common tools for data reporting
powerbi tools are used data ingestion is
the process of getting the data into
Hadoop Ford which can be done using
scoop Flume or storm data analysis
requires asking questions and finding
the answers in data the popular tools
used for data analysis are Hive Pig map
rice and
Spark data acquisition is also used for
acquiring the data for which scoop Flume
or storm tools are quite popular the
popular Big Data tools offer a lot of
advantages which can be summarized as
follows they provide the analyst with
Advanced analytics algorithm and models
they help the user to run on Big Data
platforms such as Hadoop or anih high
performance analytic systems they help
the user to work not only with
structured data but unstructured and
semi-structured data coming from
multiple sources and it's quite easy to
visualize the analyze data in a form
that helps in conveying the complex Data
Insights in a pictorial way which is
easy to understand by users Big Data
tools help you to integrate with other
Technologies very easily thank you so
much for listening to the video so let's
we took a look at the the master node
and the you know the name node and the
secondary name node let's take a look at
the cluster architecture of our Hadoop
file system the hdfs cluster
architecture so we have our name node
and it stores the metadata and we have
our block location so we have our FS
image plus our edit log and then we have
the backup FS image and edit log and
then we have so you have your rack we
have our switch on top remember I was
talking about the switch that's the most
common thing to go in the rack is the
switches and underneath the the rack you
have your different data nodes you have
your data node one 2 3 four five maybe
you have uh 10 15 on this rack you can
stacking pretty high nowadays uh used to
be you'd only get about 10 servers on
there but now you see racks that contain
a lot more and then you have multiple
racks so we're not talking about just
one rack we also have you know rack two
rack three four five six and so on until
you have Rack N so if you had 100 data
nodes we would be looking at 10 racks of
10 data nodes each and that is literally
10 commodity server computers hardware
and we have a core switch which
maintains Network bandwidth and connects
the name node to the data nodes so just
like each rack has a switch that
connects all your nodes on the rack you
now have core switches that connect all
the racks together and these also
Connect into our name node setup so now
we can look up your FS image and your
edit log and pull that information your
metadata out so we've looked at the
architecture from the name node coming
down and you have your metadata your
Block locations this then sorts it out
you have your core switches which
connect everything all your different
racks and then each individual rack has
their own switches which connect all the
different nodes and to the core switches
so now let's talk about the actual data
blocks what's actually sitting on those
commodity uh machines and so the Hadoop
file system splits massive files into
small chunks these chunks are known as
data blocks each file in the Hadoop file
system is stored as a data block and we
have a nice picture here where it looks
like a Lego if you ever played with the
Legos as a kid it's a good example we
just stack that data right on top of
each other but each block has to be the
same symmetry has to be the same size so
that it can pack it easily and the
default size of one data block is
usually 128 megabytes now you can go in
and change that this standard is pretty
solid as far as most data is concerned
when we're loading up huge amounts of
data and there's certainly reasons to
change it but 128 megabytes is pretty
standard block so why 128 megabytes if
the block size is smaller then there
will be two many data blocks along with
lots of metadata which will create
overhead so that's why you don't really
want to go smaller on these data blocks
unless you have a very certain kind of
data similarly if the block size is very
large then the processing time for each
block increases then as I pointed out
earlier each block is the same size just
like your Lego blocks are all the same
but the last block can be the same size
or less so you might only be storing 100
megabytes in the last block and you can
think of this as if you had a terabyte
of data that you're storing on here it's
not going to be exactly divided into 128
megabytes we just store all of it at 128
megabytes except for the last one which
could have anywhere between 1 and 128
megabytes depending on how evenly your
data is divided now let's look into how
files are stored in the Hado file system
so we have a file text let's say it's
520 megabytes we have block a so we take
128 megabytes out of the 520 and we
store it in block a and then we have
Block B again we're taking 128 megabytes
out of our 520 storing it there and so
on Block C block D and then block e we
only have 8 megabytes left so when you
add up 100 128 plus 128 plus 128 plus
128 you only get 512 and so that last 8
megabytes goes into its own block the
final block uses only the remaining
space for storage data node failure and
replication and this is really where
Hadoop shines this is what makes it this
is why you can use it with commodity
computers this is why you can have
multiple racks and have something go
down all the data blocks are stored in
various data notes you take each block
you store it at 128 megabytes and then
we're going to put it on different nodes
so here's our block a Block B Block C
from our last example and we have node
one node two node three node four node 5
node six and so each one of these
represents a different computer it
literally splits the data up into
different machines so what happens if
node five crashes well that's a big deal
I mean we might not even have just node
five you might have a whole rack go down
and if you're a company that's building
your whole business off of that you're
going to lose a lot of money so what
does happen when node 5 crashes or the
first rack goes down the data stored in
node 5 will be unavailable as there's no
copy stored elsewhere in this particular
image so the Hado file system overcomes
the issue of data node failure by
creating copies of the data this is
known as the replication method and you
can see here we have our six nodes
here's our block a but instead of
storing it just on the first machine
we're actually going to store it on the
second and fourth noes so now it's
spread across three different computers
and in this if these are on a rack one
of these is always on a different rack
so you might have two copies on the same
rack but you never have all three on the
same rack and you always have you never
have more than two copies on one node
there's no reason to have more than one
copy per node and you can see we do the
same thing with Block B Block C is then
also spread out across the different uh
machines and same with block D and block
e node five crashes will the data blocks
b d and e be lost well in this example
no because we have backups of all three
of those on different machines the
blocks have their copies in the other
nodes due to which the data is is not
lost even if the node 5 crashes and
again because they're also stored on
different racks even if the whole rack
goes down you are still up and live with
your Hadoop file system the default
replication factor is three in total
we'll have three copies of each data
block now that can be changed for
different reasons or purposes uh but you
got to remember when you're looking at a
data center this is all in one huge room
these switches are connecting all these
servers so they can Shuffle the data
back and forth really quick and that is
very important important when you're
dealing with big data and you can see
here um each block is by default
replicated three times that's a standard
there is very rare casions to do four
and there's even fewer reasons to do two
blocks I've only seen four used once and
it was because they had two data centers
and so each data center kept two
different copies rack awareness in the
Hado file system rack is a collection of
30 to 40 data nodes rack awareness is a
concept that helps to decide where the
replica of the data block should be
stored so here we have rack one we have
our data node one to four remember I was
saying that it used to be You' only put
10 machines and now then it went to 20
now it's 30 to 40 so you can have a rack
with 40 servers on it then we have rack
two and rack three and we put block one
on there replicas of block a cannot be
in the same rack and so I'll put the
replicas onto a different rack and
notice that these are actually these two
are on the same rack but you'll never
have all three stored on the same Rack
in case the whole rack goes down and
replicas of block a are created in rack
two and they actually do they do by
default create the replicas onto the
same rack and that has to do with the
data exchange and maximizing your
processing time and then we have of
course our Block B and it's replicated
onto rack three and Block C which will
then replicate onto rack one and so on
for all of your data all the way up to
block D or whatever how much of data you
have on there hdfs architecture so let's
look over the architecture as a bigger
picture we looked at the name node and
we store some met data names replicas
home Fu data 3 so it has all your
different your metadata stored on there
and then we have our data nodes and you
can see our data nodes are each on
different racks with our different
machines and we have our name node and
you're going to see we have a heartbeat
or pulse here and a lot of times one of
the things that um confuses people
sometimes in classes they talk about
nodes versus machines so you could have
a data node that's a Hadoop data node
and you could also have a spark node on
there Spark to different architecture
and these are each Damons that are
running on these computers that's why
you refer to them as nodes and not just
always as servers and machines so even
though I use them interchangeable be
aware that these nodes you can even have
virtual machines if you're testing
something out it doesn't make sense to
have 10 virtual nodes on one machine and
deploy it because you might as well just
run your code on the machine and so we
have our heartbeat going on here and the
heartbeat is a signal that data nodes
continuously send to the name nodes this
signal shows the status of the data node
so there's a continual pulse going up
and saying hey I'm here I'm ready for
whatever instructions or data you want
to send me and you can see here we've
divided up into rack one and rack two
and our different data nodes and it also
have the replications we talked about
how to replicate data and replicates it
in three different locations and then we
have a client machine and the client
first requests the name node to read the
data now if you're not familiar with the
client machine the client is you the
programmer the client is you've logged
in external to this Hado file system and
you're sending it instructions and so
the client whatever instructions or
script you're sending is first request
the name node to read the data the name
node allows a client to read the
requested data from the data nodes the
data is read from the data nodes and
sent to the client and so you can see
here that basically the the name node
connects the client up and says here
here's a data stream and now you have
the query that you sent out returning
the data you asked for and then of
course it goes and finishes it and says
oh metadata operations and it goes in
there and finalizes your request the
other thing the name node does is you're
sending your information cu the client
sends the information in there is your
block operations so your block
operations are uh performs creation of
data so you're going to create new files
and folders you're going to delete the
folders and also it covers the
replication of the folders which goes on
in the background and so we can see here
that we have a nice full picture you can
see where the client machine comes in it
cues a metadata the Met metadata goes
into it stores the metadata and then it
goes into block operations and maybe
you're sending the data to the hdu file
system maybe you're querying maybe
you're asking it to delete if you're
sending data in there it then does the
replication on there it goes back so you
have your data client which is writing
the data into the data node and of
course replicating it and that's all
part of the block operations and so
let's talk a little bit about read
mechanisms in the Hadoop file system the
Hadoop file system read mechanism we
have our Hadoop file system client
that's you on your computer and the
client jvm on the client node so we have
our client jvm or the Java virtual
machine that is going through and then
your client node and we're zooming in on
the read mechanism so we're looking at
this picture here as you can guess your
client is reading the data and I also
have another client down here writing
data we're going to look a little closer
at that and so we have our name node up
here we have our racks of your data
nodes and your racks of computers down
here and so our client the first thing
it does is it opens a connection up with
the distributed file system to the hdfs
and it goes hey can I get the Block
locations and so it goes by using the
RPC remote procedure call it gets those
locations and the name node first checks
if the client is authorized to access
the requested file and if yes it then
provides a block location and a token to
the client which is showing to the slave
for authentication so here's a name node
it tells the client hey here's the token
the client's going to come in and get
this information from you and it tells
the client oh hey here's where the
information is so this is what is
telling your script you sent to um query
your data whether you're writing your
script in uh one of the many setups that
you have available through the Hado file
system or a connection through your code
and so you have your client machine at
this point then reads so your FS data
input stream comes through and you have
and you can see right here we did one
and two which is verify who you are and
give you all the information you need
then three you're going to read it from
the input stream and then the input
stream is going to grab it from the
different nodes where it's at and it'll
apply the tokens to those machines
saying hey this client needs this data
here's a token for that we're good to go
let me have the data the client will
show the authentication token to the
data nodes for the reprocess to begin so
after reaching the end of the data block
the connection is closed and we can see
here where we've gone through the
different steps get Block locations we
have uh step one you open up your
connection you get the Block locations
by using the RPC then you actively go
through the fs data input stream to grab
all those different data brings it back
in into the client and then once it's
done it closes down that connection and
then once the client or in this case the
programmer you know manager gone in and
pig script you can do that there's an
actual coding in each in hop we're
pulling data called pig or Hive once you
get that data back we close the
connection delete all those randomly
huge series of tokens so they can't be
used anymore and then it's done with
that query and we can go ahead and zoom
in just a little bit more here and let's
look at this even a little closer here's
our Hado file system client and our Cent
jvm our Java virtual machine on the
client node and we have the data to be
read block a Block B and so we request
to read block A and B and it goes into
the name node uh two then it sends the
location in this case um IP addresses of
the blocks for the dn1 and dn2 where
those blocks are stored then the client
interacts with the data nodes through
the switches and so you have here the
core switch so your client node comes in
three and it goes to the core switch and
that then goes to Rack switch one rack
switch two and rack switch three now now
if you're looking at this you'll
automatically see a point of failure in
the core switch and certainly you want a
high-end switch mechanism for your core
switch you want to use Enterprise
hardware for that and then when you get
to the racks that's all commodity all
your rack switches so one of those goes
down you don't care as much you just
have to get in there and swap it in and
out really quick you can see here we
have block a which is replicated three
times and so is Block B and it'll pull
from there so we come in here and here
the data is read from the dn1 and dn2 as
they are the closest to each other and
so you can see here that it's not going
to read from two different racks it's
going to read from one rack whatever the
closest setup is for that query the
reason for this is if you have 10 other
queries going on you want this one to
pull all the data through one setup and
it minimizes that traffic so the
response from the data noes to the
client that read the operation was
successful it says Ah we've read the
data we're successful which is always
good we like to be successful I don't
know about you I like to be successful
so if we looking at the read mechanism
let's go ahead and zoom in and look at
the write mechanism for the Hado file
system sry hdfs write mechanism and so
when we have the hdfs write mechanism
here's our client machine this is again
the programmer on their in computer and
it's going through the client Java
machine or Java virtual machine the jvm
and this is all occurring on the client
node uh somebody's office or maybe it's
on the local server for the office so we
have our name node we have data nodes
and we have the distributed file system
so the client first executes create file
on distributed file system says hey I'm
going to create this file over here then
it goes through the RPC call just like
our read did The Client First executes
crate file in the distributed file
system then the DFS interacts with the
name node to create a file name node
then provides a location to write the
data and so here we have our hdfs client
and the fs data output Stream So this
time instead of the data going to the
client it's coming from the client and
so here the client writes the data
through the fs data output stream keep
in mind that this output stream the
client could be a streaming code code it
could be I mean you know I always refer
to the client as just being this
computer that your programmer is writing
on it could be you have your SQL Server
there where your data is that's current
with all your current sales and it's
archiving all that information through
scoop one of the tools in the Hadoop
file system it could be streaming data
it could be a connection to the stock
servers and you're pulling stock data
down from those servers at a regular
time and that's all controlled you can
actually set that code up to be
controlled in many of the different
features in the Hadoop file system and
some of the different resources you have
that sit on top of it so here the client
writes the data through the fs data
output stream and the fs data output
stream as you can see goes into right
packet so it take divides it up into
packets 128 megabytes and the data is
written and the slave further replicates
it so here's our data coming in and then
uh if you remember correctly that's part
of the fs data setup is it tells it
where to replicate it out but the data
node itself is like oh oh hey okay I've
got the data coming in and it's also
given the tokens of where to send the
replications to and then acknowledgement
is sent after the required replicas are
made uh so then that goes back up saying
hey successful I written data made three
replications on this as far as our you
know going through the pipeline of the
data nodes and then that goes back and
it says after the data is written the
client performs the close method so the
client's done and says okay I'm done
here's the end of the data we're
finished and after the data is written
the client performs a close method and
we can see here just a quick reshape uh
we go in there just like we did with the
read we create the connection this
creates the name node which lets it know
what's going on step two step three that
also includes uh the tokens and
everything and then we go into step
three where we're now writing through
the fs data output stream and that sorts
it out into uh whatever data node it's
going to go to and which also tells it
how to replicate it so that data node
then sends it two other data nodes so we
have a replication and of course you
finalize it and close everything up Mark
Mar s it complete to the name node and
it deletes all those magical tokens in
the background so that they can't be
reused and we can go ahead and just do
this with an example the same setup so
whether you're doing a read or a write
they're very similar as we come in here
from our client and our name node and
you can see right here we actually
depicting these as the actual rack and
the switches going on and so as the data
comes in you have your request like we
saw earlier it sends a location of the
data nodes and this actually turns your
um your IP addresses your Dynamic node
connection c s they come back to your
client your hdfs client and at that
point with the tokens it then goes into
the core switch and the core switch says
Hey here it goes the client interacts
with the data nodes through the switches
and you can see here where you're
writing in block a replication of block
a and a second replication of block a on
the second server so block a gets
replicated on the second server and the
third server at this point you now have
three replications of block a and it
comes back and says it acknowledges it
says hey we're done done and that goes
back into your Hado file system to the
client who says okay we're done and
finally the um success written to the
name node and it just closes everything
down a quick recap of the Hadoop file
system and the advantages and so the
first one is uh probably one of the all
of these are huge one you have multiple
data copies are available so it's very
fault tolerant uh whole racks can go
down switches can go down even your main
name node can go down if you have a
secondary name node something we didn't
talk too much about is how scalable it
is as it uses distributed storage you
run into there you're oh my gosh I'm out
of space or I need to do some heavier
processing let me just add another rack
of computers so you can scale it up very
quickly and it's a linear scalability
where it used to be if you bought a
server you would have to pay a lot of
money to get that uh the Craig computer
remember the big Craig computers coming
out the Craig computer runs 2 million a
year just the maintenance to liquid cool
it that's very expensive compared to
just adding more racks of computer and
extending your data center so it's very
cost effective since commodity Hardware
is used we're talking cheap knockoff
computers you still need your high-end
Enterprise uh for the name node but the
rest of them literally it is a tenth of
the cost of storing data on or
traditional high-end computers and then
the data is secure so it has a very
high-end data security and provides data
security for your data hello Learners
simply brings you a post-graduate
program in data engineering developed in
partnership with University and IBM to
learn more about this course you can
find the course Link in the description
box below so all Theory and no play
doesn't make for much fun so let's go
ahead and show you what it looks like as
far as some of the dimensions when
you're getting into pulling data or
putting data into the Hado file system
and you can see here I have Oracle
virtual machine virtual box manager I
have a couple different things loaded on
there cloud is one of them so we should
probably explain some of these things if
you're new to Virtual machines the
Oracle virtual box allows you to spin up
a machine as if it is a separate
computer so in this case this is running
I believe Centos a Linux and it creates
like a box on my computer so the Centos
is running on my machine while I'm
running my Windows 10 this happens to be
a Windows 10 computer and then
underneath here I can actually go under
uh let me just open up the general might
be hard to see there and you can see I
can actually go down to system process
I happen to be on an 8 core it has 16
dedicated threads registers of 16 CPUs
but eight cores and I've only designated
this for one CPU so it's only going to
use one of my dedicated threads on my
computer and this the Oracle virtual
machine is open source you can see right
here we're on the Oracle www.oracle.com
I usually just do a search for
downloading virtual box if you search
for virtual box all one word it will
come up with this page and then you can
download it for whatever operating
system you're working with there
certainly are number of different
options and let me go and point those
out if you're setting this up as a for
demoing for yourself the first thing to
note for doing a virtual box and doing a
cloud Dera or Horton setup on that
virtual box for doing the Hadoop system
to try it out you need a minimum of 12
gigabytes it cannot be a Windows 10 home
edition because you'll have problems
with your virtual setup and sometimes
you have to go turn on the virtual
settings so it knows it's in there so if
you're on a home home setup there are
other sources there's claa and we'll
talk a little bit about claa here in
just a second but they have the claa
online live we can go try the claer
setup I've never used it but claer is a
pretty good company claa and Horton
works are two of the common ones out
there it will actually be running a
cloud daa Hadoop cluster on on our demo
here so you have Oracle virtual machine
you also have the option of doing it on
different VMware that's another one like
virtual machine this is more of a paid
service there is a free setup for just
doing it for your self which will work
fine for this and then in the cloud daa
like again they have the new online
setup where you can go in there to the
online and for cloud era you want to go
underneath the cloud era quick start if
you type in a search for cloud era quick
start it'll bring you to this website
and then you can select your platform in
this case I did virtual box there's
VMware we just talked about Docker
Docker is a very high-end virtual setup
unless you already know it you really
don't want to mess with it then your KVM
is if you're on a Linux computer that
sets up multiple system on that computer
so the two you really want to use are
usually the virtual box or do an online
setup and you can see here with the
download if you're going into the Horton
version It's called Horton works and
they call it sandbox so you'll see the
term Horton Works sandbox and these are
all test demos you're not going to
deploy a single node Hadoop systems that
would just be kind of ridiculous and
defeat the whole purpose of having a
Horton or having a Hadoop system if it's
only installed on one computer in a
virtual node so uh a lot of different
options if you're not on a professional
Windows version or you don't have at
least 12 gigabytes Ram to run this uh
you'll want to try and see if you can
find an online version and of course
simply learn has our own Labs if you
sign up for our classes we set you up I
don't know what it is now but last time
I was in there was a five node uh setup
so you can get around and see what's
going on whether you're studying for the
admin side or for the programming side
in script writing and if I go into my
Oracle virtual box and I go under my
cloud Dera and I start this up and each
one has their own flavors um Horton uses
just a log in so you log everything in
through um a local host through your
Internet Explorer or might use Chrome
Cloudera actually opens up a full
interface so you actually are in that
setup and you can see when I've uh
started it let me go back here I once I
downloaded this this is a big download
by the ways I had to import The
Appliance in Virtual box uh the first
time I ran it it takes a long time to
configure the setup and the second time
it comes up pretty quick and with the
Cloudera quick start again this is a
pretend single node it opens up and
you'll see that it actually has Firefox
here so here's my web browser I don't
have to go to Local Host I'm actually
already in the quick start for cloud era
and if we come down here you can see
getting started has some information
analyze your data manage your cluster
your general information on there and
what I always uh want to start to do is
to go ahead and open up a terminal
window so we'll open up a terminal widen
this a little bit let me just maximize
this out here so you can see so we are
now in a virtual machine this virtual
machine is cintos Linux so I'm on a
Linux computer on my Windows computer
and so when I'm on this terminal window
and this is your basic terminal if I do
list you'll see documents Eclipse these
are the different things that are
installed with the quick start guide on
the Linux system so this is a Linux
computer and then Hadoop is running on
here so now I have Hadoop single node so
it has both the name node and the data
node and everything squished together in
one virtual machine we can then do let's
do hdfs telling it that it's the Hado
file system DFS minus LS now notice the
ls is the same I have LS for list and LS
for list and I click on here and it'll
take it just a second reading the Hadoop
file system and it comes up with nothing
so a quick recap let's go back over this
three different environments I have this
one out here let's just put this in a
bright red so you can actually see it I
have this environment out here which is
my slides I have this environment here
where I did a list that's looking at the
files on the uh Linux Centos computer
and then we have this system here which
is looking at the files on the Hadoop
file system so three completely separate
environments and then we connect them
and so right now we have I have whatever
files I my personal files and the um of
course we're also looking at the screen
for for my Windows 10 and then we're
looking at the screen here uh here's our
list there that's looking at the files
and this is the screen for Centos Linux
and then this is looking at the files
right here for the Hadoop file system so
three completely separate files this one
here which is the Linux is running in a
virtual box so this is a virtual box I'm
using one core to run it or one CPU and
everything in there is has its own file
system you can see we have our desktop
and documents and whatever in there and
then you can see here we right now have
no files in our Hadoop file system and
this Hadoop file system currently is
stored on the Linux machine but it could
be stored across 10 Linux machines 20
100 this could be stored across uh in
pedabytes I mean it could be really huge
or it could just be in this case just a
demo where we're putting it on just one
computer and then once we're in here let
me just see real quick if I can go under
view zoom in view zoom in and this is
just a standard browser so I could use
any like the Control Plus and stuff like
that to zoom in and this is very common
to be in a browser window with the hudo
file system so right now I'm in a Linux
and I'm going to do oh let's just create
a file we go file my new file and I'm
going to use VI and this is a VI editor
just a basic editor and we go ahe and
type something in here um one two three
four maybe it's columns 44 66 77 of
course I do file system just like your
regular computer can also so in VI
editor you hit your colon I actually
work with a lot of different other
editors and we'll write quit VI so let's
take a look and see what happened here
I'm in my Linux system I type in LS for
list and we should see my new file and
sure enough we do over here there it is
let me just highlight that my new file
and if I then go into the doop system
hdfs DFS minus LS for list we still show
nothing it's still empty so what I can
simply do is I can go U hdf test DFS
minus put and then we're going to put uh
my new file and this is just going to
move it from the Linux system because
I'm in this file folder into the Hadoop
file system and now if we go in and we
type in our list for Hadoop file system
you will see in here that I now have
just the one file on there which is my
new file and very similar to Linux we
can do cat and the cat command simply uh
evokes reading the file uh so hdfs DFS s
minus cat and I had to look it up
remember Cloud Dera the the format is
going to be a user uh then of course the
um path location and the file name and
in here when we did the list here's our
list so you can see it lists our file
here and we realize that this is under
uh user Cloud Dera and so I can now go
user Cloud Dera my new file and the
minus cat and we'll be able to read the
file in here and you can see right here
this is the file that was in the Linux
system is now copied into the cloud era
system and it's 1 1345 that what I
entered in there and if we go back to
the Linux and du list you'll still see
it in here my new file and we can also
do something like this in our hdfs minus
MV and we'll do uh my new file and we're
going to change it to my new new file
and if we do that underneath our Hadoop
file system the minus MV will rename it
so if I go back here to our Hadoop file
system LS you'll now see instead of my
new file it has my new new file coming
up and there it is my new new file so
we've renamed it we can also go in here
and delete this so I can now come in
here so in our hdfs DFS we can also do a
remove and this will remove the file and
so if we come in here we run this we'll
see that when I come back and do the
list the file is gone and now we just
have another empty uh folder with our
hoodo file system and just like any file
system we can take this and we can go
ahead and make directory create a new
directory so MK for make directory we'll
call this my um dur so we're going to
make a directory my dur it'll take it
just a second and of course if we do um
the list command you'll see that we now
have uh the directory in there give it
just a second there it comes Myer and
just like we did before I can go in here
and um we're going to put the file and
if you remember correctly from our files
in the um setup I called it my new file
so this is coming from the Linux system
and we're going to put that into my dur
that's the Target in my Hadoop setup and
so if I hit enter on there I can now do
uh the Hadoop list and now it's not
going to show the file CU remember I put
it in a subfolder so if I do the kup
just this will show my directory and I
can do list and then I can do my dur for
my directory and you'll see underneath
of my directory in the Hadoop file
system it now has my new file put in
there and with any good uh operating
system we need a minus help so just like
you can type in help in your Linux you
can now come in here and type in hdfs
help and it shows you a lot of the
commands in there underneath the Hadoop
file system most of them should be very
similar to the Linux on here and we can
also do something like this uh Hadoop
version and the Hadoop version shows up
that we're in Hadoop
2.60 um CDH is that we're Cloud Dera 5
and compiled by Jenkins and it has a
date and all the different information
on our Hado file system so this is some
basics in the terminal window let me go
ahead and close this out cuz if you're
going to play with this you should
really come in here let me just maximize
the cloud Dera and it opens up in a
browser window and so once we're in here
again this is a browser window which you
could access uh might look like any
access for a Hadoop file system one of
the fun things to do when you're first
starting is to go under Hue you'll see
it up here at the top has Cloud Dera Hue
Hadoop your hbase your Impala your spark
these are standard installs now and Hugh
is basically an overview of the file
system and so it'll come up here and you
can see where you can do queries as far
as if you have a hbase or a hive The
Hive database we can go over here to the
top where it says file browser and if we
go under file browser now this is the
Hadoop file system we're looking at and
once we open up the file browser you can
now see there's my directory which we
created and if I click on my directory
there's my new file which is in here uh
and if I click on my new file it
actually opens it up and you can see
from our hoodo file system this is in
the hudo file system the file that we
created so we covered uh the terminal
window you can see here's a terminal
window up here it might be if you were
in a web browser it'll look a little
different because it actually opens up
as a web browser terminal window and
we've looked a little bit at Hue which
is one of the most basic components of
Hadoop one of the original components
for going through and looking at your
data and your databases of course now
they're up to uh the Hue 4 it's gone
through a number of changes and you can
see there's a lot of different choices
in here for other different tools in the
Hado file system and I'll go ahead and
just close out of this and one of the
cool things with the virtual uh box I
can either uh save the machine State
send the shutdown signal or power off
the machine I'll go and just power off
the machine completely no suppose you
have a library that has a collection of
huge number of books on each floor and
you want to count the total number of
books present on each floor what would
be your approach you could say I will do
it myself but then don't you think that
will take a lot of time and that's
obviously not an efficient way of
counting the number of books in this
huge collection on every floor by
yourself now there could be a different
approach or an alternative to that you
could think of asking three of your
friends or three of your colleagues and
you could then say if each friend could
count the books on every floor then
obviously that would make your work
faster and easier to count the books on
every floor now this is what we mean by
parallel processing So when you say
parallel processing in technical terms
you're talking about using multiple
machines and each machine would be
contributing its RAM and CPU CES for
processing and your data would be
processed on multiple machines at the
same time now this type of process
involves parall processing in our case
or in our library example where you
would have person one who would be
taking care of books on floor one and
Counting them person two on floor two
then you have someone on floor three and
someone on floor four so every
individual would be counting the books
on every floor in parallel so that
reduces the time consumed for this
activity and then there should be some
mechanism where all these Counts from
every floor can be aggregated so what is
each person doing here each person is
mapping the data of of a particular
floor or you can say each person is
doing a kind of activity or basically a
task on every floor and the task is
counting the books on every floor now
then you could have some aggregation
mechanism that could basically reduce or
summarize this total count and in terms
of map reduce we would say that's the
work of reducer so when you talk about
Hadoop map reduce it processes data on
different node machines now this is the
whole concept of Hadoop framework right
that you not only have your data stored
across machines but you would also want
to process the data locally so instead
of transferring the data from one
machine to other machine or bringing all
the data together into some central
processing unit and then processing it
you would rather have the data processed
on the machines wherever that is stored
so we know in case of Hadoop cluster we
would have our data stored on multiple
data nodes on their multiple Diss and
that is the data which needs to be
processed but the requirement is that we
want to process this data as fast as
possible and that could be achieved by
using parallel processing now in case of
map reduce we basically have the first
phase which is your mapping phase so in
case of map reduce programming model you
basically have two phases one is mapping
and one is reducing now who takes care
of things in mapping phase it is a
mapper class and this mapper class has
the function which is provided by the
developer which takes care of these
individual map tasks which will work on
multiple nodes in parallel your reducer
class belongs to the reducing phase so a
reducing phase basically uses a reducer
class which provides a function that
will Aggregate and reduce the output of
different data nodes to generate the
final output now that's how your map
reduce Works using mapping and then
obviously reducing now you could have
some other kind of jobs which are map
only only jobs wherein there is no
reducing required but we are not talking
about those we are talking about our
requirement where we would want to
process the data using mapping and
reducing especially when data is huge
when data is stored across multiple
machines and you would want to process
the data in parallel so when you talk
about map reduce you could say it's a
programming model you could say
internally it's a processing engine of
aop that allows you to process and
compute huge volumes of data and when we
say huge volumes of data we can talk
about terabytes we can talk about
petabytes exabytes and that amount of
data which needs to be processed on a
huge cluster we could also use map
reduce programming model and run a map
reduce algorithm in a local mode but
what does that mean if you would go for
a local mode it basically means it would
do all the mapping and reducing on the
same node using the processing capacity
that is RAM and CPU cores on the same
machine which is not really efficient in
fact we would want to have our map
reduce work on multiple nodes which
would obviously have mapping phase
followed by a reducing phase and
intermittently there would be data
generated there would be different other
phases which help this whole processing
so when you talk about Hadoop map reduce
you are mainly talking about two main
components or two main phases that is
mapping and reducing mapping taking care
of map tasks reducing taking care of
reduced tasks so you would have your
data which would be stored on multiple
machines now when we talk about data
data could be in different formats we
could or the developer could specify
what is the format which needs to be
used to understand the data which is
coming in that data then goes through
the mapping internally there would be
some shuffling sorting and then reducing
which gives you your final output so the
way we Access Data from hdfs or the way
our data is getting stored on hdfs we
have our input data which would have one
or multiple files and one or multiple
directories and your final output is
also stored on sdfs to be accessed to be
looked into and to see if the processing
was done correctly so this is how it
looks so you have the input data which
would then be worked upon by multiple
map tasks now how many map tasks that
basically depends on the file that
depends on the input format so normally
we know that in a Hado cluster you would
have a file which is broken down into
blocks depending on its size so the
default block size is 128 MB which can
then still be customized based on your
average size of data which is getting
stored on the cluster so if I have
really huge files which are getting
stored on the cluster I would certainly
set a higher block size so that my every
file does not have huge number of blocks
creating a load on name nodes Ram
because that's tracking the number of
elements in your cluster or number of
objects in your cluster cluster so
depending on your file size your file
would be split into multiple chunks and
for every Chunk we would have a map task
running now what is this map task doing
that is specified within the mapper
class so within the mapper class you
have the mapper function which basically
says what each of these map tasks has to
do on each of the chunks which has to be
processed this data intermittently is
written to sdfs where it is sorted and
shuffled and then you have internal
phases such as partitioner which decides
how many reduced STS would be used or
what data goes to which reducer you
could also have a combiner phase which
is like a mini reducer doing the same
reduce operation before it reaches
reduce then you have your reducing phase
which is taken care by a reducer class
and internally the reducer function
provided by developer which would have
reduced task running on the DAT data
which comes as an output from map tasks
finally your output is then generated
which is stored on hdfs now in case of
Hadoop it accepts data in different
formats your data could be in compressed
format your data could be in par your
data could be in AO text CSV tsv binary
format and all of these formats are
supported however remember if you're
talking about data being compressed then
you have to also look into what kind of
splitability the comp mechanism supports
otherwise when map reduce processing
happens it would take the complete file
as one chunk to be processed so sdfs
accepts input data in different formats
this data is stored in sdfs and that is
basically our input which is then
passing through the mapping phase now
what is mapping phase doing as I said it
reads record by record depending on the
input format it reads the data so we
have multiple map tasks running on
multiple chunks once this data is being
read this is broken down into individual
elements and when I say individual
element I could say this is my list of
key value pairs so your records based on
some kind of D limiter or without D
limiter are broken down into individual
elements and thus your maap creates key
value pairs now these key value pairs
are not my final output these key value
pairs are basically a list of elements
which will then be subjected to further
processing so you would have internally
shuffling and sorting of data so that
all the relevant key value pairs are
brought together which basically
benefits the processing and then you
have your reducing which Aggregates the
key value pairs into set of smaller Tes
or tuples as you would say finally your
output is getting stored in the
designated directory as a list of
aggregated key value pairs which gives
you your output so when we talk about
map redu use one of the key factors here
is the parallel processing which it can
offer so we know that we our data is
getting stored across multiple data
nodes and you would have huge volume of
data which is split and randomly
distributed across data noes and this is
the data which needs to be processed and
the best way would be parallel
processing so you could have your data
getting stored on multiple data nodes or
multiple slave nodes and each slave node
would have again one or multiple discs
to process this data basically we have
to go for parallel processing approach
we have to use the map reduce now let's
look at the map reduce workflow to
understand how it works so basically you
have your input data stored on hdfs now
this is the data which needs to be
processed it is stored in input files
and the processing which you want can be
done on one single file or it can be
done on a directory which has multiple
files you could also later have multiple
out outputs merged which we achieve by
using something called as chaining of
mappers so here you have your data
getting stored on his DFS now input
format is basically something to define
the input specification and how the
input files will be split so there are
various input formats now we can search
for that so we can go to Google and we
can basically search for Hadoop map
reduce Yahoo tutorial this is one of the
good links and if I I look into this
link I can search for different input
formats and output formats so let's
search for input format so when we talk
about input format you basically have
something to Define how input files are
split so input files are split up and
read based on what input format is
specified so this is a class that
provides following functionality it
selects the files or other objects that
should be used for input it defines the
input split that break a file into tasks
provides a factory for record reader
objects that read the file so there are
different formats if you look in the
table here and you can see that the text
input format is the default format which
reads lines of a text file and each line
is considered as a record here the key
is the bite offset of the line and the
value is the line content itself you can
have key value input format which passes
lines into key value pairs everything up
to the first tab character is the key
and the remainder is the line you could
also have sequence file input format
which basically works on binary format
so you have input format and in the same
way you can also search for output
format which takes care of how the data
is handled after the processing is done
so the key value pairs provided to this
output collector are then return to
Output files the way they are return is
governed by output format so it
functions pretty much like input format
as described in earlier here right so we
could set what is the output format to
be followed and again you have text
output sequence file output format null
output format and so on so these are
different classes which take care of how
your data is handled when it is being
read for processing or how is the data
being written when the processing is
done so based on the input format the
file is broken down into splits and this
logically represents the data to be
processed by individual map tasks or you
could say individual mapper functions so
you could have one or multiple splits
which need to be processed depending on
the file size depending on what
properties have been set now once this
is done you have your input splits which
are subjected to mapping phase
internally you have a record reader
which communicates with the input split
and converts the data into key value
pairs suitable to be read by mapper and
what is mapper doing it is basically
working on these key value pairs the map
task giving you an intermittent output
which would then be forwarded for
further processing now once that is done
and we have these key value pairs which
is being worked upon my map your map
tasks as a part of your mapper function
are generating your key value pairs
which are your intermediate outputs to
be processed further now you could have
as I said a combiner phas or internally
a mini reducer phase now combiner does
do not have its own class so combiner
basically uses the same class as the
reducer class provided by the developer
and its main work is to do the reducing
or its main work is to do some kind of
mini aggregation on the key value pairs
which were generated by map so once the
data is coming in from the combiner then
we have internally a partitioner phase
which decides how outputs from combiners
are sent to the reducers or you could
also say that even if I did not have a
combiner partitioner would decide based
on the keys and values based on the type
of keys how many reducers would be
required or how many reduced tasks would
be required to work on your output which
was generated by map Tusk now once
partitioner has decided that then your
data would be then sorted and shuffled
which is then fed into the reducer so
when you talk about your reducer it
would basically have one or multiple
reduce tasks now that depends on what or
what partitioner decided or determined
for your data tobay processed it can
also depend on the configuration
properties which have been set to decide
how many radio Stars should be used now
internally all this data is obviously
going through sorting and shuffling so
that your reducing or aggregation
becomes an easier task once we have this
done we basically have the reducer which
is the code for the reducer is providing
Ed by the developer and all the
intermediate data has then to be
aggregated to give you a final output
which would then be stored on sdfs and
who does this you have an internal
record writer which writes these output
key value pairs from reducer to the
output files now this is how your map
reduce Works wherein the final output
data can be not only stored but then
read or accessed from sdfs or even used
as an input for further map reduce kind
of processing so this is how it overall
looks so you basically have your data
stored on sdfs based on input format you
have the splits then you have record
reader which gives your data to the
mapping phase which is then taken care
by your mapper function and mapper
function basically means one or multiple
map tasks working on your chunks of data
you could have a combiner phas which is
optional which is not mandatory then you
have a partitioner phe which decides on
on how many reduced task or how many
reducers would be used to work on your
data internally there is sorting and
shuffling of data happening and then
basically based on your output format
your record reader will write the output
to sdfs directory now internally you
could also remember that data is being
processed locally so you would have the
output of each task which is being
worked upon stored locally however we do
not access the data directly from data
nodes we access it from sdfs so our
output is stored on sdfs so that is your
map reduce workflow when you talk about
map ruce architecture now this is how it
would look so you would have basically a
edge node or a client program or an API
which intends to process some data so it
submits the job to the job tracker or
you can say resource manager in case of
Hadoop Yan framework right now before
this step we can also say that an
interaction with name node would have
already happened which would have given
information of data nodes which have the
relevant data stored then your master
processor so in Hadoop version one we
had job tracker and then the slaves were
called task trackers in Hadoop version
two instead of job tracker you have
resource manager and instead of task
trackers you have node managers so
basically your resource manager has to
assign the job to the task trackers or
node managers so your node managers as
we discussed in Yan are basically taking
care of processing which happens on
every note so internally there is all of
this work Happening by resource manager
node managers and application master and
you can refer to the Yar based tutorial
to understand more about that so here
your processing Master is basically
breaking down the application into tasks
what it does internally is once your
application is submitted your
application to be run on Yan process
processing framework is handled by
resource manager now forget about the
Yan part as of now I mean who does the
negotiating of resources who allocates
them how does the processing happen on
the nodes right so that's all to do with
how Yan handles the processing request
so you have your data which is stored in
sdfs broken down into one or multiple
splits depending on the input format
which has been specified by the
developer your input splits are to be
worked upon by your one or multiple map
tasks which will be running within the
container on the nodes basically you
have the resources which are being
utilized so for each map task you would
have some amount of ram which will be
utilized and then further the same data
which has to go through reducing phase
that is your reduced task will also be
utilizing some RAM and CPU cores now
internally you have these functions
which take care of deciding on number of
reducers doing mini redu and basically
reading and processing the data from
multiple data nodes now this is how your
map reduce programming model makes
parallel processing work or processes
your data which is stored across
multiple machines finally you have your
output which is getting stored on S
[Music]
DFS
so let's have a quick demo on map reduce
and see how it works on a Hadoop cluster
now we have discussed briefly about map
reduce which contains mainly two phases
that is your mapping phase and your
reducing phase and mapping phase is
taken care by your mapper function and
your reducing phase is taken care by
your reducer function now in between we
also have sorting and shuffling and then
you have other phases which is
partitioner and combiner and we will
discuss about all those in detail in
later sessions but let's have a quick
demo on how we can run a map reduce
which is already existing as a package
jar file within your Apache hadu cluster
or even in your Cloudera cluster now we
can build our own map reduce programs we
can package them as jar transfer them to
the cluster and then run it on a Hadoop
cluster on Yan or we could be using
already provided default program so
let's see where they are now these are
my two machines which I have brought up
and basically this would have my Apache
Hadoop cluster running now we can just
do a Simple Start hyphen all Dosh now I
know that this script is dicated and it
says instead use start DFS and start
yarn but then it will still take care of
startic of my cluster on these two nodes
where I would have one single name note
two dat data nodes one secondary name
node one resource manager and two note
managers now if you have any doubt in
how this cluster came up you can always
look at the previous sessions where we
had a walk through in setting up a
cluster on Apache and then you could
also have your cluster running using
less than 3gb of your total machine RAM
and you could have a Apache cluster
running on your machine now once this
cluster comes up we will also have a
look at the web UI which is available
for name node and resource manager now
based on the settings what we have given
our uis will show us details of our
cluster but remember the UI is only to
browse now here my cluster has come up I
can just do a JPS to look at Java
related processes and that will show me
what are the processes which are running
on C1 which is your data node resource
manager node manager and name node and
on my M1 machine which is my second
machine which I've configured here I can
always do a a JPS and that shows me the
process is running which also means that
my cluster is up with two data nodes
with two node managers and here I can
have a look at my web UI so I can just
do a refresh and the same thing with
this one just do a refresh so I had
already opened the web pages so you can
always access the web UI using your name
noes host name and 570 Port it tells me
what is my cluster ID what is my
blockpool ID it gives you information of
what is the space usage how many live
notes you have and you can even browse
your file system so I have put in a lot
of data here I can click on browse the
file system and this basically shows me
multiple directories and these
directories have one or multiple files
which we will use for our map reduce
example now if you see here these are my
directories which have some sample files
although these files are very small like
8.7 kilobytes if you if you look into
this directory if you look into this I
have just pulled in some of my Hadoop
logs and I have put it on my hdfs these
are a little bigger files and then we
also have some other data which we can
see here and this is data which I've
downloaded from web now we can either
run a map reduce on a single file or in
a directory which contains multiple
files let's look at that before looking
a demo on map reduce also remember map
ruce will create a output directory and
we need to have that directory created
plus we need to have the permissions to
run the map ruce job so by default since
I'm running it using admin ID I should
not have any problem but then if you
intend to run map reduce with a
different user then obviously you will
have to ask the admin or you will have
to give the user permission to read and
write from sdfs so this is the directory
which I've created which will contain my
output once the map reduce job finishes
and this this is my cluster file system
if you look on this UI this shows me
about my yarn which is available for
taking care of any processing it as of
now shows that I have total of 8 GB
memory and I have 8 we course now that
can be depending on what configuration
we have set or how many noes are
available we can look at noes which are
available and that shows me I have two
node managers running each has 8 GB
memory and 8 we course now that's not
true actually but then we have not set
the configurations for node managers and
that's why it takes the default
properties that is 8 GB RAM and 8 V
course now this is my Yan UI we can also
look at Schuler which basically shows me
the different cues if they have been
configured where you will have to run
the jobs we'll discuss about all these
in later in detail now let's go back to
our terminal and let's see where we can
find some sample applications which we
can run on the cluster so once I go to
the terminal I can well submit the map
reduce job from any terminal now here I
know that my Hadoop related directories
here and within Hadoop you have various
directories we have discussed that in
binaries you have the commands which you
can run in sbin you basically have the
startup scripts and here you also notice
there is a share directory in the end if
you look in the share directory you
would find a dup and within Hadoop you
have various subd directories in which
we will look for map reduce now this map
reduce directory has some sample jar
files which we can use to run a map
reduce on the cluster similarly if you
are working on a cloud era cluster you
would have to go into opt claer parcel
cd/ lib and in that you would have
directories for sdfs map reduce or a DFS
Yan where you can still find the same
jars it is basically a pack package
which contains your multiple
applications now how do we run a map
reduce we can just type in Hadoop and
hit enter and that shows me that I have
an option called jar which can be used
to run a jar file now at any point of
time if you would want to see what are
the different classes which are
available in a particular uh jar you
could always do a jar minus xvf for
example I could say jar xvf and I could
say user local Hadoop share Hadoop map
reduce and then list down your jar file
so I'll say Hadoop map ruce examples and
if I do this this should basically
unpack it to show me what classes are
available within this particular jar and
it has done this it has created a meta
file and it has created a org directory
we can see that by doing a LS and here
if you look in LS org since I ran the
command frame home directory I can look
into or paty Hadoop examples which shows
me the classes which I have and those
classes contain which mapper or reducer
classes so it might not be just mapper
and reducer but you can always have a
look so for example I'm targeting to use
word count program which does a word
count on files and gives me a list of
words and how many times they occur in a
particular file or in set of files and
this shows me that what are the classes
which belong to word count so we have a
int sum reducer so this is my reducer
class I have tokenizer mapper that is my
mapper class right and basically this is
what is used these classes are used if
you run a word count now there are many
other programs which are part of this
jar file and we can expand and see that
so I can say Hadoop jar and give your
path so I'll say Hadoop jar user local
Hadoop share Hadoop map radi use Hadoop
map produce examples and if I hit on
enter that will show me what are the
inbuilt classes which are already
available now these are certain things
which we can use now there are other jar
files also for example I can look at dup
and here we can look at the jar files
which we have in this particular path so
this is one Hadoop map reduce examples
which you can use you can always look in
other jar files like you can look for
for Hadoop map ruce client job client
and then you can look at the test one so
that is also an interesting one so you
can always look into Hadoop map ruce
client job client and then you have
something ending with this so if I would
have tried this one using my Hadoop jar
command so in my previous example when
we did this it was showing me all the
classes which are available and that
already has a word count now there are
other good programs which you can try
like teren to generate dummy data teras
sort to check your sorting performance
and so on and Terra validate to validate
the results similarly we can also do a
Hadoop jar as I said on Hadoop map
reduce I think that was client and then
we have job client and then test jar now
this has lot of other classes which can
be used or programs which can be used
for doing a stress testing or checking
your cluster status and so on one of
them interesting one is test d fs iio
but let's not get into all the details
in first instance let's see how we can
run a map reduce now if I would want to
run a map reduce I need to give Hadoop
jar and then my jar file and if I hit on
enter it would say it needs the input
and output it needs which class you want
to run so for example I would say word
count and again if I hit on enter it
tells me that you need to give me some
input and output to process and
obviously this processing will be
happening on cluster
that is our Yan processing framework
unless and until you would want to run
this job in a local mode so there is a
possibility that you can run the job in
a local mode but let's first try how it
runs on the cluster so how do we do that
now here I can do a hdfs LS SL command
to see what I have on my hdfs now
through my UI I was already showing you
that we have set of files and
directories which we can use to process
now we can take up one single file so
for example if I pick up new data and I
can look into the files here what we
have and we can basically run a map
reduce on a single file or multiple
files so let's take this file whatever
that contains and I would like to do a
word count so that I get a list of words
and their occurrence in this file so let
me just copy this now I also need my
output to be written and that will be
written here so here if I want to run a
map reduce I can say Hadoop which we can
pull out from history so Hadoop jar word
count now I need to give my input so
that will be new data and then I will
give this file which we just copied now
I'm going to run the word count only on
a single file and I will basically have
my output which will be stored in this
directory the directory which I have
created already Mr output so let's do
this output and this is fair enough now
you can give many other properties you
can specify how many map jobs you want
to run how many reduced jobs you want to
run do you want your output to be
compressed do you want your output to be
merged or many other properties can be
defined when you are specifying word
count and then you can pass in an
argument to pass properties from the
command line which will affect your
output now once I go ahead and submit
this this is basically running a a
simple inbuilt map reduce job on our
Hadoop cluster now obviously internally
it will be looking for name node now we
have some issue here and it says the
output already exists what does that
mean so it basically means that Hadoop
will create an output for you you just
need to give a name but then you don't
need to create it so let's give let's
append the output with number one and
then let's go ahead and run this so I've
have submitted this command now this can
also be run in background if you would
want want to run multiple jobs on your
cluster at the same time so it takes
total input paths to processes one so
that is there is only one split on which
your job has to work now it will
internally try to contact your resource
manager and basically this is done so
here we can have a look and we can see
some counters here now what I also see
is for some property which it is missing
it has run the job but it has run in a
local mode mode it has run in a local
mode so although we have submitted so
this might be related to my Yan settings
and we can check that so if I do a
refresh when I have run my application
it has completed it would have created
an output but the only thing is it did
not interact with your Yan it did not
interact with your resource manager we
can check those properties and here if
we look into the job it basically tells
me that it went for mapping and reducing
it would have created an output it
worked on my file but then it ran in a
local mode it ran in a local mode so map
reduce remember is a programming model
right now if you run it on Yan you get
the facilities of running it on a
cluster where Yan takes care of resource
management if you don't run it on Yan
and run it on a local mode it will use
your machines RAM and CPU cores for
processing but then we can quickly look
at the output and then we can also try
running this on Yan so if I look into my
hdfs and if I look into my output Mr
output that's the directory which was
not used actually let's look into the
other directory which is ending with one
and that should show me the output
created by this map reduce although it
ran in a local mode it fetched an input
file from your sdfs and it would have
created output in sdfs now that's my
part file which is created and if you
look at part minus r minus these zeros
if you would have more than one reducer
running then you would have multiple
such files created we can look into this
what does this file contain which should
have my word count and here I can say
cat which basically shows me what is the
output created by my map produce let's
have a look into this so the file which
we gave for processing has been broken
down and now we have the list of words
which occur in the file plus a count of
those words so if there is some word
which is in is is more then it shows me
the count so this is a list of my words
and the count for that so this is how we
run a sample map Produce job I will also
show you how we can run it on Yan now
let's run map ruce on Yan and initially
when we tried running a map reduce it
did not hit Yan but it ran in a local
mode and that was because there was a
property which had to be changed in map
reduce hyphen site file so basically if
you look into this file the error was
that I had given a property which says
m.f framework. name and that was not the
right property name and it was ignored
and that's why it ran in a local mode so
I changed the property to map R.F
framework. name restarted my cluster and
everything should be fine now and that
map red hyphen site file has also been
copied across the noes now to run a map
redu on a Hadoop cluster so that it uses
Yan and Yan takes care of resource
allocation on one or multiple machines
so I'm just changing the output here and
now I will submit this job which should
first connect to the resource manager
and if it connects to the resource
manager that means our job will be run
using Yan on the cluster rather than in
a local mode so now we have to wait for
this application to internally connect
to resource manager and once it starts
there we can always go back to the web
UI and check if our application has
reached Yan so it shows me that there is
one input part to be processed that's my
job ID that's my application ID which
you can even monitor status from the
command line now here the job has been
submitted so let's go back here and just
do a Refresh on my Yan UI which should
show me the new application which is
submitted it tells me that it is in
accepted State application master has
already started and if you click on this
link it will also give you more details
of how many map and reduced task would
run so as of now it says the application
Master is running it would be using this
node which is M1 we can always look into
the logs we can see that there is a one
task attempt which is being made and now
if I go back to my terminal I will see
that it is waiting to get some resources
from the cluster and once it gets the
resources it will first start with the
mapping phase where the mapper function
runs it does the map tasks one or
multiple depending on the the splits so
right now we have one file and one split
so we will have just one map task
running and once the mapping phase
completes then it will get into reducing
which will finally give me my output so
we can be toggling through these
sessions so here I can just do a refresh
to see what is happening with my
application is it proceeding is it still
waiting for resource manager to allocate
some resources now just couple of
minutes back I tested this application
on Yan and we can see that my first
application completed successfully and
here we will have to give some time so
that Yan can allocate the resources now
if the resources were used by some other
application they will have to be freed
up now internally Yan takes care of all
that which we will learn more detail in
Yan or you might have already followed
the Yan based session now here we will
have to just give it some more time and
let's see if my application proceeds
with the sources what Yan can allocate
to it sometimes you can also see a
slowness in what web UI shows up and
that can be related to the amount of
memory you have allocated to your notes
now for Apache we can have less amount
of memory and we can still run the
cluster and as I said the memory which
shows up here 16 GB and 16 course is not
the true one those are the default
settings right but then my yarn should
be able to facilitate running of this
application let's just just give it a
couple of seconds and then let's look
into the output here again I had to make
some changes in the settings because our
application was not getting enough
resources and then basically I restarted
my cluster now let's submit the
application again to the cluster which
first should contact the resource
manager and then basically the map and
reduce process should start so here I've
submitted an application it is
connecting to the resource manager and
then basically it will will start
internally an app master that is
application Master it is looking for the
number of splits which is one it's
getting the application ID and it
basically then starts running the job it
also gives you a tracking URL to look at
the output and now we should go back and
look at our Yan UI if our application
shows up here and we will have to give
it a couple of seconds when it can get
the final status changed to running and
that's where my application will be
getting resources now if you closely
notice here I have allocated specific
amount of memory that is 1.5gb for node
manager on every node and I have
basically given two cores each which my
machines also have and my Yan should be
utilizing these resources rather than
going for default now the application
has started moving and we can see the
progress bar here which basically will
show what is happening and if we go back
to the terminal it will show that first
it went in deciding map and reduce it
goes for map once the mapping phase
completes then the reducing phase will
come into existence and here my job has
completed so now it has basically used
we can always look at how many map and
reduced tasks were run it shows me that
there was one map and one reduced task
now with the number of map task depends
on the number of splits and we had just
one file which is less than 128 MB so
that was one split to be processed and
reduced task is internally decided by
the reducer or depending on what kind of
property has been set in Hado config
files now it also tells me how many
input records were read which basically
means these were the number of lines in
the file it tells me output records
which gives me the number of total words
in the file now there might be
duplicates and that which is processed
by internal combiner further processing
or forwarding that information to
reducer and basically reducer works on
335 records gives us a list of words and
their count now if I do a refresh here
this would obviously show my application
is completed it says succeeded you can
always click on the application to look
for more information it tells me where
it ran now we do not have a history
server running as of now otherwise we
can always access more information so
this leads to history server where where
all your applications are stored but I
can click on this attempt tasks and this
will basically show me the history URL
or you can always look into the logs so
this is how you can submit a sample
application which is inbuilt which is
available in the jar on your Hadoop
cluster and that will utilize your
cluster to run now you could always as I
said when you are running a particular
job remember to change the output
directory and if you would not want it
to be processing is single individual
file you could also point it to a
directory that basically means it will
have multiple files and depending on the
file sizes there would be multiple
splits and according to that multiple
map tasks will be selected so if I click
on this this would submit my second
application to the cluster which should
first connect to resource manager then
resource manager has to start an
application Master now here we are
targeting 10 splits now you have to
sometimes give couple of seconds in your
machines so that the resources which
were used are internally already freed
up so that your cluster can pick it up
and then your yarn can take care of
resources so right now my application is
in undefined status but then as soon as
my Yan provides it the resources we will
have the application running on our Yan
cluster so it has already started if you
see it is going further then it would
launch 10 map tasks and it would the
number of reduced tasks would be decided
on on either the way your data is or
based on the properties which have been
set at your cluster level let's just do
a quick refresh here on my Yan UI to
show me the progress also take care that
when you are submitting your application
you need to have the output directory
mentioned however do not create it
Hadoop will create that for you now this
is how you run a map reduce without
specifying properties but then you can
specify more properties you can look
into what are the things which can be
changed for your mapper and reducer or
basically having a combiner class which
can do a mini reducing and all those
things can be done so we will learn
about that in the later sessions now we
will compare Hadoop version one that is
with map ruce version one we will
understand and learn about the
limitations of Hadoop version one what
is the need of yan what is Yan what kind
of workloads can be running on Yan what
are Yan components what is Yan
architecture and finally we will see a
demo on Yan so Hadoop version one or map
reduce version one well that's outdated
now and nobody is using Hadoop version
one but it would be good to understand
what was in Hadoop version one and what
were the limitations of Hadoop version
one which brought in the thought for the
future processing layer that is Yan now
when we talk about Hadoop we already
know that Hadoop is a framework and
Hadoop has two layers one is your
storage layer that is your sdfs Hadoop
distributed file system which allows for
distributed storage and processing which
allows fault tolerance by inbuilt
replication and which basically allows
you to store huge amount of data across
multiple commodity machines when we talk
about processing we know that map reduce
is the oldest and the most mature
processing programming model which
basically takes care of your data
processing on your distributed file
system so in Hadoop version one map
reduce performed both data processing
and resource management and that's how
it was problematic in map reduce we had
basically when we talk about the
processing layer we had the master which
was called job tracker and then you had
the slaves which were the task trackers
so your job tracker was taking care of
allocating resources it was performing
SK scheduling and even monitoring the
jobs it basically was taking care of
assigning map and reduce tasks to the
jobs running on task trackers and task
trackers which were collocated with data
nodes were responsible for processing
the jobs so task trackers were the
slaves for the processing layer which
reported their progress to the job
tracker so this is what was happening in
Hadoop version one now when we talk
about Hadoop version one we would have
say client machines or an API or an
application which basically submits the
job to the master that is job tracker
now obviously we cannot forget that
there would already be an involvement
from name node which basically tells
which are the machines or which are the
data nodes where the data is already
stored now once the job submission
happens to the job tracker job tracker
being the master demon for taking care
of your processing request and also
resource management job scheduling would
then be interacting with your multi
multiple task trackers which would be
running on multiple machines so each
machine would have a task tracker
running and that task tracker which is a
processing slave would be collocated
with the data noes now we know that in
case of doop you have the concept of
moving the processing to wherever the
data is stored rather than moving the
data to the processing layer so we would
have task trackers which would be
running on multiple machines and these
task trackers would be responsible for
handling the tasks what are these tasks
these are the application which is
broken down into smaller tasks which
would work on the data which is
respectively stored on that particular
node now these were your slave demons
right so your job tracker was not only
tracking the resources so your task
trackers were sending heartbeats they
were sending in packets and information
to the job tracker which would then be
known knowing how many resources and
when we talk about resources we are
talking about the CPU CES we are talking
about the ram which would be available
on every node so task trackers would be
sending in their resource information to
job tracker and your job tracker would
be already aware of what amount of
resources are available on a particular
node how loaded a particular node is
what kind of work could be given to the
task tracker so job tracker was taking
care of resource management and it was
also breaking the application into tasks
and doing the job scheduling part assign
different tasks to these slave demons
that is your task trackers so job
tracker was eventually overburdened
right because it was managing jobs it
was tracking the resources from multiple
task trackers and basically it was
taking care of job scheduling so job
tracker would be overburdened and in a
case if job tracker would fail then it
would affect the overall processing so
if the master is killed if the master
demon dies then the processing cannot
proceed now this was one of the
limitations of Hadoop version one so
when you talk about scalability that is
the capability to scale due to a single
job tracker scalability would be hitting
a bottl link you cannot have a cluster
size of more than 4,000 nodes and cannot
run more than 40,000 concurrent tasks
now that's just a number we could always
look into the individual resources which
each machine was having and then we can
come up with an appropriate number
however with a single job tracker there
was no horizontal scalability for the
processing layer because we had single
processing Master now when we talk about
availability job tracker as I mentioned
would be a single point of failure now
any failure kills all the queued and
running jobs and jobs would have to be
resubmitted now why would we want that
in a distributed platform in in a
cluster which has hundreds and thousands
of machines we would want a processing
layer which can handle huge amount of
processing which could be more scalable
which could be more available and could
handle different kind of workloads when
it comes to resource utilization now if
you would have a predefined number of
map and reduced slots for each Tas
tracker you would have issues which
would relate to resource utilization and
that again is putting a burden on the
master which is tracking these resources
which has to assign jobs which can run
on multiple machines in parallel so
limitations in running non-map reduce
applications now that was one more
limitation of Hadoop version one and map
reduce that the only kind of processing
you could do is map reduce and map
reduce programming model although it is
good it is oldest it has matured over a
period of time but then it is very rigid
you will have to go for mapping and
reducing approach roach and that was the
only kind of processing which could be
done in Hadoop version one so when it
comes to doing a realtime analysis or
doing ad hoc query or doing a graph
based processing or massive parallel
processing there were limitations
because that could not be done in Hadoop
version one which was having map reduce
version one as the processing component
now that brings us to the need for Yan
so Yan it stands for yet another
resource negotiator so as I mentioned
before Yan in Hadoop version one well
you could have applications which could
be written in different programming
languages but then the only kind of
processing which was possible was map
reduce we had the storage layer we had
the processing but then kind of limited
processing which could be done now this
was one thing which brought in a thought
that why shouldn't we have a processing
layer which can handle different kind of
workloads as I mentioned might be graph
processing might be realtime processing
might be massive parallel processing or
any other kind of processing which would
be a requirement of an organization now
designed to run map reduce jobs only and
having issues in scalability resource
utilization job tracking Etc that led to
the need of something what we call as
Yan now from Hadoop version two onwards
we have the two main layers have changed
a little bit you you have the storage
layer which is intact that is your hdfs
and then you have the processing layer
which is called Yan yet another resource
negotiator now we will understand how
Yan works but then Yan is taking care of
your processing layer it does support
map ruce so map ruce processing can
still be done but then now you can have
a support to other processing Frameworks
Yan can be used to solve the issues
which Hadoop version one was posing
something something like Resource
Management something like different kind
of workload processing something like
scalability resource utilization all
that is now taken care by Yan now when
we talk about Yan we can have now a
cluster size of more than 10,000 nodes
and can run more than 100,000 concurrent
tasks that's just to take care of your
scalability when you talk about
compatibility applications which were
developed for Hadoop version one which
were primarily map reduce kind of
processing can run on Yan without any
disruption or availability issues when
you talk about resource utilization
there is a mechanism which takes care of
dynamic allocation of cluster resources
and this basically improves the resource
utilization when we talk about
multi-tenancy so basically now the
cluster can handle different kind of
workloads so you can use open source and
propriety data access engines you can
perform realtime an analysis you can be
doing graph processing you can be doing
ad hoc querying and this can be
supported for multiple workloads which
can run in parallel so this is what Yan
offers so what is Yan as I mentioned Yan
stands for yet another resource
negotiator so it is the cluster Resource
Management layer for your Apache Hadoop
ecosystem which takes care of scheduling
jobs and assigning resources now just
imagine when you would want to run a
particular application
you would basically be telling the C
cluster that I would want resources to
run my applications that application
might be a map reduce application that
might be a hive query which is
triggering a map reduce that might be a
pig script which is triggering a map
reduce that could be Hive with t as an
execution Engine That Could Be a spark
application that could be a graph
processing application in any of these
cases you would still you as in in sense
client or basically an API or the
application would be requesting for
resources Yan would take care of that so
Yan would provide the desired resources
now when we talk about resources we are
mainly talking about the network related
resources we are talking about the CPU
course or as in terms of yan we say
virtual CPU course we would talk about
Ram that is in GB or MB or in terabytes
which would be offered from multiple
machines and y would take care of this
so with Yan you could basically handle
different workloads now these are some
of the workloads which are showing up
here you have the traditional map reduce
which is mainly batch oriented you could
have an interactive execution engine
something as tase you could have hbase
which is a column oriented or a
four-dimensional database and that would
be not only storing data on sdfs but
would also need some kind of processing
you could have streaming
functionalities which would be from
storm or Kafka or spark you could have
graph processing you could have inmemory
processing such as spark and its
components and you could have many
others so these are different Frameworks
which could now run and which can run on
top of yan so how does Yan do that now
when we talk about Yan this is how a
overall Yan architecture looks so at one
end you have the client now client could
be basic basically your Edge node where
you have some applications which are
running it could be an API which would
want to interact with your cluster it
could be a user triggered application
which wants to run some jobs which are
doing some processing so this client
would submit a job request now what is
resource manager doing resource manager
is the master of your processing layer
in Hado version one we basically had job
tracker and then we had task trackers
which were running on individual nodes
so your task trackers were sending your
heartbeats to the job tracker your task
trackers were sending it their resource
information and job tracker was the one
which was tracking the resources and it
was doing the job scheduling and that's
how as I mentioned earlier job tracker
was overburdened so job tracker is now
replaced by your resource manager which
is the master for your processing layer
your task trackers are replaced by no
managers which would be then running on
every node and we have a temporary demon
which you see here in blue and that's
your appm so this is what we mention
when we say yet another resource
negotiator so appm would be existing in
a Hadoop version 2 now when we talk
about your resource manager resource
manager is the master for processing
layer so it would already be receiving
heartbeats and you can say resource SCE
information from multiple node managers
which would be running on one or
multiple machines and these node
managers are not only updating their
status but they are also giving an
information of the amount of resources
they have now when we talk about
resources we should understand that if
I'm talking about this node manager then
this has been allocated some amount of
RAM for processing and some amount of
CPU cores and that is just a por
of what the complete node has so if my
node has say imagine my node has around
100 GB RAM and I have save 60 cores all
of that cannot be allocated to node
manager so node manager is just one of
the components of Hadoop ecosystem it is
the slave of the processing layer so we
could say keeping in all the aspects
such as different Services which are
running might be Cloud era or hoton
Works related Services running system
processes running on a particular node
some portion of this would be assigned
to node manager for processing so we
could say for example say 60 GB Ram per
node and say 40 CPU course so this is
what is allocated for the node manager
on every machine similarly we would have
here similarly we would have here so
node manager is constantly giving an
update to resource manager about the
sources what it has probably there might
be some other applications running and
node manager is already occupied so it
gives an update now we also have a
concept of containers which is basically
we will we will talk about which is
about these resources being broken down
into smaller parts so resource manager
is keeping a track of the resources
which every node manager has and it is
also responsible for taking care of the
job request how do these things happen
now as we see here resource manager at a
higher level you can always say this is
the processing Master which does
everything but in reality it is not the
resource manager which is doing it but
it has internally different services or
components which are helping it to do
what it is supposed to do now let's look
further now as I mentioned your resource
manager has these services or components
which basically helps it to do the
things it is basically a an architecture
where multiple components are working
together to achieve what Yan allows so
resource manager has mainly two
components that is your scheduler and
applications manager and these are at
high level four main components here so
we talk about resource manager which is
the processing Master you have node
managers which are the processing slaves
which are running on every nodes you
have the concept of container and you
have the concept of application Master
how do all these things work now let's
look at Yan components so resource
manager basically has two main
components you can say which assist
resource manager in doing what it is
capable of so you have scheduler and
applications manager now there is when
you talk about resources there is always
a requirement for the applications which
need to run on cluster of resources so
your application which has to run which
was submitted by client needs resources
and these resources are coming in from
multiple machines wherever the relevant
data is stored and a node manager is
running so we always know that node
manager is collocated with data nodes
now what does the Schuler do so we have
different kind of schedulers here we
have basically a capacity scheduler we
have a fair Schuler or we could have a
fifo scheduler so that there are
different schedulers which take care of
resource allocation so your Schuler is
responsible for allocating resources to
various running applications now imagine
a particular environment where you have
different teams or different departments
which are working on the same cluster so
we would call the cluster as a
multi-tenant cluster and on the
multi-tenant cluster you would have
different applications which would want
to run simultaneously accessing the
resources of the cluster how is that
managed so there has to be some
component which has a concept of pooling
or queuing so that different departments
or different users can get dedicated
resources or can share resources on the
cluster so scheduler is responsible for
allocating resources to various running
applications now it does not perform
monitoring or tracking of the status of
applications that's not the part of
Schuler it does not offer any Gap
guarantee about restarting the failed
tasks due to Hardware or network or any
other failures scheduler is mainly
responsible for allocating resources now
as I mentioned you could have different
kind of schedulers you could have a fifo
Schuler which was mainly in older
version of Hadoop which stands for first
in first out you could have a fair
Schuler which basically means multiple
applications could be running in the
cluster and they would have a fair fair
share of the resources you could have a
capacity scheduler which would basically
have dedicated or fixed amount of
resources across the cluster now
whichever Schuler is being used
scheduler is mainly responsible for
allocating resources then it's your
applications manager now this is
responsible for accepting job
submissions now as I said at higher
level we could always say resource
manager State doing everything it is
allocating the resources it is
negotiating the resources
it is also taking care of listening to
the clients and taking care of job
submissions but who is doing it in real
it is these components so you have
applications manager which is
responsible for accepting job
submissions it negotiates the first
container for executing the application
specific application Master it provides
the service for restarting the
application Master now how does this
work how do these things happen in
coordination now as I said your node
manager is the slave process which would
be running on every machine slave is
tracking the resources what it has it is
tracking the processes it is taking care
of running the jobs and basically it is
tracking each container resource
utilization so let's understand what is
this container so normally when you talk
about a application request which comes
from a client so let's say this is my
client which is requesting thing or
which is coming up with an application
which needs to run on the cluster now
this application could be anything it
first contacts your master that's your
resource manager which is the master for
your processing layer now as I mentioned
and as we already know that your name
node which is the master of your cluster
has the metadata in its Ram which is
aware of the data being split into
blocks the blocks when stored on
multiple machines and other information
so obviously there was a interaction
with the master which has given this
information of the relevant nodes where
the data exist now for the processing
need your client basically the
application which needs to run on the
cluster so your resource manager which
basically has the scheduler which takes
care of allocating resources and a
resource manager has mainly these two
components which are helping it to do
its work now for a particular
application which might be needing data
from multiple machines now we know that
we would have multiple machines where we
would have node manager running we would
have a data node running and data nodes
are responsible for storing the data on
disk so your resource manager has to
negotiate the resources now when I say
negotiating the resources it could
basically ask each of these node
managers for some amount of resources
for example it would be saying can I
have 1 GB of RAM and one CPU core from
you because there is some data residing
on your machine and that needs to be
processed as part of my application can
I again have 1 GB and one CPU core from
you and this is again because some
relevant data is stored and this request
which resource manager makes of holding
the resources of total resources which
the node manager has your resource
manager is negotiating or is asking for
resources from the processing slave so
this request of holding resources can be
considered as a container so resource
manager now we know it is not actually
the resource manager but it is the
application manager which is negotiating
the resources so it negotiates the
resources which are called container so
this this request of holding resource
can be considered as a container so
basically a container can be of
different sizes we will talk about that
so resource manager negotiates the
resources with node manager now node
manager which is already giving an
update of the resources it has what
amount of resources it holds how much
busy it is can basically approve or deny
this request so node manager would
basically approve in saying yes I could
hold these resources I could give you
this container of this particular size
now once the container has been approved
or allocated or you can say granted by
your node manager resource manager now
knows that resources to process the
application are available and guaranteed
by the node manager so resource manager
starts a temporary demon called
appmaster so this is a piece of code
which would also be running in one of
the containers it would be running in
one of the containers which would then
take care of execution of tasks in other
containers so your application Master is
per application so if I would have 10
different applications coming in from
the client then we would have 10 app
Masters one appm being responsible for
per application now what does this appm
do it basically is a piece of code which
is respons responsible for execution of
the application so your appm would run
in one of the containers and then it
would use the other containers which
node manager has guaranteed that it will
give when the request application
request comes to it and using these
containers the appm will run the
processing tasks Within These designated
resources so it is mainly the
responsibility of application Master to
get the execution done and then
communicate it to the master so resource
manager is tracking the resources it is
negotiating the resources and once the
resources have been negotiated it
basically gives the control to
application Master application Master is
then running within one of the
containers on one of the nodes and using
the other containers to take care of
execution this is how it looks so
basically container as I said is a
collection of resources like CPU memory
your dis which would be used or which
already has the data and network so your
node manager is basically looking into
the request from application master and
it basically is granting this request or
basically is allocating these containers
now again we could have different sizing
of the containers let's take an example
here so as I mentioned from the total
resources which are available for a
particular node some portion of
resources are allocated to the node
manager so let's imagine this is my node
where node manager as a processing slave
is running so from the total resources
which the node has some portion of RAM
and CPU cores is basically allocated to
the node manager so I could say out of
total 100 GB Ram we can say around 60
cores which the particular node has
so this is my Ram which the node has and
these are the CPU cores which the node
has some portion of it right so we can
say might be 70% or 60% of the total
resources so we could say around 60 GB
RAM and then we could say around 40 V
course have been allocated to node
manager so there are these settings
which are given in the Yan hyphen site
file now apart from this allocation that
is 60 GB RAM and 40 V course we also
have some properties which say what will
be the container sizes so for example we
could have a small container setting
which could say my every container could
have 2 GB RAM and say one virtual CPU
core so this is my smallest container So
based on the total resources you could
calc calculate how many such small
containers could be running so if I say
2 GB Ram then I could have around 30
containers but then I'm talking about
one virtual CPU core so totally I could
have around 30 small containers which
could be running in parallel on a
particular node and as of that
calculation you would say 10 CPU CES are
not being utilized you could have a
bigger container size which could say I
would go for 2 CPU cores and 3 GB Ram so
3 GB RAM and 2 CPU cores so that would
give me around 20 containers of bigger
size so this is the container sizing
which is again defined in the Yan hyphen
site file so what we know is on a
particular node which has this kind of
allocation either we could have 30 small
containers running or we could have 20
big containers running and same would
apply to multiple nodes so node manager
based on the request from application
Master can allocate these containers now
remember it is within this one of these
containers you would have an application
Master running and other containers
could be used for your processing
requirement application Master which is
per application it is the one which uses
these resources it basically manages or
uses these resources for individual
application so remember if we have 10
applications running on Yan then it
would be 10 application Masters one
responsible for each application your
application Master is the one which also
interacts with the scheduler to
basically know how much amount of
resources could be allocated for one
application and your application Master
is the one which uses these resources
but it can never negotiate for more
resources to node Manager application
Master cannot do that application master
has to always go back to resource
manager if it needs more resources so it
is always the resource manager and
internally resource manager component
that is application manager which
negotiates the resources at any point of
time due to some node failures or due to
any other requirements if application
Master needs more resources on one or
multiple notes it will always be
contacting the resource manager
internally the applications manager for
more containers now this is how it looks
so your client submits the job request
to resource manager now we know that
resource manager internally has
scheduler and applications manager node
managers which are running on multiple
machines are the ones which are tracking
their resources giving this information
to resource manager so that resource
manager or I would say its component
applications manager could request
resources from multiple node managers
when I say request resources it it is
these containers so your resource
manager basically will request for the
resources on one or multiple nodes node
manager is the one which approves these
containers and once the container has
been approved your resource manager
triggers a piece of code that is
application Master which obviously needs
some resources so it would run in one of
the containers and we use other
containers to do the execution so your
CL submits an application to resource
manager resource manager allocates a
container or I would say this is at a
high level right resource manager is
negotiating the resources and internally
who is negotiating the resources it is
your applications manager who is
granting this request it is node manager
and that's how we can say resource
manager allocates a container
application Master basically contacts
the related node manager because it
needs to use the containers node manager
is the one which launches the container
or basically gives those resources
within which an application can run and
application Master itself will then
accommodate itself in one of the
containers and then use other containers
for the processing and it is within
these containers the actual execution
happens now that could be a map task
that could be a reduced task that could
be a spark executor taking care of spark
tasks and many other
processing
so before we look into the demo on how
Yan works I would suggest looking into
one of the blogs from cloud era so you
can just look for Yan untangling and
this is really a good blog which
basically talks about the overall
functionality which I explained just now
so as we mentioned here so you basically
have the master process you have the
worker process which basically takes
care of your processing your resource
manager being the master and node
manager being the slave this also talks
about the resources which each node
manager has it talks about the Yan
configuration file where you give all
these properties it basically shows you
node manager which reports the amount of
resources it has to resource manager now
remember if if worker node shows 8 into
to8 CPU cores and 128 GB RAM and if your
node manager says 64 V course and RAM
128 GB then that's not the total
capacity of your node it is some portion
of your node which is allocated to node
manager now once your node manager
reports that your resource manager is
requesting for containers based on the
application what is a container it is
basically a logical name given to a
combination of voree and RAM it is
within this container where you would
have basically the process running so
once your application starts and once
node manager is guaranteed these
containers your application or your
resource manager has basically already
started an application Master within the
container and what does that application
Master do it uses the other containers
where the tasks would run so this is a
very good blog which you can refer to
and this also talks about map reduce if
you have already followed the map reduce
tutorials in past then you would know
about the different kind of tasks that
is map and reduce and these map and
reduce tasks could be running within the
container in one or multiple as I said
it could be map task it could be reduced
task it could be a spark based task
which would be running within the
container now once the task finishes
basically that resources can be freed up
so the container is released and the
resources are given back to Yan so that
it can take care of further processing
if you'll further look in this blog you
can also look into the part two of it
where you talk mainly about
configuration settings you can always
look into this which talks about why and
how much resources are allocated to the
node manager it basically talks about
your operating system overhead it talks
about other services it talks about
Cloud era or hoton Works related
Services running and other processes
which might be running and based on that
some portion of RAM and CPU course would
be allocated to node manager so that's
how it would be done in the Yan hyphen
site file and this basically shows you
what is the total amount of memory and
CPU course which is allocated to node
manager then within every machine where
you have a node manager running on every
machine in the Yan hyphen site file you
would have such properties which would
say what is the minimum container size
what is the maximum container size in
terms of ram what is the minimum for CPU
course what is the maximum for CPU
course and what is the incremental size
in where RAM and CPU course can
increment so these are some of the
properties which Define how containers
are allocated for your application
request so have a look at this and this
could be uh a good information which
talks about different properties now you
can look further which talks about
scheduling if you look in this
particular blog which also talks about
scheduling where it talks about
scheduling in Yan which talks about pair
scheduler or you basically having
different cues in which allocations can
be done you also have different ways in
which cues can be managed and different
schedulers can be used so you can always
look at this series of blog you can also
be checking for Yan schedulers and then
search for uh dup definitive guide and
that could give you some information on
how it looks when you look for Hado
definitive guide so if you look into
this book which talks about the
different resources as I mentioned so
you could have a fee for scheduler that
is first in first out which basically
means if a long running application is
submitted to the cluster all other small
running applications will have to wait
there is no other way but that would not
be a preferred option if you look in
fifo scheduler if you look for capacity
scheduler it basically means that you
could have different cues created and
those cues would have sources allocated
so then you could have a production
queue where production jobs are running
in a particular queue which has fixed
amount of resources allocated you could
have a development queue where
development jobs are running and both of
them are running in parallel you could
then also look into Fair Schuler which
basically means again multiple
applications could be running on the
cluster however they would have a fair
share so when I say fair share in brief
what it means is if I had given 50% of
resources to a q for production and 50%
of resources for a q of development and
if both of them are running in parallel
then they would have access to 50% of
cluster resources however if one of the
Q is unutilized then second Q can
utilize all cluster resources so look
into the fair scheduling part it also
shows you about how allocations can be
given and you can learn more about
schedule ERS and how cues can be used
for managing multiple applications now
we will spend some time in looking into
few ways or few quick ways in
interacting with Yan in the form of a
demo to understand and learn on how Yan
works we can look into a particular
cluster now here we have a designated
cluster which can be used you could be
using the similar kind of commands on
your Apache based cluster or a Cloud era
quick start VM if you already have or if
you have a cloud era or a hoton Works
cluster running there are different ways
in which we can interact with Yan and we
can look at the information one is
basically looking into the admin console
so if I would look into Cloud era
manager which is basically an admin
console for a cloud era's distribution
of a dupe similarly you could have a hot
enworks cluster than access to the admin
console so if you have even read access
for your your cluster and if you have
the admin console then you can search
for Yan as a service which is running
you can click on Yan as a service and
that gives you different tabs so you
have the instances which tells basically
what are the different roles for your
Yan service running so we have here
multiple node managers now some of them
show in stop status but that's nothing
to worry so we have three and six node
managers we have resource manager which
is one but then that can also be in a
high availability where you can have
active and standby you also have a job
history server which would show you the
applications once they have completed
now you can look at the Yan
configurations and as I was explaining
you can always look for the properties
which are related to the allocation so
you can here search for course and that
should show you the properties which
talk about the allocations so here if we
see we can be looking for Yan app map
reduce application Master resource CPU
course what is the CPU course allocated
for map reduce map task reduce task you
can be looking at Yan node manager
resource CPU course which basically says
every node manager on every node would
be allocated with six CPU cores and the
container sizing is with minimum
allocation of one CPU core and the
maximum could be 2 CPU view course
similarly you could also be searching
for memory allocation and here you could
then scroll down to see what kind of
memory allocation has been done for the
node manager so if we look further it
should give me information of node
manager which basically says here that
the container minimum allocation is 2 GB
the maximum is 3 GB and we can look at
node manager which has been given 25 GB
per node so it's a combination of this
memory and CPU course which is the total
amount of resources which have been
allocated to every node manager now we
can always look into applications tab
that would show us different
applications which are submitted on Yan
for example right now we see there is a
spark application running which is
basically a user who is using spark
shell which has triggered a application
on spark and that is running on y done
you can look at different applications
workload information you can always do a
search based on the number of days how
many applications have run and so on you
can always go to the web UI and you can
be searching for the resource manager
web UI and if you have access to that it
will give you overall information of
your cluster so this basically says that
here we have 100 GB memory allocated so
that could be say 25 GB per node and if
we have four node managers running and
we have 24 cores which is six cores per
node if we look further here into nodes
I could get more information so this
tells me that I have four node managers
running and node managers basically have
25 GB memory allocated per node and six
scores out of which some portion is
being utilized we can always look at the
Schuler here which can give us
information what kind of Schuler has
been allocated so we basically see that
there is just a root que and within root
you have default q and you have
basically users SK based on different
users we can always scroll here and that
can give us information if it is a fair
share so here we see that my root.
default has 50% of resources and the
other queue also has 50% of resources
which also gives me an idea that a fair
scheduler is being used we can always
confirm that if we are using using a
fair Schuler or a capacity Schuler which
takes care of allocations so search for
scheduler and that should give you some
understanding of what kind of Schuler is
being used and what are the allocations
given for that particular Schuler so
here we have Fair Schuler it shows me
you have under root you have the root Q
which has been given 100% capacity and
then you have within that default which
also takes 100% so this is how you can
understand about Yan by looking into the
Yan web UI you can be looking into the
configurations you can look at
applications you can always look at
different actions now since we do not
have admin access the only information
we have is to download the client
configuration we can always look at the
history server which can give us
information of all the applications
which have successfully completed now
this is from your Yan UI what I can also
do is I can be going into Hue which is
the web interface and your web interface
also basically allows you to look into
the jobs so you can click on H web UI
and if you have access to that it should
show up or you should have a way to get
to your Hue which is a graphical user
interface mainly comes with your Cloud
era you can also configure that with
Apache hoton Works has a different way
of giving you the web UI access you can
click and get into Hue and that is also
one way where you can look at Yan you
can look look at the jobs which are
running if there are some issues with it
and these these are your web interfaces
so either you look from Yan web UI or
here in Hue you have something called as
job browser which can also give you
information of your different
applications which might have run so
here I can just remove this one which
would basically give me a list of all
the different kind of jobs or workflows
which were run so either whether it was
a spark based application or it was a
map reduce or it was coming from hive so
here I have list of all the applications
and it says this was a map reduce this
was a spark something was killed
something was successful and this was
basically a probably a hive query which
triggered a map reduce job you can click
on the application and that tells you
how many tasks were run for it so there
was a map task which ran for it you can
get into into the metadata information
which you can obviously you can also
look from the Yan UI to look into your
applications which can give you a
detailed information of if it was a map
reduce how many map and reduce tasks
were run what were the different
counters if it was a spark application
it can let you follow through spark
history server or job history server so
you can always use the web UI to look
into the jobs you can be finding in lot
of useful information here you can also
be looking at how many resources were
used and what happened to the job was it
successful did it fail and what was the
job status now apart from web UI which
always you might not have access to so
in a particular cluster in a production
cluster there might be restrictions and
the organization might not have uh
access given to all the users to
graphical user interface like you or
might be you would not have access to
the cloud era manager or admin console
because probably organization is
managing multiple clusters using this
admin console so the one way which you
would have access is your web console or
basically your Edge node or client
machine from where you can connect to
the cluster and then you can be working
so let's log in here and now here we can
give different commands so this is the
command line from where you can have
access to different details you can
always check by just typing in map red
which gives you different options where
you can look at the map reduce related
jobs you can look at different cues if
there are cues configured you can look
at the history server or you can also be
doing some admin stuff provided you have
access so for example if I just say map
R and Q here this basically gives me an
option says what would you want to do
would you want to list all the cues do
you want information on a particular
queue so let's try a list and that
should give you different cues which
which were being used now here we know
that per user a que dynamically gets
created which is under root. users and
that gives me what is the status of the
queue what is the capacity has there
been any kind of maximum capacity or
capping done so we get to see a huge
list of cues which dynamically get
configured in this environment and then
you also look at your root. default I
could have also picked up one particular
Quee and I could have said show me the
jobs so I could do that now here we can
also give a Yan command so let me just
clear the screen and I will say Yan and
that shows me different options so apart
from your web interface something like
web UI apart from your Yan's web UI you
could also be looking for information
using Yan commands here so these are
some list of commands which we can check
now you can just type in Yan and version
if you would want to see the version
which basically gives you information of
what is the Hadoop version being used
and what is the vendor specific
distribution version so here we see we
are working on cloud eras distribution
5.14 which is internally using Hadoop
2.6 now similarly you can be doing a
Yann application list so if you give
this that could be an exhaustive list of
all the applications which are running
or applications which have completed so
here we don't see any applications
because right now probably there are no
applications which are running it also
shows you you could be pulling out
different status such as submitted
accepted or running now you could also
say I would want to see the services
that I finished running so I could say
Yan application list and app States as
finished so here we could be using our
Command so I could say Yan application
list and then then I would want to see
the app states which gives me the
applications which have finished and we
would want to list all the applications
which finished now that might be
applications which succeeded right and
there is a huge list of application
which is coming in from the history
server which is basically showing you
the huge list of applications which have
completed so this is one way and then
you could also be searching for one
particular application if you would want
to search a particular application if
you have the application ID you could
always be doing a grip that's a simple
way I could say basically let's pick up
this one and if I would want to search
for this if I would want more details on
this I could obviously do that by
calling in my previous command and you
could do a GP if that's what you want to
do and if you would want to search is
there any application which is in the
list of my applications that shows my
application I could pull out more
information about my application so I
could look at the log files for a
particular application by giving the
application ID so I could say Yan logs
now that's an option and every time
anytime you have a doubt just hit enter
it will always give you options what you
need to give with a particular command
so I can say Yan logs
application ID now we copied an
application ID and we could just give it
here we could give other options like
app owner or if you would want to get
into the contain details or if you would
want to check on a particular node now
here I'm giving Yan logs and then I'm
pointing it to an application ID and it
says the log aggregation has not
completed might be this was might be
this was an application which was
triggered based on a particular
interactive cell or based on a
particular query so there is no log
existing for this particular application
you can always look at the status of an
application you can kill an application
so here you can be saying yan yan
application and then what would you want
to do with an application hit and enter
it shows you the different options so we
just tried app States you could always
look at the last one which says status
and then for my status I could be giving
my application ID so that tells me what
is the status of this application it
connects to the resource manager it
tells me what's the application ID what
kind of application it was ran it which
was the queue where the job was running
what was the start and end time what is
the progress the status of it if it is
finished or if it has succeeded and then
it basically gives me also an
information of where the application
master was running it gives me the
information where you can find this job
details in history server if you are
interested in looking into it also gives
you a aggregate resource allocation
which tells how much GB memory and how
many C cores seconds it used so this is
basically looking out at the application
details now I could kill an application
if the application was already running I
could always do a Yan application minus
skill and then I could be giving my
application now I could try killing this
however it would say the application is
already finished if I had an application
running and if my application was
already given an application ID by The
Source manager I Could Just Kill it I
can also say Yan node list which would
give me a list of the node managers now
this is what we were looking from the
Yan web UI and we were pulling out the
information so we can get this and kind
of information from your command line
always remember and always try to be
well accustomed with the command line so
you can do various things from the
command line and then obviously you have
the web UI which can help you with a
graphical interface easily able to
access things now you could be also
starting the resource manager which we
would not be doing here because we are
already running in a cluster so you
could give a Yan resource manager you
could get the logs of resource manager
if you would want by giving Yan demon so
we can try that so you can say Yan and
then demon so it says it does not find
the demon so you can give something like
this get level and here I will have to
give the node and the IP address where
you want to check the logs of resource
manager so you could be giving this for
which we will have to then get into
cloud data manager to look into the
nodes and the IP address you could be
giving a command something like this
which basically gives you the level of
the log which you have and I got this
resource manager address from the web UI
now I can be giving in this command to
look into the demon log and it basically
says you would want to look at the
resource manager related log and you
have the log 4J which is being used for
logging the kind of level which has been
set as info which can again be changed
in the way you're logging the
information now you can try any other
commands also from Yan for example
looking at the Yan RM admin so you can
always do a Yan RM admin and this
basically gives you a lot of other
informations like refreshing the cues or
refreshing the noes or basically looking
at the admin a CLS or getting groups so
you could always get group names for a
particular user now we could search for
a particular user such as Yan or hdfs
itself so I could just say here I would
want get groups and then I could be
searching for say username hdfs so that
tells me hdfs belongs to a Hadoop group
similarly you could search for say maed
or you could search for Yan so these are
service related users which
automatically get created and you can
pull out information related to these
you can always do a refresh nodes kind
of command and that is mainly done
internally this can be useful when
you're doing commissioning
decommissioning but then in case of
cloud era or hoton Works kind of cluster
you would not be manually giving this
command because if you are doing a
commissioning decommissioning from an
admin console and if you are an
administrator then you could just
restart the services which are affected
and that will take care of this but if
you were working in an Apache cluster
and if you were doing commissioning
decommissioning then you would be using
in two commands refresh nodes and
basically that's for refreshing the
nodes which should not be used for
processing and similarly you could have
a command refresh notes which comes with
hdfs so these are different options
which you can use with your Yann on the
command line you could also be using
curl commands to get more information
about your cluster by giving curl minus
X and then basically pointing out to
your resource manager web UI address now
here I would like to print out the
cluster related metrics and I could just
simply do this which basically gives me
a high level information of how many
applications were submitted how many are
pending what is the reserved resources
what is the available amount of memory
or CPU course and all the information
similarly you can be using the same curl
commands to get more information like
Schuler information so you would just
replace the metrics with Schuler and you
could get the information of the
different cues now that's a huge list we
can cancel this and that would give me
the list of all the cues which are
allocated and what are the resources
allocated for each queue you could also
get cluster information on application
IDs and Status running of applications
running in Yan so you would have to
replace the last bit of it and you would
say I would want to look at the
applications and that gives me a huge
list of applications then you can do a
grip and you can be filtering out
specific application related information
similarly you can be looking at the
notes so you can always be looking at
note specific information which gives
you how many notes you have have but
this could be mainly used when you have
an application which wants to or a web
application which wants to use a curl
command and would want to get
information about your cluster from an
HTTP interface now when it comes to
application we can basically try running
a simple or a sample map reduce job
which could then be triggered on Yan and
it would use the resources now I can
look at my application here and I can be
looking into my specific directory which
is this one which should have lot of
files and directories which we have here
now I could pick up one of these and I
could be using a simple example to do
some processing let's take up this file
so there is a file and I could run a
simple word count or I could be running
a hive query which triggers a map reduce
job I could even run a spark application
which would then show that the
application is running on the cluster so
for example if I would say spark to
Shell now I know that this is an
interactive way of working with spark
but this internally triggers a spark
submit and this runs an application so
here when you do a spark to Shell by
default it will contact Yan so it gets
an application ID it is running on Yan
with the master being Yan and now I have
access to the interactive way of working
with spark now if I go and look into
applications I should be able to see my
my application which has been started
here and it shows up here so this is my
application
3827 which has been started on Yan and
as of now we can also look into the Yan
UI and that shows me the application
which has been started which basically
has one running container which has one
CPU core allocated 2GB RAM and it's in
progress although we are not doing
anything there so we can always look at
our applications from the Yan UI or as I
mentioned from your applications tab
within Yan Services which gives us the
information and you can even click on
this application to follow and see more
information but you should be given
access to that now this is just a simple
application which I triggered using
spark shell similarly I can basically be
running a map reduce now to run a map
reduce I can say Hadoop jar and that
basically needs a class so we can look
for the default path which is opt Cloud
era Parcels CDH lib Hadoop map reduce
Hadoop map reduce examples and then we
can look at this particular jar file and
if I hit on enter it shows me the
different classes which are part of this
jar and here I would like to use word
count so I could just give this I could
say word count now remember I could run
the job in a particular queue by giving
in an argument here so I could say minus
D map red. job. q. name and then I can
point my job to a particular queue I can
even give different arguments in saying
I would want my map reduce output to be
compressed or I want it to be stored in
a particular directory and so on so here
I have the word count and then basically
what I can be doing is I can be pointing
it to a particular input path and then I
can have my output which can be getting
stored here again a directory which we
need to choose and I will say output new
and I can submit my job now once I've
submitted my job it connects to resource
manager it basically Gets a Job ID it
gets an application ID it shows you from
where you can track your application you
can always go to the Yan UI and you can
be looking at your application and the
resources it is using so my application
was not a big one and it has already
completed it triggered one map task it
launched one reduced task it was working
on around 12,46 to6 records where you
have then the output of map which is
these many number of output records
which was then taken by a combiner and
finally by a reducer which basically
gives you the output so this is my Yan
application which has completed now I
could be looking into the Yan UI and if
my job has completed you might not see
your application here so as of now it
shows up here the word count which I ran
it also shows me my previous Park shell
job it shows me my application is
completed and if you would want further
information on this you can click and go
to the history server if you have been
given access to it or directly go to the
history server web UI where your
application shows up it shows how many
map and reduce task it was running you
can click on this particular application
which basically gives you information of
your map in roduced tasks you can look
at different counters for your
application right you can always look at
map specific tasks you can always look
into one particular task what it did on
which node it was running or you can be
looking at the complete application log
so you can always click on the logs and
here you have click here for full log
which gives you the information and you
can always look for your application
which can give you information of appm
being launched or you could have search
for the word container so you could see
a job which needs one or multiple
containers and then you could say
container is being requested then you
could see container is being allocated
then you can see what is the container
size and then basically your task moves
from initializing to running in the
container and finally you can even
search for release which will tell you
that the container was released so you
can always look into the log for more
information so this is how you can
interact with Yan this is how you can
interact with your command line to look
for more information or using your Yan
web UI or you can also be looking into
your Hue for more information welcome to
scoop tutorial one of the many features
of the Hadoop ecosystem for the Hado
file system what's in it for you today
we're going to cover the need for scoop
what is scoop scoop features scoop
architecture scoop import scoop export
scoop processing and then finally we'll
have a little Hands-On demo on scoop so
you can see what it looks like so where
does the need for scoop come in in our
big data Hado file system processing
huge volumes of data requires loading
data from diverse sources into hudo
cluster you can see here we have our
data processing and this process of
loading data from the heterogeneous
sources comes with a set of challenges
so what are the challenges maintain ing
data consistency ensuring efficient
utilization of resources especially when
you're talking about big data we can
certainly use up the resources when
importing terabytes and pedabytes of
data over the course of time loading
bulk data to hudo was not possible this
one of the big challenges it came up
when they first had the Hadoop file
system going and loading data using
script was very slow in other words
you'd write a script in whatever
language you're in and then it would
very slowly load each piece and parse it
in so the solution scoop scooped helped
in overcoming all the challenges to
traditional approach and could lead bulk
data from rdbms to Hadoop very easily so
think your Enterprise server you want to
take the um from MySQL or your SQL and
you want to bring that data into your
Hadoop Warehouse your data filing system
and that's where scoop comes in so what
exactly is scoop scoop is a tool used to
transfer bulk of data between Hadoop and
external data storage such as relational
databases and my SQL server or the
Microsoft SQL Server or MySQL server so
scoop equals SQL plus Hadoop and you can
see here we have our rdbms all the data
we have stored on there and then your
scoop is the middle ground and it brings
the import into the Hadoop file system
it also is one of the features that goes
out and grabs the data from Hadoop and
exports it back out into an rdbms let's
take a look at scoop features scoop
features has parallel Import and Export
it has import results of SQL query
connectors for all major rdbms databases
Kerberos security integration provides
full and incremental load so when we
look at parallel Import in export scoop
uses yarn yet another resource
negotiator framework to Import and
Export data this provides fault
Tolerance on a top of parallelism scoop
allows us to import the result return
from an SQL carry into the Hadoop file
system or the hdfs and you can see here
where the import results of SQL query
come in scoop provides connectors for m
mulle relational database management
system rdbms's databases such as MySQL
and Microsoft SQL server and it has
connectors for all major rdbms databases
scoop supports Kerberos computer network
Authentication Protocol that allows
nodes communicating over a non-secure
network to prove their identity to one
another in a secure manner scoop can
load the whole table or parts of the
table by a single command hence it
supports full and incremental load let's
dig a little deeper into to the scoop
architecture we have our client in this
case a hooded wizard behind his laptop
you never know who's going to be
accessing the Hadoop cluster and the
client comes in and sends their command
which goes into scoop the client submits
the import export command to import or
export data data from different
databases is fetched by scoop and so we
have enter Enterprise data warehouse
document based systems you have connect
a connector for your data warehouse a
connector for document based systems
which reaches out to those two entities
and we have our connector for the rdbms
so connectors help in working with a
range of popular databases multiple
mappers perform map tasks to load the
data onto hdfs a Hu file system and you
can see here we have the map task if you
remember from Hadoop Hadoop is based on
map reduce because we're not reducing
the data we're just mapping it over it
only accesses the mappers and it opens
up multiple mappers to do parallel
processing and you can see here the hdfs
hbas B high is where the target is for
this particular one similarly multiple
map tests will export the data from hdfs
onto rdbms using scoop export command so
just like you can import it you can out
export it using the multiple map
routines scoop import so here we have
our rdbms data store and we have the
folders on there so maybe it's your um
company's database maybe it's an archive
at Google with all the searches going on
whatever it is usually you think um with
scoop you think SQL you think my SQL
server or Microsoft SQL Server that kind
of setup uh so it gathers the metadata
and you see the scoop import so
introspect database to gather metadata
primary key information and then it
submits uh so you can see submits map
only job remember we talked about map
reduce it only needs the map side of it
because we're not reducing the data
we're just mapping it over scoop divides
the input data set into splits and uses
individual map tests to push the splitch
into hdfs so right into the Hado file
system and you can see down on the right
is kind of a small depiction of a Hadoop
cluster and then you have scoop export
so we're going to go the other direction
and with the other direction you have
your Hadoop file system storage which is
your Hadoop cluster you have your scoop
job and each one of those clusters then
gets a map mapper comes out to each one
of the computers that has data on it um
so the first step is you got to gather
the metadata so step one you gather the
metadata step two submits map only job
introspect database to gather metadata
primary key information scoop divides
the input data set into splits and uses
individual map tests to push the splits
to rdbms scoop will export Hadoop files
back to rdms tables and you can think of
this in a number of different manners
one of them would be if you're restoring
a backup from the Hadoop file system
into your Enterprise machines there's
certainly many others as far as
exploring data and data science so as we
dig a little deeper into scoop input we
have our connect our jdbc in our URL so
specify the jdbc connect string
connecting manager we specify The
Connection Manager class to use you can
see here driver with the class name
manually specify the jdbc driver class
to use Hadoop map reduce home directory
override Hadoop mapped home username uh
set authentication username and of
course help print usage instructions and
with the export you'll see that we can
specify the jdbc connect string specify
The Connection Manager class to use
manually specify jdbc driver class to
use you do have to let it know to
override the Hadoop map reduce home and
that's true on both of these and set
authentication username and finally you
can print out all your help setup so you
can see the format for scoop is pretty
straightforward both Import and Export
so let's uh continue on our path and
look at scoop processing and what the
computer goes through for that and we
talk about scoop processing first scoop
runs in the Hadoop cluster it Imports
data from the rdbms the nosql database
to the Hadoop file system so remember we
might not be importing the data from a
rdbms it might actually be coming from a
nosql and there's many out there he uses
mappers to slice the incoming data into
multiple formats and load the data into
hdfs it exports data back into an rdbms
while making sure that the schema of the
data and the database is maintained so
now that we've looked at the basic
commands in our scoop in the scoop
processor or at least the basics as far
as um theory is concerned hello Learners
simply brings you a post-graduate
program in data engineering developed in
partnership with pu University and IBM
to learn more about this course you can
find the course Link in the description
box below let's just jump in and take a
look at a demo on
[Music]
scoop
for this demo I'm going to use our Cloud
era quick start if you've been watching
our other demos we've done you'll see
that we've been using that pretty
consistently certainly this will work in
any of your um your Horton sandbox which
is also a single node testing machine
Cloud Dera is one of them there's a
Docker version instead of virtual box
and you can also set up your own Hadoop
cluster plan a little extra time if
you're not an admin it's actually a
pretty um uh significant Endeavor for an
admin if you've been admin Linux
machines for a very long time and you
know a lot of the commands I find it for
most admins it takes them about two to
four hours the first time they go in and
create a virtual machine and set up
their own Hadoop in this case though and
you're just learning and getting set up
best to start with Cloudera Cloud era
also includes an install version of my
SQL that way you don't have to worry
installing the the SQL version for
importing data from and to once you're
in the cloud era quick start you'll see
it open opens a nice sentos Linux um
interface and it has the desktop setup
on there this is really nice for
learnings here not just looking at
command lines and from in here it should
open up by default to uh Hue if not you
can click on Hue Hugh is a a kind of a
fun little uh webbased interface under
Hue I can go under query I can pick an
editor and we'll go right down to scoop
uh so now I'm just going to load the
scoop editor inner Hue now I'm going to
switch over and do this all in command
line I just want to show that you can
equally do this in Hue through the web
base interface the reason I like to do
the command line is specifically on my
computer it runs much quicker or if I do
the command line here and I run it it
tends to have an extra lag or an added
layer in it so for this we're going to
go ahead and open our command line the
second reason I do this is we're going
to need to go ahead and edit our MySQL
so we have something to scoop in
otherwise I don't have anything going in
there and of course we zoom in we'll
zoom in this and increase the size of
our screen so for this demo our Hands-On
I'm going to use Oracle virtual box
manager and the cloud era quick start if
you're not familiar with this we do have
another tutorial we put out and you can
send a note in the YouTube video below
and let our team know and they'll send
you a link or come visit www.s
simplylearn
docomo box on my Windows computer so
we're going to be in Linux and it'll be
the cloud daa version uh with scoop it
will also be using MySQL my SQL Server
once inside the Cloudera virtual box U
we'll go under the Hue editor now we're
going to do everything in terminal
window but I just want you to be aware
that under the Hue editor you can go
under query editor and you'll see as we
come down here here's our scoop on this
so you can run your Scoop from in here
now before we do this we have to do a
little exploration in MySQL a MySQL
server that way we know what data is
coming in so let me go ahead and open up
a terminal window in Cloud era you have
a terminal window at the top here that
you can just click on and open it up and
let me just go ahead head and zoom in on
here go View and zoom in now to get into
my SQL Server you simply type in MySQL
and this part will depend on your setup
now the cloud era quick start comes up
that the username is root and the
password is cloud Dera kind of a strange
Quirk is that you can put a space
between the minus U and the root but not
between the minus p and the cloud
usually you put in a minus capital P and
then it prompts you for your password on
here for this demo I don't worry too
much about you knowing the uh password
on that so we'll just go right into my
SQL Server since this is the standard
password for this quick start and you
can see we're now into my SQL and we're
going to do just a couple of quick um
commands in here there show databases
and you follow by the semicolon that's
standard in most of these um shell
commands so it knows it's the end of
your shell command and you'll see in
here in the quick Cloud ER quick start
the MySQL comes with a standard set of
databases these are just um some of
these have to do do like with the uzi
which is the uzi part of Hadoop where
others of these like customers and
employees and stuff like that those are
just for demo purposes they come as a
standard setup in there so that people
going in for the first time have a
database to play with which is really
good for us so we don't have to recreate
those databases and you will see in the
list here we have a retailor DB and then
we can simply do uh use retail uncore DB
this will set that as a default in MySQL
and then we want to go ahead and show
the table and if we show the tables you
can see under the database the retail DB
database we have categories customers
departments order items orders products
uh so there's a number of tables in here
and we're going to go ahead and just use
a standard um SQL command and if you did
our Hive language you'll know remember
it's the same for um hql also on this
we're just going to select star
everything from departments so there's
our departments table and we're going to
list everything on the Departments table
and you you'll see we have about six
lines in here and it has a department ID
and a department name uh two for Fitness
three for Footwear so on and so forth
now at this point I can just go ahead
and exit but it's kind of nice to have
this data up here so we can look at it
and flap back and forth between the
screens so I'm going to open up another
terminal window and we'll go ahead and
zoom in on this also and it isn't too
important for this particular setup uh
but it's always kind of f fun to know
what your setup you're working with what
is your host name and so we'll go ahead
and just type that in this is a Linux
command and it's uh host name minus F
and you see we're on quick start Cloud
era no surprise there now this next
command is going to be a little bit
longer because we're going to be doing
our first scoop command and I I want to
do two of them we're going to list
databases and list tables it's going to
take just a moment to get through this
uh because there's a bunch of stuff
going on here so we have scoop we have
list databases we have connect and under
the connect command we need to let it
know how we're connecting we're going to
use the jdbc this is a very standard one
uh jdbc MySQL so you'll see that if
you're doing an SQL database that's how
you start it off with and then the next
part this is where you have to go look
it up it's how however it was created so
if your admin created a MySQL server
with a certain setup that's what you
have to go by and you'll see that
usually they list this as Local Host so
you'll see something like local host
sometimes there's a lot of different
formats uh but the most common is either
local host or the actual connection so
in this case we want to go ahead and do
quick start
3306 and so quick start is the name of
the Local Host database and how it's
hosted on here and when you set up the
quick start for um for had doop under
Cloud daa it's Port 3306 is where that's
coming in so that's where all that's
coming from uh and so there's our path
for that and then we have to put in our
password we type type password if you
look it up password on the cloud era
quick start is Cloudera and we have to
also let it know the username and again
if you're doing this you'd probably put
in a minus Capital you can actually just
do for prompt CL the for the password so
if you leave that out it'll prompt you
uh but for this doesn't really matter I
don't care if you see my password it's a
default one for cloud a a quick start
and then the username on here is simply
root and then we're going to put our
semicolon at the end and so we have here
our full setup and we go ahead and list
the databases uh and you'll see you may
get some warnings on here I haven't run
the updates on the quick start um I
suggest you're not running the updates
either if you're doing this for the
first time because it'll do some
reformatting on there and I quickly
pause up and you can see here's all of
our um the tables we went in there and
if we go back to on the previous window
we should see that these tables match so
here we come in and here we have our
databases and you can see back up here
where we had uh the CM customers
employees and so on so the databases
match and then we want to go ahead and
list the uh tap tables for a specific
database so let's go ahead and do that
I'm a very lazy typist so I'll put the
up arrow in and you can see here scoop
list databases we're just going to go
back and change this from databases to
list tables so we want to list the
tables in here same connection so most
of the connection is the same except we
need to know which tables we're listing
and an interesting fact is you can
create a table without being under a
database so if you left this blank it
will show the open tables that aren't
connected directly to a database right
under a database but what we want to do
is right past this last slash on the
3306 we want to put that
retailor DB because that's the database
we're going to be working with and this
will go in there and show the tables
listed under that database and here we
go we got categories customers
departments order items and products if
we flip back here real quick there it is
the same thing we had we had um
categories customers departments order
items and so on and so let's go ahead
and run our first import command and
again I'm that lazy typer uh so we're
going to do scoop and instead of list
tables we want to go ahead and import so
there's our import command and so once
we have our import command in there then
we need to tell it exactly what we're
going to import so everything else is
the same we're importing from the retail
DB so we keep that and then at the very
end we're going to tag on-- table that
tells us so we can tell it what table
we're importing from and we're going to
import departments there we go so this
is pretty straightforward because what's
nice about this is you can see the
commands are the same I got the same
connection um I change it for the
whatever database I'm in um then I come
in here our password and the username
are going to be the same that's all
under the MySQL server setup uh and then
we let it know what table we're entering
in we run this and this is going to
actually go through the mapper process
in Hadoop so this is a mapping process
it takes the data and it Maps it up to
different parts in the setup in Hadoop
on there and then saves that data into
the Hadoop file system and it does take
it a moment to zip through which which I
kind of skipped over for you uh since it
is running a you know it's designed to
run across the cluster not on a single
node so when you're running on a single
node it's going to run slow even if you
dedicate uh a couple cores to it I think
I put dedicated four cores to this one
uh and so you can see right down here we
get to the end it's now mapped in that
information and then we can go in here
we can go under um can flip back to our
Hue and under Hue on the top I have
there's databases and the second icon
over is your Hado file system and we can
go in here and look at the file system
and you'll see it show up underneath our
documents there it is departments
Cloudera departments and um you can see
there's always a delay when I'm working
in Hue which I don't like and that's the
quick start issue uh that's not
necessarily running out on a server when
I'm running out on a server you pretty
much have to run through some kind of
server interface I still prefer the
terminal window it still runs a lot
quicker uh but we'll flip back on over
here to uh the command line and we can
do the Hadoop type in the Hadoop fs and
then then list minus LS and if we run
this you'll see underneath our Hado file
system there is our departments which
has been added in and we can also do uh
Hadoop fs and this is kind of
interesting for those who've gone
through the Hadoop file system
everything you'll you'll recognize this
on here but I'm going to list it the
contents of departments and you'll see
underneath departments uh we have part
part m 00000000 1 002 003 and so this is
interesting because this is how Hadoop
saves these files this is in the file
system this is not in Hive so we didn't
directly import this into Hive uh we put
this in the Hadoop file system depending
on what you're doing you would then
write the schema for Hive to look at the
Hado file system certainly visit our
Hive tutorial for more information on
hive specific uh so you can see in here
are different files that it forms that
are part of departments and we can do
something like this we can uh look at
the contents of one of these files FS
minus LS or a number of the files and
we'll simply do uh the full path which
is user Cloud Dera and then we already
know the next one is departments and
then after departments we're going to
put slash part star so this is going to
say anything that has part in it um so
we have part- M 000000 and so on uh we
can go ahead and cat use that cat
command or that list command to bring
those up and then we can use the cat
command to actually display the contents
and that's a a Linux command Hadoop
Linux command to cat catenate not to be
confused with catatonic catastrophic
there's a lot of cat got your tongue and
we see here Fitness Footwear apparel
that should look really familiar cuz
that's what we had in our uh MySQL
server uh when we went in here we did a
select all on here there it is Fitness
Footwear apparel golf outdoors and fan
shop and then of course it's really
important flip back on over here to be
able to tell it where to put the data uh
so we go back to our import command so
here's our scoop import we have our
connect we have the DB underneath our
connection our MySQL server we have our
password our username the table going
where it's going to I mean the table
where it's coming from uh and then we
can add a Target on here we can put in
uh Target Dash directory and you do have
to put the full path that's a a doop
thing it's a good practice to be in and
we're going to add it to Department uh
we'll just do Department one and so here
we now add a Target directory in here
and user Cloud era Department one and
this will take just a moment before so
I'll go ahead and skip over the process
since it's going to run very slowly it's
only running on like I said a couple
cores and it's also on a single node and
now we can do the uh Hadoop let's just
do the up Arrow file system list we want
just straight list and when we do the
Hadoop file system uh minus LS or list
you'll see that we now have Department
one and we can of course do uh list
Department one and you can see we have
the files inside Department one and they
mirrored what we saw before with the
same files in there and the part mm o00
and so so on uh if we want to look at
them it'd be the same thing we did
before with the cat so except instead of
departments uh we'd be Department one
there we go and then that's going to
come up with the same data we had before
now one of the important things when
you're importing data and it's always a
question to ask is do you filter the
data before it comes in do we want to
filter this data as it comes in so we're
not storing everything in our file
system you would think Hadoop Big Data
put it all in there I know from
experience that put it all in there can
turn uh a couple hundred terabytes into
a petabyte very rapidly and suddenly
you're having to really add on to that
data store and you're storing duplicate
data sometimes so you really need to be
able to filter your data out and so
let's go ahead and use our up Arrow to
go to our last import uh since it's
still a lot the same stuff uh so we have
all of our commands under import we have
the target uh we're going to change this
to Department two so we're going to
create a new directory for this one and
then after Department uh there's another
command that we didn't really slide in
here and that's our mapping and I'll
show you what this looks like in a
minute but we're going to put M3 in
there that does have nothing to do with
the filtering I'll show you that in a
second though what that's for and we
just want to put in where uh so where
and what is the we in this case we want
to know where Department ID and if you
want to know where that came from we can
flip back on over here we have
Department uncore IDs this is where
that's coming from that's just the name
of the column on here so when we come in
here to Department ID is greater than
four simple um logic there you can see
where you'd use that for maybe creating
buckets for ages uh you know age from 10
to 15 20 to 30 you might be looking for
I mean there's all kinds of reasons why
you you could use the wear command on
here and filter information out maybe
you're doing word counting and you want
to know words that are used less than a
100 times you you want to get rid of
the' and is and and and uh all the stuff
that's used over and over again uh so
we'll go ahead and put the where and
then Department ID is greater than four
we'll go ahead and hit enter on here and
this will create our department 2 setup
on this uh and I'll go ahead and skip
over some of the runtime again it runs
really slow on a single node uh real
quick page through our commands uh let's
see here we go our list and we should
see underneath the list the department
two on here now and there it is
Department two uh and then I can go
ahead and do list Department 2 you'll
see the contents in here uh and you'll
see that there is only three maps and it
could be that the data created three
Maps but remember I set it up to only
use three mappers uh so there's 0o one
and two and we can go ahead and do a cat
on there remember this is Department 2
so we want to look at all the contents
of these three different files and there
it is it's greater than four so we have
golf is five outdoor six uh fan shop is
seven uh so we've effectively filtered
out our data and just storing the data
we want on our file system so if you're
going to store data on here the next
stage is is to export the data uh
remember a lot of times you have MySQL
server and we're continually dumping
that data into our long-term storage and
access the Hado file system uh but what
happens when you need to pull that data
out and uh restore a database or uh
maybe you have um you just merged with a
new company a favorite topic merging
companies and merging databases that's
listed under Nightmare and how many
different names for company can you have
so you can see where being able to
export is also equally important and
let's go ahead and do that and I'm going
to flip back over to my SQL Server here
and we'll need to go ahead and create
our database we're going to export into
now I'm not going to go too much in
detail on this command we're simply
creating a table and the tables going to
have um it's pretty much the same table
we already have in here from departments
but in this case we're going to create a
table called
DT uh so it's the same setup but it's
it's just going to we're just giving it
a different name a different schema and
so we've done that and we'll go ahead
and do a select star from
D there we go and it's empty that's what
we expect a new databas a new data table
and it's empty in there uh so now we
need to go ahead and Export our data
that we just filtered out into there so
let's flip back on over here to our
scoop setup which is just our Linux
terminal window and let's go back up to
one of our commands here's scoop Import
in this case instead of import we're
going to take the scoop and we're going
to export so we're going to just change
that export and the connection is going
to remain the same so same connect same
uh database we're also we're still doing
the retail DB uh we have the same
password so none of that changes uh the
big change here is going to be the table
instead of departments uh remember we
changed it and gave it a new name and so
we want to change it here also D so
Department we're not going to worry
about the mapper count and the wear was
part of our import there we go and then
finally it needs to know where where to
export from so instead of Target
directory we have an export directory
that's where it's coming from uh still
user Cloud era and we'll keep it as
Department 2 uh just so you can see how
that data is coming back with the that
we filtered in and let's go ahead and
run this it'll take it just a moment to
go through its steps and again because
it's slow I'm just going to go and skip
this so you don't have to sit through it
and once we've wrapped up our export
we'll flip back on over here to mySQL
you the up aor and this time we're going
to select star from department and we
can see that there it is it exported the
golf outdoors and fan shop uh and you
can imagine also that you might have to
use the wear command in your export also
so there's a lot of mixing the command
line for scoop is pretty straight
forward uh you're changing the different
variables in there whether you're
creating a table listing a table listing
databases very powerful tool for
bringing your data into the Hadoop file
system and exporting it so now that
we've wrapped up our demo on scoop and
gone through a lot of basic commands
let's dive in with a brief history of
Hive so the history of Hive begins with
Facebook Facebook begin using Hadoop as
a solution to handle the growing big
data and we're not talking about a data
that fits on one or two or even five
computers uh we're talking due to the
fits on if you've looked at any of our
other Hood dup tutorials you'll know
we're talking about very big data and
data pools and Facebook certainly has a
lot of data it tracks as we know the
Hadoop uses map reduced for processing
data map reduce required users to write
long codes and so you'd have these
really extensive Java codes very
complicated for the average person to
use not all users reversed in Java and
other coding languages this proved to be
a disadvantage for them users were
comfortable with writing queries in SQL
SQL has been around for a long time the
standard uh SQL query language Hive was
developed with the vision to incorporate
the concepts of tables columns just like
SQL so why hiive well the problem was
for processing and analyzing data users
found it difficult to code as not all of
them were well versed with the coding
languages you have your processing you
have your analyzing and so the solution
was uh required a language similar to
SQL which was well known to all the
users and thus the hive or hql language
evolved what is Hive Hive is a data
warehouse system which is used for
quering and analyzing large data sets
stored in the hdfs or the hudo file
system hive uses a query language that
we call hiveql or hql which is similar
to SQL so if we take our user the user
sends out their hive queries and then
that is converted into a map reduce
tasks and then accesses the Hadoop map
reduce system let's take a look at the
architecture of Hive architecture of
Hive we have the hive client uh so that
could be the programmer or maybe it's a
manager who knows enough SQL to do a
basic query to look up the data they
need the hive client supports different
types of client applications in
different languages prefer for
performing queries and so we have our
Thrift application and the hive Thrift
client Thrift is a software framework
Hive server is based on Thrift so it can
serve the requests from all programming
language that support Thrift and then we
have our jdbc application and the hive
jdbc driver jdbc Java database
connectivity jdbc application is
connected through the jdbc driver and
then you have the odbc application or
the hive odbc driver the odbc or open
database connectivity the odbc
application is connected through the
odbc driver with the growing development
of all of our different scripting
languages python C++ spark uh Java you
can find just about any connection in
any of the main scripting languages and
so we have our Hive Services as we look
at deeper into the architecture Hive
supports various Services uh so you have
your Hive server basically your Thrift
application or your hive Thrift client
or your jdbc or your hive jdbc driver
your odbc application or your hive obbc
driver they all Connect into The Hive
server and you have your hive web
interface you also have your CLI now the
hive web interface is a guy is provided
to execute Hive queries and we'll
actually be using that later on today so
you can see kind of what that looks like
and get a feel for what that means
commands are executed directly in CLI
and then the CLI is a direct terminal
window uh and I'll also show you that
too so you can see how those two
different interfaces work these then
push the code into the hive driver Hive
driver is responsible for all the
queries submitted so everything goes
through that driver let's take a closer
look at the hive driver The Hive driver
now performs three steps internally one
is a compiler Hive driver passes query
to compiler where it is checked and
analyzed then the optimizer kicks in and
the optimizer logical plan in the form
of a graph of map reduce and hdfs tasks
is obtained and then finally in the
executor in the final step the tests are
executed when we look at the
architecture we also have to note the
metastore metastore is a repository for
Hive metadata stores metadata for Hive
tables and you can think of this as your
schema and where is it located and it's
stored on the Apache Derby DB processing
and resource management is all handled
by the map reduce V1 you'll see map
reduce V2 the yarn and the tez these are
all different ways of managing these
resources depending on what version of
Hadoop you're in Hive uses map reduced
framework to process queries and then we
have our distributed storage which is
the hdfs and if you looked at our Hadoop
uh tutorials you'll know that these are
on commodity machines and are linearly
scalable that means they're very
affordable a lot of time when you're
talking about Big Data you're talking
about a tenth of the price of storing it
on Enterprise computers and then we look
at the data flow And Hive uh so in our
data flow And Hive we have our Hive and
the Hadoop system and underneath the
user interface or the UI we have our
driver our compiler our execution engine
and our meta store that all goes into
the map reduce and the Hado file system
so when we execute a query you see it
coming in here it goes into the driver
step one step two we get a plan what are
we going to do refers to the query
execution uh then we go to the metadata
it's like well what kind of metadata are
we actually looking at where is this
data located what is the schema on it uh
then this comes back with the metadata
into the compiler then the compiler
takes all that information and the send
plan returns it to the driver the driver
then sends the execute plan to the
execution engine once it's in the
execution engine the execution engine
acts as a bridge between Hive and Hadoop
to process the query and that's going
into your map reduce and your Hadoop
file system or your hdfs and then we
come back with the metadata operations
it goes back into the meta store to
update or let it know what's going on
which also goes to the between it's a
communication between the execution
engine and The Meta store execution
engine Communications is bidirectionally
with the metast store to perform
operations like create drop tables
metast store stores information about
tables and columns so again we're
talking about the schema of your
database and once we have that we have a
bu directional send results
communication back into the driver and
then we have the fetch results which
goes back to to the client so let's take
a little bit look at the hive data
modeling Hive data modeling so you have
your hive data modeling you have your
tables you have your partitions and you
have buckets the tables in Hive are
created the same way it is done in rdbms
so when you're looking at your
traditional SQL server or MySQL server
where you might have Enterprise
equipment and a lot of uh people pulling
and moving stuff off of there the tables
are going to look very similar and this
makes it very easy to take that
information and and let's say you need
to keep current information but you need
to store all of your years of
transactions back into the Hadoop hi so
you you match those those all kind of
look the same the tables are the same
your databases look very similar and you
can easily import them back you can
easily store them into the hive system
partitions here tables are organized
into partitions for grouping same type
of data based on partition key this can
become very important for speeding up
the process of doing queries so if
you're looking at dates as far as like
your employment dates of employees if
that's what you're tracking you might
add a partition there because that might
be one of the key things that you're
always looking up as far as employees
are concerned and finally we have
buckets uh data present in partitions
can be further divided into buckets for
efficient querying again there's that
efficiency uh at this level a lot of
times you're T you're working with the
programmer and the admin of your Hado
file system to maximize the efficiency
of that file system so it's usually a
two-person job and we're talking about
Hive data modeling you want to make sure
that they work together and you're
maximizing your resources Hive data
types so we're talking about Hive data
types we have our primitive data types
and our complex data types A lot of this
will look familiar because it mirrors a
lot of stuff in SQL in our primitive
data types we have the numerical data
types string data type date time data
type and miscellaneous data type and
these should be very they're kind of
self-explanatory but just in case uh
numerical data is your floats your
integers your short integers all of that
numerical data comes in as a number a
string of course is characters and
numbers and then you have your date time
stamp and then we have kind of a general
way of pulling your own created data
types in there that's your miscellaneous
data type and we have complex data types
so you can store arrays you can store
maps you can store structures uh and
even units in there as we dig into hive
data types uh and we have the primitive
data types and the complex data types so
we look at primitive data types and
we're looking at numeric data types data
types like an integer a float a decimal
those are all stored as numbers in the
hive data system a string data type data
types like characters and strings you
store the name of the person you're
working with uh you know John Doe the
city uh Memphis the state Tennessee
maybe it's Boulder Colorado USA or maybe
it's hyperbad India um that's all going
to be string and stored as a string
character and of course we have our date
time data type data types like timestamp
date interval those are very common as
far as tracking sales anything like that
so you can just think if you can type a
stamp of time on it or maybe you're
dealing with the race and you want to
know the interval how long did the
person take to complete whatever task it
was all that is datetime data type and
then we talk miscellaneous data type
these are like Boolean and binary and
when you get into Boolean and binary you
can actually almost create anything in
there but your yes NOS 01 now let's take
a look at complex data types a little
closer uh we have arrays so your syntax
is of data type and it's an array and
you can just think of an array as a
collection of same um entities one two 3
four if they're all numbers and you have
Maps this is a collection of key value
pairs uh so understanding Maps is so
Central to Hadoop uh so when we store
Maps you have a key which is a set you
can only have one key per mapped value
and so you in Hado of course you collect
uh the same keys and you can add them
all up or do something with all the
contents of the same key but this is our
map as a primitive type data type and
our collection of key value Pairs and
then collection of complex data with
comment so we can have a structure where
we have like column name data type
comment col column comment uh so you can
get very complicated structures in here
with your collection of data and your
comment and setup and then we have units
and this is a a collection of
heterogeneous data types uh so the
Syntax for this is Union type data type
data type and so on so it's all going to
be the same a little bit different than
the arrays where you can actually mix
and match different modes of Hive Hive
operates in two modes depending on the
number and size of data nodes we have
our local mode and our map reduce mode
when we talk about the local mode it is
used when hup is having one data node
and the data is small processing will be
very fast on a smaller data set which
are present in local machine and this
might be that you're have a a local file
stuff you're uploading into the hive and
you need to do some processes in there
you can go ahead and run those hi
processes and queries on it usually you
don't see much in the way of a single
node to dup system if you're going to do
that you might as well just use like an
SQL database or um even a Java SQ light
or something python is SQ light so you
don't really see a lot of single node
Hadoop databases but you do see the
local mode in Hive where you're working
with a small amount of data that's going
to be integrated into the larger
database and then we have the map reduce
mode this is used when Hadoop is having
multiple data nodes and the data is
spread across various data nodes
processing large data sets can be more
efficient using this mode and this you
can think of instead of it being one two
three or even five computers uh we're
usually talking with the Hadoop file
system we're looking at 10 computers 15
100 where this data is spread across all
those different Hadoop nodes difference
between Hive and
rdbms and remember rdbms stands for the
relational database management system
let's take a look at the difference
between Hive and the rdbms with Hive
Hive enforces schema on read and it's
very important that whatever is coming
in that's when hive's looking at it and
making sure that it fits the model um
the rdbms enforces a schema when it
actually writes the data into the
database so it's read the data and then
once it starts to write it that's where
it's going to going to give you the
error or tell you something's incorrect
about your scheme High data size is in
pedabytes that is hard to imagine um you
know we're looking at your personal
computer on your desk maybe have 10
terabytes if it's a high-end computer
we're talking pedabytes so that's
hundreds of computers group together
when rdbms data size is in terabytes
very rarely do you see an rdbms system
that's spread over more than five
computers and there's a lot of reasons
for that with the rdbms it actually has
um a high-end amount of rights to the
hard drive there's a lot more going on
there you're writing and polling stuff
so you really don't want to get too big
with an RDM Master you're going to run
into a lot of problems with Hive you can
take it as big as you want Hive is based
on the notion of write want and read
many times this is so important uh they
call it worm which is write w wants o
read R many times M they refer to it as
worm and that's true of any of a lot of
your Hadoop setup it's it's altered a
little bit but but in general we're
looking at archiving data that you want
to do data analysis on we're looking at
pulling all that stuff off your rdbms
from years and years and years of
business or whatever your company does
or scientific research and putting that
into a huge data pool so that you can
now do queries on it and get that
information out of it with the rdbms
it's based on the notion of read and
write many times uh so you're
continually updating this database
you're continually bringing up new stuff
new sales the account changes because
they have a different licensing now
whatever software you're selling all
that kind of stuff where the data is
continually fluctuating and then hi
resembles a traditional database by
supporting SQL but it is not a database
it is a data warehouse this is very
important and it goes with all the other
stuff we've talked about that we're not
looking at a database but a data
warehouse to store the data and still
have fast and easy access to it for
doing queries um you can think of um
Twitter and Facebook they have so many
posts that are archived back
historically um those posts aren't going
to change they made the post they're
posted they're there and they're in
their database but they have to store it
in a warehouse in case they want to pull
it back up with the rdbms it's a type of
database management system which is
based on the relational model of data
and then with Hive easily scalable at a
low cost again we're talking maybe
$1,000 per terabyte um the rdbms is not
scalable at a low cost when you first
start on the lower end you're talking
about 10,000 per terabyte of data
including all the backup on the models
and and all the added Necessities to
support it as you scale it up you have
to scale those computers and Hardware up
uh so you might start off with a basic
server and then you upgrade to a sun
computer to run it uh and you spend you
know tens of thousands of dollars for
that Hardware upgrade with Hive you just
put another computer into your Hado file
system so let's look at some of the
features of Hive uh when we're looking
at the features of Hive we're talking
about the use of SQL like language
called hiveql a lot of times you'll see
that as hql which is easier than long
codes this is nice if you're working
with your shareholders you come to them
and you say Hey you can do a basic SQL
query on here and pull up the
information you need this way you don't
have to take off have your programmers
jump in every time they want to look up
something in the database they actually
now can easily do that if they're not uh
skilled in programming and script
writing tables are used which are
similar to The rdbms hence easier to
understand um one of the things I like
about this is when I'm bringing tables
in from a MySQL server or SQL Server
there's almost a direct reflection
between the two so when you looking at
one which is a data which is continually
changing and then you're going into the
archive database it's not this huge jump
where you have to learn a whole new
language uh you mirror that same schema
into the hdfs into the hive uh making it
very easy to go between the two and then
using hiveql multiple users can
simultaneously query data uh so again
you can have multiple clients cents in
there and they send in their query
that's also true with the rdbms which
kind of cues them up because it's
running so fast you don't notice the lag
time well you get that also with the hql
as you add more computers in the query
can go very quickly depending on how
many computers and how much um resources
each machine has to pull the information
and Hive supports a variety of data
types uh so with Hive it's designed to
be on the Hadoop system which you can
put almost anything into the Hadoop file
system so with all that let's take a
look at a demo on hive ql or hql before
I dive into the Hands-On demo let's take
a look at the website hive. apache.org
that's the main website uh since Apache
it's an Apache open source uh software
this is the main software or the main
site for the build and if you go in here
you'll see that they're slowly migrating
Hive into beehive and so if you see
beehive versus Hive uh note The Beehive
is a new release is coming out that's
all it is and it reflects a lot of the
same functionality of Hive it's the same
thing and then we like to pull up some
kind of um documentation on commands and
for this I'm actually going to go to
Horton Works Hive cheat sheet and that's
because Horton works and Cloud are to
the most common used builds for Hadoop
and for which include Hive and all the
different Tools in there and so Horton
Works has a pretty good PDF you can
download cheat sheet on there I believe
Cloudera does too but we'll go ahead and
just look at the Horton one because it's
the one that comes up really good and
you can see when we look at the query
language it Compares my SQL Server to
hiveql or hql and you can see the basic
select we select from columns uh from
table where conditions exist you know
most basic uh command on there and they
have different things you can do with it
just like you do with your um SQL and if
you scroll down you'll see um data types
so here's your integer your flow your
binary double string timestamp and all
the different data types you can use
some different semantics different Keys
features functions uh for running a hive
query command line setup and of course
the hive shell uh setup in here uh so
you can see right here if we Loop
through it it has a lot of your basic
stuff and we're basically looking at SQL
across a Horton database we're going to
go ahead and run our Hadoop cluster Hive
uh demo and I'm going to go ahead and
use the Cloudera quick start and this is
in the virtual box so again we have an
oracle virtual box which is open source
and then we have our Cloudera quick
start which is the Hadoop setup on a
single node now obviously Hadoop And
Hive are designed to run across a
cluster of computers so we talk about a
single note is for Education testing
that kind of thing and if you have a
chance you can always go back and look
at our demo we had on um setting up a
Hadoop system and a single cluster just
set a note Down Below in the YouTube
video and our team will get in contact
with you and send you that that link if
you don't already have it or you can
contact us at the www.s simplylearn dcom
now in here it's always important to
note that you do need um on your
computer if you're running on Windows
because I'm on a Windows machine you're
going to need probably about 12
gigabytes to actually run this uh used
to be get by with a lot less but as
things have evolved they take up more
and more resources and you need the
professional version if you have the
home version I was able to get that to
run but boy it did take a lot of extra
work to get the home version to let use
the virtual setup on there and we'll
simply click on the cloud era quick
start and I'm going to go and just start
that up and this is starting up our
Linux so we have our Windows 10 which is
the computer I'm on and then I have the
virtual box which is going to have a
Linux operating system in it and we'll
skip ahead so you don't have to watch
the whole install something interesting
to know about the cloud era is that it's
running on Linux Centos and for whatever
reason I've always had to click on it
and hit the escape button for it to spin
up and then you'll see the Doss come in
here now that our Cloud Dera has spun up
on our virtual machine with the Linux on
uh we can see here we have our it uses
the Thunderbird browser on here by
default and automatically opens up a
number of different tabs for us and a
quick note cuz I mentioned like the
restrictions on getting set up on your
own computer if you have a home edition
computer and you're worried about
setting it up on there you can also go
in there and spin up a one Monon free
service on Amazon web service play with
this uh so there's other options you're
not stuck with just doing it on the
quick start menu you can spin this up in
many other ways now the first thing we
want to not is that we've come in here
into cloud daa and I'm going to access
this in two ways uh the first one is
we're going to use Hue and I'm going to
open up Hue and it'll take it a moment
to load from um the setup on here and
Hue is nice if I go in and use Hue as an
editor into Hive or into the had doop
setup usually I'm doing it as a um from
an admin side because it has a lot more
information a lot of visuals less to do
with you know actually diving in there
and just executing code and you can also
write this code into files and scripts
and there's other things you can other
ways you can upload it into Hive but
today we're going to look at the command
lines and we'll upload it into Hue and
then we'll go into and actually do our
work in a terminal window Under The Hive
shell now in the Hue browser window if
you go under query and click on the pull
down menu and then you go under editor
and you'll see Hive there we go there's
our Hive setup I go and click on hive
and this will open it up our query down
here and now it has a nice little B that
shows our Hive going and we can go
something very simple down here like
show databases and we follow it with the
semicolon and that's the standard in
Hive as you always add our punctuation
at the end there and I'll go ahead and
run this and the query will show up
underneath and you'll see down here uh
since this is a new quick start I just
put on here you'll see it has the
default down here for um the the
databases that's the database name I
haven't actually created any databases
on here and then there's a lot of other
like uh assistant function tables um
your databases up here there's all kinds
of things you can research you can look
at through Hue as far as a bigger
picture the down side of this is it
always seems to lag for me whenever I'm
doing this I always seem to run slow so
if you're in Cloud Dera you can open up
a terminal window they actually have an
icon at the top you can also go under
applications and under applications
system tools and terminal either one
will work it's just a regular terminal
window and this terminal window is now
running underneath our Linux so this is
a Linux terminal window or on our
virtual machine which is resting on our
regular Windows 10 machine and we'll go
ahead and zoom this in so you can see
the text better on your own video and I
simply just clicked on view and zoom in
and then all we have to do is type in
Hive and this will open up the shell on
here and it takes it just a moment to
load when starting up Hive I also want
to note that depending on your rights on
the computer you're on in your action
you might have to do pseudo hiive and
put in your password and username most
computers are usually set up with the
hive login again it just depends on how
you're accessing the Linux system and
the hive shell once we're in here we can
go ahead and do a simple uh hql command
show databases and if we do that we'll
see here that we don't have any
databases so we can go and create a
database and we'll just call it office
for today for this moment now if I do
show we'll just do up Arrow up arrow is
a hotkey that works in both Linux and in
Hive so I can go back and paste through
all the commands I've typed in and we
can see now that I have my there's of
course the default database and then
there's the office database so now we've
created a database it's pretty quick and
easy and we can go ahead and drop the
database we can do drop Database Office
now this will work on this database
because it's empty if your database was
not empty you would have to do Cascade
and that drops all the tables in the
database and the database itself now if
we do show database and we'll go ahead
and recreate our database because we're
going to use the office database for the
rest of this um Hands-On demo a really
handy command Now set with the SQL or
hql is to use office and what that does
is that sets office as a default
database so instead of having to
reference the database every time we
work with a table it now automatically
assumes that's the database being used
whatever table we're working on the
difference is is you put the database
name period table and I'll show you in
just a minute what that looks like and
how that's different if we're going to
have a table and a database we should
probably load some data into it so let
me go ahead and switch gears here and
open up a terminal window you can just
open another terminal window and it'll
open up right on top of the one that you
have Hive shell running in and when
we're in this terminal window first
we're going to go and just do a list
which is of course a Linux command you
can see all the files I have in here
this is the default load we can change
Direct to documents we can list in
documents and we're actually going to be
looking at employee. CSV a Linux command
is the cat you can use this actually to
combine documents there's all kinds of
things that cat does but if we want to
just display the um contents of our
employee. CSV file we can simply do cat
employee CSV and when we're looking at
this we want to know a couple things one
there's a line at the top okay so the
very first thing we notice is that we
have a header line the next thing we
notice is that the data is comma
separated and in this particular case
you'll see a space here generally with
these you got to be real careful with
spaces there's all kinds of things you
got to watch out for because they can
cause issues these spaces won't because
these are all strings that the space is
connected to if this was a space next to
the integer you would get a null value
that comes into the database without
doing something extra in there now with
most of Hadoop that's important to know
that you're writing the data once
reading it many times and that's true of
almost all your Hadoop things coming in
so you really want to process the data
before it gets into the database and for
those who of you have studied uh data
transformation that's the edal where you
extract transfer form and then load the
data so you really want to extract and
transform before putting it into the
hive then you load it into the hive with
the transform data and of course we also
want to note the schema we have an
integer string string integer integer uh
so we kept it pretty simple in here as
far as the way the data is set up the
last thing that you're going to want to
look up uh when you're is the source uh
since we're doing local uploads we want
to know what the path is we have the
whole path in this case it's homecloud
era documents and these are just text
documents we're working with right now
we're not doing anything fancy so we can
do a simple get edit employee. CSV and
you'll see it comes up here uh it's just
a text document so I can easily remove
these added spaces there we go um and
then we go and just save it and so now
it has a new setup in there we've edited
it the gedit is usually one of the
default that loads into Linux um so any
text editor will do back to the hive
shell so let's go ahead and create a
table ployee and what I want you to note
here is I did not put the semicolon on
the end here semicolon tells it to
execute that line so this is kind of
nice if you're you can actually just
paste it in if you have it written on
another sheet and you can see right here
where I have create table employee and
it goes into to the next line on there
so do all of my commands at once now
just so I don't have any typo errors I
went ahead and just pasted the next
three lines in and the next one is our
schema if you remember correctly from
the other side we had uh the different
values in here which was ID name
Department year of joining and salary
and the ID is an integer name is a
string department string year of joining
integer salary an integer and they're in
Brackets we put closed black brackets
around them and you could do this all as
one line and then we have row format
delimited Fields terminated by comma and
this is important because the default is
tabs so if I do it now it won't find any
terminated Fields so you'll get a bunch
of null values loaded into your table
and then finally our table properties we
want to skip the header line count
equals 1 now this is a lot of work for
uploading a single file it's kind of
goofy when you're uploading a single
file that you have to put all this in
here but keep in mind Hive and Hadoop is
designed for writing Min files into the
database you write them all in there and
then you can they're saved it's an
archive it's a data warehouse and then
you're able to do all your queries on
them so a lot of times we're not looking
at just the one file coming up we're
loading hundreds of files you have your
reports coming off of your main database
all those reports are being loaded you
have your log files you have I mean all
this different data is being dumped into
Hadoop and in this case Hive on top of
Hadoop and so we need to let it know hey
how do I handle these files coming in
and then we have the semicolon at at the
end which lets us know to go ahead and
run this line and so we'll go ahead and
run that and now if we do a show tables
uh you can see there's our employee on
there we can also describe if we do
describe employee uh you can see that we
have our ID integer name string
department string year of U joining
integer and salary integer and then
finally let's just do a select star from
employee very basic um SQL and hql
command uh selecting data and it's going
to come up and we haven't put anything
in it so as we expect there's no data in
it so if we flip back to our um Linux
terminal window you can see where we did
the cat employee. CSV and you can see
all the data we expect to come into it
and we also did our PWD and right here
you see the path you need that full path
when you are U loading data you know you
can do a browse and if I did it right
now with just the employee. CSV as a
name it will work but that is a really
bad habit in general when you're Lo
loading data because it's uh you don't
know what else is going on in the
computer you want to do the full path
almost in all your data loads so let's
go ahead and flip back over here to our
Hive shell we're working in and the
command for this is load data uh so that
says hey we're loading data that's a
hive command hql and we want local data
so you got to put down local in path so
now it needs to know where the path is
now to make this more legible I'm just
going to go ahead and hit enter and then
we'll just paste the full path in there
which I have um stored over on the side
like a good prepared um demo and you'll
see here we have home Cloudera documents
employee. CSV so it's a whole path for
this text document in here and we go
ahead and hit enter in there and then we
have to let it know where the data is
going so now we have a source and we
need a destination and it's going to go
into the table and we'll just call it
employee we'll just match the table in
there and because I want it to execute
we put the semicolon on the end it goes
ahead and executes all three lines now
if we go back if you remember remember
we did the select star from employee
just using the up aror to page through
my different um commands I've already
typed in you can see right here we have
uh as we expect we have Rose Sam Mike
and Nick and we have all their
information showing in our four rows and
then let's go ahead and do uh select and
count just look at a couple of these
different select options you can do
we're going to count everything from
employee now this is kind of interesting
because the first one just pops up with
the basic uh select because ites doesn't
need to go through the full map reduce
phase but when you start doing a count
it does go through the full map reduce
setup in the hive and Hadoop and because
I'm doing this demo on a single node
Cloud era virtual box on top of a
Windows 10 all the benefits of running
it on a cluster are gone and instead is
now going through all those added layers
so it takes longer to run you know like
I said uh when you do a single node as I
said earlier it doesn't do any good as
an actual distribution because you're
only running it on one computer and then
you've added all these different layers
to run it and we see it comes up with
four and that's what we expect we have
four rows we expect four at the end and
if you remember from U our cheat sheet
which we brought up here from Horton
it's a pretty good one there's all these
different commands we can do we'll look
at one more command where we do the U uh
what they call subqueries right down
here uh cuz that's really common to do a
lot of subqueries and so we'll do select
uh star or all different columns from
employee now if we weren't using the
office database it would look like this
from office. employee and either one
will work on this particular one because
um we have office set as a default on
there uh so from office employee and
then the command where creates a subset
and in this case we want to know where
the salary is greater than
25,000 there we go and of course we end
with our semicolon and if we run this
query you can see it pops up and there's
our salaries of people top earners uh we
have Rose and it and Mike and HR uh
kudos to them of course they're
fictitional I don't actually we don't
actually have a rose and a mic in those
positions or maybe we do so finally we
want to go ahead and do is we're done
with this um table now remember you're
dealing with the data warehouse so you
usually don't do a lot of dropping of
tables and databases but we're going to
go ahead and drop this table here before
we drop it one more quick note is we can
change it so what we're going to do is
we're going to alter table office
employee and we want to go ahead and
rename it there's some other commands
you can do in here but rename is pretty
common and we're going to rename it to
and it's going to stay in office and uh
turns out one of our sh shareholders
really doesn't like the word employee he
wants employees plural it's a big deal
to him so let's go ahead and change that
name for the table it's that easy cuz
it's just changing the metadata on there
and now if we do show tables you'll see
we now have employees not employee uh
and then at this point maybe we're doing
some house cleaning because this is all
practice so we're going to go ahead and
drop table and we'll drop table
employees because we changed the name in
there so if we did employee it just give
us an error and now if we do show tables
you'll see all the tables are gone now
the next thing we want to go and take a
look at and we're going to walk back
through the loading of data uh just real
quick cuz we're going to load two tables
in here and let me just float back to
our terminal window so we can see what
those tables are that we're loading and
so up here we have customer we have a
customer um file and we have an order
file we want to go ahead and put the
customers and the orders into here so
those are the two we're doing and of
course it's always nice to see what
you're working with uh so let's do our
cat customer. CSV we could always do
gedit but we don't really need to edit
these we just want to take a look at the
data in customer and important in here
is again we have a header so we have to
skip a line comma separated uh nothing
odd with the data we have our schema
which is um integer string integer
string integer so you know you'd want to
take that note that down or flip back
and forth when you're doing it and then
let's go ahead and do cat order. CSV and
we can see we have oid which I'm
guessing is the order ID we have a date
up something new we've done integers and
strings but we haven't done date when
you're importing new and you never
worked with the date date's always one
of the more trickier fields to port in
and that's true of just about any
scripting language I've worked with all
of them have their own idea of how
date's supposed to be formatted what the
default is this particular format or
it's year and it has all four uh digits
Das month two digits Das day is the
standard import for the hive so you'll
have to look up and see what the
different formats are if you're going to
do a different format in there coming in
or you're not able to pre-process the
data but this would be a pre-processing
of the data thing coming in if you
remember correctly from uh our edel
which is uh e just in case you weren
weren't able to hear me last time ETL
which stands for extract transform then
load so you want to make sure you
transforming this data before it gets
into here and so we're going to go ahead
and bring um both this data in here and
really we're doing this so we can show
you the basic join there is if you
remember from our setup merge join all
kinds of different things you can do but
joining different data sets is so common
so it's really important to know how to
do this we need to go ahead and bring in
these two data sets and you can see
where I just created a table customer
here's our schema the integer name age
address salary here's our eliminated by
commas and our table properties where we
skip a line well let's go ahead and load
the data first and then we'll do that
with our order and let's go ahead and
put that in here and I've got it split
into three lines so you can see it
easily we got load data local in path so
we know we're loading data we know it's
local and we have the path here's the
complete path for uh o this is supposed
to be order CSV grab the wrong one uh of
course it's going to give me errors cuz
you can't recreate the same table on
there and here we go create table here's
our um integer date customer the basic
setup that we had coming in here for our
schema row format commas table
properties skip header line and then
finally let's load the data into our
order table load data local in path home
Cloud era documents order. CSV into
table order now if we did everything
right we should be able to do select
star from customer and you can see we
have have all seven customers and then
we can do select star from order and we
have uh four orders uh so this is just
like a quick frame we have you know a
lot of times when you have your customer
databases in business you have thousands
of customers from years and years and
some of them you know they move they
close their business they change names
all kinds of things happen uh so what we
want to do is we want to go ahead and
find just the information connected to
these orders and who's connected to them
and so let's go ahead and do it's a
select because we're going to display
information so select and this is kind
of interesting we're going to do
c.id um and I'm going to Define c as
customer as a customer table in just a
minute then we're going to do C.N name
and again we're going to define the c c.
AG so this means from the customer we
want to know their ID their name their
age and then you know I'd also like to
know the order amount uh so let's do o
for do amount and then this is where we
need to go ahead and Define what we're
doing and I'll go and capitalize from
customer so we're going to take the
customer table in here and we're going
to name it C that's where the C comes
from so that's the customer table C and
we want to join order as o that's where
our o comes from so the o. amount is
what we're joining in there and then we
want to do this on we got to tell it how
to connect the two tables c.id equals o.
customer uncore ID so now we know how
they're joined and now remember we have
seven customer customers in here we have
four orders and as it processes we
should get a return of four different
names joined together and they're joined
based on of course the orders on there
and once we're done we now have the
order number the person who made the
order their age and the amount of the
order which came from the order table uh
so you have your your different
information and you can see how the join
works here very common use of tables and
hql and SQL and let's do one more thing
with our our database and then I'll show
you a couple other Hive commands and
let's go ahead and do a drop and we're
going to drop Database Office and if
you're looking at this and you uh
remember from earlier this will give me
an error and let's to see what that
looks like it says failed to execute
exception one or more tables exist so if
you remember from before you can't just
drop a database unless you tell it to
Cascade that lets it know I don't care
how many tables are in it let's get rid
of it and Hado since it's an an art it's
a warehouse a data warehouse you usually
don't do a lot of dropping uh maybe at
the beginning when you're developing the
schemas and you realize you messed up
you might drop some stuff uh but down
the road you're really just adding
commodity machines to take up so you can
store more stuff on it so you usually
don't do a lot of database dropping and
some other uh fun commands to know is
you can do um select round 2.3 as round
value you can do a roundoff in uh Hive
uh we can do as floor value which is
going to give us a two so it turns it
into an integer versus a float it goes
down you know basically truncates it uh
but it goes down and we can also do
ceiling which is going to round it up so
we're looking for the next integer above
there's a few commands we didn't show in
here because we're on a single node as
as an admin to help speedi the process
you usually add in partitions for the
data and buckets um you can't do that on
a single node because the when you add a
partition it partitions it across
separate nodes but beyond that you can
see that it's very straightforward we
have SQL coming in uh and all your basic
queries that are in SQL are very similar
to hql let's get started with pig why
Pig what is pig map reduce versus Hive
versus pig hopefully you've had a chance
to do our Hive tutorial and our map
reduce tutorial if you haven't send a
note over to Simply learn and we'll
follow up with a link to you we'll look
at Pig architecture working of pig pig
latin data model Pig execution modes a
use case Twitter and features of pig and
then we'll tag on a short demo so you
can see Pig In Action so why pig as we
all know Hadoop uses map reduce to
analyze and process big data processing
Big Data consumed more time so before we
had the Hadoop system they'd have to
spend a lot of money on a huge set of
computers and Enterprise machines so we
introduced the Hadoop map reduce and so
afterwards processing Big Data was
faster using map reduce then what what
is the problem with map reduce prior to
2006 all map reduce programs were
written in Java non-programmers found it
difficult to write lengthy Java codes
they faced issues in incorporating map
sort reduce to fundamentals of map
reduce while creating a program you can
see here matap phase Shuffle and sort
reduce phase eventually it became a
difficult task to maintain and optimize
a code due to which the processing time
increased you can imagine a manager
trying to go in there and editing a
simple query to find out data and he has
to go talk to the programmers anytime he
wants anything so that was a big problem
not everybody wants to have a on call
programmer for every manager on their
team Yahoo faced problems to process and
analyze large data sets using Java as
the codes were complex and lengthy there
was a necessity to develop an easier way
to analyze large data sets without using
timeconsuming complex Java modes and
codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it it
was developed with the vision to analyze
and process large data sets without
using complex Java codes Pig was
developed especially for non-programmers
pig used simple steps to analyze data
sets which was time efficient so what
exactly is pig pig is a scripting
platform that runs onh Hadoop clusters
designed to process and analyze large
data sets and so you have your pig which
uses SQL like queries they're definitely
not SQL but some of them resemble SQL
queries and then we use that to analyze
ize our data Pig operates on various
types of data like structured
semi-structured and unstructured data
let's take a closer look at map reduce
versus Hive versus pig so we start with
a compiled language your map reduce and
we have Hive which is your SQL like
query and then we have pig which is a
scripting language it has some
similarities to SQL but it has a lot of
its own stuff remember SQL like query
which is what Hive is based off looks
for structured data and so when you get
into scripting languages like Pig now
we're dealing more with semi-structured
and even unstructured data with a Hadoop
map reduce we have a need to write long
complex codes with Hive no need to write
complex codes you could just put it in a
simple SQL query or hql hiveql and in
pig no need to write complex codes as we
have pig latin now remember in the map
reduce it can produce structured
semi-structured and unstructured data
and as I mentioned before Hive can
process only structured data think rows
and columns where Pig can process
structured semi-structured and
unstructured data you can think of
structured data as rows and columns
semi-structured as your HTML XML
documents like you have on your web
pages and unstructured could be anything
from groups of documents and written
format Twitter tweets any of those
things come in as very unstructured data
and with our Hadoop map reduce we have a
lower level of abstraction with both
Hive and pig we have a higher level
abstraction so it's much more easy for
someone to use without having to dive in
deep and write a very lengthy map reduce
code and those map and reduce codes can
take 70 80 lines of code when you can do
the same thing in one or two lines with
Hive or Pig this is the advantage Pig
has over Hive it can process uh only
structured data in Hive while in pig it
can process structured semi-structured
and unstructured data some other
features to note that separates the uh
different query languages is when we
look at map and reduce map reduce
supports partitioning features as does
Hive Pig no concept of partitioning in
pig it doesn't support your partitioning
feature your partitioning features allow
you to predition the data in such a way
that it can be queried quicker you're
not able to do that in pig map reduce
uses Java and python while Hive uses an
SQL like query language known as hiveql
or hql Pig Latin is used which is a
procedural data flow language map reduce
is used by programmers pretty much as
straightforward on Java Hive is used by
data analysts pig is used by researchers
and programmers certainly there's a lot
of mix between all three programmers
have been known to go in and use a hive
for quick query and anybody's been able
to use Pig for quick query or research
under map and reduce code performance is
really good under Hive code performance
is lesser than map and reduce and pig
under Pig Code performance is lesser
than map reduce but better than Hive so
if we're going to look at uh speed and
time the map reduce is going to have be
the fastest performing ments on all of
those where Pig will have second and
high follows in the back let's look at
components of pig pig has two main
components we have pig Latin Pig Latin
is the procedural data flow language
used in pig to analyze data it is easy
to program using Pig Latin it is similar
to SQL and then we have the runtime
engine runtime engine represents the
execution environment created to run Pig
Latin programs it is also a compiler
that produces map reduce programs uses
hdfs or your Hardo file system for
storing and retrieving data and as we
dig deeper into the pig architecture
we'll see that we have pig latin scripts
programmers write a script in pig Len to
analyze data using Pig then you have the
grunt shell and it actually says grunt
when we start it up and we'll show you
that here in a little bit which goes
into the pig server and this is where we
have our parser parser checks the syntax
of the pig script after checking the
output will be a dag directed a celic
graph and then we have an Optimizer
which optimizes um after your dag your
logical plan is passed to The Logical
Optimizer where an optimization takes
place finally the compiler converts the
dag into map reduced jobs and then that
is executed on the map reduce under the
execution engine the results are
displayed using dump statement and
stored in hdfs using store statement and
again we'll show you that um the kind of
end you always want to execute
everything once you've created it and so
dump is kind of our execution uh
statement and you can see right here as
we were talking about earlier once we
get to the execution engine and it's
coded into map reduce then the map
reduce processes it onto the
hdfs working of pig pig latin script is
written by the users so you have load
data and right Pig script and pig
operations so when we look at the
working of pig pig latin script is
written by the users there's step one we
load data and write Pig script and step
two in this step all the pig operations
are performed by parser Optimizer and
compiler so we go into the pig
operations and then we get to step three
execute ution of the plan in this stage
the results are shown on the screen
otherwise stored in the hdfs as per the
code so it might be of a small amount of
data you're reducing it to and you want
to put that on the screen or you might
uh be converting a huge amount of data
which you want to put back into the Hado
file system for other use let's take a
look at the pig latin data model the
data model of pig latin helps pig to
handle various types of data for example
we have Adam Rob or 50 adom represents
any single value of primitive data type
in pig left like integer float string it
is stored as a string tupal so we go
from our atom which our most basic
things so if you look at just Rob or
just 50 that's an atom that's our most
basic object we have in pig latin then
you have a tupal tupal represents
sequence of fields that can be of any
data type it is the same as a row in
rdbms for example a set of data from a
single row and you can see here we have
Rob comma 5 and you can imagine with
many of our other examples we've used
you might have uh the ID number the name
where they live their age their date of
starting the job that would all be one
row and stored as a tupal and then we
create a bag a bag is a collection of
tupal it is the same as a table in rdbms
and is represented by brackets and you
can see here we have our table with Rob
five Mike 10 and we also have a map a
map is a set of key value pairs key is
of character array type and a value can
be of any type it is represented by the
bracket and so we have name and age
where the key value is Mike and 10 Pig
Latin has a fully nestable data model
that means one data type can be nested
within another here's a diagram
representation of pig latin data model
and in this particular example we have
basically an ID number a name an age and
a place and we break this apart we look
at this model from Pig Latin perspective
uh we start with our field and if you
can remember a field contains basically
an atom it is one uh particular their
data type and the atom is stored as a
string which it then converts it into
either an integer number or character
string next we have our tupal and in
this case you can see that it represents
a row so our tupal would be 3 comma Joe
comma 29 comma California and finally we
have our bag which contains three rows
in it in this particular example let's
take a quick look at Pig execution modes
Pig Works in two execution modes
depending on where the data is reciting
and where the pig script is going to to
run we have local mode here the pig
engine takes input from the Linux file
system and the output is stored in the
same file system local mode local mode
is useful in analyzing small data sets
using Pig and we have the map reduce
mode here the pig engine directly
interacts and executes in hdfs and map
reduce in the map reduce mode queries
written in pig latin are translated into
map reduce jobs and are run on a Hadoop
cluster by default Pig runs in this mode
there are three modes in pig depending
on how a pig latin code can be written
we have our interactive mode batch mode
and embedded mode the interactive mode
means coding and executing the script
line by line when we do our example
we'll be in the interactive mode in
batch mode all scripts are coded in a
file with the extension. Pig and the
file is directly executed and then
there's embedded mode Pig lets its users
Define their own functions udfs in a
programming language such as Java so
let's take a look and see how this works
in a use case in this case case use case
Twitter users on Twitter generate about
500 million tweets on a daily basis the
Hadoop map reduce was used to process
and analyze this data analyzing the
number of tweets created by a user in
the Tweet table was done using map ruce
and Java programming language and you
can see the problem it was difficult to
perform map reduce operations as users
were not well-versed with written
complex Java codes so Twitter used
Apache pig to overcome these problems
and let's see how let's start with the
problem statement analyze the user table
and tweet table and find out how many
tweets are created by a person and here
you can see we have a user table we have
Alice Tim and John with their ID numbers
one two three and we have a tweet table
in the Tweet table you have your um the
ID of the user and then what they
tweeted uh Google was a good whatever it
was tennis dot do spacecraft Olympics
politics whatever they're tweeting about
the following operations were performed
for analyze the given data first the
Twitter data is loaded into the pig
storage using load command and you can
see here we have our data coming in and
then that's going into Pig storage and
this data is probably on an Enterprise
computer so this is actually active
Twitter's going on and then it goes into
Hado file system remember the Hado file
system is a data warehouse for storing
data and so the first step is we want to
go ahead and load it into the pig
storage into our data storage system the
remaining operations performed are shown
Below in join and group operation the
tweet and user tables are joined and
grouped using co-group command
and you can see here where we add a
whole column and we go from uh usernames
and tweet to the ID link directly to the
name so Alice was user one Tim was two
and John three and so now they're listed
with their actual tweet the next
operation is the aggregation the tweets
are counted according to the names the
command used is count so it's very
straightforward we just want to count
how many tweets each user is doing and
finally the result after the count
operation is joined with with the user
table to find out the username and you
can see here where Alis head 3 Tim 2 and
John 1 Pig reduces the complexity of the
operations which would have been lengthy
using map reduce in join and group
operation the tweet and user tables are
joined and grouped using co-group
command the next operation is the
aggregation the tweets are counted
according to the names the command used
is count the result after the count
operation is joined with the user table
to find out the username and you can see
we're talking about three lines of
script versus a map reduce code of about
80 lines finally we could find out the
number of tweets created by a user in a
simple way so let's go quickly over some
of the features of pig that we already
went through most of these uh first ease
of programming as Pig Latin is similar
to SQL lesser lines of code need to be
written short development time is the
code is simpler so we can get our
queries out rather quickly instead of
having to have a programmer spend hours
on it handles all kind of data like
structured semi-structured and
unstructured pig lets us create
userdefined functions Pig offers a large
set of operators such as join filter and
so on it allows for multiple queries to
process un parallel and optimization and
compilation is easy as it is done
automatically and
[Music]
internally so so enough Theory let's
dive in and show you a quick demo on
some of the commands you can do and pick
today's setup we'll continue as we have
in the last three demos to go ahead and
use cloud era quick start and we'll be
doing this in Virtual box we do have a
tutorial in setting that up you can send
a note to our simply learn team and then
get that link to you once your Cloud era
quick start has uh spun up and remember
this is virtual box we've created a
virtual machine and this virtual machine
is Centos Linux once it's spun up you'll
be in a full Linux system here and as
you can see we have a Thunderbird
browser which opens up to the Hadoop
basic system browser and we can go
underneath the Hue where it comes up by
default if you click on the pull down
menu and go under editor you can see
there's our Impala our Hive uh Pig along
with a bunch of other query languages
you can use and we're going under Pig
and then once you're in pig we can go
ahead and use our command line here and
just click that little blue button to
start it up and running we will actually
be working in terminal window and so if
you in the cloud era quick start you can
open up the terminal window up top or if
you're in your own setup and you're
logged in you can easily use all of your
commands here in terminal window and
we'll zoom in that we get a nice view of
what's going on there we go now for our
first command we're going to ahead and
do a Hadoop command and import some data
into the Hadoop system in this case a
pig input and let's just take a look at
this we have uh Hadoop now let it know
it's going to be a Hadoop command DFS
there's actually four variations of DFS
so if you have hdfs or you know whatever
that's fine all four of them Point used
to be different setups underneath
different things and now they all do the
same thing and we want to put this file
which in this case is under home Cloud
era documents and Sample and we just
want to take that and put it into the
pig input and let's take a look at that
file if I go under my document browsers
and open this up you'll see it's got a
simple ID name profession and age we
have one Jack engineer 25 and that was
in one of our earlier things we had in
there and so let's go ahead and hit
enter and execute this and now we've
uploaded that data and it's gone into
our Pig input and then a lot of the uh
Hadoop commands mimic the Linux commands
and so you'll see we have cat as one of
our commands or has a hyphen before it
so we execute that with Hadoop DFS
hyphen cat SL Pig and input CU that's
what we called it that's where we put
our sample CSV at and we execute this
you can see from our Hadoop system is
going to go in and pull that up and sure
enough it pulls out the data file we
just put in there and then we can simply
enter the pig Latin or Pig editor mode
by typing in Pig and we can see here uh
by our grunt I told you that's how it
was going to tell you you were in pig
latin there's our grunt command line so
we are now in the pig shell and then
we'll go ahead and put our load command
in here and the way this works is I'm
going to have office equals load and
here's my load in this case it's going
to be Pig input we have that in single
brackets you remember that's where the
data is in the Hado file system where we
dumped it into there we're going to
using Pig storage our data was separated
as with a comma so there's our comma
separator and then we have as and in
this case we have an ID character array
name character array profession
character array and age character array
and we're just going to do them all as
character arrays just to keep this
simple for this one and then when I hit
um put this all in here you can see
that's our full command line going in
and we have our semicolon at the end so
when I hit enter it's now set office up
but it hasn't actually done anything yet
it doesn't do anything until we do dump
office so there's our Command to execute
whatever we've loaded or whatever setup
we have in here and we run that you can
see it go through the different
languages and this is going through the
map reduce remember we're not doing this
locally we're doing this on the Hadoop
setup and once we finished our dump you
can see we have ID name profession age
and all the information that we just
dumped into our Pig oh we can now do
let's say um oh let's say we have a
request just for we'll keep it simple in
here but just for the name and age and
so we can go office we'll call it each
as our variable underscore each and
we'll say for each office generate name
comma H and for each means that we're
going to do this for each row and if
you're think thinking map reduce you
know that this is a map function uh
because it's mapping each row and
generating name and age on here and of
course we want to go ahead and close it
with a semicolon and then once we've
created our query or the um command line
in here let's go ahead and dump
officecore each in with our semicolon
and this will g go through our map
reduce uh setup on here and if we were
on a large cluster the same processing
time would happen in fact it's really
slow because I have multiple things on
this computer computer and this
particular virtual box is only using a
quarter of my processor it's only
dedicated to this and you can see here
there it is name and age and it also
included the top row since we didn't
delete that out of there or tell it not
to and that's fine for this examle but
you need to be aware of those things
when you're processing a significantly
large amount of data or any data and we
can also do um office and we'll call
this DSC for descending so maybe the
boss comes to you and says hey can we
order office by ID descending and of
course your boss you've taught him how
to uh your shareholder sounds a little
drug attor say boss you've talked to the
shareholder and you said uh and you've
taught them a little bit of Pig Latin
and they know that they can now create
office description and we can order
office by ID description and of course
once we do that we have to dump
officecore description so that it'll
actually execute and there goes into our
map reduce it'll take just a moment for
it to come up because again I'm running
on only a quarter of my process
processor and you can see we now have
our IDs in descending order returned
let's also look at and this is so
important with anytime you're dealing
with big data let's create office with a
limit and you could of course do any of
this instead of with office we could do
this with office descending so you get
just the top two IDs on there we're
going to limit just to two and of course
you execute that we have to dump
officecore limit you can just think of
dumping uh your garbage into the pig pin
for the pig to eat uh there we go dump
office limit two and that's going to
just limit our office to the top two and
for our output we get our first row
which had our ID name profession and age
and our second row which is Jack who's
an engineer Let's do an a filter we'll
call it
officecore filter you guessed it equals
filter office by profession equals and
keep note this is uh similar to how
python does it with the double equal
signs for equal for doing a true false
statement so for your logic statement
remember to use two equal signs in Pig
and we're going to say it equals doctor
so we want to find out how many doctors
do we have in our list and we'll go
ahead and do our dump we're dumping all
our garbage into the pig pen and we're
letting Pig take over and see what it
can find out and see who's a doctor on
our list and we find uh employee ID
number two Bob is a doctor 30 years old
for this next section uh we're going to
cover something we see a lot nowadays in
data analysis and that's word counting
to tokenization that is one of the next
big steps as we move forward in our data
analysis where we go from say stock
market analysis of highs and lows and
all the numbers to what are people
saying about companies on Twitter what
are they saying on the web pages and on
Facebook suddenly you need to start
counting words and finding out how many
words are total how many are in the
first part of the document and so on
we're going to cover a very basic word
count uh example and in this case I've
created a document called word row. text
and you can see here we have simply
learn is a company supporting online
learning simply learn helps people
attain their certifications simply learn
is an online community I love simply
learn I love programming I love data
analysis and I went and save this into
my documents folder so we could use it
and let me go ahead and open up a new uh
terminal window for our word count let
me go and close the old one so we're
going to go in here and instead of doing
this as Pig we're going to do pig minus
X local and what I'm doing is I'm
telling the pig to start the pig shell
but we're going to be looking at files
local to our virtual box or the sentos
machine and let me go ahead and hit
enter on there just maximize this up
there we go and it'll load Pig up and
it's going to look just the same as the
pig we were doing which was defaulted to
high to our Hadoop system to our hdfs
this is now defaulted to the local
system and now we're going to create
lines and we're going to load it
straight from the file remember last
time we took the hdfs and loaded it into
there and then loaded it into Pig since
we're going the local we're just going
to run a local script we have lines
equals load home the actual full path
home Cloud era documents and I called it
word rows. text and as line is a
character array so each line and I've
actually you can change this to read
each document I certainly have done a
lot of document analysis and then you go
through and do word counts and different
kind of counts in there so once we go
ahead and create our line instead of
doing the dump we're going to go ahead
and start entering all of our different
setups or each of our steps we want to
go through and let's just take a look at
this next one cuz the load is
straightforward we're loading from this
particular file since we're local it's
loading it directly from here instead of
going into the Hado file system and it
says as and then the each line is read
as a character array now we're going to
do words equal for each of the lines
generate Flat tokenize Line space as
word now there's a lot of ways to do
this this is if you're a program grammar
you're just splitting the line up by
spaces there's actual ways to tokenize
it you got to look for periods
capitalization there's all kinds of
other things you play with with this but
for the most basic word count we're just
going to separate it by spaces the uh
flatten takes the line and just creates
a uh it flattens each of the words out
so this is uh we're just going to
generate a bunch of words for each line
and then each each of those words is as
a word a little confusing in there but
if you really think about it we're just
going down each line separating it out
and we're generating a list of words one
thing to note is the default for
tokenize you can just do tokenize line
without the space in there if you do
that it'll automatically tokenize it by
space so you can do either one and then
we're going to do group we're going to
group it by words so we're going to
group words by word so when we we split
it up each token is a word and it's a
list of words and so we're going to
group equals group words by words so
we're going to group all the same words
together and if we're going to group
them then we want to go ahead and count
them and so for count we'll go ahead and
create a word count variable and here's
our four each so for each grouped
grouped is our line where we group all
the words in the line that are similar
we're going to generate a group and then
we're going to count the words for each
group so for each line where we group
the words together we're going to
generate a group and that's going to
count the words we want to know the word
count in each of those and that comes
back in our word count and finally we
want to take this and we want to go
ahead and dump word count and this is a
little bit more uh what you see when you
start looking at grunt scripts you'll
see right here these these lines right
here we have each of the steps you take
to get there so we load our file for
each of our um lines we're going to
generate and tokenize it into words then
we're going to take the words and we're
going to group them by same words uh for
each group we're going to generate a
group and we're just going to count the
words so we're going to summarize all
the words in here and let's go ahead and
do our dump word count which executes
all this and it goes through our map
reduce it's actually a local Runner
you'll see down here you start seeing
where they still have map reduce but is
a special Runner we're mapping it that's
a part of each row being counted and
grouped and then when we do the word
count that's a reducer the reducer
creates these keys and you can see I is
used three times a came up once n came
up once is two can continue on down here
to to attain online people company
analysis simply learn they took the top
rating with four uh certification so all
these things are then countered in the
how many words are used uh and in data
analysis this is probably the very the
beginnings of data analysis where you
might look at it and say oh they
mentioned love uh three times so
whatever's going on in this post it's
about love and uh what do they love and
then you might attach that to the
different objects in here so you can see
that uh Pig Latin is fairly easy to use
there's nothing really you know it may
it takes a little bit to learn the
script uh depending on how good your
memory is as I get older my memory leaks
a little bit more so I don't memorize it
as much but that was pretty
straightforward the script we put in
there and then it goes through the full
map reduced localized run comes out and
like I said it's very easy to use that's
why people like Pig Latin is because
it's intuitive one of the things I like
about Pig Latin is when I'm
troubleshooting when we're
troubleshooting a lot of times you're
working with a small amount of data and
uh you start doing one line at a time
and so I can go lines equal load and
there's my loaded text and maybe I'll
just dump lines and then it's going to
run it's going to show me all the lines
that I'm working on in the small amount
of data and that way I can test that if
I got an error on there that said oh
this isn't working maybe I'll be like oh
my gosh I'm in map reduce or I'm in the
basic um grunt shell instead of the
local path grunt uh so maybe it'll
generate an error on there and you can
see here it just shows each of the lines
going down Hive versus pig on one side
we'll have our sharp Stinger on our
black and yellow friend and on the other
side our thick hide on our Pig let's
start with an introduction to H Bas back
in the days data used to be less and was
mostly structured you see we have
structured data here we usually had it
like in a database where you had uh
every field was exactly the correct
length so if you had a name field it was
exactly 32 characters I don't me
remember the old access database in
Microsoft uh the files are small if we
we had you know hundreds of people in
one database that was considered Big
Data this data could be easily stored in
relational database or
rdbms and we talk about relational
database uh you might think of Oracle
you might think of SQL Microsoft SQL
MySQL all of these have evolved even
from back then to do a lot more today
than they did but they still fall short
in a lot of ways and they're all
examples of an rdms or relationship
database then internet Evol
and huge volumes of structured and
semi-structured data got generated and
you can see here with the
semi-structured data we have email if
you look at my spam filter you know what
we're talking about all the HTML Pages
XML which is a lot of time is displayed
on our HTML and help desk Pages Json all
of this really has just even in the last
each year it almost doubles from the
year before how much of this is
generated so storing and processing this
data on an rdbms has become a major
problem and so the solution is we use
Apache hbase Apache hbas was the
solution for this let's take a look at
the history the hbas history and we look
at the hbas history we're going to start
back in 2006 November Google released
the paper on big table and then in 2017
just a few months later h-based
prototype was created as a Hadoop
contribution later on in the Year 2007
in October first usable H base along
with the Hado .15 was Rel leas and then
in January of 2008 hbas became the sub
project of Hadoop and later on that year
in October all the way into September
the next year hbase was released the 81
version the 0.19 version and 0.20 and
finally in May of 2010 hbas became
Apache top level project and so you can
see in the course of about four years
hbas started off as just an idea on
paper and has evolved all the way till
2010 as a solid project under the Apache
and since 2010 is continued to evolve
and grow as a major source for storing
data in semi-structured data so what is
hbas hbas is a column oriented database
management system derived from Google's
nosql database big table that runs on
top of the Hadoop file system or the
hdfs it's an open- source project that
is horizontally scalable and that's very
important to understand that you don't
have to buy a bunch of huge expensive
computers you're expanding it by
continually adding commodity machines
and so it's a linear cost expansion as
opposed to being exponential no SQL
database written in Java which permits
faster querying uh so Java is the
backend for the hbase setup and it's
well suited for sparse data sets so it
can contain missing or na values and
this doesn't Boggle it down like it
would another database companies using
hbase so let's take a look and see who
is using this uh nosql database for
their servers and for storing their data
and we have Horton Works which isn't a
surprise because they're one of the like
Cloud Dera Horton Works they are behind
had doop and one of the big developments
and backing of it and of course Apache
hbas is the um open source behind it and
we have Capital One as Banks you also
see Bank of America where they're
collecting information on people and
tracking it uh so their information
might be very sparse they might have one
Bank way back when they collected
information as far as the person's
family and what their income for the
whole family is and their personal
income and maybe another one doesn't
collect the family income as he starts
saying where you have data that is uh
very difficult to store where it's
missing a bunch of data hubs spots using
it Facebook uh certainly all of your
Facebook Twitter most of your social
medias are using it and then of course
there's JP Morgan Chase and Company
another bank that uses the hbas as their
data warehouse for nosql let's take a
look at an hbas use case case so we can
dig a little bit more into it to see how
it functions telecommunication company
that provides mobile voice and
multimedia Services across China the
China mobile and China mobile they
generate billions of call detailed
records or CDR and so these CDs and all
these records of these calls and how
long they are and different aspects of
the call maybe the tower they're
broadcasted from all that is being
recorded so they can track it a
traditional database systems were unable
to scale up to the vast volumes of data
and provide a cost effective solution no
good so storing in real-time analysis of
billions of call records was a major
problem for this company solution Apache
hbas hbas stores billions of rows of
detailed call records hbas performs fast
processing of Records using SQL queries
so you can mix your SQL and nosql
queries and usually just say no SQL
queries because of the way the query
Works applications of hbas one of them
would be in the medical industry hbas is
used for storing genome sequences
storing disease history of people of an
area and you can imagine how sparse that
is as far as both of those a genome
sequence might be only have pieces to it
that each person is unique or is unique
to different people and the same thing
with disease you really don't need a
column for every possible disease a
person could get you just want to know
what those diseases those people have
had to deal with in that area e-commerce
hbas is used for restoring logs about
customer search history performs
analytics and Target advertisement for
Better Business insights sports hbas
stores match details in the history of
each match uses this data for better
prediction so when we look at hbas we
all to know what's the difference
between hbas versus
rdbms that is a relational database
management system hbas versus rdbms so
the hbas does not have a fixed schema
it's schema less defines only column
fames and we'll show you what that means
later on an rdbms has a fixed schema
which describes the structure of the the
tables and you can think of this is you
have a row and you have columns and each
column is a very specific structure how
much data can go in there and what it
does with the hbas it works well with
structured and semi-structured data with
the rdbms it works only well with
structured data with the AG base it can
have denormalized data it can contain
missing or null values with the rdbms it
can store only normalized data now you
can still store a null value in the
rdbms but it still takes up the same
space as if you're storing a regular
value in many cases and it also for the
h Bas is built for wide tables it can be
scaled horizontally for instance if you
were doing a tokenizer of words and word
clusters you might have 1.4 million
different words that you're pulling up
and combinations of words so with an
rdbms it's built for thin tables that
are hard to scale you don't want to
store 1.4 million columns in your SQL
it's going to crash and it's going to be
very hard to do searches with the hbase
it only stores that data which is part
of whatever row you're working on let's
look at some of the features of the
hbase it's scalable data can be scaled
across various nodes as it is stored in
the hdfs and always think about this
it's a linear add-on for each terabyte
of data I'm adding on roughly $1,000 in
commodity Computing with an Enterprise
machine uh we're looking at about 10,000
at the lower end for each terabyte of
data and that includes all your backup
and redundant so it's a big difference
it's like a tenth of the cost to stored
across the hbas it has automatic failure
support right ahead log across clusters
which provides automatic support against
failure consistent read and write hbas
provides consistent read and write of
the data it's a Java API for client
access provides easy to ous Java API for
clients block cache and Bloom filters so
the hbas supports block cashing and
Bloom filters for high volume query
optimization let's dig a little deeper
into the h H Bas storage hbas column
oriented storage and I told you we're
going to look into this to see how it
stores the data and here you can see you
have a row key and this is really one of
the important references is each row has
to have its own key or your row ID and
then you have your column family and in
here you can see we have column family 1
column Family 2 column family 3 and you
have your column qualifiers so you can
have in column family one you can have
three columns in there and there might
not be any data in that so when you go
into column family one and do a qu for
every column that contains a certain
thing that row might not have anything
in there and not be queried where in
column Family 2 maybe you have column
one filled out and column three filled
out and so on and so forth and then each
cell is connected to the row where the
data is actually stored let's take a
look at this at what it looks like when
you fill the data in so in here we have
a row key with a row ID and we have our
employee ID 1 two 3 that's pretty
straightforward you probably would even
have that on an SQL server and then you
have your column family this is where it
starts really separate operating out
your column family might have personal
data and under personal data you would
have name City age you might have a lot
more than just that you might have
number of children you might have degree
all those kinds of different things that
go under personal data and some of them
might be missing you might only have the
name and the age of an employee you
might only have the name the city and
how many children and not the age and so
you can see with the personal data you
can now collect a large variety of data
and stored in the H Bas very easily and
that maybe you have a family of
professional data your designation your
salary all the stuff that the employee
is doing for you in that company let's
dig a little deeper into the hbas
architecture and so you can see here
what looks to be a complicated chart
it's not as complicated as you think
from the Apache hbas we have the
Zookeeper which is used for monitoring
what's going on and you have your hm
this is the hbas master assigns regions
and load balancing and then underneath
the region or the hbas master then under
the HM or hbase Master you have your
Reger server serves data for read and
write and the region server which is all
your different computers you have in
your Hado cluster you'll have a region
an hlog you'll have a store memory store
and then you have your different files
for H file that are stored on there and
those are separated across to different
computers and that's all part of the
hdfs storage system so when we look at
the Architectural Components or regions
and we're looking down we're drilling
down a little bit hbas tables are
divided horizontally by row so you have
a key range into regions so each of
those IDs you might have IDs 1 to 20 21
to 50 or whatever they are regions are
assigned to the nodes in the cluster
called region servers a region contains
all rows in the table between the region
start key and the End Key again 1 to 10
11 to 20 and so forth these servers
serve data for read and write and you
can see here we have the client and the
git and the git sends it out and it
finds out where that starts if it's
between which start keys and in keys and
then it pulls the data from that
different region server and so the
region sign data definition language
operation create delete are handled by
the hmaster so the hmaster is telling it
what are we doing with this data what's
going out there assigning and
reassigning regions for Recovery or load
balancing and monitoring all servers so
that's also part of it so you know if
your IDs if you have 500 IDs across
three servers you're not going to put
400 IDs on server one and 100 on the
server two and leaves Region Three and
region four empty you're going to split
that up and that's all handled by the
hmaster and you can see here it monitors
region servers assigns regions to region
servers assigns regions to region
servers and so forth and so forth hbas
has a distributed environment where
hmaster alone is not sufficient to
manage everything hence zookeeper was
introduced it works with hmaster so you
have an an active hmaster which sends a
heartbeat signal to zookeeper indicating
that it's Act Ive and the Zookeeper also
has a heartbeat to the region servers so
the region servers send their status to
zookeeper indicating they are ready for
read and right operation inactive server
acts as a backup if the active hmaster
fails it'll come to the rescue active
hmaster and region servers connect with
a session to zookeeper so you see your
active hmas selection region server
session they're all looking at the
Zookeeper keeping that pulse an active
hm region server connects with a session
to the Zookeeper and you can see here we
have ephemeral noes for active sessions
via heartbeats to indicate that the
region servers are up and running so
let's take a look at uh hbas read or
write going on there's a special h-based
catalog table called The Meta table
which holds a location of the regions in
the cluster here's what happens the
first time a client reads or writes data
to hbase the client gets the region
server the host the meta table from
zookeeper and you can see right here the
client has a request for your region
server and goes hey zookeeper can you
handle this and the zookeeper takes a
look at it and goes ah meta location is
stored in Zookeeper so it looks at his
metad data uh on there and then the
metadata table location is sent back to
the client the client will query The
Meta server to get the region server
corresponding to the row key if it wants
to access the client caches this
information along with the metat table
location and you can see here the client
going back and forth to the region
server with the information and it might
be going across multiple region servers
depending on what you're quering so we
get the region server for row key from
The Meta table that's where that row key
comes in and says hey this is where
we're going with this and so once it
gets a row key from the corresponding
region server we can now put row or get
Row from that region server let's take a
look at the hbas meta table special hbas
catalog table that maintains a list of
all the region servers in the hbas
storage system so you see here we have
the meta table we have a row key and a
value table key region region server so
the meta table is used to find the
region for the given table key and you
can see down here you know our meta
table comes in it's going to figure out
where it's going with the region server
and we look a little closer at the WR
mechanism in hbas we have WR ahead log
or wall as you abbreviate it kind of a
way to remember wall is WR ahead log
it's a file used to store new data that
is yet to be put on permanent storage it
is used for Recovery in the case of
failure so you can see here where the
client comes in and it literally puts uh
the new data coming in into this kind of
temporary storage or the wall on there
once it's gone into the wall then the
memory store MIM store is a right cache
that stores a new data that has not yet
been written to dis there is one MIM
store per column family per region and
once we've done that we have three act
once the data is placed in MIM store the
client then receives the acknowledgment
when the MIM store reaches the threshold
it dumps or commits the data into H file
and you can see right here we've taken
our our it's gone into the wall the wall
then sorts it into the different memory
stores uh and and then the memory stores
it says Hey we've reached we're ready to
dump that into our H files and then it
moves it into the H files H files store
the rows of data as stored key value on
disk so here we've done a lot of theory
let's dive in and just take a look and
see what some of these commands look
like and what happens in our age base
when we're manipulating a no SQL
[Music]
setup
so if you're learning a new setup it's
always good to start with where is this
coming from it's open source by Apache
and you can go to hp. apache.org and
you'll see that it has a lot of
information you can actually download
the hbas separate from the Hadoop
although most people just install the
Hadoop because it's bundled with it and
if you go in here you'll find a
reference guide and so you can go
through the Apache reference guide and
there's a number of things to look at
but we're going to be going through
through Apache hbas shell that's what
we're going to be working with and
there's a lot of other interfaces on the
setup and you can look up a lot of the
different commands on here so we go into
the Apache hbas reference guide we can
go down to read hbas shell commands from
a command file you can see here where it
gives you different options of formats
for putting the data in and listing the
data certainly you can also create files
and scripts to do this too but we're
going to look at the basics and we're
going to go through this on a basic uh
hbase shell and one last thing to look
at is of course if you continue down the
setup you can see here where they have
more detail as far as how to create and
how to get to your data on your hbase
now I will be working in a virtual box
and this is by Oracle you can download
the Oracle virtual box you can put a
note in below for the YouTube as we did
have a previous session on setting up
virtual setup to run your Hadoop system
in there I'm using the cloud era quick
start installed in here there's hens you
can also use the Amazon web server
there's a number of options for trying
this out in this case we have cladera on
the Oracle virtual box the virtual box
has Linux Centos installed on it and
then the Hadoop it has all the different
Hadoop flavors including hbas and I
bring this up because my computer is a
Windows 10 the operating system of the
virtual box is Linux and we're looking
at the hbas data warehouse and so we
have three very different entities all
running on my computer and that can be
confusing if it's a first time in and
working with this kind of setup now
you'll notice in our Cloud Dera setup
they actually have some hbas monitoring
so I can go underneath here and click on
hbas and master and it'll tell me what's
going on with my region servers it'll
tell me what's going on with our backup
tables right now I don't have any user
tables because we haven't created any
and this is only a single node and a
single hbase tour so you're not going to
expect anything too extensive in here
since this is for practice and education
and perhaps testing out package you're
working on it's not for really you can
deploy Cloud a of course but when you
talk about a quick start or a single
node setup that's what it's really for
so we can go through all the different
hbase and you'll see all kinds of
different information with zookeeper if
you saw a flash by down here what
version we're working in since zookeeper
is part of the hbas setup where we want
to go is we want to open up a terminal
window and in Cloud Dera it happens to
be up at the top and when you click on
here you'll see your Cloud era terminal
window open and let me just expand this
so we have a nice full screen and then
I'm also going to zoom in that way you
have a nice big picture and you can see
what I'm typing and what's going out on
and to open up your agbase shell simply
type hbas shell to get in and hit enter
and you'll see it takes just a moment to
load and we'll be in our hbas shell for
doing hbas commands once we've gotten
into our hbas shell you'll see you'll
have the hbas prompt information ahead
of it we can do something simple like
list this is going to list whatever
tables we have and it so happens that
there's a base table that comes wish H
Bas now we can go ahead and create and
I'm going to type in just create what's
nice about this is it's going to throw
me kind of a uh it's going to say hey
there's no just straight create but does
come up and tell me all these different
formats we can use for create so we can
create our table and one of our families
you can add splits names versions all
kinds of things you can do with this but
let's just start with a very basic one
on here and let's go ahead and create
and we'll call it new table uh let's
just call it new TBL for table new table
and then we also I want to do let's do
knowledge so let's take a look at this
I'm creating a new table and it's going
to have a family of knowledge in it and
let me hit enter and it's going to come
up it's going to take it a second to go
ahead and create it now we have our new
table in here and so if I go list you'll
now see table and new table so you can
now see that we have the new table and
of course the default table that's set
up in here and we can do something like
uh describe we can describe and then
we're going to do new TBL and when we
describe it it's going to come up it's
going to say hey name I have knowledge
data block en coding none Bloom filter
row replication scope version all the
different information you need new
minimum version zero forever deleted
cells false block size in memory and you
can look this stuff up on apache.org to
really track it down one of the things
that's important to note is versions so
you have your different versions of the
data that's stored and that's always
important to understand that we might
talk about that a little bit later on
and then after describe it we can also
do a uh St status the status says I have
one active Master going on that's our H
Bas as a whole we can do status summary
I should do the same thing as status so
we got the same thing coming up and now
that we've created let's go ahead and
put something in it so we're going to
put new TBL and then we want Row one you
know what before I even do this let's
just type in put and you can see when I
type in put it gives us like a lot of
different options of how it works and
different ways of formatting our data as
it goes in and all of them usually begin
with the new table new TBL then we have
in this case we'll call it Row one and
then we'll have knowledge if you
remember we created knowledge already
and we'll do knowledge Sports and then
in knowledge and sports we're going to
set that equal to Cricket so we're going
to put underneath this uh our knowledge
setup that we have a a thing called
Sports in there and we'll see what this
looks like in just a second let's go
ahead and put in we'll do a couple of
these let's see let's do another row one
and this time set of sports let's do
sign
you know this person not only you know
we have Row one which is both
knowledgeable in cricket and also in
chemistry so it's a chemist who plays
Cricket in row one and uh let's see if
we have let's do another row one just to
keep it going and we'll do science in
this case let's do physics not only in
chemistry but also physicist I have
quite a joy in physics myself so here we
go we have U Row one there we go and
then let's do row two let's see what
that looks like when we start putting in
row two and in row two this person is
has knowledge in economics this is a
master of business and how maybe it's
Global economics maybe it's just for the
business and how it fits in with the
country's economics and we'll call it
macroeconomics so I guess it is for the
whole country there so we have knowledge
economics macroeconomics and then let's
just do one more we'll keep it as row
two and this time our Economist is also
a musician so we'll put music and they
happen to have and they enjoy oh let's
do pop music they're into the current
pop music going on so we've loaded our
database and you'll see we have two rows
Row one and row two in here and we can
do is we can list the contents of our
database by simply doing scan uh scan
and then let's just do scan by itself so
you can see how that looks you can
always just type in there and it tells
you all the different setups you can do
with scan and how it works in this case
we want to do scan new TBL and in our
scan new TBL we have Row one row one row
two row two and you'll see Row one has a
column called knowledge science time
step value crickets value physics so it
has information as when it was created
when the Tim stamp is Row one also has
knowledge Sports and a value of cricket
so we have sports and Science and this
is interesting because if you remember
up here we also gave it originally we
told it to come in here and have
chemistry we had science chemistry and
science physics and we come down here
here I don't see the chemistry why
because we've now replaced chemistry
with physics so the new value is physics
on here let me go ahead and clear down a
little bit and in this we're going to
ask the question is enabled new table
and when I hit enter in here you're
going to see it comes out true and then
we'll go ahead and disable it let's go
ahead and disable new table make sure I
have our quotes around it and now that
we've disabled it what happens when we
do the scan when we do the scan new
table I hit enter you're going to see
that we get an error coming up so once
it's disabled you can't do anything with
it until we reenable it now before we
enable the table Let's do an alteration
on it and here's our new table and this
should look a little familiar cuz it's
very similar to create we'll call this
test info and we'll hit enter in there
it'll take just a moment for updating
and then we want to go ahead and enable
it so let's go ahead and enable our new
table so it's back up and running and
then we want to describe describe new
table and we come in here you'll now see
we have name knowledge and under there
we have our data encoding and all the
information under knowledge and then we
also have down below test info so now we
have the name test info and all the
information concerning the test info on
here and we'll simply enable it new
table so now it's enabled oops I already
did that I guess we'll enable it twice
and so let's start looking at while we
had scan new table and you can see here
where it brings up the information like
this what if we want to go ahead and get
a row so we'll do our 1 and when we do
hbas R1 you can see we have knowledge
science and it has a timestamp value
physics and we have knowledge Sports and
it has a timestamp on it and value
cricket and then let's see what happens
when we put into our new table and in
here we want Row one and if you can
guess from earlier cuz we did something
similar uh we're going to do knowledge
economics and then it's going to be
instead of uh I think it was what uh
macroeconomics is now Market economics
and we'll go back and do our get command
and now see what it looks like and we
can see here where we have knowledge
economics it has a timestamp value
market economics physics and Cricut and
this is because we have uh economics
science and sports those are the three
different columns that we have and then
each one has different information in it
and so if you managed to go through all
these commands and look at Basics on
here you'll now have the ability to
create a very basic AG base setup nosql
setup based on your columns and your
rows and just for fun we'll go back to
the cloud Dera where they have the
website up for the hbase master status
and I'll go ahead and refresh it and
then we can go down here and you'll see
user tables table set one and we can
click on details and here's what we just
did it goes through uh so if you're the
admin looking at this and go oh someone
just created new TBL and this is what
they have underneath of it in their new
table what is data science let's start
with some of the common definitions
that's doing the rounds some say that
data science is a powerful new approach
for making discoveries from data others
term it as an automated way to analyze
enormous amounts of data and extract
information from it still others refer
to it as a new discipline which combines
aspects of Statistics mathematics
programming and visualization to gain
insights now that you have looked at
some of its definitions let's learn
learn more about data
science when domain expertise and
scientific methods are combined with
technology we get data science which
enables one to find solutions for
existing
problems let's look at each of the
components of data science separately
the first component is domain expertise
and scientific methods data scientists
should also be domain experts as they
need to have a passion for data and
discover the right patterns in them
traditionally domain experts like
scientists and statisticians collected
and analyzed data in a laboratory setup
or a controlled environment the data was
then subject to relevant laws or
mathematical and statistical models to
analyze the data set and derive relevant
information from it for instance they
use the models to calculate the mean
median mode standard deviation and so on
of a data set it help them test their
hypothesis or create a new one in the
next slide we will see how data science
technology has now made this process
faster and more efficient but before we
do that let's understand the different
types of data analysis an important
aspect of data science data analysis can
either be descriptive where one studies
a data set to explain what happened or
be predictive where one creates a model
based on existing information to predict
the outcome and behavior it can also be
prescriptive where one suggests the
action to be taken in a given situation
using the collected
information we now have access to tools
and techniques that process data and
extract the information we need for
instance there are data processing tools
for data wrangling we have new and
flexible programming languages that are
more efficient and easier to use with
the creation of operating systems that
support multiple OS platforms it's now
easier to integrate systems and process
Big Data application designs and
extensive software libraries help
develop more robust scalable and
datadriven
applications data scientists use these
Technologies to build data models and
run them in an automated fashion to
predict the outcome efficiently this is
called machine learning which helps
provide insights into the underlying
data they can also use data science
technology ology to manipulate data
extract information from it and use it
to build tools applications and services
but technological skills and domain
expertise alone without the right
mathematical and statistical knowledge
might lead data scientists to find
incorrect patterns and convey the wrong
information now that you have learned
what data science is it will be easier
to understand what a data scientist does
data scientists start with a question or
a business problem
then they use data acquisition to
collect data sets from The Real World
the process of data wrangling is
implemented with data tools and modern
technologies that include data cleansing
data manipulation data Discovery and
data pattern
identification the next step is to
create and train models for machine
learning they then design mathematical
or statistical
models after designing a data model it's
represented using data visualization
Tech
techniques the next task is to prepare a
data
report after the report is prepared they
finally create data products and
services let us now look at the various
skills a data scientist should have data
scientists should ask the right
questions for which they need domain
expertise the Curiosity to learn and
create Concepts and the ability to
communicate questions effectively to
domain experts data scientists should
think analytically to understand
understand the hidden patterns in a data
structure they should Wrangle the data
by removing redundant and irrelevant
data collected from various sources
statistical thinking and the ability to
apply mathematical methods are important
traits for a data
scientist data should be visualized with
graphics and proper storytelling to
summarize and communicate the analytical
results to the audience to get these
skills they should follow a distinct
road map it's important they adopt the
required tools and techniques like
Python and its libraries they should
build projects using real world data
sets that include data.gov NYC open data
Gap minder and so on they should also
build datadriven applications for
Digital Services and data
products scientists work with different
types of data sets for various purposes
now that big data is generated every
second through different media the role
of data science has become more
important so you need to know what big
data is and how you are connected to it
to figure out a way to make it work for
you every time you record your heartbeat
through your phone's biometric sensors
post or tweet on The Social Network
create any blog or website switch on
your phone's GPS Network upload or view
an image video or audio in fact every
time you log into the internet you are
generating data about yourself your
preferences and your lifestyle big data
is a collection of these and a lot more
data that the world is constantly
creating in this age of the internet of
things or iot big data is a reality and
a need big data is usually referenced by
3vs volume velocity and variety volume
refers to the enormous amount of data
generated from various sources big data
is also characterized by velocity huge
amounts of data flow at a tremendous
speed from different devices sensors and
applications to deal with it an
efficient and timely data processing is
required variety is the third V of Big
Data because big data can be categorized
into different formats like structured
semistructured and unstructured
structured data is usually referenced to
as rdbms data which can be stored and
retrieved easily through
sqls semi-structured data are usually in
the form of files like XML Json
documents and nosql database text files
images videos or multimedia content are
examples of unstructured data in short
big data is a very large information
database usually stored on distributed
systems or machines popularly referred
to as Hadoop clusters but to be able to
use this database we have to find a way
to extract the right information and
data patterns from it that's where data
science comes in
data science helps to build information
driven
Enterprises let's go on to see the
applications of data science in
different sectors social network
platforms such as Google Yahoo Facebook
and so on collect a lot of data every
day which is why they have some of the
most advanced data centers spread across
the world having data centers all over
the world and not just in the US help
these companies serve their
International customers better and
faster without any network latency they
also help them deal effectively with the
enormous amount of data so what do all
these different sectors do with all this
big data their team of data scientists
analyze all the raw data with the help
of modern algorithms and data models to
turn it into information they then use
this information to build Digital
Services data products and information
driven webs now let's see how these
products and services work we'll first
look at LinkedIn let's suppose that you
are a data scientist based in New York
city so it's quite likely that you would
want to join a group or build
connections with people related to data
science in New York City now what
LinkedIn does with the help of data
science is that it looks at your profile
your posts and likes the city you are
from the people you are connected to and
the groups you belong to then it matches
all that information with its own
database to provide you with information
that is most relevant to you this
information could be in the form of news
updates that you might be interested in
Industry connections or professional
groups that you might want to get in
touch with or even job postings related
to your field and designation these are
all examples of data services let's now
look at something that we use every day
Google's search engine Google search
engine has the most unique search
algorithm which allows machine learning
models to provide relevant search
recommendations even as the user types
in his or her query this feature is
called
autocomplete it is an excellent example
of how powerful machine learning can be
there are several factors that influence
this feature the first one is query
volume Google's algorithms identify
unique and verifiable users that search
for any particular keyword on the web
based on that it builds a query volume
for instance Republican debate
2016 Ebola threat
CDC or the center of Disease Control and
so on are some of the most common user
queries another important factor is a
geographical location the algorithms tag
a query with the locations from where it
is generated this makes a query volume
location specific it's a very important
feature because this allows Google to
provide relevant search recommendations
to its user based on his or her location
and then of course the algorithms
consider the actual keywords and phrases
that the user types in it takes up those
words and crawls the web looking for
similar instances the algorithms also
try to filter or scrub out inappropriate
content for instance sexual violent or
terrorism related content hate speeches
and legal cases are scrubbed out from
the search
recommendations but how does data
science help you today even the
healthcare industry is beginning to tap
into the various applications of data
science to understand this let's look at
weable devices these devices have
biometric sensors and a built-in
processor to gather data from your body
when you are wearing them they transmit
this data to the big data analytics
platform via the iot Gateway ideally the
platform collects hundreds of thousands
of data points and the collected data is
ingested into the system for further
processing
processing the big data analytics
platform applies data models created by
data scientists and extracts the
information that is relevant to you it
sends the information to the engagement
dashboard where you can see how many
steps you want what your heart rate is
over a period of Time how good your
sleep was how much calories you burned
and so on knowing such details would
help you to set personal goals for a
healthy lifestyle and reduce overall
healthare and insurance costs it would
also help your doctor record your vitals
and diagnose any
issue the finance sector can easily use
data science to help it function more
efficiently suppose a person applies for
a loan the loan manager submits the
application to the Enterprise
infrastructure for processing the
analytics platform applies data models
and algorithms and creates an engagement
dashboard for the loan manager the
dashboard would show the applicant's
credit report credit history amount if
approved and risks associated with him
or her the loan manager can now easily
take a look at all the relevant
information and decide whether the loan
can be approved or
not governments across different
countries are gradually sharing large
data sets from various domains with the
public this kind of transparency makes
the government seem more Trust worthy it
provides the country data that can be
used to prepare itself for different
types of issues like climate change and
Disease Control it also helps encourage
people to create their own digital
products and services the US government
hosts and maintains data.gov a website
that offers information about the
federal government it provides access to
over 195,000 data sets across different
sectors the US government has kicked off
a number of strategic initiatives in in
the field of data science that includes
us digital service and open data we have
seen how data science can be applied
across different sectors let's now take
a look at the various challenges that a
data scientist faces in the real world
while dealing with data sets data
quality the quality of data is mostly
not up to the set standards you will
usually come across data that is
inconsistent inaccurate incomplete not
in the desirable format and with anomaly
integration data integration with
several Enterprise applications and
systems is a complex and painstaking
task unified platform data is
distributed to Hadoop distributed file
system or hdfs from various sources to
ingest process analyze and visualize
huge data sets the size of these Hadoop
clusters can vary from few nodes to
thousand nodes the challenge is to
perform analytics on these large data
sets efficiently and
effectively this is where python comes
into play with its powerful set of
libraries functions modules packages and
extensions python can efficiently tackle
each stage of data analytics that
includes data acquisition python
libraries such as Scrappy comes handy
here data wrangling python data frames
are very efficient in handling large
data sets and makes data wrangling
easier with its powerful
functions explore matplot lib libraries
are very rich when it comes to data
exploration model scikit learns
statistical and mathematical functions
to help to build models for machine
learning visualization modern libraries
such as bokeh creates very intuitive and
interactive
visualization its huge set of libraries
and functions make big data analytics
seem easy and hence solves the bigger
problem
python applications and programs are
portable and helps them scale out on any
big data
platform python is an open-source
programming language that lets you work
quickly and integrate systems more
effectively now that we have talked
about how the python libraries help the
different stages of data analytics let's
take a closer look at these libraries
and how they support different aspects
of data science numpy or numerical
python is the fundamental package for
scientific Computing scipi is the core
of scientific Computing libraries and
provides many userfriendly and
efficiently designed numerical
routines matplot lib is a python 2D
plotting Library which produces
publication quality figures in a variety
of hard copy formats and interactive
environments across
platforms scikit learn is built on numpy
scipi and Matt plot lib for data mining
and data analysis is pandas is a library
providing high performance easy to ouse
data structures and data analysis tools
for
python all these libraries modules and
packages are open source and hence using
them is convenient and
easy there are numerous factors which
positions python well and makes it the
tool for data science python is easy to
learn it's a general purpose function
and objectoriented programming language
as python is an open- Source programming
language it is readily available easy to
install and get started it also has a
large presence of open- source Community
for software development and
support Python and its tools enjoy
multiplatform support applications
developed with Pyon integrate easily
with other Enterprise systems and
applications there are a lot of tools
platforms and products in the market
from different vendors as they offer
great report and
services Python and its libraries create
unique combinations for data science
because of all these benefits it's
usually popular among academicians
mathematicians statisticians and
technologists python is supported by
well-established data platforms and
processing Frameworks that help it
analyze data in a simple and efficient
way Enterprise Big Data platform cloud
era is the Pioneer in providing
Enterprise ready Hadoop Big Data
platform and supports python Horton
works is another Hadoop Big Data
platform provider and supports python
map ruce map R is also committed to
Python and provides the Hadoop Big Data
platform big data processing framework
map ruce spark and Flink provides very
robust and unique data processing
framework and support python Java Scala
and python languages are used for big
data processing
framework but to access Big Data you
have to use a big data platform which is
a combination of the Hadoop
infrastructure also known as Hadoop
distributed file system or hdfs and an
analytics platform Hadoop is a framework
that allows data to be distributed
across clusters of computers for faster
cheaper and efficient Computing it's
completely developed and coded in Java
one of the most popular analytics
platforms is spark it easily integrates
with
hdfs it can also be implemented as a
standalone analytics platform and
integrated with multiple data sources it
helps data scientists perform their work
more efficiently spark is built using
Scala since there is a disparity in the
programming language that data
scientists use and that of the Big Data
platform it impedes data access and
Flow
as python is a data scientist's first
language of choice both Hadoop and Spark
provide python apis that allow easy
access to the Big Data platform
consequently a data scientist need not
learn Java or Scala or any other
platform specific data languages and can
instead focus on performing data
analytics there are several motivations
for python Big Data Solutions
big data is a continuously evolving
field which involves adding new data
processing Frameworks that can be
developed using any programming language
moreover new innovation and research is
driving the growth of Big Data Solutions
and platform providers it would be
difficult for data scientists to focus
on analytics if they have to constantly
upgrade themselves on information or
under the hood architecture or
implementation of the platform therefore
it's important to keep the entire data
science platform and any language
agnostic to simplify a data scientist's
job consequently almost all major
vendors solution providers and data
processing framework developers are
providing python apis this allows a data
scientist to perform big data analytics
using only python rather than learning
other languages like Java or Scala to
help them work on the big data platform
let's look at an example and understand
how data is stored AC acoss hadoop's
distributed clusters big data is
generated from different data sources a
large file usually greater than 100
megabytes gets routed from a name node
to data nodes name nodes hold the
metadata information about the files
stored on data nodes it stores the
address and information of a block of
file and the data node associated with
it data nodes hold the actual data
blocks the file is split into multiple
smaller files usually of 64 me gabt or
128 mgab size it's then copied to
multiple physical servers the smaller
files are also called file blocks one
file block gets replicated to different
servers the default replication factor
is three which means a single file block
gets copied at least three times on
different servers or data nodes there is
also a secondary name node which keeps a
backup of all the metadata information
stored on the main or primary node this
node can be used if and when the main
name node
fails now that you have understood a
little about hdfs let's look at the
second core component of Hadoop M ruce
the primary framework of the hdfs
architecture a file is split into three
blocks as split zero split one and split
two whenever request comes in to
retrieve the information the map or task
is executed on each data node that
contains the file blocks the mapper
generates an output essentially in the
form of key value pairs that are sorted
copied and merged once the mapper task
is complete the reducer works on the
data and stores the output on
hdfs this completes the map reduce
process let's discuss the map reduced
functions mapper and reducer in detail
the mapper Hop ensures that mappers run
locally on the nodes which hold a
particular portion of the data to avoid
the Network traffic multiple mappers run
in parallel and each mapper processes a
portion of the input data the input and
output of the mapper are in the form of
key value pairs note that it can either
provide zero or more key value pairs as
output the reducer after the map phase
all intermediate values for an
intermediate key are combined into a
list which is given to a reducer all
values associated with a particular
intermediate key are directed to the
same reducer this step is known as
Shuffle and sort there may be a single
reducer or multiple reducers note that
the reducer also provides outputs in the
form of zero or more than one final key
value pairs these values are then
returned to hdfs the reducer usually
emits a single key value pair for each
input
key you have seen how map produce is
critical for hdfs to function a good
thing is you don't have to learn Java or
other Hadoop Centric languages to write
a map rued program you can easily run
such Hadoop jobs with a code completely
written in Python with the help of
Hadoop streaming
API Hadoop streaming acts like a bridge
between your python code and the javab
based hdfs and let you seamlessly access
Hadoop clusters and execute map reduced
tasks
you have seen how map ruce is critical
for hdfs to function thankfully you
don't have to learn Java or other Hadoop
Centric languages to write a map reduced
program you can easily run such Hadoop
jobs with a code completely written in
Python shown here are some userfriendly
python functions that are written for
the mapper class suppose we have the
list of numbers we want to square we
have the square function defined as
shown on the
screen we can call the map function with
a list and a function which is to be
executed on each item in that list the
output of this process is as shown on
the
screen reducer can also be written in
Python here we would like to sum the
squared numbers of the previous map
operation this can be done using the sum
operation as shown on the
screen we can now call the reduce
function with the list of data which is
to be aggregated and aggregator function
in our case sum is used for this
purpose Big Data analysis requires a
large infrastructure Cloud era provides
Enterprise ready Hadoop Big Data
platform which supports python as well
to execute Hadoop jobs you have to first
install Cloud era it's preferable to
install Cloud era's virtual machine on a
Unix system as it functions test on it
to set up the cloud era hop environment
visit the cloud era link shown here
select quick start download for CDH 5.5
and VMware from the drop-down lists
click the download now
button once the VM image is downloaded
please use szip to extract the files to
download and install it visit the link
shown on
screen Cloud AA VMware has some system
prerequisites the 64-bit virtual machine
requires a 64-bit host operating system
or Os and a virtualization product that
can support a 64-bit guest
OS to use a VMware VM you must use a
player compatible with workstation 8.x
or higher such as player 4.x or higher
or Fusion 4.x or
higher you can use older versions of
workstation to create a new VM using the
same virtual disc or VM DK file but some
features in VMware tools will be
unavailable the amount of ram required
will vary depending on the runtime
option you
choose to launch the VMware Player you
will either need VMware Player for
Windows and Linux or VMware Fusion for
Mac so please visit the VMware link
shown on screen to to download the
relevant VMware Player Now launch the
VMware Player with the Cloudera VM the
default username and password is cloud
era click the terminal icon as shown
here it will launch the Unix terminal
for Hadoop hdfs
interaction to verify that the Unix
terminal is functioning correctly type
in PWD which will show you the present
working directory you can also type in
LS space hyphen LRT to list all the
current files folders and directories
these are some simple unix commands
which will come in handy later while you
are implementing map reduced
tasks you have seen how the Hadoop
distributed file system works along with
map ruce the data is written on and read
by diss map ruce jobs require a lot of
disk read and write operations which is
also known as dis iio or or input and
output reading and writing to a dis is
not just expensive it can also be slow
and impact the entire process and
operation this is specifically true for
iterative processes Hadoop is built for
right once read many type of jobs which
means it's best suited for jobs that
don't have to be updated or accessed
frequently but in several cases
particularly in analytics and machine
learning users need to write and rewrite
commands to access and compute on the
same data more than once every time such
a request is sent out map reduce
requires that data is read and or
written onto diss directly note that
though the time to access or write on
diss is measured in milliseconds when
you are dealing with large file sizes
the time Factor gets compounded
significantly this makes the process
highly timec
consuming in contrast Apache spark uses
resilient distribut Ed data sets or rdds
to carry out such
computations rdds allow data to be
stored in memory which means that every
time users want to access the same data
a dis IO operation is not required they
can easily access data stored in the
cache accessing the cache or Ram is much
faster than accessing diss for instance
if dis access is measured in
milliseconds in memory data access is
measured in sub milliseconds this
radically reduces the overall time taken
for iterative operations on large data
sets in fact programs on spark run at
least 10 to 100 times faster than on M
ruce that's why spark is gaining
popularity among most data scientists as
it is more time efficient when it comes
to running analytics and machine
learning
computations one of the main differences
in terms of Hardware requirements for
map reduce and Spark is is that while
map reduce requires a lot of servers and
CPUs spark additionally requires a large
and efficient
Ram let's understand resilient
distributed data sets in detail as you
have already seen the main programming
approach of spark is
rdd rdds are fault tolerant collections
of objects spread across a cluster that
you can operate on in parallel they are
called fault tolerant because they can
automattic Al recover from machine
failure you can create an rdd either by
copying the elements from an existing
collection or by referencing a data set
stored externally say on an
hdfs rdds support two types of
operations Transformations and actions
Transformations use an existing data set
to create a new one for example map
creates a new rdd containing the results
after passing the elements of the
original data set through a function
some other examples of Transformations
are filter and join actions compute on
the data set and return the value to the
driver program for example reduce
Aggregates all the rdd elements using a
specified function and returns this
value to the driver program some other
examples of actions are count collect
and
save it's important to note that if the
available memory is insufficient then
spark writes the data to this
here are some of the advantages of using
spark it's almost 10 to 100 times faster
than Hadoop ma ruce it has a simple data
processing framework it provides
interactive apis for python that allow
faster application
development it has multiple tools for
complex analytics operations these tools
help data scientists perform machine
learning and other analytics much more
efficiently and easily than most
existing
tools it can easily be integrated with
the existing Hadoop
infrastructure py spark is the python
API used to access the spark programming
model and perform data
analysis let's take a look at some
transformation functions and action
methods which are supported by py spark
for data
analysis these are some common
transformation
functions map returns rdd formed by
passing data elements from The Source
data set
filter returns rdd based on selected
criteria flatmap Maps items present in
the data set and returns a
sequence Reduce by key returns key value
pairs where values for each key is
aggregated by a given reduceed
function let's now look look at some
common action functions collect returns
all elements of the data set as an
array count Returns the number of
elements present in the data
set first Returns the first element in
the data
set take Returns the number of elements
as specified by the number in the
parentheses spark context or SC is the
entry point to spark for the application
and must be available at all times for
data
processing there are mainly four
components in spark tools spark SQL it's
mainly used for querying the data stored
on hdfs as a resilient distributed data
set or rdd in spark through integrated
apis in Python Java and
Scala spark streaming it's very useful
for data streaming process and where
data can be read from various data
sources ml lib it's mainly used for
machine learning processes such as
supervised and unsupervised
learning graph x it can be used to
process or generate graphs with
rdds let's set up the Apache spark
environment and also learn how to
integrate spark with Jupiter notebook
first visit the Apache link and download
aache spark to your system now use 7zip
software and extract the files to your
system's local
directory to set up the environment
variables for spark first set up the
user
variables click new and then enter spark
home in the variable name and enter the
spark installation path as variable
value now click on the path and then
click new and enter the the spark bin
path from the installed directory
location now let's set up the P spark
notebook specific
variables this will integrate The Spark
engine with Jupiter notebook type in py
spark it will launch a Jupiter notebook
after a while create a python notebook
and type in SC command to check the
spark context covering the Hadoop
ecosystem at least a very fundamental of
all the different parts that are in the
Hadoop ecosystem and it's very robust
it's grown over the years with different
things added in there's a lot of
overlapping in a lot of these tools but
we're just going to cover these basic
tools so you can see what's available in
the Hadoop ecosystem so let's go back to
our Hadoop ecosystem as you can see we
have all our different setup and let's
focus on the Hadoop part of it first
before we look at the different tools we
start with the Hadoop or hdfs is for
data storage write once read many times
you can store a lot of data on it
affordably distributed file system and
so we talk about the Hado file system it
stores different formats of data on
various machines and so you have like a
huge cluster of computers and you're
able to store Word documents
spreadsheets structured data
non-structured data semi-structured and
in the Hadoop file system there's the
two different sets of servers in there
there's the name node which is a master
we talked about that that's your
Enterprise computer and the other
component is your data nodes and so
you'll usually have like I said one to
two you'll have a name node and maybe a
backup name node and then you'll have as
many data nodes as you want and you can
just keep adding them that's what makes
it so affordable you know you have a
rack of computers and you go oh I need
more space you just add another Rack in
so it's very affordable and very easy to
expand and the way the Hado file system
itself Works behind the hood is it
splits the data into multiple blocks by
default it's 128 megabytes and the 120
megabytes it is a default setting you
can change that that works for most data
there's reasons for either processing
speed or for better distribution of the
data so if you have little tiny blocks
of data that are less than 128 megabytes
if you have a lot of those you might
want to go down in size and vice versa
for larger blocks uh and you can see
right here we have 300 megabytes and it
takes that piece of data and it just
divides it into blocks of data and each
one's 128 128 and 44 which if you had to
together equals 300 megabytes and now
that you understand the Hadoop file
system or at least the basic overview of
it it's important to note what it sits
on what's actually making all this work
on the back end and this is yarn Hadoop
yarn is a cluster Resource Management so
it's how it manages this whole cluster
right here that we just looked at and
yarn stands for yet another resource
negotiator love the title it reminds me
of an Iron Man movie with you know Tony
Starks and Jarvis just a rather
intelligent system or whatever it stood
for but yarn has become very widely used
and it's actually used as a backend for
a lot of other packages so it's not just
in Hadoop but Hadoop is where it came
from and where it set up and there's
some other ones another popular one is
uh msos which I'll mention again briefly
and so the yarn it handles the cluster
of noes it's the one that when you hear
yarn it's someone that's going Hey
where's our Ram at where's our hard
drives at or if you have a solid state
disc drive your SD where's that at how
much memory do I have what can I put
where and so so here we go nice image of
it RAM memory resources so it's
allocating all these different resources
for different applications is what it's
doing when we talk about the back to the
two major components the two major
components is your resource manager
that's on the master server or your
Enterprise computer and then that one is
in control and managing what's going on
with all of your nodes data processing
in Hado map reduce and we're going to
talk about the map reduce here in just a
second the Hadoop data processing
is all built upon map reduce map reduce
processes a large volumes of data in
perly distributed Manner and this is
very core to Hadoop but before we go on
because there's other tools out there
and things are slowly shifting and
there's all kinds of new things one of
the things you want to look at is not
just how the map reduce works but start
thinking map reduce one of the best
pieces of advice I had from a uh one of
my mentors in data science was think map
reduce this is really what you should be
learning but it is an actual process in
the Hadoop system so we have our big
data and the Big Data Maps out and so
this is the first step is if I'm looking
at my data how do I map that data out
what am I looking at and it could be
something as simple as I just loaded
into um you know I'm just looking at one
line at a time but it could be that I'm
looking at the data one line at a time
but I only need columns one and four
maybe I'm looking at it one column at a
time but I need the total of column one
one added together and column one over
column 2 so you can start to see get
some very complicated mapping here but
the mapping is what do you do with each
line of data each piece of data uh if
you're in a spreadsheet it's easy to see
you have a row whatever you do to that
row that's what you're mapping because
it doesn't look at anything else it
doesn't know anything else all it knows
is what's on that row if you're looking
at documents maybe it's pulling one
document at a time and so your map is
then a document and then it takes those
and we Shuffle and sort them how do we
sort them around whether you're grouping
them together whether you're taking the
whatever information you mapped out of
it word counts you're counting the word
a so letter A comes out with how many
for each mapping if you have 54 A's per
the one document 53 in the other ones
that's what's coming out of that mapping
and going into the shuffle and sort and
so if it's counting A's and B's and C's
it'll Shuffle and sort all the A's
together all the B's together if you're
running a big data for running
agriculture apples and oranges so puts
all the apples in one all the stuff you
mapped out that you said said hey these
are all apples these are all oranges
let's shuffle them and sort them
together and then we reduce and reduce
so each of these groups reduce into the
data you want out so you have map
Shuffle sort reduce and some important
things to know about the Hado file
system because I'm going to mention
spark in a little bit is a Hado file
system manages to do a lot of this by
writing it to the hard drive so if
you're running a really low budget which
nobody does anymore with the Hado file
system and you have all your commodity
machines are all low they only have a 8
GB memory instead of 128 GB this process
uses the hard drive and so it it pulls
it into the RAM for your mapping it Maps
a one piece of data then it writes that
map to the hard drive then it takes that
mapping from the hard drive loads it up
and shuffles it and writes it back to
the hard drive to a different spot and
then takes that information and starts
processing it in the reduce and then it
writes the reduce answer to the hard
drive it runs slower cuz you're
accessing to and from your hard drive or
solid state drive if you have an SD card
in there but you can also utilize it's a
lot more affordable you know it's like I
said you having that higher end of ram
cost even on a commodity machine so you
can save a lot of money nowadays it's so
affordable people run the spark setup on
there which does the same thing but in
Ram and again we'll talk about spark in
just a little bit and of course the
final thing is an output so again your
reducer written to the hard drive and
then your reduce is brought together to
form one output what is the maximum
number of oranges sold per an area I
don't know I'm making that up one of the
things about Hadoop is it covers so many
different things that anything you can
think of you can put in Hado the
question is do you need to do you have
enough data or enough need for the
high-end processing so again look at
mapa reduce but think of this not just
as map reduce start thinking map and
reduce when you think big data we're
going to start getting into the tools
because you had all those pictures of
all those Cool Tools we have so we're
going to look at the first two of those
tools and uh in the tools we have scoop
and Flume and this is for your data
collection and ingestion we're going to
bring spark up in just a second can also
play a major role in this cuz spark is
its own animal that's sprung from Hadoop
connects into Hadoop but it can run
completely independent but scoop and
Flume are specific to Hadoop and their
ways to bring in information into the
Hadoop file system and so when we talk
about these we'll start with scoop scoop
is used to transfer data between Hadoop
and external data stores such as
relational databases and Enterprise data
warehouses and so you can see right here
here's our Hadoop data and it's
connecting up there and it's either
pushing the data or pulling the data and
we have our relational database and
Enterprise data warehouse and there's
that magic word Enterprise that means
these are the servers that are very
high-end so maybe you have a high-end
SQL server or MySQL server or whatever
over there and this is what's coming and
going from the scoop it Imports data
from external data stores into the hdfs
hive and H base those are two specific
The Hive setup is your SQL basically and
you can see a nice little image here
here's somebody on their laptop which
would be considered the um client
machine putting together their code they
push the scoop the scoop goes into the
task manager the task manager then goes
hey what have I got for the hbas And
Hive system in our Hadoop system and
then it reaches out to the Enterprise
data warehouse or into the document
based system or relationship based
database relationship is your non-sql
document is what it sounds like is you
have HTML documents or Word documents or
text documents and it's able to map
those tasks and then bring those into
the Hadoop file system now Flume is a
distributed service for collecting
aggregating and moving large amounts of
log data so kind of focused a little bit
on a slightly different set of data
although you'll find these overlap a lot
you can certainly use Flume to do a lot
of things that you can do in scoop and
vice versa Flume ingests the data so
we're looking at like say adjacent call
to a website XML documents um
unstructured and semi-structured datas
most commonly digested by Flume best
example I saw was a Twitter account
pulling Twitter feeds into a Hadoop
system so it ing justs online streaming
data from social media Twitter log files
so we want to know what's going on with
error codes on your servers and all
those log files web server what's going
on in your web server we can bring all
this stuff in and just dump it into the
Hadoop data file system to be looked at
and processed later and it's a web
server cloud social media data again all
those different sources it can be it's
kind of endless you know it just depends
on what your company needs versatility
of Hadoop is what makes it such a
powerful source to add into a company
and so it comes in there you have your
Source it goes through the channels it
then goes through kind of a sync feature
to makees sure everything is in sync and
then it dumps it into the Hadoop file
system so we've covered in the Hadoop
file system the first two things let's
look at some of the scripting languages
they have and so we we have the two here
and you can also think of these um it
actually says scripting and SQL queries
a lot of times they're both referred to
as queries so you have both the Pig and
the hive and pig is used to analyze data
in Hado it provides a highlevel data
processing language to perform numerous
operations on the data and it's made out
of pig Latin language for scripting Pig
Latin compiler converts Pig Latin code
to execute code and then you have your
ETL the ETL provides a platform for
building data flow for ETL and ETL is
like the catch three letters now on any
job interview I look at it says ETL it
just means extract transfer and load so
all we're doing is extracting the data
transferring it to where we need it and
then loading it into in this case a Hado
file system and there's other pieces to
that you know it's actually a big thing
because whenever you're extracting data
you want to dump all the data or do you
want to do some kind of pre-processing
so you're only bringing in what you want
and then one of the cool things about
Pig Latin is 10 lines of pig latin
script is around 200 lines of map reduce
job again the map reduce is the backend
processes that go on so if we have pig
and I I'll be honest with you pig is
very easy to use but as a scripter
programmer I find it's more for people
who just need a quick pull of the data
and able to do some very basic things
very easily so if you're doing some very
high-end processing model building
usually end up in something else so
Pig's great for that like if you're in
the management you need to build a quick
query report pig is really good for that
and so it definitely has its place it's
definitely a very useful uh script to
know and the pig latin scripts they call
it the grunt shell I guess it goes with
pig CU they grunt you have your pig
server have a parser an Optimizer a
compiler an execution engine and that's
all part of the Apache Pig and this
thing goes into the map reduce which
then goes into the Hadoop file system in
the Hadoop um and you'll see Apache with
a lot of these because it's under the
open source hadoop's under the Apache
open source so all this stuff is you'll
see under Apache with the name tag on
there and if you're going to know Pig
Hive is the other one that's really
popular for easy query Hive facilitates
Reading Writing and managing large data
sets reciting in the distributed storage
using SQL Hive query language and this
is important because a lot of times
you're coming from an SQL Server there's
your Enterprise up and now you're
archiving a history of what's going on
on that server so you're continually
pulling data using scoop onto your um
into your hbase hive database and as it
comes in there it'd be nice to just use
that same query to pull the data out of
the hadu file system and that's exactly
what it does so you have hi of command
line you can also use a JBC or odbc
driver um and those drivers like if
you're working in Java or you're working
in Python you can use those drivers to
access Hive so you don't have have to go
through the high of command line but it
makes it real quick I can take a high of
command line and just punch in a couple
words and pull up my data and so it
provides user Define functions UDF for
data mining document indexing log
processing again anything that is in
some kind of SQL format if it's stored
that properly on the Hado file system
you can use your hive to pull it out and
you see even a more robust image of
what's in the hive there's your client
machine that's you on your laptop or
you're logged in wherever you're at at
writing your script that goes into the
driver where you have your compiler your
Optimizer executor you also have your
jdbc odbc connections which goes into
the high Thrift server which then goes
into the driver your hive web interface
so you can just log in on the web and
start typing away your SQL commands and
that again goes to the driver and all
those pieces and those pieces go to your
job tracker your name node and your
actual Hadoop file system so spark
realtime data analysis spark is an open-
Source distributed computer shooting
engine for processing and analyzing huge
volumes of realtime data so it runs 100
times faster than map reduce map reduce
is the basics of the Hadoop system which
is usually set up in a Java code in
spark the map reduce instead of running
um what happens in map reduce is it goes
pulls it into the ram writes it to the
hard drive reads it off the hard drive
shuffles it around writes it back to the
hard drive pulls it off the hard drive
does his processing and you get the
impression it's going in and out of ram
to the hard drive and back up again so
if you don't have a lot of RAM and you
have older computers and you don't have
the ram to process something then spark
and hop are going to run the same speed
otherwise spark goes hey let's keep this
all on the RAM and run faster and that's
what we're talking about is provides
inmemory computation of data so it's
fast you can see the guy running through
the door there speedy versus a very slow
Hadoop wondering around and it's used to
process and analyze realtime streaming
data such as stock market and baking
data so the spark can have its own stuff
going on and then it can access the
Hadoop database it can pull data just
like scoop and Flume do so it has all
those features in it and uh you can even
run spark with its own yarn outside of
Hadoop there's also spark on msos which
is another resource manager although
yarn is most commonly used and currently
spark pretty much comes installed with
Hadoop and so your spark running on top
of Hadoop will use the same nodes it has
its own manager and it utilizes the ram
so you'll have both you can have the
spark on top of there and here we go you
have your driver program your spark
context it goes into your cluster
manager your yarn then you have your
worker nodes which are going to be
executing tasks and cash and it's
important to remember when you're
running the spark setup even though
these are commodity machines we're still
usually a lot of them are like 128 GB of
RAM so that spark can run these high-end
processes certainly if it's not accessed
very much and you're building a um had
cluster you could drop that Ram way down
if you're doing just queries without any
kind of processing on there and you're
not doing a lot of queries but you know
generally spark you're looking at higher
end uh there's still commodity machines
to do your work and now we get into the
Hadoop machine learning and so machine
learning is its own animal I keep saying
that these are all kind of unique things
mahout is being phased out I mean people
still use it it's still important if you
can write your basic machine learning
that has most of the tools in there you
certainly can do it in mahout mahout
again writes to the hard drive reads
from the hard drive you can do all that
in spark and it's faster because it's
just in Ram and so spark has its own
machine learning tools in it and you can
also use py spark which then accesses
all the different python tools and
there's there's just so many tools you
can dump into spark mahout is very
limited but it's also very basic which
in itself can be really good sometimes
simple is better so myow is used to
create scalable and distributed machine
learning algorithms so so here we have
our mahout environment it builds a
machine learning application so you're
doing linear regression you're doing
clustering you're doing classification
models it has a library that contains
inbuilt algorithms for all of these and
then it has collaborative filtering and
again there's our classification and
there's our clustering regression so you
have your different machine learning
tools we can usually classify a large
amount of data using mahout so this
brings us to the next set of tools we
have or the next tool which is the
Apache ambari abar is an open- Source
tool responsible for keeping track of
running applications and their statuses
think of this as like a traffic cop U
more like a security guard you can open
it up and see what's going on and so the
Apache Andi manages monitors and
Provisions Hadoop clusters provides a
central Management Service to start stop
and configure Hadoop services so again
it's like a traffic cop hey stop over
there start keep going hey what's going
on over that area in the through Lane on
the high speed freeway and you can open
this up and you have a really easy view
of what's going on and so you can see
right here you have the umari web the
umari web is what you're looking at
that's your interface that you've logged
into the ambari server this will be
usually on the master node it's usually
if you're going to install ambari might
will just install it on the same node
and it connects up to the database and
then it has agents and each agent takes
a look and see what's going on with its
host server and if you're going to have
uh two more systems and we talked about
spark streaming there's also CFA and
Apache for streaming data coming in and
cfa's distributed streaming platform to
store and process streams of records and
it's written in Scala builds realtime
screaming data pipelines that reliably
get data between applications builds
realtime screaming applications that
transforms data into streams so it kind
of goes both ways on there so CFA uses a
messaging system for transferring data
from one application to another so we
have our sender we have our message CU
and then we have our receiver pretty
straightforward very solid setup on
there with the cafka patchy storm storm
is a processing engine that processes
realtime streeting at a very high speed
and it's written in closure they utilize
the function based programming style of
closure and it's based on list to give
it the speed that's why Apache storm is
built on there so you could think of
Kafka as a slow moving very solid
communication Network where Storm is
looking at the real- time data and
grabbing that streaming data that's
coming in fast so has ability to process
over a million jobs in a fraction of
seconds on a node so it's massive it can
really reach out there and grab the data
and it's integrated with the dupe to
harness higher Thorp puts so this is the
two big things about storm if you are
polling I think they mention stock
coming in or something like that where
you're looking for the latest data
popping up storm is a really powerful
tool to use for that so we've looked at
a couple more tools for bringing data in
we probably should talk a little bit
about security security has in Hadoop
has the Apache Ranger and Apache Knox
are the two most popular one and the
ranger the Apache Ranger Ranger is a
framework to enable Monitor and manage
data Securities across the Hadoop
platform so the first thing it does is
it provides centralized Security
Administration to manage all security
related tasks the second thing it does
is it has a standardized authorization
across all Hadoop components and third
it uses enhanced support for different
authorization methods it's role based
Access Control attribute based Access
Control Etc so your patchy Ranger can go
in there and your administrator coming
in there can now very easily monitor who
has what rights and what they can do and
what they can access so there's also
Apache nox nox is an application Gateway
for interacting with the rest apis and
the uis of Hadoop developers and so we
have our application programmer
interfaces our user interfaces and we
talk about rest apis this means we're
pulling this is looking at the actual um
data coming in what applications are
going on so if you have an an
application where people are pulling
data off of the hup system or pushing
data into the dup system the nox is
going to be on that setup and it
delivers three groups of user facing
Services one proxy Services provides
access to Hadoop via proxying the HTTP
request two authentication Services
authentication for rest API access and
web SSO flow for user interfaces so
there's our rest and finally Client
Services client development can be done
with the scripting through DSL or using
the KNX shell classes so the first one
is if you have a website coming in and
out your HTTP request again your three
different services are what's coming in
and out of the Hado file system so
there's a couple of the security setups
let's go ahead and take a look at
workflow system the uzi and there's some
other ones out there Uzi is a one that's
specific dead doop there's also like a
zookeeper out there and some other ones
Uzi is pretty good Uzi is a workflow
scheduler system to manage Hadoop jobs
and so you have a workflow engine and a
coordinator engine so it consists of two
parts what's going on and coordinating
what's going on and uh directed asli
graph dags which specifies a sequence of
actions to be executed these consist of
workflow jobs triggered by time and data
available and If you're not familiar
with directed Asic graphs or dags or
whatever terminology you want to throw
at this this is basically a flowchart
you start with process a then the next
process would be process B when process
a is done and it might be that process C
can only be done when process d e and f
are done so you want to be able to
control this you don't want it to um
process the machine learning script and
then pull the data in that you want to
process it on you want to make sure it's
going in the right order so these
consist of workflow jobs and they're
triggered by time and data availability
so maybe you're pulling stocks in the
middle of the night and once the stock
is all pulled so there's our time
sequence it says Hey they've been posted
on the other websites they post them
usually after the stock market closes
the highs and lows and everything then
once that data has been brought in you
know on a Time specific bring the data
in a certain time once the data is
available then we want to trigger our
machine learning script for what's going
to happen next and so we can see here we
have a start our map reduce program our
action node and it begins and either we
have a success then we notify the client
of success usually an email sent out in
successful completion or we don't have a
success we have an error notify client
of error email action node kill
unsuccessful termination and usually at
this point they say email action
notification but I'm mostly that's
usually a pager system and so you see
all the tech guys running to the server
room or wherever you know we're our
pager just went off we got to figure out
what went down you know the other one is
you just look at the next morning you go
oh let's make sure everything went
through this morning and check all your
successes with the eror usually set to
your pager and your emergency uh call to
open up in the middle of the night and
log in so that concludes our basic setup
with the Hadoop ecosystem so a quick
recap on the Hadoop ecosystem we covered
going looking at the middle part we had
the Hadoop as a file system and how it
stores data across multiple servers
saves money because it's about a tenth
of the cost of using Enterprise uh
computers we looked at yarn cluster
resource management and how that works
to hold everything together and then we
looked at a lot of dat processing how
does it process in and out of the Hadoop
System including the map and redu setup
which is the Hadoop basic in Java and
for that we looked at data collection
and ingestion with scoop and Flume we
looked at queries using the scripting
language Pig and the SQL queries through
Hive we glanced at spark remember spark
usually comes installed now at the
Hadoop system because it does so much of
its own processing it covers a lot of
the data in real-time data analysis
setup on there we looked at M out
machine learning we looked at Apache
ambari for management and monitoring
kind of your security guard and traffic
control uh just like we have scoop and
Flume which brings data in there we
looked at Kafka and Apache storm which
is for streaming data and then we looked
at Apache Ranger and Apache KNX for
security and finally we went in through
the we took a glance at Uzi for your
workflow system as we conclude our big
data Hardo full course we hope you have
gained valuable insights and practical
knowledge that will Empower you in the
world of data processing and Analytics
remember the field of big data is
dynamic and never evolving so stay
curious and keep learning if you found
this course helpful don't forget to like
share and subscribe for more in-depth
tutorials and Tech insights thank you
for joining on this Learning Adventure
and until next time happy coding staying
ahead in your career requires continuous
learning and upskilling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you you
covered explore our impressive catalog
of certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here