Welcome to our big data crash course
where we embark on an insightful journey
into the world of data at an
unprecedented scale we'll cover what big
data is introduce you to the top 7 Big
Data Technologies explain hdfs and dive
into the Hadoop ecosystem we'll also
showcase some exciting data engineering
projects it's a practical journey into
the realm of data Made Simple for
everyone to understand and use so let's
get started so if you are interested in
mastering the world of data engineering
look no further than our post-graduate
program in data engineering this
comprehensive course is tailor made for
professionals like you diving deep into
the essential topics such as Hadoop
framework spark based data processing
Kafka driven data pipelines and the
intricities of managing big data on AWS
and Azure Cloud infrastructures a unique
approach Blends live session Hands-On
industry projects exciting IBM
hackathons and Interactive ask me
anything sessions to provide you with
the most enriching learning experience
elevate your data engineering skills and
career prospects today for admission to
this data engineering course a
bachelor's degree with an average of 50
or higher marks is required are two plus
years of work experience is preferred
and basic understanding of object
oriented programming is preferred so
enroll now hi I am Assad Shah from
Canada and I recently upskilled myself
with the professional certification
program in data engineering offered by
simply learned in collaboration with
Purdue University after working for a
long time in SQL domain moving to Big
Data was a great challenge for me I
needed to upgrade my skills to improve
my performance in my current Workhorse
curriculum is well formulated while
industrial relevant Concepts and project
which helped increase deeper knowledge
about big data now I can easily carry
out my big data products as well as
successfully lead a team of Engineers I
even got a decent salary hike
the wall is moving at a much faster Pace
than we think make sure you don't lay
behind so upskill yourself and move to a
step forward and step closer to your
dreams
a civil engineer turned data scientist
who decided to Simply learn why are you
possible
with a course from a premier University
[Music]
true champion who upskilled to win big
how big a massive hike that transformed
my life Danish change gears pretty early
in the race but Russian wanted to
explore more to get ahead isilio snicara
simply learned from mechanical
engineering to a data analyst and a
podcaster in his free time Sr career
transformation
industry experts live
expert
s
difficult
with a well-structured course it felt
like a piece of cake that is simply
awesome what's also awesome is that no
salt
nitin didn't choose a quick fix
he just added data science into the mix
nitin how did you change the game worked
on real industry problems to become the
real deal a joint family a regular job
responsibilities but nothing could stop
nithin from getting ahead what an
all-rounder de Hoya Knight with flexible
learning you can always make it right
bashing your situations
you too will find your way to get ahead
when you simply there shortcuts simply
learn get ahead with simply learn
[Music]
what is Big Data
big data is extremely large or complex
set of data and it's so large that it's
difficult to process it using
traditional database and software
techniques
every day we are creating approximately
2.5 quintillion bytes of data
so where is this huge amount of data
getting generated from
earlier we had mobile phones with the
functionality of calling and text
messages or clicking some pictures maybe
but with the new technologies like
smartphones we have a lot of
applications for music Sports social
media like Facebook Twitter LinkedIn and
many more
also data is getting generated when we
shop online
so why does it need attention
as the data is growing companies are
capturing the data that streams into
their businesses
they can apply analytics and get
significant value from it with better
speed and efficiency
companies are leveraging the benefits of
big data by analyzing the patterns and
Trends and predicting something useful
out of it
for example companies like Amazon and
Netflix use big data to improve customer
experience
as we see here from the statistics shown
by 2020 1.7 megabytes of data will be
created every second for each human
this needs immediate attention because
this data can't be just thrown away
it's going to give profit to the
businesses
data challenges
data is not just about the volume of
data it poses other challenges as well
like velocity and variety
as a volume 40 zettabytes of data will
be created by 2020. this huge volume of
data is either human generated like from
social media YouTube or can be machine
generated like through sensors and
personal health trackers
and can also be generated with
organizations like credit card details
commercial transactions and medical
records
another challenge is velocity
data is coming into the system the data
needs to be processed with faster speed
and then there is variety of data data
is not only structured but unstructured
and semi-structured data like images
videos and tweets
so how are Enterprises using this big
data today
let us see big data popular use cases
Internet of Things
these are numerous ways in which
analytics can be applied to Internet of
Things
for example sensors are used to collect
data that can be analyzed to achieve
actionable insights tracking customer or
product movement Etc
many Enterprises are creating a
dashboard application that provides a
360 degree view of the customers that
pulls data from a variety of sources
analyzes it and presents it to customer
service
so that allows them to gather the rich
insights about businesses
Big Data popular use cases are related
information security and data warehouse
optimizations
Big Data tools are being used to remove
some of the burdens from the data
warehouses
even the healthcare industry is looking
for patterns and treatment that lead to
the best outcomes for patients
the main challenge of big data is
storing and processing the data at a
specified time span
the traditional approach is not
efficient in doing that
so Hadoop technology and various Big
Data tools have emerged to solve the
challenges faced in the Big Data
environment
so there are a lot of Big Data tools and
all of them help the user in some or
another way in Saving Time money and
uncovering business insights
these can be divided into the following
categories like data storage and
management for example the nosql
databases such as mongodb Cassandra
neo4j and hbase or popular nice nosql
databases
the talent Hadoop Microsoft HD insight
and zookeeper are popular for data
storage and management tools
next broad category is data cleaning
data needs to be cleaned up and well
structured
examples of such tools which help in
defining and reshaping the data into
usable data sets are Microsoft Excel and
open refine
data mining is a process of discovery
insights within a database
some of the popular tools used for data
mining are teradata and rapidminer
data visualization tools are a useful
way of conveying the complex Data
Insights in a pictorial way that is easy
to understand
for example Tableau and IBM Watson
analytics and plotly are the common
tools
for data reporting power bi tools are
used data ingestion is the process of
getting the data into Hadoop forward
which can be done using scoop Flume or
storm
data analysis requires asking questions
and finding the answers in data
the popular tools used for data analysis
are Hive Pig mapreduce and Spark
data acquisition is also used for
acquiring the data for which scoop Flume
or storm tools are quite popular
the popular Big Data tools offer a lot
of advantages which can be summarized as
follows
they provide the analysts with Advanced
analytics algorithm and models they help
the user to run on Big Data platforms
such as Hadoop or any high performance
analytics systems
they help the user to work not only with
structured data but unstructured and
semi-structured data coming from
multiple sources
and it's quite easy to visualize the
analyze data in a form that helps in
conveying the complex Data Insights in a
pictorial way which is easy to
understand by users
Big Data tools help you to integrate
with other Technologies very easily
thank you so much for listening to the
video in today's world we create a
massive amount of information every day
to understand and use all this data
companies use Advanced Technologies in
this video on top Big Data Technologies
in 2024 we will learn about the top Big
Data technologies that are changing how
we analyze data these tools help us
process data and use it for the things
like machine learning from managing and
processing data to conducting
sophisticated machine learning analysis
these Technologies play an important
role in using the immense power of data
they are like the tools in a data
scientist toolbox enabling businesses to
make informed decisions optimize
operations and gain a Competitive Edge
so let's get started with this video but
before we begin let me ask you a quick
question so which technology is commonly
used for processing and analyzing Big
Data option A is Apache Hadoop option b
is Microsoft Excel or is Adobe Photoshop
and option D is Javascript now you can
pause this video and answer it in the
comment section below so let's start
exploring these important Technologies
First Technology on the list is Apache
Hadoop Apache rube is like a superhero
of the Big Data world it is an amazing
open source tool that's been at the
heart of Big Data Revolution think of it
as a magical framework that's specially
designed to handle and make sense of
incredibly huge piles of data all by
using a bunch of regular computers now
the secret source of Hadoop is its two
main parts that is Hadoop distributed
file systems that is hdfs and mapreduce
hdfs is like a super organized librarian
that keeps all your data neatly in order
and mapreduce is like a team of super
fast workers who can process this data
lightning quit the cool thing about
Hadoop is that it can work with many
regular everyday computers making it
super flexible and cost effective you
don't need any fancy Hardware
Etc Hadoop can take your data and spread
it out across many computers like a big
puzzle this way it's stored safely and
processed super fast because all these
computers can work on it together this
was about Apache Hadoop now second on
the list we have is Apache spark so
Apache spark is a popular and efficient
tool that can be used instead of hadoops
mapreduce it has a special feature
called in memory processing which means
it can work with the data faster this
makes it a great choice for people who
work with data like engineers and
analysts what's cool about spark is that
it can do a lot of different things it
can handle batch processing interactive
queries and real-time data streaming and
even process data as it's coming in this
makes it a flexible and Powerful tool so
in summary Apache spark is fast and
versatile tool that's great for people
who work with data it can do a lot of
different things and its Preferred
Choice for many professionals next we
have is Apache Kafka
is a super smart messenger for data it's
designed to handle lots of information
flying around in real time imagine you
have a huge amount of data that needs to
be sent from one place to another
quickly and safely that's where kafna
comes in think of it as a super highway
for data where information can flow
smoothly and reliably it's like having a
special Mail system that never loses
your letters or packages no matter how
much you send Kafka is super important
because it helps businesses and
applications keep up with the latest
information it's like having a news feed
that never stops updating so you always
know what's happening right now one of
the cool things about Kafka is that it
can handle a lot of data at once and
it's really good at making sure none of
that data gets lost along the way so if
you're Building A system that needs to
stay update in real time Kafka is a key
tool to have in your toolkit next big
data technology in 2024 is Apache Hive
Apache Hive is a helpful tool that makes
it easier for people to analyze and
query data stored in Hadoop it does this
by providing a user-friendly interface
that looks a lot like SQL called hiveql
this means that even if you are not a
tech expert you can still use Hive to
work with data in Hadoop Hive takes the
queries you write and turns them into
tasks that can be processed by hadoop's
map reduce or Apache tests these are
just fancy terms for the behind the
scenes operations that happen in Hadoop
to process and analyze data so you don't
have to worry about the nitty-gritty
details Hive takes care it for you here
the Hadoop itself can store all sorts of
data including structured and
semi-structured in a system called hdfs
Hive is like a bridge that connects you
to this data making it accessible and
easy to work with so in a nutshell
Apache Hive is a handy tool that lets
people who may not be Tech visas easily
query and analyze data stored in Hadoop
hdfs thanks to its SQL like interface
and smart data processing capabilities
alright next we have is Presto so Presto
is a powerful tool that helps people
analyze data from different source just
like Hadoop Cassandra and regular
databases it's like a super smart
detective that can find and make sense
of data no matter where it's stored this
is really useful because it means you
can explore and study data without
having to worry about where it is kept
imagine you have a big puzzle with
pieces scattered all over the room
Presto is like the detective who can
find all those pieces and put the puzzle
together for you even if some pieces are
under the couch plus two acts like a
super smart search engine for your data
it helps you find and use your data no
matter where it is hiding so if you want
to understand your business better
Presto can help you get all the answers
you need from your data so if you are
interested in mastering the world of
data engineering look no further than a
post Gadget program in data engineering
this comprehensive course is tailor made
for professionals like you in this
course you will be diving deep into
essential topics such as Hadoop
framework spark based data processing
Kafka driven data pipelines and the
introsities of managing big data on AWS
and Azure cloud infrastructures our
unique approach Blends live session
Hands-On industry projects exciting IBM
hackathons and interactive ask me
anything sessions to provide you with
the most enriching learning experience
for admission to this data engineering
course candidates need a bachelor's
degree with an average of 50 or higher
marks two plus years of work experience
then basic understanding of
object-oriented programming that is
preferred so elevate your data
engineering skills and career prospects
today enroll now hi I am Assad Shah from
Canada and I recently applicable myself
with the professional certification
program in data engineering offered by
simply learned in collaboration with
Purdue University after working for a
long time in SQL domain moving to Big
Data was a great challenge for me I
needed to upgrade my skills to improve
my performance in my current Workhorse
curriculum is well formulated while
industrial relevant Concepts and project
which helped increase deeper knowledge
about big data now I can easily carry
out my big data products as well as
successful Elite a team of Engineers I
even got a decent salary hike
the wall is moving at a much faster Pace
than we think make sure you don't make
behind so obscure yourself and move to a
step forward and step closer to your
dream so the next big technology on the
list for 2024 is rapid minor so
rapidminer is a user friendly and open
source platform for data science that
helps organizations build and use
predictive models even if they don't
have advanced programming skills this
means that anyone who works with data
can benefit from its features regardless
of their level of expertise one of the
great things about rapidminer is that it
can handle a variety of tasks related to
data such as preparing and cleaning it
draining it and like training machine
learning models and deploying these
models to practical use this makes it
versatile tool that can be used in
different stages of data project so
whether you are a data scientist or just
someone who occasionally works with data
rapid Miner can simplify your workflow
and make it easier to achieve your goals
it is designed to be accessible and user
friendly so you don't need to be a
coding expert to get the most out of it
alright coming to the last one that is
the last big data technology for 2024
which is Apache Cassandra Apache
Cassandra is a type of database that is
specifically designed to handle a lot of
data in a highly reliable and efficient
manner it is particularly well suited
for situations where you need to store
and access large amounts of information
quickly and without any descriptions one
of the key features of Cassandra is its
ability to handle a large amount of data
points across multiple computers or
notes which makes it a greater choice
for applications that need to process a
high volume of information this
distributed nature of Cassandra also
ensures that even if one node fails the
system will still keep running smoothly
making it incredibly fault tolerant
Cassandra is often used in Industries
and applications that require fast and
reliable data storage and retrieval for
example in the internet of things that
is iot where a massive number of devices
are constantly generating data Cassandra
can efficiently manage this influx of
information similarly e-commerce
websites on on social media platforms
that need to handle numerous
transactions and user interactions find
the center to be a valuable tool so
these were the top Big Data Technologies
in 2024 to sum up these leading Big Data
Technologies Apache Hadoop Apache spark
Apache Kafka Apache Hive Presto Etc are
the Forefront of data processing and
analytics today they have transformed
the way organizations handle and utilize
data whether it's managing large data
sets enabling real-time processing or
casting data pipelines or optimal
queries Welcome to our video on hdfs the
heard of distributed file system if you
have ever worked with big data or in the
field of data engineering you have
probably heard of Hado it's
revolutionarized the way we handle and
analyze large data sets but what is hdfs
and why should you learn it hdfs is a
distributed file system designed to
handle the high throughput Computing
needs of large-scale data processing
systems it was designed to run on
clusters of community Hardware making it
cost effective way to process large
amount of data one of the main benefits
of hdfs is its scalability it can handle
petabytes of data across thousands of
nodes making it well suited for Big Data
applications another key feature of hdfs
is its fault tolerance it can handle
node failures without losing data
ensuring uninterrupted data processing
but why should you learn hdfs well if
you're planning a career in data
engineering it's an essential skill hdfs
serves as a foundational infrastructure
for the Hadoop ecosystem enabling other
tools like map reduce and Spark to
process and analyze large volume of data
efficiently so whether you are an
aspiring data engineer or just curious
about Hadoop learning hdfs is a great
place to start in this video we will
explore the basics of hdfs from its
architecture and data modeling to best
practices for managing data let's dive
in on that note consider the data
engineering postgraduate program offered
by simply learn in collaboration with
board University and IBM which present
an excellent opportunity for
professionals seeking practical exposure
this comprehensive program is designed
to participate with essential data
engineering skills and is closely
aligned with AWS and Azure
certifications ensuring industrial
relevance and recognition fast track
your career as a data engineering
professionals without data engineering
course this course covers big data and
data engineering Concepts the Hadoop
ecosystem Apache python Basics AWS EMR
quick site Sage marker the AWS Cloud
platform and Azure Services check out
the course Link in the description for
more details having previously covered
the introduction of hdfs let's now
explore a deeper understanding of the
definition of Hadoop distributed file
system Hadoop distributed file system or
hdfs is a specifically designed
distributed file system that stores and
process this large volume of data across
multiple commodity Hardware nodes it is
a fundamental element within the Apache
head of ecosystems providing fault
tolerance high throughput and
scalability for a better understanding
let's illustrate hdfs using a simple
lumens example so now imagine you have a
large library with thousands of books
managing such a massive collection
efficiently can be challenging so to
address this you organize the books into
a distributed system with multiple
bookshelves in this example the library
represents the hdfs the books in the
library represents the data such as
files and the bookshelves in the library
represents the data nodes in hdfs and
the librarian who keeps track of the
books and their location represents the
name node in the hdfs you achieve better
scalability for tolerance and efficient
data access by organizing your library
into a distributed system similarly hdfs
enables large-scale data storage fall
tolerance and parallel processing for
Big Data applications now let's look at
the architecture of hdfs from the given
diagram below name node as the master
node in hdfs it stores the metadata
about the file system including
directory tree file permissions and file
to block mapping it keeps tracks of data
blocks location and coordinate data
access operations and then we have data
nodes data nodes are the slave nodes in
hdfs they store the data blocks and
perform read and write operation as the
name node directs data nodes are
responsible for data block creation
deletion and replication gram we have
blocks so htfs stores data in large
blocks typically 64 or 128 megabytes
here each file is divided into blocks
and replicated across multiple data
nodes for fault tolerance rack so a rack
is a collection of data nodes grouped
based on a physical proximity it helps
in improving data locality and reduces
Network traffic in hdfs a client refers
to the entity or applications that
interact with the Hadoop distributed
file system it is typically a software
program or component that utilizes the
hdfs API to perform various operations
on files stored in hdfs the client
communicates with the hdfs Clusters to
request file operations like Reading
Writing or modifying now let's have a
look at the file operations like file
read and file right so here the client
can read the contents of the files
stored in htfs it communicates with the
name node to obtain the locations of the
data blocks connects the appropriate
data nodes to retrieve the data the
client can read the entire file or
specific portion of it next file write
here the client can write data to files
in hdfs it communicates with the name
node to obtain Block locations and then
interact with data nodes to write the
data blocks decline can perform single
rights or a pin data to existing files
and next we have data modeling in hdfs
hdfs follows a write once read many
model which means that once a file is
written it is not modified however new
data can be appended to the existing
file so here are some key aspects of
data modeling in hdfs first we have file
organization
fines in hdf is organized in a
hierarchical directory structure similar
to the traditional file system
directories can be created to manage and
collect the data effectively next we
have replication hdfs replicates data
blocks across multiple data nodes for
for tolerance the default replication
factor is typically 3 meaning each block
is stored on three data nodes the
replication Factor can be configured
based on the desired level of fault
tolerance and the third we have data
locality hdf is optimized data access by
keeping the computation close to the
data it tries to schedule tasks on the
same data nodes where the data blocks
are stored to the mini it tries to
schedule tasks on the same data nodes
where the data blocks are stored to
minimize network transfer and next we
are moving on to the best practices for
managing data in hdfs effectively manage
data in hdf is considered the following
best practices
first we have data organization designer
logical directory structure that suits
your data access patterns and
facilitates easy data management
organized data into meaningful
directories and subdirectories next we
have file size hdfs performs best with
large files rather than a large number
of small files aim to have multiple file
size of the block size to maximize data
locality next we have replication Factor
configure the replications factor based
on the importance of the data and the
available storage capacity higher
replication factors provide better fault
tolerance but require more storage next
monitoring and maintenance regularly
monitor your hdfs clusters health and
promptly address any issues monitor disk
space utilization block distributions
and cluster performance next backup and
Disaster Recovery Implement a backup and
Disaster Recovery strategy to ensure
data during ability regularly backup
critical data and replicate it across
different cluster or locations following
this guidelines lets you maximum hdfs
architecture and feature to effectively
store and process large scale data so if
you are interested in mastering the
world of data engineering look no
further than our postgraduate program in
data engineering this comprehensive
course is tailor made for professionals
like you diving deep into the essential
topics such as Hadoop framework spark
based data processing Kafka driven data
pipelines and the intricities of
managing big data on AWS and Azure Cloud
infrastructures a unique approach Blends
live session Hands-On industry projects
exciting IBM hackathons and interactive
ask me anything sessions to provide you
with the most enriching learning
experience
elevate your data engineering skills and
career prospects today for admission to
this data engineering course a
bachelor's degree with an average of 50
or higher marks is required a two plus
years of work experience is preferred
and basic understanding of object
oriented programming is preferred so
enroll now this lesson focus is on hive
let us explore the objectives of this
lesson in the next screen
by the end of this lesson you will be
able to describe Hive and its importance
explain Hive architecture and its
various components identify installation
and configuration steps for Hive
describe the basics of Hive programming
in the next screen we will discuss the
need for an additional data warehousing
system anyone who owned and operated a
web application would be familiar with
the problem of the storing and retrieval
of data being generated every minute
the Adaptive solution created for the
same was the use of Hadoop including the
Hadoop distributed file system or hdfs
for performing operations on data for
storage and retrieval
the Hadoop framework has a scalable and
highly accessible architecture
another problem that cropped up further
in the process was that some data could
not be expressed it was difficult to
develop a map reduce program for
expressing the data the solution was
hive
in the next screen we will discuss the
basics of hive
Hive is defined as a data warehouse
system for Hadoop that facilitates ad
hoc queries and the analysis of large
data sets stored in Hadoop it provides a
sql-like language called Hive ql or hql
due to its sql-like interface Hive is a
popular choice for Hadoop analytics it
provides massive scale out and fault
tolerance capabilities for data storage
and processing of commodity Hardware
relying on mapreduce for execution Hive
is batch oriented and has high latency
for query execution
in the next screen we will discuss the
key characteristics of hive
Hive is a system for managing and
querying unstructured data into a
structured format it uses the concept of
mapreduce for the execution of its
scripts and hdfs for storage and
retrieval of data following are the key
principles underlying hive
commands are similar to that of SQL SQL
is a data warehousing tool that is
similar to Hive hence learning high will
not be a big challenge for those who are
familiar with SQL
Hive contains extensive plugable map
reduce scripts in the language of your
choice
these scripts include Rich user-defined
data types and user-defined functions
Hive has an extensible framework to
support different files and data formats
performance is better in Hive since Hive
engine uses the best inbuilt script to
reduce the execution time thus enabling
high output in less time
in the next screen we will discuss the
system architecture and the components
of hive
the image on the screen shows the
architecture of the hive system it also
illustrates the role of Hive and Hadoop
in the development process
in the next few slides we will discuss
the components of Hive as shown on the
screen we will start with metastore in
the next screen
metastore is the component that stores
the system catalog and metadata about
tables columns partitions and so on
usually metadata is stored in a
traditional rdbms format Apache Hive
uses Derby database by default any jdbc
compliant database like MySQL can be
used for metastore
in the next screen we will discuss the
configuration of metastore
we will focus on the key attributes that
need to be configured for Hive metastore
the primary attributes that should be
configured are
connection URL
connection driver
connection user ID and connection
password
now let us look at the XML file used to
configure metastore
the template of the file displayed on
screen may vary from one version of Hive
to another
in the next screen we will focus on
driver
is responsible for managing the
lifecycle of an hql or Hive query
language statement as it moves through
hive
the driver also maintains a session
handle and a session statistics if any
it includes three basic components
namely the compiler the optimizer and
the executor
in the next screen we will focus on the
compiler query compiler is one of the
driver components it is responsible for
compiling The Hive script for errors
if the script is error free it is
converted into a directed acyclic graph
or dag of mapreduce tasks
in the next screen we will focus on the
query Optimizer query Optimizer
optimizes Hive scripts for faster
execution of the same it consists of a
chain of Transformations so that the
operator dag resulting from one
transformation is passed as an input to
the next transformation a query
Optimizer also performs tasks like
column pruning partition pruning and
re-partitioning of data in the next
screen we will discuss the role of
execution engine
the role of execution engine is to
execute the tasks produced by the
compiler in proper dependency order
the execution engine interacts with the
underlying Hadoop instance to ensure
perfect synchronization with Hadoop
services
in the next screen we will discuss the
server components in Hive Hive server is
the main component responsible for
providing a thrift interface to the user
it also maintains connectivity in
modules it provides a jdbc or odbc
server
jdbc stands for Java database
connectivity and odbc refers to open
database connectivity
Hive server enables the integration of
Hive with other applications
in the next screen we will discuss
client components
a developer uses client components to
perform development in Hive some of the
client components are command line
interface or CLI web user interface or
UI and the jdbc odbc driver
CLI is the hive prompt from where you
can enter commands and execute the same
web interface is a web console to view
and interact with hive
the jdbc odbc driver is used to
interface Hive with the existing
database components or engine
in the next screen we will discuss the
basics of Hive query language Hive query
language or hql is the query language
for Hive engine Hive supports basic SQL
queries such as the from Clause sub
query ANSI joins such as equajoin only
multi-table insert multi-group by
sampling and objects traversal hql
provides support to plugable mapreduce
scripts using the transform command in
the next screen we will focus on tables
in hive
Hive tables are analogous to tables in
relational databases a hive table
logically comprises the data being
stored and the associated metadata each
table has a corresponding directory in
hdfs
there are two types of tables in Hive
they are managed tables and external
tables to create a table in Hive use the
command shown on the screen once created
this table can be located in hdfs in a
directory mentioned on the screen
in the next screen we will focus on
external tables external tables are Hive
entities that are similar to tables the
difference is that when a table is
created the unstructured data which is
linked to this table is deleted in
external tables however unstructured
data is copied and not deleted following
are the key considerations in an
external table
external table points can be stored in
existing data directories in hdfs
external tables can create tables and
partitions
in an external table data is in Hive
compatible format
when an external table is dropped only
the metadata drops let us consider an
example on how to create an external
table the command used to create an
external table is called test underscore
extern using unstructured data available
at the location mentioned on the screen
while the table is created it is ensured
that the data is not deleted
in the next screen we will discuss data
types
the data types in Hive are primitive
complex and user-defined types click
each box to learn more
primitive data types include integers
such as Tiny int small int
int and big int
Boolean floating Point numbers type such
as float and double and string
complex types include structs maps and
arrays examples of such types are shown
on the screen
user-defined types include structures
with attributes that can be of any type
in this screen we will discuss
partitions partitions are analogous to
dense indexes on columns the features of
partitions are that they contain nested
subdirectories in hdfs for each
combination of partition column values
and allow users to retrieve the rows
efficiently an example of a partition is
shown on the screen let us now focus on
how to create a partition in hive
certain queries are used to create
partitions and insert data in them some
of these queries are given on the screen
in the next screen we will discuss
serialization and deserialization
serialization takes a Java object that
Hive has been working with and turns it
into something that Hive can write to
hdfs or another supported system
serialization is used when writing data
for example through an insert select
statement
deserialization is used during query
time to execute select statements
the other facts related to serialization
and deserialization are
the interface used for performing
serialization and deserialization is Sir
d
in some situations the interface used
for deserialization is lazy sir d
this interface allows unstructured data
to be converted into structured data due
to its flexibility while using this
interface the data is read based on the
Separation by different delimiter
characters the sir D interface is
located in the jar file mentioned on
screen in the next screen we will
explore the file formats in high Hive
lets users to store different file
formats and helps them to improve the
performance with respect to the
operations done on data such as storing
analyzing and so on
the example given on the screen shows
how SQL is used to perform a file format
operation in which the sequence file
input format class is used for input and
the sequence file output format class is
used for output
in the next screen we will focus on hive
queries one of the queries in Hive is
select the syntax of the select query is
shown on the screen similar to a
relational database the select query has
where Group by sort by and limit Clauses
the select query also supports nested
queries in the next screen we will focus
on join and insert operation hql
supports join and insert commands some
examples of join and insert commands are
shown on the screen
in the next screen we will discuss the
installation of Hive to install Hive you
need to locate The Hive tar file you
will be using Hive version
0.13.1 for your practicals you can also
use the newer versions you can get the
download location by visiting the URL
shown in the image here in the next
screen we will download The Hive tar
file and untar the same use the wget
command to download The Hive tar file in
Ubuntu system once the download is
complete you need to extract The Hive
tar file
the command for the same is shown on
screen
in the next screen we will continue with
the hive installation process
move the extracted tar file to the
location mentioned on the screen use the
command displayed on the screen to
perform this step of Hive installation
in the next screen we will set the hive
prefix and the path for hive
ensure that you move the hive folder
which was extracted in the previous step
to the location mentioned on screen
next you need to use the command shown
in the first image to set the hive
prefix
to set the path for Hive use the command
shown in the second image on the screen
in the next screen we will discuss how
to start the hive prompt
to start the hive prompt use the hive
command and press the enter button you
will get a prompt similar to the one
shown on the screen now let us focus on
the properties that are set in your hive
system to know the hive properties that
are set in your machine use the command
set hyphen V
in the next screen we will discuss how
to perform programming in hive
to show the total number of tables use
the show tables command ensure all
commands end with a semicolon
initially you will get an output similar
to the one shown on the screen this is
because currently there are no tables in
Hive so let us now create a table in the
next screen
suppose the name of the table is book
and it has one column called word use
the command shown on the screen to
create the table ensure you complete
your query with a semicolon in the next
screen we will take a look at the output
as discussed earlier Hive stores a table
in the form of a folder to view the
output browse the name node web UI and
look for the directory named user under
the user directory you will find a
folder named hive
under Hive you will find a folder named
Warehouse in the warehouse folder you
will find your table named book
in the next screen we will load the book
from hdfs to The Hive table we have
created
there is no content in the book let us
now add some content from unstructured
data ensure that you have the War and
Peace book uploaded to hdfs
in the next screen we will focus on the
command used for this purpose
the command is shown on the screen will
help to load the data from hdfs in the
next screen we will check if the table
has been successfully to verify the same
use the command to show tables in Hive
prompt if the listing shows your table
it means that the table has been
successfully created
in the next screen we will continue to
focus on programming in hive
let us check the table schema in Hive
using the command shown on screen
in the next screen we will discuss how
to check the content of the table use
the command given on the screen to check
the content present in the table you
will get the output as words
in the next screen we will focus on the
extensibility of Hive query language use
the command given on the screen to check
the content present in the table you
will get the output as words
in the next screen we will focus on the
extensibility of Hive query language The
Hive query language can be extended in
multiple ways some of the common ways
include plugable user-defined functions
pluggable map reduce scripts plugable
user-defined types and plugable data
formats
in the next screen we will focus on
user-defined function Hive has the
ability to define a function any new
user defined or UDF class needs to
inherit from this UDF class all UDF
classes need to implement one or more
methods named evaluate which will be
called by hive
evaluate should never be a void method
however it can return null if needed
after compiling UDF you have to include
it in the hive class path then once Hive
gets started you have to register the
function
next you use the function in a hive
query statement
in the next screen we will focus on
build-in function Hive provides a lot of
built-in functions such as mathematical
collection type conversion date
conditional and string
in the next slide we will focus on other
functions in hive
there are various other functions in
Hive namely aggregate function table
generating function and so on
aggregate functions create the output if
the full set of data is given the
implementation of this function is a
little complex compared to that of the
UDF the user has to implement a few more
methods but the idea is similar
therefore Hive provides a lot of
built-in user-defined aggregate
functions also known as udaf
normal user-defined functions namely
concat take in a single input row and
output a single output row
in contrast table generating functions
transform a single input row to multiple
output rows
lateral view is used in conjunction with
table generating functions
an SQL script for lateral view is given
on the screen
consider the base table named page ads
it has two columns namely page ID that
is the name of the page and add ID list
that is an array of ads appearing on the
page
a lateral view with explode can be used
to convert add ID list into separate
rows using the query given on screen
in the next screen we will focus on
mapreduce scripts mapreduce scripts can
be written in scripting languages like
python users can plug in their own
custom mappers and reducers in the data
stream to run a custom mapper script and
a custom reducer script the user can
issue a command which uses the transform
Clause to embed the mapper and the
reducer scripts for example in the
script shown on the screen by default
key value pairs will be transformed to
string and delimited by tab before
feeding to the user script the method
strip returns a copy of all the words in
which white space characters have been
stripped from the beginning and the end
of the word the method split returns a
list of all the words using tab as the
separator
in the next screen we will focus on the
comparison of UDF and udaf with
mapreduce scripts user-defined functions
are written in Java while mapreduce
scripts can be written in any language
both user-defined functions and
mapreduce scripts support one to one end
to one and one to n input to Output
however user-defined functions are much
faster than mapreduce scripts since the
latter spawns new processes for
different operations
in the next screen we will focus on a
business scenario Olivia is the EVP it
operations at Nutra worldwide
Incorporated
Clive is the AVP business interface
Clive is assigned to one of Olivia's
projects He has been asked to analyze
the distribution data at Nutro worldwide
incorporated as part of this assignment
he has to perform Advanced data
analytics Clive wants to install Hive so
that he can perform data analytics
the Demos in this lesson will illustrate
how to install Hive and perform data
analytics and partitioning
we will start with the first demo in the
next screen in this demo we will perform
installation of Hive on Ubuntu system
visit the website www.hive.apache.org
click the downloads link
HTTP colon double slash
hive.apache.org downloads.html
click on download a release Now link
HTTP colon double slash Hive dot Apache
dot o r g slash d-y-n slash closer dot
cgl slash Hive slash
click the mirror link
HTTP colon double slash mirror dot tcpd
iag dot net slash Apache slash Hive
slash
click Hive version
0.1 1.0 file
right-click The Hive hyphen
0.11.0 tar file
click on copy link location menu item
access the server shell prompt and
download Hive the command is w g e t
followed by the link mentioned on the
screen
once downloaded you will need to
uncompress the folder this is done using
the command shown on the screen
copy the extracted folder to the desired
location use the command as shown on the
screen
the next step is to install Java type
the command shown on the screen and
press the enter key to continue
now let us set the bash to do so use the
command shown on the screen
scroll down and ensure you add the lines
as highlighted in the screenshot
they are export Hive underscore prefix
equals slash USR slash local slash Hive
and Export path equals dollar path colon
dollar Hive prefix underscore slash bin
if you see an output is shown in the
screenshot it means that Java has been
installed and configured successfully
update the bash using the
exec bash command
ensure that Hadoop services are active
this can be done using the command JPS
to start Hive the command is Hive you
will get a hive prompt where the queries
are run
congratulations you have now
successfully installed and configured
hive
let us do a quick recap of the steps
performed
in this demo we will demonstrate
Advanced data analytics using hive
for example let us create a table called
book with one field of data type string
the command used is shown on the screen
press enter key to continue
observe row format delimited press enter
key to continue
observe Fields terminated by single
quotes
press enter key to continue
observe lines terminated by slash and
press the enter key
now let us load the data from the file
into the table the command 2 loaded data
is shown on the screen press enter key
to continue
let us verify the table by listing the
same the command is show tables
semicolon press enter key to continue
the highlighted part shows our table
book
let us verify the schema of the table
book the command is describe book
semicolon press enter key to continue
the output describes the table book
type select lower Open Bracket word
close bracket comma count Open Bracket
asterisk close bracket as count and
press enter key
observe from book and press enter key
observe where lower Open Bracket
substring Open Bracket word comma 1
comma 3 close bracket close bracket
equals start single quote was end single
quote
press enter key
observe Group by word and press enter
key
observe having count as greater than 50.
press enter key
observe sort by count d e s c semicolon
press enter key to start the execution
process of the query
observe the job execution
observe the output it shows the
occurrence of the word was which is 366.
congratulations you have now
successfully executed the program
let us do a quick recap of the steps
performed
let us look at the word count demo using
Hive to begin with launch Hive on your
system
create a table named book with a single
column text word to store the contents
in this case we will put all the
contents of the table in a single column
and do further transformation and
querying
use the syntax shown on the screen
the fields are terminated by slash n
since we are using a new line character
as a delimiter to read each line
the file is saved in the text file
format
the table book will be created once you
press enter after semicolon on hive
prompt
load the book
gutenberg.txt from the hdfs path use the
syntax shown on the screen
use the command as shown on the screen
enter semicolon and press enter
the contents get uploaded in the table
book in Hive Warehouse
query the table for determining a word
count by using the select statement
use the select query with account
operator
use lateral View and user-defined
functions to split the contents into
space separated words
a lateral view first applies the
user-defined function to each row of the
base table and then joins the resulting
output rows to the input rows to form a
virtual table having the supplied table
alias
to put it simply this line will put each
word on a separate record of lateral
View
now Group by word so that the count
operator can give the result by each
distinct word
execute the query and observe the result
with each word showing the count of
times it occurs in the document
in this demo we will use a document from
the document management system of
neutral worldwide Incorporated and
demonstrate how to determine word count
using hive
let us do a quick recap of the steps
performed
in this demo we will demonstrate
partitioning with hive
let us look at table partitions using
Hive to begin with launch Hive on your
system create a table president
mention the two columns Sno which means
serial number and line which includes
the president name per line
partitioned by the parameter country
which is of type string note we have not
mentioned country as a column in the
table unlike we do in most of the
relational databases
use the usual Hive syntax row format
delimited
the fields are terminated by a comma
since the data is comma separated
execute the query and use the describe
command to verify if the table is
created as per the requirement
you will observe that the table has been
created successfully
let us now load data in the table
load data in the table president using
the load data in path command since the
data is already present in hdfs
you will observe an error if the
partition is not specified
the reason for the error is that we did
not specify the partition in which data
is to be loaded
let us try to rectify the error by using
the Clause partition country equals USA
to ensure that data gets loaded in the
right partition of the table
you will observe that query has been
successfully executed
let us check the data in hdfs to see how
Hive tables and partitions are created
from The Hive prompt itself use the
command as shown on the screen to view
the table directory
please note that the directory location
may vary with Hive and Hadoop
distribution version
list the directory for the partition
country equals USA partition
note that a directory is created for the
table president a directory is also
created for the partition country equals
USA
let us now load the data for some other
countries also
let us load the data for the partition
country equals India
now load the data for the partition
country equals Russia
check out the various partitions have
been created in hdfs
use the DFS minus LS command again from
the hive prompt
you will observe that a directory for
each partition has been created you can
list the content of each directory to
view the data files present in every
partition directory
let us do a quick recap of the steps
performed
let us summarize the topics covered in
this lesson hibe is a system for
managing and querying unstructured data
into a structured format
the various components of Hive
architecture are metastore driver
execution engine and so on
metastore is a component that stores the
system catalog and metadata about tables
columns partitions and so on
Hive installation starts with locating
the latest version of tar file and
downloading it in Ubuntu system using
the wget command while programming in
Hive use the show tables command to
display the total number of tables so if
you are interested in mastering the
world of data engineering look no
further than our postgraduate program in
data engineering this comprehensive
course is tailor made for professionals
like you diving deep into the essential
topics such as Hadoop framework spark
based data processing Kafka driven data
pipelines and the introsities of
managing big data on AWS and Azure Cloud
infrastructures a unique approach Blends
live session Hands-On industry projects
exciting IBM hackathons and interactive
ask me anything sessions to provide you
with the most enriching learning
experience
elevate your data engineering skills and
career prospects today for admission to
this data engineering course a
bachelor's degree with an average of 50
or higher marks is required a two plus
years of work experience is preferred
and basic understanding of
object-oriented programming is preferred
so enroll now if difference between Hive
and rdbms
remember rdbms stands for the relational
database management
let's take a look at the difference
between Hive and the rdbms with Hive
Hive enforces schema on read and it's
very important that whatever is coming
in that's when hive's looking at it and
making sure that it fits the model the
rdbms enforces a schema when it actually
writes the data into the database so
it's read the data and then once it
starts to write it that's where it's
going to give you the error or tell you
something's incorrect about your scheme
Hive data size is in petabytes that is
hard to imagine you know when we're
looking at your personal computer on
your desk maybe you have 10 terabytes if
it's a high-end computer but we're
talking petabytes so that's hundreds of
computers grouped together when a rdbms
data size is in terabytes very rarely do
you see an rdbms system that's spread
over more than five computers and
there's a lot of reasons for that with
the rdbms it actually has a high-end
amount of rights to the hard drive
there's a lot more going on there you're
writing and pulling stuff so you really
don't want to get too big with the an
rdbmaster you're going to run into a lot
of problems with Hive you can take it as
big as you want Hive is based on the
notion of right once and read many times
this is so important and they call it
worm which is right w10o read R many
times M they refer to it as warm and
that's true of any of you a lot of your
Hadoop set up it's it's altered a little
bit but in general we're looking at
archiving data that you want to do data
analysis on we're looking at pulling all
that stuff off your rdbms from years and
years and years of business or whatever
your company does or scientific research
and putting that into a huge data pool
so that you can now do queries on it and
get that information out of it with the
rdbms it's based on the notion of read
and write many times so you're
continually updating this database
you're continually bringing up new stuff
new sales the account changes because
they have a different licensing now
whatever software you're selling all
that kind of stuff where the data is
continually fluctuating and then Hive
resembles a traditional database by
supporting SQL but it is not a database
it is a data warehouse this is very
important it goes with all the other
stuff we've talked about that we're not
looking at a database but a data
warehouse to store the data and still
have fast and easy access to it for
doing queries you can think of Twitter
and Facebook they have so many posts
that are archived back historically
those posts aren't going to change they
made the post they're posted they're
there and they're in their database but
they have to store it in a warehouse in
case they want to pull it back up with
the rdbms it's a type of database
management system which is based on the
relational model of data and then with
Hive easily scalable at a low cost again
we're talking maybe a thousand dollars
per terabyte the rdbms is not scalable
at a low cost when you first start on
the lower end you're talking about 10
000 per terabyte of data including all
the backup on the models and and all the
added Necessities to support it as you
scale it up you have to scale those
computers and Hardware up so you might
start off with a basic server and then
you upgrade to a sun computer to run it
and you spend you know tens of of
dollars for that Hardware upgrade with
Hive you just put another computer into
your Hadoop file system so let's look at
some of the features of Hive when we're
looking at the features of Hive we're
talking about the use of SQL like
language called Hive ql a lot of times
you'll see that as hql which is easier
than long codes this is nice if you're
working with your shareholders you come
to them and you say Hey you can do a
basic SQL query on here and pull up the
information you need this way you don't
have to take all have your programmers
jump in every time they want to look up
something in the database they actually
now can easily do that if they're not
skilled in programming and script
writing tables are used which are
similar to The rdbms hence easier to
understand and one of the things I like
about this is when I'm bringing tables
in from a MySQL server or SQL Server
there's almost a direct reflection
between the two so when you're looking
at one which is a data which is
continually changing and then you're
going into the archive database it's not
this huge jump where you have to learn a
whole new language you mirror that same
schema into the hdfs into the hive
making it very easy to go between the
two and then using Hive ql multiple
users can simultaneously query data so
again you have multiple clients in there
and they send in their query that's also
true with the rdbms which kind of cues
them up because it's running so fast you
don't notice the lag time well you get
that also with the hql as you add more
computers in the query can go very
quickly depending on any computers and
how much resources each machine has to
pull the information and Hive supports a
variety of data types so with Hive it's
designed to be on the Hadoop system
which you can put almost anything into
the Hadoop file system so with all that
let's take a look at a demo on hive ql
or hql before I dive into the Hands-On
demo let's take a look at the website
hive.apache.org that's the main website
since Apache it's an Apache open source
software this is the main software for
the main site for the build and if you
go in here you'll see that they're
slowly migrating Hive into beehive and
so if you see beehive versus Hive note
The Beehive is the new release is coming
out that's all it is it reflects a lot
of the same functionality of Hive it's
the same thing and then we like to pull
up some kind of documentation on
commands and for this I'm actually going
to go to hortonworks Hive cheat sheet
and that's because as Horton works and
Cloudera two of the most common used
builds for Hadoop and for which include
Hive and all the different Tools in
there and so hortonworks has a pretty
good PDF you can download cheat sheet on
there I believe Cloudera does too but
we'll go ahead just look at the Horton
one because it's the one that comes up
really good and you can see when we look
at the query language it Compares MySQL
server to Hive ql or hql and you can see
the basic select we select from columns
from table where conditions exist you
know most basic command on there and
they have different things you can do
with it just like you do with your SQL
and if you scroll down you'll see data
types so here's your integer your flow
your binary double string timestamp and
all the different data types you can use
some different semantics different Keys
features functions for running a hive
query command line setup and of course
the hive shell setup in here so you can
see right here if we Loop through it it
has a lot of your basic stuff is we're
basically looking at SQL across a Horton
database we're going to go ahead and run
our Hadoop cluster Hive demo and I'm
going to go ahead and use the Cloudera
quick start this is in the virtual box
so again we have an oracle virtual box
which is open source and then we have
our Cloudera quick start which is the
Hadoop setup on a single node now
obviously Hadoop And Hive are designed
to run across a cluster of computers so
we talk about a single node is for
Education testing that kind of thing and
if you have a chance you can always go
back and look at our demo we had on
setting up a Hadoop system in a single
cluster to set a note Down Below in the
YouTube video and our team will get in
contact with you and send you that link
if you don't already have it or you can
contact us at the
www.simplylearn.com now in here it's
always important to note that you do
need on your computer if you're running
on Windows because I'm on a Windows
machine you're going to need probably
about 12gb is to actually run this it
used to be goodbye with a lot less but
as things have evolved they take up more
and more resources and you need the
professional version if you have the
home version I was able to get that to
run but boy did it take a lot of extra
work to get the home version to let me
use the virtual setup on there and we'll
simply click on the Cloudera quick start
and I'm going to start that up and this
is starting up our Linux so we have our
Windows 10 which is a computer I'm on
and then I have the virtual box which is
going to have a Linux operating system
in it and we'll skip ahead so you don't
have to watch the whole install
something interesting to know about the
Cloudera is that it's running on Linux
Santos and for whatever reason I've
always had to click on it and hit the
escape button for it to spin up and then
you'll see the Dos come in here now that
our Cloudera is spun up on our virtual
machine with the Linux on we can see
here we have our it uses the Thunderbird
browser on here by default and
automatically opens up a number of
different tabs for us and a quick note
because I mentioned like the
restrictions on getting set up on your
own computer if you have a home edition
computer and you're worried about
setting it up on there you can also go
in there and spin up a one month free
service on Amazon web service to play
with this so there's other options
you're not stuck with just doing it on
the quick start menu you can spin this
up in many other ways now the first
thing we want to note is that we've come
in here into Cloudera and I'm going to
access this in two ways the first one is
we're going to use Hue and I'm going to
open up Hue and it'll take it a moment
to load from the setup on here and Hue
is nice if I go in and use Hue as an
editor into Hive or into the Hadoop
setup usually I'm doing it as a from an
admin side because it has a lot more
information a lot of visuals less to do
with you know actually diving in there
and just executing code and you can also
write this code into files and scripts
and there's other things you can other
ways you can upload it into high live
but today we're going to look at the
command lines and we'll upload it into
Hue and then we'll go into and actually
do our work in a terminal window Under
The Hive shell now in the Hue browser
window if you go under query and click
on the pull down menu and then you go
under editor and you'll see Hive there
we go there's our Hive setup I go and
click on hive and this will open up our
query down here and now it has a nice
little B that shows our Hive going and
we can go something very simple down
here like show databases and we follow
it with the semicolon and that's the
standard in Hive is you always add our
punctuation at the end there and I'll go
ahead and run this and the query will
show up underneath and you'll see down
here since this is a new quick start I
just put on here you'll see it has the
default down here for the databases
that's the database name I haven't
actually created any databases on here
and then there's a lot of other like
assistant function tables your databases
up here there's all kinds of things you
can research you can look at through Hue
as far as a a bigger picture the
downside of this is it always seems to
lag for me whenever I'm doing this I
always seem to run slow so if you're in
Cloudera you can open up a terminal
window they actually have an icon at the
top you can also go under applications
and under applications system tools and
terminal either one will work it's just
a regular terminal window and this
terminal window is now running
underneath our Linux so this is a Linux
terminal window or on our virtual
machine which is resting on our regular
Windows 10 machine and we'll go ahead
and zoom this in so you can see the text
better on your own video and I simply
just clicked on view and zoom in and
then all we have to do is type in Hive
and this will open up the shell on here
and it takes it just a moment to load
when starting up Hive I also want to
note that depending on your rights on
the computer you're on interaction you
might have to do pseudohive and put in
your password and username most
computers are usually set up with the
hive login again it just depends on how
you're accessing the Linux system and
the hive shell once we're in here we can
go ahead and do a simple hql command
show databases and if we do that we'll
see here that we don't have any
databases so we can go ahead and create
a database and we'll just call it office
for today for this moment now if I do
show we'll just do the up Arrow up arrow
is a hotkey that works in both Linux and
in Hive so I can go back and paste
through all the commands I've typed in
and we can see now that I have my
there's of course a default database and
then there's the office database so now
we've created a database it was pretty
quick and easy and we go ahead and drop
the database we can do drop Database
Office now this will work on this
database because it's empty if your
database was not empty you would have to
do Cascade and that drops all the tables
in the database and the database itself
now if we do show database and we'll go
ahead and recreate our database because
we're going to use the office database
for the rest of this Hands-On demo a
really handy command to Now set with the
SQL or hql is to use office and what
that does is that sets office as the
default database so instead of having to
reference the database every time we
work with a table we now automatically
assumes that's the database being used
whatever tables we're working on the
difference is you put the database name
period table and I'll show you in just a
minute what that looks like and how
that's different if we're going to have
a table and a database we should
probably load some data into it so let
me go ahead and switch gears here and
open up a terminal window you can just
open another terminal window and it'll
open up right on top of the one that you
have Hive shell running in and when
we're in this terminal window first
we're going to go ahead and just do a
list which is of course a Linux command
you can see all the files I have in here
this is the default load we can change
directory to documents we can list in
documents and we're actually going to be
looking at
employee.csv a Linux command is the cat
you can use this actually to combine
documents there's all kinds of things
that cat does but if we want to just
display the the contents of our
employee.csv file we can simply do cat
employee CSV and when we're looking at
this we want to know a couple things one
there's a line at the top okay so the
very first thing we notice is that we
have a header line the next thing we
notice is that the data is comma
separated and in this particular case
you'll see a space here generally with
these you got to be real careful with
spaces there's all kinds of things you
got to watch out for because it can
cause issues these spaces won't because
these are all strings that the space is
connected to if this was a space next to
the integer you would get a null value
that comes into the database without
doing something extra in there now with
most of Hadoop that's important to know
that you're writing the data once
reading it many times and that's true of
almost all your Hadoop things coming in
so you really want to process the data
before it gets into the database and for
those who of you have studied data
transformation that's the adult where
you extract transfer form and then load
the data so you really want to extract
and transform before putting it into the
hive then you load it into the hive with
the transform data and of course we also
want to note the schema we have an
integer string string integer integer so
we kept it pretty simple in here as far
as the way the data is set up the last
thing that you're going to want to look
up is the source since we're doing local
uploads we want to know what the path is
we have the whole path in this case it's
home slash Cloudera documents and these
are just text documents we're working
with right now we're not doing anything
fancy so we can do a simple git edit
employee.csv and you'll see it comes up
here it's just a text document so I can
easily remove these added spaces there
we go and then we go and just save it
and so now it has a new setup in there
we've edited it the G edit is usually
one of the default that loads into Linux
so any text editor will do back to the
hive shell so let's go ahead and create
a table employee and what I want you to
note here is I did not put the semicolon
on the end here a semicolon tells it to
execute that line so this is kind of
nice if you're you can actually just
paste it in if you have it written on
another sheet and you can see right here
where I have create table employee and
it goes into the next line on there so I
can do all of my commands at once now
just I don't have any typo errors I went
ahead and just pasted the next three
lines in and the next one is our schema
if you remember correctly from the other
side we had the different values in here
which was ID name Department year of
joining and salary and the ID is an
integer name is a string department
string you're joining energy salary an
integer and they're in Brackets we put
close brackets around them and you could
do this all as one line and then we have
row format delimited Fields terminated
by comma and this is important because
the default is tabs so if I do it now it
won't find any terminated Fields so
you'll get a bunch of null values loaded
into your table and then finally our
table properties we we want to skip the
header line count equals one now this is
a lot of work for uploading a single
file it's kind of goofy when you're
uploading a single file that you have to
put all this in here but keep in mind
Hive and Hadoop is designed for writing
many files into the database you write
them all in there and then you can
they're saved it's an archive it's a
data warehouse and then you're able to
do all your queries on them so a lot of
times we're not looking at just the one
file coming up we're loading hundreds of
files you have your reports coming off
of your main database all those reports
are being loaded and you have your log
files you have I mean all this different
data is being dumped into Hadoop and in
this case Hive on top of Hadoop and so
we need to let it know hey how do I
handle these files coming in and then we
have the semicolon at the end which lets
us know to go ahead and run this line
and so we'll go ahead and run that and
now if we do a show tables you can see
there's our employee on there we can
also describe if we do describe employee
you can see that we have our ID integer
names Department string year of joining
integer and salary integer and then
finally let's just do a select star from
employee very basic
sqlnhql command selecting data and it's
going to come up and we haven't put
anything in it so as we expect there's
no data in it so if we flip back to our
Linux terminal window you can see where
we did the cat
employee.csv and you can see all the
data we expect to come into it and we
also did our PWD and right here you see
the path you need that full path when
you are loading data you know you can do
a browse and if I did it right now with
just the employee.csv is the name it
will work but that is a really bad habit
in general when you're loading data
because it's you don't know what else is
going on on the computer you want to do
the full path almost in all your data
loads so let's go ahead and flip back
over here to our Hive shell we're
working in and the command for this is
load data so that says hey we're loading
data that's a hive command hql and we
want local data so you got to put down
local and path so now it needs to know
where the path is now to make this more
legible I'm just going to go ahead and
hit enter then we'll just paste the full
path in there which I have stored over
on the side like a good prepared demo
and you'll see here we have home
Cloudera documents employee.csv so it's
a whole path for this text document in
here and we go ahead and hit enter in
there and then we have to let it know
where the data is going so now we have a
source and we need a destination and
it's going to go into the table and
we'll just call it employee we'll just
match the table in there and because I
wanted to execute we put the semicolon
on the end it goes ahead and executes
all three lines now if we go back if you
remember we did the select star from
employee just using the up Arrow to page
through my different commands I've
already typed in you can see right here
we have as we expect we have Rose Sam
Mike and Nick and we have all their
information showing in our four rows and
then let's go go ahead and do select and
count we'll just look at a couple of
these different select options you can
do we're going to count everything from
employee now this is kind of interesting
because the first one just pops up with
the basic select because it doesn't need
to go through the full map reduce phase
but when you start doing a count it does
go through the full map reduce setup in
the hive in Hadoop and because I'm doing
this demo on a single node Cloudera
virtual box on top of a Windows 10. all
the benefits of running it on a cluster
are gone and instead is now going
through all those added layers so it
takes longer to run you know like I said
when you do a single node as I said
earlier it doesn't do any good as an
actual distribution because you're only
running it on one computer and then
you've added all these different layers
to run it and we see it comes up with
four and that's what we expect we have
four rows we expect four at the end and
if you remember from our cheat sheet
which we brought up here from Horton
such as a pretty good one there's all
these different commands we can do we'll
look at one more command where we do the
what they call sub queries right down
here because that's really common to do
a lot of sub queries and so we'll do
select star or all different columns
from employee now if we weren't using
the office database it would look like
this from Office dot employee and either
one will work on this particular one
because we have office set as a default
on there so from office employee and
then the command where creates a subset
and in this case we want to know where
the salary is greater than 25
000. there we go and of course we end
with our semicolon if we run this query
you can see it pops up and there's our
salaries the people top earners we have
Rose and it and Mike and HR kudos to
them of course they're fictitional I
don't actually we don't actually have a
rose and a mic in those positions or
maybe we do so finally we want to go
ahead and do is we're done with this
table now I'm remember you're dealing
with the data warehouse so you usually
don't do a lot of dropping of tables and
databases but we're going to go ahead
and drop this table here before we drop
it one more quick note is we can change
it so what we're going to do is we're
going to alter table office employee and
we want to go ahead and rename it
there's some other commands you can do
in here but rename is pretty common and
we're going to rename it to and it's
going to stay in office and it turns out
one of our shareholders really doesn't
like the word employee he wants
employees plural it's a big deal to him
so let's go ahead and change that name
for the table it's that easy because
it's just changing the meta data on
there and now if we do show tables
you'll see we now have employees not
employee and then at this point maybe
we're doing some house cleaning because
this is all practice so we're going to
go ahead and drop table and we'll drop
table employees because we changed the
name in there so if we did employee just
give us an error and now if we do show
tables we'll will see how the tables are
gone now the next thing we want to go
and take a look at and we're going to
walk back through the loading of data
just real quick because we're going to
load two tables in here and let me just
float back to our terminal window so we
can see what those tables are that we're
loading and so up here we have customer
we have a customer file and we have an
order file we want to go ahead and put
the customers and the orders into here
so those are the two we're doing and of
course it's always nice to see what
you're working with so let's do our cat
customer dot CSV we could always do G
edit but we don't really need to edit
these we just want to take a look at the
data in customer and important in here
is again we have a header so we have to
skip a line comma separated nothing odd
with the data we have our schema which
is integer string integer string integer
so you know you'd want to take that note
that down or flip back and forth when
you're doing it and then let's go ahead
and do cat
order.csv and we can see we have oid
which I'm guessing is the order ID we
have a date up something new we've done
integers and strings but we haven't done
date when you're importing new and you
never worked with the date date's always
one of the more trickier fields to port
in and that's true of just about any
scripting language I've worked with all
of them have their own idea of how dates
supposed to be formatted what the
default is this particular format or its
year and it has all four digits Dash
month two digits Dash day is the
standard import for the hive so you'll
have to look up and see what the
different formats are if you're going to
do a different format in there coming in
or you're not able to pre-process the
data but this would be a pre-processing
of the data thing coming in if you
remember correctly from our adult which
is uh e just in case you weren't able to
hear me last time
ETL which stands for extract transform
then load so you want to make sure
you're transforming this data before it
gets into here and so we're going to go
ahead and bring both this data in here
and really we're doing this so we can
show you do the basic join there is if
you remember from our setup merge join
all kinds of different things you can do
but joining different data sets is so
common so it's really important to know
how to do this we need to go ahead and
bring in these two data sets and you can
see where I just created a table
customer here's our schema the integer
name age address salary here's our
delimited by commas and our table
properties where we skip a line well
let's go ahead and load the data first
and then we'll do that with our order
and let's go ahead and put that in here
and I've got it split into three lines
you can see it easily we've got load
data local in path so we know we're
loading data we know it's local and we
have the path here's the complete path
for this is supposed to be order CSV
grab the wrong one of course it's going
to give me errors because you can't
recreate the same table on there and
here we go create table here's our
integer date customer the basic setup
that we had coming in here for our
schema row format commas table
properties skip header line and then
finally let's load the date data into
our order table load data local in path
home Cloudera documents ordered at CSV
into table order now if we did
everything right we should be able to do
select star from customer and you can
see we have all seven customers and then
we can do select star from order and we
have four orders so this is just like a
quick frame we have you know a lot of
times when you have your customer
databases in business you have thousands
of customers from years and years and
some of them you know they move they
close their business they change names
all kinds of things happen so we want to
do is we want to go ahead and find just
the information connected to these
orders and who's connected to them and
so let's go ahead and do it's a select
because we're going to display
information so select and this is kind
of interesting we're going to do c dot
ID and I'm going to Define c as customer
as a customer table in just a minute
then we're going to do c dot name and
again we're going to define the C C dot
age so this means from the customer we
want to know their ID their name their
age and then you know I'd also like to
know the order amount so let's do o for
DOT amount and then this is where we
need to go ahead and Define what we're
doing and I'll go and capitalize from
customer so we're going to take the
customer table in here and we're going
to name it C that's where the C comes
from so that's the customer table C and
we want to join order as o that's where
our o comes from so the O DOT amount is
what we're joining in there and then we
want to do this on we got to tell it how
to connect the two tables C dot ID
equals o Dot customer underscore ID so
now we know how they're joined and
remember we have seven customers in here
we have four orders and as it processes
we should get a return of four different
names joined together and they're joined
based on of course the orders on there
and once we're done we now have the
order number the person who made the
order their age and the amount of the
order which came from the order table so
you have your different information and
you can see how the join works here very
common use of tables and hql and SQL and
let's do one more thing with our
database and then I'll show you a couple
other Hive commands and let's go ahead
and do a drop and we're going to draw a
database office and if you're looking at
this and you remember from earlier this
will give me an error and let's see what
that looks like it says failed to
execute exception one or more tables
exist so if you remember from before you
can't just drop a database unless you
tell it to Cascade that lets it no I
don't care how many tables are in it
let's get rid of it and in Hadoop since
it's an R it's a warehouse a data
warehouse you usually don't do a lot of
dropping maybe at the beginning when
you're developing the schemas and you
realize you messed up you might drop
some stuff but down the road you're
really just adding commodity machines to
take up so you can store more stuff on
it so you usually don't do a lot of
database dropping and some other fun
commands to know is you can do select
round 2.3 as round value you can do a
round off in Hive we can do as floor
value which is going to give us a two so
it turns it into an integer versus a
float it goes down you know basically
truncates it but it goes down and we can
also do ceiling which is going to round
it up so we're looking for the next
integer above there's a few commands we
didn't show in here because we're on a
single node as as an admin to help
expediate the process you usually add in
partitions for the data and buckets you
can't do that on a single node because
the when you add a partition it
partitions it across separate nodes but
beyond that you can see that it's very
straightforward we have SQL coming in
and all your basic queries that are in
SQL are very similar to hql let's get
started with pig why Pig what is pig map
reduce versus Hive versus pig hopefully
you've had a chance to do our Hive
tutorial in our map reduce tutorial if
you haven't send a note over to Simply
learn and we'll follow up with a link to
you we'll look at Pig architecture
working a pig pig latin data model Pig
execution modes a use case Twitter and
features a pig and then we'll tag on a
short demo so you can see Pig In Action
so why pig as we all know Hadoop uses
mapreduce to analyze and process big
data processing Big Data consumed more
time so before we had the Hadoop system
we'd have to spend a lot of money on a
huge set of computers and Enterprise
machines so he introduced the Hadoop map
reduce and so afterwards processing Big
Data was faster using mapreduce then
what is the problem with map reduce
prior to 2006 all mapreduce programs
were written in Java non-programmers
found it difficult to write lengthy Java
codes they faced issues in incorporating
map sort reduce to fundamentals of
mapreduce while creating a program you
can see here Matt face Shuffle and sort
reduce phase eventually it became a
difficult task to maintain and optimize
a code due to which the processing time
increased you can imagine a manager
trying to go in there and needed a
simple query to find out data and he has
to go talk to the programmers anytime he
wants anything so that was a big problem
not everybody wants to have a on-call
programmer for every manager on their
team Yahoo faced problems to process and
analyze large data sets using Java as
the codes were complex and lengthy there
was a necessity to develop an easier way
to analyze large data sets without using
time-consuming complex Java modes and
codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it was
developed with a vision to analyze and
process large data sets without using
complex Java codes Pig was developed
especially for non-programmers pig used
simple steps to analyze data sets which
was time efficient so what exactly is
pick pig is a scripting form that runs
on Hadoop clusters designed to process
and analyze large data sets and so you
have your pig which uses SQL like
queries they're definitely not SQL but
some of them resemble SQL queries and
then we use that to analyze our data Pig
operates on various types of data like
structured semi-structured and
unstructured data let's take a closer
look at mapreduce versus Hive versus pig
so we start with a compiled language
your map reduce and we have Hive which
is your SQL like query and then we have
pig which is a scripting language it has
some similarities to SQL but it has a
lot of its own stuff remember SQL like
query which is what Hive is based off
looks for structured data and so when
you get into scripting languages like
Pig now we're dealing more with
semi-structured and even unstructured
data with a Hadoop map reduce we have a
need to write long complex codes with
Hive no need to write complex codes you
could just put it in a simple SQL query
or hql Hive ql and in Pig no need to
write complex codes as we have pig latin
now remember in the map reduce it can
produce structured semi-structured and
unstructured data and as I mentioned
before Hive can process only structured
data think rows and columns where Pig
can process structured semi-structured
and unstructured data you can think of
structured data as rows and columns
semi-structured as your HTML XML
documents if you have on your web pages
and unstructured could be anything from
groups of documents and written format
Twitter tweets any of those things come
in as very unstructured data and with
our Hadoop mapreduce we have a lower
level of abstraction with both Hive and
pig we have a higher level abstraction
so it's much more easy for someone to
use without having to dive in deep and
write a very lengthy map reduce code and
those map and reduce codes can take 70
80 lines of code when you can do the
same thing in one or two lines with
however Pig this is the advantage Pig
has over Hive it can process only
structured data in Hive while in pig it
can process structured semi-structured
and unstructured data some other
features to note that separates the
different query languages is we look at
map and reduce map reduce supports
partitioning features as does Hive Pig
no concept of partitioning in pigs it
doesn't support your partitioning
feature your partitioning features allow
you to partition the data in such a way
that it can be queried quicker you're
not able to do that in pig mapreduce
uses Java in Python while Hive uses an
SQL like query language known as Hive ql
or hql Pig Latin is used which is a
procedural data flow language mapreduce
is used by programmers pretty much as
straightforward on Java Hive is used by
data analysts pig is used by researchers
and programmers certainly there's a lot
of mix between all three programmers
have been known to go in and use a hive
for quick query and anybody's been able
to use Pig for a quick query or research
under map and reduce code performance is
really good under Hive code performance
is lesser than map and reduce and pig
under Pig Code performance is lesser
than mapreduce but better than Hive so
if we're going to look at speed and time
the map reduce is going to be the
fastest performance on all of those
where Pig will have second and high
follows in the back let's look at
components of pig pig has two main
components we have pig Latin Pig Latin
is the procedural data flow language
used in pig to analyze data it is easy
to program using Piglet and it is
similar to SQL and then we have the
runtime engine runtime engine represents
the execution environment created to run
Pig Latin programs it is also a compiler
that produces mapreduce programs uses
hdfs or your Hadoop file system for
storing and retrieving data and as we
dig deeper into the pig architecture
we'll see that we have pig latin scripts
programmers write a script in pig latin
to analyze data using Pig then you have
the grunt shell and it actually says
grunt when we start it up and we'll show
you that here in a little bit which goes
into the pig server and this is where we
have our parser parser checks the syntax
of the pig script after checking the
output will be a dag directed acelic
graph and then we have an Optimizer
which optimizes after your dag your
logical plant is passed to The Logical
Optimizer where an optimization takes
place finally the compiler converts the
dag into mapreduce jobs and then that is
executed on the map reduce under the
execution engine the results are
displayed using dump statement and
stored in hdfs using store statement and
again we'll show you that um you kind of
end you always want to execute
everything once you've created it and so
dump is kind of our execution statement
and you can see right here as we were
talking about earlier once we get to the
execution engine and it's coded into
mapreduce then the map reduce processes
it onto the hdfs working of pick Pig
Latin script is written by the users so
you have load data and right Pig script
and pig operations so we look at the
working of pig pig latin script is
written by the users there's step one we
load data and write Pig script and step
two in this step all the pig operations
are performed by parser Optimizer and
compiler so we go into the pig
operations and then we get to step three
execution of the plan in this stage the
results are shown on the screen
otherwise stored in the hdfs as per the
code so it might be of a small amount of
data you're reducing it to and you want
to put that on the screen or you might
be converting a huge amount of data
which you want to put back into the
Hadoop file system for other use let's
take a look at the pig latin data the
data model of pig latin helps pig to
handle various types of data for example
we have Adam Rob or 50. Adam represents
any single value of primitive data type
in pig latin like integer float string
it is stored as a string two bolts so we
go from our atom which are most basic
thing so if you look at just Rob or just
50 that's an atom that's our most basic
object we have in pig latin then you
have a tuple Tuple represents sequence
of fields that can be of any data type
it is the same as a row in rdbms for
example a set of data from a single row
and you can see here we have Rob comma
five and you can imagine with many of
our other examples we've used you might
have the ID number the name where they
live their age their date of starting
the job that would all be one row and
store it as a tuple and then we create a
bag a bag is a collection of tuples it
is the same as a table in rdbms and is
represented by brackets and you can see
here we have our table with Rob 5 mic 10
and we also have a map a map is a set of
key value pairs key is of character
array type and a value can be of any
type it is represented by the brackets
and so we have name and age where the
key value is Mike and 10. pig latin has
a fully nestable data model that means
one data type can be nested within
another here's a diagram representation
of pig latin data model and in this
particular example we have basically an
ID number a name an age and a place and
we break this apart we look at this
model from Pig Latin perspective we
start with our field and if you remember
a field contains basically an atom it is
one particular data type and the atom is
stored as a string which it then
converts it into either an integer or
number or character string next we have
our Tuple and in this case you can see
that it represents a row so our Tuple
would be three comma Joe comma 29 comma
California and finally we have our bag
which contains three rows in it in this
particular example let's take a quick
look at Pig execution modes
it works in two execution modes
depending on where the data is reciting
and where the pig script is going to run
we have local mode here the pig engine
takes input from the Linux file system
and the output is stored in the same
file system local mold local mode is
useful in analyzing small data sets
using pick and we have the mapreduce
mode here the pig engine directly
interacts and executes in hdfs and
mapreduce in the map reduce mode queries
written in pig latin are translated into
mapreduce jobs and are run on a Hadoop
cluster by default Pig runs in this mode
there are three modes in pig depending
on how a pig latin code can be written
we have our interactive mode batch mode
and embedded mode the interactive mode
means coding and executing the script
line by line when we do our example
we'll be in the interactive mode in
batch mode all scripts are coded in a
file with extension dot Pig and the file
is directly executed and then there's
embedded mode Pig lets its users Define
their own functions udfs
programming language such as Java so
let's take a look and see how this works
in a use case in this case use case
Twitter users on Twitter generate about
500 million tweets on a daily basis the
Hadoop mapreduce was used to process and
analyze this data analyzing the number
of tweets created by a user in the Tweet
table was done using mapreduce in Java
programming language and you can see the
problem it was difficult to perform
mapreduce operations as users were not
well versed with written complex Java
codes so Twitter used Apache pig to
overcome these problems and let's see
how let's start with the problem
statement analyze the user table and
tweet table and find out how many tweets
are created by a person and here you can
see we have a user table we have Alice
Tim and John with their ID numbers one
two three and we have a tweet table in
the Tweet table you have your the ID of
the user and then what they tweeted
Google was a good whatever it was
tennis. spacecraft Olympics politics
whatever they're tweeting about the
following operations were performed
analyze a given data first the Twitter
data is loaded into the pig storage
using load command and you can see here
we have our data coming in and then
that's going into Pig storage and this
data is probably on an Enterprise
computer so this is actually active
Twitter's going on and then it goes into
Hadoop file system remember the Hadoop
file system is a data warehouse for
storing data and so the first step is we
want to go ahead and load it into the
pig storage into our data storage system
the remaining operations performed are
shown Below in join and group operation
the tweet and user tables are joined and
grouped using co-group command and you
can see here where we add a whole column
and we go from user names and tweet to
the ID linked directly to the name so
Alice was user one Tim was 2 and John 3
and so now they're listed with their
actual tweet the next operation is the
aggregation the tweets are counted
according to the names the command used
is count so it's very straightforward we
just want to count how many each user is
doing and finally the result after the
count operation is joined with the user
table to find out the username and you
can see here where Alice had three Tim
two and John 1. Pig reduces the
complexity of the operations which would
have been lengthy using mapreduce and
joining group operation the tweet and
user tables are joined and grouped using
co-group command the next operation is
the aggregation the tweets are counted
according to the names the command used
as count the result after the count
operation is joined with the user table
to find out the username and you can see
we're talking about three lines of
script versus a mapreduce code of about
80 lines finally we could find out the
number of tweets created by a user in a
simple way so let's go quickly over some
of the features of pig that we already
went through most of these first ease of
programming as Pig Latin is similar to
SQL lesser lines of code need to be
written short development time as the
code is simpler so we can get our
queries out rather quickly instead of
having to have a programmer spend hours
on it handles all kind of data like
structured semi-structured and
unstructured pig lets us create user
defined functions Pig offers a large set
of operators such as join filter and so
on it allows for multiple queries to
process unparallel and optimization and
compilation is easy as it is done
automatically and internally
foreign
let's dive in and show you a quick demo
on some of the commands you can do and
pick today's setup will continue as we
have in the last three demos to go ahead
and use Cloudera quick start and we'll
be doing this in Virtual box we do have
a tutorial in setting that up you can
send a note to our simply learned team
and then get that linked to you once
your Cloudera quick start is uh spun up
and remember this is virtualbox we've
created a virtual machine and this
virtual machine is Centos Linux once
it's spun up you'll be in a full Linux
system here and as you see we have
Thunderbird browser which opens up to
the Hadoop basic system browser and we
can go underneath the Hue where it comes
up by default if you click on the pull
down menu and go under editor you can
see there's our Impala our Hive uh Pig
along with a bunch of other query
languages you can use and we're going
under Pig and then once you're in pig we
can go ahead and use our command line
here and just click that little blue
button to start it up and running we
will actually be working in terminal
window and so if you're in the Cloudera
quick start you can open up the terminal
window up top or if you're in your own
setup and you're logged in you can
easily use all of your commands here in
terminal window and we'll zoom in that
way you get a nice view of what's going
on there we go now for our first command
we're going to do a Hadoop command and
import some data into the Hadoop system
in this case a pig input let's just take
a look at this we have Hadoop now let's
know it's going to be a Hadoop command
DFS there's actually four variations of
DFS so if you have hdfs or whatever
that's fine all four of them Point used
to be different setups underneath
different things and now they all do the
same thing and we want to put this file
which in this case is under home
Cloudera documents and Sample and we
just want to take that and put it into
the pig input and let's take a look at
that file if I go under my document
browsers and open this up you'll see
it's got a simple ID name profession and
age we have one Jack engineer 25. and
that was in one of our earlier things we
had in there and so let's go ahead and
hit enter and execute this and now we've
uploaded that data and it's gone into
our Pig input and then a lot of the
Hadoop commands mimic the Linux commands
and so you'll see we have cat as one of
our commands or it has a hyphen before
it so we execute that with Hadoop DFS
hyphen cat slash Pig input because
that's what we called it that's where we
put our sample CSV at and we execute
this you can see from our Hadoop system
it's going to go ahead and pull that up
and sure enough it pulls out the data
file we just put in there and then we
can simply enter the pig Latin or Pig
editor mode by typing in pick and we can
see here by our grunt I told you that's
how it was going to tell you we're in
pig latin there's our grunt command line
so we are now in the pig shell and then
we'll go ahead and put our load command
in here and the way this works is I'm
going to have office equals load and
here's my load in this case is going to
be Pig input we have that in single
brackets remember that's where the data
is in the Hadoop file system where we
dumped it into there we're going to
using Pig storage our data was separated
as with a comma so there's our comma
separator and then we have as in this
case we have an ID character array name
character array profession character
array and age character array and we're
just going to do them all as character
arrays just to keep this simple for this
one and then when I hit put this all in
here you can see that's our full command
line going in and we have of our
semicolon at the end so when I hit enter
it's now set office up but it hasn't
actually done anything yet it doesn't do
anything until we do dump office so
there's our Command to execute whatever
we've loaded or whatever setup we have
in here and we run that you can see it
go through the different languages and
this is going through the map reduce
remember we're not doing this locally
we're doing this on the Hadoop setup and
once we've finished our dump you can see
we have ID name profession age and all
the information that we just dumped into
our pick oh we can now do let's say oh
let's say we have a request just for
we'll keep it simple in here but just
for the name and age and so we can go
office we'll call it each as our
variable underscore each and we'll say
for each office generate name comma H
and for each means that we're going to
do this for each row and if you're
thinking map reduce you know that this
is a map function because this map each
row and generating name and age on here
and of course we want to go ahead and
close it with a semicolon and then once
we've created our query or the command
line in here let's go ahead and dump
office underscore each and with our
semicolon and this will go through our
map reduce setup on here and if we were
on a large cluster the same processing
time would happen in fact it's really
slow because I have multiple things on
this computer in this particular virtual
box is only using a quarter of my
processor it's only dedicated to this
and you can see here there it is name
and age and it also included the top row
since we didn't delete that out of there
or tell it not to and that's fine for
this example you need to be aware of
those things when you're processing a
significantly large amount of data or
any data and we can also do office and
we'll call this DSC for descending so
maybe the boss comes to you and says hey
can we order office by ID descending and
of course your boss you've taught them
how to your shareholder it sounds a
little derogatory to say boss you've
talked to the shareholder and you said
and you've taught him a little bit of
Pig Latin and they know that they can
now create office description and we can
order office by ID description and of
course once we do that we have to dump
office underscore description so that
it'll actually execute and there goes
into our map reduce it'll take just a
moment for it to come up because again
I'm running on all the quarter of my
processor and you can see we now have
our IDs in descending order returned
let's also look at and this is so
important with anytime you're dealing
with big data let's create office with a
limit and you can of course do any of
this instead of with office we could do
this with office descending so you get
just the top two IDs on there we're
going to limit just to two and of course
to execute that we have to dump office
underscore limit you can just think of
dumping your garbage into the pig pen
for the pig to eat there we go dump
office limit two and that's going to
just limit our office to the top two 2
and for our output we get our first row
which had our ID name profession and age
and our second row which is Jack who's
an engineer let's do a filter we'll call
it office underscore filter you guessed
it equals filter office by profession
equals and keep note this is uh similar
to how python does it with the double
equal signs for equal for doing a true
false statement so for your logic
statement remember to use two equal
signs in Pig and we're going to say it
equals doctor so we want to find out how
many doctors do we have on our list and
we'll go ahead and do our dump we're
dumping all our garbage into the pig pen
and we're letting Pig take over and see
what it can find out and see who's a
doctor on our list and we find uh
employee ID number two Bob is a doctor
30 years old for this next section we're
going to cover something we see it a lot
nowadays in data analysis and that's
word counting tokenization that is one
of the next big steps as we move forward
in our data analysis where we go from
say stock market analysis of highs and
lows and all the numbers to what are
people saying about companies on Twitter
what are they saying on the web pages
and on Facebook suddenly you need to
start counting words and finding out how
many words are total I mean in the first
part of the document and so on we're
going to cover a very basic word count
example and in this case I've created a
document called wordrose.txt and you can
see here we have simply learned as a
company supporting online learning
simply learn helps people attain their
certifications simply learn as an online
community I love simply learn I love
programming I love data analysis and I
went and saved this into my documents
folder so we could use it and let me go
ahead and open up a new terminal window
for our word count let me go and close
the old one so we're going to go in here
and instead of doing this as Pig we're
going to do pig minus X local and what
I'm doing is I'm telling the pig to
start the pig shell but we're going to
be looking at files local to our virtual
box or the Centos machine and let me go
ahead and hit enter on there just map
this up there we go and it will load Pig
up and it's going to look just the same
as the pig we were doing which was
defaulted to high to our Hadoop system
to our hdfs this is now defaulted to the
local system now we're going to create
lines we're going to load it straight
from the file remember last time we took
the hdfs and loaded it into there and
then loaded it into Pig since we've gone
the local we're just going to run a
local script we have lines equals load
home the actual full path home Cloudera
documents and I called it wordrows.txt
and as line is a character array so each
line and I've actually you can change
this to read each document I certainly
have done a lot of document analysis and
then you go through and do word counts
and different kind of counts in there so
once we go ahead and create our line
instead of doing the dump we're going to
go ahead and start entering all of our
different setups for each of our steps
we want to go through and let's just
take a look at this next one because the
load is straightforward we're loading
from this particular file since we're
locals loading it directly from here
instead of going into the Hadoop file
system and it says as and then each line
is read as a character array now we're
going to do words equal for each of the
lines generate Flat tokenize Line space
as word now there's a lot of ways to do
this this is if you're a programmer
you're just splitting the line up by
spaces there's actual ways to tokenize
it you gotta look for periods
capitalization there's all kinds of
other things you play with with this but
for the most basic word count we're just
going to separate it by spaces the
flattened takes the line and just
creates a it flattens each of the words
out so this is we're just going to
generate a bunch of words for each line
and then each each of those words is as
a word a little confusing in there but
if you really think about it we're just
going down each line separating it out
and we're generating a list of words one
thing to note is the default for
tokenize you can just do tokenized line
without the space in there if you do
that it'll automatically tokenize it by
space you can do either one and then
we're going to do group we're going to
group it by words so we're going to
group words by word so when we we split
it up each token is a word and it's a
list of words and so we're going to
grouped equals group words by word so
we're going to group all the same words
together and if we're going to group
them then we want to go ahead and count
them and so for count we'll go ahead and
create a word count variable and here's
our four each so for each grouped
grouped is our line where we group all
the words in the line that are similar
we're going to generate a group and then
we're going to count the words for each
grouped so for each line regroup the
words together we're going to generate a
group and that's going to count the
words we want to know the word count in
each of those and that comes back in our
word count and finally we want to take
this and we want to go ahead and dump
word count and this is a little bit more
what you see when you start looking at
run scripts you'll see right here these
these lines right here we have have each
of the steps you take to get there so we
load our file for each of our lines
we're going to generate and tokenize it
into words then we're going to take the
words and we're going to group them by
same words for each grouped we're going
to generate a group and we're just going
to count the words so we're going to
summarize all the words in here and
let's go ahead and do our dump word
count which executes all this and it
goes through our mapreduce it's actually
a local Runner you'll see down here you
start seeing where they still have
mapreduce but it's a special Runner
we're mapping it that's a part of each
row being counted and grouped and then
when we do the word count that's the
reducer the reducer creates these keys
and you can see I is used three times a
came up once and came up once is to
continue on down here to attain online
people company analysis simply learn
they took the top rating with four
certification so all these things are
encountered in the how many words are
used in a data analysis this is probably
the very the beginnings of data analysis
where you might look at it and say oh
they mentioned love three times so
whatever's going on in this post it's
about love and uh what do they love and
they need my name might attach that to
the different objects in here so you can
see that pig latin is fairly easy to use
there's nothing really you know it may
it takes a little bit to learn the
script depending on how good your memory
is as I get older my memory leaks a
little bit more so I don't memorize it
as much but that was pretty
straightforward the script we put in
there and then it goes through the full
map reduce localized run comes out and
like I said it's very easy to use that's
why people like Pig Latin is because
it's intuitive one of the things I like
about Pig Latin is when I'm
troubleshooting when we're
troubleshooting a lot of times you're
working with a small amount of data and
you start doing one line at a time and
so I can go lines equal load and there's
my loaded text and maybe I'll just dump
lines and then it's going to run it's
going to show me all the lines that I'm
working on in the small amount of data
and that way I can test that if I got an
error on there that said oh this isn't
working maybe I'll be like oh my gosh
I'm in mapreduce or I'm in the basic
grunt shell instead of the local path
current so maybe it'll generate an error
on there and you can see here it just
shows each of the lines going down Hive
versus pig on one side we'll have our
sharp Stinger on our black and yellow
friend and on the other side our thick
hide on our Pig let's start with an
introduction to hbase back in the days
data used to be less and was mostly
structured when you see we have
structured data here we usually had it
like in a database where you had every
field was exactly the correct length so
if you had a name fill that was exactly
32 characters I remember the old access
database in Microsoft the files are
small if we had you know hundreds of
people in one database that was
considered big data this data could be
easily stored in relational database or
rdbms and we talk about relational
database you might think of Oracle you
might think of SQL Microsoft SQL MySQL
all of these have evolved even from back
then to do a lot more today than they
did but they still fall short in a lot
of ways and they're all examples of an
rdms or relationship database then
internet evolved and here huge volumes
of structured and semi-structured data
got generated and you can see here with
the semi-structured data we have email
if you look at my spam filter you know
we're talking about all the HTML Pages
XML which is a lot of time is displayed
on our HTML and help desk Pages Json all
of this really has just even in the last
each year it almost doubles from the
year before how much of this is
generated so storing and processing this
data on an rdbms has become a major
problem and so the solution is we use
Apache hbase Apache hbase was the
solution for this let's take a look at
the history the hbase history and we
look at the hbase history we're going to
start back in 2006 November Google
released a paper on big table and then
in 2017 just a few months later
age-based prototype was created as a
Hadoop contribution later on in the Year
2007 in October first usable hbase along
with the Hadoop .15 was released and
then in January 2008 hbase became the
sub-project of Hadoop and later on that
year in October all the way into
September the next year hbase was
released to 0.81 version the 0.19
version and 0.20 and finally in May of
2010 hbase became Apache top level
project and so you can see in the course
of about four years hbase started off as
just an idea on paper and has evolved
all the way till 2010 as a solid project
under the Apache and since 2010 it's
continued to evolve and grow as a major
source for storing data in
semi-structured data so what is hbase
hbase is a column oriented database
management system derived from Google's
no SQL database bigtable that runs on
top of the Hadoop file system or the
hdfs it's an open source project that is
horizontally scalable and that's very
important to understand that you don't
have to buy a bunch of huge expensive
computers you're expanding it by
continually adding commodity machines
and so it's a linear cost expansion as
opposed to being exponential no SQL
database written in Java which permits
faster querying so job is to back in for
the hbase setup and it's well suited for
sparse data sets so it can contain
missing or n a values and this doesn't
Boggle it down like it would in other
database companies using hbase so let's
take a look and see who is using this
nosql database for their servers and for
storing their data and we have
hortonworks which isn't a surprise
because they're one of the like Cloudera
hortonworks they are behind Hadoop and
one of the big developments and backing
of it and of course Apache hbase is the
open source behind it and we have
Capital One as Banks you also see Bank
of America where they're collecting
information on people and tracking it so
their information might be very sparse
they might have one Bank way back when
they collected information as far as the
person's family and what their income
for the whole family is and their
personal income and maybe another one
doesn't collect the family income as you
start seeing where you have data that is
very difficult to store where it's
missing a bunch of data how spots using
it Facebook certainly all of your
Facebook Twitter most of your social
medias are using it and then of course
there's JP Morgan Chase and Company
another bank that uses the hbase as
their data warehouse for nose SQL let's
take a look at an hbase use case so we
can dig a little bit more into it to see
how it functions telecommunication
company that provides mobile voice and
multimedia Services across China the
China mobile and China mobile they
generate billions of call detailed
records or CDR and so these cdrs and all
these records of these calls and how
long they are and different aspects of
the call maybe the tower they're
broadcasted from all of that is being
recorded so they can track it a
traditional database systems were unable
to scale up to the vast volumes of data
and provide a cost-effective solution no
good so storing in real-time analysis of
billions of call records was a major
problem for this company solution Apache
hbase hbase stores billions of rows of
detailed call records hp's perform forms
fast processing of Records using SQL
queries so you can mix your SQL and
nosql queries and usually just say no
SQL queries because of the way the query
Works applications of hbase one of them
would be in the medical industry hbase
is used for storing genome sequences
storing disease history of people of an
area and you can imagine how sparsat is
as far as both of those a genome
sequence might be only have pieces to it
that each person is unique or is unique
to different people and the same thing
with disease you really don't need a
column for every possible disease a
person could get you just want to know
what those diseases those people have
had to deal with in that area e-commerce
hbase is used for storing logs about
customer search history performs
analytics and Target advertisement for
Better Business insights sports hbase
stores match details in the history of
each match uses this data for better
prediction so when we look at age base
we all want to know what's the
difference between hbase versus rdbms
that is a relational database base
management system hbase versus rdbms so
the hbase does not have a fixed schema a
schema less defines only column families
and we'll show you what that means later
on an rdbms has a fixed schema which
describes the structure of the tables
and you can think of this as you have a
row and you have columns and each column
is a very specific structure how much
data can go in there and what it does
with the age base it works well with
structured and semi-structured data with
the rdbms it works only well with
structured data with the AIDS base it
can have denormalized data it can
contain missing or null values with the
rdbms it can store only normalized data
now you can still store a null value in
the rdbms but it still takes up the same
space as if we're storing a regular
value in many cases and it also for the
hbase is built for y tables it can be
scaled horizontally for instance if you
were doing a tokenizer of words and word
clusters you might have of 1.4 million
different words that you're pulling up
and combinations of words so with an
rdbms it's built for thin tables that
are hard to scale you don't want to
store 1.4 million columns in your SQL
it's going to crash and it's going to be
very hard to do searches with the age
base it only stores that data which is
part of whatever row you're working on
let's look at some of the features of
the hbase it's scalable data can be
scaled across various nodes as it is
stored in the hdfs and I always think
about this it's a linear add-on for each
terabyte of data I'm adding on roughly a
thousand dollars in commodity Computing
with an Enterprise machine we're looking
at about 10 000 at the lower end for
each terabyte of data and that includes
all your backup and redundancy so it's a
big difference it's like a tenth of the
cost to store it across the hbase it has
automatic failure support right ahead
log across clusters which provides
automatic support against failure
consistent read and write hbase provides
consistent read and write of the data
it's a Java API for client access
provides easy to use Java API for
clients block cache and Bloom filters so
the hbase supports block caching and
Bloom filters for high volume query
optimization let's dig a little deeper
into the hbase storage a space column
oriented storage and I told you we're
going to look into this to see how it
stores the data and here you can see you
have a row key this is really one of the
important references is each row has to
have its own key or your row ID and then
you have your column family and in here
you can see we have column family one
column family two column family three
and you have your column qualifiers so
you can have in column family one you
can have three columns in there and
there might not be any data in that so
when you go into column family one and
do a query for every column that
contains a certain thing that row might
not have anything in there and not be
queried where in column family two maybe
you have column one filled out and
column three filled out and so on and so
forth and then each cell is connected to
the row where the data is actually
stored let's take a look at this and
what it looks like when you fill the
data in so in here we have a row key
with a row ID and we have our employee
ID one two three that's pretty
straightforward you probably would even
have that on an SQL server and then you
have your column family this is where it
starts really separating out your column
family might have personal data and
under personal data you would have name
City age you might have a lot more than
just that you might have number of
children you might have degree all those
kinds of different things that go under
personal data and some of them might be
missing you might only have the name and
the age of an employee you might only
have the name the city and how many
children and not the age and so you can
see with the personal data you can now
collect a large variety of data and
stored in the hbase very easily and then
maybe you have a family of professional
data your designation your salary all
the stuff that the employee is doing for
you in that company let's dig a little
deeper into the hbase architecture and
so you can see here what looks to be a
complicated chart it's not as
complicated
from the Apache a space we have the
Zookeeper which is used for monitoring
what's going on and you have your age
Master this is the hbase master of
science regions and load balancing and
then underneath the region or the hbase
master then under the H master or H base
Master you have your reader server
serves data for read and write and the
region server which is all your
different computers you have in your
Hadoop cluster you'll have a region an H
log you'll have a store memory store and
then you have your different files for
each file that are stored on there and
those are separated across the different
computers and that's all part of the
hdfs storage system so we look at the
Architectural Components or regions and
we're looking at we're drilling down a
little bit hbase tables are divided
horizontally by a row so you have a key
range into regions so each of those IDs
you might have IDs one to twenty twenty
one to 50 or whatever they are regions
are assigned to the nodes in the cluster
called region servers a region contains
all rows in the table between the region
start key and the End Key again 1 to 10
11 to 20 and so forth these servers
serve data for read and write and you
can see here we have the client and the
get and the git sends it out and it
finds out where that startup is between
which start keys and end keys and then
it pulls the data from that different
region server and so the region signed
data definition language operation
create delete are handled by the H
master so the H Master is telling it
what are we doing with this data what's
going out there assigning and
reassigning regions for Recovery or load
balancing and monitoring all servers so
that's also part of it so you know if
your IDs if you have 500 IDs across
three servers you're not going to put
400 IDs on server 1 and 100 on the
server 2 and leaves Region 3 and Region
4 empty you're going to split that up
and that's all handled by the H master
and you can see here it monitors region
servers assigns regions to region
servers assigns regions to Regions
servers and so forth and so forth hspace
has its distributed environment where
age Master alone is not sufficient to
manage everything hence zookeeper was
introduced it works with h master so you
have an active age Master which sends a
heartbeat signal to zookeeper indicating
that is active and the Zookeeper also
has a heartbeat to the region servers so
the region servers send their status to
zookeeper indicating they are ready for
read and write operation inactive server
acts as a backup if the active h Master
fails it'll come to the rescue active
Ace master and region servers connect
with a session to zookeeper so you see
your active h Master selection region
server session they're all looking at
the Zookeeper keeping that pulse an
active age master and region server
connects with a session to the Zookeeper
and you can see here where we have
ephemeral nodes for active sessions via
heartbeats to indicate that the region
servers are up and running so let's take
a look at hbase read or write going on
there's a special hbase catalog table
called The Meta table which holds a
location of the regions in the cluster
here's what happens the first time a
client reads or writes data to age base
the client gets the region server the
host the meta table from zookeeper and
you can see right here the client has a
request for your region server and goes
hey zookeeper can you handle this the
Zookeeper takes a look at it and goes ah
metal location is stored in Zookeeper so
it looks at its metadata on there and
then the metadata table location is sent
back to the client the client will query
The Meta server to get the region server
corresponding to the row key if it wants
to access the client caches this
information along with the meta table
location and you can see here the client
going back and forth to the region
server with the information and it might
be going across multiple region servers
depending on what you're querying so we
get the region server for row key from
The Meta table that's where that row key
comes in and says hey this is where
we're going with this and so once it
gets the row key from the corresponding
region server we can now put row or get
Row from that region server let's take a
look at the hbase meta table special HP
catalog table that maintains a list of
all the region servers in the hbase
storage system so you see here we have
the meta table we have a row key and a
value table key region region server so
the meta table is used to find the
region for the given table key and you
can see down here you know our meta
table comes in is going to fire out
where it's going with the region server
and we look a little closer at the write
mechanism in hbase we have write a
headlock our wall as you abbreviate it
kind of a way to remember wall is right
ahead log is a file used to store new
data that is yet to be put on permanent
storage it is used for Recovery in the
case of failure so you can see here
where the client comes in and it
literally puts the new data coming in
into this kind of temporary storage or
the wall on there once it's gone into
the wall then the memory store mem store
is the right cache that stores a new
data that has not yet been written to
disk there is one mem store per column
family per region and once we've done
that we have three ack once the data is
placed in mimstore the client then
receives the acknowledgment when the
minister reaches the threshold it dumps
or commits the data into H file as you
can see right here we've taken our or
has gone into the wall the wall then
Source it into the different memory
stores and then the memory stores it
says Hey we've reached we're ready to
dump that into our H files and then it
moves it into the age files age files
store the Roses data as stored key value
on disk so here we've done a lot of
theory let's dive in and just take a
look and see what some of these commands
look like and what happens in our age
base when we're manipulating a nosql
setup
[Music]
so if you're learning a new setup it's
always good to start with where is this
coming from it's open source by Apache
and you can go to
hbase.apache.org and you'll see that it
has a lot of information you can
actually download the hbase separate
from the Hadoop although most people
just install the Hadoop because it's
bundled with it and if you go in here
you'll find a reference guide and so you
can go through the Apache reference
guide and there's a number of things to
look at but we're going to be going
through Apache H based shell that's what
we're going to be working with and
there's a lot of other interfaces on the
setup and you can look up a lot of the
different commands on here so we go into
the Apache hbase reference guide we can
go down to read hbase shell commands
from a command file you can see here
where it gives you different options of
formats for putting the data in and
listing the data certainly you can also
create files and scripts to do this too
but we're going to look at the basics
we're going to go through this on a
basic hbase shell and one last thing to
look at is of course if you continue
down the setup you can see here where
they have more detail tell as far as how
to create and how to get to your data on
your hbase now I will be working in a
virtual box and this is by Oracle you
can download the Oracle virtual box you
can put a note in below for the YouTube
as we did have a previous session on
setting up virtual setup to run your
Hadoop system in there I'm using the
Cloudera quick start installed in here
there's Hortons you can also use the
Amazon web service there's a number of
options for trying this out in this case
we have Cloudera on the Oracle virtual
box the virtual box has Linux Centos
installed on it and then the Hadoop it
has all the different Hadoop flavors
including hbase and I bring this up
because my computer is a Windows 10 the
operating system of the virtual box is
Linux and we're looking at the hbase
data warehouse and so we have three very
different entities all running on my
computer and that can be confusing if
it's a first time in and working with
this kind of setup now you'll notice in
our Cloudera setup they actually have
some hbase monitoring so I can go
underneath here and click on hbase and
master and it'll tell me what's going on
with my region servers it'll tell me
what's going on with our backup tables
right now I don't have any user tables
because we haven't created any and this
is only a single node and a single hbase
tour so you're not going to expect
anything too extensive in here since
this is for practice and education and
perhaps testing out package you're
working on it's not for really you can
deploy Cloudera of course but when you
talk about a quick start or a single
node setup that's what it's really for
so we can go through all the different
hbase and you'll see all kinds of
different information with zookeeper if
you saw it flash buy down here what
version we're working in some
zookeeper's part of the hbase setup
where we want to go is we want to open
up a terminal window and in Cloudera it
happens to be up at the top and when you
click on here you'll see your Cloudera
terminal window open and let me just
expand this so we have a nice full
screen and then I'm also going to zoom
in that way I have a nice big picture
and you can see what I'm typing and
what's going out on and to open up your
age base shell simply type h base shell
to get in and hit enter and you'll see
it takes just a moment to load and we'll
be in our age based shell for doing
hbase commands once we've gotten into
our H base shell you'll see you'll have
the hbase prompt information ahead of it
we can do something simple like list
this is going to list whatever tables we
have it so happens that there's a base
table that comes with hbase now we can
go ahead and create and I'm going to
type in just create what's nice about
this is it's going to throw me kind of a
it's going to say hey there's no just
straight create but does come up and
tell me all these different formats we
can use for create so we can create our
table and one of our families you had
splits names versions all kinds of
things you can do with this let's just
start with a very basic one on here and
let's go ahead and create and we'll call
it new table now this is to call it new
TBL for table new table and then we also
want to do let's do knowledge so let's
take a look at this I'm creating a new
table and it's going to have a family of
knowledge in it and let me hit enter
it's going to come up it's going to take
it a second to go ahead and create it
now we have our new table in here so if
I go list you'll now see table and new
table so you can now see that we have
the new table and of course the default
table that's set up in here and we can
do something like uh describe we can
describe and then we're going to do new
TBL and when we describe it it's going
to come up it's going to say hey name I
have knowledge data block encoding none
Bloom filter row or replications Go
version all the different information
you need new minimum version zero
forever deleted cells false block size
in memory and you can look this stuff up
on a apache.org to really track it down
one of the things that's important to
note is versions so you have your
different versions of the data that's
stored and that's always important to
understand that we might talk about that
a little bit later on and then after we
describe it we can also do a status the
status says I have one active Master
going on that's our H base as a whole we
can do status summary I should do the
same thing as status so we got the same
thing coming up and now that we've
created let's go ahead and put something
in it so we're going to put new TBL and
then we want Row one you know what
before I even do this let's just type
input and you can see when I type in put
it gives us like a lot of different
options of how it works and different
ways of formatting our data as it goes
in and all of them usually begin with
the new table new TBL then we have in
this case we'll call it Row one and then
we'll have knowledge if you remember we
created knowledge already and we'll do
knowledge Sports and then in knowledge
and sports we're going to set that equal
to Cricut so we're going to put
underneath this our knowledge setup that
we have a thing called Sports in there
and we'll see what this looks like in
just a second let's go ahead and put in
we'll do a couple of these let's see
let's do another row one and this time a
set of sports Let's Do Science you know
this person not only we have Row one
which is both knowledgeable and cricket
and also in chemistry so it's a chemist
who plays Cricut in row one and let's
see if we have let's do another row one
just to keep it going and we'll do
science in this case let's do physics
not only in chemistry but also physicist
I have quite a joy in physics myself so
here we go we have Row one there we go
and then let's do row two let's see what
that looks like when we start putting in
row two and in row two this person is
has knowledge in economics this is a
master of business and how or maybe it's
Global economics maybe it's just for the
business and how it fits in with the
country's economics and it would call it
macroeconomics so I guess it is for the
whole country there so we have knowledge
economics macroeconomics and then let's
just do one more we'll keep it as row
two and this time our Economist is also
a musician so we'll put music and they
happen to have knowledge and they enjoy
oh let's do pop music they're into the
current pop music going on so we've
loaded our database and you'll see we
have two rows Row one and row two in
here and we can do is we can list the
contents of our database by simply doing
scan uh scan and then let's just do scan
by itself so you can see how that looks
you can always just type in there and it
tells you all the different setups you
can do with scan and how it works in
this case we want to do scan new TBL and
in our scan new TBL we have Row one row
one row two row two and you'll see Row
one has a column called knowledge
science time step value crickets value
physics so it has information as when it
was created when the timestamp is Rule
one also has knowledge Sports and a
value of Cricket so we have sports and
Science and this is interesting because
if you remember up here we also gave it
originally we told it to come in here
and have chemistry we had science
chemistry and science physics and we
come down here I don't see the chemistry
why because we've now replaced chemistry
with physics so the new value is physics
on here let me go ahead and clear down a
little bit and in this we're going to
ask the question is enabled new table
when I hit enter in here you're going to
see it comes out true and then we'll go
ahead and disable it let's go ahead and
disable new table make sure I have our
quotes around it and now that we've
disabled it what happens when we do the
scan we do the scan new table and hit
enter you're going to see that we get an
error coming up so once it's disabled
you can't do anything with it until we
re-enable it now before we enable the
table Let's do an alteration on it and
here's our new table and this should
look a little familiar because it's very
similar to create we'll call this test
info we'll hit enter in there it'll take
just a moment for updating and then we
want to go ahead and enable it so let's
go ahead and enable our new table so
it's back up and running and then we
want to describe describe new table and
we come in here you'll now see we have
name knowledge and under there we have
our data encoding and all the
information under knowledge and then we
also have down below test info so now we
have the name test info and all the
information concerning the test info on
here and we'll simply enable it new
table so now it's enabled oops I already
did that I guess we'll enable it twice
and so let's start looking at well we
had scan new table and you can see here
where it brings up the information like
this but what if we want to go ahead and
get a row so we'll do R1 and when we do
hbase R1 you can see we have knowledge
science and it has a timestamp value
physics and we have knowledge Sports and
it has a time stamp on it and value
Cricut and then let's see what happens
we put into to our new table and in here
we want Row one and if you can guess
from earlier because we did something
similar we're going to do knowledge
economics and then it's going to be
instead of I think it was what
macroeconomics is now market economics
and we'll go back and do our git command
and now see what it looks like and we
can see here where we have knowledge
economics it has a timestamp value
market economics physics and Cricut and
this is because we have economic science
and sports those are the three different
columns that we have and then each one
has different information in it and so
if you've managed to go through all
these commands and look at Basics on
here you'll now have the ability to
create a very basic hbase setup no SQL
setup based on your columns and your
rows and just for fun we'll go back to
the Cloudera where they have the website
up for the hbase master status and I'll
go ahead and refresh it and then we can
can go down here and you'll see user
tables table set one and we can click on
details and here's what we just did it
goes through so if you're the admin
looking at this you can go oh someone
just created new TBL and this is what
they have underneath of it in their new
table and so if you are interested in
mastering the world of data engineering
look no further than our postgraduate
program in data engineering this
comprehensive course is tailor made for
professionals like you diving deep into
the essential topics such as Hadoop
framework spark based data processing
Kafka driven data pipelines and the
introsities of managing big data on AWS
and Azure Cloud infrastructures a unique
approach Blends live session Hands-On
industry projects exciting IBM
hackathons and interactive ask me
anything sessions to provide you with
the most enriching learning experience
elevate your data engineering skills and
career prospects today for admission to
this data engineering course a
bachelor's degree with an average of 50
or higher marks is required are two plus
years of work experience is preferred
and basic understanding of object
oriented programming is preferred so
enroll now today we're covering the
Hadoop ecosystem at least a very
fundamentals of all the different parts
that are in the
ecosystem and it's very robust it's
grown over the years with different
things added in there's a lot of
overlapping a lot of these tools but
we're just going to cover these basic
tools so you can see what's available in
the Hadoop ecosystem so let's go back to
our Hadoop ecosystem as you can see we
have all our different setup and let's
focus on the Hadoop part of it first
before we look at the different tools we
start with the Hadoop or hdfs is for
data storage write once read many times
you can store a lot of data on it
affordably distributed file system and
so we talk about the Hadoop file system
it stores different formats of data on
various machines and so you have like a
huge cluster of computers and you're
able to store Word documents
spreadsheets structured data
non-structured data semi-structured and
in the Hadoop file system there's the
two different sets of servers in there
there's the name node which is the
master we talked about that that's your
Enterprise computer and the other
component is your data nodes and so
you'll usually have like a said one to
two you'll have a name node and maybe a
backup name node and then you'll have as
many data nodes as you want and you can
just keep adding them that's what makes
it so affordable you know you have a
rack of computers and you go oh I need
more space you just add another Rack in
so it's very affordable and very easy to
expand and the way the Hadoop file
system itself Works behind the hood is
it splits the data into multiple blocks
by default it's 128 megabytes and the
120 megabytes it is a default setting
you can change that that works for most
data there's reasons for either
processing speed or for better
distribution of the data so if you have
little tiny blocks of data that are less
than 128 megabytes if you have a lot of
those you might want to go down in size
and vice versa for larger blocks and you
can see right here we have 300 megabytes
and it takes that piece of data and it
just divides it into blocks of data and
each one's 128 128 and 44 which if you
add together equals 300 megabytes and
now that you understand the Hadoop file
system or at least the basic overview of
it it's important to note what it sits
on what's actually making all this work
on the back end and this is yarn Hadoop
yarn is a cluster Resource Management so
it's how it manages this whole cluster
right here that we just looked at and
yarn stands for yet another resource
negotiator love the title reminds me of
an Iron Man movie with you know Tony
Starks and Jarvis just a rather
intelligent system or whatever stood for
but yarn has become very widely used and
it's actually used as a back end for a
lot of other packages so it's not just
in Hadoop but Hadoop is where it came
from and where it's set up and there's
some other ones another probably one is
mesos which I'll mention again briefly
and so the yarn it handles the cluster
of notes it's the one the when you hear
yarn it's someone that's going Hey
where's our Ram at where's our hard
drives at or if you have a solid state
disk drive your SD where's that at how
much memory do I have what can I put
where and so here we go nice image of it
RAM memory sources so it's allocating
all these different resources for
different applications is what it's
doing when we talk about the back to the
two major components the two major
components is your resource manager
that's on the master server or your
Enterprise computer and then that one is
in control and managing what's going on
with all of your nodes data processing
in Hadoop mapreduce and we're going to
talk about the map reduce here in just a
second the Hadoop data processing is all
built upon mapreduce map reduce
processes a large volumes of data in
parely distributed manner this is very
core to Hadoop but before we go on
because there's other tools out there
and things are slowly shifting and
there's all kinds of new things one of
the things you want to look at is not
just how the map reduce works but start
thinking map reduce one of the best
pieces of advice I had from a one of my
mentors in data science was think map
reduce this is really what you should be
but it is an actual process in the
Hadoop system so we have our big data
and the Big Data Maps out and so this is
the first step is if I'm looking at my
data how do I map that data out what am
I looking at and it could be something
as simple as I just loaded into you know
I'm just looking at one line at a time
but it could be that I'm looking at the
data one line at a time but I only need
columns one and four maybe I'm looking
at it one column at a time but I need
the total of column one added together
and column one over column two so you
can start to see and get some very
complicated mapping here but the mapping
is what do you do with each line of data
each piece of data if you're in a
spreadsheet it's easy to see you have a
row whatever you do to that row that's
what you're mapping because it doesn't
look at anything else it doesn't know
anything else all it knows is what's on
that row if you're looking at documents
maybe it's pulling one document at a
time and so your map is then a document
and then it takes those and we Shuffle
and sort them how do we sort them around
whether you're grouping them together
whether you're taking the whatever ever
met information you mapped out of it
word counts you're counting the word a
so letter A comes out with how many for
each mapping if you have 54 A's per the
one document 53 and the other ones
that's what's coming out of that mapping
and going into the shuffle and sort and
so if it's counting A's and B's and C's
it'll Shuffle and sort all the A's
together all the B's together if you're
running a big data for running
agriculture apples and oranges so it
puts all the apples in one all the stuff
you mapped out that you said hey these
are all apples these are all oranges
let's shuffle them and sort them
together and then we reduce and reduce
so each of these groups reduce into the
data you want out so you have map
Shuffle sort reduce and some important
things to know about the Hadoop file
system because I'm going to mention
spark in a little bit is a Hadoop file
system manages to do a lot of this by
writing it to the hard drive so if
you're running a really low budget which
nobody does anymore with the Hadoop file
system and you have all your commodity
machines are all low they only have a
eight gigabytes of memory instead of 128
gigabytes this process uses the hard
drive and so it pulls it into the RAM
for your mapping it Maps a one piece of
data then it writes that map to the hard
drive then it takes that mapping from
the hard drive loads it up and shuffles
it and writes it back to the hard drive
to a different spot and then takes that
information and starts processing it in
the reduce and then it writes the
reduced answer to the hard drive it runs
slower because you're accessing to and
from your hard drive or solid state
drive if you have an SD card in there
but you can also utilize it's a lot more
affordable you know it's like I said you
having that higher end of ram cost even
on a commodity machine so you can save a
lot of money nowadays it's so affordable
people run the spark setup on there
which does the same thing but in Ram and
again we'll talk about spark in just a
little bit and of course the final thing
is an output so again your reducer
written to the hard drive and then your
reduce is brought together to form one
output what is the maximum number of
oranges sold per an area I don't know
I'm making that up one of the things
about Hadoop is it covers so many
different things that anything you can
think of you can put in Hadoop the
question is do you need to do you have
enough data or enough need for the
high-end processing so again look at map
or reduce but think of this not just as
mapreduce start thinking map and reduce
when you think big data we're going to
start getting into the tools because you
had all those pictures of all those Cool
Tools we have so we're going to look at
the first two of those tools and in the
tools we have scoop and Flume and this
is for your data collection and
ingestion we're going to bring spark up
in just a second can also play a major
role in this because spark is its own
animal that sprung from Hadoop connects
into Hadoop but it can run completely
independent but scoop and Flume are
specific to Hadoop and there are ways to
bring in information into the Hadoop
file system and so when we talk about
these we'll start with scoop scoop is
used to transfer data between Hadoop and
external data stores such as relational
databases and Enterprise data warehouses
and so you can see right here here's our
Hadoop data and it's connecting up there
and it's either pushing the data or
pulling the data and we have a
relational database and Enterprise data
warehouse and there's that magic word
Enterprise that means these are the
servers that are very high-end so maybe
you have a high-end SQL server or a
bicycle server or whatever over there
and this is what's coming and going from
the scoop it Imports data from external
data stores into the hdfs hive and hbase
those are two specific The Hive setup is
your SQL basically and you can see a
nice little image here here's somebody
on their laptop which would be
considered the client machine putting
together their code they push the scoop
the scoop goes into the task manager the
task manager then goes hey what have I
got for the hbase And Hive system in our
Hadoop system and then it reaches out to
the Enterprise data warehouse or into
the document based system or
relationship based database relationship
is your non-sql document is just what it
sounds like is you have HTML documents
or Word documents or text documents and
it's able to map those tasks and then
bring those into the Hadoop file system
now Flume is a distributed service for
collecting aggregating and moving large
amounts of log data so kind of focused a
little bit on a slightly different set
of data although you'll find these
overlap a lot you can certainly use
Flume to do a lot of things you can do
in scoop and vice versa Flume ingests
the data so we're looking at like say a
Json call to a website XML documents
unstructured and semi-structured data is
most commonly digested by Flume best
example I saw was a Twitter account
pulling Twitter feeds into a Hadoop
system so it ingests online streaming
data from social media Twitter log files
so we want to know what's going on with
error codes on your servers and all
those log files web server what's going
on in your web server they can bring all
this stuff in and just dump it into the
Hadoop data file system to be looked at
and processed later and it's a web
server cloud social media data again all
those different sources it can be it's
kind of endless you know it just depends
on what your company needs versatility
of Hadoop is what makes it such a
powerful source to add into a company
and so it comes in there you have your
Source it goes through the channels it
then goes through kind of a sync feature
to make sure everything is in sync and
then it dumps it into the Hadoop file
system so we've covered in the Hadoop
file system the first two things let's
look at some of the scripting languages
they have and so we have the two here
and you can also think of these it
actually says scripting and SQL queries
a lot of times they're both referred to
as queries so you have both the Pig and
the hive and pig is used to analyze data
in Hadoop it provides a high level data
processing language to perform numerous
operations on the data and it's made out
of pig Latin language for scripting Pig
Latin compiler converts Pig Latin code
to execute code and then you have your
ETL the ETL provides a platform for
building data flow for ETL and ETL is a
catch three letters now on any job
interview I look at it says ETL it just
means extract transfer and load so all
we're doing is extracting the data
transferring it to where we need it and
then loading it into in this case a
Hadoop file system and there's other
pieces to that you know it's actually a
big thing because whenever you're
extracting data do you want to dump all
the data or do you want to do some kind
of pre-processing so you only bringing
in what you want and then one of the
cool things about Pig Latin is 10 lines
of pig latin script is around 200 lines
of map reduce job again the map reduce
is the back end processes that go on so
if we have pig and I'll be honest with
you pig is very easy to use but as a
scripter programmer I find it's more for
people who just need a quick pull of the
data and able to do some very basic
things very easily so if you're doing
some very high-end processing and model
building you usually end up in something
else so pigs great for that like if
you're in the management you need to
build a quick query report pig is really
good for that and so it definitely has
its place it's definitely a very useful
script to know and the pig latin scripts
I call it the grunt shell I guess it
goes with pig because they grunt you
have your pig server you have a parser
an Optimizer a compiler an execution
engine and that's all part of the Apache
Pig and this then goes into the map
reduce which then goes into the Hadoop
file system in the Hadoop and you'll see
Apache with a lot of these because it's
under the open source hadoops under the
Apache open source so all this stuff is
you'll see under Apache with the name
tag on there if you've got no Pig Hive
is the other one that's really popular
for easy query Hive facilitates Reading
Writing and managing large data sets
residing in the distributed storage
using SQL Hive query language and this
is important because a lot of times
you're coming from an SQL Server there's
your Enterprise set up and now you're
archiving a history of what's going on
on that server you're continually
pulling data using scoop onto your into
your hbase hive database and as it comes
in there it'd be nice to just use that
same query to pull the data out of the
Hadoop file system and that's exactly
what it does so you have Hive command
line you can also use a JBC or odbc
driver and those drivers like if you're
working in Java or you're working in
Python you can use those drivers to
access Hive so you don't have to go
through the hive command line but it
makes it real quick I can take a hive
command line and just punch in a couple
words and pull up my data and so it
provides user-defined functions
UDF for data mining document indexing
log processing again anything that is in
some kind of SQL format if it's stored
that properly on the Hadoop file system
you can use your hive to pull it out and
you see even a more robust image of
what's in the hive there's your client
machine that's you on your laptop or
you're logged in wherever you're at
writing your script that goes into the
driver we have have your compiler your
Optimizer executor you also have your
jdbc odbc connections which goes into
the high Thrift server which then goes
into the driver your hive web interface
so you can just log in on the web and
start typing away your SQL commands and
that again goes to the driver and all
those pieces and those pieces go to your
job tracker your name node and your
actual Hadoop file system so spark
real-time data analysis spark is an open
source distributed computing engine for
processing and analyzing huge volumes of
real-time data so it runs 100 times
faster than map reduce map reduce is the
basics of the Hadoop system which is
usually set up in a Java code in spark
the map reduce instead of running what
happens in mapreduce is it goes pulls it
into the ram writes it to the hard drive
reads it off the hard drive shuffles it
around writes it back to the hard drive
pulls it off the hard drive does its
processing and you get the impression
it's going in and out of ram to the hard
drive and back up again so if you don't
have a lot of RAM and you have older
computers and you don't have the ram to
process something then spark and Hadoop
are going to run the same speed
otherwise spark goes hey let's keep this
all on the RAM and run faster and that's
what we're talking about it provides
in-memory computation of data so it's
fast you can see the guy running through
the door there speedy versus a very slow
Hadoop wandering around when it's used
to process and analyze real-time
streaming data such as stock market and
baking data so the spark can have its
own stuff going on and then it can
access the Hadoop database it can pull
data just like scoop and Flume do so it
has all those features in it and you can
even run spark with its own yarn outside
of Hadoop there's also a spark on mesos
which is another resource manager
although yarn is most commonly used and
currently spark pretty much becomes
installed with Hadoop and so your spark
running on top of Hadoop will use the
same nodes it has its own manager and it
utilizes the ram so you'll have both you
can have the spark on top of that here
and here we go you have your driver
program your spark context it goes into
your cluster manager your yarn then you
have your worker nodes which are going
to be executing tasks and cash and it's
important to remember when you're
running the spark setup even though
these are commodity machines we're still
usually a lot of them are like 128
gigabytes of RAM so that spark can run
these high-end processes certainly if
it's not access very much and you're
building a Hadoop cluster you could drop
that Ram way down if you're doing just
queries without any kind of processing
on there and you're not doing a lot of
queries but you know generally spark
you're looking at higher end there's
still commodity machines to do your work
and now we get into the Hadoop machine
learning and so machine learning is its
own animal I keep saying that these are
all kind of unique things mahaut is
being phased out I mean people still use
it it's still important if you can write
your basic machine learning that has
most of the tools in there you certainly
can do it in my house how it again
rights to the hard drive reads from the
hard drive you can do all that in spark
and it's faster because it's just in Ram
and so spark has its own machine
learning tools in it and you can also
use Pi spark which then accesses all the
different python tools and there's
there's just so many tools you can dump
into spark the how is very limited but
it's also very basic which in itself can
be really good sometimes simple is
better so how it is used to create
scalable and distributed machine
learning algorithms so here we have our
mahau environment it builds a machine
learning application so you're doing
linear regression you're doing
clustering you're doing classification
models it has a library that contains
inbuilt algorithms for all of these and
then it has collaborative filtering and
again there's our classification and
there's our clustering regression so you
have your different machine learning
tools we can easily classify a large
amount of data using mahout so this
brings us to the next set of tools we
have or the next tool which is the
Apache ambari abari is an open source
tool responsible for keeping track of
running applications and their statuses
think of this as like a traffic cop more
like a security guard you can open it up
and see what's going on and so the
Apache ambari manages monitors and
Provisions Hadoop clusters provides a
central Management Service to start stop
and configure Hadoop services so again
it's like a traffic cop hey stop over
there start keep going hey what's going
on over that area in the through Lane on
the high-speed freeway and you can open
this up and you have a really easy view
of what's going on and so you can see
right here you have the ambari web the
ambari web is what you're looking at
that's your interface you've logged into
the ambari server this will be usually
on the master node it's usually if
you're going to install lombardium as
you'll just install it on the same node
and it connects up to the database and
then it has agents and each agent takes
a look and see what's going on with its
host server and if you're going to have
two more systems and we talked about
spark streaming there's also Kafka and
Apache for streaming data coming in and
Kafka as distributed streaming platform
to store and process streams of records
and it's written in Scala builds
real-time streaming data pipelines that
reliably get data between applications
builds real-time streaming applications
that transforms data into streams so it
kind of goes both ways on there so Kafka
uses a messaging system for transferring
data from one application to another so
we have our sender we have our message
queue and then we have our receiver
pretty straightforward very solid setup
on there with the Kafka Patty storm
storm is a processing engine that
processes real-time streeting at a very
high speed and it's written in closure
they utilize the function based
programming style of closure and it's
based on lisp to give it the speed
that's why Apache storm is built on
there so you could think of Kafka as a
slow-moving very solid communication
Network where Storm is looking at the
real-time data and grabbing that
streaming data that's coming in fast so
it has a ability to process over a
million jobs in a fraction of seconds on
a node so it's massive it can really
reach out there and grab the data and
it's integrated with Hadoop to harness
higher throughputs so this is the two
big things about storm if you are
pulling I think they mentioned stock
coming in or something like that where
you're looking for the latest data
popping up storm is a really powerful
tool to use for that so we've looked at
a couple more tools for bringing data in
we probably should talk a little bit
about security security has in Hadoop
has the Apache Ranger and Apache knocks
are the two most popular one and the
ranger the Apache Ranger Ranger is a
framework to enable Monitor and manage
data Securities across the Hadoop
platform so the first thing it does is
it provides centralized Security
Administration to manage all security
related tasks the second thing it does
is it has a standardized authorization
across all Hadoop components and third
it uses enhanced support for different
authorization methods role-based Access
Control attribute based access control
Etc so your Apache Ranger can go in
there and your administrator coming in
there can now very easily monitor who
has what rights and what they can do and
what they can access so there's also
Apache Knox is an application Gateway
for interacting with the rest apis and
the uis of Hadoop developers and so we
have our application programmer
interfaces our user interfaces and we
talk about rest apis this means we're
pulling this is looking at the actual
data coming in what applications are
going on so if you have an application
where people are pulling data off of the
Hadoop system or pushing data into the
dupe system the nox is going to be on
that setup and it delivers three groups
of user-facing services one proxy
Services provides access to Hadoop via
proxying the HTTP request to
authentication Services authentication
for rest API access and web SSO flow for
user interfaces so there's our rest and
finally Client Services client
development can be done with the
scripting through DSL or using the nox
shell classes so the first one is if you
have a website coming in and out your
HTTP request again your three different
services or what's coming in and out of
the Hadoop file system so there's a
couple of the security setups let's go
ahead and take a look at workflow system
the uzi and there's some other ones out
there Uzi is the one that's specific to
Hadoop there's also like a zookeeper out
there and some other ones uzi's pretty
good Uzi is a workflow scheduler system
to manage Hadoop jobs and so you have a
workflow engine and a coordinator engine
so it consists of two parts what's going
on and coordinating what's going on and
uh directed acelic graph dags which
specifies a sequence of actions to be
executed these consist of workflow jobs
triggered by time and data available and
If you're not familiar with directed
acylic graphs or dags or whatever
terminology you want to throw at this
this is basically a flow chart you start
with process a then the next process
would be process B when process a is
done and it might be that process C can
only be done when process d e and f are
done so you want to be able to control
this you don't want it to process the
machine learning script and then pull
the data in that you want to process it
on you want to make sure it's going in
the right order so these consist of
workflow jobs and they're triggered by
time and data availability so maybe
you're pulling stocks in the middle of
the night and once the stock is all
pulled so there's our time sequence it
says Hey they've been posted on the
other websites they post them usually
after the stock market closes the highs
and lows and everything then once that
data has been brought in you know in a
Time specifics bring the data in a
certain time once the data is available
then we want to trigger our machine
learning script for what's going to
happen next and so we can see here we
have a start our map reduce program our
action node and it begins and either we
have a success then we notify the client
of success usually an email sent out in
successful completion or we don't have a
success we have an error notify client
of error email action node kill
unsuccessful termination and usually at
this point they say email action
notification but I'm mostly that's
usually a pager system and so you see
all the tech guys running to the server
room or wherever you know where our
pager just went off we got to figure out
what went down you know the other one is
you just look at the next morning you go
oh let's make sure everything went
through this morning and check all your
successes with the error usually
assigned to your pager and your
emergency call to open up in the middle
of the night log in so that concludes
our basic setup with the Hadoop
ecosystem so a quick recap on the Hadoop
ecosystem we covered going looking at
the middle part we had the Hadoop as a
file system and how it stores data
across multiple servers saves money
because it's about a tenth of the cost
of using Enterprise computers we looked
at yarn cluster resource management and
how that works to hold everything
together and then we looked at a lot of
data processing how does it process in
and out of the Hadoop system system
including the map and reduce setup which
is the Hadoop basic in Java and for that
we looked at data collection and
ingestion with scoop and Flume we looked
at queries using the scripting language
Pig and the SQL queries through Hive we
glanced at spark remember spark usually
comes installed now at the Hadoop system
because it does so much of its own
processing it covers a lot of the data
in real-time data analysis setup on
there we looked at how machine learning
we looked at Apache ambari for
management and monitoring kind of your
security guard and traffic control just
like we have scoop and Flume which
brings data in there we looked at Kafka
and Apache storm which is for streaming
data and then we looked at Apache Ranger
and Apache Knox for security and finally
we went in through the we took a glance
at Uzi for your workflow system what is
data science let's start with some of
the common definitions that's doing the
rounds some say that data science is a
powerful new approach for making
discoveries from data
others term it as an automated way to
analyze enormous amounts of data and
extract information from it
still others refer to it as a new
discipline which combines aspects of
Statistics mathematics programming and
visualization to gain insights
now that you have looked at some of its
definitions let's learn more about data
science
when domain expertise and scientific
methods are combined with technology we
get data science which enables one to
find solutions for existing problems
let's look at each of the components of
data science separately the first
component is domain expertise and
scientific methods
data scientists should also be domain
experts as they need to have a passion
for data and discover the right patterns
in them
traditionally domain experts like
scientists and statisticians collected
and analyze the data in a laboratory
setup or a controlled environment
the data was then subject to relevant
laws or mathematical and statistical
models to analyze the data set and
derive relevant information from it for
instance they use the models to
calculate the mean median mode standard
deviation and so on of a data set and to
help them test their hypothesis or
create a new one
in the next slide we will see how data
science technology has now made this
process faster and more efficient but
before we do that let's understand the
different types of data analysis an
important aspect of data science
data analysis can either be descriptive
where one studying a data set to explain
what happened or be predictive where one
creates a model based on existing
information to predict the outcome and
behavior it can also be prescriptive or
one suggests the action to be taken in a
given situation using the collected
information
we now have access to tools and
techniques that process data and extract
the information we need for instance
there are data processing tools for data
wrangling we have new and flexible
programming languages that are more
efficient and easier to use with the
creation of operating systems that
support multiple OS platforms it's now
easier to integrate systems and process
Big Data
application designs and extensive
software libraries help develop more
robust scalable and data-driven
applications
data scientists use these Technologies
to build data models and run them in an
automated fashion to predict the outcome
efficiently
this is called machine learning which
helps provide insights into the
underlying data they can also use data
science technology to manipulate data
extract information from it and use it
to build tools applications and services
but technological skills and domain
expertise alone without the right
mathematical and statistical knowledge
might lead data scientists to find
incorrect patterns and convey the wrong
information
now that you have learned what data
science is it will be easier to
understand what a data scientist does
data scientists start with a question or
a business problem
then they use data acquisition to
collect data sets from The Real World
the process of data wrangling is
implemented with data tools and modern
technologies that include data cleansing
data manipulation data Discovery and
data pattern identification
the next step is to create and train
models for machine learning
they then design mathematical or
statistical models
after designing a data model it's
represented using data visualization
techniques
the next task is to prepare a data
report
after the report is prepared they
finally create data products and
services
let us now look at the various skills a
data scientist should have
data scientists should ask the right
questions for which they need domain
expertise the Curiosity to learn and
create Concepts and the ability to
communicate questions effectively to
domain experts
data scientists should think
analytically to understand the hidden
patterns in a data structure
they should Wrangle the data by removing
redundant and irrelevant data collected
from various sources
statistical thinking and the ability to
apply mathematical methods are important
traits for a data scientist
data should be visualized with graphics
and proper storytelling to summarize and
communicate the analytical results to
the audience
to get these skills they should follow a
distinct roadmap it's important they
adopt the required tools and techniques
like Python and its libraries they
should build projects using real world
data sets that include data.gov NYC open
data Gap minder and so on
they should also build a data-driven
applications for Digital Services and
data products
scientists work with different types of
data sets for various purposes now that
big data is generated every second
through different media the role of data
science has become more important
so you need to know what big data is and
how you are connected to it to figure
out a way to make it work for you
every time you record your heartbeat
through your phone's biometric sensors
post or tweet on The Social Network
create any blog or website switch on
your phone's GPS Network upload or view
an image video or audio in fact every
time you log into the internet you are
generating data about yourself your
preferences and your lifestyle
big data is a collection of these and a
lot more data that the world is
constantly creating in this age of the
internet of things or iot big data is a
reality and a need
big data is usually referenced by three
vs volume velocity and variety
volume refers to the enormous amount of
data generated from various sources big
data is also characterized by velocity
huge amounts of data flow at a
tremendous speed from different devices
sensors and applications
to deal with it an efficient and timely
data processing is required
variety is the third V of Big Data
because big data can be categorized into
different formats like structured
semi-structured and unstructured
structured data is usually referenced to
as rdbms data which can be stored and
retrieved easily through sqls
semi-structured data are usually in the
form of files like XML Json documents
and nosql database
text files images videos or multimedia
content are examples of unstructured
data
in short big data is a very large
information database usually stored on
distributed systems or machines
popularly referred to as Hadoop clusters
but to be able to use this database we
have to find a way to extract the right
information and data patterns from it
that's where data science comes in data
science helps to build information
driven Enterprises
let's go on to see the applications of
data science in different sectors
social network platforms such as Google
Yahoo Facebook and so on collect a lot
of data every day which is why they have
some of the most advanced data centers
spread across the world
having data centers all over the world
and not just in the US help these
companies serve their International
customers better and faster without any
network latency they also help them deal
effectively with the enormous amount of
data so what do all these different
sectors do with all this big data
their team of data scientists analyze
all the raw data with the help of modern
algorithms and data models to turn it
into information
they then use this information to build
Digital Services data products and
information driven Maps
now let's see how these products and
services work we'll first look at
LinkedIn let's suppose that you are a
data scientist based in New York city so
it's quite likely that you would want to
join a group or build connections with
people related to data science in New
York City
now what LinkedIn does with the help of
data science is that it looks at your
profile your posts and likes the city
you are from the people you are
connected to and the groups you belong
to then it matches all that information
with its own database to provide you
with information that is most relevant
to you this information could be in the
form of news updates that you might be
interested in Industry connections or
professional groups that you might want
to get in touch with or even job
postings related to your field and
designation
these are all examples of data services
let's now look at something that we use
every day Google's search engine
Google search engine has the most unique
search algorithm which allows machine
learning models to provide relevant
search recommendations even as the user
types in his or her query
this feature is called autocomplete it
is an excellent example of how powerful
machine learning can be
there are several factors that influence
this feature
the first one is query volume Google's
algorithms identify unique and
verifiable users that search for any
particular keyword on the web
based on that it builds a query volume
for instance Republican debate 2016
Ebola threat CDC or the center of
Disease Control and so on are some of
the most common user queries
another important factor is a
geographical location
the algorithms tag a query with the
locations from where it is generated
this makes a query volume location
specific it's a very important feature
because this allows Google to provide
relevant search recommendations to its
user based on his or her location
and then of course the algorithms
consider the actual keywords and phrases
that the user types in
it takes up those words and crawls the
web looking for similar instances
the algorithms also try to filter or
scrub out inappropriate content
for instance sexual violent or
terrorism-related content hate speeches
and legal cases are scrubbed out from
the search recommendations
but how does data science help you
today even the health care industry is
beginning to tap into the various
applications of data science to
understand this let's look at wearable
devices these devices have biometric
sensors and a built-in processor to
gather data from your body when you are
wearing them
they transmit this data to the big data
analytics platform via the iot Gateway
ideally the platform collects hundreds
of thousands of data points and the
collected data is ingested into the
system for further processing
the big data analytics platform applies
data models created by data scientists
and extracts the information that is
relevant to you
it sends the information to the
engagement dashboard where you can see
how many steps you want what your heart
rate is over a period of Time how good
your sleep was how much calories you've
earned and so on
knowing such details would help you to
set personal goals for a healthy
lifestyle and reduce overall health care
and insurance costs it would also help
your doctor record your vitals and
diagnose any issue
the finance sector can easily use data
science to help it function more
efficiently
suppose a person applies for a loan the
loan manager submits the application to
the Enterprise infrastructure for
processing
the analytics platform applies data
models and algorithms and creates an
engagement dashboard for the loan
manager the dashboard would show the
applicant's credit report credit history
amount if approved and risks associated
with him or her
the loan manager can now easily take a
look at all the relevant information and
decide whether the loan can be approved
or not
governments across different countries
are gradually sharing large data sets
from various domains with the public
this kind of transparency makes the
government seem more trustworthy it
provides the country data that can be
used to prepare itself for different
types of issues like climate change and
Disease Control
it also helps encourage people to create
their own digital products and services
the US government hosts and maintains
data.gov a website that offers
information about the federal government
it provides access to over 195 000 data
sets across different sectors
the US government has kicked off a
number of strategic initiatives in the
field of data science that includes U.S
digital service and open data
we have seen how data science can be
applied across different sectors
let's now take a look at the various
challenges that a data scientist faces
in the real world while dealing with
data sets data quality the quality of
data is mostly not up to the set
standards you will usually come across
data that is inconsistent inaccurate and
complete not in the desirable format and
with anomalies
integration
data integration with several Enterprise
applications and systems is a complex
and painstaking task
unified platform data is distributed to
Hadoop distributed file system or hdfs
from various sources to ingest process
analyze and visualize huge data sets the
size of these Hadoop clusters can vary
from few nodes to thousand nodes the
challenge is to perform analytics on
these large data sets efficiently and
effectively
this is where python comes into play
with its powerful set of libraries
functions modules packages and
extensions
python can efficiently tackle each stage
of data analytics that includes data
acquisition python libraries such as
Scrappy comes handy here
data wrangling python data frames are
very efficient in handling large data
sets and makes data wrangling easier
with its powerful functions
explore
matplotlib libraries are very rich when
it comes to data exploration
model
scikit learns statistical and
mathematical functions to help to build
models for machine learning
visualization modern libraries such as
Voca creates very intuitive and
interactive visualization
its huge set of libraries and functions
make big data analytics seem easy and
hence solves the bigger problem
python applications and programs are
portable and helps them scale out on any
big data platform
python is an open source programming
language that lets you work quickly and
integrate systems more effectively
now that we have talked about how the
python libraries help the different
stages of data analytics let's take a
closer look at these libraries and how
they support different aspects of data
science
numpy or numerical python is the
fundamental package for scientific
computing
scipy is the core of scientific
Computing libraries and provides many
user-friendly and efficiently designed
numerical routines
matplotlib is a python 2D plotting
Library which produces publication
quality figures in a variety of hard
copy formats and interactive
environments across platforms
scikit-learn is built on numpy scipy and
matplotlib for data mining and data
analysis
pandas is a library providing high
performance easy to use data structures
and data analysis tools for python
all these libraries modules and packages
are open source and hence using them is
convenient and easy
there are numerous factors which
positions python well and makes it the
tool for data science
python is easy to learn it's a general
purpose function and object-oriented
programming language
as python is an open source programming
language it is readily available easy to
install and get started it also has a
large presence of Open Source Community
for software development and support
Python and its tools enjoy
multi-platform support
applications developed with pycon
integrate easily with other Enterprise
systems and applications
there are a lot of tools platforms and
products in the market from different
vendors as they offer great support and
services
Python and its libraries create unique
combinations for data science because of
all these benefits it's usually popular
among academicians mathematicians
statisticians and technologists
python is supported by well-established
data platforms and processing Frameworks
that help it analyze data in a simple
and efficient way
Enterprise Big Data platform
Cloudera is the Pioneer in providing
enterprise-ready Hadoop Big Data
platform and supports python
hortonworks is another Hadoop a big data
platform provider and supports python
mapreduce map par is also committed to
Python and provides the Hadoop Big Data
platform
big data processing framework
mapreduce spark and Flink provides very
robust and unique data processing
framework and support python
Java Scala and python languages are used
for big data processing framework
but to access Big Data you have to use a
big data platform which is a combination
of the Hadoop infrastructure also known
as Hadoop distributed file system or
hdfs and an analytics platform
Hadoop is a framework that allows data
to be distributed across clusters of
computers for faster cheaper and
efficient computing
it's completely developed and coded in
Java one of the most popular analytics
platforms is Spark
it easily integrates with hdfs it can
also be implemented as a standalone
analytics platform and integrated with
multiple data sources
it helps data scientists perform their
work more efficiently
spark is built using Scala
since there is a disparity in the
programming language that data
scientists use and that of the Big Data
platform it impedes data access and Flow
as python is a data scientist's first
language of choice both Hadoop and Spark
provide python apis that allow easy
access to the Big Data platform
consequently a data scientist need not
learn Java or Scala or any other
platform-specific data languages and can
instead focus on performing data
analytics
there are several motivations for python
Big Data Solutions
big data is a continuously evolving
field which involves adding new data
processing Frameworks that can be
developed using any programming language
moreover new innovation and research is
driving the growth of Big Data Solutions
and platform providers
it would be difficult for data
scientists to focus on analytics if they
have to constantly upgrade themselves on
information or under the hood
architecture or implementation of the
platform therefore it's important to
keep the entire data science platform
and any language agnostic to simplify a
data scientist's job
consequently almost all major vendors
solution providers and data processing
framework developers are providing
python apis this allows a data scientist
to perform big data analytics using only
python rather than learning other
languages like Java or Scala to help
them work on the big data platform
let's look at an example and understand
how data is stored across hadoop's
distributed clusters
big data is generated from different
data sources a large file usually
greater than 100 megabytes gets routed
from a name node to data nodes
name nodes hold the metadata information
about the files stored on data nodes it
stores the address and information of a
block of file and the data node
associated with it
data nodes hold the actual data blocks
the file is split into multiple smaller
files usually of 64 megabytes or 128
megabyte size
it's then copied to multiple physical
servers the smaller files are also
called file blocks one file block gets
replicated to different servers the
default replication factor is three
which means a single file block gets
copied at least three times on different
servers or data nodes there is also a
secondary name node which keeps a backup
of all the metadata information stored
on the main or primary node this node
can be used if and when the main name
node fails
now that you have understood a little
about hdfs let's look at the second core
component of Hadoop mapreduce the
primary framework of the hdfs
architecture
a file is split into three blocks as
split 0 split 1 and split 2. when a
request comes in to retrieve the
information the mapper task is executed
on each data node that contains the file
blocks
the mapper generates an output
essentially in the form of key value
pairs that are sorted copied and merged
once the mapper task is complete the
reducer works on the data and stores the
output on hdfs
this completes the mapreduce process
let's discuss the mapreduce functions
mapper and reducer in detail
the mapper
Hadoop ensures that mappers run locally
on the nodes which hold a particular
portion of the data to avoid the network
traffic
multiple mappers run in parallel and
each mapper processes a portion of the
input data the input and output of the
mapper are in the form of key value
pairs note that it can either provide
zero or more key value pairs as output
the reducer
after the map phase all intermediate
values for an intermediate key are
combined into a list which is given to a
reducer all values associated with a
particular intermediate key are directed
to the same reducer this step is known
as Shuffle and sort
there may be a single reducer or
multiple reducers
note that the reducer also provides
outputs in the form of zero or more than
one final key value pairs
these values are then returned to hdfs
the reducer usually emits a single key
value pair for each input key
you have seen how mapreduce is critical
for hdfs to function a good thing is you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python with the help of Hadoop streaming
API
Hadoop streaming acts like a bridge
between your python code and the Java
based hdfs and lets you seamlessly
access Hadoop clusters and execute
mapreduce tasks
you have seen how mapreduce is critical
for hdfs to function thankfully you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python
shown here are some user-friendly python
functions that are written for the
mapper class
suppose we have the list of numbers we
want to square
we have the square function defined as
shown on the screen
we can call the map function with a list
and a function which is to be executed
on each item in that list
the output of this process is as shown
on the screen
reducer can also be written in Python
here we would like to sum the squared
numbers of the previous map operation
this can be done using the sum operation
as shown on the screen
we can now call the reduce function with
the list of data which is to be
aggregated and aggregator function in
our case sum is used for this purpose
Big Data analysis requires a large
infrastructure Cloudera provides
enterprise-ready Hadoop Big Data
platform which supports python as well
to execute Hadoop jobs you have to first
install Cloudera
it's preferable to install Cloud era's
virtual machine on a Unix system as it
functions best on it
to set up the Cloudera Hadoop
environment visit the Cloudera link
shown here
select quick start download for CDH 5.5
and VMware from the drop down lists
click the download now button
once the VM image is downloaded please
use 7-Zip to extract the files to
download and install it visit the link
shown on screen
Cloudera VMware has some system
prerequisites
the 64-bit virtual machine requires a
64-bit host operating system or Os and a
virtualization product that can support
a 64-bit guest OS
to use a VMware VM you must use a player
compatible with workstation 8.x or
higher such as player 4.x or higher or
Fusion 4.x or higher
you can use older versions of
workstation to create a new VM using the
same virtual disk or vmdk file but some
features in VMware tools will be
unavailable
the amount of ram required will vary
depending on the runtime option you
choose
to launch the VMware Player you will
either need VMware Player for Windows
and Linux or VMware Fusion for Mac
so please visit the VMware link shown on
screen to download the relevant VMware
Player Now launch the VMware Player with
the Cloudera VM
the default username and password is
Cloudera
click the terminal icon as shown here it
will launch the Unix terminal for Hadoop
hdfs interaction
to verify that the Unix terminal is
functioning correctly type in PWD which
will show you the present working
directory you can also type in LS space
hyphen LRT to list all the current files
folders and directories these are some
simple unix commands which will come in
handy later while you are implementing
mapreduce tasks
you have seen how the Hadoop distributed
file system works along with mapreduce
the data is written on and read by disks
mapreduce jobs require a lot of disk
read and write operations which is also
known as disk IO or input and output
reading and writing to a disk is not
just expensive it can also be slow and
impact the entire process and operation
this is specifically true for iterative
processes Hadoop is built for write once
read many type of jobs which means it's
best suited for jobs that don't have to
be updated or accessed frequently but in
several cases particularly in analytics
and machine learning users need to write
and rewrite commands to access and
compute on the same data more than once
every time such a request is sent out
mapreduce requires that data is read and
or written onto disks directly note that
though the time to access or write on
disks is measured in milliseconds when
you are dealing with large file sizes
the time Factor gets compounded
significantly this makes the process
highly time consuming
in contrast Apache spark uses resilient
distributed data sets or rdds to carry
out such computations
rdds allowed data to be stored in memory
which means that every time users want
to access the same data a disk i o
operation is not required they can
easily access data stored in the cache
accessing the cache or Ram is much
faster than accessing disks for instance
if disk access is measured in
milliseconds in-memory data access is
measured in sub milliseconds this
radically reduces the overall time taken
for iterative operations on large data
sets in fact programs on spark run at
least 10 to 100 times faster than on
mapreduce that's why spark is gaining
popularity among most data scientists as
it is more time efficient when it comes
to running analytics and machine
learning computations
one of the main differences in terms of
Hardware requirements for mapreduce and
Spark is that while mapreduce requires a
lot of servers and CPUs spark
additionally requires a large and
efficient Ram
let's understand resilient distributed
data sets in detail as you have already
seen the main programming approach of
spark is rdd
rdds are fault tolerant collections of
objects spread across a cluster that you
can operate on in parallel they are
called fault tolerant because they can
automatically recover from machine
failure
you can create an rdd either by copying
the elements from an existing collection
or by referencing a data set stored
externally say on an hdfs
rdds support two types of operations
Transformations and actions
Transformations use an existing data set
to create a new one for example map
creates a new rdd containing the results
after passing the elements of the
original data set through a function
some other examples of Transformations
are filter and join
actions compute on the data set and
return the value to the driver program
for example reduce Aggregates all the
rdd elements using a specified function
and returns this value to the driver
program
some other examples of actions are count
collect and Save
it's important to note that if the
available memory is insufficient then
spark writes the data to disk
here are some of the advantages of using
spark it's almost 10 to 100 times faster
than Hadoop mapreduce
it has a simple data processing
framework it provides interactive apis
for python that allow faster application
development
it has multiple tools for complex
analytics operations these tools help
data scientists perform machine learning
and other analytics much more
efficiently and easily than most
existing tools
it can easily be integrated with the
existing Hadoop infrastructure
Pi spark is the python API used to
access the spark programming model and
perform data analysis
let's take a look at some transformation
functions and action methods which are
supported by pi spark for data analysis
these are some common transformation
functions
map returns rdd formed by passing data
elements from The Source data set
filter
returns rdd based on selected criteria
flat map
s items present in the data set and
returns a sequence
Reduce by key
returns key value pairs where values for
each key is aggregated by a given reduce
function
let's now look at some common action
functions
collect returns all elements of the data
set as an array
count Returns the number of elements
present in the data set
first
Returns the first element in the data
set
take Returns the number of elements as
specified by the number in the
parentheses
spark context or SC is the entry point
to spark for the spark application and
must be available at all times for data
processing
there are mainly four components in
spark tools
spark SQL it's mainly used for querying
the data stored on hdfs as a resilient
distributed data set or rdd in spark
through integrated apis in Python Java
and Scala
spark streaming it's very useful for
data streaming process and where data
can be read from various data sources
ml lib it's mainly used for machine
learning processes such as supervised
and unsupervised learning
graph x it can be used to process or
generate graphs with rdds
let's set up the Apache spark
environment and also learn how to
integrate spark with Jupiter notebook
first visit the Apache link and download
Apache spark to your system
now use 7-Zip software and extract the
files to your system's local directory
to set up the environment variables for
spark first set up the user variables
click new and then enter spark home in
the variable name and enter the spark
installation path as variable value
now click on the path and then click new
and enter the spark bin path from the
installed directory location
now let's set up the pi spark notebook
specific variables
this will integrate The Spark engine
with Jupiter notebook
type in pi spark it will launch a
Jupiter notebook after a while
create a python notebook and type in SC
command to check the spark context so if
you are interested in mastering the
world of data engineering look no
further than our postgraduate program in
data engineering this comprehensive
course is tailor made for professionals
like you diving deep into the essential
topics such as Hadoop framework spark
based data processing Kafka driven data
pipelines and the intricities of
managing big data on AWS and Azure Cloud
infrastructures a unique approach Blends
live session Hands-On industry projects
exciting IBM hackathons and interactive
ask minuteing sessions to provide you
with the most enriching learning
experience
elevate your data engineering skills and
career prospects today for admission to
this data engineering course a
bachelor's degree with an average of 50
or higher marks is required are two plus
years of work experience is preferred
and basic understanding of object
oriented programming is preferred so
enroll now hello everyone and Welcome to
our video on data engineering have you
ever wondered how companies like Amazon
Netflix and Newber are able to analyze
and make sense of huge amounts of data
that's why data engineering comes in if
you are interested in person a career in
data engineering this video is for you
data engineering is a process of
collecting cleaning and transforming
data into a firm suitable for data
analysis and retrieval in other words is
the process of taking raw data and
transforming it into a format that can
be used for reporting analysis and
decision making data engineering is an
essential skill for anyone looking to
post your career in data analytics data
science or even machine learning with
the rise of Big Data there is a growing
demand for professionals who can design
Implement and manage data processing
workflows so why should you learn data
engineering whether you're a recent grad
transitioning into the field or looking
to deepen your skill set data
engineering is an essential skill for
anyone interested in working with data
well but how do you get into it data
engineering projects are an excellent
way to Showcase your skills and standard
in the job market many companies view
data engineering projects as a way to
evaluate a candidate's ability to
analyze data design and Implement data
processing workflows and work
collaboratively with cross-functional
teams by demonstrating your expertise in
data engineering you can position
yourself as a valuable candidate and
increase your chances of Landing a job
in this field so if you are ready to
take your career to the next level it's
time to start exploring the world of
data engineering so without any further
Ado let's get started with having said
that if you're looking to purse your
career as data engineer or want to
transition into the field of data
engineering then our data engineering
postgraduate program offered by simply
learn in collaboration with University
and IBM provides an excellent
opportunity for professionals to gain
valuable exposure in this field the data
engineering course covers a range of
important topics including Python
language SQL database Hadoop framework
spark for data processing Kafka for data
pipelines and working with web data on
AWS and Azure Cloud infrastructures this
program utilizes various learning
methods such as live sessions industry
projects IBM hackathons and asminating
sessions to provide a comprehensive and
practical learning experience so what
are you waiting for enroll now in this
postgraduate program to Kickstart your
career in data engineering the course
link is added in the description box so
make sure you check that out so without
any further Ado let us now directly jump
into our today's topic so firstly we'll
have a quick introduction to what data
engineering is now data engineering is a
field that focuses on the design
development and management of data
infrastructure and systems it involves
creating robust pipelines that extract
transform and load ETL large volumes of
data from various sources into storage
and processing systems now the primary
goal of data engineering is to enable
organizations to collect store and
process data effectively to support
data-driven decision making and
analytics now data Engineers work with
both structured and unstructured data
utilizing tools and Technologies to
ensure data quality reliability and
scalability why should you consider data
engineering now there are several
reasons why considering a career or
investing in real engineering can be
beneficial here are a few points now
firstly the rise in demand for data
skills now in today's digital age data
is being generated at an unprecedented
rate right so organizations across
Industries are recognizing the value of
data and skilled Engineers who can build
and meet in the necessary infrastructure
to handle process large scale data
data-driven decision making data
engineering plays a crucial role in
enabling data driven decision making
within organizations by constructing
robust data pipelines data Engineers
ensure that accurate and reliable data
is available for analysis this in turn
helps businesses make informed decisions
identify patterns Trends and insights to
gain a Competitive Edge in the
respective Industries and finally
carried opportunities now the field of
data engineering offers excellent career
opportunities as more organizations
Embrace data driven strategies the
demand for skilled data Engineers
continues to rise across various
Industries such as Finance e-commerce
technology health care and much more the
contribute to projects involving Big
Data machine learning cloud computing
Advanced analytics expanding the
knowledge and skill set now that we've
discussed about skill set what are the
required skills that you need to become
a data engineer well a data Engineers
expertise in data processing storage
like spark or SQL programming languages
like python Scala
ETL database systems like relational and
SQL databases data warehousing pipeline
architecture data quality governance and
knowledge on cloud computing like AWS
and gcp and also data visualization and
effective communication are the
necessary skills that you need to
possess as a data engineer now
continuous learning and staying updated
with all these emerging Technologies are
essential for success in this Dynamic
field well moving ahead let us now
understand what you should be looking
for in a data engineering project well
considering data engineering projects
there are several key aspects to look
for to ensure a successful and
fulfilling career now here are some
important factors to consider first one
is Project scope and complexity now
assess the scope and complexity of the
data engineering project that you're
working on look for projects that offer
interesting and challenging tasks that
align with your skill set and allow for
growth and learning opportunities
secondly data volume and variety now
consider the scale and diversity of the
data involved in that project working
with large data sets and various data
sources can provide valuable experience
and exposure to different data
engineering techniques and Technologies
and finally div Technologies and tools
now evaluate the Technologies and tools
used in that project look for projects
that involve cutting edge Technologies
of data engineering like cloud-based
Services Apache spark Big Data tools
data streaming platforms Python language
or much more so which will allow you to
gain Hands-On experience with industry
relevant tools and stay updated with the
current trends so now that we've
understood what you should be looking in
a data engineering project let us now
discuss some of the real-time top data
engineering projects for beginners and
some intermediate to advanced level ones
that will help you to advance in your
career and raise your profile in data
engineering so firstly let us discuss
some of the beginner level projects for
data Engineers now first one is build a
data warehouse now constructing a
student data warehouse can indeed be an
excellent project for beginners to gain
practical experience in data engineering
so here's an outline on how you can
approach this project firstly Define the
data requirements identify the data
sources that will contribute to the
student data warehouse such as student
details course management systems and
grading systems determine the specific
data elements and attributes to be
included such as student demographics
enrollment details academic performance
grades and any other additional relevant
information next you have to design data
warehouse schema now choose a suitable
data warehouse schema such as a star
schema or a snowflake schema based on
the data requirements and Analysis needs
design Dimension tables to store
descriptive attributes such as student
information courses instructors and time
dimensions and finally create a fact
table that captures the key metrics or
measures such as student grades or the
credit hours that they have pursued and
finally Define data access and analytics
now determine the reporting and Analysis
requirements of the student data
warehouse while creating appropriate
data models views and indexes to support
the required analytical queries you can
further Implement a business intelligent
tool or reporting platform to enable
users to access and analyze the data
warehouse by completing this project you
will gain hands-on experience in data
modeling data transformation and the
Practical applications of data
warehousing in this specific domain well
the SEC second project is data
extraction and transformation pipeline
project now this data extraction and
transformation pipeline project for
beginners focuses on designing and
implementing an ETL pipeline file to
extract transform and load data so
here's an overview of this project how
it works now the project involved
several steps starting with the
selection of a CSV file as an data
source now the next step is to develop a
script or utilize an ETL tool to extract
data from the CSV files now once the
data is extracted the project moves to
the data transformation phase where
tasks like cleaning standardizing
formats removing duplicates and
Performing data type conversions are
performed and finally create a design a
structure database schema now it is
another crucial aspect of this project
right this involves determining the
necessary tables columns and
relationships to effectively store the
transform data finally the project
concludes with loading the transform
data into the database creating the
required tables and defining appropriate
data types for each column upon
completion of this project you will gain
practical experience in building a basic
ETL pipeline which is a fundamental
concept or component of a data engineer
on a whole level now they will learn how
to handle data extraction perform
essential data station and load data in
the structure database this project sets
a strong foundation for beginners to
further explore and expand their skills
in the field of data engineering now
third on the list we have data modeling
for a streaming platform project now the
object of the project is to provide
students with practical data engineering
tasks focused on data modeling for a
streaming platform now the project aims
to help streaming services like let's
say uh Spotify or Ghana to enhance their
recommendation systems by gaining deeper
insight into users listening habits so
the project involves the following steps
firstly data collection and expiration
now Begin by collecting data related to
users listening habits this data can
include information such as user
profiles song metadata listening history
playlist user interactions like likes
join us shares Etc and perform
exploratory data analysis to understand
the characteristics of this collected
data next perform data modeling now you
can design an entity relationship model
to represent the key entities like
username song playlist genre Etc and
their relationships within the streaming
platform now based on this ER model
create a database schema choosing an
appropriate dbms like MySQL posture SQL
to design tables columns and constraints
to store the data efficiently and
finally data loading and Analysis now
create and Implement an ETL workflow to
transfer and transform the collected
data into the database for storage and
finally develop SQL queries to explore
and analyze the data utilize the schema
designed to answer questions about user
Behavior popular songs or artists they
go through playlist Trends and
interactions they've had this analysis
can form the foundation for enhancing
the recommendation system and by
completing this project students will
gain hands-on experience in data
modeling database design and querying
for a streaming platform they will
understand how data engineering plays a
crucial role in improving recommendation
systems by leveraging user listening
habits well these are some of the best
projects that you can undertake as a
beginner for data engineering now let us
move ahead and discuss some intermediate
level projects that you can work on
first on the list we have data pipeline
building and organizing now this data
pipeline building and organizing project
is aimed at providing a comprehensive
understanding of data engineering
particularly in the context of data
pipeline workflow management well who
already have some knowledge of data
engineering and seeks to introduce them
to the fundamentals of building and
organizing data pipelines this project
is for you so this is how the project
works now one of the primary objectives
of this project is to explore the
software-based management of data
pipeline workflows this involves
understanding the process and tools
required to design Implement and
maintain efficient data pipelines that
enable a smooth flow of data through
various stages of processing next you
have to leverage using any tools like to
achieve this objective the project
leverages Apache airflow and it open
source solution widely used in industry
for orchestrating and monitoring data
workflows Apache airflow provides a
robust framework for Designing
scheduling and executing complex data
pipelines making it an ideal choice for
managing the workflow aspects of the
project now throughout this project
participants will gain Hands-On pics
experience in utilizing Apache airflow
to build and organize data pipelines
they will learn how to define tasks
dependencies and scheduling parameters
enabling the automation of data
processing workflows additionally they
will export best practices for error
handling data validation monitoring and
ensuring reliability and integrity of
the data pipeline well by working on
this project you will delve into the key
Concepts and techniques involved in data
engineering including transformation and
loading and gain insights into the
challenges associated with data pipeline
development and management second on the
list we have the data Lake creation
project well this project aims to
establish a comprehensive scalable data
in storage infrastructure that enables
the storage management and Analysis of
vast amounts of structured and
unstructured data so this is how the
project works firstly the project
involves setting up a centralized
repository that can accommodate diverse
data types including structured data
from databases semi-structured data from
spreadsheets and log files and any sort
of unstructured data from documents
images and videos this flexibility
allows for the storage of a wide range
of data sources enabling data Engineers
to ingest and organize formation and
various systems and applications next a
data ingestion pipeline is developed to
facilitate the seamless extraction and
loading of data into the data Lake this
involves integrating with different data
sources applying necessary
Transformations and ensuring the data is
cleaned and standardized before being
stored and by utilizing Apache spark in
the AWS Cloud the establishment of a
data Lake becomes feasible and efficient
which enables large-scale data
processing and analytics by successfully
completion of this project you will
learn about the data Lake creation and
gain as much as knowledge possible about
this type of data storage management and
execution part well moving ahead we have
real time streaming analysis project now
in this real-time streaming analysis
project you will set up a streaming
pipeline using Apache Kafka and Apache
Spark by designing and implementing data
ingestion processing and storage
mechanisms to handle high volume and
real-time data with this project you
will gain the ability to perform
real-time analytics and make data driven
decisions in the movement now for
example let us consider a real world
scenario of a social media analytics
platform so the object of this project
is to develop a real-time streaming
analytic systems to process and analyze
social media data so firstly you have to
make a data ingestion and processing
what I mean from that is the system
collects streaming data from various
social media platforms like Twitter
Facebook Instagram using their
respective apis relevant information
such as hashtags user engagement and
other analysis can be extracted from the
streaming data next real-time analytics
and visualization now the process data
is analyzed in real time to generate
meaningful sites for instance the system
can identify trending topics monitor
user sentiments towards a brand or
detect viral content now these insights
can be used for immediate decision
making by making some interactive charts
graphs or any sort of visualization
tools to find insights and understand
the social media trends in real time now
by implementing this real-time streaming
analysis project you can give valuable
insights from social media data as it
happens for instance you can help a
social media marketing team track the
performance of social media campaigns
identify influential users or respond
quickly to emerging Trends and Etc well
fourth on the list we have data
warehousing optimization project now
this data warehousing optimization
project for data engineering aims to
improve the efficiency and performance
of data warehousing systems of any
organizations now the project focus is
on leveraging data engineering
techniques to optimize data storage
retrieval and processing in the data
warehouse in this data warehousing
optimization project you will Begin by
thoroughly analyzing the existing info
structure including Hardware software
and data pipelines to identify areas for
improvement and other points now once
the analysis is complete you would
Implement optimization strategies such
as data partitioning indexing query
optimization data compression ETL
pipeline optimization and scalability to
parallel processing these techniques
would be selected based on the project
requirements and the characteristics of
the data being stored and processed now
throughout the project performance
benchmarks and metrics would be
established to track the improvements
achieved using data analytics and bi
tools Additionally you should prioritize
data quality and consistency throughout
the optimization projects this would
involve reviewing data schema and models
to ensure efficient pairing and
analytics and upon completion of this
project you will gain practical skills
in optimizing data weight housing
systems enabling them to enhance data
retrieval query performance storage
efficiency and overall system
scalability and finally on the list we
have data visualization and interactive
dashboards project now this data
visualization project involves the
creation of visual appealing and
informative representations of data to
facilitate better understanding and
decision making now for example let's
consider a real life scenario where an
e-commerce company wants to track and
analyze sales performance so for that
the project can undertake various steps
such as selecting relevant data sources
integrating and cleaning the data and
choosing a suitable visualization tools
like Tableau power bi and the dashboard
is then designed with intuitive charts
and graphs to enable users to explore
and analyze sales data effectively now
metrics such as Revenue units sold
customer segments and product popularity
are presented in an interactive and user
friendly interface through all the
graphics and the visualization tools by
implementing this data visualization
dashboard project you can see how
e-commerce companies can monitor sales
identify growth opportunities and make
greater driven decisions to optimize
their business strategies so these are
some of the important intermediate data
engineering projects that you can take
upon moving ahead let us now discuss
some of the advanced level data
engineering projects well first on the
list we have this stock market data
analysis now this stock market real-time
data analysis project focuses on
applying data engineering principles to
analyze and process real-time data from
the stock market the project aims to
provide accurate and timely insights for
Traders investors and financial analysts
now key components of the project
include data collection and Analysis the
firstly real-time data is corrected from
stock exchanges financial news feeds and
social media platforms using apis and
data scraping technique now this data
includes stock prices trading volumes
company news and sentiments Etc
Technologies like Apache Kafka are
utilized to stream and handle large
volumes of real-time data now next the
collected data is stored in a Hadoop
distributed file system or inside hdfs
and big data processing Frameworks like
Apache spark is employed to perform
real-time data Transformations and
calculations and finally we can use a
programming language like python along
with its libraries like pandas and
matpatlib to analyze and visualize the
process data to identify pattern Trends
and anomalies in the stock market data
and by implementing these data
engineering techniques this stock market
real-time data analysis project will
help you understand how Traders and
investors make informed decisions based
on timely and accurate insights
extracted from the real-time stock
market data moving second on the list we
have covid-19 analysis project now we
all know how affected we were because of
kobit virus right so this Kobe data
analysis project on data engineering
focuses on applying data engineering
principles to analyze and process
covid-19 related data the project aims
to provide insights and translate to the
spread impact and mitigation strategies
of the covid-19 pandemic so here's an
overview of the project firstly again we
have a data collection now covid-19 data
is connected from various sources such
as Government Health agencies
organizations and research institutions
this data includes daily case discounts
testing rates vaccination data
hospitalization statistics and other
information next data storage and
processing now the integrated data is
stored in a suitable data storage system
such as relational database or a
distributed Source system like hdfs for
efficient data retrieval and scalability
big data processing Frameworks like
Apache spark or cloud-based services
like AWS EMR are utilized to process the
covid-19 data to perform tasks like
aggregations and other statistical
analysis now this data can be finally
published as data plots using
visualization tools like Tableau to
create interactive dashboards to
understand the impact of the virus
around the world so moving ahead third
on the list we have the logs management
and analytics project well this log
analytics project is a prime example of
how data engineering techniques are
employed to extract valuable insights
from large volumes of log data well
let's take a real world scenario let's
check the major e-commerce platform
which is Amazon implements a log
analytics project to enhance operational
efficiency troubleshoot issues and
Optimizer customer experience now the
project Begins by ingesting and storing
log data generated by various components
of the platform including web servers
databases and application servers and
for that Apache Kafka can be used as a
scalable and fault torrent streaming
platform to collect and distribute logs
in real time now with the transformed
log data in hand you can use
elasticsearch a highly scalable search
and analytics engine to in index and
store the data for efficient querying
and Analysis elasticsearch's advanced
search capabilities enables fast and
accurate retrieval of log entries you
can further use various analytics and
visualization techniques on the log data
to prevent any kind of security breaches
or system failures overall the log
analytics project showcases the power of
data Engineering in managing and
analyzing log data at scale now
throughout this log analytics project
you can effectively harness data
engineering techniques to gain
actionable insights resolved issues
promptly and optimize their overall
operations and customer satisfaction
moving ahead fourth on the list we have
the real-time fraud detection system now
in this project you will build a
real-time fraud detection system for a
financial institution let's say a credit
card company wants to detect fraudulent
transactions in real time to minimize
losses and protect their customers by
building a real-time fraud detection
system they can process transaction as
they occur well the project involves
processing large volumes of
transactional data in real time to
identify and flag potential fraudulent
activities you can leverage streaming
data processing Frameworks like Apache
Kafka to ingest and process incoming
transactions Implement data
pre-processing techniques such as data
normalization and feature engineering
now to prepare the data for machine
learning models develop and train
machine learning algorithms such as
anomaly detection or predictive models
to detect fraudulent patterns finally
integrate the system with alert
mechanisms to notify relevant
stakeholders about any kind of
suspicious activities in this way by
completing this real-time fraud
detection system you can understand or
leverage the concepts of data
engineering and how it is applied on
this out of real-time example and
finally on the list we have Cloud
migration and optimization project now
this Cloud migration and optimization
project involves migrating on-premises
data infrastructure to the cloud and
optimizing it for improved scalability
cost efficiency and performance Now by
leveraging Cloud Technologies and best
practices organizations can unlock the
benefits of scalability flexibility and
cost optimization well for example let
us consider a real life scenario where
our Healthcare organization wants to
migrate its on-premises data
infrastructure to a cloud platform like
AWS or azure R now the objective of the
project is to design and execute a
seamless migration process while
optimizing the infrastructure for cloud
benefits now the project involves
designing and implementing robust data
pipelines to collect and ingest data
from diverse sources such as electronic
health records medical devices and
variable sensors now the data is stored
in scalable and secure cloud storage
such as Amazon S3 are ensuring High
availability and durability now to
process this and analyze the data
cloud-based big processing Frameworks
like Apache spark or Google cloud data
flow are employed machine learning
algorithms and predictive models are
applied to derive valuable insights from
the data ranging from patient Health
monitoring and Predictive Analytics for
DC's outbreak detection and Healthcare
Resource optimization in summary discard
migration and optimization project in
data engineering will help you migrate
the data infrastructure to the cloud and
optimize it for scalability cost
efficiency and performance and with that
we have come to the end of today's
session guys I hope you found this
tutorial informative and helpful those
were some of the top data engineering
projects you can take as a data engineer
if you want to improve your skills and
stand out from the rest as we conclude
our big data crash course we hope you
have gained valuable insights into the
world of large-scale data you have
learned what big data is explode
essential Technologies discovered hdfs
and Hadoop ecosystem and glimpsed into
the exciting data engineering projects
armed with this knowledge you are better
prepared to harness the potential of big
data in various domains remember the
world of data is West and your journey
is just beginning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here