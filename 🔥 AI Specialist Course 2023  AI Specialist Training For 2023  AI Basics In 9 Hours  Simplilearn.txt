an AA specialist is an expert in
artificial intelligence proficient in
developing algorithms training models
and solving complex problems they work
on Cutting Edge Technologies like
machine learning and neural networks
aiming to create Innovative AI Solutions
across various Industries their roles
include research algorithm design data
processing and addressing ethical
consideration to become an AI specialist
here we bring you AI course that covers
all the fundamentals of artificial
Intelligence on that note if you are
looking to get certified in artificial
intelligence simply learns postgraduate
program in Ai and machine learning
enhance your career prospects through
our Ai and machine learning course
conducted in collaboration with ber
University and IBM gain valuable skills
including machine learning deep learning
NLP computer vision reinforcement
learning generative AI prompt
engineering chat GPT and numerous other
Cutting Edge Technologies so hurry up
and enroll Now find the complete course
details from the link in the description
box listen to what our Learners say
about our courses hey I'm sharar Jalil I
live in Ontario Canada I have been in it
sector for the past 20 years I recently
took the professional certificate
program in artificial intelligence and
machine learning the course has changed
the way I look at things and help me
back some amazing freelance projects
I started my career in 1999 and over the
years I have worked with many companies
my last tener was with IBM Canada my aim
was to restart my career and learn
something that would help accelerate my
career I thought artificial intelligence
can make me Future Ready the course in
artificial intelligence and machine
learning is provided by simply learn in
association with per University which is
why I chose the course I did not have
high expectations from online course but
my experience was simply awesome the
quality of interaction within the course
was simply amazing the course faculty
was also very experienced and
knowledgeable after the course my
knowledge has grown manifold I have
immensely benefited from Python and
coding skills
I am able to get some new freelance
projects also I am planning to start an
AI based startup with my friend where I
feel that the learning from the course
to be very helpful I am really delighted
and happy in my free time I try to
create meaningful content on YouTube and
I talk about new technologies and what
kind of courses professionals should be
taking along with many other topics I
definitely recom recommend this course
to everyone after all when it comes to
new skills to advance your career there
should be no compromise you should
always learn from the B but before we
begin consider subscribing to our
Channel and hit the Bell icon to never
miss any updates from Simply learn so
without any further delay over to our
experts artificial intelligence is
reshipping Industries and transforming
the way will leave and work it
encompasses a wide range of Technologies
including machine learning learning deep
learning natural language processing
computer vision and many more with its
ability to analyze vast amount of data
and make Intelligent Decisions AI has
become a game changer across various
domains hi everyone welcome to Simply
learns YouTube channel this AI road map
guides you in navigating the path
towards building a successful career in
artificial intelligence the ultimate
goal of artificial intelligence is to
create intelligent missions that can
perform complex task exhibit humanik
intelligence and contribute positivity
to Society AI presents exciting career
opportunity in various Industries and
sectors roles like AI engineer data
scientist NLP Engineers computer vision
Engineers AI research scientist robotic
engineers and many more offer exciting
prospects for working with cutting ITS
Technologies and making an impact
through artificial intelligence
Innovation according to glass door the
average reported salary of an AI
engineer in the United States is around
$115,000 per year however in India it is
10 lak perom leading companies worldwide
are fully aware of the immense value of
AI and are actively pursuing skilled AI
Engineers to contribute to their
research development and implementations
of AI technology among this top
companies are Google Microsoft Amazon
Goldman sa apple and JP Morgan Chase
therefore top companies hiring AI
engineer provide excellent opportunity
for aspiring professionals on that note
elevate your career with our Ai and ml
course developed in part partnership
with P University and IBM gain expertise
in high demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning chat GPT
generative Ai explainable Ai and more
Unleash Your Potential and unlock
exciting opportunity in the world of AI
and ml this course cover tools like
python tensor flow K CH GPT open AI gy
matplot LA and many more so hurry up and
find the course Link in the description
box for more details our Learners have
experienced huge success in their
careers listen to their experience find
the simply learn Course Review Link in
the description box without any further
delay let's dive into the topic now if
you desire to become an AI engineer here
are the steps you can follow to achieve
your goal firstly obtain a strong
foundation in mathematics and
programming gain a strong foundation in
creating mathematic Concepts like linear
algebra calculus and probability theory
in addition to that it is crucial to Ure
Mastery of a programming languages like
python commonly used in Ai and become
proficient in coding then earn a degree
in a relevant field that means earn a
bachelor's or master degree in computer
science data science AI or a related
field to gain a comprehensive
understanding of AI principles and
techniques next gain knowledge in
machine learning and deep learning to
become a AI engineer develop familiarity
with ML algorithms neural networks and
deep learning Frameworks example
tensorflow by TCH to train and optimize
models using real world data set
followed by that work on practical
projects gain hands-on experience and
showcase your skills by working on AI
projects moreover build a portfolio of
projects demonstrating your ability to
solve eii problems impressing potential
employers then stay updated with the
latest advancements stay updated with
the latest trends and research in AI a
rapidly involing field and expanding
your knowledge by reading research
papers participating in online courses
and workshops and joining AI communities
next collaborate and network engage with
AI communities attend conference
participate in online forums to connect
with Professionals in this field
collaborating with others can enhance
your learning experience and open new
opportunities later seek internships or
entry-level positions gain practical
experience through AI internships or
entry-level roles in industry or
research institutions this provides
valuable exposures and help develop your
skills finally continuously learn and
adapt in the rapidly involving field of
AI stay updated on new developments
explore specialized areas and embrace
emerging Technologies and tools to
pursue a career as an AI engineer now
that we have covered the essential steps
to become an AI engineer let's explore
the necessary programming languages and
algorithms for aspiring EI Engineers so
mastering programming languages like
python R Java and C++ is vital for
acquiring Proficiency in AI this
languages enables you to to construct
and deploy models effectively
additionally it would help if you gain a
thorough understanding of machine
learning algorithms such as linear
regressions K nearest neighbors na based
and support Vector missions this
languages and algorithms are fundamental
tools in the field of AI they will
enable you to develop and Implement
effective artificial intelligence models
hello everyone and Welcome to our video
on skills required for an AI engineer
with the Advent of artificial
intelligence and its Superior abilities
to improve human life the need and
demand for expert AI professional is at
an all-time high this expanding demand
has led to a lot of people applying for
AI jobs and upskilling themsel in the
field of AI but doing this is just like
playing a game but not knowing its roles
for someone to become an AI professional
and learn a job like an AI engineer they
first have to understand the demand from
this role and what skills are expected
of an AI engineer so if you're someone
who wishes to become an AI engineer or
if you wish to upgrade your skills in Ai
and get promoted as as an AI engineer
then make sure you watch this amazing
video till the very end in this video we
will be breaking down in complete detail
each and every skill that you would need
in order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
Trend it's a sesmic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing boundar
and making a profound impact on society
the demand for AI professional is
scating across industries from healthare
and finance to entertainment and
transportation organization are actively
seeking talented individuals who can
harness the power of AI to drive their
business forward but what skills does it
take to become an AI engineer how can
you embark on the thrilling Journey we
have answered to all of your questions
also accelerate your career in a ml with
a comprehensive post-graduate program in
a machine learning gain expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive aluminium
membership and ask me anything session
by IIA with three Capstone project and
25 plus industry projects using real
data set from Twitter Uber and many more
you will gain practical experience
master classes by ctech faculty and IBM
expert ensure topnotch education simply
learn job assist help you to get notice
by Leading companies this program covers
statistic python supervised and
unsupervised learning NLP neural network
computer vision G caras tensorflow and
many more skills so enroll now and
unlock exciting AI ml opportunities the
link is in the description box below so
without any further delay let's get
started some steps are crucial to master
in the field of AI and becoming an AI
engineer let's go through them real
quick first establish a strong
foundation in mathematics and
programming start by gaining a solid
understanding of critical Concepts like
linear algebra calculus and probability
Theory additionally it is crucial to
become proficient in programming
language like python which is commonly
used in Ai and develop coding skills and
the next one is pursue a degree in
relevant field earn a bachelor or master
degree in computer science data science
or AI or related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensor flow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry-level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adaptt in the fastpac world
of AI it is crucial to stay updated on
new developments explore specialized
areas and embrace emerging Technologies
and tools continuous learning and
adaptability are essential for purching
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an AI
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is
$115,000 per year however in India it is
10 lakh perom so these figures are way
better than the average selling figures
of any job road so let's discuss the
skills you need to become an AI engineer
you should have a strong background in
data science and machine learning here
is a breakdown of your skills number one
strong programming abilities the
typically refers to expertise in one or
more programming languages commonly used
in data science and machine learning
such as Python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm
implementation and the second one is
knowledge of machine learning algorithms
this involves understanding and
familiarity with the wide range of
machine learning algorithm including
both supervised and unsupervised
learning you should be able to select
and apply appropriate algorithm for
specific problem as well as evaluate and
optimize their performance and the third
third one is Proficiency in statistic
and Mathematics sound knowledge of
statistics and Mathematics is
fundamental for data analysis and
machine learning you should be
comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tens oflow pytorch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as apachi Hado or
Apaches spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the field and continuing learning to
stay at the Forefront of data science
and machine learning let me ask you a
quick question
what is the primary role of an AI
engineer a creating artistic designs for
websites B developing software
applications for mobile devices C
designing and implementing a Solutions
or D managing social media marketing
campaigns now you can pause this video
and answer in the comment section below
so let's start with the first topic
which is a engineer resume format and
structure so format and organization of
AI engineer resume neglecting the
Aesthetics and arrangement of your
resume while solely concentrating on its
textual details might waste your efforts
irrespective of your qualifications for
the position no hiring agent desires to
invest their time to go through a poorly
crafted and disorganized resum rather
ensure the arrangement of your content
follows a straightforward structure and
dedicates some time to format it in a
manner that enhances readability this
approach will guarantee that each
recruiter and hiring manager can pursue
your resume effortlessly so so what to
incorporate in a resume for an
artificial engineer rule so the
essential segments within a resume for
an artificial intelligence position
encompasses first professional
experience this is where you tell about
the jobs you have done before if you
have worked on AI related projects
that's the place to talk about them you
can mention what you did what skills you
used and what you learned from those
experiences then contact details this is
like sharing your phone number email
address and maybe even your LinkedIn
profile it's how the company can get in
touch with you if they want to talk more
about the job third is skill set here
you list down the things you're good at
for an AI engineer this could be stuff
like programming languages python for
example machine learning deep learning
data analysis or any other technical
skills you have all right after that
educational background this is where you
talk about your schooling and any
degrees you have earned like telling
them what you have studied and what you
have learned other than these you can
include Awards and certifications
personal projects and languages Etc now
coming to the third point which is how
to create a resume or objective for an
AI engineer
role so when you're making a summary or
objective for your
resume as an AI engineer here's what you
can do resume summary this is a short
paragraph that gives a quick overview of
your a related experience skills and
what makes you good fit for the job for
example dedicated a engineer with a
strong background in machine learning
and deep learning algorithms experienced
in developing AO solutions for diverse
application proficient in Python tensor
flow and data analysis seeking to
contribute expertise to an Innovative
Tech Team so this is how you can create
a resume summary now let's understand
how to create a objective resume
objective so this is a brief statement
of your career goals and what you want
to achieve in the AI field so an example
of resume objective is aspiring AI
engineer looking to apply academic
knowledge in Practical projects eager to
learn and collaborate in Dynamic AI
Focus environment to enhance skills in
algorithm development and data
manipulation remember both the summary
and objective should be concise and
focus on highlighting your AI skills and
what you can bring to the job so this is
the summary and OB summary or resume
objective all right now moving on to the
next topic which is important skills for
an AI engineer so now we will be
discussing some of the important skills
important for an a
engineer first would be neural network
architecture so Proficiency in designing
implementing various neural network
structures to solve complex problems
this is the first skill and second is
algorithm optimization algorithm
optimization is the ability to enhance
and fine-tune algorithms for improved
efficiency and performance in AI
application third is NLP that is natural
language processing skill in developing
models that understand and generate
human language enabling applications
like chat boards and language
translation next is statistical analysis
statistical analysis is the capability
to analyze and interpret data using
statistical methods crucial for making
informed decisions in AI projects next
important skill for AI engineer is deep
learning strong grasp of deep learning
techniques enables the creation of
advanced AI models for tasks like image
recognition and predictive analysis the
next skill is tensor flow/ pite torch
familiarity with popular deep learning
Frameworks for building and training AI
models is also essential for an AI
engineer the next is computer vision
applying understanding of principles and
computer vision encompassing image
recognition detection of objects and
segmentation of images in is also an
important or essential skill for an AI
engineer so if you are interested in
becoming an AIML engineer then check our
puru postgraduate program in a ml to
elevate your career designed to
understand complex Concepts such as
machine learning deep learning natural
language processing computer vision and
generative AI this certification equips
you with essential skills for success in
this Dynamic field led by industry
experts you will gain access to
Dedicated live sessions that Del into
the latest trends in AI including
generative modeling chat gbt and
explainable AI join us to expand your
knowledge enhance your expertise and
unlock exciting possibilities in the
world of AI and ml Engineering also
listen what a Learners has to say about
our course you need to keep updating our
skills the course material was
comprehensive and the faculty was
extremely experienced uh The Faculty was
able to adjust their teaching style in
order to cater to the overall skill set
of the class in the rapidly evolving
world of technology it's important to
keep upskilling for every working
professional stay relevant continue
learning all right so this was about
important skills for an AI engineer now
let's move on to the next topic which is
how to write your work experience for AI
engineer resume so while writing the
work experience section for an AI
engineer resume you can structure it in
this way first position name this is
where you state your job title during
your time in the company it could be
machine learning engineer AI researcher
or any relevant title that accurately
reflects your role then the company name
write the name of the company or
organization where you were employed
this gives a clear idea of the context
of your work after that dates mention
the period you worked in this position
include both the month and year of your
start and end dates this helps
employeers understand your tenture
following that responsibilities and
achievements this is the heart of your
work experience section here you will
describe what you did in your role and
what you
accomplished this is your chance to
Showcase your impact and contributions
but now the question arises what if I do
not have any work experience so in that
case you can write about internships or
volunteer experience if you have taken
part in internships even if they aren't
directly focused on AI make sure to talk
about them similarly if you have done
any volunteer work and and that lets you
showcase skills relevant to AI include
that to then you can include personal
projects if you have created your own AI
models developed apps or Taken part in
open source projects on your own this is
a great chance to highlight that what
you have complished you can also include
certifications if you have earned any
certifications that are linked to a or
programming even if they weren't given
any traditional education institutions
make sure to mention them when composing
your CV or resume it's crucial to
consider the perspective of a recruiter
if they encounter extensive paragraphs
that are difficult to navigate they
could become frustrated and move on to
the next CV employ bullet points to
outline your roles main responsibilities
emphasizing technical skills software
skills and knowledge whenever
possible keep your points short and to
the point then now let's discuss the
education
section in your AI engineer resume the
education section is where you provide
information about your academic
background here's how to effectively
write about your
education first you can do is degree
type and major so first point you can
write like this degree type and major
after that University name then years
studied then GPA GPA or percentage
whatever is preferred in your
institution the education section allows
potential employees to understand your
academic background and any special
knowledge that is important to AI
engineer role you're pursuing all right
so now coming to the tips for AI
engineer resume part so now we will be
discussing some tips for AI engineer
resume so first tip is highlight AI
skills emphasizing skills like machine
learning deep learning and programming
languages python for example these show
you ready for AI tasks second is Project
power if you have worked on AI projects
mention them it could be class projects
or personal ones they prove your
practical abilities third is tailor to
job adjust your resume for each job
highlight skills and experiences that
match what the job requires quantify
achievements use number to Show
achievements like improved accuracy by
20% it is more impactful using words is
a good practice in resume then the last
one is use keywords it is one of the
most important tip for a engineer resume
many companies use softwares to scan
resum√©s use terms from the job
descriptions to improve
visibility so these were some of the
tips for AA engineer resume so in
conclusion keep it concise engaging and
tailor to make a lasting impression on
potential employees good luck in
creating a compelling resume that
reflects your AI expertise and sets you
on the path to a rewarding career in a
world fueled by Innovation and driven by
technology the there exists a breed of
professionals who have the power to
shape the future they are the AI
Engineers The Architects of intelligence
the ones who bridge the gap between
human potential and machines capability
but have you ever wondered what lies
behind the curtain of their remarkable
work today we embark on a journey to
unravel the mysteries of the AI
Engineers world the demand for AI
Engineers has skyrocketed companies
across various sectors are competing to
secure these exception tants and they
are willing to offer High salary
packages according to Industry reports
the salaries of AI Engineers have been
on a remarkable upward trajectory with
their expertise in high demand AI
Engineers have become one of the most
sought out after professions in the job
market in this video we will cover
topics like the rising demand for AI
Engineers factors affecting AI Engineers
salary skills required for AI Engineers
salary of AI engineers and companies
hiring for AI Engineers now before
moving for forward let me ask you a
quick question what is the term used to
describe an AI system's ability to learn
and improve the experience without being
explicitly
programmed option A is machine learning
option b is artificial neural network
option C is reinforcement learning and
option D is deep learning now you can
pause this video and answer in the
comment section below so let's move on
to the first topic which is the rising
demand for AI engineers
the need for AI Engineers is growing
rapidly because of the increasing
popularity of artificial intelligence we
see AI in things like cars that can
drive themselves and digital assistants
like Ci Alexa and Google Assistant which
can understand and respond to human
voice commands providing helpful
information and Performing tasks AI is
becoming more and more common in our
everyday lives and that's why there is a
big demand for AI Engineers these
Engineers have a special mix of
knowledge about technology and skills
for solving problems with their
expertise they can create new and
creative AI Solutions so in a nutshell
the rising demand for AI Engineers is a
result of transformative impact of AI on
various Industries now let's understand
the factors affecting AI engineer
salary so first is experience AI
Engineers with several years of hands-on
experience in developing a algorithms
and systems command higher salaries due
to their expertise and track record of
successful projects Junior AI Engineers
while earning relatively less have
opportunities for growth and skill
development now second is location the
geographical location plays a
significant role in determining a
engineer salaries Tech hubs such as
Silicon Valley New York City and London
tend to offer higher salaries due to the
concentration of AI driven companies and
fierce competition for talent third is
educational back background the
education background of AI Engineers can
greatly impact their salaries AI
Engineers who have advanced degrees tend
to earn higher salaries employers
recognize the expertise and in-depth
understanding and are willing to offer
higher compensation to attract and
retain these highly qualified
professionals so this was about factors
affecting AI engineer salary now let's
understand about the skills required for
AI engineer to become an AI engineer
certain skills are essential firstly
Proficiency in programming languages
such as python R Java Scala typescript
and C++ is crucial these languages are
commonly used in AI development and
allow Engineers to implement and
optimize AI algorithms effectively AI
Engineers should possess expertise in
data handling and Analysis they should
be skilled in collecting cleaning and
manipulating large data sets to train AI
models accurately in terms of machine
learning AI Engineers need a
comprehensive understand understanding
of machine learning algorithms neural
networks and deep understanding of
Frameworks like tensor flow or pytorch a
strong Foundation of mathematics is
necessary knowledge of probability
statistics calculus and linear algebra
is vital for AI Engineers these
mathematical Concepts underpin the
algorithms and models used in AI systems
and understanding them is essential for
building accurate and efficient AI
models now coming to the exciting part
of this video that is salary of AI
engineer
artificial intelligence engineer in the
United States can expect to earn
salaries within a range of
$18,000 to
$162,000 with a median salary of
$135,000 these figures reflect the
competitive salary packages offered to
Professionals in this field the salary
range takes into account various factors
including experience location industry
and the specific company in India the
salary range for a Engineers with less
than one one year to 6 years of
experience is between 3 lakhs to 22
lakhs the average annual salary for AI
engineers in this range is 7.8 lakhs so
this was about the salary packages of
air Engineers now let's discuss some
companies hiring for air Engineers
several prominent companies are
effectively hiring for air Engineers to
fuel their technological advancements
and Innovation these companies recognize
the importance of artificial
intelligence in driving their business
strategies some notable companies hiring
for AA Engineers include so first is
Amazon second is NVIDIA and third is
Microsoft so let's discuss about these
Amazon with 133% of open AI jobs on
glass store Amazon has been at the
Forefront of a integration across its
various products and services they
prioritize AI Engineers to develop
Cutting Edge Solutions and enhance
customer experiences second is NVIDIA
Nvidia holds 6% of open AI jobs on glass
store enhancing their commitment to a a
research and development they focus on
creating Advanced Hardware and software
solutions to enable AI driven
Technologies in Industries like gaming
autonomous vehicles and data centers
Microsoft a technology giant possesses
4% of open AI jobs on glass they heavily
invest in AI research and development
working on projects like natural
language processing computer vision and
machine learning to empower businesses
and consumers with AI driven solution
IBM IBM has been a significant player in
the AI space for years offering various
AI power services and products they
continue to hire AI Engineers to expand
their capabilities in areas like Watson
AI cloud computing and data
analytics Accenture as a global
Professional Service Company leverages
AI to transform businesses across
various Industries they hire a Engineers
to develop and Implement AI strategies
focusing on areas such as intelligent
automation
data analysis and AI driven customer
experiences at number six we have
Facebook Facebook a leading social media
platform actively recruits AI Engineers
to enhance their a driven features
including content recommendation systems
facial recognition and natural language
processing so these were the companies
hiring for air Engineers along with this
there are many more companies that are
actively hiring for AI
Engineers ultimately pursuing a career
as an AI engineer offers not only
attractive salaries but also the chance
to make a significant impact in a
rapidly advancing field as AI continues
to shape the future the expertise and
contributions of a Engineers will be
instrumental in unlocking new
possibilities if you are looking to get
certified in artificial intelligence
simply learns postgraduate program in Ai
and machine learning find the complete
cause details from the link in the
description box first let's understand
what really is artificial intelligence
artificial intelligence is the science
of building intelligent machines from
vast volumes of
data this data can be structured
semi-structured or unstructured in
nature AI systems learn from past
experiences and perform humanlike tasks
artificial intelligence enhances the
speed precision and effectiveness of
human efforts AI uses sophisticated
algorithms and methods to build machines
that can make decisions on their own
deep learning and machine learning are
the two sub sets of artificial
intelligence so you need both machine
learning algorithms and deep learning
networks to build intelligence systems
AI is now being widely used in almost
every sector of business such as
Transportation Health Care banking
retail entertainment and
e-commerce now let's look at the
different types of artificial
intelligence so AI can be classified
based on capabilities and
functionalities under capabilities there
are three types of artificial
intelligence they are narrow AI General
Ai and super AI under functionalities we
have four types of artificial
intelligence reactive machine limited
memory theory of mind and self-awareness
let's look at them one by
one first we will look at the different
types of artificial intelligence based
on
capabilities so what is narrow AI
narrow AI also known as weak AI focuses
on one narrow task and cannot perform
Beyond its limitations it aims at a
single subset of cognitive abilities and
advances in that Spectrum applications
of narrow AI are becoming increasingly
common in our day-to-day lives as
machine learning and deep learning
methods continue to
evolve Apple Siri is a simple example of
a narrow AI that operates with a limited
predefined range of functions Siri often
has challenges with tasks outside its
range of abilities IBM Watson
supercomputer is another example of
narrow AI which applies cognitive
Computing machine learning and natural
language processing to process
information and answer your
questions IBM Watson once outperformed
human contestant Ken Jenkins to become
the champion on the popular game show
Jeopardy other examples of narrow AI
include Google Translate image
recognition software recommendation
systems spam filtering and Google's page
ranking Al
gthm next we have General artificial
intelligence or general AI General AI
also known as strong AI has the ability
to understand and learn any intellectual
task that a human can General artificial
intelligence has received a$1 billion
investment from Microsoft through open
AI it allows the machine to apply
Knowledge and Skills in different
context AI researchers and scientists
have not achieved strong AI so far to
succeed they would need to find a way to
make machines conscious programming a
full set of cognitive
abilities Fujitsu built the K computer
which is one of the fastest computers in
the world it is one of the most notable
attempts at achieving strong AI it took
40 minutes to simulate a single second
of neural activity so it is difficult to
determine whether or not strong AI will
be achieved in the near
future tan he 2 is a superc computer
created by China's National University
of Defense technology it currently holds
the record for CPS at 33.86 petaflops
although it sounds exciting the human
brain is estimated to be capable of one
exaflop now CPS means characters per
second that A system can
process third in the list of AI that is
based on capabilities we have super AI
super AI exceeds human intelligence and
can perform any task better than than a
human the concept of artificial super
intelligence sees AI evolved to be so
akin to human emotions and experiences
that it doesn't just understand them it
evokes emotions needs beliefs and
desires of its own its existence is
still hypothetical some of the key
characteristics of super AI include the
ability to think solve puzzles make
judgments and decisions on its
own now if you have enjoyed watching
this video so far please make sure to
subscribe to our YouTube channel and hit
the Bell icon to stay updated with all
the latest Technologies also if you have
any questions related to this video
please put it in the chat section our
team of experts will help you address
your
questions moving ahead now we will see
the different types of artificial
intelligence based on
functionalities in this category first
we have reactive machine a reactive
machine is the basic form of AI that
that does not store memories or use past
experiences to determine future actions
it works only with present data they
simply perceive the world and react to
it reactive machines are given certain
tasks and don't have capabilities Beyond
those
duties IBM's deep blue which defeated
chess Grandmaster Gary casprov is a
reactive machine that sees the pieces on
a chest board and reacts to them it
cannot refer to any of its prior
experiences and cannot improve with
practice deep blue can identify the
pieces on a chessboard and know how each
moves it can make predictions about what
moves might be next for it and its
opponent it can choose the most optimal
moves from among the possibilities deep
blue ignores everything before the
present moment all it does is look at
the pieces on the chest board as it
stands right now and choose from
possible next
moves up next we have limited memory
limited memory AI learns from past data
to make decisions the memory of such
systems is
shortlived while they can use this data
for a specific period of time they
cannot add it to a library of their
experiences this kind of technology is
used for self-driving Vehicles they
observe how other vehicles are moving
around them in the present and as time
passes that ongoing collected data gets
added to the static data within the AI
machine such as Lane markers and traffic
lights they're included when the vehicle
decides to change lanes to avoid cutting
of another driver or being hit by a
nearby vehicle Mitsubishi Electric is a
company that has been figuring out how
to improve such technology for
applications like self-driving
cars then we have theory of Mind theory
of Mind represents a very advanced class
of technology and exists as a concept
this kind of AI requires a thorough
understanding that the people and the
things within an environment can alter
feelings and
behaviors it should be able to
understand people's emotions sentiment
and
thoughts even though a lot of
improvements are there in this field
this kind of AI is not complete yet one
real world example of theory of Mind AI
is Kismet a robot head made in the late
' 90s by a Massachusetts Institute of
Technology researcher kiss
can mimic human emotions and recognize
them both abilities are key advancements
in theory of Mind a but KET can't follow
gazes or convey attention to humans
sopia from Hansen robotics is another
example where the theory of Mind AI was
implemented cameras within Sophia's eyes
combined with computer algorithms allow
her to see see and follow faces sustain
eye contact and recognize individuals
she is able to process speech and and
have conversations using natural
language
subsystem finally we have
self-awareness self- awareness AI only
exists
hypothetically such systems understand
that internal traits States and
conditions and perceive human
emotions these machines will be smarter
than the human
mind this type of AI will not only be
able to understand and evoke emotions in
those it interacts with but also have
Emo
needs and beliefs of its own while we
are probably far away from creating
machines that are self-aware we should
focus our efforts towards understanding
memory learning and the ability to base
decisions on past experiences let's look
at the definition of each of these
learning
techniques supervised learning uses
labeled data to train machine learning
models labeled data means that the
output is already known to you the model
just needs to map the inputs to the
outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal below
you can see we have a trained model that
identifies the picture of a cat
unsupervised learning uses unlabelled
data to train machines unlabelled data
means there is no fixed output variable
the model learns from the data discovers
patterns and features in the data and
Returns the output here is an example of
an unsupervised learning technique that
uses the images of vehicles to classify
if it's a bus or a truck so the model
learns by identifying the paths of a
vehicle such as the length and width of
the vehicle the front and rear end
covers roof hoods the types of Wheels
used Etc based on these features the
model classifies if the vehicle is a bus
or a
truck reinforcement learning trains a
machine to take suitable acction and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different paths
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algor MMS that come
under these learning techniques some of
the commonly used supervised learning
algorithms are linear regression
logistic regression support Vector
machines K nearest neighbors decision
tree random forest and knife
base examples of unsupervised learning
algorithms are K means clustering
hierarchical clustering DB scan
principal component analysis and others
choosing the right algorithm depends on
the type of problem you're trying to
solve some of the important
reinforcement learning algorithms are Q
learning Monte Carlo sarsa and deep Q
Network now let's look at the approach
in which these machine learning
techniques
work so supervised learning takes
labeled inputs and Maps it to known
outputs which means you already know the
target
variable unsupervised learning finds
patterns and understands the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data but while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning
methods so supervised learning methods
need external supervision to train
machine learning models and hence the
name supervised they need guidance and
additional information to return the
result unsupervised learning techniques
do not need any supervision to train
models they learn on their own and
predict the output similarly
reinforcement learning methods do not
need any supervision to train machine
learning models and with that let's
focus on the types of problems that can
be solved using these three types of
machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next
slide an unsupervised learning is used
for clustering and Association problems
while reinforcement learning is
reward-based so for every task or for
every step completed there will be a
reward received by the agent and if the
task is not achieved correctly there
will be some penalty
used now let's look at a few
applications of supervised unsupervised
and reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or
banine now talking about the
applications of unsupervised learning we
have custom segmentation So based on
customer Behavior likes dislikes and
interests you can segment and cluster
similar customers into a group another
example where unsupervised learning
algorithms are used is customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks profit
estimation of a company if I was going
to to invest in a company I would like
to know how much money I could expect to
make so we'll take a look at a venture
capitalist firm and try to understand
which companies they should invest in so
we'll take the idea that we need to
decide the companies to invest in we
need to predict the profit the company
makes and we're going to do it based on
the company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing
uh we might have the location we might
have what kind of administration it's
going through based on all this
different information we would like to
calculate the profit now in actuality
there's usually about 23 to 27 different
markers that they look at if they're a
heavy duty investor we're only going to
take a look at one basic one we're going
to come in and for Simplicity let's
consider a single variable R&D and find
out which companies to invest in based
on that so when we take a R&D and we're
plotting The Profit based on the R&D
expend
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we can draw a line right
through the data when you look at that
you can see how much they invest in the
R&D is a good marker as to how much
profit they're going to have we can also
note that companies spending more on R&D
make good profit so let's invest in the
ones that spend a higher rate in their
R&D what's in it for you first we'll
have an introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rainfall how much would be the crop
yield so here we have our crops we have
our rainfall and we want to know how
much we're going to get from our crops
this year so we're going to introduce
two variables independent and dependent
the independent variable is a variable
whose value does not change by the
effect of other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does does control the
crop so we talk about the independent
variable controlling the dependent
variable let's Define dependent variable
as a variable whose value change when
there is any manipulation in the values
of the independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values let's
take our data coming in and this is kind
of random data from any kind of project
we want to divide it up into numerical
and categorical so numerical is numbers
age salary height where categorical
would be a description the color a dog's
breed gender categorical is limited to
very specific items where numerical is a
range of information now that you've
seen the difference between numerical
and categorical data let's take a look
at some different machine learning
definitions when we look at a different
machine learning algorithms we can
divide them into three areas supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on the these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through through it now that
you see where regression model fits into
the machine learning algorithms and
we're specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical iCal model used to predict
the relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable able so y
depends on whatever X is m in this case
is the slope of the line where m equals
the difference in the Y 2 - y1 and X2 -
X1 and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have have our rainfall remember we
cannot change rainfall and we have our
crop yield which is dependent on the
rainfall so we have our independent and
are dependent variables we're going to
take this and draw a line through it as
best we can through the middle of the
data and then we look at that we put the
red point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
that you're going to see here is already
done automatically for you you don't
have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable y and when X was 1 we
got yal 2 when X was uh two y was 4 and
so on and so on if we go ahead and plot
this data on a graph we can see how it
forms a nice line through the middle you
can see where it's kind of grouped going
upwards to the right the next thing we
want to know is what the means is of
each of the data coming in the X and the
Y the means doesn't mean anything other
than the average so we add up all the
numbers and divide by the total so 1
plus 2 + 3 + 4 + 5 over 5 = 3 and the
same for y we get four if we go ahead
and plot the means on the graph we'll
see we get 3 comma 4 which draws a nice
line down the middle a good estimate
here we're going to dig deeper into the
math behind the regression line now
remember before I said you don't have to
have all these formulas memorized or
fully understand them even though we're
going to go into a little more detail of
how it works and if you're not a math
whz and you don't know if you've never
seen the sigma character before which
looks a little bit like an e that's
opened up that just means summation
that's all that is so when you see the
sigma character it just means we're
adding everything in that row and for
computers this is great because as a
programmer you can easily iterate
through each of the XY points and create
all the information you need so in the
top half you can see where we've broken
that down into pieces and as it goes
through the first two points it computes
the squared value of x the squared value
of y and x time Y and then it takes all
of X and adds them up all of Y adds them
up all of x s adds them up and so on and
so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x * Y where the sum equals
66 this all comes from our formula for
calculating a straight line where y
equals the slope * X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is and just
just a minute and where that information
comes from it's called the square means
error but we'll go into that in detail
in a few minutes all you need to do is
look at the formula and see how we've
gone about Computing it line by line
instead of trying to have a huge set of
numbers pushed into it and down here
you'll see where the slope m equals and
on the top part if you read through the
brackets you have the number of data
points times the sum of x * Y which we
computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X2 easily computed as 86 for
the sum minus I'll take all that and
subtract the sum of X squ and we end up
as we come across with our formula you
can plug in all those numbers which is
very easy to do on the computer you
don't have to to do the math on a piece
of paper or calculator and you'll get a
slope of 6 and you'll get your C
coefficient if you continue to follow
through that formula you'll see it comes
out as equal to 2.2 continuing deeper
into what's going behind the scenes
let's find out the predicted values of Y
for corresponding values of X using the
linear equation where M = 6 and C = 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y = 6 *
where x = 1 + 2.2 = 2.8 so on and so on
and here the Blue Points represent the
actual y values and the brown points
represent the predicted y values based
on the model we created the distance
between the actual and predicted values
is known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as e
s if we put these into a nice chart
where you can see X and you can see Y
where we actual values were and you can
see y predicted you can easily see where
we take Yus y predicted and we get an
answer what is the difference between
those two and if we square that Yus y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2.4 we check this
error for each line and conclude the
best fit line having the least e sare
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square distance between the data
points and the regression line now we
only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc which what you want to take away
from this is whatever formula is being
used you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multiple
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attached to it as you can
see here we have M1 M2 M3 and we still
just have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y = M1 * X1 + M2 * X2
so on all the way to m to the n x to the
n and then you add your coefficient on
there implementation of linear
regression now we get into my favorite
part let's understand how multiple
linear regression works by implementing
it in Python if you remember before we
were looking at a company and just based
on its R&D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict his profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic librar Aries and we're going
to be looking through the data before we
do any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it CU it's such a
visual to look at and so easy to use um
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbx box you always import that right
off the beginning pandas although you
don't have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import matplot
library. pyplot as PLT and then caborn
as SNS Seaborn works with the matplot
library so you have to always import
matplot library and then Seaborn sits on
top of it and we'll take a look at what
that looks like you could use any of
your own plotting libraries you want
there's all kinds of ways to look at the
data these are just very common ones and
the caborn is so easy to use it just
looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed map plot
Library inline that is only because I'm
doing an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting because we're just
setting up variables in fact it's not
going to do anything that we can see but
it is importing these different
libraries and setup the next step is
load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CS V and then below
there we're going to set the x equals to
companies under the I location and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandas data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies. head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the panda sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
0 1 2 3 4 and then it shows your
different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300
compared to the administration cost of
$136,800 so on so on helps to create the
profit of
$2,261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
this point we're going to use a single
line of code to get a lot of information
so we can see where we're going with
this let's go ahead and paste that into
our notebook and see what we got going
and so we have the visualization and
again we're using SNS which is pandas as
you can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline code
without this it wouldn't display and you
could display it to a file and other
means and that's the matap plot library
in line with the Amber sign at the
beginning so here we come down to the
single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the caborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns and
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter not notebook and what we're
bringing in is we're going to bring in
the sklearn pre-processing where we're
going to import the label encoder and
the one hot encoder to use the label
encoder we're going to create a variable
called label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. had
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York has a zero or one
or a two and then finally we need to do
a one hot encoder which equals one hot
encoder categorical features equals 3
and then we take the X and we go ahead
and do that equal to one hot encoder fit
transform X to array this final
transformation preps our data for us so
it's completely set the way we need it
as just a row of numbers even though
it's not in here let's go ahead and
print X and just take a look what this
data is doing you'll see you have an
array of arrays and then each array
array is a row of numbers and if I go
ahead and just do row zero you'll see I
have a nice organized row of numbers
that the computer now understands we'll
go ahead and take this out there because
it doesn't mean a whole lot to us it's
just a row of numbers next on setting up
our data we have avoiding dummy variable
trap this is very important why because
the computer is automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x = x
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label en coding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do do that in two steps the
first step is going to be in splitting
the data now whenever we create a
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referenes when
we're doing different uh models usually
see that a capital x and you see the
train and the test and the lowercase Y
what this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals. 2 that basically means 20% So
20% of the are going to be tested we're
going to put them off to the side so
since we're using a th000 lines of data
that means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahe and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good luck and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
where we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at uh setting it
up with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor do fet in this case we do
X train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data side so we're going
to do a y predict variable and we're
going to put in the X test and let's see
what that looks like scroll up a little
bit paste that in here predicting the
test set results so here we have y
predict equals regressor do predict X
test going in and this gives us y
predict now because I'm in Jupiter in
line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict
predict so if you're in a different IDE
that's not an inline setup like the
Jupiter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side it's going to produce
200 answers this is what it says the
profit are for those 200 predictions but
let's don't stop there let's keep going
and take a couple look we're going to
take just a short detail here and
calculating the coefficients and the
intercepts this gives us a quick flash
at what's going on behind the line we're
going to take a short tour here and
we're going to be calculating the
coefficient and intercepts so you can
see what those look like what's really
nice about our regressor we created is
it already has the coefficients for us
and we can simply just print regressor
do coefficient uncore when I run this
you'll see our coefficients here and if
we can do the regressor coefficient we
can also do the regressor intercept and
let's run that and take a look at that
this all came from the multiple
regression model and we'll flip over so
you can remember where this is is going
into and where it's coming from you can
see the formula down here where y = M1 *
X1 + M2 * X2 and so on and so on plus C
the coefficient so these variables fit
right into this formula y = slope 1 *
column 1 variable plus slope 2 * column
2 variable all the way to the m into the
n and x to the N plus C the coefficient
or in this case you have -
8.89 to the^ of 2 etc etc time the First
Column and the second column and the
third column and then our intercept is
the minus1
0309 point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating in the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us that we know our
true the Y predict of those 200 values
is what we think it was true and when we
go ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squar value of0 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression the why linear
regression now that we have come this
far I'm planning to take a closer look
at our CS so that we can estimate cells
in the future how about we hire a data
scientist you can see our two uh
corporate individuals looks more like
they should be agents or secret agents
someplace discussing how to better
forward their company and so they're
going to come in and ask the data
scientist to come in good idea this will
help us to keep a constant track of our
cells so why do we need linear
regression let's assume that we need to
predict the number of skiers based on
snowfall so this happens to be uh we've
kind of jumped one business to the next
so we're looking at the uh skiing
business very popular in a lot of areas
and it's based on snowfall a lot of
times you figure you don't have snow you
don't have skiers but can we actually
use something more specific instead of
look it's snowing and instead of saying
Hey look it's snowing we can actually
start drawing graphs and the graph shows
that with an increase in snowfall the
frequency of Gears also increases so and
there's a pretty direct correlation if
you've ever been up to ski areas when
there's a lot of snowfalls the skiers
all show up because they know it's going
to be better skiing right afterward so
it's kind of easy to see why skiers and
snowfall would go together and it
usually draws a nice straight line and
you can easily predict how many skiers
and that way you can also predict how
many people you need to service them how
many lifts to have up and running and
all the stuff that goes with running a
ski area thus we see that the numbers
are directly proportional to the amount
of snowfall regression models a relation
between a dependent Y and an independent
X variable this is real important to
understand regression because when we
talk about linear regression it's the
basis of almost all our machine learning
algorithms out there and it's usually an
underlying part of the math in our deep
learning so it all starts here with
linear regression and when we talk about
a dependent Y and an independent X
variable these are numbers we're usually
talking about float so we want to have
an actual number value coming out and
that's different than something that's
categorical where we want to know yes or
no true false so regression means we're
looking for a number the independent
variable is known as predictor variable
and dependent variable is known as the
response variable so we have one
predictor variable the amount of
snowfall and one response variable how
many skiers are going to show up and
then we can take this relationship and
it can be expressed as y equals beta
we're going to use a Greek um letter
beta and we have beta KN plus beta 1 X1
and if you continue out it'd be plus
beta 2 X2 and so on till the nth degree
in this case snowfall is an independent
variable and skars is a dependent
variable so we kind of had a little
quick overview let's go ahead and talk a
little bit more about what is linear
regression so we're going to go from why
predicting a number of skares for
snowfall and uh we looked at the formula
a little bit but let's look a bit closer
at what exactly is going on with linear
regression and the question is going to
come up in an interview what is linear
regression linear regression is a type
of statistical analysis that attempts to
show the relationship between two
variables linear regression creates a
predictive model on any data showing
Trends in data the model is found by
using the least Square method there are
other methods the least Square method
happens to be the most commonly used out
there and we usually start with the
least Square method since it's the most
common and usually works the best on
most models and we're already looking at
at how linear regression works but let's
dig deeper into it and let's take a
closer look how linear regression works
I will provide you with a data set which
has rent area other information in it
looks like our secret agents have uh put
on their casual wear for this one you
need to predict rent accordingly we are
given area and rent here so you can do
linear regression with more variables
but we're just going to look at the area
and then from that try to figure out
what the rent should be we plot the
graph and if you look at it here you can
see that the graph kind of a linear
pattern with a little dip in it so the
area seems the rent seems to be based on
the area in most of our work as data
scientists we usually try to start with
a physical graph if we can so we can
actually look at it and say hey does
what I'm fitting to this graph look
right you can solve a lot of problems
that arise by looking at the graph and
saying no that doesn't look right at all
or oh I should be looking over here for
this then we find the mean of area and
rent so the mean just means average and
if you take 1 2 3 4 five and add them
all together and divide by in this case
there's five areas you get three and the
rent is 2 4 6 5 and 8 if you add them
all together and divide by five you get
five and we plot the mean on the graph
so you can see right here here's the
mean of the rent and the area it's kind
of a very Central Point which is what
we're looking for the average of
everything in there the best fit line
passes through the mean this is real
important when you're um eyeballing it
as humans we can do this fairly easy if
it's a straight line through a bunch of
data it gets more complicated
mathematically when you start adding
multiple variables and instead of a line
you have a curve but for basic linear
regression we're going to draw a line
through the data and the line should go
through the means that's going to be
part of the best fit for that line but
we see there are multiple lines that can
pass through the means so depending on
how you look at it you can kind of
Wiggle the line around back and forth so
we keep moving the line to ensure that
the best fit line has the least Square
distance from the data points and this
is you can see right here residual we
have our data point and then we look at
this distance between them and we square
that distance and that's what they mean
by the least squared distance we want
all those distances to add up to the
smallest amount we can when we talk
about that distance we call it the
residual it equals the Y actual minus
the Y predicted very straightforward
just looking at the distance you can
accidentally switch these and it'll come
out the same because we're going to
square it but when you're working with
other sets of linear regression you want
to make sure you do the Y actual minus
the Y predicted the value of M and C for
the best fit line y = mx + C can be
calculated using the mentioned formula
and you can see here we have m equals
the number of points that's what n
stands for then we have the sigma the
Greek symbol is there which is a
summation of x * y minus the summation
of x * the summation of Y over the
number of the summation of x^2 minus the
summation of X squar of X all of it
squared that can be a little confusing
trying to say that and then of course
your C value is going to be the
summation of y * the summation of x^2
minus the summation of x * the summation
of x * y over the total count of the
summation x^2 minus the summation of x^
squar that's a mouthful normally we
don't worry too much about these
formulas other than to know they're
there now if you're a mathematician you
might go back in there and work on those
and those people who originally started
to put together the different models in
R of course had to know all this math to
build them and had to test it out that's
one of the nice things about R what is
important is that you know about these
formulas so that you know where it's
coming from so if you're using one
linear regression model this is how this
one works there's other linear
regression models based on different
math and so you'd have to be aware when
that math changes and how that changes
the model we find the corresponding
values so in this case if you're going
to break this up and you're building
your own model instead of using the one
in R you would have your rent you would
have your area you would find your rent
squared your area squared and your rent
times area and if we go back one we can
see that that's all the different little
pieces in this model we have x * y we
have X squar we have the sum of X which
is going to be squared we have the sum
of X the sum of Y this is the same these
are all the little pieces in here that
we see for m equals and C equals and so
once we compute all these we have 15 25
55 145 88 very easy to do on a computer
thankfully you could have millions of
these points and it would do it in uh
very short period we can then plug in
those values very easily into this
formula and get a final answer and we'll
see that m equal 1.3 and C = 1.1 and
then when we take this for the Y
predicted equals m of i x of I + C we
can take these and actually run the data
we have so we're going to see how it
predicts so now we find the Y the value
of y predicted so we have our xn and we
want to know what Y what we think y will
be based on our formula we generated the
linear regression formula and so we come
in here we have our X we have our actual
y then we have our y predict what we
think it's going to B and then we have
uh we take y minus y predict and we get
those values minus point4 it's off by3
this one's off by one this one's on off
by minus 1.3 by point4 and we go ahead
and square that and then we can sum that
square and average it out and we'll get
a value this is just a summation down
here of Yus y y predicted square is 3.1
and we call that the least Square value
for this line is 3.1 and we go ahead and
divide that by five so when we plot the
Y predict and this is the best fit line
and you can see that does a pretty good
job going right through the middle of
lines and in something like rent versus
area if you're trying to figure out how
much rent to charge or how many people
are allowed to be in the area that's
going to work but if you're looking for
the rent value compared to the area this
gives you a good idea based on the area
of what you should rent the place for
it's close enough that in the business
world this would work you're not
Computing something down to the
millimeters or micrometers or Nuclear
Physics we're just charging people and
so if you're off by a couple dollars
it's not a big deal you'll be pretty
close to what you think it's worth and
get the right value for your property
use case predicting the revenue using
linear regression now that you have a
good idea of how linear regression works
and we're kicking back on our lounging
sofa today let's work on a real life
scenario where you have to predict the
revenue so we're going to take a data
set and we'll pull this up in just a
minute we'll have uh paid organic social
and revenue there are three attributes
we'll be working on the first one is our
paid traffic so all the traffic through
the advertisement that comes in the
non-paid traffic from search engines so
these are people coming to the website
and are doing so because they did a
search for something specific so it's
pulling it from your website all the
traffic coming from social networking
sites so we want to know what's coming
from Twitter and Facebook and somehow
they've linked into your website and
coming into your marketing area to buy
something and we'll use this to predict
the revenue so we want to know how all
this traffic comes together and how it's
going to affect our cells if the
traffic's coming in we're going to make
use of the multiple linear regression
model so you'll see that this is very
similar to we had before where we had y
= m * x + C but instead we have y = M1
X1 + M2 X2 + M3 X3 plus c m is the slope
so you have M1 M2 M3 the three different
slopes of three different lines Y is the
revenue so we're only looking for one
variable and remember this is regression
so we're looking for a number X1 is
going to be the paid traffic and of
course M1 the corresponding slope X2 the
organic traffic and X3 the social
traffic we will follow these steps to
create the regression model we'll start
by generating inputs using the CSV files
we'll then import the libraries
splitting the data into train and test
very important whenever you're doing any
kind of data science applying regression
on pay traff traffic organic traffic
social traffic so we're going to apply a
regression model on them and then we're
going to validate the model that's the
whole reason we split it to a training
and testing is so we can now validate to
see just how good our model is now
before we open up our I always like to
just take a quick look at the data
itself and we're using the rev. CSV file
and this is a CS file or comma separated
variable file we'll just open that with
word pad any kind of text editor would
be nice to show you what's going on and
you can see we have paid organic social
and revenue across the top so those your
titles for your columns and the values
going down and this is the same as what
we saw on the slide just a minute ago
paid organic social and then revenue
from them they probably have it in more
of a spreadsheet than just a word pad
and I'm using the newer version of our
studio which already has a nice layout
automatically opens up with your console
terminal window down and left you have
your script and we'll run it in script
and then when I run it line by line in
script you'll see it show up on the
console bottom bot left I could type it
straight into the console and it would
do the same thing and then on the right
we have our environment and under the
environment we'll see uh plots if you're
in an older version the plots will pop
up and you usually are running just in
the console and then you have to open up
a different window to run the script and
you can run the script from there this
opens up everything at the same time
when you use the new R Studio setup and
the first thing we want to do is we want
to go ahead and generate a variable
we'll call it uh cells and we want to
load this with our data set and so uh in
R we use the less than minus sign that's
the same as assign to it's like an arrow
pointing it cells and it says whatever
it goes past here is going to be
assigned to cells and we want to read in
our CSV file and then I went ahead and
uh fixed the CSV file up a little bit
when I say the CSV file I mean the path
to it um and we'll go ahead and just
paste that in there and this is my
edited version and you'll see I took
wherever we had the um backward slash I
switched it for a forward slash because
a backward slash is an escape character
and whenever you're programming in just
about any scripting language you want to
do the forward slash instead so uh like
a good Chef I prepped this and copied it
into my clipboard and we can see the
full path here and the full path ends in
our rev. CSV file that's the path on my
machine and then I'm going to run that
and now we have uh down here you see it
appear right here uh oh must have an
error in my path it says object rev not
found let me fix that real quick and run
it again and this time it does assign
the cells the data set and then we'll
just type in cells and this is the same
as printing it so if you work in other
languages you'll do like print and then
sales or maybe print sales.ad if you're
in pandas and python or something like
that but in R you just type in the
variable we'll run this and you can see
it printed out all those columns so
that's the same thing we just looked at
and it says uh Max print so it emitted
750 rows so so it has a total of a
thousand rows and we can scroll up here
on our console where the answer came in
and if I scroll up to the top whoop way
up to the top there we go you'll see
paid organic social and revenue so
everything looks good on this and there
there's a lot of cool things we can do
with it one of the things we can do is I
can summary it so let's do a summary of
cells and we'll run that and the summary
of Sals comes in it tells you your Min
first quarter median mean third quarter
Max for paid organic social breaks it up
and you can actually get kind of a quick
view as to what the data is doing you
can take more time to study this a lot
of people look at this and you can see
see from the median the mean that tells
you a lot and is divided into quarters
so you have your first quarter value and
your third quarter setup on there and
Max it just gives you a quick idea of
what's going on in the data summary is a
really wonderful tool one of the reasons
I like to jump into R before I might do
a major project and something else or
you can run everything in our and let's
take a look at the head head of cells
and let me run this and you can see it
just shows the head of our data the
first six rows in this case and it
starts with one important to know other
different systems start with zero and go
up R is one that starts with the first
as one and then uh if we want to do a
plot I'll go ahead and do a plot cells
another really powerful tool that you
can just jump right into with r once you
have your data in here and you'll see
that the plot comes over on the right
hand side and what it does is it
Compares them so if I look at my paid
and my organic and you cross the two
you'll see there's a nice line down the
middle big black line where it plots the
two together let me just widen this a
little bit so we can see more more of it
there we go the same thing with paid and
social paid and revenue so each one of
these has a clear connection to each
other so that's a good sign we have a
nice correlation between the data and
you can just eyeball that that's a good
way to jump in and explore your data
rather quickly and see what's going on
so we'll go ahead and I'll put a um
pound sign there hashtag and uh
splitting the data into training and
test data and we want to split it so
that we have have something to work with
to train our data set and when we do our
data set you do this with any data set
because you want to make sure you have a
good model and then once we've trained
it we're going to take the other set of
data we pulled off to the side and we'll
test it to see how good it is if you
test it with the training data it's
already got that memorized and should
give you a really good answer in fact
when you get into more advanced machine
learning tools you watch those very
closely you watch your training set and
how it does versus your testing set and
if your training set starts doing better
than your testing set then you're
starting to overtrain it now that's more
with deep learning and some other
packages that are beyond what we're
working with today but you know this is
important it's very important to
understand that that test and that
training correlate with each other gives
you an idea what's going on and make
sure that you're set up correctly and
we'll go ahead and set a seed of two and
that's for our random generators so that
when we run them you could seed them
with just a r there's a ways to
randomize a number you could do like
date and stuff like that but that way we
always use the same random numbers so
we'll SE it with two and then we're
going to need a library in this case
we're going to import the library and we
want to import CA tools and CA tools has
our split function so we're going to be
able to split our data along with many
the other tools we're going to need for
this example so they're all built into
the ca tools that's why we need to
import that a lot of times they call
them library or sometimes you hear them
referred to as packages so let's go
ahead and run that line and that brings
the library in so now we can use all
those different tools that are in there
and then I'm going to do a split split
and we're going to assign to the split
that's going to be our variable sample
split okay so that's one of the tools we
just brought in is to sample is the
actual keyword or the function word
sample dosit and we're going to take our
cells and we're going to go ahead and a
split ratio equals
to7 and let's do 0.7 so we make sure it
knows it's a decimal point or float and
what this is saying is that we're going
to take our um cells data the variable
we created called cells that has the
data in there and I want to split it and
I'm going to split it so that 70% of it
goes in one side we'll use that for
training and 30% will go into the other
side which we'll use then for testing
out our model let me go ahe and run that
and oops hold on one second got an error
there spit is definitely very different
than split I don't think we want to spit
our data out we which is actually kind
of what we're doing is spitting it out
but we want to split we want to split
the data so let me get that spelled
correctly and then when I type in Split
we can just run that that CU that's now
a variable you'll see that it has true
false false true so it generates a
number of interesting different
statistics going across there as far as
the way it splits the data and right
here if you have not used R in a while
or um if you're new to R completely that
line one that's what the little one
means down there and then true false
false true that means that's how we're
looking at the data we're splitting all
those different pieces of data in line
one different directions and so we now
want to create our train set and we're
we're going to assign that and when we
take and we assign to the train set a
subset of cells and then the subset of
cells is going to be based on split
equal and put this in Brackets true so
you can see the values down here is
capital T capital r capital u capital E
so I just want to reflect that on the
train and this is going to take
everything where our split variable is
equal to true and it's going to set
cells equal to that and we'll go ahead
and run that and then we're going to set
our test variable has a subset of cells
and if we assign the true to the train
set then we want to assign the false to
our test set so now we'll have uh this
will have 30% of the variables in it and
train will have 70% we'll go ahead and
run that so we now created both our
training set and our test set and we can
just real quickly type in train hit the
run on there and you can see our train
set if we scroll way up to the top I'll
have the column names on there which
should match what we had before paid
organic social and revenue and then
we'll type in test when I hit run on
there that's going to do the same thing
it'll sped out all the test variables
going out so now that we have the test
and train variables we want to go ahead
and create the model and you'll see this
in any of the machine learning tutorials
they always refer to them as models
we're modeling a function of some kind
on the data to fit the data and then
we're going to put the the test data
through there to see how well it does
and so we're going to create the
variable called models and I'm going to
assign that LM that is our linear
regression model I love how simplified R
makes it just LM linear regression and
then model M um I guess it dates back
cuz the linear regression model is like
one of the first major models Ed so they
kept it easy on there and uh Revenue
happens to be the main variable that we
want to track so if you remember
correctly from our formula Y is the
revenue and then we have X is our paid
traffic X2 organic traffic and X3 are
social traffic so Y is what we want to
predict in here and then you'll see this
notation where we have our squiggle and
the period which means we're going to
match up the revenue lines with the
lines of the data we're putting in here
and then I'll put comma and then our
data equals train and of course that's
the training data that we created so
when I hit the Run remember we did all
that discussion about all those
different functions and formulas to
compute our model and how that's set up
when it comes down to it we spend all
our time setting up our data and then I
hit the Run button on the single line
and our model's been trained so we now
have a trained model here and we can do
a summary of the model summary is such a
wonderful command because you can do
that on r that works on all kinds of
different things on there oops and of
course it does help if I remember that
my model has a capital N when I run it
and you'll see right here tells you a
little bit about the model use the
summary on there comes down here it has
our residuals this is all the
information on there as far as like the
minimum the median the Max has all our
coefficients in there if you remember
correctly from our formula let's just go
ahead and switch back over to that we
have M1 X1 M2 X2 M3 X3 plus C well
that's right here here's our intercept
and then we have our different values
for each of these we have our intercept
our paid organic and social and then it
also shows us error and information on
the error and uh one of the really nice
things about when you're working with r
you can come down here and you see um
where we have our Stars down here and it
says uh three stars really good two
stars maybe one star probably no
correlation and we can see with all of
these it has three stars on them so out
of three stars we get three stars on all
these there's no four stars four stars
would mean that you have the exact same
data going in as coming out and then if
we're going to do this we want to
actually run a prediction on here and
that's what we saved our test uh data
for so let's come down here we'll do our
prediction we'll take this variable and
we'll sign it predict model and remember
the predict comes from when we imported
that package the ca tools that's all
part of the ca Tools in there so we're
going to predict we're going to use the
model and then we're going to use our
test data pretty straightforward quick
and easy we'll run this and then if we
go ahead and type in the predict it'll
print out what's in that variable and
you'll see down here the predicted
values it expects to come out it's going
to see our revenue is and it goes
through and it gives it both the line
number and the actual Revenue value so
that's quick and easy boy we got a
prediction really fast and this is also
how you would do it if you had new data
coming in after this you could predict
the revenue based on these other factors
so now that we have a training model
which or we trained the data with our
training data or we trained the model
with our training data and we've done a
prediction on our model with our test
data we want to look this up and we took
a quick glance from our training thing
our training said it trained really well
but that's not the final word on it the
final word is going to be comparing it
so we want to go ahead and going to do
comparing predicted vers ver actual
values what I'm going to do is I'm going
to do a plot and I'm going to do our
test and we're going to do test revenue.
type equals and we're going to put this
in as a type uh type one and or L and
then we have lty y = 1.8 and we're going
to set up for column blue that's a lot
of stuff in there let's go ahead and
just run that so we can see what that
looks like and what's that doing it's
going to generate this nice graph we see
on the right there's our graph you can
see the data data comes up and down um
we're plotting the prediction on there
so this is the values we predicted and
then let's go ahead and do uh lines and
I actually did this backwards so let me
try that again and we'll start with the
red one we're going to do the first one
in red and I want to start with the
actual test Revenue so here's our test
revenue and we'll go ahead and run this
and so we have our red plot over here in
red go and take a look at that pull that
over you can see how that looks goes up
and down hard to track there and then
we're going to do a couple things here
we'll plot our test
revenue and then we'll make it a little
pretty although it's kind of hard to say
see the way I have it scrunched up here
on the right as far as size and sizing
and everything but we'll go ahead and
have our um columns in blue and run our
prediction on there too you can see they
overlap the two graphs so we have our
test revenue and our prediction and then
finally what I started with is we'll go
ahead and plot the prediction fully on
the bottom and run that and if you look
over here on the graph we put the blue
lines over the red lines and so you'll
see a couple spots where there's red
underneath there but for the most part
our prediction is pretty right on so it
looks really tight looks like a good set
of predictions for what we're working on
and this is what we were looking at the
slide right before we started diving
into the r Studio we can see in the
slide here here's a red just a little
bit better spread out than uh what I
have on my screen for my R studio and
the graph shows a predicted Revenue we
see that the two lines are very close so
again they're tight it's right on this
is what we're looking for in our
prediction model but it's not good
enough just having a graph and you
always do both and the reason we do both
is you want to have a visual of it cuz
sometimes you look at these and you get
to the graph you're like oh my gosh what
is that and then sometimes you look at
the graph you go that's right on how
come my um accuracy doesn't look right
and you realize that your accuracy might
be off or you're plotting it incorrectly
so let's go ahead and look up the
accuracy and we'll use rmsse that's
going to be the variable name we're
going to give it and sqrt the square
root of the mean and mean just means
average and then we want prediction
minus sales revenue and then we're going
to take this whole thing and we're going
to square it so what we've got here is
we're going to go through let's just
explain this formula we're using is I'm
going to look at the what I predict it
for to be and what the actual sales
what's our prediction versus our sales
comparison to the revenue and when we
compare those two we're going to find
the average and then I'm going or we're
going to square each one of those values
and then we're going to average aage the
square and then find the square root of
that and that's quite a mouthful the
reason we do it this way the reason we
Square the value and then we find the
means is to generate an answer based on
doesn't matter whether it's plus or
minus there's a lot of other formulas
you can use there to check your accuracy
and all kinds of other things you can do
but a quick straightforward way of doing
it is just like this and then let's go
ahead and run
this and then we type in the rmsse and
that'll give us an actual printed value
out let's just see what that looks like
and so we have
8666 338 for our accuracy and so we look
at the
866.686.1808 learning if you're looking
for a course that covers everything from
the fundamentals to Advanced Techniques
like machine learning algorithm
development and unsupervised learning
look no further than our celtech
postgraduate program in Ai and machine
learning in partnership with IBM this a
ml course CES the latest tools and
Technologies from the AI ecosystem and
features master classes by ctech faculty
and IBM experts hackathons and ask me
anything sessions this program showcases
Celtic ctm's excellence and IBM's
industry progress the artificial
intelligence Cod course c key Concepts
like statistics data science with python
machine learning deep learning NLP and
reinforcement learning through an
Interactive Learning model with live
sessions androll now and unlock exciting
a ml opportunities the course link is
mentioned in the description box below
what is logistic regression let's say we
have to build a predictive model or a
machine learning model to predict
whether the passengers of the Titanic
ship have survived or not the ship right
so how do we do that so we use log
listic regression to build a model for
this how do we use logistic regression
so we have the information about the
passengers their ID whether they have
survived or not their class and name and
so on and so forth and we use this
information where we already know
whether the person has survived or not
that is the labeled information and we
help the system to train based on this
information based on this labeled data
this is known as labeled data and during
the process of building the model we
probably will remove some of the
non-essential parameters or attributes
here we only take those attributes which
are really required to make these
predictions and once we train the model
we run new data through it whereby the
model will predict whether the passenger
has survived or not let's start with
what is supervised learning supervised
learning is one of the two main types of
machine learning methods here we use
what is known as labeled data to help
the system learn this is very similar to
how we human beings learn so let's say
you want to teach a child to recognize
an apple how do we do that we never tell
the child okay this is an apple has a
certain diameter on the top certain
diameter at the bottom and this has a
certain RGB color no we just show an
apple to the child and tell the child
this is Apple and then next time when we
show an apple child immediately
recognizes yes this is an app supervised
learning works very similar on the
similar lines so where does logistic
regression fit into the overall machine
learning process machine learning is
divided into two types mainly two types
there is a third one called
reinforcement learning but we will not
talk about that right now so one is
supervised learning and the other is
unsupervised learning unsupervised
learning uses techniques like clustering
and Association
and supervised learning uses techniques
like classification and regression now
supervised learning is used when you
have labeled data you have historical
data then you use supervised learning
when you don't have labeled data then
you used unsupervised learning it's in
supervised learning there are two types
of techniques that are used
classification and regression based on
what is the kind of problem we are
solved let's say we want to take the
data and classify it it could be binary
classif classification like a zero or a
one an example of classification we have
just seen whether the passenger has
survived or not survives like a zero or
one that is known as binary
classification regression on the other
hand is you need to predict a value what
is known as a continuous value
classification is for discrete values
regression is for continuous values
let's say you want to predict a share
price or you want to predict the
temperature that will be there what will
be the temperature tomorrow that is
where you use regression whereas
classification are discrete values is
will the customer buy the product or
will not buy the product will you get a
promotion or you will not get a
promotion I hope you're getting the idea
or it could be multiclass classification
as well let's say you want to build an
image classification model so the image
classification model would take an image
as an input and classify into multiple
classes whether this image is of a cat
or a dog or an elephant or a tiger so
there are multiple classes so not
necessarily binary classification so
that is known as multiclass
classification so we are going to focus
on classification because logistic
regression is one of the algorithms used
for classification now the name may be a
little confusing in fact whenever people
come across logistic regression it
always causes confusion because the name
has regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we going to focus on
classification and then Within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few dision trees are quite
popular and very easy to understand and
therefore they used for classification
then we have K nearest neighbors this is
another algorithm for performing
classification and then there is
logistic regression and this is what we
are going to focus on in this video and
we are going to go into a little bit of
details about logistic regression if you
are looking to get certified in
artificial intelligence simply learns
post-graduate program in Ai and machine
learning find the complete cause details
from the link in the description box
what is logistic regression as I
mentioned earlier logistic regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if it is going to break down in the new
future so this is like a classification
problem find out whether your car will
break down or not so how are we going to
perform this classification so here's
how it looks if we plot the information
along the X and Y AIS X is the number of
years since the last service was
performed and why is the probability of
your car breaking down and let's say
this information was this data rather
was collected from several car users
it's not just your car but several car
users so that is our labeled data so the
data has been collected and um for for
the number of years and when the car
broke down and what was the probability
and that has been plotted along X and Y
axis so this provides an idea or from
this graph we can find out whether your
car will break down or not we'll see how
so first of all the probability can go
from 0 to one as you all aware
probability can be between zero and one
and as we can imagine it is intuitive as
well as the number of years are on the
Lower Side maybe one year 2 years or 3
years till after the service the chances
of your car breaking down are very
limited right so for example chances of
your car breaking down or the
probability of your car breaking down
within 2 years of your last service are
0.1 probability similarly 3 years is
maybe3 and so on but as the number of
years increases let's say if it was six
or seven years there is almost a
certainty that your car is going to
break down that is what this graph shows
so this is an example of a application
of the classification algorithm and we
will see in little details how exactly
logistic regression is applied here one
more thing needs to be added here is
that the dependent variables outcome is
discrete so if we are talking about
whether the car is going to break down
or not so that is a discrete value the Y
that we are talking about the dependent
variable that we are talking about what
we are looking at is whether the car is
going to break down or not yes or no
that is what we are talking about so
here the outcome is discrete and not a
continuous value so this is how the
logic IC regression curve looks let me
explain a little bit what exactly and
how exactly we are going to uh determine
the class at the outcome rather so for a
logistic regression curve a threshold
has to be set saying that because this
is a probability calculation remember
this is a probability calculation and
the probability itself will not be zero
or one but based on the probability we
need to decide what the outcome should
be so there has to be a threshold like
for example 0.5 can be the threshold
let's say in this case so any value of
the probability below 0.5 is considered
to be zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so that can
cause confusion so let's try to remove
that confusion so what is linear
regression linear regression is a
process is once again an algorithm for
supervised learning however here you're
going to find a continuous value you're
going to determine a continuous value it
could be the price of a real estate
property it could be your hike how much
hike you're going to get or it could be
a stock price these are all continuous
values these are not discrete compared
to a yes or a no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say the HR team
of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not dis discret like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on that
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors the root mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0 and one but still the straight line
doesn't look right therefore instead of
using a linear equation we need to come
up with something different and
therefore the logistic regression model
looks somewhat like this so we calculate
the probability and if we plot that
probability not in the form of a
straight line but we need to use some
other that equation we will see very
soon what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odds so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to 0 and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1 now this is how an
equation of a straight line Looks So Y
is equal to Beta 0 plus beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 minus PX
this is an extension of the calculation
of odds that we have seen right and that
is equal to Beta 0 + beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of PX you will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of of PX like this PX is
equal to 1 by 1 + e ^ of- beta 0 + beta
1 x and recall this is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and um linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s-shaped curve this another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or an elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying object otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and um show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks so first
thing we do is import all the libraries
and then we basically take a look at the
images and see what is the total number
of images we can display using Matt plot
lip some of the images or a sample of
these images and um then we split the
data into training and test as I
mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonal should have the
maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like 100%
accurate model okay so that's uh just of
how to use this Matrix how to use this
uh confusion Matrix I know the name uh
is a little funny sounding confusion
Matrix but actually it is not very
confusing it's very straightforward so
you just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also also known as the ground
truth sometimes okay these are some
fancy terms that are used so predicted
label and the actual label that's all it
is okay yeah so we are showing a little
bit more information here so 38 have
been predicted and here you will see
that all of them have been predicted
correctly there have been 38 zeros and
the the predicted value and the actual
value is is exactly the same whereas in
this case right it has uh there are I
think 37 + 5 yeah 42 have been add the
images 42 images are of Digit three and
uh the accuracy is only 37 of them have
been accurately predicted three of them
have been predicted as number seven and
two of them have been predicted as
number eight and so on and so forth okay
all right so with that let's go into
Jupiter notebook and see how the code
looks so this is the code in in Jupiter
notebook for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numi and map plot and we will take a
look at uh some of the sample images
that we have loaded so this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh B
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the way it works is the
pixel information so as you can see here
this is an 8X 8 pixel kind of a image
and uh the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn for which
number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and test so that the training
data set is used to train the system so
we pass this probably multiple times uh
and then we test it with the test data
set and the split is usually in the form
of there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
2023 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xor train and
since this is labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that this this is what will be
used for comparison to find out whether
the the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for the
training data set xcore test Yore test
is for the test data set okay so let me
go ahead and execute this code as well
and then we can go and check quickly
what is the how many entries are there
and in each of this so xcore train the
shape is 13 83 by 64 and ycore train has
1383 because there is uh nothing like
the second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psychic learn
so we we don't have to implement the
logistic regression process itself we
just call these the function and uh let
me go go ahead and execute that so that
uh we have the logistic regression
Library imported now we create an
instance of logistic regression right so
logistic RR is a is an instance of
logistic regression and then we use that
for training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line is where we are
passing our data that training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be f
so let me just go ahead and test the
entire data set okay that's basically
what we will do so now we want to find
out how accurately this has um performed
so we use the score method to find what
is the percentage of accuracy and we see
here that it has performed up to 94%
Accurate okay so that's uh on this part
now what we can also do is we can um
also see this accuracy using what is
known as a confusion Matrix so so let us
go ahead and try that as well uh so that
we can also visualize how well uh this
model has uh done so let me execute this
piece of code which will basically
import some of the libraries that are
required and um we we basically create a
confusion Matrix an instance of
confusion matrix by running confusion
Matrix and passing these uh values so we
have so this confusion underscore Matrix
meth method takes two parameters one is
the Yore test and the other is the
prediction so what is the Yore test
these are the labeled values which we
already know for the test data set and
predictions are what the system has
predicted for the test data set okay so
this is known to us and this is what the
system has uh the model has generated so
we kind of create the confusion Matrix
and we will print it and and uh this is
how the confusion Matrix looks as the
name suggests it is a matrix and um the
key point out here is that the accuracy
of the model is determined by how many
numbers are there in the diagonal the
more the numbers in the diagonal the
better the accuracy is okay and first of
all the total sum of all the numbers in
this whole Matrix is equal to the number
of observations in the test data set
that is the first thing right so if you
add up all these numbers that will be
equal to the number of observations in
the test data set and then out of that
the maximum number of them should be in
the diagonal that means the accuracy is
pretty good if the the numbers in the
diagonal are less and in all other
places there are a lot of numbers uh
which means the accuracy is very low the
diagonal indicates a correct prediction
that this means that the actual value is
same as the predicted value here again
actual value is same as the predicted
value and so on on right so the moment
you see a number here that means the
actual value is something and the
predicted value is something else right
similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44 + 43 and so on and
divide that by the total to number of
test observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map uh looks uh and
as you can can see here the diagonals
again are are all the values are here
most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94% this is calculated as I
mentioned by adding all these numbers
divided by the total test values or the
total number of observations in test
data set okay so this is the confusion
Matrix for logistic
regression all right so now now that we
have seen the confusion Matrix let's
take a quick sample and see how well uh
the system has classified and we will
take a few examples of the data so if we
see here we we picked up randomly a few
of them so this is uh number four which
is the actual value and also the
predicted value both are four this is an
image of zero so the predicted value is
also zero actual value is of course zero
then this is the image of nine so this
has also been predicted correctly N9 and
actual value is n and this is a image of
one and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images Okay so
let's summarize what we have learned in
this video we discussed about what is
logistic regression and how it fits into
the overall machine learning we
discussed about the different types of
machine learning and how logistic
regression is one of the algorithms for
classification and then we compared
logistic regression with linear
regression because the names may be
sounding similar but these are
completely different algorithms linear
regression is used for predicting
continuous values whereas logistic
regression is used for finding discrete
vales and then we talked about the math
behind log listic regression and we
derived that function as well the
sigmoid function we did a quick table
comparison between linear and logistic
regression we saw some examples of
day-to-day life how logistic regression
is used for some of the class for
solving some of the classification
problems and then in the end we looked
at the code to see how we can Implement
logistic regression to train the model
to identify images there's all kinds of
regression models that come out of this
so we put them side to side we have our
linear regression which is a predictive
number used to predict a dependent
output variable based on Independent
input variable accuracy is a measured uh
using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the number is coming out linear
usually means we're looking for that
line versus a different model and it's
used in business domain forecasting
stocks uh it's used as a basis of of
most um uh predictions with numbers so
if you're looking at a lot of numbers
you're probably looking at uh a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value a low
value volume uh that kind of thing so
when you're looking at something the
straight numbers um and are connected in
that way usually you're talking about a
linear regression model and that's where
you want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um when we're talking about linear
regression you definitely are talking
about straight line although there is
other regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of uh like a
window of things so you could take a
window of stock history and you could CL
generate classifications based on that
and separate the data that way if it's
going to be that this particular pattern
occurs it's going to be upward trending
or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care do I even want to look at
it so let's do a demo so you can get a
picture of what this looks like in Pyon
Pyon code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and non-poisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous
this so first off before we begin the
demo I'm in the Anaconda Navigator in
this one I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
lgit T um is one of the one of the names
that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um
and so in programming uh we got a lot of
stuff to unfold here in our in our
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet you have rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um um
clear all that there we go uh these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it measure
that we expect it to be you know if
you're an insurance company you don't
want to promis to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this is
loaded the data set on here here and
then because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know to give you a quote for what we're
going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 two 3 four five six
seven so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn um what to expect them to charge
on there
uh so is this going to be you know is
this person going to cost me uh
$16,814.23
[Music]
data is so much of the work in data
science um sex well how do you how do
you deal with female versus male um are
you a smoker yes or no what does that
mean region how do you look at Region
it's not a number how do you draw a line
between Southwest and
Northwest um you know they're they're
objects it's either your Southwest or
your Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. se. drop
duplicates and then then for data set
sex we're going to do the label
transform the data sex and so we're
looking right here at um male or female
and so it usually just converts it to
a01 because there's only two choices on
here same thing with the smoker it's Z
one so we're going to transfer the trans
change the smoker uh 01 on this and then
finally we did region down here region
does it a little bit different we'll
take a look at that and um it it's I
think in this case it's probably going
to do it because we did it on this label
transform um with this particular setup
it gives each region a number like 0 1 2
3 so let's go a and take a look and see
what that looks like go and run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
uh 01 two three so there three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 01 2 3 A lot of times
you increase your features and so you
would have region Northwest would be one
column yes or no region Southwest would
be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um
why linear regression uh data set drop
the charges because that's what we want
to
predict and so our X I'm sorry our X
linear data set drop the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to take um
03 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the data and
then finally the big ones uh this is
where all the the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from and this
comes from the sidekit uh learn and this
is the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I looking at this is
I look for
methods and you'll see here's our fit
that we're using right right now and
here's our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so when we go in this and we
go ahead and run this you'll see it
loads the linear regression model and
just has a nice output that says hey I
loaded the linear regression model and
then the second part is we did the fit
and so this model is now trained our
linear model is now trained on the
training
data
and so one of the things we can look at
is the um um for idx and col a name and
a numerate x linear train columns come
an interesting thing this prints out the
coefficients uh so when you're looking
at the back end of the data you remember
we had that formula uh BX X1 plus bxx2
plus the in plus the uh intercept uh and
so forth these are the actual
coefficients that are in here this is
what it's actually multiplying these
numbers
by and you can see like region gets a
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know what uh smoker look how smoker
impacts the cost um it's just massive uh
so this is is a flag that hey the value
of the smoker really affects this model
and then you can see here where the body
mass index uh so somebody who is
overweight is probably less healthy and
more likely to have cost money and then
of course age is a factor um and then
you can see down here we have uh sex is
than a factor also and it just it
changes as you go in there negative
number it probably has its own meaning
on there again it gets really
complicated when when you dig into the
um workings and how the linear model
works on that and so um we can also look
at the intercept this is just kind of
fun um so it starts at this negative
number and then adds all these numbers
to it that's all that means that's our
intercept on there and that fits the
data we have on that and so you can see
right here we can go back and oops give
me just a second there we go we can go
ahead and predict the unknown data and
we can print that out and if you're
going to create a model to predict
something uh we'll go ahead and predict
it here's our y prediction value linear
model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this take a look at
the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on this
I'm going to guess by the notes that the
class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a labeling
code
which again there's a lot of different
encoders uh with this particular label
encoder it's just switching it to 0 1 2
3 and giving it an integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns in the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this gets
shifted from uh xbxx K to 01 2 3 4 5 or
whatever it is classes
01 one being poisonous zero looks like
it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring 0 1 2 3 4 5 um in
this particular example it's not going
to make that big of a difference how we
encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our um train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because then that that's
what it does is it goes through the math
and it goes hey can we minimize the
error and it keeps finding a lower and
lower error and it still is changing
that number so that means it hasn't
conversed yet it hasn't find the lowest
amount of error it can and the default
is 100 uh there's a lot of settings in
here so when we go in here to let me
pull that up from the sklearn uh so we
pull that up from the sklearn model
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and it'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this
and you can see here it's now loaded
this up there we now have a uh uh
logistic model and we've got ahead and
done a predict here also just like I was
showing you earlier uh so here is where
we're actually predicting the data so we
we've done our first two lines of code
as we create the model we fit the model
to our training data and then we go
ahead and predict for our test data now
in the previous model we didn't dive
into the test score um I I think I just
showed you a graph and we can go in
there and there's a lot of tools to do
this we're going to look at the uh model
score on this one and let me just go
ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here um
let's see I believe its poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being uh um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you um start eat
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as uh the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science talking about your machine
learning
code and hopefully you got a little
classification is probably one of the
most widely used tools in machine
learning in today's world it is also one
of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithm
and there are a lot out there so we're
only going to touch briefly on a variety
of them so you can see how they the
different flavors work and we'll have
some handson Demos in Python embedded
throughout the tutorial classification
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and where you
end when you start running these um
machine learning algorithms and
classification and the classification
algorithms is a little black box in a
lot of respects and we'll look into that
you can see what I'm talking about when
we start swapping in and out different
models let's say we are given the task
of classifying a given bunch of fruits
and vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be group
together and so we have a data set we'll
call it the bunch is divided into
clusters one of which consists of the
fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out in images
to see what is malignant what is benign
very popular one can you classify
flowers the iris uh data set uh
certainly in Wildlife can you classify
different animals and track where
they're going classification is really
really the bottom starting point or the
Baseline for a lot of machine learning
and setting it up and trying to figure
out how we're going to break the data up
so we can use it in a way that is
beneficial so here the fruits and the
vegetables are grouped into clusters and
each clusters has a specific
characteristic I.E whether they are a
fruit or a vegetable and you can see we
have a pile of fruits and vegetables we
feed it into the algorithm and the
algorithm separates them out and you
have fruits and vegetables so some
important terminology before we dig into
how it sorts them out and what that all
means uh when we look at the
terminologies you have a classifier
that's the algorithm that is used to map
the input data to a specific category
the classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times the
classifier is part of the classification
model um and then you choose which
classifier you use after you choose
which model you're using where features
are what goes in labels are what comes
out so your classifier model is right in
the middle of that that's that little
black box we were just talking about
clusters they are a group of data points
which have some common characteristics
binary classification it is a
classification condition with two
outcomes which are either true or false
multi-label classification this is a
classification condition where each
sample is assigned to a set of labels or
targets multiclass classification the
classification with more than two
classes here each sample is assigned to
one and only one label when we look at
this group of terminologies a few
important things to notice uh going from
the top clusters when we cluster data
together we don't necessarily have to
have an end goal we just want to know
what features cluster together these
features then are mapped to the outcome
we want in many cases the first step
might not even care about the outcome
only about what data connects with other
data and there's a lot of clustering
algorithms out there that do just the
clustering part binary classification it
is a classification condition with two
outcomes which are either true or false
we're talking usually um it's a uh it's
either a cat or it's not a cat it's
either a dog or it's not a dog um that's
the kind of thing we talk about binary
classification and then that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multiclass classification where
each one's a binary uh you can either be
a cat or a dog you can't be both a cat
and a dog real world applications so to
make sense of this uh of course the
challenge is always in the details is to
understand how we apply this in the real
world so in real world applications we
use this all the time we have email spam
classifier so you have your email inbox
coming in uh it goes through the email
filter that we usually don't see in the
background and it goes this is either
valid email or it's a Spam and it puts
it in the spam filter if that's what it
thinks it is uh Alexa's voice classifier
Google Voice any of The Voice
classifiers they're looking for points
so they try to group words together and
then they try to find those groups of
words trigger a CL classifier so it
might be that the classifier is to open
your task program or open your text
program so you can start sending a text
sentimental sentiment analysis is really
big uh when we're tracking products
we're tracking marketing trying to
understand uh whether something is liked
or disliked is huge uh that's like one
of the biggest driving forces and sales
nowadays and you almost have to have
these different filters going on if
you're running a large business of any
kind fraud detection uh you can think of
banks uh they find different things on
your bank statement and they detect that
there's something going on there they
have algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned the cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with a classification
when the wild animals in our area are
out like foxes we actually have a
mountain line that lives in the area so
it's nice to know when he's here
handwriting prediction uh classifying A
B C D and then classifying words to go
with that so let's go ahead and uh roll
our sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms uh let's go
back and take a look at our definitions
we have a classifier and a
classification model so we're looking at
the classifier an algorithm that is used
to map the input data to a specific
category one of those algorithms is a
logistic regression the logistic
regression is a classification algorithm
used to model the probability of a
certain class or event existing such as
pass fail or win lose Etc it provides
its output using the logistic function
or sigmoid function to return the
probability value that can then be
mapped to two or more discret classes a
sigmoid function is an activation
function that fits the variable and
limit the output to a range between 0
and one a standard sigmoid function or
logistic function is represented by the
formula FX = 1/ 1 + e- x where X is the
equation of the line and E is the
exponential just taking a quick look at
this um you can think of this as being a
point of uncertainty and so as we get
closer and closer to the middle of the
line it's either um activated or not and
we want to make that just shoot way up
uh so you'll see a lot of the activation
formulas kind of have this nice s curb
where it approaches one and approaches
zero and based on that there's only a
small region of error and so you can see
in the sigmoid logistic function uh the
1 over 1 + e- x to the minus X you can
see it fam that that nice s curb uh we
also can use a tangent variation there's
a lot of other different uh models here
as far as the actual algorithm this is
the most commonly used one let's go
ahead and roll up our sleeves and take a
look at a demo that is going to use a
logistic regression so we're going to
have the activation formula and the
model because you have to have you have
to have both for this we will go into
our Jupiter notebook now I personally
use the Anaconda navigator to open up
the Jupiter notebook to set up my IDE as
a web based it's got some advantages
that it's very easy to display in uh but
it also has some disadvantages in that
if you're trying to do multi- threads
and multi-processing you start running
into a single git issues with python and
then I jump to uh py charm really
depends on whatever ID you want just
make sure you've installed uh numpy and
the sklearn modules into your Python and
whatever environment you're working in
so that you'll have access to that for
this demo now the team in the back has
prepared my code for me which I'll start
bringing in one section at a time so we
can go through it before before we do
that it's always nice to actually see
where this information is coming from
and what we're working with uh so the
first part is we're going to import our
packages which you need to install into
your Python and that's going to be your
numpy um we usually use numpy as NP and
then from sklearn the learn model we're
going to use a logistic regression and
from sklearn metrics we're going to
import the classification report
confusion Matrix and if we go ahead and
open up S uh the scikit-learn dorg or
and go under their API you can see all
the different um features and models
they have and we're looking at the
linear model uh logistic regression one
of the more common classifiers out there
and if we go ahead and go into that and
dig a Little Deeper you'll see here
where they have the different settings
and it even says right here note the
regularization is applied by default so
by default that is the activation
formula being used now we're not going
to spend we might come back to this look
at some of the other models cuz it's
always good to see what you're working
with but let's go ahead and jump back in
here and we have our Imports we're going
to go ahead and run those uh so these
are now available to us as we go through
our Jupiter notebook script and they put
together a little piece of data for us
this is simply um going through uh 0 to1
actually let's go ahead and print this
out over here we'll go ahead and print X
just so you can see what we're actually
looking for and when we run this you can
see that we have our X is 01 2 through 9
we reshaped it the reason for this is
just looking for a row of data usually
we have multiple features we just have
the one feature which happens to be 0o
through nine and then we have our 10
answers right down here uh 0 1 0
00111111 you can bring in a lot of
different data depending on what you're
working with uh you can make your own
you can instead of having this as just a
single you could actually have like
multiple features in here but we just
have the one feature for uh this
particular demo and this is really where
all the magic happens right here uh I
told you it's like a black box that's
the part that is is is kind of hard to
follow and so if you look right here we
have our model we talked about the model
right there and then we went ahead and
set it for Library linear as I showed
you earlier that's actually default so
it's not that important uh random State
equals zero this stuff you don't worry
too much about and then with the S kit
learn you'll see the model fit this is
very common to S kit they use similar
stuff in a lot of other different
packages but you'll you'll see that
that's very common you have to fit your
data and that means we're just taking
the data and we're fitting our X right
here which is our features that's our X
and here's y these are the labels we're
looking for so before we were looking at
is it fraud is it not is it cat is it
not um that kind of thing and this is
looking at 01 so we want to have a
binary setup on this and we'll go ahead
and run this uh you can see right here
it just tells us what we loaded it with
as our defaults and that this model has
now been created and we've now fit our
data to it and then comes the fun part
you work really hard to clean your data
to um bake it and cook it there's all
kinds of I don't know why they go with
the cooking terms as far as how we get
this data formatted then you go through
and you pick your model you pick your
solver and you have to test it to see
hey which one's going to be best and so
we want to go ahead and evaluate the
model and you do this is that once
you've figured out which one is going to
work the best for you you want to
evaluate it so you can compare it to
your last Model and you can either
update it to create a new one or maybe
change the um solver to something else I
mentioned tangent that's one of the
other common ones that's commonly used
with language for some reason the
tangent even though it looks almost to
me identical to the one we're using with
the sigmoid function uh it for some
reason it activates better with language
even though it's a very small shift in
the actual math behind it we already
looked at the um data early but we'll go
and look at it again just you can see we
look at we have our rows of Zer one row
it only has one entity and we have our
output that matches these rows and these
do have to match you'll get an error if
you put in something with a different
shape so if you have um 10 rows of data
and nine answers it's going to give you
an error because you need to have 10
answers for it a lot of times you
separate this too when you're doing
larger models uh but for this we're just
going to take a quick look at that the
first thing we want to start looking at
is uh The Intercept one of the features
inside our linear Reg regression model
we'll go ahead and run that and print it
uh you'll see here we have an intercept
of minus 1.51 6 and if we're going to
look at the uh intercept we should also
look at our um coefficients and if you
run that you'll see that we get a we get
the um our coefficient is the 7035 you
can just think of this as your eidan
geometry for very basic model like this
where it intercepts the Y at some point
and we have a coefficient multiplying by
it a little more complicated in the back
end but that is the just of this simple
model with just the one feature in there
and we'll go ahead and uh we'll reprint
the Y because I want to put them on top
of each other with the Y predict and so
these were the Y values we put in and
this is the Y predict we had coming out
and you can see yeah here we go uh
there's the Y actual and there's what
the prediction comes in uh now keep in
mind that we used the actual complete
data as part of our training uh uh that
is if you're doing a real model a big
stopper right there because you can't
really see how good it did unless you
split some data off to test it on this
is the first step is you want to see how
your model actually tests on the data
you trained it with and you can see here
there is this point right here where it
has it wrong and this point right here
where it also has it wrong and it makes
sense because we're going our input is 0
1 0 through 9 and it has to break it
somewhere and this is where the break is
uh so it says this half the data is
going to be zero because that's what it
looked like to me if I was looking at it
without an algorithm and this data is
probably going to be one and I I didn't
I forgot to point this out so let's go
back up here I just kind of glanced over
this window here where we did a lot of
stuff let's go back and and just take a
look at that what was done here is we
ran a prediction uh so this is where our
predict comes in is our model. predict
so we had a model fit we created the
model we programmed it to give us the
right answer uh now we we go ahead and
predict what we think it's going to be
there's our model. predict probability
of X and then we have our y predict
which is very similar but this is has to
do more with the probability numbers so
if you remember down below we had the
setup where we're looking at uh that
sigmoid function that's what this is
returning and the Y predict is returning
a zero or a one and then we have our um
confusion Matrix we'll look at that and
we have our report which just basically
Compares our y to our y predict which we
just did it's kind of nice it's simple
data so it's really easy to see what
we're doing that's why we do use the
simple data this can get really
complicated when you have a lot of
different features and things going on
and splits uh so here we go we've had
our I printed out our actual and our
prediction so this is the actual data
this is what the predict ran um and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that uh this is great
if you have a lot of data to look at but
you can see right here a confusion
Matrix says if you remember from the
confusion Matrix we have the two this is
two correct one two and uh it's been a
while since I looked at a a confusion
Matrix there's the two and then we have
this one which is our six that's where
the six comes from and then we have this
one which is the um one false this is
the two on So we have this one here and
this one here which is
misclassified this really depends on
what data you're working with as to what
you're is important um you might be
looking at this model and if this model
this confusion Matrix comes up and says
that uh you've
misclassified even one person as being
non malignant cancer that's a bad model
uh I wouldn't want that classification
I'd want this number to be zero I
wouldn't care if this false positive was
off by a little bit more long as I knew
that I was correct on the important
factor that I don't have cancer so you
can see that this confusion Matrix
really aims you in the right direction
of what you need to change in your model
how you need to adjust it uh and then
there's of course a report reports are
always nice um if you notice we
generated a report earlier we'll go and
just print the report up and you can
remember this is our report it's a
classification report y comma y predict
so we're just putting in our two values
basically what we did here visually with
our actual and our predicted value and
we'll go ahead and run the report and
you can see it has the Precision uh the
recall your F1 score your support uh
translated into a accuracy macro average
and weighted average so it has all the
numbers a lot of times when working with
um clients or with the shareholders in
the company this is really where you
start because it has a lot of data and
they can just kind of stare at it and
try to figure it out and then you start
bringing in like the confusion Matrix I
Almost Do This in Reverse as to what
they show I would never show your
shareholders The Intercept of the
coefficient that's for your internal
team only working on machine language uh
but the confusion Matrix and the report
are very important those are the two
things you really want to be able to
show on these uh and you can see here we
did a decent job of um classifying the
data managed to get a significant
portion of it correct uh we had our was
it accuracy here is a 080 F1 score uh
that kind of thing so you know it's a
pretty accurate model of course this is
pretty goofy it's very simple model and
it's just splitting the model between uh
ones and zeros so that was our demo of
the um on logistic regression on there
let's go take a look at K nearest
neighbors uh this one is another very
highly used and important algorithm to
understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based based on the similarity measure
the K nearest neighbor finds out the
class of the new data point by finding
its nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the new data
point then the KN andn classifies the
new data point as Class A the K and K
nearest neighbors is the number of
nearest neighbors we are looking for I
I.E if we say k equal 3 this means that
we are looking for nearest three
neighbors of unclassified data point us
we take the K value between 3 to 10 as
it leads to a better result a smaller
value of K means that noise will have a
bigger influence on the result and a
larger value of K makes it
computationally expensive hence the data
scientists prefer the range of K between
three and 10 when we talk about noise
remember the data we just looked at was
01 1 0 0 it had some some values where
cut it and said everything to the right
is a one everything to the left is a
zero but it had some some ones and zeros
mixed in there that's called noise
that's what they're talking about is
there's some things that are right in
the middle in the classification which
makes it very hard to classify so
suppose we're trying to find the class
for a new Point indicated by the red
color and you can see is kind of right
between the cat right between the dogs
let k equal three so we are finding the
three Inn for the red data point but
looking at the plot on the right we can
see that the red data point belongs to
the class dogs as it has two votes for
class dog and one vote for class cat and
if you ask the question well what are
you measuring the distance what is that
distance um it could be the measurements
of the ears whether they're pointed or
floppy that might be one of the features
you're looking at is how floppy the ears
are um another one might be the whiskers
versus the nose um and then you take
those measurements and using uh one of
the most common things in K means
measurement is the Clan geometry you can
figure out the distance between those
points there's a lot of different
algorithms for that but you can think
about it that you do have to have some
kind of solid data to measure and so we
can conclude that the new data point
belongs to the class dog so let's go
ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our Jupiter notebook and open
up a new um Python Programming script
page of course once we're in here we'll
want to look at the uh s kitar Le um I
did just a quick search for SK neighbors
Ken neighbors classifier um this
actually is the older version .0 no 023
is the one we want and you'll see here
that we have all their defaults in
Neighbors equals 5 at defaults we were
talking about that between three and 10
there's different ways to weigh it
there's an algorithm based on it I
mentioned ukian geometry finding the
distance there's other algorithms for
figuring out what that distance is and
how to weight those uh and there's a lot
of other print parameters you can adjust
for the most part the K means uh basic
setup is a good place to start and just
let the defaults go uh we might play
with some of those we'll see what the
guys in the back did and from here we're
going to import numpy we're going to use
pandas if you haven't been running
pandas pandas is our data frame which
sits on top of numpy uh data frames are
you know numpy is our number array
pandas is our data frame matap plot
Library cuz we're going to plot some
graphs everybody likes some pretty
pictures it makes it a lot easier to see
what's going on when you have a nice
display and that's also what the Seaborn
is in here in the uh setup that sits on
top of the matplot library the ones we
really want to look at right here are
the what we're bringing in from sklearn
these ones right here uh so from sklearn
we're going to load I mentioned the
breast cancer that's a very popular one
because it has I believe it's 36
measurements so there's 36 features and
unless you're a expert you're not going
to know what any of those features
really mean you can sort of guess what
their special measurements they take of
when they take a image uh and of course
our confusion Matrix so that we can take
a look and see what the data looks like
and how good we did uh and then we have
our K Neighbors classifier on here uh
and then I mentioned that uh whenever
you do training and testing you want to
split the data up you don't want to
train the data and then test it on the
same data that just tells you how good
your training model is it doesn't tell
you whether it actually works on unknown
data and so this just splits it off so
that we can train it and then we can
take a look at data we don't have in
there and see how good it did and we'll
go ahead and load our uh data up so
here's our our setup on that oops there
we go so we're going to go ahead and
load the data up uh we have our x value
and that's going to come from our breast
cancer. data and column breast cancer
feature names so there's our actual um
all our different features we'll print
that out here in a second and then we
have our um mean area mean compactness
so I guess we're going to take the data
and we're only going to use a couple of
the columns this just makes it easier to
read um of course when you actually were
going to do this you'd want to use all
your columns and then we have our Y and
this is simply um whether it's either
malignant or B9 and then um we want to
go ahead and drop the first line because
that's how it came in on there and we'll
go ahead let's just take a look at this
a little closer here let's go and run
this real quick and just because I like
to see my data before I run it we can
look at this and we can look at the
original features remember we're only
going to use two features off of here
just to make it a little easier to
follow and here's the actual data and
you can see that this is just this
massive stream of data coming in here
it's going to just skip around because
there's so much in there to set up I
think there's like 500 if I remember
correctly and you can see here's all the
different measurements they take but we
don't we don't really need to see that
on here we're just going to take a look
at just the two columns and then also
our solution we'll go a and just do a
quick uh print y on here so you can see
what the Y looks like and it is simply
just 00 0 you know B9 0000001 so a one
means it's B9 a zero means it's
malignant uh is what we're looking at on
that go and cut that out of there the
next stage is to go ahead and split our
dat data I mentioned that earlier uh
we'll just go ahead and let them do the
splitting forest for us uh we have XT
train X test y train y test and so we go
ahead and train test split XY random
State equals 1 makes it nice and easy
for us we'll go and run that and so now
we have our training and our testing
train means we're going to use that to
train the model and then we're going to
use the test to test to see how good our
model does and then we'll go ahead and
create our model here's our KNN model
the K Neighbors classifier n neighbors
equals 5 the metrics is ukian remember I
talked about ukian uh this is simply
your c¬≤ = A2 + B2 + u u A2 = B2 + C2 +
C2 + d^2 and then you take the square
root of all that that's what they're
talking about here it's just the length
of the hypotenuse of a triangle but you
can actually do that in multiple
Dimensions just like you do in two
Dimensions with a regular triangle and
here we have our fit this should start
to look familiar since we already did
that in our last example
that's very standard for s s kit and any
other one although sometimes the fit
algorithms look a little bit more
complicated because they're doing more
things on there especially when you get
into neural networks uh and then you
have your K neighbors it just tells you
we created a um a k neighbor setup they
kind of wanted us to reformat the Y but
it's not that big of a deal for this uh
and it comes out and shows you that
we're using ukian uh metric for our
measurement so now we've created a model
here's our live model we fitted the data
to it we say hey here's our training
data uh let's go ahead and predict it so
we're going to take our y predict equals
KNN predict y test so this is data we
have not this model has not seen this
data and so we're going to create a
whole new set of data off of there now
before we look at our prediction in fact
let's um I'm going to bring this down
and and put it back in here later let's
take a look at our X test data versus
the Y test what does it look like and so
we have our mean area we're going to
compare comped to our mean compactness
we're going to go ahead and run that and
we can see here the data if you look at
this just eyeballing it we put it in
here we have a lot of blue here and we
have a lot of orange here and so these
dots in the middle especially like this
one here and these here these are the
ones that are going to give us false
negatives and so we should expect this
is your noise this is where we're not
sure what it is and then B9 is in this
case is done in blue and malignant is
done in one uh so if you look at it
there's two points based on these
features which makes it really hard to
have 100% where the 100% is down here um
or up here that's kind of the thing I'd
be looking for when we're talking about
cancer and stuff like that where you
really don't want any uh false negatives
you want everything false positive great
you're going to go in there and have
another setup in there where you might
get a get a oopsy or something like that
done on it again very data specific on
here uh so now let's go ahead and pull
in and and get our um our prediction in
here and we'll create our y prediction
we'll go and run that so now this is
loaded with what we think the unknown
data is going to be and we can go ahead
and take that and go ahead and plot it
because it's always nice to have some
pretty pictures and when we plot it
we're going to do the mean area versus
mean compactness again you look at this
map and you can see that there's some
clear division here we can clear Clearly
say on some of the stuff that are why
prediction if we look at this map up
here and this map down here we probably
got some pretty good deal it looks
pretty good like they match a lot this
is of course just eyeballing it really
you don't want to eyeball these things
you want to show people the pictures so
that they can see it and you can say hey
this is what it looks like uh but we
really want the confusion Matrix and we
do the Y test and the Y predict we can
see in the confusion Matrix here it did
pretty good um and we'll just go ahead
and point that out real quick quick um
here's our 42 which is positive and our
79 and if I remember correctly I'd have
to look at the data which one of these
is a false negative I believe it's the
nine that's scary I would not want to be
one of those nine people told that I
don't have cancer and then suddenly find
out I do uh so we would need to find a
way to sort this out and there is
different ways to do that uh a little
bit past this but you can start messing
with the actual ukian geomet and the
activation uh measurements and start
changing those and how they interact but
that's very Advanced there's also other
ways to classify them or to create a
whole another class right here of we
don't knows those are just a couple of
the solutions you might use for that but
for a lot of things this works out great
uh you can see here you know maybe
you're trying to sell something well if
this was not uh life dependent and this
was if I display this ad 42 of these
people are going to buy it and if I this
other if I don't display it 79 people
are going to go a different direction or
whatever it is so maybe you're trying to
display whether they're going to buy
something if you added on to the website
in which case that's really a good
numbers you've just added a huge number
of cells to your company so that was our
K nearest neighbors uh let's go ahead
and take a look at support Vector
machines so support Vector machines uh
is the main objective of a support
Vector machine algorithm is to find a
hyperplane in an n-dimensional space in
is a number of features that distinctly
classifies the data points and if you
remember we were just looking at those
nice graphs we had earlier in fact Let
Me Go a and flip back on over there if
we were looking at this data here we
might want to try to find a nice line
through the data and that's what we're
talking about with uh this next setup so
the main objective of support Vector
machine algorithm is to find a
hyperplane in an N dimensional space in
is a number of features that distinctly
classifies the data points to separate
the two classes of data there are many
hyperplanes that can be chosen our
objective is to find the plane that has
the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyper
plane depends on the number of features
if there are two input features then the
hyper plane is just a line if there are
three features then the hyper plane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support vector classifier
or hard margin and that's why I just
showed you on the other data you can see
here we look for a line that splits the
data evenly the problem with hard margin
is that it doesn't allow outliers and
doesn't work with nonlinearly separable
data and we just were looking at that
let me flip back on over here and when
we look at this setup in here and we
look at this data here we go look how
many outliers are in here these are all
with all these blue dots on the wrong
side of the line would be considered
outlier and the same with the red line
and so it becomes very difficult to
divide this data unless there's a clear
space there we go therefore we introduce
soft margins which accept the new data
point and optimize a model for nonlinear
data points soft margins pass through
the data points at the border of the
classes the support Vector machine can
be used to separate the two classes of
shapes here we can see that although
triangles and diamond shapes have pointy
edges but we are able to classify them
in two categories using a support Vector
machine let's go ahead and see what that
looks like in a demo we flip back on
over to our Jupiter notebook we always
want to start with taking a look at the
sklearn um API in this case the svm SVC
there is a significant number of
parameters because SK the um svm has
been a lot of development in the recent
years and has become very popular if we
scroll all the way down to methods
you'll see right here as our fit and our
predict that's what you should see in
most of the um sidekit learn packages so
this should look very familiar to what
we've already been working on and we'll
go ahead and bring in our import and run
that uh we should already have pandas
numpy our map plot Library which we're
going to be using to graph some of the
things a little bit different right here
you'll see that we're going to go ahead
and bring in um one of the fun things
you can do with test data is uh make
circles circles are really hard to
classify you have a ring on the inside
and a ring on the outside and you can
see where that can cause some issues and
we'll take a look at that a little
closer in a minute here's our svm uh
setup on there and then of course our
metrics because we're going to use that
to take a closer look at things so we go
and run this whoops already did once
we've gone ahead and done that we're
going to go ahead and start making
ourselves some data and this part
probably a little bit more complicated
uh than we need I'm not going to go too
much in detail on it uh we're going to
make a mesh grid and we'll see what that
looks like in a minute where we're
defining the minimums you can see in
here create a mesh grid of points here's
our parameters of x y and H it's going
to return XX y y you can also send a
note to us make sure you get a copy of
this if you want to dig deeper into this
particular code especially and we have a
y men y Max uh y men y Max + one here's
our mesh grid we're actually going to
make XX and y y plot the Contours all
the way through and this is just a way
of creating data it's kind of a fun way
to create some data we go plot Contours
ax LF xxy Y and uh return it out add
some perimeters here so that when we're
doing it we have our setup on there
train property and then we'll go ahead
and make our data just throw that right
in there too in the same setup and run
that uh so now we have X and Y we're
going to make our circles we have n
samples equals samples in this case
we're going to have 500 samples we went
ahead and gave it uh noise of 005 random
State 123 these are all going into oops
go back up here we go make our mesh grid
make circles there it is uh so this is
going into our make circles up here and
this is part of that setup on this and
then once we've gone ahead and make the
circle let's go ahead and plot it uh
that way you can see what we're talking
about right now what I'm saying is
really confusing because without a
visual it doesn't really mean very much
much what we're actually doing so I'm
going go ahead and run this with the
plot and let's go back up here and just
take a look we've made our Circle we
have our in samples equals sample so
we're going to have 500 we're going to
have training property point8 here's our
data frame we go and load it into the
data frame so we can plot it groups DF
Group by label uh this is kind of a fun
way if you have multiple columns you can
really quickly pull whatever setup is in
there and then we go ahead and plot it
and you can see here we have two rings
that are formed and that's all this is
doing is just making this data for us
this is really hard data to figure out
um a lot of programs get confused in
this because there's no straight line or
anything like that but we can add planes
and different setups on here and so you
can see we have some very interesting
data we have our zero is in blue and our
one's in the yellow in the middle and
the data points are um an XY coordinate
plot on this one of the things we might
want to do on here is go ahead and find
the men to Max ratio uh setup in there
and we can even do let's just do a print
X so you can see what the data looks
like that we're producing on this there
we go so u xal x - Xmen over x max -
Xmen all we're doing is putting this
between zero and one whatever this data
is we want um a zero to one set up on
here and if you look at this all our
data is
8.5 that's what this particular line is
doing that's a pretty common thing to do
in processing data especially in neural
networks uh neural networks don't like
big numbers they create huge biases and
cause all kinds of problems and that's
true in a lot of our models some of the
models it doesn't really matter but when
you're doing enough of these you just
start doing them uh you just start
putting everything between zero and one
there's even some algorithms to do that
in the SK learn although it's pretty as
you can see it's pretty easy to do it
here so uh let's go ahead and jump into
the next thing which is a linear kind of
setup Cals 1.0 this is the spvm
regularization parameter we're going to
use models here's our svm and let's go
and scroll up just a notch there we go
now we so here we are with our model
we're going to create the the setup with
the SVC kernel is linear and we'll come
back to that because that's an important
uh setup in there as far as what our
kernel is and you'll you'll see why
that's so important here in a minute
because we're going to look at a couple
of these and so this one is we're
actually going to be changing how that
processes it uh and then our C here's
our one our 0.0 and then the rest of
this is uh plotting we're just adding
titles making the Contours so it's a
pretty graph you can actually spend this
would be a whole class just to do all
the cool things you can do with Scatter
Plots and regular plots and colors and
things like that in this case we're
going to create a graph with a nice
white line down the middle so that you
can see what's going on here and when we
do this you can see that as it as it
split the data uh the linear did just
that it drew a line through the data and
it says this is supposed to be blue and
this is supposed to be red and it
doesn't really fit very well that's
because we used a linear division of the
data and it's not very linear data on
it's anything but linear so when we when
we look at that it's like well okay that
didn't work what's the next option well
there's a lot of choices in here one of
them is just simply we can change this
from the kernel being linear to poly and
we'll just go back up here and use the
same chart um oops here we go uh so here
we go uh linear kernel we'll change this
to poly and then when we come in here
and create our model here's our model up
here linear we can actually just go
right in and change this to the poly
model and if you remember when we go
back over here to the SBC and let's
scroll back up here there's a lot of
different options oops even further okay
so when we come up here we start talking
about the kernel here's the kernel
there's linear there's poly RBF sigmoid
precomputed there's a lot of different
ways to do this setup um and their
actual default is RBF very important to
note that uh so when you're running
these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the uh spvm the kernel is so
important you really need to know that
and we switched our kernel to poly and
when we run this uh you can see it's
changed a little bit we now have quite
an interesting looking diagram and you
can see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is uh we do a a polyfit uh you
can see if you have a split in data
where there's a group in the middle this
one kind of data and the groups on the
outside are different the polyfit or the
poly kernel is what's going to be fit
for that so uh if that doesn't work then
what are we going to use well uh they
have the RBF kernel and let's go ahead
and take a look and see what the RBF
looks like and uh let me go and turn
there we go turn my drawing off and the
RBF kernel oops there we go RBF and then
of course for our title it's always nice
to have it match with the RBF kernel and
we go ahe and run this and you can see
that the RBF kernel does a really good
job uh it actually has divided the DAT
on here and this is the kind of what you
expect here was that ring here's our
inner ring and an outer ring of data and
so the RBF fits this data package quite
nicely um and that when we talk about
svm it really is powerful in that it has
this kind of sorting feature to it in
its algorithms this is something that is
really hard to get the SK uh means to do
or the K means uh setup and so when you
start looking at these different machine
learning algorithms understanding your
data and how it's grouped is really
important it makes a huge difference as
far as what you're Computing and what
you're doing with it so that was the
demo on the support Vector uh certainly
you could have done continued on that
and done like a confusion Matrix and all
that kind of fun stuff to see how good
it was and split the data up to see how
it uh uh vectorizes the visual on that
so important it makes a big difference
just to see what it looks like and that
giant Don it and why it it does Circle
so well or your poly version or your
linear version so we've looked at some
very numerical kind of uh setups where
there's a lot of math involved ukian
geometry um that kind of thing a totally
different machine learning algorithm for
approaching this is the decision trees
and there's also Forest that go with the
decision trees they're based on multiple
trees combined the decision tree is a
supervised learning algorithm used for
classification it creates a model that
predicts the value of a Target variable
by learning simple decision rules
inferred from the data features a
decision tree is a hierarchal tree
structure where an internal node
represents features or tribute the
branch represents a decision Rule and
each Leaf node represents the outcome
and you can see here where they have the
first one uh yes or no and then you go
either left or right and so forth one of
the coolest things about decision trees
um is and I'll see people actually run a
decision tree even though their final
model is different because a decision
tree allows you to see what's going on
you can actually look at it and say why
did you go right or left what was the
choice where's that break uh and that is
really nice if you're trying to share
that information with somebody else as
to why when you start getting into the
why this is happening decision trees are
very powerful so the topmost note of a
decision tree is known as the root note
it learns to partition on the basis of
the attribute value it partitions the
tree in a recursive manner so you have
your decision node if you get yes you
you go down to the next note that's a
decision note and either yes you go to
if it ends on a leaf note then you know
your answer uh which is yes or no so
there's your there's your in
classification set up on there here's an
example of a decision tree that tells
whether I'll sleep or not at a
particular evening mine would be
depending on whether I have the news on
or not do I need to sleep no okay I'll
work uh yes is it raining outside yes
I'll sleep no I'll work so I guess if
it's uh not raining outside it's harder
to fall asleep where they have that nice
uh ring coming in and again this is
really cool about a decision tree is I
can actually look at it and go oh I like
to sleep when it rains outside so when
you're looking at all the data you can
say oh this is where the switch comes in
when it rains outside I'll sleep really
good if it's not raining or if I don't
need sleep then I'm not going to sleep
I'm going to go work so let's go ahead
and take a look at that that looks like
in the code just like we did before we
go ahead and open up the py kit setup
just to tell you what the decision tree
classifier has yeah your parameters
which we'll look a little bit more in
depth at as we write the code but it has
uh different ways of splitting it the
strategy used to choose a split at each
node uh Criterion max depth remember the
tree how far down do you want it do you
want it to take up the space of your
whole computer with a and and map every
piece of data or you know the smaller
that number is the smaller the level the
tree is and the less processing it takes
but is also more General so you're less
likely to get as in-depth an answer um
and then of course minimal samples you
need for it to split samples for the
leaf there's a lot of things in here as
far as what how big the tree is and how
to define it and when do you define it
and how to weight it and they have their
different attributes which you can dig
deeper into uh that can be very
important if you want to know the why of
things uh and then we go down here to
our methods and you'll see just like
everything else we have our fit method
very important uh and our predict uh the
two main things that we use what is what
we're going to predict our X to be equal
to and we'll go ahead and go up here and
start putting together the code uh we're
going to import our numpy our pandas
there's our confusion Matrix our train
test Split Decision tree classifier
that's the big one that we're actually
working with uh that's the line right
here where we're going to be oops
decision tree there it is decision tree
classifier that's the one I was looking
for and of course we want to know the
accuracy and the classification report
on here and we're going to do a little
different than we did in the other
examples and there's a reason for this
let me go and run this and load this up
here uh we're going to go ahead and
build things on functions and this is
when you start splitting up into a team
this is the kind of thing you start
seeing a lot more both in teams and for
yourself because you might want to swap
one data to test it on a different data
depending on what's going on uh so we're
going to have our import data here um
the data set length the balance and so
forth um this just returns balance data
let me just go ahead and print CU I'm
curious as to what this looks like
import data and it's going to return the
balance data so if I run that uh if we
go ahead and print this out here and run
that you can see that we have uh a whole
bunch of data that comes in there and
some interesting setup on here has uh
let's see BR RR I'm not sure exactly
what that represents on here uh 111 112
and so forth so we have a different set
of data here the shape is uh five
columns 1 2 3 4 five uh seems to have a
number at the beginning which I'm going
to guess uh b r l a letter I mean and
then a bunch of numbers in there one one
one one let's see down here we got 555
uh set up on this and let's see balance
data and since it said balance data I'm
going to guess that uh b means balanced
R means you need to move it right and L
means it's um needs to be moved left or
skewed to left I'm not sure which one uh
let's go and close that out and we'll go
ahead and create a function to split the
data set uh X balance data equals data
values y balance equals data values of
zero there's that letter remember left
right and balance then we're looking for
the values of 1 through five and we go
ahead and split it just like you would X
train y train set random State 100 test
size is.3 so we're taking 30% of the
data and it's going to return your X
your y your y train your uh your X train
your test your y train your y test again
we do this because if you're running a
lot of these you might want to switch
how you split the data and how you train
it I tend to use a bfold method I'll
take a third of the data and I'll train
it on the other two thirds and test it
on that third and then I'll switch it
I'll switch which third is a test data
and then I can actually take that
information and correlate it and it
gives me a a really uh robust package
for figuring out what the compl complete
accuracy is uh but in this case we're
just going to go ahead this is our
function for splitting data and this is
where it kind of gets interesting
because remember we were talking a
little bit about uh the different
settings in our model and so uh in here
we're going to create a decision tree
but we're going to use the Gen Genie
setup and where did that come from uh
what's the genie on here uh so if we go
back to the top of their page and we
have what uh Ian are we going to use
we're going to use Genie they have Genie
and entropy those are the two main ones
that they use for the decision tree uh
so this one's going to be Genie and if
we're going to have a function that
creates the Genie model and it even goes
down here and here's our fit train of
the Genie model uh we'll probably also
want to create one for entropy sometimes
I even just um I might even make this
just one function with the different
setups and I know one of my one of the
things I worked on recently I had to
create a one that tested across multiple
models and so I would send the
parameters to the models or I would send
this part right here where it says
decision tree classifier that whole
thing might be what I send to create the
model and I know it's going to fit we're
going to have our XT Trin and we're
going to have our predict and all that
stuff is the same so you can just send
that model to your function uh for
testing different models again this just
gives you one of the ways to do it and
you can see here we're going to train
train with the genie and we're also
going to chain train with the entropy to
see how that works and if you're going
to have your models going two separate
models you're sending there we'll go
ahead and create a prediction this
simply is our y predict equals our uh
whatever object we sent whatever model
we sent here the CL LF object and
predict against our X test and you can
see here print y predict and return y
predict set up on here we'll load that
definition up and then if you're going
to have a function that it runs to
predict and print some things out uh we
should also have our accuracy function
so here's our calculate the accuracy
what are we sending we're sending our y
test data this could also be y actual
and Y predict and then we'll print out a
confusion Matrix uh then we'll print out
the accuracy of the um score on here and
print a report classification report
bundle it all together there so if we
bring this all together we have um all
the steps we've been working towards
which is importing our data by the ways
you'll spend 80% of your time importing
data in most machine learning setups and
cooking it and burning it and getting it
formatted so that it it uh works with
whatever models you're working with the
decision tree has some cool features in
that if you're missing data it can
actually pick that up and just skip that
and says I don't know how to split this
there's no way of knowing whether it
rained or didn't rain last night so I'll
look at something else like whether you
watched uh TV after 8:00 you know that
blue screen thing uh so we have our
function importing our data set we bring
in the data we split the data so we have
our X test test and Y train and then we
have our different models our clf Genie
so it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy uh and
then once we have that we have our
function for making the prediction and
we have our function for calculating the
accuracy uh and then if we're going to
have that we should probably have our
main code involved here let probably
looks more familiar if you're depending
on what you're working on if you're
working on like a pie charm then you
would see this in throwing something up
real quick in jupyter Notebook uh so
here's our our main data import which
we've already defined uh we get our
split data we create our Genie we create
our entropy so there's our two models
going on here there's our two models so
these are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie and we want to go ahead
with the genie and print our um our
predictions YX test to the genie and
calculate the accuracy on here and then
we want to print the results using
entropy so this is just the same thing
coming down like we did with the genie
we're going to put out our y predict
entropy and our calculations so let's go
ahead and run that and just see what
this uh piece of code does uh we do have
like one of our data needs to be is
getting a warning on there that's
nothing major cuz just a simple warning
probably an update of a new version's
coming out uh and so here we are we have
our data set it's got 625 you can
actually see an example of the data set
B meaning balanced I guess and here's
our five data points 1111 means it's
balanced it's skewed to the right with
1112 uh and so forth on here and then
we're going to go ahead and predict from
our prediction whether it's to the right
or to the left you can think of a
washing machine that's SK that's banging
on one side of the thing or maybe it's
an automated car where we're down the
middle of the road that's imbalance and
it starts going veering to the right so
we need to correct for it uh and when we
put out the confusion Matrix we have
three different variables r l and B so
we should three the three different
variables on here and you have as far as
whether it predicts in this case the
balance there's not a lot of balance
loads on here and didn't do a good job
guessing whether it's balanced or not
that's what I took from this first one
uh the second one I'm guessing is the
right
so it did pretty good job guessing the
right balance you can see that a bunch
of them came up left unbalanced um
probably not good for a automated car as
it tells you 18 out of the uh 18 missed
things and tells you to go the wrong
direction and here we are going the
other way uh 19 to 71 and of course we
can back that up with an accuracy report
on here and you can see the Precision
how well the left and right balance is
79% 79% precision and so forth and then
we went in use the entropy and let me
just see if we can get so we can get
them both next to each other here's our
entropy of our um the first setup our
first model which is the Genie model
6718
1971
6322
2070 pretty close the two models you
know that's not a huge difference in
numbers this second one of entropy did
slightly it looks like slightly worse
cuz it did one better as far as as the
right balance and did what is this four
worse on the left balance or whatever uh
so slightly worse if I was guessing
between these I'd probably use the first
one they're so close though that
wouldn't be it wouldn't be a Clear
Choice as to which one worked better and
there's a lot of numbers you can play
with here which might give better
results depending on what the data is
going in now uh one of the takeaways you
should have from the different category
routines we ran is that they run very
similar you you certainly change the
perimeters in them as to whether you're
using what model you're using and how
you're using it and what data they get
applied to but when you're talking about
the S kit learn package it does such an
awesome job of making it easy uh you
split your data up you train your data
and you run the prediction and then you
see what kind of accuracy what kind of
confusion confusion Matrix It generates
so um we talk about algorithm selection
logistic regression K nearest Neighbors
uh logistic regression is used when we
have a binomial outcome for example to
predict whether an email is Spam or not
whether the tumor is malignant or not
the logistic regression works really
good on that you can do it in a k
nearest neighbors also the question is
which one will it work better in um I
find the logistic regression models work
really good in a lot of raw numbers so
if you're working with say the stock
market is this a good investment or a
bad investment um so that's one of the
things it handles the numbers better K
nearest neighbors are used in scenarios
where
nonparametric no fixed number of
perimeters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection uh so K means
really good in finding the patterns um
I've seen that as a pre-processor to a
lot of other processors where you use
the K nearest neighbors to figure out
what data groups together very powerful
package support Vector machines uh
support Vector machines are used
whenever the data has higher Dimensions
the human genome microarray svms are
extensively used in the hard handwriting
recognition models and you can see that
we were able to switch between the
parabolic and the circular setup on
there where you can now have that donnut
kind of data and be able to filter that
out with the support Vector machine and
then decision trees are mostly used in
operational researches specifically
decision analysis to help identify a
strategy most likely to reach any goal
they are pre preferred where the model
is easy to understand I like that last
one that's a good description is it easy
to understand so you have data coming in
when am I going to go to bed you know is
it raining outside you can go back and
actually look at the pieces and see
those different decision modes takes a
little bit more to dig in there and
figure out what they're doing uh but you
can do that and you can actually help
you figure out why um people love it for
the why Factor so uh strings and
limitations big one on all of these the
strings and limitations we talk about
logistic regressions uh the strings are
it is easy to implement and efficient to
train it is relatively easy to
regularize the data points remember how
we put everything between zero and one
when you look at logistic regression
models uh you don't have to worry about
that as much limitations has a high
Reliance on proper representation of
data that can only predict a categorical
outcome with the the K nearest neighbors
it doesn't need a separate training
period new data can be added seamlessly
without affecting the accuracy of the
model uh kind of an interesting thing
because you can do partial training uh
that can become huge if you're running
across really large data sets or the
data is coming in it can continually uh
do a partial fit on the data with the K
nearest neighbors and continue to adjust
that data uh doesn't doesn't work on
high dimensional and large data sets we
were looking at the breast cancer
uh 36 different features what happens
when you have 127 features or a million
features and you say well what do you
have a million features in well if I was
analyzing uh log um the legal documents
I might have a tokenizer that splits a
words up to be analyzed and that
tokenizer might create 1 million
different words available that might be
in the document for doing weights uh
sensitive to noisy data outliers and
missing values that's that's a huge one
with K nearest neighbors they really
don't know what to do with a missing
value how do you compute the the
distance if you don't know what the
value is uh the svm uh Works more
efficiently on high dimensional data it
is relatively memory efficient so it's
able to create those planes with only a
few different variables in there as
opposed to having to store a lot of data
for different uh features and things
like that it's not suitable for a large
data sets uh the svm you start running
this over gigabytes of data causes some
huge issues underperforms if the data
has noise or overlapping that's a big
one we were looking at that where the
spvm splits it and it creates a soft
buffer but what happens when you have a
lot of stuff in the middle uh that's
hard to sort out it doesn't know what to
do with that causes SPM to start
crashing or not perform as well decision
trees handles nonlinear perimeters and
missing values efficiently the missing
values is huge uh I've seen this in uh
was it the wine tast data sets where
they have three different data sets and
they share certain features uh but then
each one has some features that aren't
in the other ones and it has to figure
out how to handle those well the
decision tree does that automatically
instead of having to figure a way to
fill that data in before processing like
you would with the other models uh it's
easy to understand and has less training
period so it trains pretty quickly uh
comes up there and just keeps forking
the tree down and moving the parts
around and so it doesn't have to go
through the data multiple times guessing
and adjusting it just creates the tree
as it goes overfitting and high variants
are the most annoying part of it that's
that's an understatement uh that has to
do with how many leavs and how many
decisions you have a du the more you
have the more overfit it is to the data
and also uh just in making the choices
and how the choices come in it might
overfit to a specific feature because
that's where it started at and that's
what it knows and it really um is
challenged with large data sets they've
been working working on that with the
data Forest but it's not suitable for
large data sets it's really something
you'd probably run on a single machine
and not across um not across a uh data
pool or anything decision tree one of
the many powerful tools in the machine
learning library begins with a problem I
think I have to buy a car so in making
this question you want to know how do I
decide which one to buy and you're going
to start asking questions is a mileage
greater than 20 is a price less than 15
will it be sufficient for six people
does it have enough airbag antiock bre
breaks all these questions come up then
as we feed all this data in we make a
decision and that decision comes up oh
hey this seems like a good idea here's a
car so as we going through this decision
process using a decision tree we're
going to explore this maybe not in
buying a car but in how to process data
what's in it for you let's start by
finding out what is machine learning and
why we even want to know about it for
processing our data and we'll go into
the three basic types of machine
learning and the problems that are used
by Machine learning to solve finally
we'll get into what is a decision tree
what are the problems a decision tree
solves what are the advantages and
disadvantages of using a decision tree
and then we want to dig in a little deep
into the mechanics how does the decision
tree work and then we'll go in and do a
case loan repayment prediction where we
actually going to put together some
python code and show you the basic
python code for generating a decision
tree what is machine learning there are
so many different ways to describe what
is machine learning in today's world and
illustrate it we're going to take a
graphic here and uh making decisions or
trying to understand what's going on and
really underlying machine learning is
people want to wish they were smarter
wish we could understand the world
better so you can see a guy here who's
uh saying hey how can I understand the
world better and someone comes up and
says let's use artificial intelligence
machine learning is a part of artificial
intelligence and now he get a big smile
on his face because now he has
artificial intelligence to help him make
his decisions uh and they can think in
new ways so this brings in new ideas so
what is machine learning this is a
wonderful graph here you can see where
we have a learn predict decide these are
the most basic three premises of machine
learning in learning we can describe the
data new ways and able to learn new
aspects about what we're looking at and
then we can use that to predict things
and we can use that to make decisions so
maybe it's something that's never
happened before but we can make a good
guess whether it's going to be a good
investment or not it also helps us
categorize stuff so we can remember it
better so it's easier to pull it out of
the catalog we can analyze data in new
ways we never thought possible and then
of course there's the uh very large
growing industry of recognize we can do
facial recognition driver recognition
automated car recognition all these are
part of machine learning going back to
our guy here who's in his ordinary
system and would like to be smarter make
better choices what happens with machine
learning is an application of artificial
intelligence wherein the system gets the
ability to automatically learn and
improved based on experience so this is
exciting cuz you have your ordinary guy
who now has has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you'd already have a
list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learning tool and that lets you
predict on the next person whether
they're going to be able to make their
payments or not on their loan if you
have one category where you already know
the answers the next one would be you
don't know the answers you just have a
lot of information coming in
unsupervised learning allows you to
group like liked information together so
if you're analyzing photos it might
group all the images of trees together
and all the images of houses together
without ever knowing what a house or a
tree is which leads us to the third type
of machine learning the third type of
machine learning is reinforcement
learning unlike supervised or
unsupervised learning you don't have the
data prior to starting so you get the
data one line at a time and then whether
you make a good choice or a bad choice
the machine learning tool has to then
adjust accordingly so you get a plus or
minus feedback you can liken this to the
way a human learns we experience life
one minute at a time and we learn from
that and either our memories is good or
we learn to avoid something problems in
machine learning to understand where the
decision tree fits into our machine
learning tools we have to understand the
basics of some of the machine learning
problems and three of the primary ones
fall underneath classification problems
with categorical Solutions like yes or
no true or false one or zero this might
be does it belong to a particular group
yes or no then we have regression
problems where there's a continuous
value needs to be predicted like product
prices profit and you can see here this
is a very simple linear graph uh you can
guess what the next value is based on
the first four it kind of follows a
straight line going up and clustering
this is problems where the data needs to
be organized to find specific patterns
like in the case of product
recommendation they group all the
different products that people just like
you viewed on a shopping site and say
people who buy this also bought this the
most commonly used for the decision tree
is for classification for figuring out
is it red or is it not is it a fruit or
is it a vegetable yes or no true false
left or right 01 and so when we talk
about classification we're going to look
at the basic machine learning these are
the four main tools used in
classification there's the Nave Bay
logistic regression decision tree and
random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or a curve
through the data they work Wonderful in
a lot of problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest but
today we're just going to focus on the
decision
tree what is a decision tree let's go
through a very simple example before we
dig in deep decision tree is a tree
shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction let's start with
a simple question how do identify a
random vegetable from a shopping bag so
we have this group of vegetables in here
and we can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
it's going to be a what looks to be a
red chili and if it's true it's going to
be a bell pepper from the capsicum
family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then condition to classify
problems for example discriminating
between three types of flowers based on
certain features in regression a
regression tree is used when the target
variable is numerical or continuous in
nature we fit the regression model to
the Target variable using each of the
independent variables each split is made
based on the sum of squared error before
we dig deeper into the mechanics of the
decision tree let's take a look at the
advantages of using a decision tree and
we'll also take a glimpse at the
disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and nonlinear parameters don't
affect its performance so even if the
data doesn't fit an easy curve graph you
can still use it to create an effective
decision or prediction if we're going to
look at the advantages of a decision
tree we also need to understand the
disadvantages of a decision tree the
first disadvantage is overfitting
overfitting occurs when the algorithm
captures noise in the data that means
you're solving for one specific instance
instead of a general solution for all
the data High variant the model can get
unstable due to small variation in data
low bias tree a highly complicated
decision tree tends to have a low bias
which makes it difficult for the model
to work with new data decision tree
important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is the measure
of decrease in entropy after the data
set is split so in this case based on
the color yellow we've split one group
of animals on one side as true and those
who aren't yellow yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
uh deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python cuz it'll do it for you but we'll
look on the actual math that how they
compute entropy finally we went under
the different parts of our tree and they
call the leaf node Leaf node carries the
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with the gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a e funky e of K where I equals
1 to k k would represent the number of
animal the different animals in there
where value
or P value of I would be the percentage
of that animal times the log base 2 of
the same the percentage of that animal
let's try to calculate the entropy for
the current data set and take a look at
what that looks like and don't be afraid
of the math you don't really have to
memorize this math just be aware that
it's there and this is what's going on
in the background and so we have three
giraffes two tigers one monkey two
elephants a total of eight animals
gathered and if we plug that into the
formula we get an entropy that equals 3
over 8 so we have three drafts a total
of 8 times the log usually they use base
two on the log so log base 2 of 3 over 8
plus in this case let's say it's the
elephants 2 over 8 2 elephants over
total of 8 * log base 2 2 over8 plus one
monkey over total of 8 log base 2 1
over8 and plus 2 over8 of the Tigers log
base 2 over 8 and if we plug that into
our computer our calculator I obviously
can't do log logs in my head we get an
entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has de decreased considerably however we
still need some splitting at both the
branches to attain an inpe value equal
to zero so we decide to split both the
nodes using height as a condition since
every Branch now contains single label
type we can say that entropy in this
case has reached the least value and
here you see we have the giraffes the
Tigers the monkey and the elephants all
separated into their own groups this
tree can now predict all the classes of
animals present in the data set with
100% accuracy that was easy use case
loan repayment prediction let's get into
my favorite part and open up some Python
and see what the programming code and
the scripting looks like in here we're
going to want to do a prediction and we
start with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into into the code in our first few
steps of implementation we're going to
start by importing the necessary
packages that we need from Python and
we're going to load up our data and take
a look at what the data looks like so
the first thing I need is I need
something to edit my Python and run it
in so let's flip on over and here I'm
using the Anaconda Jupiter notebook now
you can use any python IDE you like to
run it in but I find the Jupiter
notebook's really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we start let's talk a little bit
about what we're bringing in and then
we're going to do a couple things in
here we have to make a couple changes as
we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numers it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use panda as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to taking your basic data and storing it
in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just just a minute we're
going to change that and we'll go over
that too and then there's also the sk.
tree import decision tree classifier
that's the actual tool we're using
remember I told you don't be afraid of
the mathematics it's going to be done
for you well the decision tree
classifier has all that mathematics in
there for you so you don't have to
figure it back out again and then we
have SK learn. metrics for accuracy
score we need to score our our setup
that's the whole reason we're splitting
it between the training and testing data
and finally we still need the sklearn
import tree and that's just the basic
tree function is needed for the decision
tree classifier and finally we're going
to load our data down here and I'm going
to run this and we're going to get two
things on here one we're going to get an
error and two we're going to get a
warning let's see what that looks like
so the first thing we had is we have an
error why is this error here well it's
looking at this it says I need to read a
file and when this was written the
person who wrote it this is their path
where they stored the file so let's go
ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this is very lengthy setup on
here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full full file name I'm going to
go ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opport Unity
to understand what's going on here if
you read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have
sklearn doc cross validation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file cuz it's a comma separated variable
file and in a spreadsheet this is what
it looks like as a basic text file you
can see at the top they've created a
header and it's got 1 2 3 four five
columns and each column has data in it
and let me flip this over cuz we're also
going to look that this uh in an actual
spreadsheet so you can see what that
looks like and here I've opened it up in
the open Office calc which is pretty
much the same as um Excel and zoomed in
and you can see we've got our columns
and our rows of data little easier to
read in here we have we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working with
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lind function from python
and when I run this you'll see that it's
a th long and that's what we expected
there's a th lines of data in there if
you subtract the column head and this is
one of the nice things when we did the
uh balance data from the panda read CSV
you'll see that the header is row zero
so it automatically removes a
row and then shows the data separate it
is a good job sorting that data out for
us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance unor data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about this shape is not
only does it give me the length of the
data we have a thousand lines it also
tells me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now now that we've taken a look at
the length and the shape let's go ahead
and use the uh pandas module for head
another beautiful thing in the data set
that we can utilize so let's put that on
our sheet here and we have print data
set and balance data. head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of data set just a
simple print statement and when we run
that and let's just take a close look at
that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as the head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set is you want to start
with the Z position and that is what the
data head does it pulls the first five
rows of data puts in a nice format that
you can look at and view very powerful
tool to view the data so instead of
having to flip and open up an Excel
spreadsheet or open Office Cal or trying
to look at a word dock where it's all
scrunched together and hard to read you
can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember we always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal equ
to. 3 so that's roughly 30% will be used
in the test and then we use a random
state so it's completely random which
rows it takes out of there and then
finally we get to actually build our
decision tree and they've called it here
clf entropy that's the actual decision
tree or decision tree classifier and in
here they've added a couple variables
which we'll explore in just a minute and
then finally we need to fit the data to
that so we take our clf entropy that we
created and we fit the XT train and
since we know the answers for X train
are the Y train we go ahe and put those
in and let's go ahead and run this and
what most of these sklearn modules do is
when you set up the variable in this
case when we set the clf entropy equal
decision tree classifier it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that it's
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves is five
so it's going to have at least five
leavs at the end so I'll have at least
three splits I'll have no more than
three layers and at least five end
leaves with the final result at the
bottom now that we've created our
decision tree classifier not only cre it
but trained it let's go ahead and apply
it and see what that looks like so let's
go ahead and make a prediction and see
what that looks like we're going to
paste our predict code in here and
before we run it let's just take a quick
look at what's it's doing here we have a
variable y predict that we're going to
do and we're going to use our variable
clf entropy that we
created and then you'll see do predict
and that's very common in the sklearn
modules that they're different tools
have the predict when you're actually
running a prediction in this case we're
going to put our X test data in
here now if you delivered this for use
an actual commercial use and distributed
it this would be the new loans you're
putting in here to guess whether the
person's going to be uh pay them back or
not in this case so we need to test out
the data and just see how good our
sample is how good of our tree does at
predicting the loan payments and finally
since Anaconda jupyter notebook is works
as a command line for python we can
simply put the Y predict e in to print
it I could just as easily have put the
print and put brackets around y predict
e in to print it out we'll go ahead and
do that it doesn't matter which way you
do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th000 so you
should have about 300 answers in here
and this tells you which each one of
those lines of our uh test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from sklearn docs import accuracy
score and you could probably run a
script make your own script to do this
very easily how accurate is it how many
out of 300 do we get right and so we put
in our y Tex test that's the one we ran
the predict on and then we put in our y
predict that's the answers we got and
we're just going to multiply that by a
100 because this is just going to give
us an answer as a decimal and we want to
see it as a percentage and let's run
that and see what it looks like and if
you see here we got an accuracy of 93.
66667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not there's no
better time to train yourself in the
exciting field of machine learning if
you're looking for a course that covers
everything from the fundamentals to
Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our CCH postgraduate program in AI
machine learning in partnership with IBM
this& ml course sces the latest tools
and Technologies from the a EOS system
and features master classes by ctech
faculty and IBM experts hecaton and ask
me anything sessions this program
showcases CCH ctm is excellence and
IBM's industry progess the artificial
intelligence course coveres key Concepts
like statistics data science with python
machine learning deep learning NLP and
reinforcement learning through an
Interactive Learning model with live
sessions androll now and unlock exciting
a& ml opportunities the course link is
mentioned in the description box below
Forest currently today is used in remote
sensing uh for example they're used in
the etm devices if you're a space buff
that's the enhanced thermatic mapper
they use on satellites which see uh far
outside the human Spectrum for looking
at land masses and they acquire images
of the Earth's surface the accuracy is
higher and training time is less than
many other machine learning tools out
there also object detection multiclass
object detection is done using random
Forest algorithms a good example is a
traffic where you try to sort out the
different cars buses and things and it
provides a better detection in
complicated environments they very
complicated up there and then we have uh
another example connect and let's take a
little closer look at connect connect
they use a random Forest as part of the
game console and what it does is it
tracks the body movements and it
recreates it in the game and let's see
what that looks like uh we have a user
who performs a step in this case it
looks like Elvis Presley going there
that is then recorded So so the connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have uh prints going on this one
from Elvis Presley to Prince it's great
uh so it marks user base on the accuracy
if we look at that a little closer we
have a training set to identify body
parts where are the hands where are the
feet uh what's going on with the body
that then goes into a random Forest
classifier that learns from it once we
trained the classifier and then
identifies the body parts while the
person's dancing it's able to represent
that in a computer format and then based
on that it scores the game and how
accurate you are as being Elvis Presley
or prince in you're dancing let's take
an overview of what we're going to cover
today what's in it for you we're going
to start with is what is machine
learning we're not going to go into
detail on that we're going to
specifically look how the random Force
fits in the machine learning hierarchy
then we're going to look at some applic
ations of random Forest what is
classification which is its primary use
why use random Forest what's the
benefits of it and how does it actually
come together what is random forest and
then we'll get into random forest and
the decision tree how all that's like
the final step and how it works and
finally we'll get some python code in
there and we'll use the case the iris
flower analysis now if you don't know
what any of these terms mean or where
we're going with this don't worry we're
going to cover all the basics and have
you up and running and even have doing
some basic script in Python by the end
let's take a closer look at types of
machine learning specifically we're
going to look at where the decision tree
fits in with the different machine
learning packages out there we'll start
with the basic types of machine learning
there's supervised learning where you
have lots of data and you're able to
train your models there's unsupervised
learning where it has to look at the
data and then divide it based on its own
algorithms without having any training
and then there's reinforcement learning
where you get a plus or negative if you
have have the answer correct this
particular tool belongs to the
supervised learning let's take a closer
look at that what that means in
supervised learning uh supervised
learning falls into two groups
classification and regression we'll talk
about regression a little later and how
that differs this particular format goes
underneath classification so we're
looking at supervised learning and
classification in the machine learning
tools classification is a kind of
problem wherein the outputs are
categorical in nature like yes or no
true or false or zero or one in that
particular framework there's the knnn
where the NN stands for nearest neighbor
Nave Bays the decision tree which is
part of the random Forest that we're
studying today so why random Forest It's
always important to understand why we
use this tool over the other ones what
are the benefits here and so with the
random Forest the first one is there's
no overfitting if you use of multiple
trees reduce the risk of overfitting
fting training time is less overfitting
means that we have fit the data so close
to what we have as our sample that we
pick up on all the weird parts and
instead of predicting the overall data
you're predicting the weird stuff which
you don't want high accuracy runs
efficiently on large database for large
data it produces highly accurate
predictions in today's world of uh Big
Data this is really important and this
is probably where it really shines this
is where y random Forest really comes in
it estimates missing data data in
today's world is very messy so when you
have a random Forest it can maintain the
accuracy when a large proportion of the
data is missing what that means is if
you have data that comes in from five or
six different areas and maybe they took
one set of Statistics in one area and
they took a slightly different set of
Statistics in the other so they have
some of the sh same shared data but one
is missing like the uh number of
children in the house if you're doing
something over demographics and the the
other one is missing the size of the
house it will look at both of those
separately and build two different trees
and then it can do a very good job of
guessing which one fits better even
though it's missing that data let us dig
deep into the theory of exactly how it
works and let's look at what is random
Forest random forest or random decision
Forest is a method that operates by
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and this uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree 2 where we have
that picture of the fruit goes in and
this one decides that it's a lemon and
the decision 3 tree gets another image
and it decides it's an apple and then
this all comes together in what they
call the random forest and this random
Forest
then looks at it and says okay I got two
votes for apple one vote for lemon the
majority is Apples so the final decision
is apples to understand how the random
Forest works we first need to dig a
little deeper and take a look at the
random forest and the actual decision
tree and how it builds that decision
tree and looking closer at how the
individual decision trees work we'll go
ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diag rammed used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's that's what's
going to come up is the color orange
well goes hm orange or red well if it's
true then it comes out as the orange and
if it's false that leaves apples so in
this example it sorts out the fruit in
the bowl or the images of the fruit a
decision tree these are very important
terms to know because these are very
Central to understanding the decision
tree and when working with them the
first is entropy everything on the
decision tree and how it makes a
decision is based on entropy entropy is
is a measure of Randomness or
unpredictability in the data set uh then
they also have Information Gain the leaf
node the decision node and the root node
we'll cover these other four terms as we
go down the tree but let's start with
entropy so starting with entropy we have
here a high amount of Randomness what
that means is that whatever is coming
out of this decision if it was going to
guess based on this data it wouldn't be
able to tell you whether it's a lemon or
an apple it would just say it's a fruit
uh so the first thing we want to do is
we want to split this apart and we take
the initial data set we're going to
create a data set one and a data set two
we just split it in two and if you look
at these new data sets after splitting
them the entropy of each of those sets
is much less so for the first one
whatever comes in there it's going to
sort that data and it's going to say
okay if this data goes this direction
it's probably an apple and if it goes
into the other direction it's probably a
lmon so that brings us up to Information
Gain it is the measure of decrease in
the entropy after the data set is split
what that means in here is that we've
gone from one set which has a very high
entropy to two lower sets of entropy and
we've added in the values of E1 for the
first one and E2 for the second two
which are much lower and so that
information gain is increased greatly in
this example and so you can find that
the information grain simply equals uh
decision E1 minus E2 as we're going down
our list of uh definitions we'll look at
the leaf node and the leaf node carries
the classification or the decision so we
look down here to the leaf node we
finally get to our set one or our set
two when it comes down there and it says
okay this object's gone into set one if
it's gone into set one it's going to be
split by some means and we'll either end
up with apples on the leaf node or a
lemon on the leaf node and on the right
it'll either be an apple or lemons those
Leaf nodes or those final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more branches and you
can see here where we have the uh five
apples and one lemon and in the other
case the five lemons and one apple they
have to make a choice of which tree It
Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information so far we've looked at a
very general image U with the fruit
being split let's look and see exactly
what that means to split the data and
how do we make those decisions on there
uh let's go in there and find out how
does a decision tree work so let's try
to understand this and let's use a
simple example and we'll stay with the
fruit we have a bowl of fruit and so
let's create a problem statement and the
problem is we want to classify the
different types of fruits in the bowl
based on different features the data set
in the bowl is looking quite messy and
the entropy is high in this case so if
this bow was our decision maker it
wouldn't know what choice to make it has
so many choices which one do you pick
Apple grapes or lemons and so we look in
here we're going to start with a dra a
training set so this is our data that
we're training our data with and we have
a number of options here we have the
color and under the color we have red
yellow purple uh we have a diameter uh
331 331 and we have a label Apple lemon
Grapes apple lemon grapes and how do we
split the data we have to frame the
conditions to split the data in such a
way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 2 3 four threes
that's a pretty hard selection so let's
say the condition gives us a maximum
gain of three so we have the most pieces
fall into that range so our first split
from our decision node is we split the
data based on the diameter is it greater
than or equal to three if it's not
that's false it goes into the great bowl
and if it's true it goes into a bowl
fold of lemon and apples the entropy
after splitting has decreased
considerably so now we can make two
decisions if you look at there very uh
much less chaos going on there this note
is has already attained an entropy value
of zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
on our choices of color and diameter too
and if the color is yellow it's going to
go to the right bowl and if it's false
it's going to go to the left Bowl so the
entropy in this case is now zero so now
we have three bowls with zero entropy
there's only one type of data in each
one of those bowls so we can predict a
lemon with 100% accuracy and we can
predict the Apple also with 100%
accuracy along with our grapes up there
so we've looked at kind of a basic tree
in our forest but what we really want to
know is how does a random Forest work as
a whole so to begin our um random Forest
classifier let's say we already have
built three trees and we're going to
start with the first tree that looks
like this just like we did in the
example this tree looks at the diameter
if it's greater than or equal to three
it's true otherwise it's false so one
side goes to the smaller diameter one
side goes to larger diameter and if the
color is orange it's going to go to the
right true we're using oranges now
instead of lemons and if it's red it's
going to go to the left false and we
build a second tree very similar but
split differently instead of the first
one being split by a diameter uh this
one when they created it if you look at
that first Bowl it has a lot of red
objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this this bowl so that would
be the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to left if
it's true it goes off to the right let's
go ahead and bring these three trees so
you can see them all in one image so
this would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out
you can't see the color on it so it's
missing data remember one of the things
we talked about earlier is that a random
Forest works really good if you're
missing data if you're missing pieces so
this fruit has an image but maybe a
person had a black and white camera when
they took the picture and we're going to
take a look at this and it's going to
have um they put the color in there so
ignore the color down there but the
diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is a diameter greater than equal to
three is a color orange well it doesn't
really know on this one but it if you
look at the value it say true and it go
to the right tree 2 classifies it as
cherries is a color equal red is the
shape of circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal one well that's false does it grow
in the summer true so it go goes down
and looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says H
it's an orange and you can guess that if
you have two oranges and one says it's a
cherry uh when you add that all together
the majority of the vote says orange so
the answer is it's classified as an
orange even though we didn't know the
color and we're missing data on it I
don't know about you but I'm getting
tired of fruit so let's switch and I did
promise you we'd start looking at a case
example and get into some python coding
today we're going to use the case the
iris flower analysis o this is the
exciting part as we roll up our sleeves
and actually look at some python coding
before we start the python coding we
need to go ahead and create a problem
statement wonder what species of Iris do
these flowers belong to let's try to
predict the species of the flowers using
machine learning in Python let's see how
it can be done so here we begin to go
ahead and implement our Python codee and
you'll find that the first half of our
implementation is all about organizing
and exploring the data coming in let's
go ahead and take this first step which
is loading the different modules into
Python and let's go ahead and put that
in our favorite editor whatever your
favorite editor is in this case I'm
going to be using the Anaconda Jupiter
notebook which is one of my favorites
certainly there's notepad++ and eclipse
and dozens of others or just even using
the python terminal window any of those
will work just fine to go ahead and
explore this python coding so here we go
let's go ahead and flip over to our
Jupiter notebook and I've already opened
up a new page for Python 3 code and I'm
just going to paste this right in there
and let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the SK learn.
dat sets import load Iris now this isn't
the actual data this is just the module
that allows us to bring in the data the
load Iris and the iris is so popular
it's been around since 1936 when Ronald
Fischer published a paper on it and
they're measuring the different parts of
the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import a random forest
classifier from the sklearn module so SK
learn. Ensemble import random forest
classifier and then we want to bring in
two more modules um and these are
probably the most commonly used modules
in Python and data science with any of
the um other modules that we bring in
and one is going to be pandas we're
going to import pandas as PD PD is the
common term used for pandas and pandas
is basically creates a data format for
us where when you create a panda data
frame it looks like an Xcel spreadsheet
and you'll see that in a minute when we
start digging deeper into the code
Panda's just wonderful cuz it plays nice
with all the other modules in there and
then we have numpy which is our numbers
Python and the numbers python allows us
to do different mathematical sets on
here we'll see right off the bat we're
going to take our NP and we're going to
go ahead and Seed the randomness with it
with zero so np. random. seed is seeding
that as zero this code doesn't actually
show anything we're going to go ahead
and run it cuz I need to make sure I
have all those loaded and then let's
take a look at the next module on here
the next six slides including this one
are all about exploring the data
remember I told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our jupyter notebook and
here we go we've gone ahead and uh run
the import and I'm going to paste the
code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells it how to
get the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so that's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what's going on
here the first thing you want to notice
is that our DF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. data down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
alliv Iris and see what that looks like
so when I print oliv Iris I get this
long list of information and you can
scroll through here and see all the
different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of seel length seal
width petal length petal width these are
the different names they have for the
four different columns and if you scroll
down far enough you'll also see data
down here oh goodness it came up right
towards the top and uh data is equal to
the different data we're looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go ahead and set
that back to the head and this is one of
the neat features of pandas and Panda
data frames is when you do DF do head or
the panda datf frame. head it'll print
the first five lines of the data set in
there along with the headers if you have
them in this case we have the column
headers set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be 0 1 2 3 4 not 1 2 3 4 5 so
now we've got our Iris data imported
into a data frame let's take a look at
the next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have a DF
species and this is interesting because
right here you'll see where I have DF
species in Brackets which is uh the key
code for creating another column
and here we have iris. Target now these
are both in the pandas setup on here so
in pandas we can do either one I could
have just as easily done Iris and then
in Brackets Target depending on what I'm
working on both are um acceptable let's
go ahead and run this code and see how
this changes and what we've done is
we've added the target from the iris
data set as another column on the
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what aosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more colum I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here and we're going to
run that and let's talk a little bit
about what we're doing now we're
exploring data and one of the challenges
is knowing how good good your model is
did your model work and to do this we
need to split the data and we split it
into two different parts they usually
call it the training and the testing and
so in here we're going to go ahead and
put that in our database so you can see
it clearly and we've set it DF and
remember you can put brackets this is
creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy random. uniform so
we're generating a random number between
0 and one and we're going to do it for
each of the rows that's where the length
DF comes from so each row gets a
generated number and if it's less than
75 it's true and if it's greater than 75
it's false this means we're going to
take 75% of the data roughly because
there's a Randomness involved and we're
going to use that to train it and then
the other 25% we're going to hold off to
the side and use that to test it later
on so let's flip back on over and see
what the next step is so now that we've
labeled our database for which is
training and which is testing let's go
ahead and sort that into two different
variables train and test and let's take
this code and let's bring it into our
project and here we go let's paste it on
down here and before I run this let's
just take a quick look at what's going
on here is we have up above we created
remember there's our def. head which
prints the first five rows and we've
added a column is train at the end and
so we're going to take that we're going
to create two variables we're going to
create two new data frames one's called
train one's called test 75% in train 25%
in test and then to sort that out we're
going to do that by doing DF our main
original data frame with the iris data
in it and if DF is train equals true
it's going to go in the train and if DF
is train equals false it goes in the
test and so when I run this we're going
to print out the number in each one
let's see what that looks like and
you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75% in one and
25% for us to test our model on
afterward so let's jump back to our code
and see where this goes in the next two
steps we want to do one more thing with
our data and that's make it readable to
hum um I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sea length sea width
pedal length and pedal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first
four one thing I want you to notice here
is that when you're in a command line
whether it's jupyter notebook or you're
running command line in the uh terminal
window if you just put the name of it
it'll print it out this is the same as
doing
print
features and the Shand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing but for this we want to go ahead
and we'll just leave it as features
because it doesn't really matter and
this is one of the fun thing about
Jupiter notebooks is I'm just building
the code code as we go and then we need
to go ahead and create the labels for
the other part so let's take a look and
see what that for our final step in
prepping our data before we actually
start running the training and the
testing is we're going to go ahead and
convert the species on here into
something the computer understands so
let's put this code into our script and
see where that takes
us all right here we go we've set y
equal to pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what trained species is and why
we're looking at the group zero on there
and let's go up here and here is our
species remember this on that we created
this whole column here for species and
then it has Sosa Sosa Sosa Sosa and if
you scroll down enough you'd also see
virginica and verac color we need to
convert that into something the computer
understands zeros and ones so the
trained species of zero because this is
in the format of a of an array of arrays
so you have to have the zero on the end
and then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
Zer ones and twos representing the three
different kinds of flowers we have so
now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our full script that we're running
and let's paste this in here and let's
take a look and see what this is we have
we're creating a variable CF and we're
going to set this equal to the random
forest classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals 2 if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts zero is fine
for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more
cf. fit so we're fitting we're training
it we we are actually creating our
random Forest classifier right here this
is the code that does everything and
we're going to take our training set
remember we kept our test off to the
side and we're going to take our
training set with the features and then
we're going to go ahead and put that in
and here's our Target the Y so the Y is
01 and two that we just created and the
features is the actual data going in
that we put into the training set let's
go ahead and run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you were actually running your code that
wouldn't be the case and what it's
printed out is it shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals 2 you can see the random
State equals zero those are the two that
we sent in there you would really have
to dig deep to find out all these
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing word there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process
and leaf nodes that's the end nodes
remember we had the fruit and we're
talking about the leaf nodes like I said
there's a lot in this we're looking at a
lot of stuff here so you might have uh
in this case there's probably only think
three leaf nodes maybe four you might
have thousands of leaf noes at which
point you do need to put a cap on that
and say okay it can only go so far and
then we're going to use all of our
resources on processing this and that
really is what most of these are about
is limiting the process and making sure
we don't uh overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false warm start
is if you're programming at one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the
sklearn and if you're digging into deep
and running a major project on here for
today though all we need to do is fit
our train our features and our Target y
so now we have our training model what's
next if we're going to create a
model we now need to test it remember we
set aside the test feature test group
25% of the data so let's go ahead and
take this code and let's put it into our
uh script and see what that looks like
okay here we go and we're going to run
this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the Sosa the
virginica and the Versa color and what
we're putting into our predict is the
test features and I always kind of like
to know what it is I am looking at so
real quick we're going to do
test features and remember features is
an
array of SEO length SEO width petal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this case case we're going to go
ahead and put those into our predict so
we're going to put each one of these
lines of data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 00001 21122 two and and uh
again this is the flower type satos of
virginica and Versa color so now now
that we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our
script so we're pasting it in here and
you'll see that we're doing uh predict
and we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means so when
we do the predict underscore pra for
probability it generates three numbers
so we had three leaf nodes at the end
and if you remember from all the theory
we did this is the predictors the first
one is predicting a one for Sosa it
predicts a zero for virginica and it
predicts a zero for versacolor and so on
and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh Sosa
gets zero votes virginica gets .5 votes
versacolor gets 0.5 votes but let's just
go with the virginica since these two
are equal and so on and so on down the
list you can see how they vary on here
so now we've looked at both how to do a
basic predict of the features and we've
looked at the predict probability let's
see what's next on here so now we want
to go ahead and start mapping names for
the plants we want to attach names so
that it makes a little more sense for us
and that's what we're going to do in
these next two steps we're going to
start by setting up our predictions and
mapping them to the name so let's see
what that looks like and let's go ahead
and paste that code in here and run it
and this goes along with the next piece
of code so we'll skip through this
quickly and then come back to it a
little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we' been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're going
ahead and run it again cf. predict test
features remember that Returns the Z the
ones and the twos and then we're going
to set that equal to predictions so this
time we're actually putting it in a
variable and when I run
this it distributes and it it comes out
as an array and the array is sosa Sosa
Sosa Sosa Sosa we're only looking at the
first five we could actually do let's do
the first 25 just so we can see a little
bit more on there and you'll see that it
starts uh mapping it to all the
different flower types the versacolor
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and I went ahead and did uh
predictions through 25 let's just do
five and so we have cetosis cetosis
cetosis cetosis that's what we're
predicting from our test model and then
we come down here and we look at test
species I remember I could have just
done test. species. head and you'll see
it says Sosa stosa Sosa Sosa and they
match so the first one is what our
forest is doing and the second one is
what the actual data is now is we need
to combine these so that we can
understand what that means we need to
know how good our forest is how good it
is at predicting the features so that's
where we come up to to The Next Step
which is lots of fun we're going to use
a single line of code to combine our
predictions and our actuals so we have a
nice chart to look at and let's go ahead
and put that in our script in our
Jupiter notebook here let's see let's go
ahead and paste that in and then I'm
going to because I'm on the Jupiter
notebook I can do a control minus so you
can see the whole line
there there we go resize it and let's
take a look and see what's going on here
we're going to create in pandas remember
PD stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
Sosa Versa color vinica and the actual
species setosa versacolor vinica and so
the way to read this chart and let's go
ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
ver color where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ided 32 and if we multiply by
100 we can say that it is 93% accurate
so we have a 93% accuracy with our model
I did want to add one more quick thing
in here on our scripting before we wrap
it up so let's flip back on over to my
script in here we're going to take this
uh line of code from up above I don't
know if you remember it but predict
equals the iris. target names so we're
going to map it to the names and we're
going to run the prediction and we read
it on test features but you know we're
not just testing it we want to actually
deploy it so at this point I would go
ahead and change this and this is an
array of arrays this is really important
when you're running these to know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here and then uh I
actually want to see what the answer is
so let's go ahead and type in PRS and
print that out and when I run this
you'll see that I've now predicted two
flowers that maybe I measured in my
front yard as Versa color and Versa
color not surprising since I put the
same data in for each one this would be
the actual uh end product going out to
be used on data that you don't know the
answer
for so that's going to conclude our
scripting part of this so what is gamein
clustering K means clustering is an
unsupervised learning algorithm in this
case you don't have labeled data unlike
in supervised learning so you have a set
of data and you want to group them and
as the name suggests you want to put
them into clusters which means objects
that are similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically a is a
number so we need to tell the system how
many clusters we need to perform so if K
is equal to 2 there will be two clusters
if K is equal to 3 three clusters and so
on and so forth that's what the k stands
for and of course there is a way of
finding out what is the best or Optimum
value of K for a given data we will look
at that so that is K means clustering so
let's take an example K means clustering
is used in many many scenarios but let's
take an example of cricket the game of
cricket let's say you received data of a
lot of players from maybe all over the
country or all over the world and this
data has information about the runs
scored by the people or by the player
and the wickets taken by the player and
based on this information we need to
Cluster this data into two clusters
batsmen and Bowlers so this is an
interesting example let's see how we can
perform this so we have the data which
consists of primarily two
characteristics which is the runs and
the wickets so the bowlers basically
take wickets and the batsmen score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsman who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Bowlers
so how does this work let's say this is
how the data is so there are information
there is information on the y- AIS about
the Run code and on the xais about the
wickets taken by the players so if we do
a quick plot this is how it would look
and um when we do the clustering we need
to have the Clusters like shown in the
third diagram out here so we need to
have a cluster which consists of people
who have scored High runs which is
basically the batsman and then we need a
cluster with people who have taken a lot
of wickets which is typically the
bowlers there may be a certain amount of
overlap but we will not talk about it
right now so with kin's clustering we
will have here that means K is equal to
2 and we will have two clusters which is
batsmen and Bowlers so how does this
work the way it works is the first step
in C in's clustering is the allocation
of two centroids randomly so two points
are assigned as so-called centroids so
in this case we want two clusters which
means K is equal to 2 so two points have
been randomly assigned as centroids keep
in mind these points can be anywhere
there are random points they are not
initially they are not really the
centroids centroid means it's a central
point of a given data set but in this
case when it starts off it's not really
the cide okay so these points though in
our presentation here we have shown them
one point closer to these data points
and another closer to these data points
they can be assigned randomly anywhere
okay so that's the first step the next
step is to determine the distance of
each of the data points from each of the
randomly assigned centroids so for
example we take this point and find the
distance from this centroid and and the
distance from this centroid this point
is taken and the distance is form from
this centroid and this Cent and so on
and so forth so for every point the
distance is measured from both the
centroids and then whichever distance is
less that point is assigned to that
centroid so for example in this case
visually it is very obvious that all
these data points are assigned to this
centroid and all these data points are
assigned to this centroid and that's
what is represented here in blue color
and in this yellow color the next step
is to actually determine the central
point or the actual centroid for these
two clusters so we have this one initial
cluster this one initial cluster but as
you can see these points are not really
the centroid centroid means it should be
the central position of this data set
Central position of this data set so
that is what needs to be determined as
the next step so so the central point or
the actual centroid is determined and
the original randomly allocated centroid
is repositioned to the actual centroid
of this new clusters and this process is
actually repeated now what might happen
is some of these points may get
reallocated in our example that is not
happening probably but it may so happen
that the distance is found between each
of these data points once again with
these centroids and if there is if if it
is required some points may be
reallocated we will see that in a later
example but for now we will keep it
simple so this process is continued till
the centroid repositioning stops and
that is our final cluster so this is our
so after iteration we come to this
position this situation where the
centroid doesn't need any more
repositioning and that means our
algorithm has converged convergence has
occurred
and we have the cluster two clusters we
have the Clusters with a centroid so
this process is repeated the process of
calculating the distance and
repositioning the centroid is repeated
till the repositioning stops which means
that the algorithm has converged and we
have the final cluster with the data
points and the centroids so this is what
you're going to learn from this session
we will talk about the types of
clustering what is K me's clustering
application of K me's clustering K's
clustering is done using distance
measure so we will talk about the common
distance measures and then we will talk
about how kain's clustering works and go
into the details of Cain's clustering
algorithm and then we will end with a
demo and a use case for kain's
clustering so let's begin first of all
what are the types of clustering there
are primarily two categories of of
clustering hierarchical clustering and
then partitional clustering and each of
these categories are further subdivided
into agglomerative and divisive
clustering and K means and fuzzy c means
clustering let's take a quick look at
what each of these types of clustering
are in hierarchical clustering the
Clusters have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottomup
approach we begin with each element as a
separate cluster and merge them into
successively larger clusters so for
example we have a b CDE e f we start by
combining BNC form one cluster D and
form one more then we combine de and f
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top- down
approach we begin with the whole set and
proceed to divide it into successively
smaller clusters so we have a b CDE e f
we first take that as a single cluster
and then break it down into a b c d e
and f then we have partitional
clustering split into two subtypes k
means clustering and fuzzy c means in K
means clustering the objects are divided
into the number of clusters mentioned by
the number K that's where the K comes
from so if we say K is equal 2 the
objects are divided into two clusters C1
and C2 and the way it is done is the
features or characteristics are compared
and all objects having similar
characteristics are clubbed together so
that's how K means clustering is done we
will see it in more detail as we move
forward and fuzzy c means is very
similar to K means in the sense that it
clubs objects that have similar
characteristics together but while in
kin's clust clustering two objects
cannot belong to or any object a single
object cannot belong to two different
clusters in c means objects can belong
to more than one cluster so that is the
primary difference between K means and
fuzzy c means so what are some of the
applications of Cain's clustering kain's
clustering is used in a variety of
examples or variety of business cases in
real life starting from academic
performance diagnostic system search
engines
and wireless sensor networks and many
more so let us take a little deeper look
at each of these examples academic
performance So based on the scores of
the students students are categorized
into a b c and so on clustering forms a
backbone of search engines when a search
is performed the search results need to
be grouped together the search engines
very often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially K means clustering uses
distance measure so let's take a look at
what is distance measure so while these
are the different types of clustering in
this video we will focus on K mean's
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is ukian distance there is
Manhattan distance then we have squared
ukian distance measure and cosine
distance measure these are some of the
distance measures supported by K means
clustering let's take a look at each of
these what is ukian distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
- y1 s + X2 - X1 s so this is an
extension of that formula so that is the
ukian distance between two points what
is the squared ukian distance measure
it's nothing but the square of the ukian
distance as the name suggests so instead
of taking the square root
we leave the square as it is and then we
have Manhattan distance measure in case
of Manhattan distance it is the sum of
the distances across the x-axis and the
Y AIS and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
K means now let's go and check how
exactly K means clustering
works okay so this is how kain's
clustering works this is like a
flowchart of the whole process there is
a starting point and then we specify the
number of clusters that we want now
there are couple of ways of doing doing
this we can do by trial and error so we
specify a certain number maybe K is
equal to 3 or four or five to start with
and then as we progress we keep changing
until we get the best clusters or there
is a technique called Elbo Technique
whereby we can determine the value of K
what should be the best value of K how
many clusters should be formed so once
we have the value of K we specify that
and then the system will assign that
many centroids so it picks randomly that
to start with randomly that many points
that are considered to be the centroids
of these clusters and then it measures
the distance of each of the data points
from these centroids and assigns those
points to the corresponding centroid
from which the distance is minimum so
each data point will be assigned to the
centroid Which is closest to it and
thereby we have K number of initial
clusters however this is not the final
clusters The Next Step it does is for
the new groups for the Clusters that
have been formed it calculates the main
position thereby calculates the new
centroid position the position of the
centroid moves compared to the randomly
allocated one so it's an iterative
process once again the distance of each
point is measured from this new centroid
point and if required the data points
are reallocated to the new centroids and
the mean position or the new centroid is
calculated once again if the centroid
moves then the iteration continues which
means the convergence has not happened
the clustering has not converged so as
long as there is a movement of the
centroid this iteration keeps happening
but once the centroid stops moving which
means that the cluster
has converged or the clustering process
has converged that will be the end
result so now we have the final position
of the centroid and the data points are
allocated accordingly to the closest
centroid I know it's a little difficult
to understand from this simple flowchart
so let's do a little bit of
visualization and see if we can explain
it better let's take an example if we
have a data set for a grocery shop so
let's say we have a data set for a
grocery shop and now we want to find out
how many clusters this has to be spread
across so how do we find the optimum
number of clusters there is a technique
called the elbow method so when these
clusters are formed there is a parameter
called within sum of squares and the
lower this value is the better the
cluster is that means all these points
are very close to each other so we use
this within sum of squares as a measure
to find the optimum number of clusters
that can be formed for a given data set
so we create clusters or we let the
system create clusters of a variety of
numbers maybe of 10 10 clusters and for
each value of K the within SS is
measured and the value of K which has
the least amount amount of within SS or
WSS that is taken as the optimum value
of K so this is the diagrammatic
representation so we have on the y- AIS
the within sum of squares or WSS and on
the x-axis we have the number of
clusters so as you can imagine if you
have K is equal to 1 which means all the
data points are in a single cluster the
withes value will be very high because
they are probably scattered all over the
moment you split it into two there will
be a drastic fall in the within SS value
and that's what is represented here but
then as the value of K increases the
decrease the rate of decrease will not
be so high it will continue to decrease
but probably the rate of decrease will
not be high so that gives us an idea so
from here we get an idea for example the
optimum value of K should be either two
or three or or at the most four but
beyond that increasing the number of
clusters is not dramatically changing
the value in WSS because that pretty
much gets stabilized okay now that we
have got the value of K and let's assume
that these are our delivery points the
next step is basically to assign two
centroids randomly so let's say C1 and
C2 are the centroids assigned randomly
now the distance of each location from
the centroid is measured and each point
is assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned but now in this step the
actual centroid will be calculated which
may be for each of these data sets
somewhere in the middle so that's like
the main point that will be calculated
and the centroid will actually be
positioned or repositioned there same
with C2 so the new centroid for this
group is C2 in this new position and C1
is in this new position once again the
distance of each of the data points is
calculated from these centroids now
remember it's not necessary that the
distance Still Remains the or each of
these data points still remain in the
same group by recalc calculating the
distance it may be possible that some
points get reallocated like so you see
this so this point earlier was closer to
C2 because C2 was here but after
recalculating repositioning it is
observed that this is closer to C1 than
C2 so this is the new grouping so some
points will be reassigned and again the
centroid will be calculated and if the
centroid doesn't change change so that
is a repeative process iterative process
and if the centroid doesn't change once
the centroid stops changing that means
the algorithm has converged and this is
our final cluster with this as the
centroid C1 and C2 as the centroids
these data points as a part of each
cluster so I hope this helps in
understanding the whole process
iterative process of K means clustering
so let's take a look at the K means
clustering algorithm let's say we have
X1 X2 X3 n number of points as our
inputs and we want to split this into K
clusters or we want to create K clusters
so the first step is to randomly pick K
points and call them centroids they are
not real centroids because centroid is
supposed to be a center point but they
are just called centroids and we
calculate the distance of each and every
in put point from each of the centroids
so the distance of X1 from C1 from C2 C3
each of the distances we calculate and
then find out which distance is the
lowest and assign X1 to that particular
random centroid repeat that process for
X2 calculate its distance from each of
the centroids C1 C2 C3 up to CK and find
which is the lowest distance and assign
X2 to that particular centroid same with
X3 and so on so that is the first round
of assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and uh so there are K groups
all these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
we have to calculate the actual
centroids for each of these groups which
is like the main position which means
that the position of the randomly
selected centroids will now change and
they will be the main positions of these
newly formed K groups and once that is
done we once again repeat this process
of calculating the distance right so
this is what we are doing as a part of
step four we repeat step two and three
so we again calculate the distance of X1
from the centroid C1 C2 C3 and then see
which is the lowest value and assign X1
to that calculate the distance of X2
from C1 C2 C3 or whatever up to CK and
find whichever is the lowest distance
and assign X2 to that centroid and so on
in this process there may be some
reassignment X1 was probably assigned to
Cluster C2 and after doing this
calculation maybe now X1 is assigned to
C1 so that kind of reallocation may
happen so we repeat the steps two and
three till the position of the centroids
don't change or stop changing and that's
when we have convergence so let's take a
detail look at at each of these steps so
we randomly pick K cluster centers we
call them centroids because they are not
initially they are not really the Cent
roids so we let us name them C1 C2 up to
CK and then step two we assign each data
point to the closest Center so what we
do we calculate the distance of each x
value from each C value so the distance
between X1 C1 distance between X1 C2 X1
C3 and then we find which is the lowest
value right that's the minimum value we
find and assign X1 to that particular
centroid then we go next to X2 find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to CK and then
assign it to the point or to the
centroid which has the lowest value and
so on so that is Step number two in Step
number three We Now find the actual
centroid for each group so what has
happened as a part of Step number two we
now have all the point points all the
data points grouped into K groups
because we we wanted to create K
clusters right so we have K groups each
one may be having a certain number of
input values they need not be equally
distributed by the way based on the
distance we will have K groups but
remember the initial values of the C1 C2
were not really the centroids of these
groups right we assigned them randomly
so now in step three we actually
calculate the centroid of each group
which means the original point which we
thought was the centroid will shift to
the new position which is the actual
centroid for each of these groups okay
and we again calculate the distance so
we go back to step two which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in step two here okay so that might
change then we perform step three once
again to find the new centroid of this
new group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of uh K mean's
clustering we will actually see some
live Demos in uh python notebook using
python notebook but before that let's
find out what's the problem that we are
trying to solve the problem statement is
let's say Walmart wants to open a chain
of stores across the state of Florida
and uh it wants to find the optimal
store locations now the issue here is if
they open too many stores close to each
other obviously the they will not make
profit but if they if the stores are too
far apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their their customers
in their database so they can actually
use this information or this data and
use K means clustering to find the
optimal location now before we go into
the python notebook and show you the
Live code I wanted to take you through
very quickly a summary of the code in
the slides and then we will go into the
python notebook so in this block we are
basically importing all all the required
libraries like numpy matplot lib and so
on and we are loading the data that is
available in the form of let's say the
addresses for Simplicity sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct group groups already
being formed so you can actually get an
idea about how the cluster would look
and how many clusters what is the
optimal number of clusters and then
starts the actual K means clustering
process so we will assign each of these
points to the centroids and then check
whether they are the optimal distance
which is the shortest distance and
assign each of the points data points to
the centroids and then go through this
iterative process till the whole process
converges and finally we get an output
like this so we have four distinct
clusters and um which is we can say that
this is how the population is probably
distributed across Florida State and uh
these centroids are like the location
where the store should be the optimum
location where the store should be so
that's the way we determine the best
locations for the store and that's how
we can help Walmart find the best
locations for the stores in Florida so
now let's take this into python notebook
let's see how this looks when we are
learning running the code live all right
so this is the code for K means
clustering in Jupiter notebook we have a
few examples here which we will
demonstrate how K clustering is used and
even there is a small implementation of
C's clustering as well okay so let's get
started okay so this block is basically
importing the various libraries that are
required like matplot lib and numpy and
so on and so forth which would be used
as a part of the code then we are going
and creating blobs which are similar to
clusters now this is a very neat feature
which is available in psychic learn make
blobs is a nice feature which creates
clusters of data sets so that's a
wonderful functionality that is readily
available for us to create some test
data kind of thing okay so that's
exactly what we are doing here we are
using make blobs and we can specify how
many clusters we want so centers we are
mentioning here so it will go ahead and
so we just mentioned four so it will go
ahead and create some test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard K means functionality that
is readily available so we really don't
have to implement K means itself the K
means functionality or the the function
is readily available you just need to
feed the data and we create the Clusters
so this is the code for that we import K
means and then we create an instance of
K means and we specify the value of K
this ncore clusters is the value of K
remember K means in K means K is
basically the number of clusters that
you want to create and it is a integer
value so this is where we are specifying
that so we have K is equal to 4 and so
that instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training uh kind
of thing but that's the call okay so we
are calling fit and what we are doing
here we are just passing the data so X
has these values the data that has been
created right so that is what we are
passing here and uh this will go ahead
and create the Clusters
and uh then we are
using after doing uh fit We Run The
predict which basically assigns for each
of these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or will actually
start from zero cluster zero one 2 and
three maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in Yore
K means when we call predict that is
what it does and we can take a quick
look at these uh Yore K means or the
cluster numbers that have been assigned
for each observation so this is the
cluster number assigned for observation
one maybe this is for observation two
observation three and so on so we have
how many about I think 300 samples right
so all the 300 samples there are 300
values here each of them the cluster
number is given and the cluster number
goes from 0 to three so there are four
clusters so the numbers go from 0 1 2 3
so that's what is seen here okay now so
this was a quick example of generating
some dummy data and then clustering that
okay and this can be applied if you have
proper data you can just load it up into
X for example here and then run the C so
this is the central part part of the K's
clustering program example so you
basically create an instance and you
mention how many clusters you want by
specifying this parameter andore
clusters and that is also the value of K
and then pass the data to get the values
now the next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the K
means algorithm so we will just walk you
through I will walk you through the code
uh at each step what it is doing and
then we will see a couple of more
examples of how K me's clustering can be
used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing K means clustering and
there is a function for a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what K means
does right so it calculates the distance
of each point or each data set from
predefined CID and then based on
whichever is the lowest this particular
data point is assigned to that Cent so
that is basically available as a
standard function and we will be using
that here so as explained in the slides
the first step that is done in case of K
means clustering is to randomly assign
assign some centroids so as a first step
we randomly allocate a couple of
centroids which we call here we're
calling as
centers and then we put this in a loop
and we take it through an iterative
process for each of the data points we
first find out using this function
pairwise distance argument for each of
the points we find out which one which
Center or which randomly selected
centroid is the closest and accordingly
we assign that data or the data point to
that particular centroid or cluster and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
the center position right so we
calculate the new centroid and then we
check if the new centroid is the
coordinates or the position is the same
as the previous croid the positions we
will compare and if it is the same that
means the process has converged so
remember we do this process till the
centroids or the centroid doesn't move
anymore right so the centroid gets
relocated each time this reallocation is
done so the moment it doesn't change
anymore the position of the centroid
doesn't change anymore we know that
convergence has occurred so till then so
you see here this is like an infinite
Loop while true is an infinite Loop it
only breaks when the centers are the
same the new center and the old Center
positions are the same and once that is
uh done we return the centers and the
labels now of course as explained this
is not a very sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in the
change will be very minor so in that
case also that is actually convergence
right so for example the change is
01 we can consider that as convergence
otherwise what will happen is this will
either take forever or it will be never
ending so that's a small flaw here so
that is something additional checks may
have to be added here but again as
mentioned this is not the most
sophisticated implementation this is
like a kind of a rough implementation of
the K means clustering okay so if we
execute this code this is what we get as
the output so this is the definition of
this particular function and then we
call that find underscore clusters and
we pass our data X and the number of
clusters which is four and if we run
that and plot it this is the output that
we get so this is of course each cluster
is represented by a different color so
we have a cluster in green color yellow
color and so on and so forth and these
big points here these are the centroids
is the final position of the centroids
and as as you can see visually also this
appears like a kind of a center of all
these points here right similarly this
is like the center of all these points
here and so on so this is the example or
this is an example of a implementation
of K means clustering and uh next we
will move on to see a couple of examples
of how K means clustering is used in
maybe some real life scenarios or use
cases in the next example or demo we're
going to see how we can use K me's
clustering to perform color compression
we will take a couple of images so there
will be two examples and uh we will try
to use Cam's clustering to compress the
colors this is a common situation in
image processing when you have an image
with millions of uh colors but then you
cannot render it on some devices which
may not have enough memory uh so that is
the scenario where where something like
this can be used so before again we go
into the python notebook let's take a
look at quickly the the code as usual we
import the libraries and then we import
the image and uh then we will flatten it
so the reshaping is basically we have
the image information is stored in the
form of pixels and if the image is like
for example 427 by 640 and it has three
colors so that's the overall dimension
of the of the initial image we just
reshape it and um then feed this to our
algorithm and this will then create
clusters of only 16 clusters so this
this colors there are millions of colors
and now we need to bring it down to 16
colors so we use K is equal to 16
and um this is how when we visualize
this is how it looks there are these are
all about 16 million possible colors the
input color space as 16 million possible
colors and we just some compress it to
16 colors so this is how it would look
when we compress it to 16 colors and
this is how the original image looks and
after compression to 16 colors this is
how the new image looks as you can see
there is not a lot of information that
has been lost though the image quality
is definitely reduced a little bit so
this is an example which we are going to
now see in Python notebook let's go into
the python note and once again as always
we will import some libraries and load
this image called flower. jpg okay so
let we load that and this is how it
looks this is this is the original image
which has I think 16 million colors and
uh this is the shape of this image which
is basically what is the shape is
nothing but the overall size right so
this is 427 pixel by 640 pixel and then
there are three layers which is this
three basically is for RGB which is red
green blue so color image will have that
right so that is the shape of this now
what we need to do is data let's take a
look at how data is looking so let me
just create a new cell and show you what
is in data basically we have captured
this
information so data is what let me just
show you
here all right so let's take a look at
China what are the values in China and
uh if we see here this is how the data
is stored this is nothing but the pixel
values okay so this is like a matrix and
each one has about for for this 427 by
640 pixels all right so this is how it
looks now the issue here is these values
are large the numbers are large so we
need to normalize them to between zero
and one right so that's why we will
basically create one more variable which
is data which will contain the values
between zero and one and the way to do
that is divide by 255 so we divide China
by 255 and we get the new values in data
so let's just run this piece of code and
this is the shape so we now have also
yeah what we have done is we changed
using reshape we converted into the
three-dimensional into a two-dimensional
data set and let us also take a look at
how let me just
insert probably a cell here and take a
look at how data is looking all right so
this is how data is looking and now you
see this is the values are between Zer
and one right so if you earlier noticed
in case of china the values were large
numbers now everything is between 0er
and one this is one of the things we
need to do all right so after that the
next thing that we need to do is to
visualize this and uh we can take random
set of maybe 10,000 points and plot it
and check and see how this looks so let
us just flot this and so this is how the
original the color the pixel
distribution is these are two plots one
is red against Green and another is red
against Blue and this is the original
distribution of the color so then what
we will do is we will use C's clustering
to create just 16 clusters for the
various colors and then apply that to
the image now what will happen is since
the data is large because there are
millions of colors using regular K means
maybe a little time consuming so there
is another version of K means which is
called mini batch gaming so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors uh would look so this
is red against Green and this is red
against Blue there is uh quite a bit of
similar ity between this original color
schema and the new one right so it
doesn't look very very completely
different or anything like that now we
apply this the newly created colors to
the image and uh we can take a look how
this is uh looking now we can compare
both the images so this is our original
image and this is our new image so as
you can see there is not a lot of
information that has been lost uh it
pretty much looks like the original
image yes we can see that for example
here there is a little bit uh it appears
a little dlish compared to this one
right because uh we kind of took off
some of the finer details of the color
but overall the high level information
has been maintained at the same time the
main advantage is that now this can be
this is an image which can be rendered
on a device which may not be that very
sophisticated now let's take one more
example with the different image in the
second example we will take an image of
the Summer Palace in China and we repeat
the same process this is a
highdefinition color image with millions
of colors and also uh
three-dimensional uh now we will reduce
that to 16 colors using K means
clustering and um we do the same process
like before we reshape it and then we
cluster the colors to 16 and and then we
render the image once again and we will
see that the color the quality of the
image slightly deteriorates as you can
see here this has much finer details in
this which are probably missing here but
then that's the compromise because there
are some devices which may not be able
to handle this kind of high density
images so let's run this code in Python
notebook all right so let's apply the
same technique for another picture which
is uh even more intricate and has
probably much complicated color schema
so this is the image now once again uh
we can take a look at the shape which is
427 by 640x 3 and this is the new data
would look somewhat like this compared
to the flower image so we have some new
values here and we will also bring this
as you can see the numbers are much big
so we will much bigger so we will now
have to uh scale them down to values
between zero and one and that is done by
dividing by 255 so let's go ahead and uh
do that and reshape it okay so we get a
two-dimensional Matrix and uh we will
then as a next step we will go ahead and
visualize this how it looks the the 16
colors and this is basically how it
would look 16 ion colors and now we can
create the Clusters out of this the 16
cin clusters we will create so this is
how the distribution of the pixels would
look with 16 colors and then we go ahead
and uh apply this and visualize how it
is looking for with the with the new
just the 16 color so once again as you
can see this looks much rich richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done uh using
kin's clustering and we have also seen
in the previous examples of how to
implement K means the code to roughly
how to implement K means clustering and
we use some sample data using blob to
just execute the C in's clustering all
right so have you ever wondered how your
mail provider implements spam filtering
or how online news channels perform news
text classification or how companies
perform sentimental analysis of Their
audience on social media all of this and
more is done through through a machine
learning algorithm called naive Baye
classifier what is naive Bay let's start
with a basic introduction to the Bas
theorem named after Thomas baze from the
1700s who first coined this in the
western literature naive Baye classifier
works on the principle of conditional
probability as given by the base theorem
before we move ahead let us go through
some of the simple Concepts in the
probability that we will be using let us
consider the following example of
tossing two coins here we have two
quarters and if we look at all the
different possibilities of what they can
come up as we get that they can come up
as head heads they come up as head tail
tell head and tell tail when doing the
math on probability we usually denote
probability as a p a capital P so the
probability of getting two heads equals
1/4 you can see on our data set we have
two heads and this occurs once out of
the four possibilities and then the
probability of at least one tail occurs
3/4 of the time you'll see on three of
the coin tosses we have tails in them
and out of four that's 3/4s and then the
probability of the second coin being
head given the first coin is tail is 1/2
and the probability of getting two heads
given the first coin is a head is 1/2
we'll demonstrate that in just a minute
and show you how that math works now
when we're doing it with two coins it's
easy to see but when you have something
more complex you can see where these Pro
these formulas really come in and work
so the base theorem gives us a
conditional probability of an event a
given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event a we
want to know what that is so we're going
to label that a since that's our focus
and then given another event B has
occurred in the Baye theorem as you can
see on the left the probability of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred c times the probability of
a over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B time
probability of b equals the probability
of B given a Time probability of a you
can easily move that around and multiply
it and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with tells in
them giving us 3/4 or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins because you can easily understand
the math the probability of throwing a
tail doesn't matter what comes before it
and the same with the head so still
going to be 50% or 1/2 but when that
come when that probability ility gets
more complicated let's see you have a D6
dice or some other instance then this
formula really comes in handy but let's
stick to the simple example for now in
this sample space let a be the event
that the second coin is head and B be
the event that the first coin is Tails
again we reversed it because we want to
know what the second event is going to
be so we're going to be focusing on a
and we write that out as a probability
of a given B and we know this from our
formula that that equals the probability
of B given a Time the probability of a
over the probability of B and when we
plug that in we plug in the probability
of the first coin being Tails given the
second coin is heads and the probability
of the second coin being heads given the
first coin being over the probability of
the first coin being Tails when we plug
that data in and we have the probability
of the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1/2
* 1/2 over 1 12 or 12 = .5 or 1/4 so the
base theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understanding naive bays
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
Bas fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsup supervised learning there's also
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also regression
but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is naive
Bay used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
ailments and news classification when
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with a
naive Bay understanding naive Bay
classifier now we already went through a
basic understanding with the coins and
the two heads and two tails and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Bay classifier is
based on the Baye theorem which gives a
conditional probability of event a given
event B and that's where the probability
of a given b equals the probability of B
given a times probability of a over
probability of B remember this is an
algebraic function so we can move move
these different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easily come up with
a new formula for the probability of B
to me staring at these algebraic
functions kind of gives me a slight
headache it's a lot better to see if we
can actually understand how this data
fits together in a table and let's go
ahead and start applying it to some
actual data so you can see what that
looks like so we're going to start with
the shopping demo problem statement and
remember we're going to solve this first
in uh table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's uh on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead and
take a look at the data set we have a
small sample data set of 30 rows we're
showing you the first 15 of those rows
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like when we put that into
tables also note in today's world we're
not usually looking at three different
variables in 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
table for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those days
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a week day
a weekend a holiday and did they buy yes
or no as we dig in deeper to this table
for our Baye theorem let the event buy
ba a now remember when we looked at the
coins I said we really want to know what
the outcome is did the person buy or not
and that's usually the event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so is 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total total
number of days so we're looking at a
total of 30 days let's start pulling
some information from our chart and see
where that takes us and when we fill in
the chart on the right you can see that
N9 out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and eight out of 24 purchases on
a holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dates two out of six didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out 11 out
of 30 so out of the 30 people who came
into the store throughout the weekend
weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or02 so there's a
20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of BFA in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making a
purchase at all doesn't matter what day
of the week is roughly 02 or 20% and the
probability of a weekday no purchase is
roughly two out of six so two out of six
of our no purchases were made on the
weekday and then finally we take our P
of ab if you looked we've kept the
symbols up there we got P of uh
probability B probability of a
probability of B if a we should remember
that the probability of a if B is equal
to the first one times the probability
of no per buys over the probability of
the weekday so we could calculate it
both off the uh table we created we can
also calculate this by the formula and
we get the 367 which equals or 33 * 2
over 367 which equals 179 or roughly uh
17 to 18% and that'd be the probability
of no purchase done on the weekday and
this is important because we can look at
this and say as the probability of
buying on the weekday is more than the
probability of not buying on the weekday
we can conclude that customers will most
likely buy the product on a weekday now
we've kept our chart simple and we're
only looking at one aspect so you should
be able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we are actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday did they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is BC and D or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in
just a second in the formula times the
full probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of AF of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the pro probability of a no buy
is across all the data so that's where
we get the 6 out of 30 we divide that
out by the probability of each category
over the total number so we get the 20
out of 30 had a discount 23 out of 30
had a yes for free delivery and 11 out
of 30 we're on a holiday we plug all
those numbers in we get 178 so so in our
probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get 19 over 24 * 21 over 24 * 8 over 24
* the P of a 24 over 30 divided by the
probability of the discount the free
delivery times the day or 20 over 30 23
over 30 * 11 over 30 and that gives us
our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals
986 we have a prob probability of no
purchase equals 178 so finally we have a
conditional probabilities of purchase on
this day let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals
98686 plus. 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is
84.7% and the likelihood of no purchase
is
15.29% given these three different
variables so it's if it's on a holiday
if it's uh with a discount and has free
delivery then there's an 84.7 one%
chance that the customer is going to
come in and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages is so we're going
to move on and we're going to do two
steps the first step is I want you to
understand why you want to why you want
to use the naive Bays what are the
advantages of naive Bay and then once we
understand those advantages we'll just
look at that briefly then we're going to
dive in and do some python coding
advantages of naive Bay classifier so
let's take a look at the six advantages
of the naive Baye classifier and we're
going to walk around this lovely wheel
looks like an origami folded paper the
first one is very simple and easy to
implement certainly you could walk
through the tables and do this by hand
you got to be a little careful because
the notations can get confusing you have
all these different probabilities and I
certainly mess those up as I put them on
you know is it on the top or the bottom
got to really pay close attention to
that when you put it into python it's
really nice because you don't have to
worry about any of that you let the
python handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amount B of data
this is great powerful tool for that
handles both continuous and discret data
it's highly scalable with number of
predictors and data points so as you can
see you just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast it
can be used in real time predictions
this is so important this is why it's
used in a lot of our predictions on
online shopping carts uh referrals spam
fil filters is because there's no time
delay as it has to go through and figure
out a neural network or one of the other
mini setups where you're doing
classification and certainly there's a
lot of other tools out there in the
machine learning that can handle these
but most of them are not as fast as the
naive bays and then finally it's not
sensitive to irrelevant features so it
picks up on your different probabilities
and if you're short on date on one
probability you can kind of it
automatically adjust for that those
formulas are very automatic and so you
can still still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
and another one that overlaps and
because the two overlap they can then
predict the unknowns for the group that
they haven't done the second study on or
vice versa so it's very powerful in that
it is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
walk through it since we walk through
all the information on it but we're
going to do a python code doing text
classification very popular for doing
the the naive Bays so we're going to use
our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bay classifier now we're at my favorite
part we're actually going to write some
python script roll up our sleeves and
we're going to start by doing our
Imports these are very basic Imports
including our news group and we'll take
a quick glance at the Target names then
we're going to go ahead and start
training our data set and putting it
together we'll put together a nice graph
because it's always good to have a graph
to show what's going on and once we've
trained it and we've shown you a graph
of what's going on then we're going to
explore how to use it and see what that
looks like now I'm going to open up my
favorite editor or inline editor for
python you know you don't have to use
this you can use whatever your editor
that you like whatever uh interface IDE
you want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
uh and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly you
should also work in a lot of the 2x you
just have to make sure all of the
versions of the modules match your
python version and and in here you'll
notice the first line is your percentage
met plot library in line now three of
these lines of code are all about
plotting the graph this one let's The
Notebook notes and this is the inline
setup that we want the graphs to show up
on this page without it in a notebook
like this which is an Explorer interface
it won't show up now a lot of idees
don't require that a lot of them like on
if I'm working on one of my other setups
it just has a pop up and the graph pops
up up on there so you have a that setup
also but for this we want the matap plot
library in line and then we're going to
import numpy as inp that's number python
which has a lot of different formulas in
it that we use for both of our sklearn
module and we also use it for any of the
upper math functions in Python and it's
very common to see that as NP nump as NP
the next two lines are all about our
graphing remember I said three of these
were about graphing well we need our
matplot library. pyplot as PLT and
you'll see that PLT is a very common
setup as is the SNS and just like the NP
and we're going to import caborn as SNS
and we're going to do the SNS do set now
caborn sits on top of pip plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale the term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so so those
you'll see that those three lines of
code here are just importing the graph
function so we can graph it and as a
data scientist you always want to graph
your data and have some kind of visual
it's really hard just to shove numbers
in front of people and they look at it
and it doesn't mean anything and then
from the SK learn. dat set we're going
to import the fetch 20 news groups very
common one for analyzing tokenizing
words and setting them up and exploring
how the words work and how do you
categorize different things when you're
dealing with documents and then we set
our data equal to fetch 20 news groups
so our data variable will have the data
in it and we're going to go ahead and
just print the target names data. Target
names and let's see what that looks like
and you'll see here we have alt atheism
comp Graphics comp osms windows.
misscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different Topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually here going to go ahead and get
it get the data all set up and take a
look at our data and let's move this
over to our Jupiter notebook and let's
see what this code does first we're
going to set our categories now if you
noticed up here I could have just as
easily set this equal to data. Target
names because it's the same thing
but we want to kind of spell it out for
you so you can see the different
categories it kind of makes it more
visual so you can see what your data is
looking like in the background once
we've created the categories we're going
to open up a train set so this training
set of data is going to go into fetch 20
news groups and it's a subset in there
called train and categories equals
categories so we're pulling out those
categories that match and then if you
have a train set you should also have
the testing set we have test equals
fetch 20 news groups subset equals test
and category equals categories let's go
down one side so it all fits on my
screen there we go and just so we can
really see what's going on let's see
what happens when we print out one part
of that data so it creates train and
under train it creates train. dat and
we're just going to look at data piece
number five and let's go ahead and run
that and see what that looks like and
you can see when I print train. dat
number five under train it prints out
one of the Articles this is article
number five you can go through and and
read it on there and we can also go in
here and change this to test which
should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number five is a a different article but
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
length of train. data and if we run that
you'll see that the training data has
11,314 articles so we're we're not going
to go through all those articles that's
a lot of articles but um we can look at
one of them just so you can see what
kind of information is coming out of it
and what we're looking at and we'll just
look at number five for today and here
we have it rewarding the Second
Amendment IDs VT line 58 lines 58 in
article uh Etc and you can scroll all
the way down and see all the different
parts to there now we've looked at it
and that's pretty complicated when you
look at one of these articles to try to
figure out how do you wait this if you
look down here we have different words
and maybe the word from well from is
probably in all the Articles so it's not
going to have a lot of meaning as far as
trying to figure out whether this
article fits one of the categories or
not so trying to figure out which
category it fits in based on these words
is where the challenge comes in now that
we've viewed our data we're going to
dive in and do the actual predictions
this is the actual naive base and we're
going to throw another model at you or
another module at you here in just a
second we can't go into too much detail
but it deals specifically working with
words and text and what they call
tokenizing those words so let's take
this code and let's uh skip on over to
our Jupiter notebook and walk through it
and here we are in our jupyter notebook
let's paste that in there and I can run
this code right off the bat it's not
actually going to display anything yet
but it has a lot going on in here so the
top we had the print module from the
earlier one I didn't know why that was
in there so we're going to start by
importing our necessary packages and
from the SK learn features extraction.
text we're going to import tfidf
vectorizer I told you we're going to
throw a module at you we can't go too
much into the math behind this or how it
works you can look it up the notation
for the math is usually tf.idf and
that's just a way of weighing the words
and it weighs the words based on how
many times they're used in a document
how many times or how many documents
they're used in and it's a well-used
formula it's been around for a while
it's a little confusing to put this in
here uh but let's let it know that it
just goes in there and waits the
different words in the document for for
us that way we don't have to wait and if
you put a weit on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value in telling you what
this document's about same with words
like in an article in articles in cost
of un maybe cost might or where words
like criminal weapons destruction these
might have a heavier weight because they
describe a little bit more what the
article's doing well how do you figure
out all the weights in the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our SK learn. naive Bay and
that's our multinomial NB multinomial
naive Bay pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline import
make pipeline now the make pipeline is
just a cool piece of code because we're
going to take the information we get
from the tfidf vector izer and we're
going to pump that into the multinomial
INB so a pipeline is just a way of
organizing how things flow it's used
commonly you probably already guess what
it is if you've done any businesses they
talk about the sales pipeline if you're
on a work crew or project manager you
have your pipeline of information that's
going through where your projects and
what has to be done in what order that's
all this pipeline is we're going to take
the tfid vectorizer and then we're going
to push that into the multinomial in B
now we've designated that as the
variable model we have our pipeline
model and we're going to take that model
and this is just so elegant this is done
in just a couple lines of code model.fit
and we're going to fit the data and
first the train data and then the train
Target now the train data has the
different articles in it you can see the
one we were just looking at and the
train. target is what category they
already categorized that that particular
article as and what's Happening Here is
the train data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it weights all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial NB and once we go into
our naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial INB
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn US
the term predict to let us know that
we've now trained the model and now we
want to get some answers and we're going
to put our test data in there because
our test data is the stuff we held off
to the side we didn't train it on there
and we don't know what's going to come
up out of it and we just want to find
out how good our labels are do they
match what they should be now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just trending
our model creating the label so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there there we go no reason not
to use a whole screen too big so we have
here from sklearn metrics import
confusion Matrix and that's just going
to generate a set of data that says I
the prediction was such the actual truth
was either agreed with it or is
something different and it's going to
add up those numbers so we can take a
look and just see how well it worked and
we're going to set a variable mat equal
to confusion Matrix we have our test
Target our test data that was not part
of the training very very important in
data science we always keep our test
data separate otherwise it's not a valid
model if we can't properly test it with
new data and this is the labels we
created from that test data these are
the ones that we predict it's going to
be so we go in and we create our SN heat
map the SNS is our caborn which sits on
top of the PIP plot so when we create a
SNS do heat map we take our confusion
Matrix and it's going to be uh matt. T
and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 00001 format D and C
equals false have to do with the uh
format if you take those out you'll see
that some things disappear and then the
X tick labels and the Y tick labels
those are our Target names and you can
see right here that's the alt atheism
comp Graphics comp osms windows.
miscellaneous and then finally we have
our PLT dox label remember the SNS or
the caborn sits on top of our map plot
Library our PLT and so we want to just
tell it X label equals a true is is true
the labels are true and then the Y label
is prediction label so when we say a
true this is what it actually is and the
prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way I
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides cuz they have a black
background they put in there that helps
it shine a little bit better so you can
see the graph a little bit easier so in
reading this graph what we want to look
at is how the color scheme has come out
and you'll see a line right down the
middle diagonally from upper left to
bottom right what that is is if you look
at the labels we have our predicted
label on the left and our true label on
the right those are the numbers where
the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk politics
miscellaneous social religion Christian
versus Alt atheism it mislabeled some of
those and those are very similar topics
so you could understand why it might
mislabel them but overall it did a
pretty good job if we're going to create
these models we want to go ahead and be
able to use use them so let's see what
that looks like to do this let's go
ahead and create a definition a function
to run and we're going to call this
function let me just expand that just a
notch here there we go I like mining big
letters predict categories so we want to
predict the category we're going to send
it s a string and then we're sending it
train equals train we have our training
model and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model.
predicts so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and uh tokenize it and put it through
the TF IDF convert that into numbers and
weights for all the different documents
and words and then it'll put that
through our naive Bay and from it we'll
go ahead and get our prediction we're
going to predict what value it is and so
we're going to return train. Target
names predict of zero and remember that
the train. target names that's just
categories I could have just as easily
put uh categories in there. predict of
zero so we're taking the prediction
which is a number and we're converting
it to an actual category we're
converting it from um I don't know what
the actual numbers are but let's say Zer
equals alt atheism so we're going to
convert that zero to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out is returning the
variable train. Target names it'll
automatically print that for you in your
own ID you might have to put in print
let's see where else we can take this
and maybe you're a space science buff so
how about sending load to
International Space
Station and if we run that we get
science space or maybe you're a uh
automob Bill buff and let's do um oh
they were going to tell me Audi is
better than BMW but I'm going to do BMW
is better than an Audi so maybe your car
buff and we run that and you'll see it
says recreational I'm assuming that's
what recc stands for Autos so I did a
pretty good job labeling that one how
about uh if we have something like a
caption running through there president
of India and if we run that it comes up
and says talk politics miscellaneous
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that covers everything from the
fundamentals to Advanced Techniques like
machine learning algorithm development
and unsupervised learning look no
further than our CCH postgraduate
program in Ai and machine learning in
partnership with IBM this a ml course
covers the latest tools and Technologies
from the AI ecosystem and features
master classes by ctech faculty and IBM
experts hecaton and ask me anything
sessions this program showcases Celtic
ctm is excellence and IBM's industry
progess the artificial intelligence
course covers key Concepts like
statistics data science with python
machine learning deep learning NLP and
reinforcement learning through an
Interactive Learning model with live
sessions androll now and unlock exciting
a ml opportunities the course link is
mentioned in the description box below
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify texts into different
groups based on which category they
believe belong to using the naive Bas
classifier now we did throw in the
pipeline the TF IDF vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bay setup or
classifier but they're important to know
one of the main uses for the naive Bay
is with the TF IDF tokenizer vectorizer
where it tokenizes the word as labels
and we use the pipeline because you need
to push all that data through and it
makes it really really easy and fast you
don't have to know those to understand
naive Bays but they certainly help for
understanding the industry and data
science and we can see our categorizer
our naive Bay classifier we were able to
predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our train
model by now we all know machine
learning models make predictions by
learning from the past data available so
we have our input values our machine
learning model Builds on those inputs of
what we already know and then we use
that to create a predicted output is
that a dog little kid looking over there
and watching the black cat cross their
path no dear you can differentiate
between a cat and a dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purrs doesn't love to
play around dogs they have dull claws
bigger length of ears spks loves to run
around you usually don't see a cat
running around people although I do have
a cat that does that where dogs do and
we can look at these we can say uh we
can evaluate the sharpness of the claws
how sharp are their claws and we can
evaluate the length of the ears and we
can usually sort out cats from dogs
based on even those two
characteristics now tell me if it is a
cat or a dog notd question usually
little kids no cats and dogs by now
unless they live a place where there's
not many cats or dogs so if we look at
the sharpness of the cloth the length of
the ears and we can see that the cat has
smaller ears and sharper claws than the
other animals it's features are more
like cats it must be a cat sharp claws
length of ears and it goes in the cat
group because KNN is based on feature
similarity we can do classification
using KNN classifier so we have our
input value the picture of the black cat
it goes into our trained model and it
predicts that this is a cat coming out
so what is knen n what is the K&N
algorithm K nearest neighbors is what
that stands for it's one of the simplest
supervised machine learning algorithms
mostly used for classification so we
want to know is this a dog or it's not a
dog is it a cat or not a cat it
classifies a data point based on how its
neighbors are classified KNN stores all
available cases and classifies new cases
based on a similarity measure and here
we gone from cats and dogs right into
wine another favorite of mine K&N stores
all available cases and classifies new
cases based on a similarity measure and
here you see we have a measurement of
sulfur dioxide versus the chloride level
and then the different wines they've
tested and where they fall on that graph
based on how much sulfur dioxide and how
much chloride K and KNN is a perimeter
that refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equal 5
we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals 5 I mean that's
was the value we put in there so we're
going to talk about it how do we choose
a factor k k andn algorithm is based on
feature similarity choosing the right
value of K is a process called parameter
tuning and is important for better
accuracy so at k equal 3 we can classify
we have a question mark in the middle as
either a as a square or not is it a
square or is it in this case a triangle
and so if we set k equals to three we're
going to look at the three nearest
neighbors we're going to say this is a
square and if we put k equals to seven
we classify as a triangle depending on
what the other data is around and you
can see as the K changes depending on
where that point is that drastically
changes your answer and uh we jump here
we go how do we choose the factor of K
you'll find this in all machine learning
choosing these factors that's the face
you get it's like oh my gosh did I
choose the right K did I set it right my
values in whatever machine learning tool
you're looking at so that you don't have
a huge bias in One Direction or the
other and in terms of K andn the number
of K if you choose it too low the bias
is based on it's just too noisy it's
it's right next to a couple things and
it's going to pick those things and you
might get a skewed answer and if your K
is too big then it's going to take
forever to process so you're going to
run into processing issues and resource
issues so what we do the most common use
and there's other options for choosing K
is to use the square root of n so N is a
total number of values you have you take
the square root of it and most cases you
also if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where I you might
get into gig of data if it's really
clean doesn't have a lot of noise
because K&N is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not they
underweight on the basis of the given
data we have to classify the below set
as normal or under weight using KNN so
if we have new data coming in that says
57 kg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the square root of x -
a^ 2 + y - b^ 2 and you can remember
that from the two edges of a triangle
we're Computing the third Edge since we
know the X side and the yide let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance D1 is a square otk
of 170 - 167 2 + 57 - 51 SAR which is
about 6.7 and distance two is about 13
and distance three is about 13 4
similarly we will calculate the ukian
distance of unknown data point from all
the points in the data set and because
we're dealing with small amount of data
that's not that hard to do and it's
actually pretty quick for a computer and
it's not a really complicated Mass you
can just see how close is the data based
on the ukian distance hence we have
calculated the ukan distance of unknown
data point from all the points as
showing where X1 and y1 equal 57 and 170
whose class we have to classify so now
we're looking at that we're saying well
here's the ukian distance who's going to
be their closest neighbors now let's
calculate the nearest neighbor at k
equals 3 and we can see the three
closest neighbors puts them at normal
and that's pretty self-evident when you
look at this graph it's pretty easy to
say okay what you know we're just voting
normal normal normal three votes for
normal this is going to be a normal
weight so majority of neighbors are
pointing towards normal hence as per KNN
algorithm the class of 57 170 should be
normal so recap of knnn positive integer
k is specified along with a new sample
we select the K entries in our database
which are closest to the new sample we
find the most common classification of
these entries this is the classification
we give to the new sample so as you can
see it's pretty straightforward we're
just looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes or not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
1 2 3 4 5 6 78 um eight columns with a
particular tribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes because
that's why they're taking it and that
could cause issue on some of the machine
learning packages but for very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
didn't take very much to open it up um I
can scroll down to the bottom of the
data there's
768 it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see have a
number of windows open up at the top the
one we're working in and uh since we're
working on the KNN predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
sell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run as python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy as NP
pandas is the pandas data frame and
numpy is a number array very powerful
tool to use in here so we have our
Imports so we've brought in our pandas
our numpy our two general python tools
and then you can see over here we have
our train test split by now youed to be
familiar with splitting the data we want
to split part of it for training our
thing and then training our particular
model and then we want to go ahead and
test the remaining data just see how
good it is pre-processing a standard
scaler pre-processor so we don't have a
bias of really large numbers remember in
the data we had like number pregnancy
isn't going to get very large where the
amount of insulin they take and get up
to 256 so 256 versus 6 that will skew
results so we want to go ahead and
change that so they're all uniform
between minus one and one and then the
actual tool this is the K neighbors
classifier we're going to use and
finally the last three are three tools
to test all about testing our model how
good is it we just put down test on
there and we have our confusion Matrix
our F1 score and our accuracy so we have
our two two general python modules we're
importing and then we have our six
modules specific from the sklearn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so that we can see what we're
doing so here's our data set equals pd.
read CSV that's a pandas command and the
diabetes folder I just put in the same
folder where my I python script is if
you put in a different folder You' need
the full length on there we can also do
a quick length of uh the data set that
is a simple python command Len for
length we might even let's go ahead and
print that we'll go print and if you do
it on its own line link. data set in the
jupyter notebook it'll automatically
print it but when you're in most of your
different setups you want want to do the
print in front of there and then we want
to take a look at the actual data set
and since we're in pandas we can simply
do data set head and again let's go
ahead and add the print in there if you
put a bunch of these in a row you know
the data set one head data set two head
it only prints out the last one so I us
always like to keep the print statement
in there but because most projects only
use one data frame Panda data frame
doing it this way doesn't really matter
the other way works just fine and you
can see when we hit the Run button we
have the 768 lines which we knew and we
have our pregnancies it's automatically
given a label on the left remember the
head only shows the first five lines so
we have zero through four and just a
quick look at the data you can see it
matches what we looked at before we have
pregnancy glucose blood pressure all the
way to age and then the outcome on the
end and we're going to do a couple
things in this next step we're going to
create a list of columns where we can't
have zero there's no such thing as zero
skin thickness or zero blood pressure
zero glucose uh any of those you'd be
dead so not a really good Factor if they
don't if they have a zero in there
because they didn't have the data and
we'll take a look at that because we're
going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values talked
about glucose blood pressure skin
thickness uh and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on uh very common thing
to do and then for this particular setup
we certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the na but we're
going to go ahead and do it as a data
set column equals data set column.
replace this is this is still pandas you
can do a direct there's also one that
that you look for your n a lot of
different options in here but the Nan
nump Nan is what that stands for is is
non doesn't exist so the first thing
we're doing here is we're replacing
the zero with a numpy none there's no
data there that's what that says that's
what this is saying right here so put
the zero in and we're going to replace
zeros with no data so if it's a zero
that means the person's well hopefully
not dead hopefully they just didn't get
the data the next thing we want to do is
we're going to create the mean which is
the in integer from the data set from
the column do mean where we skip Naas we
can do that that is a panda's command
there the skip na so we're going to
figure out the mean of that data set and
then we're going to take that data set
column and we're going to replace all
the
npnn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did not get the data and
the data is missing one of the tricks is
you replace it with the average what is
the most common data for that this way
you can still use the rest of those
values to do your computation and it
kind of just brings that particular
value or those missing values out of the
equation let's go ahead and take this
and we'll go ahead and run it doesn't
actually do anything so we're still
preparing our data if you want to see
what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our data set with a printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before
before we do any processing before
proceeding any further we need to split
the data set into our train and testing
data that way we have something to train
it with and something to test it on and
you're going to notice we did a little
something here with the uh Panda
database code there we go my drawing
tool we've added in this right here off
the data set and what this says is that
the first one in pandas this is from the
PD pandas it's going to say within the
data set we want to look at the iocation
and it is all rows that's what that say
so we're going to keep all the rows but
we're only looking at zero column 0 to 8
remember column 9 here it is right up
here we printed it in here is outcome
well that's not part of the training
data that's part of the answer yes
column nine but it's listed as eight
number eight so 0 to eight is nine
columns so uh eight is the value and
when you see it in here zero this is
actually 0 to 7even it doesn't include
the last one and then we go down here to
Y which is our answer and we want just
the last one just column eight and you
can do it this way with this particular
notation and then if you remember we
imported the train test split that's
part of the SK learn right there and we
simply put in our X and our y we're
going to do random State equals zero you
don't have to necessarily seed it that's
a seed number I think the default is one
when you seed it I'd have to look that
up and then the test size test size is
0.2 that simply means we're going to
take 20% of the data and put it aside so
that we can test it later that's all
that is and again we're going to run it
not very exciting so far we haven't had
any print out other than to look at the
data but that is a lot of this is
prepping this data once you prep it the
actual lines of code are quick and easy
and we're almost there with the actual
writing of our KNN we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scaler which means instead of
the data being from you know 5 to 303 in
one column and the next column is 1 to
six we're going to set that all so that
all the data is between minus one and
one that's what that standard scaler
does keeps it standardized and we only
want to fit the scaler with the training
set but we want to make sure the testing
set is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scaler we're going to call it scor X for
the scaler and we're going to import the
standard scaler into this variable and
then our X train equals scor x. fit
transform so we're Crea the scaler on
the XT train variable and then our X
test we're also going to transform it so
we've trained and transformed the X
train and then the X test isn't part of
that training it isn't part of that of
training the Transformer it just gets
transformed that's all it does and again
we're going to go and run this if you
look at this we've now gone through
these steps all three of them we've
taken care of replacing our zeros for
key columns that shouldn't be zero and
we replace that with the mean
of those columns that way that they fit
right in with our data models we've come
down here and we split the data so now
we have our test data and our training
data and then we've taken and we scaled
the data so all of our data going in now
no we don't tra we don't train the Y
part the Y train and Y test that never
has to be trained it's only the data
going in that's what we want to train in
there then Define the model using K
neighbors classifier and fit the train
data in the model so we do all that data
prep and you can see down here we're
only going to have a couple lines of
code where we're actually building our
model and training it that's one of the
cool things about Python and how far
we've come it's such an exciting time to
be in machine learning because there's
so many automated tools let's see before
we do this let's do a quick length of
and let's do y we want let's just do
length of Y and we get 768 and if we
import math we do math. square root
let's do y train there we go it's
actually supposed to be X train before
we do this let's go ahead and do import
math and do math square root length of Y
test and when I run that we get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love Jupiter notebook cuz
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier n neighbors equal
11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the ukian metric there are other means
of measuring the distance you can do
like square square means value there's
all kinds of this but the ukian is the
most common one and it works quite well
it's important to evaluate the model
let's use the confusion Matrix to do
that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score and finally
accuracy score which is probably the
most commonly used quoted number when
you go into a meeting or something like
that so let's go ahead and paste that in
there and we'll set the cm equal to
confusion Matrix y test y predict so
those the two values we're going to put
in there and let me go ah and run that
and print it out and the way you
interpret this is you have the Y
predicted which would be your title up
here we could do uh let's just do p r d
predicted across the top and actual
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and and
prediction and the actual agreed on 94
and 32 this number here the 13 and the
15 those are what was wrong so you could
have like three different if you're
looking at this across three different
variables instead of just two you'd end
up with the third row down here and the
column going down the middle so in the
first case we have the the and I believe
the zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
at high risk and the 32 two that had
diabetes it had correct but our
prediction said another 15 out of that
15 it classified as incorrect so you can
see where that classification comes in
and how that works on the confusion
Matrix then we're going to go ahead and
print the F1 score let me just run that
and you see we get a 69 in our F1 score
the F1 takes into account both sides of
the balance of false positives where if
we go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're a data
scientist and you're talking to other
data scientists they're going to ask you
what the F1 score the F score is if
you're talking to the general public or
the U decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics and running an
sklearn and running the KNN the K
nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurate score we got the
0818 was pretty close to what we got and
we can pretty much round that off and
just say we have an accuracy of 80%
tells us that is a pretty fair fit in
the model so what is the Asar algorithm
before we talk about the AAR algorithm
let's talk a bit about search
algorithms in computer science a search
algorithm is an algorithm which
typically solves a search problem
they're used to retrieve information
which is stored within some data
structure like arrays Stacks trees hash
buckets Etc or
calculate the search space of a problem
domain either with discrete or
continuous
variables so consider you have a stack s
which contains four
elements now a stack is an ordered
collection of items in which you can add
a new item and remove an existing item
at the same time in Stacks you can put
in an item on top of another item and
you can't access the elements below the
top element without removing the top
element first so suppose we want to find
the index of uh say an element in the
stack say 64 so we're going to run it
through a search algorithm to find 64
within our data
structure what the search algorithm is
going to do is that it's going to take a
look at the top element and see that
it's not 64 it's going to remove the top
element and move on to the next element
again it's going to compare and see if
it's 64 if it's not this element two
will get removed and then it will move
on to the next element in the stack
which is
64 in this case it's going to return the
index of this
element which is at stack index 2 and
using stack index 2 we can access this
element so in a way we found the element
in the stack
this is how search algorithms work this
is just an instance or a criteria on
which we were searching but using search
algorithms you can have multiple
different search conditions so you can
search for every element which is below
a certain number say 40 within this
stack or you can also search for
elements which are at a certain index so
this is how powerful search algorithms
are some examples of search algorithms
include binary search linear search jump
search interpolation search Etc now a
vital aspect of search algorithms is
path finding which is used to find paths
that can be taken to Traverse from one
point to another by finding the most
optimum route the AAR algorithm is one
algorithm like this it is a searching
algorithm that is used to find the
shortest path between an initial and a
final
point
so assume that you have a grid or a maze
in this case your person is is at the
start and they have to reach the end of
the maze which is here at this point
denoted by this
flag now using AAR
algorithm we can perform map traversal
and find the shortest path which has to
be taken within a map to go from the
starting position to the end
position now in this case there are many
different paths that can be taken one
option is to go along the top edges of
the map the other one is to go along the
bottom
edges and the shortest path as you can
see which takes only four grids is to go
directly through the center of the map
cut across this diagonal and reach the
end now one major aspect of AAR Al
algorithm is that it always takes the
shorter Parts first which makes it an
optimal and complete
algorithm an optimal algorithm will find
the least cost outcome for a problem
while a complete algorithm finds all the
possible outcomes of a problem AAR
algorithm combines all of these to find
all the possible outcomes of a problem
starting with the least cost
solution now this is what makes a
algorithms so
handy AAR was initially designed as a
graph traversal problem to help build a
robot that can find its own course and
because of this it Still Remains a
widely popular algorithm for graph
traversal now let's take a look at the
basic concept behind the AAR
algorithm so we already know that AAR
algorithm is is an optimal and complete
algorithm but another aspect that makes
it so powerful is the use of weighted
graphs in its
implementation a weighted graph is
nothing but a graph which uses numbers
to represent the cost of taking each
path or course of action this means that
the algorithms can take the path with
the least cost and find the best route
in terms of distance and time in path
finding
algorithms for algorithm such as a star
you're obviously going to need a graph
or a map or a path to travel right
so for computers we feed in something
called a graph a graph contains of two
major
components first it contains something
called a
node so this is a
node and it contains something called a
path a path is nothing but a line which
connects two
nodes using this we know the direction
or the path that we have to take when
we're traversing through a graph right
so um first let's just make a very
simple graph okay uh right now we have
two nodes A and B I'm just going to add
two more
C and
D
now this connection with four nodes a b
c and d and their paths is called a
graph this whole thing basically this
whole thing is your graph these are the
graphs that these algorithms will have
to travel through and find the shortest
distance on so now how do we know where
we need to go in our
graph for this we have something called
a start node and an end node so let's
say our a is nothing but a start
node and our C over
here this
node is our
end
node right so now we know that we have
to go from this node a all the way to
this node C in this case it becomes very
easy I can either go from A to B to to C
or from a to d to C but how do I know
which path is the most Optimum path or
the path which has the least cost this
is made very easy by the use of weighted
graphs this is a basic graph but in
weighted graphs all these paths between
nodes have a certain weight to them or a
certain number assigned to
them like say let's just say this is two
the cost of this path is three this path
has a cost of five and this path has a
cost of one so now as you can see we've
assigned numbers to the paths between
different nodes these numbers tell us
the cost of traversing through that path
so when I take this path between a to B
I have I get a cost of two incured on me
and when I go from B to C I have a cost
of three similarly when I go from a to D
if I take this path I'll have a cost of
five and D to C will have a cost of 1 so
this helps us in finding out which paths
we should be taking when we're
traversing from one node to the other
now in this case let's look at the
different possible paths we can take to
reach C the first option is that we go
from A to B and this will give us a cost
of two and then from B
we go to
C which will give us a cost of
three now the total cost of taking these
paths can easily be obtained by finding
the summation of the individual cost of
these parts so this will give us five
all we have to do is add up the weights
that each path has when we're traveling
it and we get to know what is going to
be the cost of a path taken so a path a
b to C has a cost of five similarly our
path a to D and D to C has a cost of 5 +
1 because a to d is 5 and D to C is 1
which is
6 so between five and six we know that
five is
lesser which means that this path A to B
to
C is going to be a least cost path and
that is the path that we should be
taking so AAR algorithm makes use of
something called weighted graphs right
so AAR algorithm makes use of these
graphs in these ways now let's consider
that I have some other node here let's
say that I have a
node e okay now B is connected to e and
e is also connected to C let's say that
the cost of taking this path B to e is 1
and E to C is zero so now let's
calculate the cost of taking the path a
b e and C okay so A to B to e to C is
nothing but 2 + 1 + 0 which is equal to
3 when compared to A to B to C
which is nothing but 2 + 3 which is 5
and a to d to C which is nothing but 5 +
1 which is 6 this path is obviously
going to be the shorter path right but
let's consider our programmer or let's
say we ourselves don't actually want to
take this path because coding for it is
hard or because it's very much isolated
from the map so we prefer to go
transverse along some other
nodes and this is not a preferred node
but looking at the graph no matter what
we do depending on how the weights are
distributed we are still going to have
to go via this
node now to overcome this programmers
make use of something called heuristic
values what are heuristic values so far
using a weighted graph we've given
importance to our parts we found out
which parts are the easiest to take and
which parts are the ones which give us a
higher cost of
implementation Now using heuristic
values we're going to do the same to our
nodes the heuristic value of a node in a
graph attempts to capture the importance
of that nodes value within that graph so
the heuristic value is nothing but an
arbitrary value which is assigned to
these nodes based on the programmer's
whims and needs just like how we have
the weightage of Parts theistic value
gives us the weight of a node and tells
us how expensive it is to Traverse
through that node it is represented by H
ofn so now let's assign heuristic values
to our graph and understand how exactly
it changes how we Traverse this graph
right so let's say our initial start
node a has a heuristic value of one and
our node B has a huris value of say
three a node e which is out of the way
and which we don't really want to
Traverse much on uh we can assign a very
high htic value to it to ensure that
it's not a path which is regularly taken
so we can assign a tic value of 10 to e
let's assign other arbitary values to
the other nodes let's give d a euristic
value of one and see a heuristic value
of
zero so now as you can see
even though the paths over here take on
a very short value it would become a lot
more expensive to Travers through e just
because of its single high heuristic
value as compared to the other nodes how
does having a heuristic value in our
graph change how we are traversing so
after implementing the heuristic value
to a graph we now have two major values
which come into the play the heuristic
value which is nothing but the value of
the node and the path
weight we can represent this as G of n
which is nothing but the weights in our
weighted graph right so as you can see
when we're traversing from one node a to
a node B we're going to have to consider
both of these values so the cost
function of a graph or the total cost of
taking a path
becomes the value of the path weight
which is given by the weighted
graph along with theistic value which is
assigned by the programmer or by you
yourself and this here is the basic
function behind our a star algorithm
this is the formula that our AAR
algorithm works
on so initially the AAR algorithm will
calculate the cost to all of its
immediate neighboring nodes and choose
the one which is incurring the least
cost this whole process will repeat
until no new nodes can be chosen and all
the paths have been
traversed and then you should consider
the best path among them and how do you
find the cost function again you're just
going to add up the heuristic value and
the path value that you've taken
now G of n which is the path value is
nothing but the cost of traversing from
one node to another very obviously this
will vary from node to node whereas HFN
a heuristic approximation of a nod value
is not a real value but an approximation
cost that you or some other person has
assigned to various
nodes now that we know the basic concept
behind the AAR algor algorithm let's
take a look at how exactly the AAR
algorithm works with the help of a case
study so now let's take a look at how
the AAR algorithm Works to do this we're
first going to consider a weighted graph
since ASR algorithm uses weighted graphs
to make its calculations
easier so consider this to be our
weighted graph we have a node
a a node B
a node
c a node
D and node
e
okay now node a is connected to node B
node B is connected to node
e node e is connected to node d d is
connected to C and C in turn is
connected to a C is also connected to b
and a is also connected to e so now
let's assign the path values to all the
different paths in our weighted graph
let's say A to B has a path value of 1 B
to e has a path value of six D2 e has a
path value of uh five c2d has a path
value of 9 a to c has a path value of 10
and a to e has a path value of two and
this has a path value of three now let's
assign heuristic values let's give a the
heuristic value of two uh B theistic
value of um five let's just give this a
seven let's make
this
four and let's give C A humanistic value
of zero okay so now this is what I
weighted graph looks like um and uh from
the basic uh working you already know
that the cost function of your weighted
graph is nothing but the path
value along with
theistic value so let's now
say that a here is where we have to
start
from and E and E is
our Target node okay and this is where
we are going to
be starting
from now a is a starting node so there
are three different parts that we can
take a to b a to c or a to e so first
let's consider that we take a to B okay
the first thing that you have to
consider is the huris IC value of a
itself so F of a is the first value that
you're going to be
considering right now F of a here is
going to be the uh path Value Plus
heuristic value now because a is the
starting node and there's no other node
leading up to it it is the parent node
itself so the path value here is going
to be zero because there's no path
leading up to it it's the starting node
and the starting node always has a path
value of zero next we're going to add
theistic value of a in this case 2 so 0
+ 2 so F of a is
2 right so this is the first node that
you've considered okay now after taking
node a we have three different options a
to c a to E or a to
B
so the cost function of taking the path
A to B is going to be so now let's find
the cost of taking the path A to B right
A to B so you have to add the graph node
Value Plus theistic value the weight of
taking the node A to B is nothing but
1
and the heuristic value of B is five so
this is going to give us C next
a to c sorry starting next let's look at
the cost of taking the path a to e right
F of a to e so this is going to be two
because the cost of taking this path is
two along with the heuristic value of e
which is 7 so this is going to be nine
and let's look at F of a to
c now the cost of taking this is going
to be 10 plus the value of C which is
zero so this is going to be
10 now the way how AAR algorithm works
is it will calculate the cost function
of all the neighboring nodes and go with
the node which has the least cost
function in this case the node being B
so when not going to take this path or
this path because the cost function
value is way more than the cost function
value of f of a to
B so after a we are going to be at node
B so right now this is the direction
that we've traveled in now when we at
node B to reach node e we have two
different options such we have two
options that we can take we can either
go from B B to e or we can go from B to
C okay uh the goal of a star algorithm
is to always
minimize the cost of the path that
you're taking so using this in mind
let's
calculate F of B to
C which is going to be nothing but the
cost of the path from B to C which is
three plus the heuristic value of C
which which is zero so this is going to
be three next let's calculate the cost
of B to e now this is going to be again
nothing but the path value in this case
six plus heuristic value in this case of
e which is
7 which is nothing but
13 so again as you can see this is the
correct path to be taken
and we are not going to be taking this
part because its value is way too high
so the next part that we're going to be
taking is this path B to C so now we at
node C from node C we can either go to a
which is not possible because we do not
go back in the direction that we came
from to B again that's not what we're
going to be doing or to D so the only
option that we have available is to go
from C to d right so what we're going to
do is we're going to find the cost of
going from C to D again this is going to
be the value of the path which is 9 plus
theistic value of D which is 4 the cost
of traversing this path is 13 but we
don't have any other path available to
us because this path has already been
closed you can't back backtrack in AAR
algorithm or go back to the path where
you came from and again we've already
considered a to c before right like over
here we've already considered a to c and
we've already ruled it out so again this
is not a path that you can take so what
we're going to be doing right now is
that we're going to be traversing from C
to D and the cost of taking this path is
13 so now
this is the path that we're taken from C
to D we're at D now at D again we can't
go back to this path we can't go back to
C the only way we can go is e so we're
going to consider F of D to e and this
is going to be again the path that
you're going to take from D to e which
is
five plus the heuristic value of e which
is
7 this whole thing is going to be
12 now once we're at C our AAR algorithm
is going to check if this is the target
node and lo and behold it is so we've
traversed from a to e let's see the path
we've taken first the starting node that
we started off from was a from there we
looked at the cost of taking a to c a to
E and A to B and finally we settled for
going from A to B from B again we
consider the paths B to e and B to C we
cannot consider a to B because this path
has already been traveled on and it's
already been considered
before now after considering B to e and
B to C we found that B to C was the
shortest path from C we didn't have any
other option we couldn't back drag or go
back back to B and we couldn't go to a
because this path was already been
considered and already it has been ruled
out so the only path we could take from
here was to D and again from D the only
path that we could take was to e so now
let's look at the path that we've taken
while using our Asar algorithm we
started off at the node a this was our
starting node from node a we had three
options we could go to node B to node e
or to node C after considering all of
these options we realized that A to B
was the path of least cost that we could
take once at B we considered two paths B
to e and B to C and we found out that B
to C was the path of least cost from C
there was no other path we could take we
cannot go back to the node we came from
which is B because AAR algorithm does
not support backtracking and we couldn't
go back to a either because this path
had already been considered and we'
already ruled this path out right so the
only path we could take was
to
D now once we arrived at node D the only
node that we could travel to again was
e so using AAR algorithm this is the
final path that we've taken to Traverse
from a start node a to a Target node
e and with the help of this case study
we've also explained to you exactly how
the AAR algorithm works we we use the
path values and the heuristic values to
find the cost of taking different paths
available to us at each
node and then we take the path which has
the least cost among all available paths
the same process will continue until
we've reached the final Target node and
again remember you cannot go back on the
path that you came from or you cannot go
on a path which has already been
considered so now you know how the AAR
algorithm works but what is the
flowchart of the AAR
algorithm now a flowchart is a picture
of the separate steps of a process in a
sequential order it's basically a
generic tool that can be adopted for a
wide variety of purposes and you can use
it to describe various processes in this
case this is the basic flowchart of the
AAR algorithm it tells the direction of
control flow within the AAR program and
using this you can Implement AAR
algorithm in any desired language of
your
choice so the first step of our AAR
algorithm is to initialize a start node
and put put it in an open list now in
this algorithm we're going to first
create two lists an open list and a Clos
list the open list will contain all the
nodes that have been visited but the
neighbors have yet not been explored on
the other hand the close list contains
nodes that along with their neighbors
have already been
visited so the first node or our
starting node is going to be put in an
open list because we're still exploring
it then we're going to calculate the
cost function of the starting
node from here onwards we're going to
remove this node from the open list and
put it in the Clos list since we've
already visited it and calculated its
cost function if you're also looking at
the Neighbors which are surrounding this
node or if this node has many different
nodes around it then we're going to
calculate the cost function for those
nodes too and save the index index of
the node which has the smallest cost
function
f next we're going to look at if the
node we're currently
on is a final node or a Target node if
that's the case we're going to terminate
the algorithm and use pointers of all of
our nodes to get the desired
path if that's not the case then we're
going to look at the successive nodes of
n or all the nodes which come after this
node which which are not in the close
list we're going to calculate the cost
function of each of these neighboring
nodes remove them from the open list as
they're not already in the closed list
so they're obviously going to be in the
open list so we're going to remove them
from the open list and then we're going
to put them in the closed list and again
we're going to save the index of the
neighboring node with the smallest cost
function and then this process is going
to keep repeating over and over until
we've reached the target node which is
where our program will get terminated so
basically what we're doing is we're
going to a certain node calculating the
cost function for that
node looking at all the successive
neighboring nodes of it and going to the
neighbor which has the least cost
function now this process will again get
repeated so from this neighboring node
we're going to find all the other nodes
which are connected to it
and then we are going to go to the node
which has the least cost function so
using this we're going to hop from one
node to another depending on which node
has the lowest cost
function once this is over we're going
to again take some other node and we're
going to repeat this process until we
found all the possible paths to a final
destination
node and in this way the AAR algorithm
is not only going to find the the
shortest path but it's also going to
find all possible
paths now let's see how the AAR
algorithm can be implemented with the
help of
python in this demo we will use the ASR
algorithm to find the least cost path
between various nodes in the weighted
graph which is shown below so in this
graph as you can see we have six
different nodes a b c d e and G the
weights of the paths between these nodes
is denoted by these numbers in black and
theistic values of the nodes itself are
denoted by the numbers in red Now using
these values we can find the shortest
distance between any two nodes let's see
how we can do this with the help of
python so now let's start by creating a
class for the AAR algorithm called the
AAR
algo now in this algorithm we're going
to have to describe
the open and the close list and to do
this we're going to be using two sets so
our sets are our open list and a closed
list we'll also be using two
dictionaries one to store distance from
the starting node and another for parent
nodes and we're going to start by
initializing to them to zero the open
set is going to have the start node
already in it now
we all know that the distance of the
starting node from itself is zero itself
so we're going to initialize the
distance of the start node to zero and
we also know that the start node is the
root node and it has no parent nodes so
the start node is its own parent node
the first step of initializations is to
put the start node in the Clos set to
set the distance of the start node to
itself as
zero and to set set the start node as
its own parent node now that we're done
with the first couple of
initializations let's move
on to finding all the neighboring
nodes so now we're going to have to find
the neighboring nodes with the lowest
value of the cost
function we also have to code for the
condition of reaching the destination
node if this is not the case we're going
to have to put the current nodes in the
open list and if they're not already in
it then we're going to set the parent
nodes so that's what we're going to do
first we're going to find the node with
the lowest cost function we're going to
take the entire open set and while the
length of it is greater than zero or
basically while we're not at the parent
node we're going to Loop through it
we're going to go to all the neighboring
nodes of the open node
and then we're going to find their uh
cost function now here this is the
function that you're going to use to
find the cost
function now here what we're going to do
is we're going to calculate theistic
value for n and if the cost function of
n is zero or if its cost function is
less than the cost function of its
neighboring node then we're going to go
we're going to move move on to the next
node which is V now if n is equal to the
stop node or we've looped back and come
to n
itself then we're not going to go ahead
or we're going to exit this
Loop and we're going to move on to some
other
node otherwise if the neighbor has a
lower G value than the current node and
is in the closed list we're going to
replace this new node as the neighbor's
paing
and that's what we're doing here exactly
now if the neighbor is not in both the
lists then we're going to add it to the
open list and set its G value and that's
basically what's going on here so let's
run this for the AAR
algorithm now we're going to define a
function to get the different neighbors
and their
distances next we're going to create a
function to store theistic values and
and retrieve them as and when
required now we're going to describe our
graph here so these are all of our nodes
and the nodes that they're connected to
along with the weight of their parts so
here a is connected to B and the weight
of this path is two a is also connected
to e and the weight of this path is
three in the same way B is connected to
C and the path has a weight of one and G
B is also connected to G and and the
path has a weight of
nine so here this is what we're going to
do to describe our entire uh weighted
graph and then we're going to finally
call our AAR algorithm and give a
starting node and our final
node
and it very quickly finds the path
between our starting node and our final
node and over here as we can see the
best part to take would be from a to E
to D and finally to G phas detection
system it is a form of biometric
recognition a method for identifying or
confirming someone's identity by
glancing at their face is called facial
recognition people can be identified by
securing a match on facial ID using this
technique realtime visuals videos and
photos can be the sources to run phas
detection this technology is mostly
employed in security and law enforcement
ment open CV is the best technology to
create it a python package called open
CV is made specifically to address
computer vision jobs computer vision is
a process used in the processing of
images by computers it is concerned with
the in-depth comprehension of digital
photos or films it is necessary to
automate operations that can be
performed by human visual systems
therefore a computer should be able to
identify items like a statue or a lamp
poost or even the face of a human being
second one is chatbot it is the best
idea if you have chosen chatbot as your
project topic this will make your resume
more attractive in case you're looking
for a job an oracle survey suggests that
80% of the businesses uses chatbot you
can use Python Java Ruby C++ or PHP as a
programming Lang language to develop a
chatbot designing and building NLP
chatbots that accept speech and Text
data is made easier by dialog flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chatbot
regardless of how excellent your chatbot
is there is always room for development
the finest chatboard developers
constantly enhance their bards over the
time using Ai and machine learning
social media recommendation system the
rise of web services like Netflix Amazon
and YouTube has increased the use of
recommended systems in our daily lives
they are algorithms that insist users in
finding information that is pertinent to
them recommenda systems are important in
some firms since they can generate a lot
of income or allow you to set yourself
apart from rivals in order to provide
recommendations it evaluates the Rel
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirants one will love to develop stock
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can provide them a
better understanding of various aspects
of the field coming to medical diagnosis
this project is advantageous from a
medical standpoint AI projects can be
created to detect heart-based diseases
and also detect cancer it is intended to
offer patients with heart illness online
medical advice and guidance after
processing the data this system will
search the database for any illnesses
that might be connected to the given
details using data mining techniques
this intelligent system determines the
disease that the patient's information
most closely
resembles based on the system diagnosis
users can then speak with qualified
medical professionals users of the
system can also view information about
various doctors coming to an important
project that is search engine search
engines are utilized by all we look for
information on the greatest product to
buy a nice area to hang out or solutions
to any questions we have MLP is quite
significant in modern search engines
because a lot of language processing
takes place there python is the widely
used language to develop any search
engine search engine is mainly confined
with lots and lots of data it is helpful
for any AI career aspirant next is
virtual assistance here the challenge is
to build a virtual assistant to assist
user why do you need virtual assistant
in your devices when you are building
your own it is also interesting for a ml
developer to build a virtual assistant
it involves NLP and data mining
voice-based virtual assistants are
popular today because they make life
easier for users NLP is utilized to
comprehend human language in order to
construct this system when a voice
command is received the system will
translate it into machine language and
store the commands in its database hey
ha speech detection in social media
automated hate speech detection is a
crucial weapon in the fight against hate
speech propagation especially on social
media for the job many techniques have
been developed including a recent
explosion of deep learning based system
for the objective of detecting hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning and combination of of both of
them on the other hand a number of data
set benchmarks including Twitter
sentiment analysis have been introduced
and made available for the evolution of
the performance of these algorithms and
the last one is predicting house price
you will need to estimate the sale price
of a brand new home in any place for
this assignment the data set for this
project includes information on the cost
of homes in various City neighborhoods
the UCI machine learning repository is
the place where you may find the data
set needed for This research you will
also receive other data set with
information on the age of the population
the city's crime rate and the location
of non- retail Enterprises in addition
to the pricing of various residences
it's an excellent project to test your
knowledge if you are an new popular
market research projects that the market
for artificial intelligence will grow
from $59.7 billion in in 2021 to 44 22.4
billion in 2028 artificial intelligence
Automation and Robotics are disrupting
almost every business companies that
don't invest in AI Services risk
becoming obsolete whether in machine
learning smart applications Appliance
digital assistants or autonomous vles
numerous companies stand to benefit from
AI but a handful of them have proven to
be the game changers for 2023 and Beyond
so hey everyone you are already watching
simply learn and here we are with the
list of top 10 AI companies in 2023 if
you love watching videos like these do
hit the Subscribe button to never miss
an update from Simply learn so let's
explore and learn about them one by one
starting with the 10th position which is
uipath robotic process automation or RPA
is a technology developed by uipath that
helps businesses to boost productivity
and reduced cost by automating timec
consuming and repetitive operations
software robots can be equipped with
with uip paath AI Technologies to carry
out duties including reading documents
and emails interpreting language and
visual cues and comprehending the
content and the intent of the
communications the RPA Market is
estimated to be worth 38 billion and UI
paath offers distinct advantages over
the competitors thanks to its patent
computer vision technology and wide
range of B Technologies in the ninth
position we have dines the surfaces
offered by dries include obser
availability and infrastructure
monitoring the devis AI engine developed
by the business can analyze 368 trillion
dependencies each second devis can
quickly locate problems in a company's
digital ecosystem explain what went
wrong and evaluate and rate likely
commercial ramifications the company
estimated a market worth
285.0 million and is poised to grow
Revenue by roughly 20% annually
throughout at least 2025 then at the
eighth position we have talented
Technologies data analytics software
startup palente Technologies uses AI to
analyze data according to revenue and
market share palente was ranked as the
to AI software platform globally by IDC
in September 2022 the United States
intelligence and defense Industries
account for a sizable portion of
valente's Revenue the business just won
a new 85.1 million contract from the US
Army Marshal command to assist with the
developing Ai and machine learning
Technologies to enhance equipment
reliability improve predictive
maintenance and improve Supply chains
palente May deliver revenues closer to
the higher end of analysis estimates
roughly 2.5 billion in 2023 then at the
seventh position is workday workday is a
provider of cloud-based applications
with an emphasis on human resources
management with a systems of workday
distinctive AI based optimization engine
businesses can manage challenges with
hiring and Staffing fluctuation labor
demand shift scheduling and prioritizing
and More in April 2022 work day
Incorporated Barista and AI powered
virtual assistant from espressive into
their platform vday is Raising 2023
markets Worth to almost 5.53 billion
representing year-over-year growth of
22% now in the sixth position we have
intuitive surgical The DaVinci surgical
system offered by intuitive surgical
performs minimally invasive procedures
using Cutting Edge robots and
computerized visualization technology
with the help of a big data and AI
intuitive is developing tools that will
help help surgeons by improving their
training and offering them realtime
Direction despite the fact that the D in
surgical system has been approved by
usfda for 22 years intuitive reported a
13% year-over-year increase is installed
in the system most recent quarter the
global intuitive surgical Market is
projected to reach $6.5 billion by 2023
moving towards the fifth position we
have IBM for years IBM has been
developing strategies to use its AI
supercomputer to alter the legal banking
academic and Healthcare Industries IBM's
clients harness the power of AI to stay
current on everchanging regulations by
providing endtoend risk and compliance
management IBM continues to dominate the
market for the other breakthroughs in Ai
and its Auto Ai and auto AML products
can help data scientist build and
improve Ai and machine learning models
the market worth of IBM in 2023 will
range from 14.4 billion to 16 million
following that in the fourth position is
NVIDIA high-end chipmaker Nvidia
provides the significant computing power
needed for advanced AI applications in
reality one of the world's fastest
supercomputers Leonardo is powered by
Nvidia Graphics processing units the
parent company of Facebook meta
platforms is building the biggest
artificial intelligence supercomputer in
the world meta also owns nidia's Quantum
INF band networking system and
680a 100 nvda graphics processing
processes Nvidia is a significant
supplier for fast growing Industries
like high-end gaming business Graphics
cloud computing artificial intelligence
and advanced automative technology
nvidia's outlook for 2023 revenue is
expected to be around 6 billion here
comes the top three in the list and in
the third position we have Amazon AI is
integrated into every aspect of Amazon's
business including e-commerce search
engines targeted advertising and Amazon
web services Amazon Alexa one of the
most popular virtual assistant is
already present in many American Homes
Amazon said in August that it would pay
1.7 billion to acquire robot scorp RBT a
maker of Home robots this action can
allow Amazon to boost the use of AI in
household worldwide Amazon expects a 491
billion Market worth between 2023 and
2027 in the second position is alphabet
alphabet the company that owns Google
and YouTube uses Ai and Automation in
practically every part of its business
including ad pricing Gmail spam filters
and content promotion in addition it is
the parent company of the autonomous car
manufacturer voo and the AI software
business Deep Mind which created history
in 2022 when it became the first
completely autonomous commercial taxi
service to operate on public routes the
market worth of alphabet in 2023 will be
around $70 billion and the first
position on the list is Microsoft in
2020 Microsoft announced the
construction of a new supercomputer
hosted on azure Microsoft's cloud
computing Network the super computer was
created in collaboration with open AI to
train AI models producing substantial AI
models and related infrastructure for
programmers and other businesses in
October Microsoft released Microsoft
designer a graphic design tool that
makes original social media post
invitations and other Graphics using AI
technology Microsoft the tech giant is
expected to earn a market worth of 49.73
billion and with that we have come to
the end of this list of of the top 10
artificial intelligence companies of
2023 opportunities in AI have increased
significantly in recent years the surge
in jobs related to AI is legitimate due
to its widespread involvement in crucial
Fields according to inded the pay range
for jobs involving artificial
intelligence is between
$160,000 to
$350,000 and can go even higher so we
have come up with the list of leading
career opportunities in artificial
intelligence along with companies hiring
for those roles but before we begin here
is a question for you who do you think
is currently leading the AI Market let
us know in the comment section below so
without any further delay let's get
started starting with our list we have
robotics engineer an engineer in
robotics create prototypes constructs
and test machines and updates the
software that manages them additionally
they investigate the most affordable and
secure way to make their robotics system
aspirant should have a bachelor degree
in computer science and pursue a career
in robotics with additional training or
certification in automation is
advantageous robotics engineers are in
high demand with 9,000 plus job openings
in India and 8,000 plus openings in the
US top companies like Amazon Bosch and
flybase are hiring robotics engineers
with salaries as high as
$160,000 in the US and rupes 20 7 lakhs
in India next up on our list we have
product manager the role of a product
manager is to recognize and express user
requirements market research and
creating competitive evaluations and
also creating a product's vision and
putting emphasis on a product strengths
and qualities freshers cannot join as a
product manager it requires basic
experience of two to three years in the
field of product and software
development according to indie.com and
every aage of 2,000 plus jobs are
currently available in India and 20,000
plus job vacancies are there in the US
with top companies hiring are Tesla
Amazon TCS sprinkler cognizant Etc with
a salary as high as 160,000 per year in
the US and rupees 20 lakhs perom in
India up next we have data scientist
they mainly deal with lots of data
extraction of Knowledge from all
collected data is the subject matter of
data science it was also voted the
sexiest job of the 21st century good
news is there is no need for any special
degree as a data scientist you need to
have knowledge of machine learning
programming in Python R Java and
mathematical modeling being a fresher
you can join as a data scientist too
data scientist are in demand with
177,000 plus vacancies in the US and
3,000 plus vacancies in India and big
businesses like accenter TCS IBM Google
JP Morgan Deo Bank of America recruiting
data employees with salary going as high
as $200,000 per year in the US and
rupees 15 lakhs perom in India moving
forward in our list we have ai data
analyst data mining data cleaning data
interpretation are the three main task
of an AI data analyst data cleaning
allows for the Gathering of the
necessary information for data analysis
an AI data analyst make inferences from
the data using statistical tools and
techniques you need a bachelor degree in
computer science or mathematics to work
as an AI data analyst to get this job
you must have a thorough understanding
of regression and be able to use msxl
according to indie.com there are
currently 700 plus AI data analyst jobs
available in the United States and
16,000 plus job vacancies in India with
top companies like Accenture IBM vpro
TCS and caps
recruiting them and salaries going as
high as
$100,000 per year in the US and rupees
8.5 lakhs per anom in India the next job
opportunity in AI is business
intelligence engineer a business
intelligence developers main duty is to
take both business end and AI into
account they evaluate complex data sets
to identify various business Trends they
assist in boosting a company's earnings
by planning creating and sustaining
business inter IG solutions to become a
business intelligence engineer you are
required to have a bachelor degree in
mathematics and computer science
according to ind.com there are 8,000
plus job available in India and 5,000
plus job vacancies in United States
companies hiring are Amazon Development
Center IBM AWS Etc with this salary
going as high as $100,000 per year in
the USA and rupees 15 lakhs per year in
India coming to the top three jobs
available in AI at number three we have
machine learning engineer artificial
intelligence is known to include machine
learning it runs simulations using the
Vari data provided and produces precise
results they are constantly in demand
from the businesses and their position
seldom goes unfilled they handle
enormous amount of data and have
exceptional data management skills the
ability to code use computers and
understand mathematics a requirement for
for success as a machine learning
engineer a master's degree in computer
science or mathematics is highly
preferred the necessary technological
Stacks include python R Scala and Java a
deep understanding of neural networks
deep learning and machine learning
algorithm is very helpful coming to job
opportunities glass door.com predicts
13,000 plus job vacancies in the us. and
27,000 plus jobs available in India with
companies like mathwork Google Google
Amazon Microsoft azour IBM Etc hiring
them with salary going as high as
150,000 perom in the US and rupees 10
lakhs perom in India coming at runnerup
position we have data architect data
Architects are it Specialists who
utilize their knowledge of computer
science and database architecture to
assess and analyze and organization data
infrastructure fresher graduates cannot
become data Architects a bachelor degree
in computer science and computer
engineering or related subject is the
absolute minimum requirement to become a
data architect data Architects are in
high demand with 52,000 job vacancies in
the United States and 7,000 plus jobs in
India and top companies hiring data
Architects are Amazon IBM Tesla Intel
Vio with the average salary quite high
as
$200,000 per year in the United States
and R 21 lakhs perom in India and
Topping our list of job opportunities in
AI is AI engineer AI Engineers are
problem solvers who create test and use
various artificial intelligence models
they manage the AI infrastructure weth
to create practical AI models they use
neural network knowledge and machine
learning methods these models enable one
to gain business insights which AIDS the
organization in making Vice business
decisions us or PG degree in the field
of AI or computer science is required in
addition to that you can hold
certifications from any reputed
organization which will add up to your
resume please check out and enroll to
the AI program by simply learn in
collaboration with puru University the
link is provided in the description
below job openings for an AI engineer
are quite high with 35,000 plus job
vacancies in the US and 4,000 plus jobs
in India and Tech giants like Accenture
TCS IBM Google JP Morgan Deo Bank of
America recruiting them with salary
going as high as
$200,000 per year in the US and rupees
10 lakhs perom in India if you are
looking to get certified in artificial
intelligence simply learns postgraduate
program in Ai and machine learning find
the complete cause details from the link
in the description box the term
generative AI has emersed seemingly out
of no where in recent months with a
notable search in interest according to
Google Trends even within the past year
the spike in curiosity can be attributed
to the introduction of generative models
such as d 2 B and
chbt however what does generative Ai and
tail as a part of our introductory
series on generative AI this video will
provide a comprehensive overview of a
subject starting from the basics the
explanation Will C to all levels of
familiarity ensuring that viewers gain a
better understanding of how this
technology operates and its growing
integration to our daily lives
generative AI is after all a tool that
is based on artificial intelligence a
professional who elit seats to switch
careers with AI by learning from the
experts then try giving a short simply
lar postgraduate program in Ai and
machine learning from perd University in
collaboration with IBM the link in the
description box should navigate to the
homepage where you can find a complete
overview of the program being offered
take action upskill and get ahead what
is generative AI generative AI is a form
of artificial intelligence possesses the
capability of to generate a wide range
of contact including text visual audio
and synthetic data the recent excitement
surrounding generative AI stems from the
userfriendly interfaces that allow users
to effortlessly create high quality text
graphics and video within a seconds now
moving forward let's see how does
generative AI Works generative AI begin
a prompt which can take form of text
image video design design audio musical
notes or any input that AI system can
process various AI algorithm that
generate new content in response to the
given prompt this content can range from
essay and problem solution to realistic
created using images or audio of a
person in the early stages of generative
AI utilizing the technology involved
submitting data through an API or a
complex process developers need to
acquain themel with a specialized tool
and writing application using
programming language like python some of
the re and fully operational generative
AIS are Google Bart Dal open AI chat gbt
Microsoft Bing and manyo so now let's
discuss chat GPT Dal and B which are the
most popular generative AI interfaces so
first is DAL 2 which was developed using
open as GPT implementation in 2021
exemplify a multimodel AI application it
has been trained on a v data set of
images and their corresponding textual
description Dal is capable of
establishing connection between various
media forms such as Vision text audio it
specifically links the meaning of words
to visual elements open a introduced an
enhanced version called d 2 in 2022
which empowers user to generate imagery
in multiple Styles based on their
prompts and the next one is chat GPT in
November 2022 chat GPT an AI power
chatboard built on open AI GPD 3.5
implementation game immense popularity
worldwide open AI enabled user to
interact with and F tune the chatbot
text response through a chat interface
with interactive feedback unlike earlier
version of GPT that was solely
accessible via an API chat GPT brought a
more interactive experience on March 14
2023 open a released GPT 4 chat GPT
integrate the conversational history
with a user making a genuine dialogue
Microsoft impressed by the success of
new chgb interface an announced a
substantial investment in open Ai and
integrated a version of GPT into its
Bing search engine and the next one is
Bard Google bard Google was also an
earlier Fortuner in advancing
Transformer AI techniques for language
processing protein analysis and other
content types it made some of these
model open source for researchers but
were not made available through a public
interface in response to Microsoft
integration of GPT into being Google
hardly Lo a public facing chat Bo Nam
Google b b debut was met by an error
when the language model incorrectly
claimed that the web telescope was the
first to discover a planet in a foreign
solar system as a consequences Google
stock price suffered a significant
decline meanwhile Microsoft
implementation of chat GPT and GPT power
system also face criticism for producing
inaccurate result and displaying ER
actic behavior in their early irritation
so moving forward let's see what are the
use use cases of generative AI
generative AI has broad applicability
and can be employed across a wide range
of use cases to generate diverse form of
content recent advancement like GPT have
made this technology more accessible and
customizable for various application
some notable use cases for generative AI
are as follows chatbot implementation
generative AI can be utilized to develop
chatbots for customer service and
Technical Support enhancing interaction
with users and providing efficient
assistance the second one is language
dubbing announcement in the real in the
realm of movies and educational content
generative AI can contribute to
improving dubbing in different languages
ensuring accurate and high quality
translation and the third one is content
writing generative AI can assist in
writing email response dating profiles
resum√©s and term papers offering
valuable support and generating
customized content tailor to specific
requirement and the fourth one is Art
generation leveraging generative AI
artists can create photo realistic
artwork in various Styles enabling the
exploration of new artistic expression
and enhancing creativity the fifth one
is product demonstration videos
generative AI can hun to enhance product
demonstration video making them more
engaging visually appealing and
effective in showcasing product features
and benefits so generative AI
versatility allow it to employ it in
many other application making it a
valuable tool for Content creation and
enhancing user experience across diverse
domains so after seeing use cases of
generative AI let's see what are the
benefits of generative AI so generative
AI offers extensive application across
various business domains simplifying the
interpretation and comprehension of
existing content while also enabling the
automated creation of a new content
developers are actively exploring ways
to leverage generative AI in order to
enhance the optimize existing workflows
and even to reshape workflows entirely
to harness the potential of Technology
fully implementing generative AI can
bring numerous benefits incling
including automated content creation
generative AI can automate the manual
process of writing content saving time
and effort by generating text or other
form of content the next one is
efficient email response responding to
emails can be made more efficient with
generative AI reducing the effort
required and improving response time and
the third one is enhanced technical
support generative AI can improve
responses to specific technical queries
providing accurate and helpful
information to users or customers and
the fourth one is realistic person
Generation by leveraging generative AI
it becomes possible to create realistic
representation of people enabling
applications like virtual characters or
avatars and the fifth one is coherent
information summarization generative AI
can summarize complex information into a
coherent narrative distilling key points
and making it easier to understand and
communicate complex concept the
implementation of generative AI offers a
range of potential benefits steamingly
processed and enhancing content Creation
in various areas of business operation
so after seeing advantages of generative
AI let's move forward and see what are
the limitations of generative AI early
implementation of generative AI serve as
Vivid examples highlighting the numerous
limitation associated with this
technology several challenges arise from
the specific approaches employed to
implement various use gifts for instance
while a summary of a complex topic May
more reader friendly than explanation
incorporating multiple supporting
sources the ease of readability comes at
the expense of transpant identifying the
information sources so the first one is
when implementing or utilizing a
generative a application it is important
to consider the following limitation I
repeat the first one is lack of source
identification generative AI does not
always provide clear identification of
content Source making it difficult to
trace and verify origin of the
information the the second one is
assessment of bias assessing the bias of
original sources used generative AI can
be challenging as it may be difficult to
determine the underlying perspective or
agenda of the data utilized in the
training process the third one is
difficulty in identifying inaccurate
information generative AI can generate
realistic content making identifying
inaccuracy or false hoods within the
generated output harder and the fourth
one is adaptability to a new
circumstances understanding how to
fine-tune generative AI for a new
circumstances or specific context can be
complex requiring careful consideration
and expertise to achieve desired result
and the fifth one is glossing over bias
Prejudice and hatred generative AI
results May amplify or prep petate
biases prejudices or hateful content
present in the training data requiring
Vigilant scrutiny to prevent such issues
so awareness of these limit ation is
crucial when the implementing of
utilizing generative AI as it helps
users and developers critically evaluate
and mitigate potential risk and
challenges associated with the
technology so future of generative AI
furthermore advances in AI development
platforms will contribute to the
accelerated progress of research and
development in the realm of generative
AI the development will Encompass
various domains such as text images
videos 3D contact drugs Supply chains
logistic and business processes while
the current stand loan tools are
impressive the true transformative
impact generative AI will realize while
these capabilities are seemingly
integrated in the IIT into the existing
tools with regular use hello and welcome
to this video on edge AI for artificial
intelligence if you're interested in
learning more about how AI is
revolutionizing the world around us this
video is for you AI is a New Concept in
the world of AI that refers to devices
and sensors at the edge of the network
that can collect and process process
data in real time this allows for faster
more accurate decision making and
greater efficiency in a variety of
applications such as self-driving
Vehicles Smart Homes and health
monitoring so how is Aji different from
cloud computing and cloud computing AI
the main difference is the location
where the data is processed AI enables
devices at the edge of the network to
process data locally without having to
send it to the cloud this can lead to
faster more responsive systems and can
help you preserve bandwidth and reduce
latency in contrast cloud computing AI
processes data in the cloud where it can
assess vast amount of data and compute
resources but also requires a reliable
internet connection and can be slower as
Aji becomes more advanced it has the
potential to transform many Industries
such as healthare Transportation
manufacturing and a lot it can enable
new applications such as autonomous
vehicles remote surgery and real-time
monitoring of machine machines and
equipments while cloud computing AI has
many benefits Aji offers the advantages
of faster decision- making reduced
latency and improve efficiency so if
you're interested in learning more about
the future of AI Aji is definitely a
topic worth exploring on that note hello
everyone and welcome to Simply learn in
today's video we'll understand what Edge
Computing is and how it is different
from traditional cloud computing but
before we begin make sure to subscribe
to our Channel to stay updated with all
the latest Technologies and hit that
Bell icon to never miss any updates from
us so without any further delay let's
get started but wait if you want to
become an expert in h Computing then
look no further than Caltech
postgraduate program in a machine
learning offered by simply learn which
is an on-end program designed to provide
students with a comprehensive
understanding of AI and machine learning
Concepts tools and techniques this
program is created in collaboration with
CCH ctme and IBM and it is designed to
to provide Learners with a strong
foundation in Ai and machine learning
Concepts this program covers various
topics such as data science statistics
deep learning computer vision and NLP
and help you gain the right skill set on
various tools such as caras ma part Leb
tens oflow Jango and many more
furthermore the curriculum is structured
around interactive online classes live
sessions with industry experts and
Hands-On projects so why wait hurry up
enroll now in Caltech postgraduate
program in machine learning and create
your own successful career link is add
in the description box below so make
sure you check that out so without any
further Ado let's jump directly into our
today's topic so firstly let us
understand what is Edge Computing the
term Edge Computing for AI refers to the
process of running Ai computations and
processing close to the data source or
Edge devices at the edge of a network as
opposed to relying on centralized
cloud-based servers in order to enable
real-time data analysis pick a response
times and a decreased Reliance on cloud
infrastructure it deploys AI models and
algorithms directly on edge devices or
local
servers AI Edge Computing typically
entails the deployment of lightweight AI
models designed to use on edge devices
with constrained computational power
then these models are frequently
deployed to the edge after being trained
using cloud-based resources The Edge
devices gather data from their sensors
or from outside sources process the data
locally using the AI models that have
been deployed and give prompt responses
or processes based on the data but what
Le to the rise of H aai well with rapid
advancement in technology in every
sector and businesses are looking to
increase automation to increase workflow
productivity and security and for that
computer programs must be able to
identify patterns and Carry Out tasks
repeatedly and safely in order to assist
people now the range of task that human
perform however covers infer situations
that are impossible to fully describe in
quotes and rules because the world is is
very unstructured well with the help of
developments in haai machines and
gadgets can Now function with the
intelligence of human cognition whenever
they may be smart applications with AI
capabilities can learn to carry out the
same task under various conditions just
like in real life and here are the three
recent innovations that explain why
using AI models at the edge is on the
rise well first on the list we have
advancement of neural networks now over
the years as you know neural networks
and related AI infrastructure have
advaned and matured allowing for more
generalized machine learning
capabilities this means organizations
have to gain a deeper understanding on
how to train a models effectively and
deploy them in production at the
edge development in computational
infrastructure now running AI at the
edge requires substantial computational
power the field has witnessed notable
advancements in computational
infrastructure to support AI at the edge
particularly recent strikes in highly
parallel graphic processing units are
gpus and CPUs have enabled efficient
execution of neural networks gpus are
specifically designed to handle complex
parallel computation making them a
suitable choice of processing Ai
workloads and finally widespread usage
of iot devices now the widespread
adoption of Internet of think devices
has generated vast amounts of data these
devices such as industrial sensors smart
cameras and Robotics provide a rich
source of data that can be utilized for
AI analysis at the edge the availability
of diverse iot devices and their ability
to capture real-time data has
facilitated the deployment of AI models
at the edge but why deploy AI at the
edge what are the benefits of edge
Computing now Edge Computing for AI
addresses the limitations and challenges
associated with conventional Cloud
Centric AI methods so here are some of
the advantages which it offers firstly
decreased latency Now by performing AI
processing at the edge there is a
significant reduction in network latency
since data no longer needs to be
transmitted to the cloud and back this
is particularly beneficial for
applications that that demand real-time
responses or decision making let's say
like autonomous vehicles or Industrial
Automation secondly real-time data
analysis Edge Computing for AI enables
IM data analysis and inference directly
on the edge devices themselves this is
especially valuable for time sensitive
applications that require realtime
insights or predictions eliminating the
need for Reliance on cloud
connectivity enhanced privacy and
security well Edge Computing enhances
privacy and security by keeping data and
AI processing local sensitive
information can be processed on the edge
devices without being transmitted to the
cloud minimizing the risk of data
breaches and ensuring compliance with
data privacy regulations next we have
bandwidth optimization Edge Computing
reduces the volume of data uh that needs
to be transmitted to the cloud sending
only relevant or let's say summarized
data now this optimization minimizes
bandwidth requirements reduces Network
cost and even lightens the load on the
cloud infrastructure into total and
finally it offers offr functionality
Edge Computing enables AI models to
function even in situation which limited
or no internet connectivity this
capability is crucial for scenarios that
require continuous operation in remote
or disconnected environments such as
certain iot applications or field
deployments so these are some of the
reasons why we need to deploy AI at the
edge and the benefit it's offering now
comes the main question how exactly does
Edge Computing works well H Computing is
primarily focused on the proximity and
location of data processing and storage
the fundamental principle is simple when
it's not feasible to bring data to a
centralized data center the alternative
is to bring the data center close to the
data now in order to enable machines to
perform tasks like let's say visual
perception object detection driving cars
understanding speech speaking walking
and emulating other human skills they
need to replicate human intelligence in
a functional manner so for that that AI
uses a deep neural network as a data
structure to emulate human cognition
these dnns undergo training in data
centers and are exposed to numerous
examples of specific task or question
type along with their correct answer and
once the training is complete the model
advances to become a inference engine
capable of addressing real world
questions now from there on here's how
it typically Works firstly deployment of
inference engine now in a deployments
the inference engine which is the
component response for making
predictions or performing AI tasks is
deployed on local computers or devices
located in diverse environments like
factories hospitals cars satellites
homes and much
more problem identification and data
upload now while the inference engine
operates at the edge it may encounter
situation where it struggles to provide
accurate predictions or encounter
challenging scenarios in such cases the
problematic data associated with these
instances is commonly identified and
upload to a centralized Cloud structure
now from there on there is a continuous
cloud-based model training now the cloud
infrastructure typically in a data
center receives the problematic data
from Edge devices data scientists and AI
experts can then utilize this data to
refine and train the original AI models
further this involves using the Cloud's
computational resources larger data sets
and collaboration tools for model
training and Improvement now and finally
with the updated model deployed at the
edge after being retrained the AI system
continues to operate gaining new data
and insights from its environment the a
model can collect further additional
data which can be periodically uploaded
to the cloud and further training and
continuous Improvement this feedback
loops ensures that the edji models
become progressively smarter over time
so this is in a nutshell how Edge
Computing for AI Works hope you have
understood well let us talk some of the
use cases of edge Computing for AI in
real world well the combination of AI
iot and Edge Computing has opened up a
multitude of opportunities for Edge AI
revolutionizing the various aspects of
our daily lives so let's Del you into
some key areas where Edge AI is making a
significant impact first on the list we
have healthare haai is enhancing medical
Diagnostics and patient care for
instance by deploying AI algorithms on
medical imaging devices Radiologists can
accurately and efficiently identify and
analyze pathologies variable devices
with local Edge analytics enable
realtime monitoring of patient data
facilitating any early detection of
anomalies And Timely
interventions next autonomous vehicles
Edge air plays a pivotal role in
enabling real-time navigation and
decision making for autonomous vehicles
AI algorithms running on edge devices
within Vehicles process sensor data
analyze the environment and make spitse
second decisions to ensure safe and
efficient driving agriculture Aji is
being used to address agriculture
challenges as well such as pollination
autonomous drones equipped with a AI
capabilities can identify and pollinate
plants aiding in crop cultivation and
maximizing yelds this application
demonstrates the diverse potential of Ed
AI Beyond traditional domains next we
have smart home and cities now Edge AI
enhances the functionality and
efficiency of Smart Homes and cities
local processing and Analysis of data
from iot devices enable real-time
automation Energy Management security
surveillance and personalized user
experience AJ power system adapt to
individual preferences optimize
resources usage and respond to events in
real time next we have industrial
applications Ed AI is transforming
Industries thoroughly through realtime
monitoring predictive maintenance and
process optimization Edge devices
equipped with AI algorithms analyze
sensor data detect anomalies and predict
equipment failures reducing downtime and
improving
efficiency next we have security and
surveillance while a plays a crucial
role in security and surveillance which
empow realtime video analytics for
security and surveillance applications
Edge devices with AI capabilities can
analyze video feeds locally detect
suspicious activities like theft
identify objects or individuals of
interest and Trigger alerts or responses
enhancing security measures and reducing
response times and finally we have
personal devices hiai is increasingly
being integrated into personal devices
such as your smartphones laptops
variables and smart home assistants this
allows for on device AI processing
enabling faster and more personalized
experience without raying heavily on
cloud connectivity which can support
voice recognition facial recognition
among other features as well well as
more businesses and industries recognize
the advantage of Hai its adoption is
rapidly growing the ability to process
data locally enhance privacy reduce
latency and make real-time decision
empass organizations in various sectors
to enhance operations deliver better
services and improve user experience
experience so these are some of the
realtime applications of EDI and with
that we've come to the end of today's
session on what Ed Computing for AI is
in this video we are going to take you
through an interesting AI tool which can
help you in different ways the flare AI
we are going to discuss and this AI can
help you create your own advertisement I
mean if you own a product or if you want
to advertise something then this tool is
the go-to option for you and we are
going to see how to use this tool in
this particular video but before we
begin if you enjoy watching these videos
and if you find them interesting then
subscribe to Simply learns YouTube
channel because we bring the best videos
for you daily also hit the Bell icon to
never miss any updates from Simply learn
we all know that AI is expanding at a
very rapid Pace CH GPT is a great
example so in this time begging a job in
this field is super beneficial because
of course it offers great salary
packages and AI is interesting as we all
know so on that note we present to you
the simply learns postgraduate program
in Ai and machine learning in
collaboration with puru University boost
your career with this Ai and ml course
delivered in collaboration with pdu
University and IBM you can learn in
demand skills such as machine learning
deep learning NLP computer vision
reinforcement learning generative AI
prompt engineering chat GPT and many
more you'll receive a prestigious
certificate and ask me anything session
by IBM with five capstones in different
domain using real data sets you will
gain practical experience as well master
classes by pdu faculty and IBM experts
ensure top-notch education simply learns
job assist helps you get noticed by
Leading companies so this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision Gan caras
tensorflow and many more skills so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below so let's begin with Flare AI what
we have to do first is we have to go to
Google and just write here
flare AI fine so this is the first
option you'll get click on it flare AI
so this is the interface of flare AI the
AI design tool for Branded content so it
can help you with your brand definitely
you'll get better reach because what
usually happens is we use different
tools like Adobe Photoshop and other
tools to just show a brand item in
different ways but with this AI tool we
can use this and it's not time consuming
as well so let's start what we'll do is
we'll right here create with there so
here you can see we have a demo a demo
video basically so here you can see we
have a product a perfume bottle and here
we have different items or we can say
different options we can generate we can
customize and that's it just right here
click generate it will generate a
beautiful image for you so let's start
or if you want to watch the video here's
the video so here you can see this image
turned into that so we don't have to do
much of editing everything was done by
AI so let's see how we can do it so
we'll click on create with flayer now
continue with
Google so you will have to log in
although it's free to use this AI tool
is completely free so here we are we are
going to click on create new project so
here you can see we have a box for
placing our product here so let's say we
are placing something out of here fine
so here you can see we have a Starbucks
coffee let's place it in the middle so
generated image images will appear here
see here you can see generated images
will appear here it's written now click
on generate what you want to do so here
you can see we have different templates
fine we have this template we can use
this we can use any of them so let's
use this one fine
and click on this click on generate so
paper cup on a rock surrounded by trees
in front of Canyon and River fine so
here you can see it's still progressing
and now here you can see we have a
coffee mug present over here Starbucks
coffee if I zoom in for you a little bit
so here you can see how nice it's
looking it's not even looking like that
this cup is edited it's looking like
that this cup is placed there so in
another way if we want to do the same
thing what we have to do is we have to
use ad do Photoshop we have to use
different tools right but here you can
see how detailed it is we have shadows
for the cup as well not just that but if
you click here on generate and if you'll
find the mirror one you'll get to see
the reflection completely so if I click
here paper cup on a wet surface
surrounded by r brain in front of
Spotlight and strong Shadows so it's
pretty good we'll copy it as well we'll
need it later on I'll tell you why we
need while we need it so here you can
see the it's good right so you here you
can see
and here you can see we have a image the
paper cup what it said paper cup on a
wet surface the surface is wet
surrounded by rain so here you can see
rain in front of a spot light and strong
shadows in front of Spotlight and strong
Shadows so here you can see the shadow
of a paper cup fine similarly we can use
different tools we have different
elements as well like if you want to
use for example if you go on humans if
you want to use uh we have a hand thing
so here you can see if you want to put
it here and if you want to bring it to
the front so here you can see it's on a
hand so here you can see how we can use
this tool fine so we have a number of
options available here if I if we go to
ass sets so here you can see we have a
perfume bottle as well we'll delete this
one we'll use a perfume bottle it's from
shinel and now what we are going to do
is we are going to delete them as well
now what we are going to do is we are
going to generate something else fine so
now we are going to use the editor so
here you can see we have a product which
is a perfume bottle so we have to edit
the prompt in the form below fine the
product is a perfume bottle then we have
a placement where we want to place it on
a mirror or here you can see we have
different options so let's say we are
using wooden surface surrounded
by surrounded by let's say flowers fine
now create this image in your mind we
have a perfume bottle which is present
on a wooden surface its placement is on
a wooden surface and it is surrounded by
flowers fine now number of results how
many number of results you want I'll
make it extra strong extra strong color
strength and outline strength just to
show you guys how it looks now click on
generate and wait for a few seconds not
even minutes but it'll be done in few
seconds here you can see how fast this
AI is so this AI tool is very useful if
you have your own brand if you have your
own products not services but product so
here you can see it's present on a
wooden surface and we have flowers on it
fine so what we'll do is we'll write
here next
to Orange fine generate it's to it
totally depends on your creativity how
you want to create your image how you
want to present your brand service or
your BR brand product so here you can
see we are done with almost 85% we have
to wait for a few more seconds not even
a minute as I already mentioned and it
will be done so here you can see it's
present on
a wooden surface
and orange is not there basically what
we have to do is we have to go to
generate we have to use something else
then we'll
use Els fine let's see if it works or
not perfume bottle on a wooden surface
next to alements in front of Spotlight
and strong Shadow okay we are not going
to use
this what we are going to do is
nature in the background let's
see how it looks wait for few more
seconds and again it will give you an AI
generated image which will be present on
a wooden surface next to alements in
front of nature in the background now
here you can see we have a wooden
surface we have almonds present over
here and we have nature behind it wow it
looks it looks fine not that good so
what we'll do next is we'll write here
circular glass platform and we'll write
here next to a mug we are not going to
use jungle we are going to use orange
sunset in the background maybe it will
look better because sunsets are always
beautiful so let's see how it comes out
so here we
have the output so it's present on a
circular glass platform okay it's fine
we have a mug okay the mug is not
visible or maybe this is the mug
it's not that good and then we have the
sunset so Sunset is here here you can
see now the image looks real that's the
point it's not something that's that
looks like that it's edited or it's just
cropped from somewhere and put it over
there no it looks real fine right it has
the shadows and all the edges and
saturation it's changed as per the
background so right now the background
is sunset so here you can see the orange
lights here present over here fine so
that's how you can use this tool to
create an advertisement for your brand
product now the next thing is not just
that but it allows you to upload your
own pictures so it's quite interesting
let me just delete all of
them so what you have to do is you have
to download the picture of your product
upload it from from here and we have a
product of an iPhone so here I have this
picture okay so I'll skip this for now
because I'm going to show you if I write
here let's say it's an
iPhone done now here you can see we have
a product placed here so here you can
see we have a background right we have
blue and white this base as
well so the first thing we are going to
do is we are going to click on that we
are going to to click on edit now the
best part is we can remove this
background and it turns out to be so
good let's check it out so click on
remove background it will take few
seconds to remove all this from your
background and you'll notice something
that the background the edges of the
phone how nicely it has done it so here
you can see there's nothing wrong and
every Edge is perfectly fine right the
background is completely removed now now
what we have to do is we have to
generate a
image so we have an iPhone we want it to
be present at a circular glass platform
we
will golden sparkles in front
of let's remove it and see what else we
have desert in the background let's try
this generate it and it will again take
few
seconds so are you guys ready to check
out an iPhone present in the desert so
basically the idea of this AI tool is it
saves a lot of time for anyone right so
here you can see the iPhone is present
in a circular glass platform
right and we have what else we had okay
we had golden sparkles golden sparkles
are here
somewhere and we have a desert in the
background so desert here you can see we
have a
desert now what we'll do is we'll write
here
mirror and we want it to be present
in
water and
droplets fine so let's try it I haven't
tried it e either till now so it will
take some time the iPhone will be placed
on water with some droplets surrounding
it and the background will be okay a
mirror so it looks good right so here
you can see the shadow actually the
shadow of the phone how realistic it
looks so this is how you can use this AI
tool and not just that but you can
download your images so here you can see
click on download image and done the
image is downloaded and just click you
have from this to this and it totally
depends on your creativity how you want
to present your brand product so here we
wrap up EI specialist course if you like
this video consider subscribing to
Simply learn and hit the Bell icon to
never miss any updates from us if you
have any questions add it in the comment
section our experts will be happy to
help thanks for watching staying ahead
in your career requires continuous
learning and upscaling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs and Cutting
Edge domains including data science
cloud computing cyber security AI
machine machine learning or digital
marketing designed in collaboration with
leading universities and top
corporations and delivered by industry
experts choose any of our programs and
set yourself on the path to Career
Success click the link in the
description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here