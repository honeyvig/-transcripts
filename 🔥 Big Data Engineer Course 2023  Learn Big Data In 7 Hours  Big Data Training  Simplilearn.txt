foreign
of data Innovation and exploration the
advanced big data flow in this video
session we invite you to embark on a
transformative learning experience that
will equip you with Essential Knowledge
and expertise to navigate the vast
landscape of Big Data just as data has
become the compass guiding business to
success our course is designed to be
your Guiding Light in the data driven
era whether you're a seasoned
professional aiming to leverage or a
curious mind eager to delve into the
depths of data science this
comprehensive course is your ultimate
gateway to becoming a true master of Big
Data from understanding the fundamental
concepts to understanding the advanced
concepts we will Empower you with the
skills needed to transform raw data into
actionable intelligence first we will
learn
what is Big Data followed by that we
will go through Big Data tools and
Technologies next we will Master the
fundamentals of hdfs cluster
architecture then we will understand
what is Big what is Hive difference
between Hive and R dbms next we will
dive into the mapreduced up next we will
cover the Hadoop ecosystem following
that we will understand yarn tutorial
next we will discuss what is h base
followed by that we have big data
analytics and and finally we will cover
some important Hadoop interview
questions let's embark on this
transformative journey together but
before we begin take a look at simply
learns postgraduate program in data
engineering offered in collaboration
with per University and IBM this
comprehensive program is a gateway to
unmatched professional exposure align
with AWS and Azure certifications it
offers Hands-On learning enabling you to
master essential data engineering skills
elevate your career with this applied
Learning Journey enroll now and get
certified get ahead
amount of data
according to a wire article Google's you
mentioned that between the dawn of time
and 2003 humans had created 5 hexabytes
of data and by 2010 this much data was
being generated every two days and by
2021 it was happening every 40 minutes
in fact there is a saying that every
grain of sand on the planet has around
400 000 bytes of data
carrying these interesting facts let's
move forward to know more about Big Data
hi everyone I am sorry welcome to Simply
launch YouTube channel in this session
we will explore Big Data if you want
more amazing Tech related videos do like
subscribe and hit the Bell icon to
let's get to the video to learn more
about big data without any further delay
we will begin this session by discussing
the evolution of Big Data what data is
and its workflow
then we will have a look at its
applications
moving forward we will understand the
different types of big data and then the
five ways concept
after that we will understand the
benefits and challenges of your data
and then we will explore its various
career opportunities
and then next we will understand what
major tools are required in Big Data and
finally we will end the session by
discussing its future
let's look at the evolution of big data
in human history since the dawn of time
as we know humans have created large
amounts of data since evolution
one interesting fact is that according
to folks ninety percent of the words
data has been generated in the past
couple of years
furthermore the technological
advancement made in the last century are
more than those in made about 2000 years
now we know how big data has evolved
let's understand what big data is in a
similar manner
big data is a collection of huge amounts
of data that is continuously growing
with ever increasing speed
this data has a diverse quantity of
information
the word generates approx 4D Alpha bytes
of data in the form of text Music phone
calls emails Etc and could be collected
from anywhere like social media networks
and websites
Theta is also so massive that
traditional database management system
can't manage it so now we have a better
understanding of Big Data let's look at
how it works
in a way we can say the data under
standards more than we understand
ourselves the motive behind big data is
that the further you learn the more you
can acquire and make a decision or
discover an answer
but to understand this concept we need
to know how big data works
so the main steps are
collecting data
collecting data is a process of
gathering information from raw datum
that can be used to evaluate outcomes
and find answers to relevant questions
processing data
it includes converting The information
collected from raw data into valuable
and usable information
cleaning data it includes detecting and
removing corrupt and inaccurate data
from processed information
analyzing data it consists of various
methods to evaluate the data according
to the requirements now that we know how
the data Works let's look at some of its
applications
by examining vast amount of data Big
Data helps firms make better decisions
web server locks internal extreme data
social media content and activity report
text from customers emails phone call
details and machine data gathered by
numerous sensors
data Finds Its application in various
sectors like manufacturing Healthcare
education media and entertainment
Banking and securities
so let's understand all these big data
applications in detail in manufacturing
there are some main advantages of
adopting Big Data applications such as
monitoring product quality and flaws
planning the supply
forecasting of output raising energy
Effectiveness and support for
manufacturing Mass customer efficient
Health Care
big data analytics have enhanced health
care so researchers are analyzing the
data to find out which therapies are
more efficient for specific elements
quadrants and Drug side effects and
gather other crucial knowledge that can
benefit patients and lower cost coming
to education
big data is used by organization that
conduct online educational courses to
find candidates who are interested in
those courses
if someone searches for a YouTube
tutorial video on a certain topic an
organization offering online or offline
course on that topic will subsequently
send that individual an online
advertisement for their course
coming to media and entertainment
companies that offer media and
entertainment services such as Netflix
Amazon Prime and Spotify analyzed by
user data they acquired to determine the
business strategy information is
gathered and evaluated about the types
of videos and music consumers are
watching and listening to how long they
spend on a website Etc
coming to Banking and securities
dictator is used by the security
Exchange Commission to track the
activity in the financial markets
huge data is used by retail Traders big
Banks hedge funds and other big boys in
the financial markets this sector also
heavily relies on big data for risk
analytics including Enterprises risk
management and anti-money laundering
so these were some applications of big
data in various sectors
knowing about application we have come
through a lot now let's discuss its
types
there are three types of Big Data
structured data
it's an organized set of data that has
defined parameters it is arranged in a
table like format with rows and columns
so it can be worked on the days
for example it includes the date address
credit card numbers and slow notations
and structured data an unorganized set
of data that compromises nearly 80 to 90
percent of the total data this data is
created daily through social networks
and many other things and is not
transcribed or tag in organized manner
for example ml Ai iot and Analysis
coming to semi-structured data a
combination of structured and
unstructured data
it also includes organized text and data
not formatted in conventional ways
some example of semi-structured data
sources are emails XML and other markup
languages
talking about Big Data it's five weeks
concept always plays a major role in
understanding the data
let's have a look at each of its five
concept
understanding the five ways enables data
scientists to get more value out of
their data and helps the organization
where they work to become more
customer-centric coming to volume
the first of the five ways of big data
is volume a first to the quantity of
data available on the initial size of
the data is collected which can be
thought as the foundation of Big Data as
an example Netflix has over 86 million
subscriber worldwide and sales more than
125 million hours material thing which
is over 60 petal white in size
next is velocity velocity is the second
of the 5v's of big Theta
describes the rate at which Theta is
created and transferred in order for
business to make informed business
decision data has to flow quickly so
that it is accessible when needed
as an example Google receives about 63
000 queries per second variety variety
is the next week in the five weeks of
Big Data the diversity of the data is
referred to as variety data may be
gathered by an organization from a
variety of sources with different degree
of value
for example data could be in any format
like structured axle sheets or
semi-structured example tiles next is
velocity
the fourth of the five epic data vises
velocity
speaks of the reliability and Excellence
of the data the gathered information can
be incomplete or unable to offer any
useful information so in general it
refers to the degree of confidence in
the data that has been gathered for
example data collected in the medical
experiments like DNA analysis highly
volatile value is the fifth and final V
in the dictator this refers to the
benefit that big data can offer it is
necessary to be able to extract value
from Big Data because the value of Big
Data greatly depends on the inside stack
can be obtained from them
for example
the analytics used in the healthcare
provide us the information which is very
helpful for the society
nevertheless the advantage is that big
data provide are well worth the time
manifolds so let's talk about its
benefit
customer experience
it can be improved by using the
information Gadget from Big Data by
suggesting products that are more
individualized to the customers and
according to their choices
so the next benefit is decision making
by identifying patterns in Big Data it
can be tracked whether the solution is
solving the problem or not
cost saving organization can gain by
analyzing big data and have an increase
in their revenue and a cost reduction
so the next benefit is fraud detection
this benefits in Saving cost and
maximizing Revenue
offering a better customer experience
also speed up the fraud analysis
eliminating manual work identifying from
before it occurs and ensuring a high
fraud detection rate by removing false
positive
so now let's discuss the last benefit on
our list which is data security
big data security can be readily
addressed with the information resources
qualified labor a clear coping plan and
commitment to data integrity and privacy
business can achieve their ultimate
objective of utilizing data for better
customer experience and retention if
there are no risk to Big Data
as we know big data has some benefits
but it also has some challenges so let's
discuss that challenges in Big Data big
data has some problem in implementations
as they require quick attention and must
be dealt quickly as they can have a
negative outcome
so let's examine some big data
challenges lack of knowledge
professionals any company that deals
with big data challenges know how
difficult it is to find weekly the
experts so data handling tools have
advanced ability but most professionals
have not to close this Gap effective
measures are needed
the next challenge that we have is
integrating data
in a business information is gathered
from a variety of sources including
social networking Pages ARB software
customer logs Financial reports emails
PowerPoint presentation and employee
written reports
it could be difficult to organize
reports after combining all of this data
the next challenge on our list is Big
Data tool selection when choosing the
simplest tool for massive data analysis
and storage companies are often confused
with status storage technology is
simpler hbase or cash syndrome now let's
discuss our last challenge
securing data
one of the biggest issue of big data is
securing these enormous databases
companies generally put data security to
later phases because they are so busy
understanding storing and analyzing
their data sets
a stolen document on knowledge base
could cost companies Millions
so I hope I am able to make you
understand the benefits and challenges
of Big Data now it's time to get an
understanding of its various career
opportunities
those with a talent for understanding
and applying data for decision making
will be wise to pursue a profession and
dictator
this section provides a high level
overview of job opportunities and
fundamental skill needs in the Big Data
domain
the following are some titles and
responsibilities
the data engineer Big Data Engineers use
massive amount of data to produce
insights for corporate diseases
additionally they are response for
developing and managing the hardware and
software architecture of the company you
typically need two or three years of
expertise in the field to be hired for
the role of a data engineer next we have
a data architect
an IT expert who implements data storage
and management solution and plan future
databases using computer and design
talents recruiters prefer applicants who
are fluent and have excellent SQL and
XML skills
data scientist a person with a keen
knowledge of scientific methods and
algorithms to extract necessary insights
from noisy and under structured data the
position of data scientists often
requires a master degree next on the
list we have a data analyst
it can be defined as someone with the
skill to drag out useful information
required by the organization
our technical graduate degree and
knowledge of Microsoft Excel SharePoint
and SQL database are minimal
qualification let's talk about the last
position on our list that you can get in
the world of Big Data security engineer
an engineer with a job of managing and
controlling network security protocols
and protecting sensitive data
security breached protocols and computer
forensic knowledge are needed
data tools assess in the management of
large data sets and the identification
of patents
the main challenge of big data is
storing and processing the data at a
specified time span
the traditional approach is not
efficient in doing that
so Hadoop technology and various Big
Data tools have emerged to solve the
challenges faced in the Big Data
environment
so there are a lot of Big Data tools and
all of them help the user in some or
another way in Saving Time money and
uncovering business insights
these can be divided into the following
categories like data storage and
management for example the nosql
databases such as mongodb Cassandra
neo4j and hbase or popular nice nosql
databases
the talent Hadoop Microsoft HD insight
and zookeeper are popular for data
storage and management tools
next broad category is data cleaning
data needs to be cleaned up and well
structured examples of such tools which
help in defining and reshaping the data
into usable data sets are Microsoft
Excel and open refine
data mining is a process of discovery
insights within a database
some of the popular tools used for data
mining are teradata and rapidminer
data visualization tools are a useful
way of conveying the complex Data
Insights in a pictorial way that is easy
to understand
for example Tableau and IBM Watson
analytics and plotly are the common
tools
for data reporting power bi tools are
used data ingestion is the process of
getting the data into Hadoop forward
which can be done using scoop Flume or
storm
data analysis requires asking questions
and finding the answers in data
the popular tools used for data analysis
are Hive Pig mapreduce and Spark
data acquisition is also used for
acquiring the data for which scoop Flume
or storm tools are quite popular
the popular Big Data tools offer a lot
of advantages which can be summarized as
follows
they provide the analysts with Advanced
analytics algorithm and models they help
the user to run on Big Data platforms
such as Hadoop or any high performance
analytics systems
they help the user to work not only with
structured data but unstructured and
semi-structured data coming from
multiple sources
and it's quite easy to visualize the
analyze data in a form that helps in
conveying the complex Data Insights in a
pictorial way which is easy to
understand by users
Big Data tools help you to integrate
with other Technologies very easily so
let's we took a look at the the master
node and the you know the name node and
the secondary name node
look at the cluster architecture of our
Hadoop file system the hdfs cluster
architecture so we have our name node
and it stores some metadata and we have
our block location so we have our FS
image plus our edit log and then we have
the backup FS image and edit log and
then we have so you have our rack we
have our switch on top remember I was
talking about the switch that's the most
common thing to go in the rack is the
switches and underneath the rack you
have your different data nodes you have
your data node one two three four five
maybe you have 10 15 on this rack you
can stack them pretty high nowadays uh
used to be you only get about 10 servers
on there but now you see racks that
contain a lot more and then you have
multiple racks so we're not talking
about just one rack we also have you
know rack two rack three four five six
and so on until you have Rack in so if
you had a hundred data nodes we would be
looking at 10 racks of 10 data nodes
each and that is literally 10 commodity
server computers hardware and we have a
core switch which maintains the network
bandwidth and connects the name node to
the data nodes so just like each rack
has a switch that connects all your
nodes on the rack you now have core
switches that connect all the racks
together and these also Connect into our
name node setup so now I can look up
your FS image in your edit log and pull
that information your metadata out so
we've looked at the architecture from
the name node coming down and you have
your metadata at your Block locations
this then sorts it out you have your
core switches which connect everything
all your different racks and then each
individual rack has their own switches
which connect all the different nodes
and to the core switches so now let's
talk about the actual data blocks what's
actually sitting on those commodity
machines and so the Hadoop file system
splits massive files into small chunks
these chunks are known as data blocks
each file in the Hadoop file system is
stored as a data block and we have a
nice picture here where it looks like a
Lego if you ever played with the Legos
as a kid it's a good example we just
stack that data right on top of each
other but each block has to be the same
symmetry has to be the same size so that
it can track it easily and the default
size of one data block is usually 128
megabytes now you can go in and change
that this standard is pretty solid as
far as most data is concerned when we're
loading up huge amounts of data and
there's certainly reasons to change it
the 128 megabytes is pretty standard
block so why 128 megabytes if the block
size is smaller then there will be two
mini data blocks along with lots of
metadata which will create overhead so
that's why you don't really want to go
smaller on these data blocks unless you
have a very certain kind of data
similarly if the block size is very
large then the processing time for each
block increases then as I pointed out
earlier each block is the same size just
like your Lego blocks are all the same
but the last block can be the same size
or less so you might only be storing 100
megabytes in the last block and you can
think of this as if you had a terabyte
of data that you're storing on here it's
not going to be exactly divided into 128
megabytes we just store all of it 128
megabytes except for the last one which
could have anywhere between 1 and 128
megabytes depending on how evenly your
data is divided now let's look into how
files are stored in the Hadoop file
system so we have a file a text let's
see it's 520 megabytes we have block a
so we take 128 megabytes out of the 520
and we store it in block a and then we
have Block B again we're taking 128
megabytes out of our 520 storing it
there and so on Block C block D and then
block e we only have eight megabytes
left so when you add up 128 plus 128
plus 128 plus 128 you only get 512 and
so that last eight megabytes goes into
its own block the final block uses only
the remaining space for storage data
node failure and replication and this is
really where Hadoop shines this is what
makes it this is why you can use it with
commodity computers this is why you can
have multiple racks and have something
go down all the data blocks are stored
in various data notes you take each
block you store it at 128 megabytes and
then we're going to put it on different
nodes so here's our block a Block B
Block C from our last example and we
have node one node two node 3 node four
node 5 node six and so each one of these
represents a different computer it
literally splits the data up into
different machines so what happens if
node 5 crashes well that's a big deal I
mean we might not even have just node
five you might have a whole rack go down
and if you're a company that's building
your whole business off of that you're
going to lose a lot of money so what
does happen when node 5 crashes or the
first rack goes down the data stored in
node 5 will be unavailable as there's no
copy stored elsewhere in this particular
image so the Hadoop file system
overcomes the issue of data node failure
by creating copies of the data this is
known as the replication method and you
can see here we have our six nodes
here's our block a but instead of
storing it just on the first machine
we're actually going to store it on the
second and fourth notes so now it's
spread across three different computers
and in this if these are on a rack one
of these is always on a different rack
so you might have two copies on the same
rack but you never have all three on the
same rack and you always have you never
have more than two copies on one node
there's no reason to have more than one
copy per node and you can see we do the
same thing with Block B Block C is then
also spread out across the different
machines and same with block D and
blocky node 5 crashes will the data
blocks b d and e be lost well in this
example no because we have backups of
all three of those on different machines
the blocks have their copies and the
other nodes due to which the data is not
lost even if the node 5 crashes and
again because they're also stored on
different racks even if the whole rack
goes down you are still up and live with
your Hadoop file system the default
replication factor is three and total we
all have three copies of each data block
now that can be changed for different
reasons or purposes but you got to
remember when you're looking at a data
center this is all in one huge room
these switches are connecting all these
servers so they can Shuffle the data
back and forth really quick and that is
very important when you're dealing with
big data and you can see here each block
is by default replicated three times
that's the standard there is very rare
occasions to do four and there's even
fewer reasons to do two blocks I've only
seen four used once and it was because
they had two data centers and so each
data center kept two different copies
rack awareness in the Hadoop file system
rack is a collection of 30 to 40 data
nodes rack awareness is a concept that
helps to decide where the replica of the
data block should be stored so here we
have rack one we have our data node one
to four remember I was saying they used
to be you only put 10 machines and then
it went to 20 now it's 30 to 40. so you
can have a rack with 40 servers on it
then we have rack 2 and rack three and
we put block one on there replicas a
block a cannot be in the same rack and
so it'll put the replicas onto a
different rack and notice that these are
actually these two are on the same rack
but you'll never have all three stored
on the same Rack in case the whole rack
goes down and remember replicas of block
a are created in rack two and they
actually do they do by default create
the replicas onto the same rack and that
has to do with the data exchange and
maximizing your processing time and then
we have of course our Block B and it's
replicated onto rack three and Block C
which will then replicate onto rack one
and so on for all of your data all the
way up to block D or whatever how much
ever data you have on there hdfs
architecture so let's look at where the
architecture is a bigger picture we
looked at the name node and we store
some metadata names replicas home food
data three so it has all your different
info your metadata stored on there and
then we have our data nodes and you can
see our data nodes are each on different
racks with our different machines and we
have our name node and you're going to
see we have a heartbeat or pulse here
and a lot of times one of the things
that confuses people sometimes in
classes they talk about nodes versus
machines so you could have a data node
that's a Hadoop data node and you could
also have a spark node on Sparks a
different architecture and these are
each demons that are running on these
computers that's why you refer to them
as nodes and not just always as servers
and machines so even though I use them
interchangeable be aware that these
nodes you can even have virtual machines
if you're testing something out it
doesn't make sense to have 10 virtual
nodes on one machine and deploy it
because you might as well just run your
code on the machine and so we have our
heartbeat going on here and the
heartbeat is a signal that data nodes
continuously send to the name nodes this
signal shows the status of the data node
so there's a continual pulse going up
and saying hey I'm here I'm ready for
whatever instructions or data you want
to send me and you can see here we've
divided up into rack one and rack two
and our different data nodes and it also
have the replications we talked about
how to replicate data and replicates it
in three different locations and then we
have a client machine and the client
first requests the name node to read the
data now if you're not familiar with the
client machine the client is you the
programmer the client is you've logged
in external to this Hadoop file system
and you're sending it instructions and
so the client whatever instructions or
script you're sending is first request a
name node to read the data the name node
allows a client to read the requested
data from the data nodes the data is
read from the data nodes and sent to the
client and so you can see here that
basically the the name node connects the
client up and says here here's a data
stream and now you have the query that
you've sent out returning the data you
asked for and then of course it goes and
finishes it and says oh metadata
operations and it goes in there and
finalizes your request the other thing
the name node does is you're sending
your information because the client
sends the information in there is your
block operations so your block
operations are performs creation of data
so you're going to create new files and
folders you're going to delete the
folders and also it covers the
replication of the folders which goes on
in the background and so we can see here
that we have a nice full picture you can
see where the client machine comes in it
cues a metadata the metadata goes into
it stores the metadata and then it goes
into block operations and maybe you're
sending the data to the Hadoop file
system maybe you're querying maybe
you're asking it to delete if you're
sending data in there it then does the
replication on there it goes back so you
have your data client which is writing
the data into the data node and of
course replicating it and that's all
part of the block operations and so
let's talk a little bit about read
mechanisms in the Hadoop file system the
Hadoop file system read mechanism we
have our Hadoop file system client
that's you on your computer and the
client jvm on the client node and so we
have our client jvm or the Java virtual
machine that is going through and then
your client node and we're zooming in on
the read mechanism so we're looking at
this picture here as you can guess your
client is reading the data and I also
have another client down here writing
data we're going to look a little closer
at that and so we have our name node up
here we have our racks of your data
nodes and your racks of computers down
here and so our client the first thing
it does is it opens a connection up with
the distributed file system to the hdfs
and it goes hey can I get the Block
locations and so it goes by using the
RPC remote procedure call it gets those
locations and the name node first checks
if the client is authorized to access
the requested file and if yes it then
provides a block location and a token to
the client which is shown to the slave
for authentication so here's the name
node it tells the client hey here's the
token the client's going to come in and
get this information from you and it
tells the client oh hey here's where the
information is so this is what is
telling your script you sent to query
your data whether you're writing your
script in one of the many setups that
you have available through the Hadoop
file system or a connection through your
code and so you have your client machine
at this point then reads so your FS data
input stream comes through and you have
and you can see right here we did one
and two which is verify who you are and
give you all the information you need
then three you're going to read it from
the input stream and then the input
stream is going to grab it from the
different nodes where it's at and Supply
the tokens to those machines saying hey
this client needs this data here's a
token for that we're good to go let me
have the data the client will show the
authentication token to the data nodes
for the read process to begin so after
reaching the end of the data block the
connection is closed and we can see here
where we've gone through the different
steps get Block locations we have step
one you open up your connection you get
the Block locations by using the RPC
then you actively go through the fs data
input stream to grab all those different
data brings it back into the client and
then once it's done it closes down that
connection and then once the client or
in this case the programmer you know
manager's gone in and pig script you can
do that there's an actual coding in each
in Hadoop or pulling data called pig or
Hive once you get that data back we
close the connection delete all those
randomly huge series of tokens so they
can't be used anymore and then it's done
with that query and we'll go ahead and
zoom in just a little bit more here and
let's look at this even a little closer
here's our Hadoop file system client in
our client jvm our Java virtual machine
on the client node and we have the data
to be read block a Block B and so we
request to read block A and B and it
goes into the name node two then it
sends the location in this case IP
addresses of the blocks for the dn1 and
dn2 where those blocks are stored then
the client interacts with the data nodes
through the switches and so you have
here the core switch so your client node
comes in three and it goes to the core
switch and that then goes to rec switch
one rack switch 2 and rack switch 3. now
if you're looking at this you'll
automatically see a point of failure in
the core switch and certainly you want a
high-end switch mechanism for your core
switch you want to use Enterprise
hardware for that and then when you get
to the Rex that's all commodity all your
rack switches so if one of those goes
down you don't care as much you just
have to get in there and swap it in and
out really quick you can see here we
have block a which is replicated three
times and so is Block B and it'll pull
from there so we come in here and here
the data is read from the dn1 and dn2 as
they are the closest to each other and
so you can see here that it's not going
to read from two different racks it's
going to read from one rack whatever the
closest setup is for that query the
reason for this is if you have 10 other
queries going on you want this one to
pull all the data through one setup and
it minimizes that traffic so the
response from the data nodes to the
client that read the operation was
successful it says Ah we've read the
data we're successful which is always
good we like to be successful I don't
know about you I like to be successful
so if we look at the read mechanism
let's go ahead and zoom in and look at
the right mechanism for the Hadoop file
system sorry hdfs rate mechanism and so
when we have the hdfs write mechanism
here's our client machine this is again
the programmer on their in-computer and
it's going through the client Java
machine or Java virtual machine the jvm
this is all occurring on the client node
somebody's office or maybe it's on the
local server for the office so we have
our name node we have data nodes and we
have the distributed file system so the
client first executes create file on
distributed file system says hey I'm
going to create this file file over here
then it goes through the RPC call just
like our read did The Client First
executes crate file in the distributed
file system then the DFS interacts with
the name node to create a file name node
then provides a location to write the
data and so here we have our hdfs client
and the fs data output Stream So this
time instead of the data going to the
client it's coming from the client and
so here the client writes the data
through the fsdata output stream keep in
mind that this output stream the client
could be a streaming code it could be I
mean you know I always refer to the
client as just being this computer that
your programmer is writing on it could
be you have your SQL Server there where
your data is it's current with all your
current sales and it's archiving all
that information through scoop one of
the tools in the Hadoop file system it
could be streaming data it could be a
connection to the stock servers and
you're pulling stock data down from
those servers at a regular time and
that's all controlled you can actually
set that code up to be controlled in
many of the different different features
in the Hadoop file system and some of
the different resources you have that
sit on top of it so here the client
writes the data through the fs data
output stream and the fs data output
stream as you can see goes into right
packet so it divides it up into packets
128 megabytes and the data is written
and the slave further replicates it so
here's our data coming in and then if
you remember correctly that's part of
the fs data setup is it tells it where
to replicate it out but the data node
itself is like oh hey okay I've got the
data coming in and it's also given the
tokens of where to send the replications
to and then acknowledgment is sent after
the required replicas are made so then
that goes back up saying hey successful
I've written data made three
replications on this as far as our you
know going through the pipeline of the
data nodes and then that goes back and
says after the data is written the
client performs the close method so the
client's done and says okay I'm done
here's the end of the data we're
finished and after the data is written
the client performs a close method and
we can see here just a quick reshape we
go in there just like we did with the
Reed we create the connection this
creates a name node which lets it know
what's going on step two step three that
also includes the tokens and everything
and then we go into step three where
we're now writing through the fs data
output stream and that sorts it out into
whatever data node it's going to go to
and which also tells it how to replicate
it so that data node then sends it to
other data nodes so we have a
replication and of course you finalize
it and close everything up marks it
complete to the name node and it deletes
all those magical tokens in the
background so that they can't be reused
and we can go ahead and just do this
with an example the same setups and
whether you're doing a read or write
they're very similar as we come in here
from our client and our name node and
you can see right here we actually
depicting these as the actual rack and
the switches going on and so as the data
comes in you have your request like we
saw earlier it sends the location of the
data nodes and this actually turns your
your IP addresses your Dynamic node
connect they come back to your client
your hdfs client and at that point with
the tokens it then goes into the core
switch and the core switch says Hey here
it goes the client interacts with the
data nodes through the switches and you
can see here where you're writing in
block a replication of block a and a
second replication of block a on the
second server so blocking is replicated
on the second server and the third
server at this point you now have three
replications of block a and it comes
back and says it acknowledges and says
hey we're done and that goes back into
your Hadoop file system to the client it
says okay we're done and finally the
success written to the name node and it
just closes everything down a quick
recap of the Hadoop file system and the
advantages and so the first one is
probably one of the all of these are
huge one you have multiple data copies
are available so it's very fault
tolerant whole racks can go down
switches can go down even your main name
node could go down if you have a
secondary name node something we didn't
talk too much about out is how scalable
it is as it uses distributed storage you
run into there and you're oh my gosh I'm
out of space or I need to do some
heavier processing let me just add
another rack of computers so you can
scale it up very quickly it's a linear
scalability where it used to be if you
bought a server you would have to pay a
lot of money to get that the Craig
computer remember the big Craig
computers coming out the Craig computer
runs 2 million a year just the
maintenance to liquid cool it that's
very expensive compared to just adding
more racks of computer and extending
your data center so it's very cost
effective since commodity Hardware is
used we're talking cheap knockoff
computers you still need your high-end
Enterprise for the name node but the
rest of them literally it is a tenth of
the cost of storing data on more
traditional high-end computers and then
the data is secure so it has a very
high-end data security and provides data
security for your data hello Learners
simply and brings you a postgraduate
program in data engineering developed in
partnership with body universe City and
IBM to learn more about this course you
can find the course Link in the
description box below
Theory and no play doesn't make for my
much fun so let's go ahead and show you
what it looks like as far as some of the
dimensions when you're getting into
pulling data or putting data into the
Hadoop file system and you can see here
I have Oracle virtual machine virtual
box manager I have a couple different
things loaded on there Cloudera is one
of them so we should probably explain
some of these things if you're new to
Virtual machines the Oracle virtual box
allows you to spin up a machine as if it
is a separate computer so in this case
this is running I believe Centos a Linux
and it creates like a box on my computer
so the Centos is running on my machine
while I'm running my windows 10. it
happens to be a Windows 10 computer and
then underneath here I can actually go
under let me just open up the general
might be hard to see there and you can
see I can actually go down to system
processor I happen to be on an 8 core it
has 16 dedicated threads registers of 16
CPUs but eight cores and I've only
designated this for one CPU so it's only
going use one of my dedicated threads on
my computer and this the Oracle virtual
machine is open source you can see right
here we're on the Oracle
www.oracle.com I usually just do a
search for downloading virtualbox if you
search for virtualbox all one word it
will come up with this page and then you
can download it for whatever operating
system you're working with there
certainly are a number of different
options and let me go and point those
out if you're setting this up as a for
demoing for yourself the first thing to
note for doing a virtual box and doing a
Cloudera or Horton setup on that virtual
box for doing the Hadoop system to try
it out you need a minimum of 12
gigabytes it cannot be a Windows 10 home
edition because you'll have problems
with your virtual setup and sometimes
you have to go turn on the virtual
settings so it knows it's in there so if
you're on a home setup there are other
sources there's Cloudera and we'll talk
a little bit about Cloudera here in just
a second but they have the Cloudera
online live so we can go try the
Cloudera setup I've never used it but
Cloudera is a pretty good company
Cloudera and hortonworks are two of the
common ones out there and we'll actually
be running a Cloudera Hadoop cluster on
on our demo here so you have Oracle
virtual machine you also have the option
of doing it on different VM where that's
another one like virtual machine this is
more of a paid service there is a free
setup for just doing it for yourself
which will work fine for this and then
in the Cloudera like again they have the
new online setup where you can go in
there to the online and for Cloudera you
want to go underneath the Cloudera quick
start if you type in a search for
Cloudera quick start it'll bring you to
this website and then you can select
your platform in this case I did virtual
box there's VMware we just talked about
Docker Docker is a very high-end virtual
setup unless you already know it you
really don't want to mess with it then
your KVM is if you're on a Linux
computer that sets up multiple systems
on that computer so the two you really
want to use are usually the virtual box
or do an online setup and you can see
here with the download if you're going
into the Horton versions called
hortonworks and they call it sandbox so
you'll see the term hortonworks sandbox
and these are all test demos you're not
going to deploy a single node Hadoop
systems that would just be kind of
ridiculous and defeat the whole of
having a Horton or having a Hadoop
system if it's only installed on one
computer in a virtual node so a lot of
different options if you're not on a
professional Windows version or you
don't have at least 12 gigabytes Ram to
run this uh you'll want to try and see
if you can find an online version and of
course simply learn has our own Labs if
you sign up for our classes we set you
up I don't know what it is now but last
time I was in there was a five node
setup so you could get around and see
what's going on whether you're studying
for the admin side or for the
programming side in script writing and
if I go into my Oracle virtual box and I
go under my Cloudera and I start this up
and each one has their own flavors
Horton uses just to log in so you log
everything in through a local host
through your Internet Explorer or like
you use Chrome Cloudera actually opens
up a full interface you actually are in
that setup and you can see when I
started it let me go back here once I
downloaded this this is a big download
by the way if I had to import The
Appliance in virtualbox the first time I
ran it it takes a long time to configure
the setup and the second time it comes
up pretty quick and with the Cloudera
quick start again this is a pretend
single node it opens up and you'll see
that it actually has Firefox here so
here's my web browser I don't have to go
to a local host I'm actually already in
the quick start for Cloudera and if we
come down here you can see getting
started I have some information analyze
your data manage your cluster your
general information on there and what I
always want to start to do is to go
ahead and open up a terminal window so
I'll open up a terminal widen this a
little bit let me just maximize this out
here so you can see so we are now in a
virtual machine this virtual machine is
Centos Linux so I'm on a Linux computer
on my Windows computer and so when I'm
on this terminal window this is your
basic terminal if I do list you'll see
documents Eclipse these are the
different things that are installed with
the quick start guide on the Linux
system so this is a Linux computer and
then Hadoop is running on here so now I
have Hadoop single node so it has both
the name node and the data node and
everything squished together in one
virtual machine we can then do let's do
hdfs telling it that it's a Hadoop file
system DFS minus LS now notice the ls is
the same I have LS for list and LS for
list and I click on here it'll take you
just a second reading the Hadoop file
system and it comes up with nothing so a
quick recap let's go back over this
three different environments I have this
one out here let's just put this in a
bright red so you can actually see it I
have this environment out here which is
my slides I have this environment here
where I did a list that's looking at the
files on the Linux Centos computer and
then we have this system here which is
looking at the files on the Hadoop file
system so three completely separate
environments and then we connect them
and so right now we have I have whatever
files I have my personal files and the
course we're also looking at the screen
for my Windows 10 and then we're looking
at the screen here here's our list there
that's looking at the files and this is
the screen for Centos Linux and then
this is looking at the files right here
for the Hadoop file system so three
completely separate files this one here
which is the Linux is running in a
virtual box so this is a virtual box I'm
using one core to run it or one CPU and
everything in there is it has its own
file system you can see we have our
desktop and documents and whatever in
there and then you can see here we right
now have no files and our Hadoop file
system and this Hadoop file system
currently is stored on the Linux machine
but it could be stored across 10 Linux
machines 20 a hundred this could be
stored across in petabytes I mean it
could be really huge or it could just be
in this case just a demo where we're
putting it on just one computer and then
once we're in here let me just see real
quick if I can go under view zoom in
view zoom in this is just a standard
browser so I could use any like the
Control Plus and stuff like that to zoom
in and this is very common to be in a
browser window with the Hadoop file
system so right now I'm in a Linux and
I'm going to do oh let's just create a
file go file my new file and I'm going
to use buy and this is a VI editor just
a basic editor and we go ahead and type
something in here one two three four
maybe it's columns 44 66 77 of course I
do file system just like your regular
computer can also so in our value editor
you hit your colon I actually work with
a lot of different other editors and
we'll write quit VI so let's take a look
and see what happened here I'm in my
Linux system I type in LS for list and
we should see my new file and sure
enough we do over here there it is let
me just highlight that my new file and
if I then go into the Hadoop system hdfs
DFS minus LS for list we still show
nothing it's still empty so what I can
simply do is I can go hdfs DFS minus put
and then we're going to put my new file
and this is just going to move it from
the Linux system because I'm in this
file folder into the Hadoop file system
and now if we go in and we type in our
list for a new file system you will see
in here that I now have just the one
file on there which is my new file and
very similar to Linux we can do cat and
the cat command simply evokes reading
the file so hdfs DFS minus cat and I had
to look it up remember Cloudera the the
format is going to be a user then of
course the path location and the file
name and in here when we did the list
here's our list so you can see it lists
our file here and we realize that this
is under a user Cloud Dara and so I can
now go user Cloudera my new file and the
minus cat and won't be able to read the
file in here and you can see right here
this is the file that was in the Linux
system is now copied into the Cloudera
system and it's one three four five that
what I entered in there and if we go
back to the Linux and do list you'll
still see it in here my new file and we
can also do something like this in our
hdfs minus MV and we'll do my new file
and we're going to change it to my new
new file and if we do that underneath
our Hadoop file system the minus MV will
rename it so if I go back here to our
Hadoop file system LS you'll now see
instead of my new file it has my new new
file coming up and there it is my new
new files we've renamed it we can also
go in here and delete this so I can now
come in here so in our hdfs DFS we can
also do a remove and this will remove
the file and so if we come in here we
run this we'll see that when I come back
and do the list the file is gone and now
we just have another empty folder with
our Hadoop file system and just like any
file system we can take this and we can
go ahead and make directory create a new
directory so MK for make directory we'll
call this my dir so we're going to make
a directory reminder I'll take it just a
second and of course if we do the list
command you'll see that we now have the
directory in there give it just a second
there it comes myder and just like we
did before I can go in here and we're
going to put the file and if you
remember correctly from our files in the
setup I called it my new file so this is
coming from the Linux system and we're
going to put that into my dir that's the
Target in my Hadoop setup and so if I
hit enter on there I can now do the
Hadoop list and that's not going to show
the file because remember I put it in a
subfolder so if I do the quadrupt just
this will show my directory and I can do
list and then I can do my dur for my
directory and you'll see underneath the
my directory in the Hadoop file system
it now has my new file put in there and
with any good operating system we need a
minus help so just like you can type in
help in your Linux you can now come in
here and type in hdfs help and it shows
you a lot of the commands in there
underneath the Hadoop file system most
of them should be very similar to the
Linux on here and we can also do
something like this a Hadoop version and
the Hadoop version shows up that we're
in Hadoop 2.60
CDH is it where Cloudera 5 and compiled
by Jenkins and it has a date and all the
different information on our Hadoop file
system so this is some basics in the
terminal window let me go ahead and
close this out because if you're going
to play with this you should really come
in here let me just maximize the
Cloudera and it opens up in a browser
window and so once we're in here again
this is a browser window which you could
access might look like any access for a
Hadoop file system one of the fun things
to do when you're first starting is to
go under hue you'll see it up here at
the top as Cloudera Hue Hadoop near
hbase your Impala your spark these are
standard installs now and Hue is
basically an overview of the file system
and so come up here and you can see
where you can do queries as far as if
you have a hbase or a hive The Hive
database we can go over here to the top
where it says file browser and if we go
under file browser now this is the
Hadoop file system we're looking at and
once we open up the file browser you can
now see there's my directory which we
created and if I click on my directory
there's my new file which is in here and
if I click on my new file it actually
opens it up and you can see from our
Hadoop file system this is in the Hadoop
file system the file that we created so
we covered the terminal window you can
see here's a terminal window up here it
might be if you were in a web browser
it'll look a little different because it
actually opens up as a web browser
terminal window and we've looked a
little bit at Hue which is one of the
most basic components of Hadoop one of
the original components for going
through and looking at your data in your
databases of course now they're up to
the Hue four it's gone through a number
of changes and you can see there's a lot
of different choices in here for other
different tools in the Hadoop file
system and I'll go ahead and just close
out of this and one of the cool things
with the virtual uh box I can either
save the machine State send the shutdown
signal or power off the machine I'll go
and just power off the machine
completely now suppose you have a
library that has a collection of huge
number of books on each floor and you
want to count the total number of books
present on each floor what would be your
approach you would say I will do it
myself but then don't you think that
will take a lot of time and that's
obviously not an efficient way of
counting the number of books in this
huge collection on every floor by
yourself now there could be a different
approach or an alternative to that you
could think of asking three of your
friends or three of your colleagues and
you could then say if each friend could
count the books on every floor then
obviously that would make your work
faster and easier to count the books on
every floor now this is what we mean by
parallel processing So when you say
parallel processing in technical terms
you're talking about using multiple
machines and each machine would be
contributing its RAM and CPU cores for
processing and your data would be
processed on multiple machines at the
same time now this type of process
involves parallel processing in our case
or in our library example where you
would have person one who would be
taking care of books on floor 1 and
Counting them person two on floor 2 then
you have someone on floor 3 and someone
on floor four so every individual would
be counting the books on every floor in
parallel so that reduces the time
consumed for this activity and then
there should be some mechanism where all
these Counts from every floor can be
aggregated so what is each person doing
each person is mapping the data of a
particular floor or you can say each
person is doing a kind of activity or
basically a task on every floor and the
task is counting the books on every
floor now then you could have some
aggregation mechanism that could
basically reduce or summarize this total
count and in terms of map reduce we
would say that's the work of reducer so
when you talk about Hadoop mapreduce it
processes data on different node
machines now this is the whole concept
of Hadoop framework right that you not
only have your data stored across
machines but you would also want to
process the data locally so instead of
transferring the data from one machine
to other machine
into some central processing unit and
then processing it you would rather have
the data processed on the machines
wherever that is stored so we know in
case of Hadoop cluster we would have our
data stored on audible data notes on
their multiple disks and that is the
data which needs to be processed but the
requirement is that we want to process
this data as fast as possible and that
could be achieved by using parallel
processing now in case of mapreduce we
basically have the first phase which is
your mapping phase so in case of
mapreduce programming model you
basically have two phases one is mapping
and one is reducing now who takes care
of things in mapping phase it is a
mapper class and this mapper class has
the function which is provided by the
developer which takes care of these
individual map tasks which will work on
multiple nodes in parallel your reducer
class belongs to the reducing phase so a
reducing phase basically uses a reducer
class which provides a function that
will Aggregate and reduce the output of
different data nodes to generate the
final output now that's how your map
reduce Works using mapping and then
obviously reducing now you could have
some other kind of jobs which are map
only jobs wherein there is no reducing
required but we are not talking about
those we are talking about our
requirement where we would want to
process the data using mapping and
reducing especially when data is huge
when data is stored across multiple
machines and you would want to process
the data in parallel so when you talk
about mapreduce you could say it's a
programming model you could say
internally it's a processing engine of
Hadoop that allows you to process and
compute huge volumes of data and when we
say huge volumes of data we can talk
about terabytes we can talk about
petabytes exabytes and that amount of
data which needs to be processed on a
huge cluster we could also use mapreduce
programming model and run a mapreduce
algorithm in a local mode but what does
that mean if you would go for a local
mode it basically means it would do all
the mapping and reducing on the same
node using the processing capacity that
is RAM and CPU cores on the same machine
which is not really efficient in fact we
would want to have our map reduce work
on multiple nodes which would obviously
have mapping phase followed by a
reducing phase and intermittently there
would be data generated there would be
different other phases which help this
whole processing so when you talk about
Hadoop map reduce you are mainly talking
about two main components or two main
phases that is mapping and reducing
mapping taking care of map tasks
reducing taking care of reduced tasks so
you would have your data which would be
stored on multiple machines now when we
talk about data data could be in
different formats we could or the
developer could specify what is the
format which needs to be used understand
the data which is coming in that data
then goes through the mapping internally
there would be some shuffling sorting
and then reducing which gives you your
final output so the way we Access Data
from sdfs or the way our data is getting
stored on sdfs we have our input data
which would have one or multiple files
and one or multiple directories and your
final output is also stored on sdfs to
be accessed to be looked into and to see
if the processing was done correctly so
this is how it looks so you have the
input data which would then be worked
upon by multiple map tasks now how many
map tasks that basically depends on the
file that depends on the input format so
normally we know that in a Hadoop
cluster you would have a file which is
broken down into blocks depending on its
size so the default block size is 128 MB
which can then still be customized based
on your average size of data which is
getting stored on the cluster so if I
have really huge files which which are
getting stored on the cluster I would
certainly set a higher block size so
that my every file does not have huge
number of blocks creating a load on name
nodes Ram because that's tracking the
number of elements in your cluster or
number of objects in your cluster so
depending on your file size your file
would be split into multiple chunks and
for every Chunk we would have a map task
running now what is this map task doing
that is specified within the mapper
class so within the mapper class you
have the mapper function which basically
says what each of these map tasks has to
do on each of the chunks which has to be
processed this data intermittently is
written to stfs where it is sorted and
shuffled and then you have internal
phases such as partitioner which decides
how many reduce chance would be used or
what data goes to which you could also
have a combiner face which is like a
mini reducer doing the same reduce
operation operation 4 it reaches reduce
then you have your reducing phase which
is taken care by a reducer class and
internally the reducer function provided
by developers which would have reduced
task running on the data which comes as
an output from map task finally your
output is then generated which is stored
on hdfs now in case of Hadoop it accepts
data in different formats your data
could be in compressed format your data
could be in bar k your data could be in
Avro text CSV tsv binary format and all
of these formats are supported however
remember if you are talking about data
being compressed then you have to also
look into what kind of splitability the
compression mechanism supports otherwise
when mapreduce processing happens it
would take the complete file as one
chunk to be processed so sdfs accepts
input data in different formats this
data is stored in sdfs and that is
basically our input which is then
passing through the mapping phase now
what is mapping phase doing as I said it
reads record by record depending on the
input format it reads the data so we
have multiple map tasks running on
multiple chunks once this data is being
read this is broken down into individual
elements and when I say individual
element I could say this is my list of
key value pairs so your records based on
some kind of delimiter or without
delimiter are broken down into
individual elements and thus your math
creates key value pairs now these key
value pairs are not my final output
these these key value pairs are
basically a list of elements which will
then be subjected to further processing
so you would have internally shuffling
and sorting of data so that all the
relevant key value pairs are brought
together which basically benefits the
processing and then you have your
reducing which Aggregates the key value
pairs into set of smaller tuples or
tuples as you would say finally your
output is getting stored in the
designated directory as a list of
aggregated key value pairs which gives
you your output so when we talk about
mapreduce one of the key factors here is
the parallel processing which it can
offer so we know that we our data is
getting stored across multiple data
nodes and you would have huge volume of
data which is split and randomly
distributed across data nodes and this
is the data which needs to be processed
and the best way would be parallel
processing so you could have your data
on multiple data nodes or multiple slave
nodes in each slave node would have
again one or multiple disks to process
this data basically we have to go for
parallel processing approach we we have
to use the mapreduce now let's look at
the mapreduce workflow to understand how
it works so basically you have your
input data stored on sdfs now this is
the data which needs to be processed it
is stored in input files and the
processing which you want can be done on
one single file or it can be done on a
directory which has multiple files you
could also later have multiple outputs
merged which we achieve by using
something called as chaining of mappers
so here you have your data getting
stored on sdfs now input format is
basically something to define the input
specification and how the input files
will be split so there are various input
formats now we can search for that so we
can go to Google and we can basically
search for Hadoop map reduce Yahoo
tutorial this is one of the good links
and if I look into this link I can
search for different input formats and
output formats so let's search for input
format so when we talk about input
format you basically have something to
Define how input files are split so
input files are split up and read based
on what input format is specified so
this is a class that provides following
functionality it selects the files or
other objects that should be used for
input it defines the input split that
break a file into tasks provides a
factory for record reader objects that
read the file so there are different
formats if you look in the table here
and you can see that the text input
format is the default format which reads
lines of a text file and each line is
considered as a record here the key is
the byte offset of the line and the
value is the line content itself you can
have key value input format which parses
lines into key value pairs everything up
to the first tab character is the key
and the remainder is the line you could
also have sequence file input format
which basically works on binary format
so you have input format and in the same
way you can also also search for output
format which takes care of how the data
is handled after the processing is done
so the key value pairs provided to this
output collector are then returned to
Output files the way they are written is
governed by output format so it
functions pretty much like input format
as described in earlier right so we
could set what is the output format to
be followed and again you have text
output sequence file output format null
output format and so on so these are
different classes which take care of how
your data is handled when it is being
read for processing or how is the data
being written when the processing is
done so based on the input format the
file is broken down into splits and this
logically represents the data to be
processed by individual map tasks or you
could say individual mapper functions so
you could have one or multiple splits
which need to be processed depending on
the file size depending on what
properties have been set now once this
is done you have your input splits which
are subjected to mapping phase
internally you have a record reader
which communicates with the input split
and converts the data into key value
pairs suitable to be read by mapper and
what is mapper doing it is basically
working on these key value pairs the map
task giving you an intermittent output
which would then be forwarded for
further processing now once that is done
and we have these key value pairs which
is being worked upon my map your map
tasks as a part of your mapper function
are generating your key value pairs
which are your intermediate outputs to
be processed further now you could have
as I said a combiner phase or internally
a mini reducer phase Now combiner does
not have its own class so combiner
basically uses the same class as the
reducer class provided by the developer
and its main work is to do the reducing
or its main work is to do some kind of
mini aggregation on the key value pairs
which were generated by map so once the
data is coming in from the combiner then
we have internally a partitioner phase
which decides how outputs from combiners
are sent to the reducers or you could
also say that even if I did not have a
combiner partitioner would decide based
on the keys and values based on the type
of keys how many reducers would be
required
many reduced tasks would be required to
work on your output which was generated
by map task now once partitioner has
decided that then your data would be
then sorted and shuffled which is then
fed into the reducer so when you talk
about your reducer it would basically
have one or multiple reduced tasks now
that depends on what or what partitioner
decided or determined for your data to
be processed it can also depend on the
configuration properties which have been
set to decide how many radio Stars
should be used now internally all this
data is obviously going through sorting
and shuffling so that you are reducing
your aggregation becomes an easier task
once we have this done we basically have
the reducer which is the code for the
reducer is provided by the developer and
all the intermediate data has then to be
created to give you a final output which
would then be stored on sdfs and who
does this you have an internal record
writer which writes these output key
value pairs from reducer to the output
files now this is how your map reduce
Works wherein the final output data can
be not only stored but then read or
accessed from hdfs or even used as an
input for further mapreduce kind of
processing so this is how it overall
looks so you basically have your data
stored on sdfs based on input format you
have the splits then you have record
reader which gives your data to the
mapping phase which is then taken care
by your mapper function and mapper
function basically means one or multiple
map tasks working on your chunks of data
you could have
interface which is which additional
which is which is mandatory then you
have a partitioner phase which decides
on how many reduce tasks or how many
reducers would be used to work on your
data internally there is sorting and
shuffling of data happening and then
basically based on your output format
your record reader will write the output
to sdfs directory now internally you
could also remember that data is being
processed locally so would have the
output of each task which is being
worked upon stored locally however we do
not access the data directly from data
nodes we access it from sdfs so our
output is stored on sdfs so that is your
map reduce workflow when you talk about
mapreduce architecture now this is how
it would look so you would have
basically a edge node or a client
program or an API which intends to
process some data so it submits the job
to the job tracker or you can say
resource manager in case of Hadoop yarn
framework right now before this step we
can also say that an interaction with
name node would have already happened
which would have given information of
data nodes which have the relevant data
stored then your master processor so in
Hadoop version one we had job tracker
and then the slaves were called task
trackers in Hadoop version 2 instead of
job tracker you have resource manager
answer of task trackers you have node
managers so basically your resource
manager has to assign the job to the
task trackers or node managers so your
node managers as we discussed in yarn
are basically taking care of processing
which happens on every node so
internally there is all of this work
Happening by resource manager node
managers and application Master then you
can refer to the yarn based tutorial to
understand more about that so here your
processing Master is basically breaking
down the application into tasks what it
does internally is once your application
is submitted you
run on yarn processing framework is
handled by resource manager now forget
about the yarn part as of now I mean who
does the negotiating of resources who
allocates them how does the process
happen on the nodes right so that's all
to do with how yarn handles the
processing request so you have your data
which is stored in sdfs broken down into
one or multiple splits depending on the
input format which has been specified by
the developer your input splits are to
be worked upon by your one or multiple
map tasks which will be running within
the container on the nodes basically you
have the resources which are being
utilized so for each map task you would
have some amount of ram which will be
and then further the same data which has
to go through reducing phase that is
your reduced task will also be utilizing
some RAM and CPU cores now internally
you have these functions which take care
of deciding on number of reducers doing
a mini reduce and basically reading and
processing the data from multiple data
nodes now this is how your mapreduce
programming model makes parallel
processing work or processes your data
which is stored across multiple machines
finally you have your output which is
getting stored on sdfs
[Music]
so let's have a quick demo on mapreduce
and see how it works on a Hadoop cluster
now we have discussed briefly about
mapreduce which contains mainly two
phases that is your mapping phase and
your reducing phase and mapping phase is
taken care by your mapper function and
your reducing phase is taken care by
your reducer function now in between we
also have sorting and shuffling and then
you have other phases which is
partitioner and combiner and we will
discuss about all those in detail in
later sessions but let's have a quick
demo on how we can run a map reduce
which is already existing as a package
jar file within your Apache Hadoop
cluster or even in your cloud data
cluster now we can build our own
mapreduce programs we can package them
as jar transfer them to the cluster and
then run it on a Hadoop cluster on yarn
or we could be using already provided
default program so let's see where they
are now these are my two machines which
I have brought up and basically this
would have my Apache Hadoop cluster
running now we can just do a Simple
Start hyphen all dot SH now I know that
this script is deprecated and it says
instead you start DFS and start yarn but
then it will still take care of static
of my cluster on these two nodes where I
would have one single name node two data
nodes one secondary name node one
resource manager and two node managers
now if you have any doubt in how this
cluster came up you can always look at
the previous sessions where we had a
walkthrough in setting up a cluster on
Apache and then you could also have your
cluster running using Less Than 3 GB of
your total machine RAM and you could
have a Apache cluster running on your
machine now once this cluster comes up
we will also have a look at the web UI
which is available for name node and
resource manager now based on the
settings what we have given our UI eyes
will show us details of our cluster but
remember the UI is only to browse now
here my cluster has come up I can just
do a JPS to look at Java related
processes and that will show me what are
the processes which are running on C1
which is your data node resource manager
node manager and name node and on my M1
machine which is my second machine which
I have configured here I can always do a
jpeg s and that shows me the processes
running which also means that my cluster
is up with two data nodes with two node
managers and here I can have a look at
my web UI so I can just do a refresh and
the same thing with this one just do a
refresh so I had already opened the web
pages so you can always access the web
UI using your name nodes host name and
570 Port it tells me what is my cluster
ID what is my block pool ID it gives you
information of what is the space usage
how many live nodes you have and you can
even browse your file system so I have
put in a lot of data here I can click on
browse the file system and this
basically shows me multiple directories
and these directories have one or
multiple files which we will use for our
mapreduce example now if you see here
these are my directories which have some
sample files although these files are
very small like 8.7 kilobytes if you
look into this directory if you look
into this I have just pulled in some of
my Hadoop blocks and I have put it on my
sdfs these are a little bigger files and
then we also have some other data which
we can see here and this is data which I
have downloaded from web now we can
either run a map reduce on a single file
or in a directory which contains
multiple files let's look at that before
looking a demo in map reduce also
remember map reduce will create a output
directory and we need to have that
directory created plus we need to have
the permissions to run the mapreduce job
so by default since I'm running it using
admin ID I should not have any problem
but then if you intend to run mapreduce
with a different user then obviously you
will have to ask the admin or you will
have to give the user permission to read
and write from sdfs so this is the
directory which I've created which will
contain my output once the mapreduce job
finishes and this is my cluster file
system if you look on this UI this shows
me about my yarn which is available for
taking care of any processing it as of
now shows that I have total of a memory
and I have eight week course now that
can be depending on what configuration
we have set or how many nodes are
available we can look at nodes which are
available and that shows me I have two
node managers running each has 8 GB
memory and 8 V cores now that's not true
actually but then we have not set the
configurations for node managers and
that's why it takes the default
properties that is 8GB RAM and 8B cores
now this is my yarn UI we can also look
at scheduler which basically shows me
the different queues if they have been
configured where you will have to run
the jobs we'll discuss about all these
in later in detail now let's go back to
our terminal and let's see where we can
find some sample applications which we
can run on the cluster so once I go to
the terminal I can well submit the
mapreduce job from any terminal now here
I know that my Hadoop
directories here and within Hadoop you
have various directories we have
discussed that in binaries you have the
commands which you can run in s bin you
basically have the startup scripts and
here you also notice there is a share
directory in the end if you look in the
Shell directory you would find Hadoop
and within Hadoop you have various sub
directories in which we will look for
mapreduce now this mapreduce directory
has some sample jar files which we can
use to run a mapreduce on the cluster
similarly if you are working on a cloud
error cluster you would have to go into
opt Cloudera parcel CDH slash lib and in
that you would have directories for sdfs
mapreduce or sdfs yarn where you can
still find the same jars it is basically
a package which contains your multiple
applications now how do we run a map
reduce we can just type in Hadoop and
hit enter and that shows me that I have
an option called jar which can be used
to run a jar file now at any point of
time if you would want to see what are
the different classes which are
available in a particular
jar you could always do a jar minus xvf
for example I could say jar x v f and I
could say user local Hadoop share Hadoop
map reduce and then list down your jar
file so I'll say Hadoop map reduce
examples and if I do this this should
basically unpack it to show me what
classes are available within this
particular jar and it has done this it
has created a meta file and it has
created a org directory we can see that
by doing a LS and here if you look in LS
org since I ran the command frame your
phone directory I can look into org
patchy Hadoop examples which shows me
the classes which I have and those
classes contain which mapper or reducer
classes so it might not be just mapper
and reducer but you can always have a
look so for example I am targeting to
use word count program which does a word
count on files and gives me a list of
words and how many times they occur in a
particular file or in set of files and
this shows me that what are the classes
which belong to word count so we have a
in some reducer so this is my reducer
class I have tokenizer mapper that is my
mapper class right and basically this is
what is used these classes are used if
you run a word count now there are many
other programs which are part of this
jar file and we can expand and see that
so I can say Hadoop jar and give your
path so I'll say Hadoop jar user local
Hadoop share Hadoop map reduce Hadoop
map reduce examples and if I hit on
enter that will show me what are the
inbuilt classes which are already
available now these are certain things
which we can use now there are other jar
files also for example I can look at
dupe and here we can look at the jar
files which we have in this particular
path so this is one Hadoop mapreduce
examples which you can use you can
always look in other jar files like you
can look for Hadoop mapreduce client job
client and then you can look at the
tests one so that is also an interesting
one so you can always look into Hadoop
map reduce client job client and then
you have something ending with this so
if I would have tried this one using my
Hadoop jar command so in my previous
example when we did this it was showing
me all the classes which are available
and that already has a word count now
there are other good programs which you
can try like terrajen to generate dummy
data Terra saw to check your sorting
performance and so on and tell a
validate to validate the results
similarly we can also do a Hadoop jar as
I said on Hadoop mapreduce I think that
was client and then we have job client
and then test start now this has a lot
of other classes which can be used or
programs which can be used for doing a
stress testing or checking your cluster
status and so on one of them interesting
one is test dfsio but let's not get into
all the details in first instance let's
see how we can run a map reduce now if I
would want to run a map reduce I need to
give Hadoop jar and then my jar file and
if I hit on enter it would say it needs
the input and output it needs which
class you want to run so for example I
would say word count and again if I hit
on enter it tells me that you need to
give me some input and output to process
and obviously this processing will be
happening on cluster that is our yarn
processing framework unless and until
you would want to run this job in a
local mode so there is a possibility
that you can run the job in a local mode
but let's first try how it runs on the
cluster so how do we do that now here I
can do a hdfs LS slash command to see
what I have on my sdfs now through my UI
I was already showing you that we have
set of files and directories which we
can use to process now we can take up
one single file so for example if I pick
up new data and I can look into the
files here what we have and we can
basically run a map reduce on a single
file or multiple file so let's take this
file whatever that contains and I would
like to do a word count so that I get a
list of words and their occurrence in
this file so let me just copy this now I
also need my output to be written and
that will be written here so here if I
want to run a map reduce I can say
Hadoop which we can pull out from
history so Hadoop jar word count now I
need
my input so that will be new data and
then I will give this file which we just
copied now I am going to run the word
count only on a single file and I will
basically have my output which will be
stored in this directory the directory
which I have created already Mr output
so let's do this output and this is fair
enough now you can give many other
properties you can specify how many map
jobs you want to run how many reduced
jobs you want to run do you want your
output to be compressed do you want your
output to be merged or many other
properties can be defined when you are
specifying word count and then you can
pass in an argument to pass properties
from the command line which will affect
your output now once I go ahead and
submit this this is basically running a
simple inbuilt mapreduce job on our
Hadoop cluster now obviously internally
it will be looking for name node now we
have some issue here here and it says
the output already exists what does that
mean so it basically means that Hadoop
will create an output for you you just
need to give a name but then you don't
need to create it so let's give let's
append the output with number one and
then let's go ahead and run this so I
have submitted this command now this can
also be done in background if you would
want to run multiple jobs on your
cluster at the same time so it takes
total input paths to processes one so
that is there is only one split on which
your job has to work now it will
internally try to contact your resource
manager and basically this is done so
here we can have a look and we can see
some counters here now what I also see
is for some property which it is missing
it has run the job but it has run in a
local mode it has run in a local mode so
although we have submitted so this might
be related to my yarn settings and we
can check that so if I do a refresh my
and I have run my application it has
completed it would have created an
output but the only thing is it did not
interact with your yarn it did not
interact with your resource manager we
can check those properties and here if
we look into the job it basically tells
me that it went for mapping and reducing
it would have created an output it
worked on my file but then it ran in a
local mode it ran in a local mode so
mapreduce remember is a programming
model right now if you run it on yarn
you get the facilities of running it on
a cluster where yarn takes care of
resource management if you don't run it
on yarn and run it on a local mode it
will use your machine 's RAM and CPU
cores for processing but then we can
quickly look at the output and then we
can also try running this on yarn so if
I look into my hdfs and if I look into
my output Mr output that's the directory
which was not used actually let's look
into the other directory which is ending
with one and that should show me the
output created by this map reduce
although it ran in the local mode it
fetched an input file from your sdfs and
it would have created output in sdfs now
that's my part file which is created and
if you look at part minus r minus these
zeros if you would have more than one
reducer running then you would have
multiple such files created we can look
into this what does this file contain
which should have my word count and here
I can say cat which basically shows me
what is the output created by my map
reduce let's have a look into this so
the file which we gave for processing
has been broken down and now we have the
list of words which occur in the file
plus a count of those words so if there
is some word which is in is more then it
shows me the count so this is a list of
my words and the count for that so this
is how we run a sample mapreduce job I
will also show you how we can run it on
yeah now let's run mapreduce on yarn and
initially when we tried running a map
reduce it did not hit yarn but it ran in
a local mode and that was because there
was a property which had to be changed
in mapreduce hyphen site file so
basically if you look into this file the
error was that I had given a property
which says
mapred.framework.name and that was not
the right property name and it was
ignored and that's why it ran local mode
so I change the property to mapreduce
dot framework.name restarted my cluster
and everything should be fine now and
that mapred hyphen site file has also
been copied across the nodes now to run
a map reduce on a Hadoop cluster so that
it uses yarn and yarn takes care of
resource allocation on one or multiple
machines so I'm just changing the output
here and now I will submit this job
which should first connect to the
resource manager and if it connects to
the resource manager that means our job
will be run using yarn on the Clusters
rather than in a local mode so now we
have to wait for this application to
internally connect to resource manager
and once it starts there we can always
go back to the web UI and check if our
application has reached yarn so it shows
me that there is one input part to be
processed that's my job ID that's my
application ID which you can even
monitor status from the command line now
here the job has been submitted so let's
go back here and just do a Refresh on my
yarn UI which should show me the new
application which is submitted it tells
me that it is an accepted State
application master has already started
and if you click on this link it will
also give you more details of how many
map and reduced tasks would run so as of
now it says the application Master is
running it would be using the node which
is M1 we can always look into the logs
we can see then there is a one task
attempt which is being made and now if I
go back to my terminal I will see that
it is waiting to get some resources from
the cluster and once it gets the
resources it will first start with the
mapping phase where the mapper function
runs it does the map tasks one or
multiple depending on the splits so
right now we have one file and one split
so we will have just one map task
running and once the mapping phase
completes then it will get into reducing
which will finally give me my output so
we can be toggling through these
sessions so here I can just do a refresh
to see what is happening with my
application is it proceeding is it still
waiting for resource manager to allocate
some resources now just couple of
minutes back I tested this application
on yarn and we can see that my first
application completed successfully and
here we will have to give some time so
that that yarn can allocate the
resources now if the resources were used
by some other application they will have
to be freed up now internally yarn takes
care of all that which we will learn
more detail in yarn or you might have
already followed the yarn based session
now here we will have to just give it
some more time and let's see if my
application proceeds with the resources
what Yan can allocate to it sometimes
you can also see a slowness in what web
UI shows up and that can be related to
the amount of memory you have allocated
to your notes now for Apache we can have
less amount of memory and we can still
run the cluster and as I said the memory
which shows up here 16 GB and 16 cores
is not the true one those are the
default settings right but then my yarn
should be able to facilitate running of
this application let's just give it
couple of seconds and then let's look
into the output here again I had to make
some changes in the settings because our
application was not getting enough
resources and then basically I restarted
my cluster now let's submit the
application again to the cluster which
first should contact the resource
manager and then basically the map and
reduce process should start so here I
have submitted an application it is
connecting to the resource manager and
then basically it will start internally
an app master that is application Master
it is looking for the number of splits
which is one it's getting the
application ID and it basically then
starts running the job it also gives you
a tracking URL to look at the output and
now we should go back and look at our
yarn UI if our application shows up here
and we will have to give it a couple of
seconds when it can get the final status
change to running and that's where my
application will be getting resources
now if you closely notice here I have
allocated specific amount of memory that
is 1.5 GB for node manager on every node
and I have basically given two cores
each which my machines also have and my
yarn should be utilizing these resources
rather than going for default now the
application has started moving and we
can see the progress bar here which
basically will show what is happening
and if we go back to the terminal it
will show that first it went in deciding
map and reduce it goes for map once the
mapping phase completes then the
reducing phase will come into existence
and here my job has completed so now it
has basically used we can always look at
how many map and reduced SAS were run it
shows me that there was one map and one
reduced task now with the number of map
tasks depends on the number of splits
and we had just one file which is less
than 128 MB so that was one split to be
processed and reduced task is internally
decided by the reducer or depending on
what kind of property has been set in
Hadoop config files now it also tells me
how many input records were read which
basically means these were the number of
lines in the file it tells me output
records which gives me the number of
total words in the file now the there
might be duplicates and that which is
processed by internal combiner further
processing or forwarding that
information to reducer and basically
ready user works on 335 records gives us
a list of words and their account now if
I do a refresh here this would obviously
show my application is completed it says
succeeded you can always click on the
application to look for more information
it tells me where it ran now we do not
have a history server running as of now
otherwise we can always access more
information so this leads to history
server where all your applications are
stored but I can click on this attempt
tasks and this will basically show me
the history URL or you can always look
into the logs so this is how you can
submit a sample application which is
inbuilt which is available in the jar on
your Hadoop cluster and that will
utilize your cluster to run now you
could always as I said when you are
running a particular job remember to
change the output directory and if you
would not want it to be processing is a
single individual file you could also
point it to a directory that basic means
it will have multiple files and
depending on the file sizes there would
be multiple splits and according to that
multiple map tasks will be selected so
if I click on this this would submit my
second application to the cluster which
should first connect to resource manager
then resource manager has to start an
application Master now here we are
targeting 10 splits now you have to
sometimes give couple of seconds in your
machines so that the resources which
were used are internally already freed
up so that your cluster can pick it up
and then your yarn can take care of
resources so right now my application is
an undefined status but then as soon as
my yarn provides it the resources we
will have the application running on our
yarn cluster so it has already started
if you see it is going further then it
would launch 10 map tasks and it would
the number of reduced tasks would be
decided on either the way your data is
or based on the proper
have been set at your cluster level
let's just do a quick refresh here on my
yarn UI to show me the progress also
take care that when you are submitting
your application you need to have the
output directory mentioned however to
not create it Hadoop will create that
for you now this is how you run a map
reduce without specifying properties but
then you can specify more properties you
can look into what are the things which
can be changed for your mapper and
reducer or basically having a combiner
class which can do a mini reducing and
all those things can be done so we will
learn about that in the later sessions
now we will compare Hadoop version one
that is with mapreduce version one we
will understand and learn about the
limitations of Hadoop version 1 what is
the need of yarn what is yarn what kind
of workloads can be running on yarn what
are yarn components what is yarn
architecture and finally we will see a
demo on yeah take a look at simply
learns postgraduate program in data
engineering offered in collaboration
with per University and IBM elevate your
career with this applied Learning
Journey enroll now and get certified get
ahead so Hadoop person 1 or map reduce
version 1 well that's outdated now and
nobody is using Hadoop version 1 but it
would be good to understand what was in
Hadoop version 1 and what were the
limitations of Hadoop version 1 which
brought in the thought for the future
processing layer that is yarn now when
we talk about Hadoop we already know
that Hadoop is a framework and Hadoop
has two layers one is your storage layer
that is your hdfs Hadoop distributed
file system which allows for distributed
storage and processing which allows
fault tolerance by inbuilt replication
and which basically allows you to store
huge amount of data across multiple
commodity machines when we talk about
process we know that map reduce is the
oldest and the most mature processing
programming model which basically takes
care of your data processing on your
distributed file system so in Hadoop
version 1 mapreduce performed both data
processing and resource management and
that's how it was problematic in
mapreduce we had basically when we talk
about the processing layer we had the
master which was called job tracker and
then you had the slaves which were the
task records so your job tracker was
taking care of allocating resources it
was performing scheduling and even
monitoring the jobs it basically was
taking care of a signing map and reduced
tasks to the jobs running on task
records and task trackers which were
co-located with data nodes were
responsible for processing the jobs so
task trackers were the slaves for the
processing layer which reported their
progress to the job tracker so this is
what was happening in Hadoop version one
now when we talk about Hadoop version 1
we would have say client machines or an
API or an application which basically
submits the job to the master that is
job tracker now obviously we cannot
forget that there would already be an
involvement from name node which
basically tells which are the machines
or which are the data nodes where the
data is already stored now once the job
submission happens to the job tracker
job tracker being the master demon for
taking care of your processing request
and also resource management job
scheduling would then be interacting
with your multiple task trackers which
would be running on multiple machines so
each machine would have a task tracker
running and that task tracker which is a
processing slave would be co-located
with the data nodes now we know that in
case of Hadoop you have the concept of
moving the processing to wherever the
data is stored rather than moving the
data to the processing layer so we would
have task trackers which would be
running on multiple machines and these
Stars trackers would be responsible for
handling the tasks what are these tasks
these are the application which is
broken down into smaller tasks which
would work on the data which is
respectively stored on that particular
node now these were your slave domains
right so your job tracker was not only
tracking the resources so your task
trackers were sending heartbeats they
were sending in packets and information
to the job tracker which would then be
knowing how many resources and when we
talk about resources we are talking
about the CPU course we are talking
about the ram which would be available
on every node so task trackers would be
sending in their resource information to
job tracker and your job tracker would
be already aware of what amount of
resources are available on a particular
node how loaded a particular node is
what kind of work could be given to the
task tracker so job tracker was taking
care of resource management and it was
also breaking the application into tasks
and doing the job scheduling part assign
different tasks to these slave demons
that is your task trackers so job
tracker was eventually overburdened
right because it was managing jobs it
was tracking the resources from multiple
task trackers and basically it was
taking care of job scheduling so job
tracker would be overburdened and in a
case if job tracker would fail then it
would affect the overall processing so
if the master is skilled if the master
demand dies then the processing cannot
proceed now this was one of the
limitations of Hadoop person one so when
you talk about scalability that is the
capability to scale due to a single job
tracker scalability would be hitting a
bottleneck you cannot have a cluster
size of more than 4000 nodes and cannot
run more than 40 000 concurrent tasks
now that's just a number we could always
look into the individual resources which
each machine was having and then we can
come up with an appropriate number
however with a single job tracker there
was no horizontal scalability for the
processing layer because we had single
processing Master now when we talk about
availability job tracker as I mentioned
would be a single point of failure now
any failure kills all the queued and
running jobs and jobs would have to be
resubmitted now why would we want that
in a distributed platform in a cluster
which has hundreds and thousands of
machines we would want a processing
layer which can handle huge amount of
processing which could be more scalable
which could be more available and could
handle different kind of workloads when
it comes to resource utilization now if
you would have a predefined number of
map and reduce slots for each task
tracker you would have issues with which
would relate to resource utilization and
that again is putting a burden on the
master which is tracking these resources
which has to assign jobs which can run
on multiple machines in parallel so
limitations in running non mapreduce
applications now that was one more
limitation of Hadoop version 1 and
mapreduce that the only kind of
processing you could do is mapreduce and
mapreduce programming model although it
is good it is oldest it has matured over
a period of time but then it is very
rigid you will have to go for mapping
and reducing approach and that was the
only kind of processing which could be
done in Hadoop version one so when it
comes to doing a real-time analysis or
doing ad hoc query or doing a graph
based processing or massive parallel
processing there were limitations
because that could not be done in Hadoop
person 1 which was having mapreduce
version 1 as the processing component
now that brings us to the need for yarn
so yarn it stands for yet another
resource negotiator so as I mentioned
before yarn in Hadoop version one well
you could have applications which could
be written in different programming
languages but then the only kind of
processing which was possible was
mapreduce we had the storage layer we
had the processing but then kind of
limit processing which could be done now
this was one thing which brought in a
thought that why shouldn't we have a
processing layer which can handle
different kind of workloads as I
mentioned might be graph processing
might be real-time processing might be
massive parallel processing or any other
kind of processing which would be a
requirement of an organization now
designed to run mapreduce jobs only and
having issues in scalability resource
utilization job tracking Etc that led to
the need of something what we call as
yarn now from Hadoop version 2 onwards
we have the two main layers have changed
a little bit you have the storage layer
which is intact that is your sdfs and
then you have the processing layer which
is called ya yet another resource
negotiator now we will understand how
yarn works but then yarn is taking care
of your processing layer it does support
mapreduce So mapreduce processing can
still be done but then now you can have
a support to other processing Frameworks
yarn can be used to solve the issues
which Hadoop version 1 was posing
something like Resource Management
something like different kind of
workload processing something like
scalability resource utilization all
that is now taken care by yarn now when
we talk about yarn we can have now a
cluster size of more than 10 000 nodes
and can run more than 100 000 concurrent
tasks that's just to take care of your
scalability when you talk about
compatibility applications which were
developed for Hadoop version 1 which
were primarily mapreduce kind of
processing can run on yarn without any
disruption or availability issues when
you talk about resource utilization
there is a mechanism which takes care of
dynamic allocation of cluster resources
and this basically improves the resource
utilization when we talk about multi
tenancy so basically now the cluster can
handle different kind of workloads so
you can use open source and propriety
data access engines you can perform
real-time analysis you can be doing
graph processing you can be doing ad hoc
querying and this can be supported for
multiple workloads which can run in
parallel so this is what yarn offers so
what is Yan as I mentioned yarn stands
for yet another resource negotiator so
it is the cluster Resource Management
layer for your Apache Hadoop ecosystem
which takes care of scheduling jobs and
assigning resources now just imagine
when you would want to run a particular
application you would basically be
telling the cus cluster that I would
want resources to run my applications
that application might be a mapreduce
application that might be a hive query
which is triggering a mapreduce that
might be a pick script which is
triggering a mapreduce that could be
Hive with days as in execution Engine
That Could Be a spark application that
could be a graph processing application
in any of these cases you would still
you as in in sense client or basically
an API or the application would be
requesting for resources yarn would take
care of that so yarn would provide the
desired resources now when we talk about
resources we are mainly talking about
the network related resources we are
talking about the CPU cores or as in
terms of yarn we say virtual CPU course
we would talk about Ram that is in GB or
MB or in terabytes which would be
offered from multiple machines and Yan
would take care of this so with yarn you
could basically handle different
workloads now these are some of the
workloads which are showing up here you
have the traditional mapreduce which is
mainly batch oriented you could have an
interactive execution engine something
as days you would have H base which is a
column oriented or a four dimensional
database and that would be not only
storing data on sdfs but would also need
some kind of processing you could have
streaming functionalities which would be
from storm or Kafka or spark you could
have graph processing you could have
in-memory processing such as spark and
its components and you could have many
others so these are different Frameworks
which could now run and which can run on
top of yarn so how does yarn do that now
when we talk about yarn this is how a
overall yarn architecture looks so at
one end you have the client now client
could be basically your Edge node where
you have some applications which are
running it could be an API which would
want to interact with your cluster it
could be a user triggered application
which wants to run some jobs which are
doing some processing so this client
would submit a job request now what is
resource manager doing this Source
manager is the master of your processing
layer in Hadoop version 1 we basically
had job tracker and then we had task
trackers which were running on
individual nodes so your task trackers
were sending your heartbeats to the job
tracker your task trackers were sending
it their resource information and job
tracker was the one which was tracking
the resources and it was doing the job
scheduling and that's how as I mentioned
earlier job tracker was overburdened so
job tracker is now replaced by your
resource manager which is the master for
your processing layer your task trackers
are replaced by node managers which
would be then running on every node and
we have a temporary demon which you see
here in blue and that's your app master
so this is what we mentioned when we say
yet another resource negotiator so App
Master would be existing in a Hadoop
version and two now when we talk about
your resource manager resource manager
is the master for processing layer so it
would already be receiving heartbeats
and you can say resource information
from multiple node managers which would
be running on one or multiple machines
and these node managers are not only
updating their status but they are also
giving an information of the amount of
resources they have now when we talk
about resources we should understand
that if I'm talking about this node
manager then this has been allocated
some amount of RAM for processing and
some amount of CPU cores and that is
just a portion of what the complete node
has so if my node has say imagine my
node has around 100 GB RAM and I have
saved 60 cores all of that cannot be
allocated to node manager so node
manager is just one of the com
components of Hadoop ecosystem it is the
slave of the processing layer so we
could say keeping in all the aspects
such as different Services which are
running might be Cloud era or hot and
works related Services running system
processes running on a particular node
some portion of this would be assigned
to node manager for processing so we
could say for example say 60 GB Ram per
node and say 40 CPU cores so this is
what is allocated for the node manager
on every machine similarly we would have
here similarly we would have here so
node manager is constantly giving an
update to resource manager about the
resources what it has probably there
might be some other applications running
and node manager is already occupied so
it gives an update now we also have a
concept of containers which is basically
we will we will talk about which is
about these resources being broken down
into smaller parts so resource man
manager is keeping a track of the
resources which every node manager has
and it is also responsible for taking
care of the job request how do these
things happen now as we see here
resource manager at a higher level you
can always say this is the processing
Master which does everything but in
reality it is not the resource manager
which is doing it but it has internally
different services or components which
are helping it to do what it is supposed
to do now let's look further now as I
mentioned your resource manager has
these services or components which
basically helps it to do the things it
is basically a an architecture where
multiple components are working together
to achieve what yarn allows so resource
manager has mainly two components that
is your scheduler an applications
manager and these are at high level four
main components here so we talk about
resource manager which is the processing
Master you have node managers which are
the processing slaves which are running
on every nodes you have the concept of
container and you have the concept of
application Master how do all these
things work now let's look at yarn
components so resource manager basically
he has two main components you can say
which assist resource manager in doing
what it is capable of so you have
scheduler and applications manager now
there is when you talk about resources
there is always a requirement for the
applications which need to run on
cluster of resources so your application
which has to run which was submitted by
client needs resources and these
resources are coming in from multiple
machines wherever the relevant data is
stored and a node manager is running so
we always know that node manager is
co-located with data nodes now what does
the scheduler do so we have different
kind of schedulers here we have
basically a capacity scheduler we have a
fair scheduler or we could have a fee
for scheduler so there are different
schedulers which take care of resource
allocation so your scheduler is
responsible for allocating resources to
various applications now imagine a
particular environment where you have
different teams or different departments
which are working on the same cluster so
we would call the cluster as a
multi-tenant cluster and on the
multi-terrent cluster you would have
different applications which would want
to run simultaneously accessing the
resources of the cluster how is that
managed so there has to be some
component which has a concept of pooling
or queuing so that different departments
or different users can get dedicated
resources or can share resources from
the cluster so scheduler is responsible
for allocating resources to various
running applications now it does not
perform monitoring or tracking of the
status of applications that's not the
part of scheduler it does not offer any
guarantee about restarting the failed
tasks due to Hardware or network or any
other failures scheduler is mainly
responsible for allocating resources now
as I mentioned you could have different
kind of schedulers you could have a fee
for scheduler which was mainly in older
version of Hadoop which stands for first
in first out you could have a fair
scheduler which basically means multiple
applications could be running in the
cluster and they would have a fair share
of the resources you could have a
capacity scheduler which would basically
have dedicated or fixed amount of
resources across the cluster now
whichever scheduler is being used
scheduler is mainly responsible for
allocating resources then it's your
applications manager now this is
responsible for accepting job
submissions now as I said at higher
level we could always say resource
managers stay doing everything it is
allocating the resources it is
negotiating the resources it is also
taking care of listening to the clients
and taking care of job submissions but
who is doing it in real it is these
components so you have applications
manager which is responsible for
accepting job submissions it negotiates
the first container for executing the
application specific application master
it provides the service for restarting
the application Master now how does this
work how do these things happen in
coordination now as I said your node
manager is the slave process which would
be running on every machine slave is
tracking the resources what it has it is
tracking the processes it is taking care
of running the jobs and basically it is
tracking each container resource
utilization so let's understand what is
this container so normally when you talk
about a application request which comes
from a client so let's say this is my
client which is requesting or which is
coming up with an application which
needs to run on the cluster now this
application could be anything it first
contacts your master that's your
resource manager which is the master for
your processing layer now as I mentioned
and as we already know that your name
node which is the master of your your
cluster has the metadata in its Ram
which is aware of the data being split
into blocks the blocks will stored on
multiple machines and other information
so obviously there was a interaction
with the master which has given this
information of the relevant nodes where
the data exists now for the processing
need your client basically the
application which needs to run on the
cluster so your resource manager which
basically has the scheduler which takes
care of allocating resources and
resource manager has mainly these two
components which are helping it to do
its work now for a particular
application which might be needing data
from multiple machines now we know that
we would have multiple machines where we
would have node manager running we would
have a data node running and data nodes
are responsible for storing the data on
disk so your resource manager has to
negotiate shade the resources now when I
say negotiating the resources it could
basically ask each of these node
managers for some amount of resources
for example it would be saying can I
have one GB of RAM and one CPU core from
you because there is some data residing
on your machine and that needs to be
processed as part of my application can
I again have 1GB and one CPU core from
you and this is again because some
relevant data is stored and this request
which resource manager makes of holding
the resources of total resources which
the node manager has your resource
manager is negotiating or is asking for
resources from the processing slave so
this request of holding resources can be
considered as a container so resource
manager now we know it is not actually
the resource manager but it is the
application education manager which is
negotiating the resources so it
negotiates the resources which are
called containers so this request of
holding resource can be considered as a
container so basically a container can
be of different sizes we will talk about
that so resource manager negotiates the
resources with node manager now node
manager which is already giving an
update of the resources it has what
amount of resources it holds how much
busy it is can basically approve or deny
this request so node manager would
basically approve in saying yes I could
hold these resources I could give you
this container of this particular size
now once the container has been approved
or allocated or you can say granted by
your node manager resource manager now
knows that resources to process the
application are available and guaranteed
by the node manager so resource manager
starts a temporary demon called App
Master so this is a piece of code which
would also be running in one of the
containers it would be running in one of
the containers which would then take
care of execution of tasks in other
containers so your application Master is
per application so if I would have 10
different applications coming in from
the client then we would have 10 app
Masters one app Master being responsible
for per application now what does this
app Master do it basically is a piece of
code which is responsible for execution
of the application so your app Master
would run in one of the containers and
then it would use the other containers
which node manager is guaranteed that it
will give when the request application
request comes to it and using these
containers the App Master will run the
processing tasks Within These they
designated resources so it is mainly the
responsibility of application Master to
get the execution done and then
communicate it to the Masters so
resource manager is tracking the
resources it is negotiating the
resources and once the resources have
been negotiated it basically gives the
control to application Master
application Master is then running
within one of the containers on one of
the nodes and using the other containers
to take care of execution this is how it
looks so basically container as I said
is a collection of resources like CPU
memory your disk which would be used or
which already has the data and network
so your node manager is basically
looking into the request from
application master and it basically is
granting this request or basically is
allocating these containers now again we
could have different sizing of the
containers let's take an example here so
as I mentioned from the total resources
which are available for a particular
node some portion of resources are
allocated to the node manager so let's
imagine this is my node where node
manager as a processing slave is running
so from the total resources which the
node has some portion of RAM and CPU
cores is basically allocated to the node
manager so I could say out of total 100
GB Ram we can say around 60 cores which
the particular node has so this is my
Ram which the node has and these are the
CPU cores which the node has some
portion of it right so we can say might
be 70 or 60 percent of the total
resources so we could say around 60 GB
RAM and then we could say say around 40
week course have been allocated to node
manager so there are these settings
which are given in the yarn hyphen site
file now apart from this allocation that
is 60 GB RAM and 40v cores we also have
some properties which say what will be
the container sizes so for example we
could have a small container setting
which could say my every container could
have 2GB RAM and say one virtual CPU
core so this is my smallest container So
based on the total resources you could
calculate how many such small containers
could be running so if I say 2GB Ram
then I could have around 30 containers
but then I'm talking about one virtual
CPU core so totally I could have around
30 small containers which could be
running in parallel on a particular node
and as of that calculation you would say
10 CPU cores are not being utilized you
could have a bigger container size which
could say I would go for two CPU cores
and 3 GB Ram so 3gb RAM and two CPU
cores so that would give me around 20
containers of bigger size so this is the
container sizing which is again defined
in the yarn hyphen site file so what we
know is on a particular node which has
this kind of allocation either we could
have 30 small containers running or we
could have 20 big containers running and
same would apply to multiple nodes so
node manager based on the request from
application Master can allocate these
containers now remember it is within
this one of these containers you would
have an application Master running and
other containers could be used for your
processing requirement application
Master which is per application it is
the one which uses these resources in
basically manages or uses these
resources for individual application so
remember if we have 10 applications
running on yarn then it would be 10
application Masters one responsible for
each application your application Master
is the one which also interacts with the
scheduler to basically know how much
amount of resources could be allocated
for one application and your application
Master is the one which uses these
resources but it can never negotiate for
more resources to node Manager
application Masters cannot do that
application master has to always go back
to resource manager if it needs more
resources so it is always the resource
manager and internally resource manager
component that is application manager
which negotiates the resources at any
point of time due to some node failures
or due to any other requirements if
application Master needs more resources
on one or multiple nodes it will always
be contacting the resource manager
internally the applications manager for
more containers now this is how it looks
so your client submits the job request
to resource manager now we know that
resource manager internally has
scheduler an applications manager node
managers which are running on multiple
machines are the ones which are tracking
their resources giving this information
to the source manager so that resource
manager or I would say its component
applications manager could request
resources from multiple node managers
when I say request resources it is these
containers
so your resource manager basically will
request for the resources on one or
multiple nodes node manager is the one
which approves these containers and once
the container has been approved your
resource manager triggers a piece of
code that is application Master which
obviously needs some resources so it
would run in one of the containers and
will use other containers to do the
execution so your client submits an
application to resource manager resource
manager allocates a container or I would
say this is at a high level right
resource manager is negotiating the
resources and internally who is
negotiating the resources it is your
applications manager who is granting
this request it is node manager and
that's how we can say resource manager
locates a container application Master
basically contacts the related node
manager because it needs to use the
containers node manager is the one which
launches the container or basically
gives those resources within which an
application can run an application
Master itself will then accommodate
itself in one of the containers and then
use other containers for the processing
and it is within these containers the
actual execution happens now that could
be a map task that could be a reduced
task that could be a spark executor
taking care of smart tasks and many
other processing
all
so before we look into the demo on how
yarn works I would suggest looking into
one of the blogs from Cloudera so you
can just look for yarn untangling and
this is really a good blog which
basically talks about the overall
functionality which I explained just now
so as we mentioned here so you basically
have the master process you have the
worker process which basically takes
care of your processing your resource
manager being the master and node
manager being the slave this also talks
about the resources which each node
manager has it talks about the yarn
configuration file where you give all
these properties it basically shows you
node manager which reports the amount of
resources it has to resource manager now
remember if worker node shows 18 to 8
CPU cores and 128 GB RAM and if your
node manager says 64 V cores and RAM 128
GB then that's not the total capacity
city of your node it is some portion of
your node which is allocated to node
manager now once your node manager
reports that your resource manager is
requesting for containers based on the
application what is a container it is
basically a logical name given to a
combination of vcore and RAM it is
within this container where you would
have basically the process running so
once your application starts and once
node manager is guaranteed these
containers your application or your
resource manager has basically already
started an application Master within the
container and what does that application
Master do it uses the other containers
where the tasks would run so this is a
very good blog which you can refer to
and this also talks about mapreduce if
you have already followed the mapreduce
tutorials in past then you would know
about the different kind of tasks that
is map and reduce and these map and
reduced tasks could be running within
the container in one or multiple as said
it could be map task it could be reduced
task it could be a spark based task
which would be running within the
container now once the task finishes
basically that resources can be freed up
so the container is released and the
resources are given back to yarn so that
it can take care of further processing
if you'll further look in this blog you
can also look into the part 2 of it
where you talk mainly about
configuration settings you can always
look into this which talks about why and
how much resources are allocated to the
node manager it basically talks about
your operating system overhead it talks
about other services it talks about
Cloudera or hortonworks related Services
running and other processes which might
be running and based on that some
portion of RAM and CPU cores would be
allocated to node manager so that's how
it would be done in the yarn hyphen site
file and this basically shows you what
is the total amount of memory and CPU
course which is allocated to node
manager then within every machine where
you have a node manager running on every
machine in the yarn hyphen site file you
would have such properties which would
say what is the minimum container size
what is the maximum container size in
terms of ram what is the minimum for CPU
cores what is the maximum for CPU cores
and what is the incremental size in
where RAM and CPU cores can increment so
these are some of the properties which
Define how containers are allocated for
your application request so have a look
at this and this could be a good
information which talks about different
properties now you can look further
which talks about scheduling if you look
in this particular blog which also talks
about scheduling where it talks about
scheduling in yarn which talks about
Fair scheduler or you basically having
different cues in which allocations can
be done you also have different ways in
which queues can be managed and
different schedulers can be used so you
can all always look at this series of
Vlog you can also be checking for yarn
schedulers and then search for uh Hadoop
definitive guide and that could give you
some information on how it looks when
you look for Hadoop definitive guide so
if you look into this book which talks
about the different resources as I
mentioned so you could have a fee for
scheduler that is first in first out
which basically means if a long running
application is submitted to the cluster
all other small running applications
will have to wait there is no other way
but that would not be a preferred option
if you look in V4 scheduler if you look
for capacity scheduler which basically
means that you could have different
queues created and those queues would
have resources allocated so then you
could have a production queue where
production jobs are running in a
particular queue which has fixed amount
of resources allocated you could have a
develop queue where development jobs are
running and both of them are running in
parallel you could then also look into
Fair scheduler which basically means
again multiple applications could be
running on the cluster however they
would have a fair share so when I say
fair share in brief what it means is if
I had given 50 percent of resources to a
queue for production and 50 of resources
for a queue of development and if both
of them are running in parallel then
they would have access to 50 percent of
cluster resources however if one of the
queue is unutilized then second queue
can utilize all cluster resources so
look into the fair scheduling part it
also shows you about how allocations can
be given and you can learn more about
schedulers and how queues can be used
for managing multiple applications now
we will spend some time in looking into
few ways or few quick ways in
interacting with yarn in the form of a
demo to understand and learn on how yarn
works we can look into a particular
cluster now here we have a designated
cluster which can be used you could be
using the similar kind of commands on
your Apache based cluster or a Cloudera
quick start VM if you already have or if
you have a Cloudera or a hortonworks
cluster running there are different ways
in which we can interact with yarn and
we can look at the information one is
basically looking into the admin console
so if I would look into Cloud error
manager which is basically an admin
console for a cloudera's distribution of
Hadoop similarly you could have a
hortonworks cluster then access to the
admin console so if you have even read
access for your cluster and if you have
the admin console then you can search
for yarn as a service which is running
you can click on yarn as a service and
that gives you different tabs so you
have the instances which tells basically
what are the different roles for your
yarn service running so we have here
multiple node managers now some of them
show in stop status but that's nothing
to worry so we have three and six node
managers we have resource manager which
is one but then that can also be in a
high availability where you can have
active and standby you also have a job
history server which would show you the
applications once they have completed
now you can look at the yarn
configurations and as I was explaining
you can always look for the properties
which are related to the allocation so
you can here search for course and that
should show you the properties which
talk about the allocations so here if we
see we can be looking for yarn App
mapreduce application Masters resource
CPU course what is the CPU course
allocated for map reduce map tasks
reduce task you can be looking at yarn
node manager resource CPU course which
basically says every node manager on
every node would be allocated with six
CPU cores and the container sizing is
with minimum allocation of one CPU core
and the maximum could be two CPU cores
similarly you could also be searching
for memory allocation and here you could
then scroll down to see what kind of
memory allocation has been done for the
node manager so if we look further it
should give me information of node
manager which basically says here that
the container minimum allocation is 2GB
the maximum is 3 GB and we can look at
node manager which has been given 25 GB
per node so it's a combination of this
memory and CPU cores which is the total
amount of resources which have been
allocated to every node manager now we
can always look into applications tab
that would show us different
applications which are submitted on yarn
for example right now we see there is a
spark application running which is
basically a user who is using spark
shell which has triggered a application
on spark and that is running on yarn you
can look at different applications
workload information you can always do a
search based on the number of days how
many applications have run and so on you
can always go to the web UI and you can
be searching for the resource manager
web UI and if you have access to that it
will give you overall information of
your cluster so this basically says that
here we have 100 GB memory allocated so
that could be say 25 GB per node and if
we have four node managers running and
we have 24 cores which is 6 cores per
node if we look further here into nodes
I could get more information so this
tells me that I have four node managers
running and node managers basically have
25 GB memory allocated per node and 6
cores out of which some portion is being
utilized we can always look at the
scheduler here which can give us
information what kind of scheduler has
been allocated so we basically see that
there is just a root q and within root
you have default queue and you have
basically users queue based on different
users we can always scroll here and that
can give us information if it is a fair
share so here we see that my root dot
default has 50 of resources and the
other queue also has 50 of it so which
also gives me an idea that a fair
scheduler is being used we can always
confirm that if we are using a fair
scheduler or a capacity scheduler which
takes care of allocation so search for
schedule
and that should give you some
understanding of what kind of scheduler
is being used and what are the
allocations given for that particular
scheduler so here we have Fair scheduler
it shows me you have under root you have
the root queue which has been given 100
capacity and then you have within that
default which also takes hundred percent
so this is how you can understand about
yarn by looking into the yarn web UI you
can be looking into the configurations
you can look at applications you can
always look at different actions now
since we do not have admin access the
only information we have is to download
the client configuration we can always
look at the history server which can
give us information of all the
applications which have successfully
completed now this is from your yarn UI
what I can also do is I can be going
into Hue which is the web interface and
your web interface also basically allows
you to look into the jobs so you can
click on 1 Hue web UI and if you have
access to that it should show up or you
should have a way to get to your Hue
which is a graphical user interface
mainly comes with your Cloud era you can
also configure that with Apache
hortonworks has a different way of
giving you the web UI access you can
click and get into Hue and that is also
one way where you can look at yarn you
can look at the jobs which are running
if there are some issues with it and
these these are your web interfaces so
either you look from yarn web UI or here
in Hue you have something called as job
browser which can also give you
information of your different
applications which might have run so
here I can just remove this one which
would basically give me a list of all
the different kind of jobs or workflows
which were run so either it was a spark
based application or it was a map reduce
or it was coming from hive so here I
have list of all the applications and it
says this was a mapreduce this was a
spark something was killed something was
successful and this was basically a
probably a hive query which triggered a
mapreduce job you can click on the
application and that tells you how many
tasks were run for it so there was a map
task which ran for it you can get into
the metadata information which you can
obviously you can also look from the
yarn UI to look into your applications
which can give you a detailed
information of if it was a map reduce
how many map and reduced us were run
what were the different counters if it
was a spark application it can let you
follow through spark history server or
job history server so you can always use
the web UI to look into the jobs you can
be finding in a lot of useful
information here you can also be looking
at how many resources were used and what
happened to the job was it successful
did it fail and what was the jobs status
now apart from web UI which always you
might not have access to so in a
particular cluster in a production
cluster there might be restrictions and
the organization might not have access
given to all the users to graphical user
interface like you or might be you would
not have access to the cloud era manager
or admin console because probably
organization is managing multiple
clusters using this admin console so the
one way which you would have access is
your web console or basically your Edge
node or client machine from where you
can connect to the cluster and then you
can be working so let's login here and
now here we can give different commands
so this is the command line from where
you can have access to different details
you can always check by just typing in
mapred which gives you different options
where you can look at the mapreduce
related jobs you can look at different
queues if there are queues configured
you can look at the history server or
you can also be doing some admin stuff
provided you have access so for example
if I just say mapred and queue here this
basically gives me an option says what
would you want to do would you want to
list all the queues do you want
information on a particular queue so
let's try a list and that should give
you different queues which were being
used now here we know that per user a
queue dynamically gets created which is
under root dot users and that gives me
what is the status of the queue what is
the capacity has there been any kind of
maximum capacity or capping done so we
get to see a huge list of cues which
dynamically get configured in this
environment and then you also look at
your root dot default I could have also
picked up one particular queue and I
could have said show me the jobs so I
could do that now here we can also give
a yarn command so let me just clear the
screen and I will say yarn and that
shows me different options so apart from
your web interface something like web UI
apart from your Yarns web UI you could
also be looking for information using
yarn commands here so these are some
list of commands which we can check now
you can just type in yarn and version if
you would want to see the version which
basically gives you information of what
is the Hadoop version being used and
what is the vendor specific distribution
version so here we see we are working on
cloud errors distribution 5.14 which is
internally using Hadoop 2.6 now
similarly you can be doing a yarn
application list so if you give this
that could be an exhaustive list of all
the applications which are running or
applications which have completed so
here we don't see any applications
because right now probably there are no
applications which are running it also
shows you you could be pulling out
different status such as submitted
accepted or running now you could also
say I would want to see the services
that I've finished running so I could
say yarn application list and app States
as finished so here we could be using
our Command so I could say yarn
application list and then I would want
to see the app states which gives me the
applications which have finished and we
would want to list all the applications
which finished now that might be
applications which succeeded right and
there is a huge list of application
which is coming in from the history
server which is basically showing you
the huge list of applications which have
completed so this is one way and then
you could also be searching for one
particular application if you would want
to search a particular application if
you have the application ID you could
always be doing a grip that's a simple
way I could say basically let's pick up
this one and if I would want to search
for this if I would want more details on
this I could obviously do that by
calling in my previous command and you
could do a grip if that's what you want
to do and if you would want to search is
there any application which is in the
list of my applications that shows my
application I could pull out more
information about my application so I
could look at the log files for a
particular application by giving the
application ID so I could say yarn logs
now that's an option and every time
anytime you have a doubt just hit enter
it will always give you options what you
need to give with a particular command
so I can say yarn log
application ID now we copied an
application ID and we could just give it
here we could give other options like
app owner or if you would want to get
into the Container details or if you
would want to check on a particular node
now here I'm giving yarn logs and then
I'm pointing it to an application ID and
it says the log aggregation has not
completed might be this was might be
this was an application which was
triggered based on a particular
interactive cell or based on a
particular query so there is no log
existing for this particular application
you can always look at the status of an
application you can kill an application
so here you can be saying yarn yarn
application and then what would you want
to do with an application hit and enter
it shows you the different options so we
just tried app States you could always
look at the last one which says status
and then for my status I could be giving
my application ID so that tells me what
is the status of this application it
connects to the resource manager it
tells me what's the application ID what
kind of application it was who ran it
which was the queue where the job was
running what was the start and end time
what is the progress the status of it if
it is finished or if it has succeeded
and then it basically gives me also an
information of where the application
master was running it gives me the
information where you can find this job
details in history server if you are
interested in looking into it also gives
you a aggregate resource allocation
which tells how much GB memory and how
many core seconds it used so this is
basically looking out at the application
details now I could kill an application
if the application was already running I
could always do a yarn application minus
skill and then I could be giving my
application now I could try killing this
however it would say the application is
already finished if I had an application
running and if my application was
already given an application ID by The
Source manager I Could Just Kill it I
can also say yarn node list which would
give me a list of the node managers now
this is what we were looking from the
yarn web UI and we were pulling out the
information so we can get this and kind
of information from your command line
always remember and always try to be
well accustomed with the command line so
you can do various things from the
command line and then obviously you have
the web uis which can help you with a
graphical interface easily able to
access things now you could be also
starting the resource manager which we
would not be doing here because we are
already running in a cluster so you
could give a yarn resource manager you
could get the logs of resource manager
if you would want by giving yarn demin
so we can try that so you can say yarn
and then demon so it says it does not
find the demon so so you can give
something like this get level and here I
will have to give the node and the IP
address where you want to check the logs
of resource manager so you could be
giving this for which we will have to
then get into Cloudera manager to look
into the nodes and the IP address you
could be giving a command something like
this which basically gives you the level
of the log which you have and I got this
resource manager address from the web UI
now I can be giving in this command to
look into the demand log and it
basically says you would want to look at
the resource manager related log and you
have the log 4J which is being used for
logging the kind of level which has been
set as info which can again be changed
in the way you're logging the
information now you can try any other
commands also from yarn for example
looking at the yarn RM admin so you can
always do a yarn RM admin and this
basically gives you a lot of other
informations like refreshing the cues or
refreshing the nodes or basically
looking at the admin ACLS or getting
groups so you could always get group
names for a particular user now we could
search for a particular user such as
yarn or hdfs itself so I could just say
here I would want get groups and then I
could be searching for say username hdfs
so that tells me sdfs belongs to a
Hadoop group similarly you could search
for say mapred or you could search for
yarn so these are service related users
which automatically get created and you
can pull out information related to
these you can always do a refresh nodes
kind of command and that is mainly done
internally this can be useful when you
are doing commissioning decommissioning
but then in case of Cloudera or
hortonworks kind of cluster you would
not be manually giving this command
because if you are doing a commissioning
decommissioning from an admin console
and if you are an administrator then you
could just restart the services which
are affected and that will take care of
this but if you were working in an
Apache cluster and if you were doing
commissioning decommissioning then you
would be using in two commands refresh
notes and base basically that's for
refreshing the nodes which should not be
used for processing and similarly you
could have a command refresh notes which
comes with stfs so these are different
options which you can use with your yarn
on the command line you could also be
using curl commands to get more
information about your cluster by giving
curl minus X and then basically pointing
out to your resource manager web UI
address now here I would like to print
out the cluster related metrics and I
could just simply do this which
basically gives me a high level
information of how many applications
were submitted how many are pending what
is the reserved resources what is the
available amount of memory or CPU cores
and all the information similarly you
can be using the same curl commands to
get more information like scheduler
information so you would just replace
the metrics with scheduler and you could
get the information of the different
queues now that's a huge list we can
cancel this and that would give me the
list of all the queues which are
allocated and what are the resources
allocated for each queue you could also
get cluster information on application
IDs and Status running of applications
running in yarn so you would have to
replace the last bit of it and you would
say I would want to look at the
applications and that gives me a huge
list of applications then you can do a
grip and you can be filtering out
specific application related information
similarly you can be looking at the
notes so you can always be looking at
node specific information which gives
you how many nodes you have but this
could be mainly used when you have an
application which wants to or a web
application which wants to use a curl
command and would want to get
information about your cluster from an
HTTP interface now when it comes to app
application we can basically try running
a simple or a sample mapreduce job which
could then be triggered on yarn and it
would use the resources now I can look
at my application here and I can be
looking into my specific directory which
is this one which should have a lot of
files and directories which we have here
now I could pick up one of these and I
could be using a simple example to do
some processing let's take up this file
so there is a file and I could run a
simple word count or I could be running
a hive query which triggers a mapreduce
job I could even run a spark application
which would then show that the
application is running on the cluster so
for example if I would say spark to
Shell now I know that this is an
interactive way of working with spark
but this internally triggers a spark
submit and this runs an application so
here when you do a spark 2 Shell by
default it will contact yarn so it gets
an application ID it is running on yarn
with the master being yarn and now I
have access to the interactive way of
working with spark now if I go and look
into applications I should be able to
see my application which has been
started here and it shows up here so
this is my application
3827 which has been started on yarn and
as of now we can also look into the yarn
UI and that shows me the application
which has been started which basically
has one running container which has one
CPU core allocated 2GB RAM and it's in
progress although we are not doing
anything there so we can always look at
our applications from the yarn UI or as
I mentioned from your applications tab
within yarn Services which gives us the
information and you can even click on
this application to follow and see more
information but you should be given
access to that now this is just a simple
application which I triggered using
spark shell similarly I can basically be
running a map reduce now to run a map
reduce I can say Hadoop jar and that
basically needs a class so we can look
for the default path which is opt
Cloudera Parcels CDH lib Hadoop map
reduce Hadoop map reduce examples and
then we can look at this particular jar
file and if I hit on enter it shows me
the different classes which are part of
this jar and here I would like to use
word count so I could just give this I
could say word count now remember I
could run the job in a particular queue
by giving in an argument here so I could
say minus D mapred dot job dot Q dot
name and then I can point my job to a
particular queue I can even give
different arguments in saying I would
want my mapreduce output to be
compressed or I want it to be showed in
a particular directory and so on so here
I have the word count and then basically
what I can be doing is I can be pointing
it to a particular input path and then I
can have my output which can be getting
stored here again a directory which we
need to choose and I will say output new
and I can submit my job now once I have
submitted my job it connects to resource
manager it basically Gets a Job ID it
gets an application ID it shows you from
where you can track your application you
can always go to the yarn UI and you can
be looking at your application and the
resources it is using so my application
was not a big one and it has already
completed it triggered one map task it
launched one reduce task it was working
on around 12 466 records where you have
then the output of map which is these
many number of output records which was
then taken by combiner and finally by a
reducer which basically gives you the
output so this is my yarn application
which has completed now I could be
looking into the yarn UI and if my job
has completed you might not see your
application here so as of now it shows
up here the word count which I ran it
also shows me my previous Park shell job
it shows me my application is completed
and if you would want further
information on this you can click and go
to the history server if you have been
given access to it or directly go to the
history server web UI where your
application shows up it shows how many
map and reduce tasks it was running you
can click on this particular application
which basically gives you information of
your map and reduce tasks you can look
at different counters for your
application right you can always look at
map specific tasks you can always look
into one particular task what it did on
which node it was running or you can
below looking at the complete
application log so you can always click
on the logs and here you have click here
for full log which gives you the
information and you can always look for
your application which can give you
information of App Master being launched
or you could have search for the word
container so you could see a job which
needs one or multiple containers and
then you could say container is being
requested then you could see container
is being allocated then you can see what
is the container size and then basically
your task moves from initializing to
running in the container and finally you
can even search for release which will
tell you that the container was released
so you can always look into the log for
more information so this is how you can
interact with yarn this is how you can
interact with your command line to look
for more information or using your yarn
web UI or you can also be looking into
your Hue for more information welcome to
scoop tutorial one of the many features
of the Hadoop ecosystem for the Hadoop
file system what's in it for you today
we're going to cover the need for scoop
what is scoop scoop features scoop
architecture scoop import scoop export
scoop processing and then finally we'll
have a little Hands-On demo on scoop so
you can see what it looks like so where
does the need for scoop come in in our
big data Hadoop file system processing
huge volumes of data requires loading
data from diverse sources into Hadoop
cluster you can see here we have our
data processing and this process of
loading data from the heterogeneous
sources comes with a set of challenges
so what are the challenges maintaining
data consistency ensuring efficient
utilization of resources especially when
you're talking about big data we can
certainly use up the resources when
importing terabytes and petabytes of
data over the course of time loading
bulk data to Hadoop was not possible
it's one of the big challenge as it came
up when they first had the Hadoop file
system going and loading data using
script was very slow in other words
you'd write a script in whatever
language you're in and then it would
very slowly load each piece and parse it
in so the solution scoop scooped helped
in overcoming all the challenges to
traditional approach and could lead bulk
data from rdbms to Hadoop very easily so
think your Enterprise server you want to
take the from MySQL or your SQL and you
want to bring that data into your Hadoop
Warehouse your data filing system and
that's where scoop comes in so what
exactly is scoop scoop is a tool used to
transfer bulk of data between Hadoop and
external data stores such as relational
databases and MySQL server or the
Microsoft SQL server or MySQL server so
scoop equals SQL plus Hadoop and you can
see here we have our rdbms all the data
we have stored on there and then your
scoop is the middle ground and brings
the import into the Hadoop file system
it also is one of the features that goes
out and grabs the data from Hadoop and
exports it back out into an rdbms let's
take a look at scoop features scoop
features has parallel Import and Export
it has import results of SQL query
connectors for all major rdbms databases
Kerberos security integration provides
full and incremental load so we look at
parallel Import and Export scoop uses
yarn yet another resource negotiator
framework to Import and Export data this
provides fault Tolerance on a top of
parallelism scoop allows us to import
the result return from an SQL carry into
the Hadoop file system or the hdfs and
you can see here where the import
results of SQL query come in school
provides connectors for multiple
relational database management system
rdbms's databases such as MySQL and
Microsoft SQL server and it has
connectors for all major rdbms databases
scoop supports Kerberos computer network
Authentication Protocol that'll allows
nodes communicating over a non-secure
network to prove their identity to one
another in a secure manner scoop can
load the whole table or parts of the
table by a single command hence it
supports full and incremental load let's
dig a little deeper into the scoop
architecture we have our client in this
case a hooded wizard behind his laptop
you never know who's going to be
accessing the Hadoop cluster and the
client comes in and sends their command
which goes into scoop the client submits
the import export command to import or
export data data from different
databases is fetched by scoop and so we
have our Enterprise data warehouse
document based systems you have connect
connector for your data warehouse a
connector for document based systems
which reaches out to those two entities
and we have our connector for the rdbms
so connectors help in working with a
range of popular databases multiple
mappers perform map tasks to load the
data onto hdfs the Hadoop file system
and you can see here we have the map
task if you remember from Hadoop Hadoop
is based on mapreduce because we're not
reducing the data we're just mapping it
over it only accesses the mappers and it
opens up multiple mappers to do parallel
processing and you can see here the hdfs
hbase hive is where the target is for
this particular one similarly multiple
map tests will export the data from hdfs
onto rdbms using scoop export command so
just like you can import it you can now
export it using the multiple map
routines scoop import so here we have
our dbms data store and we have the
folders on there so maybe it's your
company's database maybe it's an archive
at Google with all the searches going on
whatever it is usually you think with
scoop you think SQL you think MySQL
server or Microsoft SQL Server that kind
of setup so it gathers the metadata and
you see the scoop import so introspect
database to gather metadata primary key
information and then it submits so you
can see submits map only job remember we
have about mapreduce it only needs the
map side of it because we're not
reducing the data we're just mapping it
over scoop device the input data set
into splits and uses individual map
tests to push the splits into hdfs so
right into the Hadoop file system and
you can see down on the right is kind of
a small depiction of a Hadoop cluster
and then you have scoop export so we're
going to go the other direction and with
the other direction you have your Hadoop
file system storage which is your Hadoop
cluster you have your scoop job and each
one of those clusters then gets a map
mapper comes out to each one of the
computers it has data on it so the first
step is you've got to gather the
metadata so step one you gather the
metadata step two submits map only job
introspect database to gather metadata
primary key information scoop divides
the input data set into splits and uses
individual map tests to push the splits
to rdbms scoop will export Hadoop files
back to rdms tables and you can think of
this in a number of different manners
one of the would be if you're restoring
a backup from the Hadoop file system
into your Enterprise machines there's
certainly many others as far as
exploring data and data science so as we
dig a little deeper into scoop input we
have our connect our jdbc and our URL so
specify the jdbc connect string
connecting manager we specify The
Connection Manager class to use you can
see here driver with the class name
manually specify the jdbc driver class
to use Hadoop map reduce home directory
override Hadoop mapped home username set
authentication username and of course
help print uses instructions and with
the export you'll see that we can
specify the jdbc connect string specify
The Connection Manager class to use
manually specify jdbc driver class to
use you do have to let it know to
override the Hadoop map reduce home and
that's true on both of these and set
authentication username and finally you
can print out all your help set up so
you can see the format for scoop is
pretty straightforward forward both
Import and Export so let's continue on
our path and look at scoop processing
and what the computer goes through for
that and we talk about school processing
first scoop runs in the Hadoop cluster
it Imports data from the rdbms the nosql
database to the Hadoop file system so
remember it might not be importing the
data from a rdbms it might actually be
coming from a nosql and there's many out
there it uses mappers to slice the
incoming data into multiple formats and
load the data into hdfs it exports data
back into an rdbms while making sure
that the schema of the data in the
database is maintained so now that we've
looked at the basic commands in our
scoop in the scoop processing or at
least the basics as far as theory is
concerned hello Learners simply and
brings you a postgraduate program in
data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below let's jump in and
take
and take
for this demo I'm going to use our
Cloudera quick start if you've been
watching our other demos we've done
you'll see that we've been using that
pretty consistently certainly this will
work in any of your your Horton sandbox
which is also a single node testing
machine Cloudera is one of um there's a
Docker version instead of virtual box
and you can also set up your own Hadoop
cluster plan a little extra time if
you're not an admin it's actually a
pretty significant Endeavor for an admin
if you've been admitting Linux machines
for a very long time and you know a lot
of the commands I find for most admins
it takes them about two to four hours
the first time they go in and create a
virtual machine and set up their own
Hadoop in this case though I mean you're
just learning and getting set up best to
start with Cloudera Cloudera also
includes an installed version of MySQL
that way you don't have to worry install
the the SQL version for importing data
from and two once you're in the Cloudera
quick start you'll see it opens a nice
Centos Linux interface and it has a
desktop setup on there this is really
nice for learnings here not just looking
at command lines and from in here it
should open up by default to Hue if not
you can click on Hue here's a kind of a
fun little web-based interface under Hue
I can go under query I can pick an
editor and we'll go right down to scoop
so now I'm just going to load the scoop
editor in our Hue now I'm going to
switch over and do this all in command
line I just want to show that you can
actually do this in a hue through the
web-based interface the reason I like to
do the command line is specifically on
my computer it runs much quicker or if I
do the command line here and I run it it
tends to have an extra lag or an added
layer in it so for this we're going to
go ahead and open our command line the
second reason I do this is we're going
to need to go ahead and edit our MySQL
so we have something to scoop in other
words I don't have anything going in
there and of course we zoom in we'll
zoom in this and increase the size of
our screen so for this demo our Hands-On
I'm going to use Oracle virtualbox
manager and the Cloudera quick start if
you're not familiar with this we do have
another tutorial we put out and you can
send a note in the YouTube video below
and let our team know and they'll send
you a link or come visit
www.simplylearn.com now this creates a
Linux box on my Windows computer so
we're going to be in Linux and it'll be
the Cloudera version with scoop it will
also be using MySQL MySQL server once
inside the Cloudera virtual box we'll go
under the Hue editor now we're going to
do everything in terminal window I just
want you to be aware that under the Hue
editor you can go under query editor and
you'll see as we come down here here's
our scoop on this so you can run your
Scoop from in here now before we do this
we have to do a little exploration in my
SQL and MySQL server that way we know
what data is coming in so let me go
ahead and open up a terminal window in
Cloudera you have a terminal window at
the top here that you can just click on
and open it up and let me just go ahead
and zoom in on here go View and zoom in
now to get into MySQL server you
typically type in MySQL and this part
will depend on your setup now the
Cloudera quick start comes up that the
username is root and the password is
Cloudera kind of a strange Quirk is that
you can put a space between the minus U
and the root but not between the minus p
and the Cloudera usually you'd put in a
minus capital P and then it prompts you
for your password on here for this demo
I don't worry too much about you knowing
the password on that so we'll just go
right into MySQL server since this is
the standard password for this quick
start and you can see we're now into
MySQL and we're going to do just a
couple of quick commands in here there's
show databases and you follow by the
semicolon that's standard in most of
these shell commands so it knows it's
the end of your shell command and you'll
see in here in the quick start Cloudera
quick start the MySQL comes with a
standard set of databases these are just
some of these have to do like with the
uzi which is the uzi part of Hadoop
where others of these like customers and
employee fees and stuff like that those
are just for demo purposes they come as
a standard setup in there so that people
going in for the first time have a
database to play with which is really
good for us so we don't have to recreate
those databases and you will see in the
list here we have a retail underscore DB
and then we can simply do uh use retail
underscore DB this will set that as a
default in MySQL and then we want to go
ahead and show the tables and if we show
the tables you can see under the
database the retail DB database we have
categories customers departments order
items orders products so there's a
number of tables in here and we're going
to go ahead and just use a standard SQL
command and if you did our Hive language
you'll note remember it's the same for
hql also on this we're just going to
select star everything from departments
so there's our departments table and
we're going to list everything on the
Departments table and you'll see we've
got six lines in here and it has a
department ID and a department name two
for Fitness three for Footwear so on and
so forth now at this point I can just go
ahead and exit but it's kind of nice to
have this data up here so we can look at
it and flap back and forth between the
screens so I'm going to open up another
terminal window and we'll go ahead and
zoom in on this also and it isn't too
important for this particular setup but
it's always kind of fun to know what
your setup you're working with what is
your host name and so we'll go ahead and
just type that in this is a Linux
command and it's uh hostname minus F and
you see we're on quick start Cloudera no
surprise there now this next command is
going to be a little bit longer because
we're going to be doing our first scoop
command and I want to do two of them
we're going to list databases and list
tables it's going to take just a moment
to get through this because there's a
bunch of stuff going on here so we have
scoop we have list databases we have
connect and under the connect command we
need to let it know how we're connecting
we're going to use the jdbc this is a
very standard one jdbc MySQL so you'll
see that if you're doing an SQL database
that's how you start it off with and
then the next part this is where you
have to go look it up it's however it
was created so if your admin created a
MySQL server with a certain setup that's
what you have to go by and you'll see
that usually they list this as localhost
so you'll see something like localhost
sometimes there's a lot of different
formats but the most common is either
localhost or the actual connection so so
in this case we want to go ahead and do
quick start
3306 and so quick start is the name of
the localhost database and how it's
hosted on here and when you set up the
quick start for for Hadoop under
Cloudera it's Port 3306 is where that's
coming in so that's where all that's
coming from and so there's our path for
that and then we have to put in our
password we typically typed password if
you look it up password on the cloud era
quick start is Cloudera and we have to
also let it know the username and again
if you're doing this you'd probably put
in a minus Capital you can actually just
do it for a prompt for the password so
if you leave that out it'll prompt you
but for this doesn't really matter I
don't care if you see my password it's
the default one for Cloudera quick start
and then the username on here is simply
root and then we're going to put our
semicolon at the end and so we have here
our full setup and we go ahead and list
the databases and you'll see you may get
some warnings on here I haven't run the
updates on the quick start I suggest
you're not running the updates either if
you're doing this for the first time
because it'll do some reformatting on
there and it quickly pops up and you can
see here's all of our the tables we went
in there and if we go back to on the
previous window we should see that these
tables match so here we come in and here
we have our databases and you can see
back up here where we had the CM
customers employees and so on so the
databases match and then we want to go
ahead and list the tables for a specific
database so let's go ahead and do that
I'm a very lazy typist so I'll put the
up arrow in and you can see here scoop
list databases we're just going to go
back and change this from databases to
list tables so we want to list the
tables in here same connection so most
the connection is the same except we
need to know which tables we're listing
an interesting fact is you can create a
table without being under a database so
if you left this blank it will show the
open tables that aren't connected
directly to a database or under a
database but what we want to do is right
past this last slash on the 33 306 we
want to put that retail underscore DB
because that's the database we're going
to be working with and this will go in
there and show the tables listed under
that database and here we go we got
categories customers departments order
items and products if we flip back here
real quick there it is the same thing we
had we had categories customers
departments order items and so on and so
let's go ahead and run our first import
command and again I'm that lazy typer so
we're going to do scoop and instead of
list tables we want to go ahead and
import so there's our import command and
so once we have our import command in
there then we need to tell it exactly
what we're going to import so everything
else is the same we're importing from
the retail DB so we keep that and then
at the very end we're going to tag on
dash dash table that tells it so we can
tell it what table we're importing from
and we're going to import departments
there we go so this is pretty
straightforward because what's nice
about this is you can see the commands
are the same I got the same connection
um I change it for the whatever database
I'm in then I come in here our password
and the username are going to be the
same that's all under the MySQL server
setup and then we let it know what table
we're entering in we run this and this
is going to actually go through the
mapper process in Hadoop so this is a
mapping process it takes the data and it
Maps it up to different parts in the
setup in Hadoop on there and then saves
that data into the Hadoop file system
and it does take it a moment to zip
through which I kind of skipped over for
you since it is running a you know it's
designed to run across a cluster not on
a single node so when you're running on
a single node it's going to run slow
even if you dedicate a couple cores to
it I think I put dedicated four cores to
this one and so you can see right down
here we get to the end it's now mapped
in that information and then we can go
in here we can go under we can flip back
to our Hue and under Hue on the top I
have there's databases the second icon
over is your Hadoop file system and we
can go in here and look at the Hadoop
file system and you'll see it show up
underneath our documents there it is
departments Cloudera departments and you
can see there's always a delay when I'm
working in Hue which I don't like and
that's the quick start issue that's not
necessarily running out on a server when
I'm running it on a server you pretty
much have to run through some kind of
server interface I still prefer the
terminal window it still runs a lot
quicker but we'll flip back on over here
to the command line and we can do the
Hadoop type in the Hadoop fs and then
list minus LS and if we run this you'll
see underneath our Hadoop file system
there is our departments which has been
added in and we can also do Hadoop fs
and this is kind of interesting for
those who've gone through the Hadoop
file system everything you'll you'll
recognize this on here I'm going to list
it the contents of departments and
you'll see underneath departments we
have part part m0001
002003 and so this is interesting
because this is how Hadoop saves these
files and this is in the file system
this is not in Hive so we didn't
directly import this into Hive we put
this in the Hadoop file system depending
on what you're doing you would then
write the schema for Hive to look at the
Hadoop file system certainly visit our
Hive tutorial for more information on
hive specific uh so you can see in here
are different files that it forms that
are part of departments and we can do
something like this we can look at the
contents of one of these files FS minus
LS or a number of the files and we'll
simply do the full path which is user
Cloudera and then we already know the
next one is departments and then after
departments we're going to put slash
part star so this is going to see
anything that has part in it so we have
part Dash m000 and so on we can go ahead
and cat use that cat command or that
list command to bring those up and then
we can use the cat command to actually
display the contents and that's a a
Linux command Hadoop Linux command to
catinate not to be confused with
catatonic catastrophic there's a lot of
cat got your tongue and we see here
Fitness Footwear apparel that should
look really familiar because that's what
we had in our MySQL server when we went
in here we did a select all on here
there it is Fitness Footwear apparel
golf outdoors and fan shop and then of
course it's really important slip back
on over here to be able to tell it where
to put the data so we go back to our
import command so here's our scoop
import we have our connect we have the
DB underneath our connection our MySQL
server we have our password our username
the table going where it's going to I
mean the table where it's coming from
and then we can add a Target on here we
can put in Target Dash directory and you
do have to put the full path that's a
Hadoop thing it's a good practice to be
in and we're going to add it to
Department we'll just do Department one
and so here we now add a Target
directory in here in user Cloudera
and this will take just a moment before
so I'll go ahead and skip over the
process since it's going to run very
slowly it's only running on like I said
a couple cores and it's also on a single
node and now we can do the uh Hadoop
let's just do the up Arrow file system
list we want just straight list and when
we do the Hadoop file system uh minus LS
or list you'll see that we now have
Department one and we can of course do a
list Department one and you can see we
have the files inside Department one and
they mirrored what we saw before with
the same files in there and the part mm0
and so on if we were to look at him it'd
be the same thing we did before with the
cat so except instead of departments
we'd be Department one there we go one
thing that's going to come up with the
same data we had before now one of the
important things when you're importing
data and it's always a question to ask
is do you filter the data before it
comes in do we want to filter this data
as it comes in so we're not storing
everything in our file system you would
think Hadoop Big Data put it all in
there I know from experience that
putting it all in there can turn a
couple hundred terabytes into a petabyte
very rapidly and suddenly you're having
to really add on to that data store and
you're storing duplicate data sometimes
so you really need to be able to filter
your data out and so let's go ahead and
use our up Arrow to go to our last
import since it's still a lot the same
stuff so we have all of our commands
under import we have the target we're
going to change this to Department two
so we're going to create a new directory
for this one and then after departments
there's another command that we didn't
really slide in here and that's our
mapping and I'll show you what this
looks like in a minute but we're going
to put M3 in there that doesn't have
nothing to do with the filtering I'll
show you that in a second though what
that's for and we just want to put in
where
so where and what is the where in this
case we want to know where Department ID
and if you want to know where that came
from we can flip back on over here we
have Department underscore IDs this is
where that's coming from that's just the
name of the column on here so we come in
here to Department ID is greater than
four simple logic there you can see
where you'd use that for maybe creating
buckets for ages you know age from 10 to
15 20 to 30. you might be looking for I
mean there's all kinds of reasons why
you could use the where command on here
in filter information out maybe you're
doing word counting and you want to know
words that are used less than a hundred
times you want to get rid of the and is
and and all the stuff that's used over
and over again so we'll go ahead and put
the where and then Department ID is
greater than four we'll go ahead and hit
enter on here and this will create our
department to set up on this and I'll go
ahead and skip over some of the runtime
again it runs really slow on a single
node real quick page through our
commands
here we go our list and we should see
underneath the list the department two
on here now and there it is Department
two and then I can go ahead and do list
Department two you'll see the contents
in here and you'll see that there is
only three maps and it could be that the
data created three Maps but remember I
set it up to only use three mappers so
there's zero one and two and we can go
ahead and do a cat on there remember
this is Department two so we want to
look at all the contents of these three
different files and there it is it's
greater than four so we have golf is
five outdoor six uh fan shop is seven so
we've effectively filtered out our data
and just storing the data we want on our
file system so if you're going to store
data on here the next stage is to export
the data remember a lot of times you
have MySQL server and we're continually
dumping that data into our long-term
storage and access the Hadoop file
system but what happens when you need to
pull that data out and restore a
database or uh maybe you have um you
just merged with a new company a
favorite topic merging companies
emerging databases that's listed under
Nightmare and how many different names
for company can you have so you can see
where being able to export is also
equally important and let's go ahead and
do that and I'm going to flip back over
to my SQL Server here and we'll need to
go ahead and create our database we're
going to export into now I'm not going
to go too much in detail on this command
we're simply creating a table and the
table is going to have it's pretty much
the same table we already have in here
from departments but in this case we're
going to create a table called Dept so
it's the same setup but it's it's just
gonna we're just giving a different name
a different schema and so we've done
that and we'll go ahead and do a select
star from Dept there we go and it's
empty that's what we expect a new
database a new data table and it's empty
in there so now we need to go ahead and
Export our data that we just filtered
out into there so let's flip back on
over here to our our scoop setup which
is just our Linux terminal window and
let's go back up to one of our commands
here's scoop Import in this case instead
of import we're going to take the scoop
and we're going to export so we're going
to just change that export and the
connection is going to remain the same
so same connect same database we're also
we're still doing the retail DB we have
the same password so none of that
changes the big change here is going to
be the table instead of departments
remember we changed it and gave it a new
name and so we want to change it here
also Dept so Department we're not going
to worry about the mapper count and the
where was part of our import there we go
and then finally it needs to know where
to export from so instead of Target
directory we have an export directory
that's where it's coming from still user
Cloudera and we'll keep it as Department
two just so you can see how that data is
coming back with the that we've filtered
in and let's go ahead and run this it'll
take it just a moment to go through in
steps and again because it's low I'm
just going to go ahead and skip this so
you don't have to sit through it and
once we've wrapped up our export we'll
flip back on over here to mySQL use the
up arrow and this time we're going to
select star from department and we can
see that there it is it exported the
golf outdoors and fan shop and you can
imagine also that you might have to use
the where command in your export also so
there's a lot of mixing the command line
for scoop is pretty straightforward
you're changing the different variables
in there whether you're creating a table
listing a table listing databases very
powerful tool for bringing your data
into the Hadoop file system and
exporting it so now that we've wrapped
up our demo on scoop and gone through a
lot of basic commands let's dive in with
a brief history of Hive so the history
of Hive begins with Facebook Facebook
begin using Hadoop as a solution to
handle the growing big data and we're
not talking about a data that fits on
one or two or even five computers uh are
talking due to the fifth sign if you've
looked at any of our other Hadoop
tutorials you'll know we're talking
about very big data and data pools and
Facebook certainly has a lot of data it
tracks as we know the Hadoop uses map
reduce for processing data mapreduce
required users to write long codes and
so you'd have these really extensive
Java codes very complicated for the
average person to use not all users
reversed in Java and other coding
languages this proved to be a
disadvantage for them users were
comfortable with writing queries in SQL
SQL has been around for a long time the
standard SQL query language Hive was
developed with the vision to incorporate
the concepts of tables columns just like
SQL so why Hive well the problem was for
processing and analyzing data users
found it difficult to code as not all of
them were well versed with the coding
languages you have your processing you
have your analyzing and so the solution
was required a language similar to SQL
which was well known to all the users
and thus the The Hive or hql language
evolved what is Hive Hive is a data
warehouse system which is used for
querying and analyzing large data sets
stored in the hdfs or the Hadoop file
system Hive uses a query language that
we call Hive ql or hql which is similar
to SQL so if we take our user the user
sends out their hive queries and then
that is converted into a mapreduce tasks
and then accesses the Hadoop mapreduce
system let's take a look at the
architecture of Hive architecture of
Hive we have the hive client so that
could be the programmer or maybe it's a
manager who knows enough SQL to do a
basic query to look up the data they
need the hive client supports different
types of client applications in
different languages prefer for
performing queries and so we have our
Thrift application in the hive Thrift
client Thrift is a software framework
Hive server is based on Thrift so it can
serve the request from all programming
language that support Thrift and then we
have our jdbc application and the hive
jdbc driver jdbc Java database
connectivity jdbc application is
connected through the jdbc driver and
then you have the odbc application or
the hive odbc driver the odbc or open
database connectivity the odbc
application is connected through the
odbc driver with the growing development
of all of our different scripting
languages python C plus plus spark Java
you can find just about any connection
in any of the main scripting languages
and so we have our Hive Services as we
look at deeper into the architecture
Hive supports various services so you
have your Hive server basically your
Thrift application or your hive Thrift
client or your jdbc or your hive jdbc
driver your odbc application or your
hive odbc driver they all Connect into
The Hive server and you have your hive
web interface you also have your CLI now
the hive web interface is a GUI is
provided to execute Hive queries and
we'll actually be using that later on
today so you can see kind of what that
looks like and get a feel for what that
means commands are executed directly in
CLI and then the CLI is a direct
terminal window and I'll also show you
that too so you can see how those two
different interfaces work these then
push the code into the hive driver Hive
driver is responsible for all the
queries submitted so everything goes
through that driver let's take a closer
look at the hive driver The Hive driver
now performs three steps internally one
is a compiler Hive driver passes query
to compiler where it is checked and
analyzed then the optimizer kicks in and
the optimized logical plan in the form
of a graph of mapreduce and hdfs tasks
is obtained and then finally in the
executor in the final step the tasks are
executed when we look at the
architecture we also have to note the
metastore metastore is a repository for
five metadata stores metadata for Hive
tables and you can think of this as your
schema and where is it located and it's
stored on the Apache Derby DB processing
and resource management is all handled
by the mapreduce V1 you'll see mapreduce
V2 the yarn and the Tes these are all
different ways of managing these
resources depending on what version of
Hadoop you're in Hive uses mapreduce
framework to process queries and then we
have our distributed storage which is
the hdfs and if you looked at our Hadoop
tutorials you'll know that these are on
commodity machines on our linearly
scalable that means they're very
affordable a lot of time when you're
talking about Big Data you're talking
about a tenth of the price of storing it
on Enterprise computers and then we look
at the data flow And Hive so in our data
flow And Hive we have our Hive in the
Hadoop system and underneath the user
interface or the UI we have our driver
our compiler our execution engine and
our metastore that all goes into the map
reduce and the Hadoop file system so
when we execute a query you can see it
coming in here it goes into the driver
step one step two we get a plan what are
we going to do refers to the query
execution then we go to the metadata
it's like well what kind of metadata are
we actually looking at where is this
data located what is the schema on it uh
then this that comes back with the
metadata into the compiler then the
compiler takes all that information and
the send plan returns it to the driver
the driver then sends the execute plan
to the execution engine once it's in the
execution engine the execution engine
acts as a bridge between Hive and Hadoop
to process the query and that's going
into your map reduce and your Hadoop
file system or your hdfs and then we
come back with the metadata operations
it goes back into the metastore to
update or let it know what's going on
which also goes to the between it's a
communication between the execution
engine and the metastore execution
engine Communications is
bi-directionally with the metastore to
perform operations like create drop
tables metastore stores information
about tables and columns so again we're
talking about the schema of your
database and once we have that we have a
bi-directional send results
communication back into the driver and
then we have the fetch results which
goes back to the client so let's take a
little bit look at the hive data
modeling Hive data modeling so you have
your high data modeling you have your
tables you have your partitions and you
have buckets the tables in however
created the same way it is done in rdbms
so when you're looking at your
traditional SQL server or MySQL server
where you might have Enterprise
equipment and a lot of people pulling
and moving stuff off of there the tables
are going to look very similar and this
makes it very easy to take that
information and let's say you need to
keep current information but you need to
store all of your years of transactions
back into the Hadoop Hive so you match
those those all kind of look the same
the tables are the same your databases
look very similar and you can easily
import and bet you can easily store them
into the hive system partitions here
tables are organized into partitions for
grouping same type of data based on
partition key this can become very
important for speeding up the process of
doing queries so if you're looking at at
dates as far as like your employment
dates of employees if that's what you're
tracking you might add a partition there
because that might be one of the key
things that you're always looking up as
far as employees are concerned and
finally we have buckets data present in
partitions can be further divided into
buckets for efficient querying again
there's that efficiency at this level a
lot of times you're you're working with
the programmer and the admin of your
Hadoop file system to maximize the
efficiency of that file system so it's
usually a two-person job and we're
talking about Hive data modeling you
want to make sure that they work
together and you're maximizing your
resources Hive data types so we're
talking about Hive data types we have
our primitive data types and our complex
data types A lot of this would look
familiar because it mirrors a lot of
stuff in SQL in our primitive data types
we have the numerical data types string
data type date time data type and
Melissa miscellaneous data type and
these should be very they're kind of
selfish explanatory but just in case
numerical data is your floats your
integers your short integers all of that
numerical data comes in as a number a
string of course is characters and
numbers and then you have your date time
step and then we have kind of a general
way of pulling your own created data
types in there that's your miscellaneous
data type and we have complex data types
so you can store arrays you can store
maps you can store structures and even
units in there as we dig into Hive data
types and we have the primitive data
types and the complex data types so we
look at primitive data types and we're
looking at numeric data types data types
like an integer a float a decimal those
are all stored as numbers in the hive
data system a string data type data
types like characters and strings you
store the name of the person you're
working with you know John Doe the city
um
State Tennessee maybe it's Boulder
Colorado USA or maybe it's hyperbad
India that's all going to be string it's
stored as a string character and of
course we have our date time data type
data types like time stamp date interval
those are very common as far as tracking
cells anything like that so you just
think if you can type a stamp of time on
it or maybe you're dealing with the race
and you want to know the interval how
long did the person take to complete
whatever task it was all that is date
time data type and then we talked
miscellaneous data type these are like
Boolean in binary and when you get into
Boolean and binary you can actually
almost create anything in there but your
yes nose zero one now let's take a look
at complex data types a little closer we
have arrays so your syntax is of data
type and it's an array and you can just
think of an array as a collection of
same entities one two three four if
they're all numbers and you have Maps
this is a collection of key value pairs
so understanding is so Central to Hadoop
so we store Maps you have a key which is
a set you can only have one key per
mapped value and so you can in Hadoop of
course you collect the same keys and you
can add them all up or do something with
all the contents of the same key but
this is our map as a primitive type data
type in our collection of key value
Pairs and then collection of complex
data with comment so we can have a
structure we have a column name data
type comment column comment so you can
get very complicated structures in here
with your collection of data and your
commented setup and then we have units
and this is a collection of
heterogeneous data types so the syntax
for this is Union type data type data
type and so on so it's all going to be
the same a little bit different than the
arrays where you can actually mix and
match different modes of Hive Hive
operates in two modes depending on the
number and size of data nodes we have
our local mode and our map reduce mode
when we talk about the local mode it is
used when dupe is having one data node
and the data is small processing will be
very fast in a smaller data sets which
are present in local machine and this
might be that you have a local file of
stuff you're uploading into the hive and
you need to do some processes in there
you can go ahead and run those High
processes and queries on it usually you
don't see much in the way of a single
node Hadoop system if you're going to do
that you might as well just use like an
SQL database or even a Java sqlite or
something python ssqlite so you don't
really see a lot of single node Hadoop
databases but you do see the local mode
in Hive where you're working with a
small amount of data that's going to be
integrated into the larger database and
then we have the map reduce mode this is
used when Hadoop is having multiple data
nodes and the data is spread across
various data nodes processing large data
sets can be more efficient using this
mode and this you can think of instead
of it being one two three or even five
computers we're usually talking with the
Hadoop file system we're looking at 10
computer
15 100 where this data is spread across
all those different Hadoop nodes take a
look at simply learns postgraduate
program in data engineering offered in
collaboration with per University and
IBM elevate your career with this
applied Learning Journey enroll now and
get certified get ahead oh why pig as we
all know Hadoop mapreduce to analyze and
process big data processing Big Data
consumed more time so before we had the
Hadoop system they'd have to spend a lot
of money on a huge set of computers and
Enterprise machines so he introduced the
Hadoop mapreduce and so afterwards
processing Big Data was faster using
mapreduce then what is the problem with
map reduce prior to 2006 all mapreduce
programs were written in Java
non-programmers found it difficult to
write lengthy Java codes they faced
issues in incorporating map sort reduce
to fundamentals of mapreduce while
creating a program you can see here map
phase Shuffle and sort reduce phase
eventually it became a difficult task to
maintain and optimize a code due to
which the processing time increased you
can imagine a manager trying to go in
there and needed in a simple query to
find out data and he has to go talk to
the programmers anytime he wants
anything so that was a big problem not
everybody wants to have a on-call
programmer for every manager on their
team Yahoo faced problems to process and
analyze large data sets using Java as
the codes were complex and lengthy there
was a necessity to develop an easier way
to analyze large data sets without using
time-consuming complex Java modes and
codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it was
developed with a vision to analyze and
process large data sets without using
complex Java codes Pig was developed
especially for non-programmers pig used
simple steps to analyze data sets which
was time efficient so what exactly is
pick pig is a scripting platform that
runs on Hadoop clusters designed to
process and analyze large data sets and
so you have your pig which uses SQL like
queries they're definitely not SQL but
some of them resemble SQL queries and
then we use that to analyze our data Pig
operates on various types of data like
structured semi-structured and
unstructured data let's take a closer
look at mapreduce versus Hive versus Pig
so we start with a compiled language
your map reduce and we have Hive which
is your SQL like query and then we have
pig which is a scripting language it has
some similarities to SQL but it has a
lot of its own stuff remember SQL like
query which is what Hive is based off
looks for structured data and so when
you get into scripting languages like
Pig now we're dealing more with
semi-structure and even unstructured
data with a Hadoop map reduce we have a
need to write long complex codes with
Hive no need to write complex codes you
could just put it in a simple SQL query
or hql Hive ql and in pig no need to
write complex codes as we have pig lat
now remember in the map reduce it can
produce structured semi-structured and
unstructured data and as I mentioned
before Hive can process only structured
data think rows and columns where Pig
can process structured semi-structured
and unstructured data you can think of
structured data as rows and columns
semi-structured as your HTML XML
documents if you have on your web web
pages and unstructured could be anything
from groups of documents and written
format Twitter tweets any of those
things come in as very unstructured data
and with our Hadoop mapreduce we have a
lower level of abstraction with both
Hive and pig we have a higher level
abstraction so it's much more easy for
someone to use without having to dive in
deep and write a very lengthy map reduce
code and those map and reduce codes can
take 70 80 lines of code when you can do
the same thing in one or two lines with
however Pig this is the advantage Pig
has over Hive it can process only
structured data in Hive while in pig it
can process structured semi-structured
and unstructured data some other
features to note that separates the
different query languages is we look at
map and reduce map reduce supports
partitioning features as does Hive Pig
no concept of partitioning in pigs it
doesn't support your partitioning
feature your partitioning features allow
you to partition the data in such a way
that it can be queried quicker you're
not able to do that in pig mapreduce
uses Java and python while Hive uses an
SQL like query language known as Hive ql
or hql Pig Latin is used which is a
procedural data flow language mapreduce
is used by programmers pretty much as
straightforward on Java Hive is used by
data analysts pig is used by researchers
and programmers certainly there's a lot
of mix between all three programmers
have been known to go in and use a hive
for quick query and anybody's been able
to use Pig for a quick query or research
under map and reduce code performance is
really good under Hive code performance
is lesser than map and reduce and pick
under Pig Code performance is lesser
than mapreduce but better than Hive so
if we're going to look at speed and time
the map reduce is going to be the
fastest performance on all of those
where Pig will have second and high
follows in the back let's look at
components of pig pig has two main
components we have pig Latin Pig Latin
is a procedure data flow language used
in pig to analyze data it is easy to
program using piglat and it is similar
to SQL and then we have the runtime
engine runtime engine represents the
execution environment created to run Pig
Latin programs it is also a compiler
that produces mapreduce programs uses
hdfs or your Hadoop file system for
storing and retrieving data and as we
dig deeper into the pig architecture
we'll see that we have pig latin scripts
programmers write a script in pig latin
to analyze data using Pig then you have
the grunt shell and it actually says
grunt when we start it up and we'll show
you that here in a little bit which goes
into the pig server and this is where we
have our parser parser checks the syntax
of the pig script after checking the
output will be a dag directed acelic
graph and then we have an Optimizer
which optimizes after your dag your
logical plan is passed to The Logical
Optimizer where an optimization takes
place finally the compiler converts the
dag into mapreduce jobs and then that is
executed on the map reduce under the
execution engine the results are
displayed using dump statement and
stored in hdfs using store statement and
again we'll show you that um they kind
of end you always want to execute
everything once you've created it and so
dump is kind of our execution statement
and you can see right here as we were
talking about earlier once we get to the
execution engine and it's coded into
mapreduce then the map reduce processes
it onto the hdfs working of pick Pig
Latin script is written by the users so
you have load data and right Pig script
and pig operations so we look at the
working of pig pig latin script is
written by the users there's step one we
load data and write Pig script and step
two in this step all the pig operations
are performed by parser Optimizer and
compiler so we go into the pig
operations and then we get to step three
execution of the plan in this stage the
results are shown on the screen
otherwise stored in the hdfs as per the
code so it might be of a small amount of
data you're reducing it to and you want
to put that on the screen or you might
be converting a huge amount of data
which you want to put back into the
Hadoop file system for other use let's
take a look at the pig latin data the
data model of pig latin helps pig to
handle various types of data for example
we have Adam Rob or 50. Adam represents
any single value of primitive data type
in pig latin like integer float string
it is stored as a string two bolts so we
go from our atom which are most basic
thing so if you look at just Rob or just
50 that's an atom that's our most basic
object we have in pig latin then you
have a tuple Tuple represents sequence
of fields that can be of any data type
it is the same as a row in rdbms for
example a set of data from a single row
and you can see here we have Rob comma
five and you can imagine with many of
our other examples we've used you might
have the ID number the name where they
live their age their date of starting
the job that would all be one row and
store it as a tuple and then we create a
bag a bag is a collection of tuples it
is the same as a table in rdbms and is
represented by brackets and you can see
here we have our table with Rob 5 Mike
10 and we also have a map a map is a set
of key value pairs key is of character
array type and a value can be of any
type it is represented by the brackets
and so we have name and age where the
key value is Mike and 10. pig latin has
a fully nestable data model that means
one data type can be nested within
another here's a diagram representation
of pig latin data model and in this
particular example we have basically an
ID number a name an age and a place and
we break this apart we look at this
model from Pig Latin perspective we
start with our field and if you remember
a field contains basically an atom it is
one particular data type and the atom is
stored as a string which it then
converts it into either an integer or
number or character string next we have
our Tuple and in this case you can see
that it represents a row so our Tuple
would be three comma Joe comma 29 comma
California and finally we have our bag
which contains three rows in it in this
particular example let's take a quick
look at Pig execution modes
it works in two execution modes
depending on where the data is reciting
and where the pig script is going to run
we have local mode here the pig engine
takes input from the Linux file system
and the output is stored in the same
file system local mold local mode is
useful in analyzing small data sets
using pick and we have the mapreduce
mode here the pig engine directly
interacts and executes in hdfs and
mapreduce in the map reduce mode queries
written in pig latin are translated into
mapreduce jobs and are run on a Hadoop
cluster by default Pig runs in this mode
there are three modes in pig depending
on how a pig latin code can be written
we have our interactive mode batch mode
and embedded mode the interactive mode
means coding and executing the script
line by line when we do our example
we'll be in the interactive mode in
badge mode all scripts are coded in a
file with extension dot Pig and the file
is directly executed introduction to
mapreduce
mapreduce is a programming model that
processes and analyzes huge data sets
logically into separate clusters
while map sorts the data reduce
segregates the data into logical
clusters thus removing the bad data and
retaining necessary information
why mapreduce
prior to 2004 huge amounts of data were
stored in single servers if any program
ran a query for the data stored in
multiple servers logical integration of
the search results and Analysis of the
data was a nightmare
not to mention the massive efforts and
expenses that were involved
the threat of data loss challenge of
data backup and reduced scalability
resulted in the issue snowballing into a
crisis of sorts
to counter this Google introduced
mapreduce in December of 2004.
and the analysis of data sets was done
in less than 10 minutes rather than 8 to
10 days
queries could run simultaneously on
multiple servers and search results
could be logically integrated and data
could be analyzed in real time
the USPS of mapreduce are its fault
tolerance and scalability
let's look at a mapreduce analogy the
mapreduce steps
counting after an election as an analogy
in step one each polling Booth ballot
papers are counted by a teller this is a
pre-map reduced step called input
splitting
in Step 2 tellers of all Boos count the
ballot papers in parallel
as multiple tellers are working on a
single job
the execution time will be faster this
is called the map method
in step 3 the ballot count of each Booth
under the assembly and Parliament seat
positions is found and the total count
for the candidates is generated
this is known as the reduce method thus
map and reduce help to execute the job
quicker than an individual counter the
mapreduce analogy vote counting is an
example to understand the use of
mapreduce
the key reason to perform mapping and
then reducing is to speed up the job
execution of a specific process this can
be done by splitting a process into a
number of tasks thus enabling
parallelism
if one person counts all of the ballot
papers and waits for others to finish
the ballot count it could take a month
to receive the election results when
many people count the ballot papers
simultaneously the results are obtained
in one or two days this is how mapreduce
works
let's look at a word count example in
this screen the mapreduce operation is
explained using a real-time problem
the job is to perform a word count of
the given paragraph on the left
the sentence in input says the quick
brown fox jumps over a lazy dog
and a dog is a man's best friend we will
then take this sentence through the
corresponding steps of splitting mapping
shuffling and reducing the mapreduce
process then begins with the input phase
which refers to providing data for which
the mapreduce process is to be performed
the sentence used is as input here
a Hadoop ecosystem at least a very
fundamentals of all the different parts
that are in the Hadoop ecosystem and
it's very robust it's grown over the
years with different things added in
there's a lot of overlapping in a lot of
these tools but we're just going to
cover these basic tools so you can see
what's available in the Hadoop ecosystem
so let's go back to our Hadoop ecosystem
as you can see we have all our different
setup and let's focus on the Hadoop part
of it first before we look at the
different tools we start with the Hadoop
or hdfs is for data storage write once
read many times you can store a lot of
data on it affordably distributed file
system and so we talk about the Hadoop
file system it stores different formats
of data on various machines and so you
have like a huge cluster of computers
and you're able to store Word documents
spreadsheets structured data
non-structured data semi-structured and
in the Hadoop file system there's the
two different sets of servers in there
there's the name node which is the
master we talked about that that's your
Enterprise computer and the other
component is your data notes and so
you'll usually have like I said one to
two you'll have a name node and maybe a
backup name node and then you'll have as
many data nodes as you want and you can
just keep adding them that's what makes
it so affordable you know you have a
rack of computers and you go oh I need
more space you just add another Rack in
so it's very affordable and very easy to
expand and the way the Hadoop file
system itself Works behind the hood is
it splits the data into multiple blocks
by default it's 128 megabytes and the
120 megabytes it is a default setting
you can change that that works for most
data there's reasons for either
processing speed or for better
distribution of the data so if you have
little tiny blocks of data that are less
than 128 megabytes if you have a lot of
those you might want to go down in size
and vice versa for larger blocks and you
can see right here we have 300 megabytes
and it takes that piece of data and it
just divides it into blocks of data and
each one 128 128 and 44 which if you add
together equals 300 megabytes and now
that you understand the Hadoop file
system or at least a basic overview of
it it's important to note what it sits
on what's actually making all this work
on the back end and this is yarn a
Hadoop yarn is a cluster Resource
Management so it's how it manages this
whole cluster right here that we just
looked at and yarn stands for yet
another resource negotiator love the
title it reminds me of an Iron Man movie
with you know Tony Starks and Jarvis
just a rather intelligent system or
whatever stood for but yarn has become
very widely used and it's actually used
as a back end for a lot of other
packages so it's not just in Hadoop but
Hadoop is where it came from and where
it's set up and there's some other ones
another probably one is mesos which I'll
mention again briefly and so the yarn it
handles the cluster of notes it's the
one the when you hear yarn it's someone
that's going Hey where's our Ram at
where's our hard drives at or if you
have a solid state disk drive your SD
more is that at how much memory do I
have what can I put where and so here we
go nice image of it RAM memory resources
so it's allocating all these different
resources for different applications is
what it's doing when we talk about the
back to the two major components the two
major components is your resource
manager that's on the master server or
your Enterprise computer and then that
one is in control and managing what's
going on with all of your nodes data
processing in Hadoop mapreduce and we're
going to talk about the map reduce here
in just a second the Hadoop data
processing is all built upon map reduce
map reduce processes a large volumes of
data in parely distributed manner this
is very core to Hadoop but before we go
on because there's other tools out there
and things are slowly shifting and
there's all kinds of new things one of
the things you want to look at is not
just how the map reduce works but start
thinking map reduce one of the best
pieces of advice I had from a one of my
mentors in data science was think map
reduce this is really what you should be
learning but it is an actual process in
the Hadoop system so we have our big
data and the Big Data Maps out and so
this is the first step is if I'm looking
at my data how do I map that data out
what am I looking at and it could be
something as simple as I just loaded
into you know I'm just looking at one
line at a time but it could be that I'm
looking at the data one line at a time
but I only need columns one and four
maybe I'm looking at it one column at a
time but I need the total of column one
added together and column one over
column two so you can start to see and
get some very complicated mapping here
but the mapping is what do you do with
each line of data data each piece of
data if you're in a spreadsheet it's
easy to see you have a row whatever you
do to that row that's what you're
mapping because it doesn't look at
anything else it doesn't know anything
else all it knows is what's on that row
if you're looking at documents maybe
it's pulling one document at a time and
so your map is then a document and then
it takes those and we Shuffle and sort
them how do we sort them around whether
you're grouping them together whether
you're taking the whatever information
you mapped out of it word counts you're
counting the word a so letter A comes
out with how many for each mapping if
you have 54 A's per the one document 53
and the other ones that's what's coming
out of that mapping and going into the
shuffle and sort and so if it's counting
A's and B's and C's it'll Shuffle and
sort all the A's together all the B's
together if you're running a big data
for running agriculture apples and
oranges so it puts all the apples in one
all the stuff you mapped out that you
said hey these are all apples these are
all oranges let's shuffle them and sort
them together and then we reduce and
reduce so each of these groups reduce
into the data you want out so you have
map Shuffle sort reduce and some
important things to know about the
Hadoop file system because I'm going to
mention spark in a little bit is a
Hadoop file system manages to do a lot
of this by writing it to the hard drive
so if you're running a really low budget
which nobody does anymore with the
Hadoop file system and you have all your
commodity machines are all low they only
have a eight gigabytes memory instead of
128 gigabytes this process uses the hard
drive and so it pulls it into the RAM
for your mapping it Maps a one piece of
data then it writes that map to the hard
drive then it takes that mapping from
the hard drive loads it up and shuffles
it and writes it back to the hard drive
to a different spot and then takes that
information and starts processing it in
the reduce and then it writes the
reduced answer to the hard drive it runs
slower because you're accessing to and
from your hard drive or solid state
drive if you have an SD card in there
but you can also utilize it's a lot more
affordable you know it's like I said you
having that higher end of ram cost even
on a commodity machine so you can save a
lot of money nowadays it's so affordable
people run the spark setup on there
which does the same thing but in Ram and
again we'll talk about spark in just a
little bit and of course the final thing
is an output so again your reducer
written to the hard drive and then your
reduce is brought together to form one
output what is the maximum number of
oranges sold per an area I don't know
I'm making that up but the things about
Hadoop is it covers so many different
things that anything you can think of
you can put in Hadoop the question is do
you need to do you have enough data or
enough need for the high-end processing
so again look at mapper reduce but think
of this not just as mapreduce start
thinking map and reduce when you think
big data we're going to start getting
into the tools because you had all those
pictures of all those Cool Tools we have
so we're going to look at the first two
of those tools and in the tools we have
scoop and Flume and this is for your
data collection and ingestion we're
going to bring spark up in just a second
can also play a major role in this
because spark is its own animal that
spray rung from Hadoop connects into
Hadoop but it can run completely
independent but scoop and Flume are
specific to Hadoop in their ways to
bring in information into the Hadoop
file system so we talk about these we'll
start with scoop scoop is used to
transfer data between Hadoop and
external data stores such as relational
databases and Enterprise data warehouses
and so you can see right here here's our
Hadoop data and it's connecting up there
and it's either pushing the data or
pulling the data and we have a
relational database and Enterprise data
warehouse and there's that magic word
Enterprise that means these are the
servers that are very high-end so maybe
you have a high-end SQL server or a
bicycle server or whatever over there
and this is what's coming and going from
the scoop it Imports data from external
data stores into the hdfs hive and hbase
those are our two specific The Hive
setup is your SQL basically and you can
see a nice little image here here's
somebody on their laptop which would be
considered the client machine putting
together their code mode they push the
scoop the scoop goes into the task
manager the task manager then goes hey
what have I got for the hbase And Hive
system in our Hadoop system and then it
reaches out to the Enterprise data
warehouse or into the document based
system or relationship based database
relationship is your non-sql document is
just what it sounds like because you
have HTML documents or Word documents or
text documents and it's able to map
those tasks and then bring those into
the Hadoop file system now Flume is a
distributed service for collecting
aggregating and moving large amounts of
log data so kind of focused a little bit
on a slightly different set of data
although you'll find these overlap a lot
you can certainly use Flume to do a lot
of things you can do in scoop and vice
versa Flume ingests the data so we're
looking at like say a Json call to a
website XML documents unstructured and
semi-structured data is most commonly
digested by Flume best example I saw was
a Twitter I'm pulling Twitter feeds into
a Hadoop system so it ingests online
streaming data from social media Twitter
log files so we want to know what's
going on with error codes on your
servers and all those log files web
server what's going on in your web
server they can bring all this stuff in
and just dump it into the Hadoop data
file system to be looked at and
processed later and it's a web server
cloud social media data again all those
different sources it can be it's kind of
endless you know it just depends on what
your company needs versatility of Hadoop
is what makes it such a powerful source
to add into a company and so it comes in
there you have your Source it goes
through the channels it then goes
through kind of a sync feature to make
sure everything is in sync and then it
dumps it into the Hadoop file system so
we've covered in the Hadoop file system
the first two things let's look at some
of the scripting languages they have and
so we have the two here and you can also
think of these it actually says
scripting and SQL queries a lot of times
they're both referred to as queries so
you have both the Pig and the hive and
pig is used to analyze data in Hadoop it
provides a high-level data processing
language to perform numerous operations
on the data and it's made out of pig
Latin language for scripting Pig Latin
compiler converts Pig Latin code to
execute code and then you have your ETL
the ETL provides a platform for building
data flow for ETL and ETL is like the
catch three letters now on any job
interview I look at it says ETL it just
means extract transfer and load so all
we're doing is extracting the data
transferring it to where we need it and
then loading it into in this case a
Hadoop file system and there's other
pieces to that you know it's actually a
big thing because whenever you're
extracting data do you want to dump all
the data or do you want to do some kind
of pre-processing so you only bring it
in what you want and then one of the
cool things about Pig Latin is 10 lines
of pig latin script is around 200 lines
of map reduce job again the map reduce
is the back end processes that go on so
if we have pig and I'll be honest with
you pig is very easy to use but as a
scripter programmer I find it's more for
people who just need a quick pull of the
data and able to do some very basic
things very easily so if you're doing
some very high-end processing and model
building you usually end up in something
else so pigs great for that like if
you're in the management you need to
build a quick query report pig is really
good for that and so it definitely has
its place it's definitely a very useful
script to know and the pig latin scripts
I call it the grunt shell I guess it
goes with pig because they grunt you
have your pig server you have a parser
an Optimizer a compiler an execution
engine and that's all part of the Apache
Pig and this then goes into the map
reduce which then goes into the Hadoop
file system in the Hadoop and you'll see
Apache with a lot of these because it's
under the open source uh hadoops under
the Apache open source so all this stuff
is you'll see under Apache with the name
tag on there if you fear no Pig Hive is
the other one that's really popular for
easy query Hive facilitates Reading
Writing and managing large data sets
residing in the distributed storage
using SQL Hive query language and this
is important because a lot of times
you're coming from an SQL Server there's
your Enterprise set up and now you're
archiving a history of what's going on
on that server so you're continually
pulling data using scoop onto your into
your hbase hive database and as it comes
in there it'd be nice to just use that
same query to pull the data out of the
Hadoop file system and that's exactly
what it does so you have Hive command
line you can also use a JBC or odbc
driver and those drivers like if you're
working in Java or you're working in
Python you can use those drivers to
access Hive so you don't have to go
through the hive command line but it
makes it real quick I can take a high of
command line and just punch in a couple
words and pull up my data and so it
provides user-defined functions
UDF for data document indexing log
processing again anything that is in
some kind of SQL format if it's stored
that properly on the Hadoop file system
you can use your hive to pull it out and
you see even a more robust image of
what's in the hive there's your client
machine that's you on your laptop or
you're logged in wherever you're at
writing your script that goes into the
driver where you have your compiler your
Optimizer executor you also have your
jdbc odbc connections which goes into
the high Thrift server which then goes
into the driver your hive web interface
so you can just log in on the web and
start typing away your SQL commands and
that again goes to the driver and all
those pieces and those pieces go to your
job tracker your name node and your
actual Hadoop file system so spark
real-time data analysis spark is an open
source distributed computing engine for
processing and analyzing huge volumes of
real-time data so it runs 100 times
faster than map reduce map reduce is the
basics of the Hadoop system which is
usually so up in a Java code in spark
the map reduce instead of running what
happens in mapreduce as it goes pulls it
into the ram writes it to the hard drive
reads it off the hard drive shuffles it
around writes it back to the hard drive
pulls it off the hard drive does its
processing and you get the Impressions
going in and out of ram to the hard
drive and back up again so if you don't
have a lot of RAM and you have older
computers and you don't have the ram to
process something then spark and Hadoop
are going to run the same speed
otherwise spark goes hey let's keep this
all on the RAM and run faster and that's
what we're talking about this provides
in-memory computation of data so it's
fast you can see the guy running through
the door there speedy versus a very slow
Hadoop wandering around when it's used
to process and analyze real-time
streaming data such as stock market and
baking data so the spark can have its
own stuff going on and then it can
access the Hadoop database it can pull
data just like scoop and Flume do so it
has all those features in it and you can
even run Spark with its own yarn outside
of Hadoop there's also a spark on mesos
which is another resource manager
although yarn is most commonly used and
currently spark pretty much becomes
installed with Hadoop and so your spark
running on top of Hadoop will use the
same nodes it has its own manager and it
utilizes the ram so you'll have both you
can have the spark on top of there and
here we go you have your driver program
your spark context it goes into your
cluster manager your yarn then you have
your worker nodes which are going to be
executing tasks and cash and it's
important to remember when you're
running the spark setup even though
these are commodity machines we're still
usually a lot of them are like 128
gigabytes of RAM so that spark can run
these high-end processes certainly if
it's not accessed very much and you're
building a Hadoop cluster you could drop
that Ram way down if you're doing just
queries without any kind of processing
on there and you're not doing a lot of
queries but you know generally spark
you're looking at higher end they're
still come machines to do your work and
now we get into the Hadoop machine
learning and so machine learning is its
own animal I keep saying that these are
all kind of unique things mahout is
being phased out I mean people still use
it it's still important if you can write
your basic machine learning that has
most of the tools in there you certainly
can do it in mahout and how it again
writes to the hard drive reads from the
hard drive you can do all that in spark
and it's faster because it's just in Ram
and so spark has its own machine
learning tools in it you can also use Pi
spark which then accesses all the
different python tools and there's
there's just so many tools you can dump
into spark the how is very limited but
it's also very basic which in itself can
be really good sometimes simple is
better so how it is used to create
scalable and distributed machine
learning algorithms so here we have our
mahau environment it builds a machine
learning application so you're doing
linear regression you're doing
clustering you're doing classification
models it has a library that can retains
inbuilt algorithms for all of these and
then it has collaborative filtering and
again there's our classification and
there's our clustering regression so you
have your different machine learning
tools we can easily classify a large
amount of data using mahout so this
brings us to the next set of tools we
have or the next tool which is the
Apache ambari abari is an open source
tool responsible for keeping track of
running applications and their statuses
think of this as like a traffic cop more
like a security guard you can open it up
and see what's going on and so the
Apache ambari manages monitors and
Provisions Hadoop clusters provides a
central Management Service to start stop
and configure Hadoop services so again
it's like a traffic cop hey stop over
there start keep going hey what's going
on over that area in the through Lane on
the high-speed freeway and you can open
this up and you have a really easy view
of what's going on and so you can see
right here you have the ambari web the
ambari web is what you're looking at
that's your interface you've logged into
the MI server this will be usually on
the master node it's usually if you're
going to install on barium but you'll
just install it on the same node and it
connects up to the database and then it
has agents and each agent takes a look
and see what's going on with its host
server and if you're going to have two
more systems and we talked about spark
streaming there's also Kafka and Apache
for streaming data coming in and kafka's
distributed streaming platform to store
and process dreams of records and it's
written in Scala builds real-time
streaming data pipelines that reliably
get data between applications builds
real-time streaming applications that
transforms data into streams so it kind
of goes both ways on there so Kafka uses
a messaging system for transferring data
from one application to another so we
have our sender we have our message
queue and then we have our receiver
pretty straightforward very solid setup
on there with the Kafka patchy storm
storm is a processing engine that
processes real-time streeting at a very
high speed and it's written in closure
they utilize the function based
programming style of closure and it's
based on lisp to give it the speed
that's why Apache storm is built on
there so you could think of Kafka as a
slow-moving very solid communication
Network where Storm is looking at the
real-time data and grabbing that
streaming data that's coming in fast so
it has a ability to process over a
million jobs in a fraction of seconds on
a node so it's massive it can really
reach out there and grab the data and
it's integrated with Hadoop to harness
higher throughputs so this is the two
big things about storm if you are
pulling I think they mentioned stock
coming in or something like that where
you're looking for the latest data
popping up storm is a really powerful
tool to use for that so we've looked at
a couple more tools for bringing data in
we probably should talk a little bit
about security security has in Hadoop
has the Apache Ranger and Apache knocks
are the two most popular one and the
ranger the Apache Ranger Ranger is a
framework to enable Monitor and manage
data Securities across across the Hadoop
platform so the first thing it does is
it provides centralized Security
Administration to manage all security
related tasks the second thing it does
is it has a standardized authorization
across all Hadoop components and third
it uses enhanced support for different
authorization methods role-based Access
Control attribute-based Access Control
Etc so your Apache Ranger can go in
there and your administrator coming in
there can now very easily monitor who
has what rights and what they can do and
what they can access so there's also
about Apache Knox is an application
Gateway for interacting with the rest
apis and the uis of Hadoop developers
and so we have our application
programmer interfaces our user
interfaces and we talk about rest apis
this means we're pulling this is looking
at the actual data coming in what
applications are going on so if you have
an application where people are pulling
data off of the Hadoop system or pushing
data into the dupes system that knocks
is going to be on that setup and it
delivers three groups of user-facing
services one proxy Services provides
access to Hadoop via proxying the HTTP
request to authentication Services
authentication for rest API access and
web SSO flow for user interfaces so
there's our rest and finally Client
Services client development can be done
with the scripting through DSL or using
the Knox shell classes so the first one
is if you have a website coming in and
out your HTTP request again your three
different services are what's coming in
and out of the Hadoop file system so
there's a couple of the security setups
let's go ahead and take a look at
workflow system the uzi and there's some
other ones out there Uzi is the one
that's specific to Hadoop there's also
like a zookeeper out there and some
other ones uzi's pretty good Uzi is a
workflow scheduler system to manage
Hadoop jobs and so you have a workflow
engine and a coordinator engine so it
consists of two parts what's going on
and coordinating what's going on and uh
directed acelic graph dags which
specifies a sequence of actions to be
executed these consist of workflow jobs
triggered by time and data available and
If you're not familiar with directed
acylic graphs or dags or whatever
terminology you want to throw at this
this is basically a flow chart you start
with process a the next process would be
process B when process a is done and it
might be that process C can only be done
when process d e and f are done so you
want to be able to control this you
don't want it to process the machine
learning script and then pull the data
in that you want to process it on you
want to make sure it's going in the
right order so these consist of workflow
jobs and they're triggered by time and
data availability so maybe you're
pulling stocks in the middle of the
night and once the stock is all pulled
so there's our time sequence it says Hey
they've been posted on the other
websites they post them usually after
the stock market closes the highs and
lows and everything then once that data
has been brought n you know in a Time
specifics bringing the data in a certain
time once the data is available then we
want to trigger our machine learning
script for what's going to happen next
and so we can see here we have a start
our map reduce program our action node
and it begins and either we have a
success then we notify the client of
success usually an email sent out in
successful completion or we don't have a
success we have an error notify client
of error email action node kill
unsuccessful termination and usually at
this point they say email action
notification but I'm mostly that's
usually a pager system and so you see
all the tech guys running to the server
room or wherever you know we're our
pager just went off we got to figure out
what went down you know the other one is
you just look at the next morning you go
oh let's make sure everything went
through this morning and check all your
successes with the error usually it's
sent to your pager and your emergency
call to open up in the middle of the
night log in so that concludes our basic
setup with the Hadoop ecosystem so a
quick recap on the Hadoop ecosystem we
covered going looking at the middle part
we had the Hadoop as a file system and
how it stores data across multiple
servers saves money because it's about a
tenth of the cost of using Enterprise
computers we looked at yarn cluster
resource management and how that works
to hold everything together and then we
looked at a lot of data processing how
does it process in and out of the Hadoop
System including the map and reduce
setup which is the Hadoop basic in Java
and for that we looked at data
collection and ingestion with scoop and
Flume we looked at queries using the
scripting language Pig and the SQL
queries through Hive we glanced at spark
remember spark usually comes installed
now at the Hadoop system because it does
so much of its own processing it covers
a lot of the data in real-time data
analysis setup on there we looked at how
machine learning we looked at Apache
ambari for management monitoring kind of
your security guard and traffic control
just like we have scoop and Flume which
brings data in there we looked at Kafka
and Apache storm which is for streaming
data and then we looked at Apache Ranger
and Apache Knox for security and finally
we went in through the we took a glance
at Uzi for your workflow system yarn is
the acronym for yet another resource
negotiator
yarn is a resource manager created by
separating the processing engine and the
management function of mapreduce
it monitors and manages workloads
maintains a multi-tenant environment
manages the high availability features
of Hadoop and implements security
controls
before 2012 users could write mapreduce
programs using scripting languages such
as Java Python and Ruby
they could also use Pig a language used
to transform data
no matter what language was used its
implementation depended on the mapreduce
processing model
in May 2012 during the release of Hadoop
version 2.0 yarn was introduced you are
no longer limited to working with the
mapreduce framework anymore as yarn
supports multiple processing models in
addition to mapreduce such as Spark
other features of yarn include
significant performance Improvement and
a flexible execution engine
now let's discuss yarn with the help of
an example
Yahoo was the first company to embrace
Hadoop and this became a trendsetter
within the Hadoop ecosystem
in late 2012 Yahoo struggled to handle
iterative and stream processing of data
on the Hadoop infrastructure due to
mapreduce limitations
both iterative and stream processing
were important to Yahoo in facilitating
its move from batch Computing to
continuous Computing after implementing
yarn in the first quarter of 2013
Yahoo installed more than 30 000
production nodes on spark for iterative
processing
storm for stream processing and Hadoop
for batch processing allowing it to
handle more than 100 billion events such
as clicks Impressions email content
metadata and so on per day
this was possible only after yarn was
introduced and multiple processing
Frameworks were implemented
the single cluster approach provides a
number of advantages including
higher cluster utilization where
Resources unutilized by a framework can
be consumed by another
lower operational costs because only one
do it all cluster needs to be managed
reduced data motion as there's no need
to move data between Hadoop yarn and
systems running on different clusters of
computers
the yarn infrastructure is responsible
providing computational resources such
as CPU or memory needed for application
executions
yarn infrastructure and hdfs are
completely independent
the former provides resources for
running an application while the latter
provides storage
the mapreduce framework is only one of
the many possible Frameworks that run on
yarn
the fundamental idea of mapreduce
version 2 is to split the two major
functionalities of resource management
and job scheduling and monitoring into
separate demons
yarn and its architecture
in this topic we will discuss yarn and
its architecture
three elements of yarn architecture
the three important elements of the yarn
architecture are resource manager
application master and node managers
the resource manager or RM which is
usually one per cluster is the master
server
resource manager knows the location of
the data node and how many resources
they have
this information is referred to as rack
awareness
the RM runs several Services the most
important of which is the resource
scheduler that decides how to assign the
resources
the application Master is a framework
specific process that negotiates
resources for a single application that
is a single job or a directed acyclic
graph of jobs which runs in the first
container allocated for the purpose
each application Master requests
resources from the resource manager and
then works with the containers provided
by node managers
the node managers can be many in one
cluster
they are the slaves of the
infrastructure
when it starts it announces itself to
the RM and periodically sends a
heartbeat to the RM
each node manager offers resources to
the cluster
the resource capacity is the amount of
memory and the number of V cores short
for virtual core
at runtime the resource scheduler
decides how to use this capacity
a container is a fraction of the node
manager capacity and it is used by the
client to run a program
each node manager takes instructions
from the resource manager and reports
and handles containers on a single node
in the next few screens you will see a
detailed explanation of the three
elements
the first element of yarn architecture
is resource manager
the RM mediates the available resources
in the cluster among competing
applications with the goal of Maximum
cluster utilization
it includes a plugable scheduler called
the yarn scheduler which allows
different policies for managing
constraints such as capacity fairness
and service level agreements
the resource manager has two main
components scheduler and applications
manager the scheduler is responsible for
allocating resources to various running
applications depending on the common
constraints of capacities cues and so on
the scheduler does not monitor or track
the status of the application
also it does not restart the tasks in
case of any application or Hardware
failures
the scheduler performs its function
based on the resource requirements of
the applications
it does so based on the abstract notion
of a resource container that
incorporates elements such as memory CPU
disk and Network
the scheduler has a policy plugin which
is responsible for partitioning the
cluster resources among various cues and
applications
the current mapreduce schedulers such as
capacity scheduler and the fare
scheduler are some examples of the
plugin
the capacity scheduler supports
hierarchical cues to enable a more
predictable sharing of cluster resources
the application manager is an interface
which maintains a list of applications
that have been submitted currently
running or completed
the applications manager is responsible
for accepting job submissions
negotiating the first container for
executing the application-specific
application master and restarting the
application Master container on failure
let's discuss how each component of the
resource manager work together
the resource manager communicates with
the clients through an interface called
The Client Service
a client can submit or terminate an
application and gain information about
the scheduling queue or cluster
statistics through the client service
administrative requests are served by a
separate interface called the admin
service through which operators can get
updated information about the cluster
operation
in parallel the resource tracker service
receives node heartbeats from the node
manager to track new or decommissioned
nodes
the NM liveliness Monitor and nodes list
manager keep an updated status of which
nodes are healthy so that the scheduler
and the resource tracker service can
allocate work appropriately
the application Master service manages
application Masters on all nodes keeping
the scheduler informed
the am liveliness monitor keeps a list
of application managers and their last
heartbeat times to let the resource
manager know what applications are
healthy on the cluster
any application master that does not
send a heartbeat within a certain
interval is marked as dead and
rescheduled to run on a new container
before Hadoop 2.4 the resource manager
was the single point of failure in a
yarn cluster
the high availability or h a feature
adds redundancy in the form of an active
standby resource manager pair to remove
the single point of failure
resource met h a is realized through the
active standby architecture
at any point of time one of the RMS is
active and one or more RMS are in
standby mode waiting to take over should
anything happen to the active
the trigger to transition to active
comes from either the admin through the
command line interface or through the
integrated failover controller the RMS
have an option to embed the
zookeeper-based active standby elector
to decide which RM should be active
when the active goes down or becomes
unresponsive another RM is automatically
elected to be the active
note that there is no need to run a
separate zkfc Daemon like in hdfs
because the active standby elector
embedded in RMS act as a failure
detector and a leader elector the second
element of yarn architecture is the
application master
the application master in yarn is a
framework specific Library which
negotiates resources from the RM and
works with the node manager or managers
to execute and monitor containers and
their resource consumption
while an application is running the
application manager manages the
application lifecycle Dynamic
adjustments to Resource consumption
execution flow faults and it provides
status and metrics
the application Master is architected to
support a specific framework and can be
written in any language
it uses extensible communication
protocols with the resource manager and
the node manager
the application Master can be customized
to extend the framework or run any other
code
because of this the application Master
is not considered trustworthy and is not
run as a trusted service
in reality every application has its own
instance of an application master
however it's feasible to implement an
application Master to manage a set of
applications for example
an application Master for pig or Hive to
manage a set of mapreduce jobs
the third element of yarn architecture
is the node manager
when a container is leased to an
application the node manager sets up the
container's environment
the environment includes the resource
constraints specified in the lease and
any kind of dependencies such as data or
executable files the node manager
monitors the health of the node
reporting to the resource manager when a
hardware or software issue occurs so
that the scheduler can divert resource
allocations to healthy nodes until the
issue is resolved
the node manager also offers a number of
services to Containers running on the
Node such as a log aggregation service
the node manager runs on each node and
manages the activities such as container
lifecycle management container
dependencies container leases node and
container resource usage node health and
log management and reports node and
container status to the resource manager
a yarn container is a collection of
specific set of resources to use in
certain amounts on a specific node
it is allocated by the resource manager
on the basis of the application
the application manager presents the
container to the node manager on the
Node where the container has been
allocated thereby granting access to the
resources now let's discuss how to
launch the container
the application manager must provide a
container launch context or CLC
this includes information such as
environment variables dependencies on
the requirement of data files or shared
objects brought to the launch
security tokens and the command to
create the process to launch the
application
the CLC supports the application Master
to use containers
this helps to run a variety of different
kinds of work from simple shell scripts
to applications to Virtual operating
system
owing to yarn's generic approach a
Hadoop yarn cluster runs various
workloads
this means a single Hadoop cluster in
your data center can run mapreduce storm
spark Impala and more
broadly there are five steps involved in
yarn to run an application
first the client submits an application
to the resource manager then the
resource manager allocates a container
then the application Master contacts the
related node manager
then the related node manager launches
the container and finally the container
executes the application master
in the next few screens you will learn
about each step in detail
users submit applications to the
resource manager by typing the Hadoop
jar command
the resource manager maintains the list
of applications on the cluster and
available resources on the Node manager
the resource manager determines the next
application that receives a portion of
the cluster resource
the decision is subject to many
constraints such as Q capacity Access
Control lists and fairness
when the resource manager accepts a new
application submission one of the first
decisions the scheduler makes is
selecting a container
then the application Master is started
and is responsible for the entire
lifecycle of that particular application
first it sends resource requests to the
resource manager to ask for containers
to run the application's tasks
a resource request is simply a request
for a number of containers that satisfy
resource requirements such as the
following
amount of resources expressed as
megabytes of memory and CPU shares
preferred location specified by hostname
or rack name
priority within this application and not
across multiple applications
the resource manager allocates a
container by providing a container ID
and a hostname which satisfies the
requirements of the application Master
after a container is allocated the
application Master asks the node manager
managing the host on which the container
was allocated to use these resources to
launch an application specific task
this task can be any process written in
any framework such as a mapreduce task
the node manager does not monitor tasks
it only monitors the resource usage in
the containers
for example it kills a container if it
consumes more memory than initially
allocated
throughout its life the application
Master negotiates containers to launch
all the tasks needed to complete its
application
it also monitors the progress of an
application and its tasks restarts
failed tasks in newly requested
containers
and reports progress back to the client
that submitted the application
after the application is complete the
application Master shuts itself and
releases its own container
though the resource manager does not
monitor the tasks within an application
it checks the health of the application
master
if the application Master fails it can
be restarted by the new resource manager
in a new container
thus the resource manager looks after
the application Master while the
application Master looks after the tasks
Hadoop includes three tools for yarn
Developers
yarn web UI
huge job browser
yarn command line
these tools enable developers to submit
Monitor and manage jobs on the yarn
cluster
yarn web UI runs on 8088 Port by default
it also provides a better view than Hue
however you can't control or configure
from yarn web UI you will see a
demonstration on yarn web UI in the
later screen the Hue job browser allows
you to monitor the status of a job kill
a running job and view logs you will
understand this by viewing a
demonstration in the later screen
most of the yarn commands are for the
administrator rather than the developer
a few useful commands for developers are
as follows
to list all commands of yarn type the
command line shown on screen
to print the version
type the command line shown on the
screen
to view logs of a specified application
ID
type the command line shown on your
screen
using yarn web UI Hue job browser and
yarn command line
in this demo you will learn how to work
on yarn web UI Hue job browser and yarn
command line
you will use the file
wordcount.py which is a python file
this file will calculate the number of
times each word appears
you will view how to execute this file
with the help of yarn
type spark hyphen submit
let's define the master as yarn hyphen
client followed by the name of the
Python file
you need each word count which is
present in the word directory loudacre
KB
click job browser
while the program is running on the
terminal let's see how the steps
executing in yarn appear in hue
you will notice the python word count
status as running
it will also show you the status of map
and reducer
click ID
you can also click on ID option to view
more details of a job
you can also click on kill option to end
this job from UI
once you enter the application ID you
can view the running metadata of that
yarn
in this page you can also view the
information such as start time end time
and the amount of memory per second that
a running program consumes
let's go back to the terminal
you can check similar details in web
user interface which you just viewed
from yarn resource manager
you need to navigate to localhost colon
8088 cluster
you will again notice the running ID
click this ID to view more details
you will be able to view information
such as user name of the program
application type current state of the
running program and a few additional
information you can also scroll through
the accepted jobs currently running jobs
finished jobs and a few more details
so this is another web UI page in which
you can monitor your yarn progress
once this program is complete you can
view the output in the terminal
it displays the number of times each
word appears
this brings you to the end of this demo
in this demo you learned the steps to
calculate the word count for a file
you have also learned the steps to
monitor your yarn progress in a web user
interface take a look at simply learns
postgraduate program in data engineering
offered in collaboration with per
University and IBM elevate your career
with this applied Learning Journey
enroll now and get certified get ahead
what is hbase well hbase is a database
management system designed in 2007 by
powerset which is now a Microsoft
company
hspace rests on top of hdfs and enables
real-time analysis of data
well it can store a huge amount of data
in a tabular format for extremely fast
reads and writes
hspace is mostly used in a scenario that
requires regular and consistent
inserting and overwriting of data even
in a hdfs environment
why hbase we know that hdfs stores
processes and manages large amounts of
data efficiently
however it performs only batch
processing where the data is accessed in
a sequential manner
this means one has to insert the entire
data set for even the simplest of jobs
therefore a solution is required to
access read or write data anytime
regardless of its sequence in the
clusters of data
hspace is modeled after Google's big
table which is a distributed storage
system for structured data
just as bigtable leverages the
distributed data storage provided by the
Google file system Apache hbase provides
bigtable like capabilities on top of
Hadoop and hdfs
what are some of the characteristics of
hbase
hspace is a type of nosql database and
is classified as a key Value Store
some characteristics of hbase are values
which are identified with the key
both key and values are byte array of
type
which means binary formats can be stored
easily
values are stored in key orders and can
be quickly accessed by their keys
and it represents a database in which
tables have no schema column families
and not columns are defined at the time
of table creation nosql is a form of
unstructured storage this means that
nosql databases do not have a fixed
table structure like the ones found in
relational databases
as you see on the left a structured
database is very heavily dependent on
rows columns and tables
the unstructured database has a wide
variety of different types of data
why nosql
well with the explosion of social media
sites such as Facebook and Twitter the
demand to manage large data has grown
tremendously
nosql databases have resolved the
challenges that were faced in storing
managing analyzing And archiving Big
Data
also these databases have gained a lot
of popularity due to high performance
high scalability and ease of access
these databases may include key value
pair databases document databases and
column based data stores
types of nosql
there are basically four types of nosql
databases key value document column and
graph
the key value database
has a big hash table of keys and values
oracle's nosql a redis server and
scalaris are examples of key Value Store
databases
document-based databases this type
stores documents made up of tagged
elements examples include mongodb
couchdb orientdb and Raven DB
column-based databases each storage
block contains data from only one column
examples are bigtable Cassandra hbase
and hypertable
and lastly graph based database
this is a network database that uses
nodes to represent and store data
examples are neo4j infogrid infinite
graph and flock DB
the availability of choices in nosql
databases has its own advantages and
disadvantages
the advantage is it allows you to choose
a design according to your system
requirements
however because you have to make a
choice based on requirements there is
always a chance that the same database
product may not be used properly
RDMA OS versus nosql
data storage
for data storage in an rdbms data is
stored in a relational model in tabular
format with numerous rows and columns
nosql comprises a host of different
databases with different data storage
models
for schemas each record in an rdbms
follows a fixed schema
The Columns are defined and locked
before data entry
on the other hand schema is in nosql are
Dynamic you can add new columns at any
time
performance
relational databases are not optimized
for high performance in applications
where massive data is stored and
processed frequently
no SQL databases perform much better in
scenarios with large amounts of data
and scalability
rdbms supports vertical scaling
to handle more data rdbms typically
requires a bigger server
however this will increase the cost and
is much more difficult to do than
horizontal scaling that is supported by
nosql
you can scale a horizontal nosql across
multiple servers
multiple servers are cheap commodity
Hardware or Cloud instances that make
scaling cost effective horizontally much
more efficient compared to Vertical
scaling
and reliability relational databases are
highly consistent and Reliant
most nosql databases do not support
reliability features such as atomicity
consistency isolation and durability
which are natively supported by a
relational database hspace real life
connect
Facebook's messenger platform needs to
store over 135 trillion messages a month
where do they store such data the answer
is hbase
Facebook chose hbase because it needed a
system that could handle two types of
data patterns one an ever-growing data
set that is rarely accessed and an
ever-growing data set that is highly
volatile
you read what's in your inbox and then
you rarely look at it again
hbase architecture
hbase has two types of nodes master and
region server
there is only one master node running at
a time
whereas there can be one or more region
servers
the high availability of the master node
is maintained by zookeeper a service for
distributed systems
the master node manages cluster
operations such as assignment load
balancing and splitting
it is not a part of the read or write
path of data
the region server it has one or more
region servers running at a time post
tables performs reads and writes of data
clients communicate with the region
server in order to read and write their
data
a region in hbase is a subset of a
table's rows
the master node detects the status of
region servers and assigns regions to it
hbase components
the image represents the h-space
components which include hbase master
and at the bottom multiple region
servers
the hbase master is responsible for the
management of the schema which is stored
in hdfs
region servers act like availability
servers that enable the maintenance of a
part of the complete data stored in hdfs
based on the requirements of the user
the region servers perform this task by
using the H file and the right ahead log
or wow service
the region servers always stay in sync
with the hbase master
it is the Zookeeper that makes the
region servers perform a stable sync
with the hbase Master Storage model of
hbase
the two major components of the storage
model
are partitioning and persistence and
data availability
partitioning is one part of the storage
model where a table is horizontally
partitioned into regions
each region is composed of a sequential
range of keys and then managed by a
region server
a region server May hold multiple
regions
persistence and data availability are
also important components of the storage
model
hbase soars its data in hdfs
does not replicate region servers and
relies on hdfs replication for data
availability
the region data is first cached in
memory
updates and reads are then served from
the in-memory cache which is called the
mem store
periodically the in-memory cache or
memostore is flushed back to hdfs
the wal stored in hdfs is used for the
durability of the updates
row distribution of data between region
servers
the image describes the distribution of
rows in a structured database using
hbase it shows how the data is sliced
and then maintained in individual region
servers depending on the requirement of
the user
this type of distribution ensures
availability of data for a specific user
as you can see with the data A1 through
z55 some data is split based on the
index value for example null to A3 is in
the first region server
A3 to F34 is then stored in the second
region server and so on
data storage in hbase
data is stored in files called H files
or store files that are usually saved in
hdfs
an H file is a key value map generated
due to the mapreduce operations that are
performed by ado
when data is added is written to the wow
and stored in memory
this in-memory data store is called the
mem store
H files are immutable since hdfs does
not support updates to an existing file
to control the number of H files and
keep the cluster well balanced hbase
periodically performs data compactions
across the hdfs cluster the data model
the features of the data model in hbase
are as follows
in hbase all tables are sorted by row
keys
at a time of table creation you need to
Define its column families only
each family may consist of any number of
columns and each column consists of any
number of versions
columns exist only when inserted however
nulls are free
columns within a family are sorted and
stored together
everything except table names is stored
as a byte array
a row key a column family with columns
and a timestamp with version will
identify a specific row value
the example shows two column families
see it column family one and column
family two
as you can see there's no restriction to
adding additional columns to column
family one
when you get to column Family 2 you will
have the same ability to continuously
expand as many columns as you like
data mode features
the data model has many other features
first identifier of the data model is a
row key
column families are associated with
column qualifiers each row has a
timestamp and an Associated value
in the example you can see that each
qualifier supports both a time stamp and
a value this allows versioning of your
data within the database
when should I use hbase
hbase is used when you have enough data
in millions or even billions of rows it
can be used when you have sufficient
commodity Hardware with at least five
nodes
and developers can use hbase for random
selects and range scans by key
they can also utilize hbase when you
have a variable schema hbase versus
rdbms
hbase provides advantages in comparison
to the relational database management
system
hbase allows automatic partitioning as
compared to manually partitioned rdbms's
hbase can scale linearly and
automatically with new nodes
however rdbms primarily scales
vertically by adding more Hardware
resources
furthermore as part of the Hadoop
ecosystem hbase uses commodity Hardware
while rdbms relies on expensive servers
hbase has mechanisms for fault tolerance
at rdbms may or may not have
hbase leverages batch processing with
mapreduce distributed processing
whereas in rdbms relies on multiple
threads or processes rather than
mapreduce distributed processing
connecting to hbase
you can connect to hspace using any of
the following meta
the Java application programming
interface or API which can be used to
conduct usual operations such as get
scan put and delete
you can also use Thrift or rest Services
which can be used by non-java clients
jruby a built-in convenience shell for
performing a majority of operations
including admin functions from the
command line of Hive Pig H catalog or
hue hbase shell commands
some of the commands that can be used
from the hbase shell include creation
of tables using the create command
in this case we see a create table where
we're passing the table name from a
dictionary of specifications per column
family and a dictionary of table
configurations which is optional
you can use the describe command to get
information about the names table
and you can start the disabling of a
name table by typing in disable and the
table name
you can also drop the name table
note that the table must first be
disabled prior to doing a drop
and you can list all tables in hbase you
can use the optional regular expression
parameter to filter the output of the
list command
additional commands include things like
count for counting the number of rows in
a table or delete for deleting a cell
value or get for getting the contents of
a row or cell and put for putting a cell
value and scan for scanning a table's
values what is data science let's start
with some of the common definitions
that's doing the rounds some say that
data science is a powerful new approach
for making discoveries from data
others term it as an automated way to
analyze enormous amounts of data and
extract information from it
still others refer to it as a new
discipline which combines aspects of
Statistics mathematics programming and
visualization to gain insights
now that you have looked at some of its
definitions let's learn more about data
science
when domain expertise and scientific
methods are combined with technology we
get data science which enables one to
find solutions for existing problems
let's look at each of the components of
data science separately the first
component is domain expertise and
scientific methods data scientists
should also be domain experts as they
need to have a passion for data and
discover the right patterns in them
traditionally domain experts like
scientists and statisticians collected
and analyzed the data in a laboratory
setup or a controlled environment
the data was then subject to relevant
laws or mathematical and statistical
models to analyze the data set and
derive relevant information from it for
instance they use the models to
calculate the mean median mode standard
deviation and so on of a data set it
helped them test their hypothesis or
create a new one
in the next slide we will see how data
science technology has now made this
process faster and more efficient but
before we do that let's understand the
different types of data analysis an
important aspect of data science
data analysis can either be descriptive
where one studies a data set to explain
what happened or be predictive where one
creates a model based on existing
information to predict the outcome and
behavior
it can also be prescriptive where one
suggests the action to be taken in a
given situation using the collected
information
we now have access to tools and
techniques that process data and extract
the information we need for instance
there are data processing tools for data
wrangling we have new and flexible
programming languages that are more
efficient and easier to use
with the creation of operating systems
that support multiple OS platforms it's
now easier to integrate systems and
process Big Data
application designs and extensive
software libraries help develop more
robust scalable and data-driven
applications
data scientists use these Technologies
to build data models and run them in an
automated fashion to predict the outcome
efficiently
this is called machine learning which
helps provide insights into the
underlying data
they can also use data science
technology to manipulate data extract
information from it and use it to build
tools applications and services but
technological skills and domain
expertise alone without the right
mathematical and statistical knowledge
might lead data scientists to find
incorrect patterns and convey the wrong
information
now that you have learned what data
science is it will be easier to
understand what a data scientist does
data scientists start with a question or
a business problem
then they use data acquisition to
collect data sets from The Real World
the process of data wrangling is
implemented with data tools and modern
technologies that include data cleansing
data manipulation data Discovery and
data pattern identification
the next step is to create and train
models for machine learning
they then design mathematical or
statistical models
after designing a data model it's
represented using data visualization
techniques
the next task is to prepare a data
report
after the report is prepared they
finally create data products and
services
let us now look at the various skills a
data scientists should have
data scientists should ask the right
questions for which they need domain
expertise the Curiosity to learn and
create Concepts and the ability to
communicate questions effectively to
domain experts
data scientists should think
analytically to understand the hidden
patterns in a data structure
they should Wrangle the data by removing
redundant and irrelevant data collected
from various sources
statistical thinking and the ability to
apply mathematical methods are important
traits for a data scientist
data should be visualized with graphics
and proper storytelling to summarize and
communicate the analytical results to
the audience
to get these skills they should follow a
distinct roadmap it's important they
adopt the required tools and techniques
like Python and its libraries
they should build projects using real
world data sets that include data.gov
NYC open data Gap minder and so on
they should also build a data-driven
applications for Digital Services and
data products
scientists work with different types of
data sets for various purposes now that
big data is generated every second
through different media the role of data
science has become more important
so you need to know what big data is and
how you are connected to it to figure
out a way to make it work for you
every time you record your heartbeat
through your phone's biometric sensors
post or tweet on The Social Network
create any blog or website switch on
your phone's GPS Network upload or view
an image video or audio in fact every
time you log into the internet you are
generating data about yourself your
preferences and your lifestyle
big data is a collection of these and a
lot more data that the world is
constantly creating in this age of the
internet of things or iot big data is a
reality and a need
big data is usually referenced by three
vs volume velocity and variety
volume refers to the enormous amount of
data generated from various sources
big data is also characterized by
velocity huge amounts of data flow at a
tremendous speed from different devices
sensors and applications
to deal with it an efficient and timely
data processing is required
variety is the third V of Big Data
because big data can be categorized into
different formats like structured
semi-structured and unstructured
structured data is usually referenced to
as rdbms data which can be stored and
retrieved easily through sqls
semi-structured data are usually in the
form of files like XML Json documents
and nosql database
text files images videos or multimedia
content are examples of unstructured
data
in short big data is a very large
information database usually stored on
distributed systems or machines
popularly referred to as Hadoop clusters
but to be able to use this database we
have to find a way to extract the right
information and data patterns from it
that's where data science comes in data
science helps to build information
driven Enterprises
let's go on to see the applications of
data science in different sectors
social network platforms such as Google
Yahoo Facebook and so on collect a lot
of data every day which is why they have
some of the most advanced data centers
spread across the world
having data centers all over the world
and not just in the US help these
companies serve their International
customers better and faster without any
network latency they also help them deal
effectively with the enormous amount of
data so what do all these different
sectors do with all this big data
their team of data scientists analyze
all the raw data with the help of modern
algorithms and data models to turn it
into information
they then use this information to build
Digital Services data products and
information driven webs
now let's see how these products and
services work we'll first look at
LinkedIn let's suppose that you are a
data scientist based in New York city so
it's quite likely that you would want to
join a group or build connections with
people related to data science in New
York City
now what LinkedIn does with the help of
data science is that it looks at your
profile your posts and likes the city
you are from the people you are
connected to and the groups you belong
to then it matches all that information
with its own database to provide you
with information that is most relevant
to you this information could be in the
form of news updates that you might be
interested in Industry connections or
professional groups that you might want
to get in touch with or even job
postings related to your field and
designation
these are all examples of data services
let's now look at something that we use
every day Google's search engine
Google search engine has the most unique
search algorithm which allows machine
learning models to provide relevant
search recommendations even as the user
types in his or her query
this feature is called autocomplete it
is an excellent example of how powerful
machine learning can be
there are several factors that influence
this feature
the first one is query volume Google's
algorithms identify unique and
verifiable users that search for any
particular keyword on the web
based on that it builds a query volume
for instance Republican debate 2016
Ebola threat CDC or the center of
Disease Control and so on are some of
the most common user queries
another important factor is a
geographical location
the algorithms tag a query with the
locations from where it is generated
this makes a query volume location
specific it's a very important feature
because this allows Google to provide
relevant search recommendations to its
user based on his or her location
and then of course the algorithms
consider the actual keywords and phrases
that the user types in
it takes up those words and crawls the
web looking for similar instances
the algorithms also try to filter or
scrub out inappropriate content
for instance sexual violent or
terrorism-related content hate speeches
and legal cases are scrubbed out from
the search recommendations
but how does data science help you today
even the healthcare industry is
beginning to tap into the various
applications of data science to
understand this let's look at wearable
devices these devices have biometric
sensors and a built-in processor to
gather data from your body when you are
wearing them
they transmit this data to the big data
analytics platform via the iot Gateway
ideally the platform collects hundreds
of thousands of data points and the
collected data is ingested into the
system for further processing
the big data analytics platform applies
data models created by data scientists
and extracts the information that is
relevant to you
it sends the information to the
engagement dashboard where you can see
how many steps you want what your heart
rate is over a period of Time how good
your sleep was how much calories you
burned and so on
knowing such details would help you to
set personal goals for a healthy
lifestyle and reduce overall health care
and insurance costs it would also help
your doctor record your vitals and
diagnose any issue
the finance sector can easily use data
science to help it function more
efficiently
suppose a person applies for a loan
the loan manager submits the application
to the Enterprise infrastructure for
processing
the analytics platform applies data
models and algorithms and creates an
engagement dashboard for the loan
manager
the dashboard would show the applicant's
credit report credit history amount if
approved and risks associated with him
or her
the loan manager can now easily take a
look at all the relevant information and
decide whether the loan can be approved
or not
governments across different countries
are gradually sharing large data sets
from various domains with the public
this kind of transparency makes the
government seem more trustworthy it
provides the country data that can be
used to prepare itself for different
types of issues like climate change and
Disease Control
it also helps encourage people to create
their own digital products and services
the US government hosts and maintains
data.gov a website that offers
information about the federal government
it provides access to over 195 000 data
sets across different sectors
the U.S government has kicked off a
number of strategic initiatives in the
field of data science that includes U.S
digital service and open data
we have seen how data science can be
applied across different sectors
let's now take a look at the various
challenges that a data scientist faces
in the real world while dealing with
data sets data quality the quality of
data is mostly not up to the set
standards you will usually come across
data that is inconsistent inaccurate and
complete not in the desirable format and
with anomalies
integration
data integration with several Enterprise
applications and systems is a complex
and painstaking task
unified platform data is distributed to
Hadoop distributed file system or hdfs
from various sources to ingest process
analyze and visualize huge data sets
the size of these Hadoop clusters can
vary from few nodes to thousand nodes
the challenge is to perform analytics on
these large data sets efficiently and
effectively
this is where python comes into play
with its powerful set of libraries
functions modules packages and
extensions
python can efficiently tackle each stage
of data analytics that includes data
acquisition python libraries such as
Scrappy comes handy here
data wrangling python data frames are
very efficient in handling large data
sets and makes data wrangling easier
with its powerful functions
explore
matplotlib libraries are very rich when
it comes to data exploration
model
scikit learns statistical and
mathematical functions to help to build
models for machine learning
visualization modern libraries such as
Voca creates very intuitive and
interactive visualization
its huge set of libraries and functions
make big data analytics seem easy and
hence solves the bigger problem
python applications and programs are
portable and helps them scale out on any
big data platform
python is an open source programming
language that lets you work quickly and
integrate systems more effectively
now that we have talked about how the
python libraries help the different
stages of data analytics let's take a
closer look at these libraries and how
they support different aspects of data
science
numpy or numerical python is the
fundamental package for scientific
computing
scipy is the core of scientific
Computing libraries and provides many
user-friendly and efficiently designed
numerical routines
matplotlib is a python 2D plotting
Library which produces publication
quality figures in a variety of hard
copy formats and interactive
environments across platforms
scikit-learn is built on numpy scipy and
matplotlib for data mining and data
analysis
pandas is a library providing high
performance easy to use data structures
and data analysis tools for python
all these libraries modules and packages
are open source and hence using them is
convenient and easy
there are numerous factors which
positions python well and makes it the
tool for data science
python is easy to learn it's a general
purpose function and object-oriented
programming language
as python is an open source programming
language it is readily available easy to
install and get started it also has a
large presence of Open Source Community
for software development and support
Python and its tools enjoy
multi-platform support
applications developed with pycon
integrate easily with other Enterprise
systems and applications
there are a lot of tools platforms and
products in the market from different
vendors as they offer great support and
services
Python and its libraries create unique
combinations for data science because of
all these benefits it's usually popular
among account emissions mathematicians
statisticians and technologists
python is supported by well-established
data platforms and processing Frameworks
that help it analyze data in a simple
and efficient way
Enterprise Big Data platform
Cloudera is the Pioneer in providing
enterprise-ready Hadoop Big Data
platform and supports python
hortonworks is another Hadoop Big Data
platform provider and supports python
mapreduce map R is also committed to
Python and provides the Hadoop Big Data
platform
big data processing framework
mapreduce spark and Flink provides very
robust and unique data processing
framework and support python
Java Scala and python languages are used
for big data processing framework
but to access Big Data you have to use a
big data platform which is a combination
of the Hadoop infrastructure also known
as Hadoop distributed file system or
hdfs and an analytics platform
Hadoop is a framework that allows data
to be distributed across clusters of
computers for faster cheaper and
efficient computing
it's completely developed and coded in
Java one of the most popular analytics
platforms is Spark
it easily integrates with hdfs
it can also be implemented as a
standalone analytics platform and
integrated with multiple data sources
it helps data scientists perform their
work more efficiently
spark is built using Scala
since there is a disparity in the
programming language that data
scientists use and that of the Big Data
platform it impedes data access and Flow
as python is a data scientist's first
language of choice both Hadoop and Spark
provide python apis that allow easy
access to the Big Data platform
consequently a data scientist need not
learn Java or Scala or any other
platform-specific data languages and can
instead focus on performing data
analytics
there are several motivations for python
Big Data Solutions
big data is a continuously evolving
field which involves adding new data
processing Frameworks that can be
developed using any programming language
moreover new innovation and research is
driving the growth of Big Data Solutions
and platform providers
it would be difficult for data
scientists to focus on analytics if they
have to constantly upgrade themselves on
information or under the hood
architecture or implementation of the
platform therefore it's important to
keep the entire data science platform
and any language agnostic to simplify a
data scientist's job
consequently almost all major vendors
solution providers and data processing
framework developers are providing
python apis this allows a data scientist
to perform big data analytics using only
python rather than learning other
languages like Java or Scala to help
them work on the big data platform
let's look at an example and understand
how data is stored across hadoop's
distributed clusters
big data is generated from different
data sources a large file usually
greater than 100 megabytes gets routed
from a name node to data nodes
name nodes hold the metadata information
about the files stored on data nodes it
stores the address and information of a
block of file and the data node
associated with it
data nodes hold the actual data blocks
the file is split into multiple smaller
files usually of 64 megabytes or 128
megabyte size
it's then copied to multiple physical
servers the smaller files are also
called file blocks one file block gets
replicated to different servers the
default replication factor is 3 which
means a single file block gets copied at
least three times on different servers
or data nodes
there is also a secondary name node
which keeps a backup of all the metadata
information stored on the main or
primary node this node can be used if
and when the main name node fails
now that you have understood a little
about hdfs let's look at the second core
component of Hadoop mapreduce the
primary framework of the hdfs
architecture
a file is split into three blocks as
split 0 split one and split two
when a request comes in to retrieve the
information the mapper task is executed
on each data node that contains the file
blocks
the mapper generates an output
essentially in the form of key value
pairs that are sorted copied and merged
once the mapper task is complete the
reducer works on the data and stores the
output on hdfs
this completes the mapreduce process
let's discuss the mapreduce functions
mapper and reducer in detail
the mapper
Hadoop ensures that mappers run locally
on the nodes which hold a particular
portion of the data to avoid the network
traffic
multiple mappers run in parallel and
each mapper processes a portion of the
input data the input and output of the
mapper are in the form of key value
pairs note that it can either provide
zero or more key value pairs as output
the reducer after the map phase all
intermediate values for an intermediate
key are combined into a list which is
given to a reducer all values associated
with a particular intermediate key are
directed to the same reducer this step
is known as Shuffle and sort there may
be a single reducer or multiple reducers
note that the reducer also provides
outputs in the form of zero or more than
one final key value pairs
these values are then returned to hdfs
the reducer usually emits a single key
value pair for each input key
you have seen how mapreduce is critical
for hdfs to function a good thing is you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python with the help of Hadoop streaming
API
Hadoop streaming acts like a bridge
between your python code and the Java
based hdfs and lets you seamlessly
access Hadoop clusters and execute
mapreduce tasks
you have seen how mapreduce is critical
for hdfs to function thankfully you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python
shown here are some user-friendly python
functions that are written for the
mapper class
suppose we have the list of numbers we
want to square
we have the square function defined as
shown on the screen
we can call the map function with a list
and a function which is to be executed
on each item in that list
the output of this process is as shown
on the screen
reducer can also be written in Python
here we would like to sum the squared
numbers of the previous map operation
this can be done using the sum operation
as shown on the screen
we can now call the reduce function with
the list of data which is to be
aggregated and aggregator function in
our case sum is used for this purpose
Big Data analysis requires a large
infrastructure Cloudera provides
enterprise-ready Hadoop Big Data
platform which supports python as well
to execute Hadoop jobs you have to first
install Cloudera
it's preferable to install Cloud era's
virtual machine on a Unix system as it
functions best on it
to set up the Cloudera Hadoop
environment visit the Cloudera link
shown here
select quick start download for CDH 5.5
and VMware from the drop down lists
click the download now button
once the VM image is downloaded please
use 7-Zip to extract the files to
download and install it visit the link
shown on screen
Cloudera VMware has some system
prerequisites
the 64-bit virtual machine requires a
64-bit host operating system or Os and a
virtualization product that can support
a 64-bit guest OS
to use a VMware VM you must use a player
compatible with workstation 8.x or
higher such as player 4.x or higher or
Fusion 4.x or higher
you can use older versions of
workstation to create a new VM using the
same virtual disk or vmdk file but some
features in VMware tools will be
unavailable
the amount of ram required will vary
depending on the runtime option you
choose
to launch the VMware Player you will
either need a VMware Player for Windows
and Linux or VMware Fusion for Mac
so please visit the VMware link shown on
screen to download the relevant VMware
Player Now launch the VMware Player with
the Cloudera VM
the default username and password is
Cloudera
click the terminal icon as shown here it
will launch the Unix terminal for Hadoop
hdfs interaction
to verify that the Unix terminal is
functioning correctly type in PWD which
will show you the present working
directory you can also type in LS space
hyphen LRT to list all the current files
folders and directories these are some
simple unix commands which will come in
handy later while you are implementing
mapreduce tasks
you have seen how the Hadoop distributed
file system works along with mapreduce
the data is written on and read by disks
mapreduce jobs require a lot of disk
read and write operations which is also
known as disk IO or input and output
reading and writing to a disc is not
just expensive it can also be slow and
impact the entire process and operation
this is specifically true for iterative
processes Hadoop is built for right once
read many type of jobs which means it's
best suited for jobs that don't have to
be updated or accessed frequently but in
several cases particularly in analytics
and machine learning users need to write
and rewrite commands to access and
compute on the same data more than once
every time such a request is sent out
mapreduce requires that data is read and
or written onto disks directly note that
though the time to access or write on
disks is measured in milliseconds when
you are dealing with large file sizes
the time Factor gets compounded
significantly this makes the process
highly time consuming
in contrast Apache spark uses resilient
distributed data sets or rdds to carry
out such computations
rdds allow data to be stored in memory
which means that every time users want
to access the same data a disk i o
operation is not required they can
easily access data stored in the cache
accessing the cache or Ram is much
faster than accessing disks for instance
if disk access is measured in
milliseconds in-memory data access is
measured in sub milliseconds this
radically reduces the overall time taken
for iterative operations on large data
sets in fact programs on spark run at
least 10 to 100 times faster than on
mapreduce that's why spark is gaining
popularity among most data scientists as
it is more time efficient when it comes
to running analytics and machine
learning computations
one of the main differences in terms of
Hardware requirements for mapreduce and
Spark is that while mapreduce requires a
lot of servers and CPUs spark
additionally requires a large and
efficient Ram
let's understand resilient distributed
data sets in detail as you have already
seen the main programming approach of
spark is rdd
rdds are fault tolerant collections of
objects spread across a cluster that you
can operate on in parallel they are
called fault tolerant because they can
automatically recover from machine
failure
you can create an rdd either by copying
the elements from an existing collection
or by referencing a data set stored
externally say on an hdfs
rdds support two types of operations
Transformations and actions
Transformations use an existing data set
to create a new one for example map
creates a new rdd containing the results
after passing the elements of the
original data set through a function
some other examples of Transformations
are filter and join
actions compute on the data set and
return the value to the driver program
for example reduce Aggregates all the
rdd elements using a specified function
and returns this value to the driver
program
some other examples of actions are count
collect and Save
it's important to note that if the
available memory is insufficient then
spark writes the data to disk
here are some of the advantages of using
spark it's almost 10 to 100 times faster
than Hadoop mapreduce
it has a simple data processing
framework it provides interactive apis
for python that allow faster application
development
it has multiple tools for complex
analytics operations these tools help
data scientists perform machine learning
and other analytics much more
efficiently and easily than most
existing tools
it can easily be integrated with the
existing Hadoop infrastructure
Pi spark is the python API used to
access the spark programming model and
perform data analysis
let's take a look at some transformation
functions and action methods which are
supported by pi spark for data analysis
these are some common transformation
functions
map returns rdd formed by passing data
elements from The Source data set
filter
returns rdd based on selected criteria
flatmap
s items present in the data set and
returns a sequence
reduced by key
returns key value pairs where values for
each key is aggregated by a given
reduced function
let's now look at some common action
functions
collect returns all elements of the data
set as an array
count Returns the number of elements
present in the data set
first Returns the first element in the
data set
take Returns the number of elements as
specified by the number in the
parentheses
spark context or SC is the entry point
to spark for the spark application and
must be available at all times for data
processing
there are mainly four components in
spark tools
spark SQL it's mainly used for querying
the data stored on hdfs as a resilient
distributed data set or rdd in spark
through integrated apis in Python Java
and Scala
spark streaming it's very useful for
data streaming process and where data
can be read from various data sources
ml lib it's mainly used for machine
learning processes such as supervised
and unsupervised learning
graph x it can be used to process or
generate graphs with rdds
let's set up the Apache spark
environment and also learn how to
integrate spark with Jupiter notebook
first visit the Apache link and download
Apache spark to your system
now use 7-Zip software and extract the
files to your system's local directory
to set up the environment variables for
spark first set up the user variables
click new and then enter spark home in
the variable name and enter the spark
installation path as variable value
now click on the path and then click new
and enter the spark bin path from the
installed directory location
now let's set up the pi spark notebook
specific variables
this will integrate The Spark engine
with jupyter notebook
type in pi spark it will launch a
jupyter notebook after a while
create a python notebook and type in SC
command to check the spark context
take a look at simply learns
postgraduate program in data engineering
offered in collaboration with per
University and IBM elevate your career
with this applied Learning Journey
enroll now and get certified get ahead
we have some list of questions and an
explanation of those so that you can be
well prepared for your Hadoop interviews
now let's look at some general Hadoop
questions so what are the different
vendor specific distributions of Hadoop
now all of you might be aware that
Hadoop or Apache Hadoop is the core
distribution of Hadoop and then you have
different vendors in the market which
have packaged the Apache Hadoop in a
cluster management solution which allows
everyone to easily deploy manage monitor
upgrade your clusters so here are some
vendor specific distributions we have
Cloudera which is the dominant one in
the market we have hortonworks and now
you might be aware that clouder and
hortonworks have merged so it has become
a bigger entity you have map bar you
have Microsoft Azure IBM's infosphere
and Amazon web services so these are
some popularly known vendor-specific
distributions if you would want to know
more about the Hadoop distributions you
should basically look into Google and
you should check for Hadoop different
distributions wiki page so if I type
Hadoop different distributions and then
I check for the Wiki page that will take
me to the distributions and Commercial
support page and this basically says
that the sold products that can be
called a release of Apache Hadoop come
from apache.org so that's your open
source community and then you have
various vendor specific distributions
which basically are running in one or
the other way Apache Hadoop but they
have packaged it as a solution like a
installer so that you can easily set up
clusters on set of machines so have a
look at this page and read through about
different distributions of Hadoop coming
back let's look at our next question so
what are the different Hadoop
configuration files now whether you are
talking about Apache Hadoop Cloudera
hortonworks map r or no matter which
other distribution these config files
are the most important and existing in
every distribution of Hadoop so you have
Hadoop environment.sh wherein you will
have environment variables such as your
Java path what would be your process ID
path where will your logs get stored
what kind of metrics will be collected
and so on your core hyphen site file has
the hdfs path now this has many other
properties like enabling trash or
enabling High availability or discussing
or mentioning about your zookeeper but
this is one of the most important file
you have hdfs hyphen site file now this
file will have other information related
to your Hadoop cluster such as your
replication Factor where will name node
store its metadata on disk if a data
node is running where would data node
store its data if a secondary name node
is running where would that store a copy
of name nodes metadata and so on your
mapred hyphen site file is a file which
will have properties related to your map
reduce processing you also have Masters
and slaves now these might be deprecated
in a vendor specific distribution and in
fact you would have a yarn hyphen site
file which is based on the yarn
processing framework which was
introduced in Hadoop version 2 and this
would have all your resource allocation
Source manager and node manager related
properties again if you would want to
look at default properties for any one
of these for example let's say hdfs
hyphen site file I could just go to
Google and type in one of the properties
for example I would say DFS dot name
node.name dot directory and as I know
this property belongs to hdfs hyphen
site file and if you search for this it
will take you to the first link which
says sdfs default XML you can click on
this and this will show you all the
properties which can be given in your
stfs hyphen site file it also shows you
which version you are looking at and you
can always change the version here so
for example if I would want to look at
2.6.5 I just need to change the version
and that should show me the properties
similarly you can just give a property
which belongs to say core hyphen site
file for example I would say FS dot
default fs and that's a property which
is in core hyphen side file and
somewhere here you would see core minus
default dot XML and this will show you
all the properties so similarly you
could search for properties which are
related to yarn hyphen site file or
mapred hyphen site file so I could say
yarn dot resource manager and I could
look at one of these properties which
will directly take me to yarn default
XML and I can see all the properties
which can be given in yarn and similarly
you could say map reduce dot job dot
reduces and I know this property belongs
to mapreduce hyphen site file and this
takes you to the default XML so these
are important config files and no matter
which distribution of Hadoop you are
working on you should be knowing about
these config files whether you work as a
Hadoop admin or you work as a Hadoop
developer knowing these config
properties would be very important and
that would also showcase your internal
knowledge about the configs which drive
your Hadoop cluster let's look at the
next question so what are the three
modes in which Hadoop can run so you can
have Hadoop running in a standalone mode
now that's your default mode it would
basically use a local file system and a
single Java process so when you say
Standalone mode it is as you downloading
Hadoop related package on one single
machine but you would not have any
process running that would just be to
test Hadoop functionalities you you
could have a pseudo distributed mode
which basically means it's a single node
Hadoop deployment now Hadoop as a
framework has many many services so it
has a lot of services and those Services
would be running irrespective of your
distribution
service would then have multiple
processes so your pseudo distributed
mode is a mode of cluster where you
would have all the important processes
belonging to one or multiple Services
running on a single node if you would
want to work on a pseudo distributed
mode and using a Cloudera you can always
go to Google and search for cloudera's
quick start VM you can download it by
just saying Cloud era quick start VM and
you can search for this and that will
allow you to download a quick start VM
follow the instructions and you can have
a single node Cloudera cluster running
on your virtual machines for more
information you can refer to the YouTube
tutorial where I have explained about
how to set up a quick start VM coming
back you could have finally a production
setup or a fully distributed mode which
basically means that your Hadoop
framework and its components would be
spread across multiple machines so so
you would have multiple services such as
hdfs yarn Flume scope Kafka hbase Hive
Impala and for these Services there
would be one or multiple processes
distributed across multiple nodes so
this is normally what is used in
production environment so you could say
Standalone would be good for testing
pseudo distributed could be good for
testing and development and fully
distributed would be mainly for your
production setup now what are the
differences between regular file system
and hdfs So when you say regular file
system you could be talking about a
Linux file system or you could be
talking about a Windows based operating
system so in regular file system we
would have data maintained in a single
system so the single system is where you
have all your files and directories so
it is having low fault tolerance right
so if the machine crashes your data
recovery would be very difficult unless
and until you have a backup of that data
that also affects your processing so if
the machine crashes or if the machine
fails then your processing would be
blocked now the biggest challenge with
regular file system is the seek time the
time taken to read the data so you might
have one single machine with huge amount
of disks and huge amount of ram but then
the time taken to read that data when
all the data is stored in one machine
would be very high and that would be
with least fault tolerance if you talk
about sdfs your data is distributed so
sdfs stands for Hadoop distributed file
system so here your data is distributed
and maintained on multiple systems so it
is never one single machine it is also
supporting reliability so whatever is
stored in hdfs say a file being stored
depending on its size is split into
blocks and those blocks will be spread
across multiple nodes not only that
every block which is stored on a node
will have its replicas stored on other
nodes replication Factor depends but
this makes sdfs more reliable in cases
of your slave nodes or data nodes
clashing you will rarely have data loss
because of Auto replication feature now
time taken to read the data is
comparatively more as you might have
situations where your data is
distributed across the nodes and even if
you are doing a parallel read your data
read might take more time because it
needs coordination for multiple machines
however if you are working with huge
data date which is getting stored it
will still be beneficial in comparison
to reading from a single machine so you
should always think about its
reliability through Auto replication
feature its fault tolerance because of
your data getting stored across multiple
machines and its capability to scale so
when you talk about sdfs we are talking
about horizontal scalability or scaling
out when you talk about regular file
system you are talking about vertical
scalability which is scaling up now
let's look at some specific sdfs
questions what is this why is sdfs Fault
tolerant now as I just explained in
previous slides your sdfs is Fault
tolerant as it replicates data on
different data nodes so you have a
master node and you have multiple slave
nodes or data nodes where actually the
data is getting stored now we also have
a default block size of 128 MB that's
the minimum since Hadoop version 2. so
any file which is up to 128 MB would be
using one logical block and if the file
size is bigger than 128 MB then it will
be split into blocks and those blocks
will be stored across multiple machines
now since these blocks are stored across
multiple machines it makes it more fault
tolerant because even if your machines
fail you would still have a copy of your
block existing on some other machine now
there are two aspects here one we talk
about the first rule of replication
which basically means you will never
have two identical blocks sitting on the
same machine and the second rule of
replication is in terms of rack
awareness so if your machines are placed
in racks as we see in the right image
you will never have all the replicas
placed on the same rack even if they are
on different machines so it has to be
fault tolerant and it has to maintain
redundancy so at least one replica will
be placed on some other node on some
other rack that's how sdfs is Fault
tolerant now here let's understand the
architecture of sdfs now as I mentioned
earlier you would in a Hadoop cluster
the main service is your hdfs so for
your sdfs service you would have a name
node which is your master process
running on one of the machines and you
would have data nodes which are your
slave machines getting stored across
marketing or the processes running
across multiple machines each one of
these processes has an important role to
play when you talk about sdfs whatever
data is written to hdfs that data is
split into blocks depending on its size
and the blocks are randomly distributed
across nodes with auto replication
feature these blocks are also Auto
replicated across multiple machines with
the first condition that no two
identical blocks will sit on the same
machine now as soon as the cluster comes
up your data nodes which are part of the
cluster and based on config files would
start sending their heartbeat to the
name node and this would be every three
seconds what does name node do with that
name node will store this information in
its Ram so name node starts building a
metadata in its RAM and that metadata
has information of what are the data
nodes which are available in the
beginning now when a data writing
activity starts and the blocks are
distributed across data nodes data nodes
every 10 seconds will also send a block
report to name node so name node is
again adding up this information in its
Ram or the metadata in Ram which earlier
had only data node information now name
node will also have information about
what are the files the files are split
in which blocks the blocks are stored on
which machines and what are the file
permissions now while name node is
maintaining this metadata in Ram name
node is also maintaining metadata in
disk so that is what we see in the red
box which basically has information of
whatever information was written to hdf
so to summarize your name node has
metadata in Ram and metadata in disk
your data nodes are the machines where
your blocks or data is actually getting
stored and then there is a auto
replication feature which is always
existing unless and until you have
disabled it and your read and write
activity is a parallel activity however
replication is in sequential activity
now this is what I mentioned about when
you talk about name node which is the
master process hosting metadata in disk
and RAM so when we talk about disk it
basically has a edit log which is your
transaction log and your FS image which
is your file system image right from the
time the cluster was started this
metadata in disk was existing and this
gets appended every time read write or
any other operations happen on stfs
metadata in Ram is dynamically built
every time the cluster comes up which
basically means that if your cluster is
coming up name node in the initial few
seconds or few minutes would be in a
safe mode which basically means it is
busy registering the information from
data nodes so name node is one of the
most critical processes if name node is
down and if all other processes are
running you will not be able to access
the cluster name nodes metadata in disk
is very important for name node to come
up and maintain the cluster name nodes
metadata in Ram is basically for all or
satisfying all your client requests now
when we look at data nodes as I
mentioned data nodes hold the actual
data blocks and they are sending these
block reports every 10 seconds so the
metadata in name nodes Ram is constantly
getting updated and metadata in disk is
also constantly getting updated based on
any kind of write activity happening on
the cluster now data node which is
storing the block will also help in any
kind of read activity whenever a client
requests so whenever a client on an
application or an API would want to read
the data it would first talk to name
node name node would look into its
metadata on RAM and confirm to the
client which machines could be reached
to get data that's where your client
would try to read the data from sdfs
which is actually getting the data from
data nodes and that's how your read
write requests are satisfied now what
are the two types of metadata in name
node server holds as I mentioned earlier
metadata in disk very important to
remember edit log NFS image metadata in
Ram which is information about your data
nodes files files being split into
blocks blocks residing on data nodes and
file permissions so I will share a very
good link on this and you can always
look for more detailed information about
your metadata so you can search for sdfs
metadata directories explained now this
is from hortonworks however it talks
about the metadata in disk which name
node manages and details about this so
have a look at this link if you are more
interested in learning about metadata on
disk coming back let's look at the next
question what is the difference between
Federation and high availability now
these are the features which were
introduced in Hadoop version 2. both of
these features are about horizontal
scalability of name node subscribe to
version 2 the only possibility was that
you could have one single Master which
basically me means that your cluster
could become unavailable if name node
would crash so Hadoop version 2
introduced two new features Federation
and high availability however High
availability is a popular one so when
you talk about Federation it basically
means any number of name nodes so there
is no limitation to the number of name
nodes your name nodes are in a Federated
cluster which basically means name nodes
still belong to the same cluster but
they are not coordinating with each
other so whenever a write request comes
in one of the name node picks up that
request and it guides that request for
the blocks to be written on data nodes
but for this your name node does not
have to coordinate with other name node
to find out if the block ID which was
being assigned was the same one as
assigned by other name node so all of
them below belong to a Federated cluster
they are linked via a cluster ID so
whenever an application or an API is
trying to talk to Cluster it is always
going via an cluster ID and one of the
name node would pick up the read
activity or write activity or processing
activity so all the name nodes are
sharing a pool of metadata in which each
name node will have its own dedicated
pool and we can remember that by a term
called namespace or name service so this
also provides High fault tolerance
suppose your one name node goes down it
will not affect or make your cluster
unavailable you will still have your
cluster reachable because there are
other name nodes running and they are
available now when it comes to
heartbeats all your data nodes are
sending their heartbeats to all the name
nodes and all the name nodes are aware
of all the data nodes when you talk
about high availability this is where
you would only have two name nodes so
you would have an active and you would
have a standby now normally in any
environment you would see a high
availability setup with zookeeper so
zookeeper is a centralized coordination
service so when you talk about your
active and stand by name notes election
of a name node to made as active and
taking care of a automatic failover is
done by your zookeeper High availability
can be set up without zookeeper but that
would mean that a admins intervention
would be required to make a name node as
active from standby or also to take care
of failover now at any point of time in
high availability a active name node
would be taking care of storing the
edits about whatever updates are
happening on sdfs and it is also writing
these edits to a shared location standby
name node is the one which is constantly
looking for these latest updates and
applying to its metadata which is
actually a copy of whatever your active
name node has so in this way your
standby name node is always in sync with
the active name node and if for any
reason active name node fails your
standby name node will take over and
become the active remember zookeeper
plays a very important role here it's a
centralized coordination service one
more thing to remember here is that in
your high availability secondary name
node will not be allowed so you would
have a active name node and then you
will have a standby name node which will
be configured on a separate machine and
both of these will be having access to a
shared location now that shared location
could be NFS or it could be a quorum of
Journal nodes so for more information
refer to the tutorial where I have
explained about sdfs high availability
and Federation now let's look at some
logical question here
have a input file of 3 50 mb which is
obviously bigger than 128 MB how many
input splits would be created by sdfs
what would be the size of each input
split so for this you need to remember
that by default the minimum block size
is 128 MB now that's customizable if
your environment has more number of
larger files written on an average then
obviously you have to go for a bigger
block size if your environment has lot
of files being written but these files
are of smaller size you could be okay
with 128 MB remember in Hadoop every
entity that is your directory on hdfs
file on sdfs and a file having multiple
blocks each of these are considered as
objects and for each object hadoop's
name notes Ram 150 bytes is utilized so
if your block size is very small then
you would have more number of blocks
which would directly affect the name
nodes of M if you keep a block size very
high that will reduce the number of
blocks but remember that might affect in
processing because processing also
depends on splits more number of splits
more the parallel processing so setting
of block size has to be done with
consideration about your parallelism
requirement and your name nodes Ram
which is available now coming to the
question if you have a file of 350 MB
that would be split into three blocks
and here two blocks would have 128 MB
data and the third block although the
block size would still be 128 it would
have only 94 MB of it so this would be
the split of this particular file now
let's understand about rack awareness
how does rack awareness work or why do
we even have racks so organizations
always would want to place their nodes
or machines in a systematic way there
can be different approaches you could
have a rack which would have machines
running on the master processes and the
intention would be that this particular
rack could have higher bandwidth more
cooling dedicated power supply top of
rack switch and so on the second
approach could be that you could have
one master process running on one
machine of every rack and then you could
have other slave processes running now
when you talk about your rack awareness
one thing to understand is that if your
machines are placed within racks and we
are aware that Hadoop follows Auto
replication the rule of replication in a
rack aware cluster would be that you
would never have all the replicas placed
on the same rack so if we look at this
if we have block a in blue color you
will never have all the three blue boxes
in the same rack even if they are on
different nodes because that makes us
that makes it less fault tolerant so you
would have at least one copy of block
which would be stored on a different
rack on a different node now let's look
at this so basically here we are talking
about replicas being placed in such a
way now somebody could ask a question
can I have my block and its replicas
spread across three racks and yes you
can do that but then in order to make it
more redundant you are increasing your
bandwidth requirement so the better
approach would be two blocks on the same
rack on different machines and one copy
on a different track now let's proceed
how can you restart name node and all
demons in Hadoop so if you were working
on an Apache Hadoop cluster then you
could be doing a start and stop using
Hadoop demon scripts so there are these
Hadoop demon scripts which would be used
to start and stop your Hadoop and this
is when you talk about your Apache
Hadoop so let's look at one particular
file which I would like to show you more
information here and this talks about
your different clusters so let's look
into this and so let's look at the start
and stop and here I have a file let's
look at this one and this gives you
highlights so if you talk about Apache
Hadoop this is how the setup would be
done so you would have it download the
Hadoop tar file you would have to unturn
it edit the config files you would have
to do formatting and then start your
cluster and here I have said you using
scripts so this is in case of Apache
Hadoop you could be using a start all
script that internally triggers start
DFS and start yarn and these scripts
start DFS internally would run Hadoop
demon multiple times based on your
configs to start your different
processes then your start yarn would run
yarn demand script to start your
processing related processes so this is
how it happens in Apache Hadoop now in
case of cloud era or hortonworks which
is basically a vendor specific
distribution you would have say multiple
Services which would have one or
multiple demons running across the
machines let's take an example here that
you would have machine 1 Machine 2 and
machine 3 with your processes spread
across however in case of cloud era and
hot remarks these are cluster Management
Solutions so you would never be involved
in running a script individually to
start and stop your processes in fact in
case of Cloudera you would have a
Cloudera SCM server running on one of
the machines and then Cloudera SCM
agents running on every machine if you
talk about hortonworks you would have
ambari server and ambari agent running
so your agents which are running on
every machine are responsible to monitor
the processes send also their heartbeat
to the master that is your server and
your server is the one or a service
which basically will give instructions
to the agents so in case of vendor
specific distribution your start and
stop of processes is automatically taken
care by these underlying services and
these Services internally are still
running these commands however only in
Apache Hadoop you have to manually
follow these to start and stop coming
back we can look into some command
related questions so which command will
help you find the status of blocks and
file system health so you can always go
for a file system check command now that
can show you the files for a particular
sdfs path it can show you the blocks and
it can also give you information on
status such as under replicated blocks
over replicated blocks misreplicated
blocks default replication and so on so
your fsck file system check utility does
not repair if there is any problem with
the blocks but it can give you
information of blocks related to the
files on which machines they are stored
if they are replicated as per the
replication factor or if there is any
problem with any particular replica now
what would happen if you store too many
small files in a cluster and this
relates to the block information which I
gave some time back so remember Hadoop
is coded in Java so here every directory
every file and file Creator block is
considered as an object and for every
object within your Hadoop cluster name
nodes Ram gets utilized so more number
of blocks you have more would be usage
of name node slam and if you're storing
too many small files it would not affect
your disk it would directly affect your
name node's Ram that's why in production
clusters admin guys or infrastructure
specialist will take care that everyone
who is writing data to hdfs follows a
quota system so that you could be
controlled in the amount of data you
write plus the count of data and
individual writes on hdfs now how do you
copy data from local system onto sdfs so
you can use a put command or a copy from
local and then given your local path
which is your source and then your
destination which is your sdfs path
remember you can always do a copy from
local using a minus F option that's a
flag option and that also helps you in
writing the same file or a new file to
hdfs so with your minus F you have a
chance of overwriting or rewriting the
data which is existing on sdfs so copy
from local or minus put both of them do
the same thing and you can also pass an
argument when you're copying to control
your replication or other aspects of
your file now when do you use DFS admin
refresh notes or RM admin refresh notes
so as the command says this is basically
to do with refreshing the node
information so your refresh notes is
mainly used when say a commissioning or
decommissioning of nodes is done so when
in node is added into the cluster or
when a node is removed from the cluster
you are actually informing Hadoop master
that this particular node would not be
used for storage and would not be used
for processing now in that case you
would be once you are done with the
process of commissioning or
decommissioning you would be giving
these commands that is refresh notes and
RM admin refresh notes so internally
when you talk about commissioning
decommissioning there are include and
exclude files which are updated and
these include and exclude files will
have entry of machines which are being
added to the cluster or machines which
are being removed from the cluster and
while this is being done the cluster is
still running so you do not have to
restart your master process however you
can just use this refresh commands to
take care of your combusting
decommissioning activities now is there
any way to change replication of files
on sdfs after they are already written
and the answer is of course yes so if
you would want to set a replication
Factor at a cluster level and if you
have admin access then you could edit
your sdfs hyphen site file or you could
say Hadoop hyphen site file and that
would take care of replication Factor
being set at a cluster level however if
you would want to change the replication
after the data has been written you
could always use a set rep command so
set rep command is basically to change
the replication after the data is
written you could also write the data
with a different replication and for
that you could use a minus D DFS dot
replication and give your replication
Factor when you are writing data to the
cluster so in Hadoop you can let your
data be replicated as per the property
set in the config file you could write
the data with a different replication
you could change the replication after
the data is written so all these options
are available now who takes care of
replication consistency in a Hadoop
cluster and what do you mean by under
over replicated blocks now as I
mentioned your fsck command can give you
information of over or under replicated
blocks now in a cluster it is always and
always name node which takes care of
replication consistent
so for example if you have set up a
replication of three and since we know
the first rule of replication which
basically means that you cannot have two
replicas residing on the same node it
would mean that if your replication is 3
you would need at least three data nodes
available now say for example you had a
cluster with three nodes and replication
was set to three at one point of time
one of your name node crashed and if
that happens your blocks would be under
replicated that means there was a
replication Factor set but now your
blocks are not replicated or there are
not enough replicas as per the
replication Factor set this is not a
problem your master process or name node
will wait for some time before it will
start the replication of data again so
if a data road is not responding or if a
disk has crashed and if name node does
not get information of a replica name
node will wait for some time and then it
will start re-replication of those
missing blocks from the available nodes
however while name node is doing it the
blocks are in under replicated situation
now when you talk about over replicated
this is a situation where name node
realizes that there are extra copies of
law now this might be the case that you
had three nodes running with the
replication of three one of the node
went down
down failure or some other issue within
few minutes name node re-replicated the
data and then the failed node is back
with its set of blocks again name node
is smart enough to understand that this
is a over replication situation and it
will delete set of blocks from one of
the nodes it might be the node which has
been recently added it might be your old
node which has joined your cluster again
or any node that depends on the load on
a particular
now we discussed about Hadoop we
discussed about sdfs now we will discuss
about mapreduce which is the programming
model and you can say processing
framework what is distributed cash in
mapreduce
you see know that when we talk about
mapreduce the data which has to be
processed might be existing on multiple
nodes so when you would have your
mapreduce program running it would
basically read the data from the
underlying disks now this could be a
costly operation if every time the data
has to be read from disk so distributed
cache is a mechanism wherein data set or
data which is coming from the disk can
be cached and available for all worker
nodes now how will this benefit so when
a map reduce is running instead of every
time reading the data from disk it would
pick up the data from distributed cache
and this this will benefit your map
reduce
so distributed cash can be set in your
job conf where you can specify that a
file should be picked up from
distributed cache now let's understand
about these roles so what is a record
reader what is a combiner what is a
partitioner and what kind of roles do
they play in a mapreduce processing
Paradigm or map reduce operation so
record reader communicates with the
input split and it basically converts
the data into key value Pairs and these
key value pairs are the ones which will
be worked upon by the mapper your
combiner is an optional face it's like
mini reduce so combiner does not have
its own class it relies on the reducer
class basically your combiner would
receive the data from your map tasks
which would have completed works on it
based on whatever reducer class mentions
and then passes its output to the
reducer phase partitioner is basically a
phase which decides how many reduced
tasks would be used aggregate or
summarize your data so partitioner is a
phase which would decide based on the
number of keys based on the number of
map tasks your partitioner would decide
if one or multiple reduced tasks
so either it could be partitioner which
decides on how many reduce tasks would
run or it could be based on the
properties which we have set within the
cluster which will take care of the
number of reduced tasks which would be
used always remember your partitioner
decides how outputs from combiner are
sent to reducer and to how many reducers
it controls the partitioning of keys of
your intermediate map outputs so map
phase whatever output it generates is an
intermediate output and that has to be
taken by your partitioner or by a
combiner and then partitioner to be sent
to one or multiple reduce tasks this is
one of the common questions which you
might face why is mapreduce slower in
processing so we know mapreduce goes for
parallel processing we know we can have
multiple map tasks running on multiple
nodes at the same time we also know that
multiple reduced tasks could be running
now why does then mapreduce become a
slower approach first of all your map
reduce is a batch oriented operation now
mapreduce is very rigid and it strictly
uses mapping and reducing phases so no
matter what kind of processing you would
want to do you would have to still
provide the mapper function and the
reducer function to work on data not
only this whenever your map phase
completes
the output of your map face which is an
intermittent output would be written to
hdfs and thereafter underlying disks and
this data would then be shuffled and
sorted and picked up for reducing phase
so every time your data being written to
htfs and retrieved from sdfs makes
mapreduce a slower approach the question
is for a map release job is it possible
to change the number of mappers to be
created Now by default you cannot change
the number of map tasks because number
of map tasks depends on the input splits
however there are different ways in
which you can either set a property to
have more number of map tasks which can
be used or you can customize your code
or make it use a different format which
can then control the number of map tasks
by default number of map tasks are equal
to the number of splits of file you are
so if you have a 1GB of file that is
split into eight blocks or 128 MB there
would be eight map tasks running on the
cluster these map tasks are basically
running your mapper function if you have
hard coded properties in your mapred
hyphen site file to specify more number
of map tasks then you could control the
number of map tasks let's also talk
about some data types so when you
prepare for Hadoop when you want to get
into Big Data field you should start
learning about different data for
now there are different data formats
such as Avro par k
have a sequence file or binary format
and these are different formats which
are used now when you talk about your
data types in Hadoop the SE are
implementation of your writeable and
writable comparable interfaces so for
every data type in Java you have a
equivalent in Hadoop so end in Java
would be in writable in Hadoop float
would be float writable long would be
long writeable double writeable Boolean
writable array writable map writable and
object
these are your different data types that
could be
your mapreduce program and these are
implementation of writable and writable
comparable interfaces what is
speculative execution
now imagine you have a cluster which has
huge number of nodes and your data is
spread across multiple slave machines or
multiple nodes now at a point of time
due to a disk D grade on network issues
or machine heating up or more load being
on a particular node there can be a
situation where your data node will
execute task in a slower manner now in
this case if speculative execution is
turned on there would be a shadow task
or a another similar task running on
some other node for the same processing
so whichever task finishes first will be
accepted and the other task would be
killed so speculative execution could be
good if you are working in a intensive
workload workload kind of environment
where if a particular node is slower you
could benefit from a unoccupied or a
node which has less load to take care of
your processing going further this is
how we can understand so node a which
might be having a slower task you would
have a scheduler which is maintaining or
having knowledge of what are the
resources available so if speculative
execution as a property is turned on
then the task which was running slow a
copy of that task or you can say shadow
transport run on some other node and
whichever task completes first will be
considered this is what happens in your
speculative execution now how is
identity mapper different from chain
mapper now this is where we are getting
deeper into mapreduce Concepts so when
you talk about mapper identity mapper is
the default mapper which is chosen when
no mapper is specified in mapreduce
driver class so for every mapreduce
program you would have a map class which
is taking care of your mapping phase
which basically has a mapper function
and which would run one or multiple map
tasks right your programming your
program would also have a reduce class
which would be running a reducer
function which takes care of reduced
tasks running on multiple nodes now if a
mapper is not specified within your
driver class so driver class is
something which has all information
about your flow what's your map class
what is your reduce class what's your
input format what's your output format
what are the job configurations and so
on so identity mapper is the default
mapper which is chosen when no mapper
class is mentioned in your driver class
it basically implements an identity
function which directly writes all its
key pairs into output and it was defined
in old map reduce API in this particular
package but when you talk about chaining
mappers or chain mapper this is
basically a class to run multiple
mappers in a single map task or
basically you could say multiple map
tasks would run as a part of your
processing the output of first mapper
would become as an input to Second
mapper and so on and this can be defined
in the under mentioned class or
what are the major configuration
parameters required in a mapreduce
program obviously we need to have the
input location we need to have the
output location so input location is
where the files will be picked up from
and this would preferably on sdfs
directory output location is the path
where your job output would be written
by your map reduce program you also need
to specify input and output formats if
you don't specify the defaults are
considered then we need to also have the
classes which have your map and reduce
functions and if you intend to run the
code on a cluster you need to package
your class in a jar file export it to
your cluster and then this jar file
would have your mapper reducer and
Driver classes so these are important
configuration parameters which you need
to consider for a map reduce program now
what is the difference or what do you
mean by map side join and reduce side
joint map side join is basically when
the join is performed at the mapping
level or at the mapping phase or is
performed by the mapper so each input
data which is being worked upon has to
be divided into same number of
partitions
input to each map is in the form of a
structured partition and is in sorted
order so Maps I join you can understand
it in a simpler way that if you compare
it with rdbms Concepts where you had two
tables which were being joined it will
always be advisable to give your bigger
table as the left side table or the
first table for your join condition and
it would be your smaller table on the
left side and your bigger table on the
right
which basically means the smaller table
could be loaded in memory and could be
used for joining so map side drawing is
a similar kind of mechanism where input
data is divided into same number of
partition when you talk about reduced
side join here the join is performed by
the reducer so it is easier to implement
than map side join as all the sorting
and shuffling will send the values or
send all the values having identical
keys to the same reducer so you don't
need to have your data set in a
structured form so look into your map
site drawing or reduce side join and
other joints just to understand how
mapreduce Works however I would suggest
not to focus more on this because
mapreduce is still being used for
processing but the amount of mapreduce
based processing has decreased overall
or across the industry now what is the
role of output committer class in a
mapreduce job so output committer as the
name says describes the commit of task
output for a mapreduce job so we would
have this as mentioned our Apache Hadoop
map reduce output committer you could
have a class which extends output
committer class
so mapreduce relies on this map reduce
relies on the output committer of the
job to set up the job initialization
cleaning up the job after the job
completion that means all the resources
which were being used by a particular
job setting up the task temporary output
checking whether a task needs a commit
committing the task output and
discarding the task so this is a very
important class and can be used within
your mapreduce job what is the process
of spilling in mapreduce what does that
mean so spilling is basically a process
of copying the data from memory buffer
to disk when obviously the buffer usage
reaches a certain threshold so if there
is not enough memory in your buffer in
your memory then the content which is
stored in buffer or memory has to be
flushed out so by default a background
thread starts spilling the content from
memory to disk after 80 percent of
buffer size is filled now when is the
buffer being used so when your mapreduce
processing is happening the data from
data is being read from the disk loaded
into the buffer and then some processing
happens
same thing also happens when you are
writing data to the cluster so you can
imagine for a hundred megabytes size
buffer the spilling will start after the
content of buffer reaches 80 megabytes
this is customizable how can you set the
mappers and reducers for a mapreduce job
so these are the properties so number of
mappers and reducers as I mentioned
earlier can be customized so by default
your number of map tasks depends on the
split and number of reduced tasks
depends on the partitioning phase which
decides number of reduced tasks which
would be used depending on the key
member we can set these properties
either in
in config files or provide them on the
command line or also make them part of
our code and this can control the number
of map tasks or reduce tasks which would
be run for a particular job let's look
at one more interesting question what
happens when a node running a map task
fails before sending the output to the
reducer so there was a node which was
running a map task and we know that
there could be one or multiple map tasks
running on one or multiple nodes and all
the map tasks have to be completed
before the further stages that such as
combiner or reducer come into existence
so in a case if in node crashes where a
map task was assigned to it the whole
task will have to be run again on some
other node so in Hadoop version 2 yarn
framework has a temporary demon called
application master so your application
Master is taking care of execution of
your application and if a particular
task on a particular node failed due to
unavailability of node it is the role of
application Master to have this task
scheduled on some other node now can we
write the output of mapreduce in
different formats of course we can so
Hadoop supports various input and output
formats so you can write the output of
mapreduce in different formats so you
could have the default format that is
text output format wherein records are
written as line of text you could have
sequence file which is basically to
write sequence files or your binary
format files where your output files
need to be fed into another map reduce
jobs
go for a map file output format to write
output as map files you could go for a
sequence file as a binary output format
so that's again a variant of your
sequence file input format it basically
writes keys and values to
SC so when we talk about binary format
we are talking about a non-human
readable format DB output format now
this is basically used when you would
want to write data to say relational
databases or say no SQL databases such
as hbase so this format also sends the
reduce output to a SQL table now let's
learn a little bit about yarn yarn which
stands for yet another resource
negotiator it's the processing framework
so what benefits did yarn bring in
Hadoop version 2 and how did it solve
the issues of mapreduce version 1. so
map reduce version 1 had major issues
when it comes to scalability or
availability because sorry in Hadoop
version 1 you had only one master
process for processing layer and that is
your job tracker so your job tracker was
listening to all the task trackers which
were running on multiple machines so
your job tracker was responsible for
resource tracking and job scheduling in
yarn you still have a processing Master
but that's called resource manager
instead of job tracker and now with
Hadoop version 2 you could even have
resource manager running in high
availability mode you have node managers
which would be running on multiple
machines and then you have a temporary
demon called application master so in
case of Hadoop version 2 your resource
manager or Master is only handling the
client connections and taking care of
tracking the resources the jobs
scheduling or basically taking care of
execution across multiple nodes is
controlled by application Master till
the application completes
so in yarn you can have different kind
of resource allocations that could be
done and there is a concept of container
so container is basically a combination
of RAM and CPU cores yarn can run
different kind of workloads so it is not
just map reduce kind of workload which
can be run on Hadoop version 2 but you
would have graph processing massive
parallel processing you could have a
real-time processing and huge processing
applications could run on a cluster
based on yarn so when we talk about
scalability in case of your Hadoop
version 2 you can have a cluster size of
more than 10 000 nodes and can run more
than 100 000 concurrent tasks and this
is because for every application which
is launched you have this temporary
demon called application
so if I would have 10 applications
running I would have 10 app Masters
running taking care of execution of
these applications across multiple nodes
compatibility so Hadoop version 2 is
fully compatible with whatever was
developed as per Hadoop version 1 and
all your processing needs would be taken
care by yarn
so Dynamic allocation of cluster
resources taking care of different
workloads allocating resources across
multiple machines and using them for
execution all that is taken care by yarn
multi-tenancy which basically means you
could have multiple users or multiple
teams
you could have open source and
proprietary data access engines and all
of these could be basically hosted using
the same cluster now how does yarn
allocate resources to an application
with help of its architecture so
basically you have a client or an
application or an API which talks to
resource manager resource manager is as
I mentioned managing the resource and
location in the Clusters when you talk
about resource manager you have its
internal two components one is your
schedule
and one is your applications manager so
when we say resource manager being the
master is tracking the resources The
Source manager is the one which is
negotiating the resources with slave it
is not actually resource manager who is
doing it but these internal components
so you have a scheduler which allocates
resources to various running
applications so scheduler is not
bothered about tracking your resources
or basically tracking your applications
so we can have different kind of
schedulers such as feed
[Music]
first out you could have a fair
scheduler or you could have a capacity
scheduler and these schedulers basically
control how resources are allocated to
multiple applications when they are
running in parallel so there is a queue
mechanism so scheduler will schedule
resources based on requirements of
application but it is not monitoring or
tracking the status
your applications manager is the one
which is accepting the job submissions
it is monitoring and restarting the
application Masters so it's application
manager which is basically basically
then launching a application Master
which is responsible for an application
so this is how it looks so whenever a
job submission happens we already know
that resource manager is aware of the
resources which are available with every
node manager so on every node which has
fixed amount of RAM and CPU cores some
portion of resources that RAM and CPU
course
located to node manager
resource manager is all
how much resources are available across
mode so whenever a client request comes
in resource manager will make a request
to node manager it will basically
request node manager to hold some
resources for processing node manager
would basically approve or disapprove
this request
holding the sources and these resources
that is a combination of RAM
course are nothing but containers we can
allocate containers of different sizes
within yarn hyphen site file so your
node manager based on the request from
resource manager guarantees the
container which would be available for
processing that's when your resource
manager starts a temporary demon called
application Master to take care of your
execution so your app Master which was
launched by resource manager or we can
say internally applications manager will
run in one of the containers because
application Master is also a piece of
Port so it will run in one of the
containers and then other containers
will be utilized for execution this is
how yarn is basically taking care of
your allocation your application Master
is managing resource needs it is the one
which is interacting with scheduler and
if a particular node crashes it is the
responsibility of App Master to go back
to the master which is resource manager
and negotiate for more resources so your
app Master will never ever negotiate
Resources with node manager directly it
will always talk to resource manager and
the source manager is the one which
negotiates the resources container as I
said is a collection of resources like
your RAM CPU Network bandwidth and your
container is located based on the
availability of resources on a
particular node so which of the
following has occupied the place of a
job tracker of mapreduce so it is your
resource manager so resource manager is
the name of the master process in Ado
version 2. now if you would have to
write yarn commands to check the status
of an application so we could just say
yarn application minus status and then
the application ID and you could kill it
also from the command line remember your
yarn has a UI and you can even look at
your applications from the UI you can
even kill your applications from the UI
however knowing the command line
commands would be very useful can we
have more than one resource manager in a
yarn-based cluster yes we we can that is
what Hadoop version 2 allows
have a high availability yarn cluster
where you have a active and standby and
the coordination is taking care by your
zookeeper at a particular time there can
only be one active resource manager and
if active resource manager fails your
standby resource manager comes and
becomes active however zookeeper is
playing a very important role remember
zookeeper is the one which is
coordinating the server State and it is
doing the election of active to standby
failover what are the different
schedulers available in yarn so you have
a fee for scheduler that is first in
first out and this is not a desirable
option because in this case a longer
running application might block all
other small running applications Your
Capacity scheduler is basically a
scheduler where dedicated queues are
created and they have fixed amount of
resources so you can have multiple
applications accessing the cluster at
the same time and they would be using
their own queues and the resources
allocated to them if you talk about Fair
scheduler you don't need to have a fixed
amount of resources you can just have a
percentage and you could decide what
kind of fairness is to be followed which
basically means that if you are
allocated 20 gigabytes of memory however
the cluster has 100 gigabytes and the
other team was assigned 80 gigabytes of
memory then you have 20 access to the
cluster another team has 80 percent
however if the other team does not come
up or does not use the cluster in a fair
scheduler you can go up to maximum 100
percent
to find out more information about your
schedulers you could either look in
Hadoop definitive guide or what you
could do is you could just go to Google
and you could type for example yarn
scheduler let's search for yarn
scheduler and then you can look in
Hadoop definitive guide and so this is
your Hadoop definitive guide and it
beautifully explains about your
different schedulers how do multiple
applications run and that could be in
your fifo kind of scheduling it could be
in capacity scheduler or it could be in
a fair scheduling so have a look at this
link it's a very good link you can also
search for yarn untangling and this is a
Blog of four or this is a series of four
blocks where it's beautifully explained
about your yarn how it works how the
resource allocation happens what is a
container and what runs within the
container so you can scroll down you can
be reading through this and you can then
also search for part two of it which
talks about allocation and so on so
coming back
we basically have these schedulers what
happens if a resource manager fails
while executing an application in a high
availability cluster so in a high
availability cluster we know that we
would have two resource managers one
being active one being standby and
zookeeper which is keeping a track of
the server States so if a RM fails in
case of high availability the standby
will be elected as active and then
basically your resource manager or the
standby would become the active one and
this one would instruct the application
Master to abort in the beginning then
your resource manager recovers its
running state so there is something
called as RM State Store where all the
applications which are running their
status is stored so resource manager
recovers its running state by looking at
your state store by taking advantage of
container statuses and then continues to
take care of your process now in a
cluster of 10 data nodes each having 16
GB and 10 cores what would be total
processing capacity of the cluster take
a minute to think 10 data nodes 16 GB
Ram per node 10 cores so if you mention
the answer as 160 GB RAM and 100 cores
then you went wrong now think of a
cluster which has 10 data nodes each
having 16 GB RAM and 10 cores remember
on every node in a Hadoop cluster you
would have one or multiple processes
running those processes would need RAM
the machine itself which has a Linux
file system would have its own processes
so that would also be having some RAM
usage which basically means that that if
you talk about 10 data nodes you should
deduct at least 20 to 30 percent towards
the overheads towards the cloud database
Services towards the other processes
which are running and in that case I
could say that you could have 11 or 12
GB available on every machine for
processing and say six or seven cores my
multiply that by 10 and that's your
processing capacity remember the same
thing applies to the disk usage also so
if somebody asks you in a 10 data node
cluster where each machine has 20
terabytes of disks what is my total
storage capacity available for stfs so
the answer would not be 200 you have to
consider the overheads and this is
basically which gives you your
processing capacity now let's look at
one more question so what happens if
requested memory or CPU cores Beyond or
goes beyond the size of container now as
I said you can have your configurations
which can say that in a particular data
node which has 100 GB Ram I could
allocate say 50 GB for the processing I
out of 100 cores I could say 50 cores
for processing so if you have 100 GB RAM
and 100 cores you could ideally allocate
100 for processing but that's not
ideally possible so if you have 100 GB
Ram you would go for 50 GB and if you
have 100 cores you would go for 50 cores
now within this RAM and CPU course you
have the concept of containers right so
container is a combination of the RAM
and CPU cores so you could have a
minimum size container and maximum size
now at any point of time if your
application starts demanding more memory
or more CPU cores and this cannot fit
into a container location your
application will fail your application
will fail because you requested for a
memory or a combination of memory and
CPU cores which is more than the maximum
container size so look into this yarn
tangling website which I mentioned and
look for the second blog in those series
which explains about these allocate now
here we will discuss on hive Peg hbase
and these components of two which are
being used in the industry for various
use cases let's look at some questions
here and let's look how you should
prepare for them so first of all we will
learn on hive which is a data
warehousing package so the question is
what are the different components of a
hive architecture now when we talk about
Hive we already know that Hive is a data
warehousing package which basically
allows you to work on structured data or
data which can be structurized so
normally people are well versed with
querying or basically processing the
data using SQL queries a lot of people
come from database backgrounds and they
would find it comfortable if they know
structured query language hi is one of
the data warehousing package which
resides within a Hadoop ecosystem it
uses hadoop's distributed file system to
store the data and it uses rdbm mess
usually to store the metadata although
metadata can be stored locally also so
what are the different components of a
hive architecture so it has a user
interface so user interface calls the
execute interface to the driver this
creates a session to the query and then
it sends the query to the compiler to
generate an execution plan for it
usually whenever Hive is set up it would
have its metadata stored in an rdbms now
to establish the connection between
rdbms and Hadoop we need odbc or jdbc
connector jar file and that connector
jar file has a driver class now this
driver class is mandatory to create a
connection between Hive and Hadoop so
user interface creates this interface
using the driver now we have metastore
metastore stores the metadata
information so any object which you
create such as database table indexes
their metadata is stored in metastore
and usually this meta store is stored in
an rdbms so that multiple users can
connect to Hive so your meta store
stores the metadata information and
sends that to the compiler for execution
of a query what does the compiler do it
generates the execution plan it has a
tag now tag stands for direct cycle
craft so it has a tag of stages where
each stage is either a metadata
operation a map or reduced job or an
operation on sdfs and finally we have
execution engine that acts as a bridge
between Hive and Hadoop to process the
query so execution engine communicates
bi-directionally with metastore to
perform operations like create or drop
tables so these are four important
components of Hive architecture now what
is the difference between external table
and manage stable and Hive so we have
various kinds of table in Hive such as
external table manage table partition
table the major difference between your
managed and external table is in respect
to what happens to the data if the table
is dropped usually whenever we create a
table in Hive it creates a manage table
or we could also call that as an
internal table now this manages the data
and moves it into warehouse directory by
default whether you create a manage
stable or external table usually the
data can reside in hive's default
Warehouse directory or it could be
residing in a location chosen however
when we talk about manage table if one
drops a manage table not only the
metadata information is deleted but the
tables data is also deleted from sdfs if
we talk about external table it is
created with an external keyword
explicitly and if an external table is
dropped nothing happens to the data
which resides in sdfs so that's the main
difference between your managed and
external table what might be the use
case if somebody asks you there might be
a migration kind of activity or you are
interested in creating a lot of tables
using your queries so in that case you
could dump all the data on sdfs and then
you could create a table by pointing to
a particular directory or multiple
directories now you could then do some
testing of your tables and would decide
that you might not need all the tables
so in that case it would be advisable to
create external tables so that even if
the table is later dropped the data on
sdfs will be intact unlike your manage
table where dropping of table will
delete the data from sdfs Also let's
learn a little bit on partition so what
is partition And Hive and why is
partitioning required in high life if
somebody asks you that now normally in
world of rdbms partition is the process
to group similar type of data together
and that is usually done on basis of a
column or what we call as partitioning
key now each table usually has one
column in context of rdbms which could
be used to partition the data and why do
we do that so that we can avoid scanning
the complete table for a query and
restrict the scan to set of data or to a
particular partition in Hive we can have
any number of partition keys so
partitioning provides granularity in
Hive table it reduces the query latency
by scanning only relevant partition data
instead of whole data set we can
partition at various levels now if I
compare rdbms with Hive in case of rdbms
you could have one column which could be
used for partitioning and then then you
could be squaring the specific partition
so in case of rdbms your partition
column is usually a part of the table
definition so for example if I have an
employee table I might have employee ID
employee name employee age and employee
salary has four columns and I would
decide to partition the table based on
salary column now why would I partition
it because I feel that employee table is
growing very fast it is or it will have
huge amount of data and later when we
query the table we don't want to scan
the complete table so I could split my
data into multiple partition based on a
salary column giving some ranges in Hive
it is a little different in Hive you can
do partitioning and there is a concept
of static and dynamic partitioning but
in Hive the partition column is not part
of table definition so you might have an
employee table with employee ID name a
each and that that's it that would be
the table definition but you could then
have partitioning done based on salary
column which will then create a specific
folder on sdfs in that case when we
query the data we can see the partition
column also showing up so we can
partition the transaction data for a
bank for example based on month like
Chan Feb Etc and any operation regarding
a particular month will then allow us to
query that particular folder that is
where partitioning is useful now why
does Hive not store metadata information
in a CFS if somebody asks you so we know
that hives data is stored in sdfs which
is Hadoop distributed file system
however the metadata is either stored
locally and that mode of high would be
called as embedded mode or you could
have hives metadata stored in rdbms so
that multiple clients can initiate a
connection now this metadata which is
very important for Hive would not be
stored in sdfs so we already know that
sdfs read and write operations are time
consuming it is a distributed file
system and it can accommodate huge
amount of data so Hive stores metadata
information in metastore using rdbms
instead of sdfs so this allows to
achieve low latency and faster data
access
now if somebody asks what are the
components used in Hive query processor
so usually we have the main components
are your parser your execution engine
logical plan generation Optimizer and
type checking so whenever a query is
submitted it will go through a parser
and parser would check the syntax it
would check for objects which are being
queried and other things to see if the
query is fine now internally you have a
semantic analyzer which will also look
at the query you have an execution
engine which basically will work on the
execution part that is the best
generated execution plan which could be
used to get the results for the query
you could also have user defined
functions which a user would want to use
and these are normally created in Java
or Java programming language and then
basically these user defined functions
are added to the class path now you
would have a logical plan generation
which which basically looks at your
query and then generates a logical plan
or the best execution path which would
be required to get to the results
internally there is a physical plan
generated which is then looked in by
Optimizer to get the best path to get to
the data and that might also be checking
your different operators which you are
using within your query finally we would
also have type checking so these are
important components in Hive so somebody
might ask you if you are querying your
data using Hive what are the different
components involved or if you could
explain what are the different
components which work when a query is
submitted so these are the components
now let's look a scenario based question
Suppose there are a lot of small CSV
files which are present in a is DFS
directory and you want to create a
single Hive table from these files so
data in these files have Fields like
registration number name email address
so if this is what needs to be done what
will be your approach to solve it where
will you create a single Hive table for
lots of small files without degrading
the performance of the system so there
can be different approaches now we know
that there are a lot of small CSV files
which are present in a directory so we
know that when we create a table in Hive
we can use a location parameter so I
could say create table give a table name
give the column and their data types I
could specify the delimiters and finally
I could say location and then point it
to a directory on sdfs in this directory
might be the directory which has lot of
CSV files so in this case I will avoid
loading the data in the table because
table being Point table pointing to the
directory will directly pick up the data
from one or multiple files and we also
know that Hive does schema check on read
so does not do a schema check on write
so in case there were one or two files
which did not follow the schema of the
table it would not prevent data loading
data would anyways be loaded only when
you query the data it might show you
null values if data which was loaded
does not follow the schema of the table
this is one approach what is the other
approach so let's look at that you can
think about sequence file format which
is basically a smart format or a binary
format and you can group these small
files together to form a sequence file
now this could be one other smarter
approach so we could create a temporary
table so we could say create table give
a table name give the column names and
their data types we could specify the
delimiters as it shows here that is row
format and Fields terminated by and
finally we can store that as text file
then we can load data into this table by
giving a local file system path and then
we can create a table that will store
data in sequence file format so my point
one is storing the data in this text
file 0.3 would be storing the data in
sequence file format so we say create
table give the specifications we say row
format delimited fields are terminated
by comma stored as sequence file then we
can move the data from test table into
test sequence file table so I could just
say insert overwrite my new table as
select star from other tape remember in
Hive you cannot do insert update delete
however if the table is existing you can
do a insert overwrite from an existing
table into a new table so this could be
one approach where we could have lot of
CSV files or smaller files club together
as one big sequence file and then store
it in the table now if somebody asks you
write a query to in insert a new column
that is integer data type into a hive
table and the requirement might be that
you would want to insert this table at a
position before an existing column now
that's possible by doing an alter table
giving your table name and then
specifying change column giving you a
new column with the data type before an
existing column this is a simple way
where you can insert a new column into a
hive table what are the key differences
between Hive and pick now some of you
might have heard High Visa data
warehousing package and Peg is more of a
scripting language both of them are used
for data analysis or Trend detection
hypothesis testing data transformation
and many other use cases so if we
compare Hive and big Hive uses a
declarative language called Hive ql that
is Hive querying language similar to SQL
and it is for reporting or for data
analysis even for data transformation or
for your data extraction big uses a high
level procedural language called Pig
Latin for programming both of them
remember use mapreduce processing
framework so when we run a query in Hive
to process the data or when we create
and submit a big script both of them
trigger a mapreduce job unless and until
we have set them to Local mode Hive
operates on the server side of the
cluster and basically works on
structured data or data which can be
structuralized pig usually works or
operates on the client side of the
cluster and allows both structured
unstructured or even I could say
semi-structured data Hive does not
support Avro file format by default
however that can be done by using the
write serializer deserializer so we can
have Hive table related data stored in
Avro format in sequence file format in
parquet format or even as a text file
format however when we are working on
smarter formats like Avro or sequence
file or parquet we might have to use
specific serializers deserializers for
Avro this is the package which allows us
to use Avro format Pig supports Agro
format by default Hive was developed by
Facebook and it supports partitioning
and Peg was developed by Yahoo and it
does not support partitioning so these
are high level differences there are
lots and lots of differences remember
Hive is more of a data warehousing
package and Peg is more of a scripting
language or a strictly procedural flow
following scripting language which
allows us to process the data now let's
get more and let's get more deeper and
learn about big which is as I mentioned
a scripting language which can be used
for your data processing it also uses
map reduce although we can even have big
run in a local mode let's learn about
pig in the next section now let's learn
on some questions about Pig which is a
scripting language and it is extensively
used for data processing and data
analysis so the question is how is
Apache Pig different from mapreduce now
we all know that mapreduce is a
programming model it is it's quite rigid
when it comes to processing the data
because you have to do the mapping and
reducing you have to write huge code
usually mapreduce is written in Java but
now it can also be written in Python it
can be written in Scala and other
programming languages so if we compare
pick with mapreduce pig obviously is
very concise it has less lines of code
when compared to mapreduce now we also
know that big script internally will
trigger a mapreduce job however user
need not know about mapreduce
programming model they can simply write
simple scripts in Pig and that will
automatically be converted into
mapreduce however mapreduce has more
lines of code Peak is high level
language which can easily perform join
operations or other data processing
operations map reduce is a low level
language which cannot perform job join
operations easily so we can do join
using mapreduce however it's not really
easy in comparison to Pig now as I said
on execution every Pig operator is
converted internally into a mapreduce
job so every big script which is run
which would be converted into mapreduce
job now map reduce overall is a batch
oriented processing so it takes more
time to compile it takes more time to
execute either when you run a mapreduce
job or when it is triggered by Pink
script big works with all versions of
Hadoop and when we talk about mapreduce
program which is written in one Hadoop
version may not work with other versions
it might work or it might not it depends
on what are the dependencies what is the
compiler you're using what programming
language you have used and what version
of Hadoop you are working on so these
are the main differences between Apache
Pig and mapreduce what are the different
ways of executing pick script so you
could create a script file store it in
dot pick or dot text and then you could
execute it using the pick command you
could be bringing up the grunt shell
that is Pig's shell now that usually
starts with mapreduce mode but then we
can also bring it up in a local mode and
we can also run pick embed it as an
embedded script in other programming
language so these are the different ways
of executing your pick script now what
are the major components of pig
execution environment this is this is a
very common question interviewers would
always want to know different components
of Hive different component currents of
pig even different components which are
involved in Hadoop ecosystem so when we
want to learn about major components of
big execution environment here also so
you have pick scripts now that is
written in pig latin using built-in
operators and user-defined functions and
submitted to the execution environment
that's what happens when you would want
to process the data using pick now there
is a parser which does type checking and
checks the syntax of the script the
output of parser is a tag direct a
cyclic graph so look in Wikipedia for
dag so tag is basically a sequence of
steps which run in One Direction then
you have an Optimizer now this Optimizer
performs optimization using merge
transform split Etc it aims to reduce
the amount of data in the pipeline
that's the whole purpose of Optimizer
you have a internal compiler so pick
compiler converts the optimized code
into a mapreduce job and here user need
not know the mapreduce programming model
or how it works or how it is written
they all need to know about running the
pick script which would be internally
converted into a mapreduce job and
finally we have an execution engine so
mapreduce jobs are submitted to the
execution engine to generate the desired
results so these are major components of
pick execution environment now let's
learn about different complex data types
in big big supports various data types
the main ones are Tuple bag and map what
is Tuple or Tuple as you might have
heard a tuple is an ordered set of
fields which can contain different data
types for each field so in Array you
would have multiple elements but that
would be of same types list can also
have different types your Tuple is a
collection which has different fields
and each field can be of different type
now we could have an example is 1 comma
3 or 1 comma 3 comma a string or a float
element and all of that form a tuple bag
is a set of tuples so that's represented
by curly braces so you could also
imagine this like a dictionary which has
various different correction elements
what is a map map is a set of key value
pairs used to represent data so when you
work in Big Data field you need to know
about different data types which are
supported by Peg which are supported by
Hive which are supported in other
components of Ado so pupil pack map
array array buffer you can think about
list you can think about dictionaries
you can think about map which is key
value pair so these are your different
complex data types other than the
primitive data types such as integer
character string Boolean float and so on
now what are the various diagnostic
operators available in Apache pick so
these are some of the operators or
options which you can give in a pick
script you can do a thumb now dump
operator runs the pig latin scripts and
displays the result on the screen so
either I could do a dumb and see the
output on the screen or I can even do a
dump into and I could store my output in
a particular file so we can load the
data using load operator in Pig and then
Pig also has different internal storage
like Json loader or pick storage which
can be used if you are working on
specific kind of data and then you could
do a dump either before processing or
after processing and dump would produce
the result the result could be stored in
a file or seen on the screen you also
have a describe operator now that is
used to view the schema of a relation so
you can load the data and then you can
view the schema of relation using
describe operator explain as we might
already know displays the physical
logical and mapreduce execution plans so
normally in rdbms when we use X-Plane we
would like to see what happens behind
the scenes when a particular script or a
query runs so we could load the data
using load operator as in any other case
and if we would want to display The
Logical physical and mapreduce execution
plans we could use explain operator
there is also an illustrate operator now
that gives the step-by-step execution of
sequence of statements so sometimes when
we would want to analyze our script to
see how good or bad they are or would
that really serve our purpose we could
use illustrate and again you can test
that by loading the data using load
operator and you could just use a
illustrate operator to have a look at
the step-by-step execution of the
sequence of statements which you would
want to execute so these are different
diagnostic operators available in Apache
pick now if somebody asks State the
usage of group order by and distinct
keywords in big script so as I said big
is a scripting language so you could use
various operators so group basically
collects various records with the same
key and groups the data in one or more
relations here is an example you could
do a group data so that is basically a
variable or you can give some other name
and you can say group relation Name by H
now say I have a file where I have field
various fields and one of the field is a
relational name so I could group that by
a different field order by is used to
display the contents of relation in a
sorted order whether ascending or
descending so I could create a variable
called relation 2 and then I could say
order relation name one by ascending or
descending order distinct basically
removes the duplicate records and it is
implemented only on entire records not
on individual records so if you would
like want to find out the distinct
values and relation name field I could
use distinct what are the relational
operators in pig so you have various
relational operators which help data
scientists or data analysts or
developers who are analyzing the data
such as go Group which joins two or more
tables and then performs group operation
on the join table result you have cross
it is used to compute the cross product
that is a Cartesian product of two or
more relations for each is basically to
do some iteration so if it will iterate
through tuples of a relation generating
a data transformation so for example if
I say variable a equals and then I load
a file in a and then I could create a
variable called B where I could say for
each a I would want to do something say
group join is to join two or more tables
in a relation limit is to limit the
number of output tuples or output
results split is to split the relation
into two or more relations Union is to
get a combination that's it will merge
the contents of two or more relations
and order is to get a sorted result so
these are some relational operators
which are extensively used in pig for
analysis what is the use of having
filters in Apache pick now say for
example I have some data which has three
Fields here product quantity and this is
my phone sales data so filter operator
could be used to select the required
values from a relation based on a
condition it also allows you to remove
unwanted records from data file so for
example filter the products where
quantity is greater than thousand so I
see that I have one row wherein or
multiple rows where the quantity is
greater than thousands such as fifteen
hundred Seventeen hundred twelve hundred
so I could create a variable called a I
would load my file using pick storage as
I explained earlier big storage is an
internal parameter which can be used to
specify the delimiters now here my
delimiter is comma so I could say using
pick storage as and then I could specify
the data type for each field so here
being integer product being character
array and quantity being integer then B
I could say filter a whatever we have in
a by quantity greater than thousand so
it's very concise it's very simple and
it allows us to extract and process data
in a simpler way now Suppose there is a
file called test dot txt having 150
records in sdfs so this is a file which
is stored on sdfs and it has 150 records
where we can consider every record being
one line and if somebody asks you to
write a pick command to retrieve the
first 10 records of the file first we
will have to load the data so I could
create a variable called test underscore
data and I would say load my file using
pick storage specifying the delimiter S
comma as and then I could specify my
Fields what whatever Fields our file
have and then I would want to get only
10 records for which I could use the
limit operator so I could say limit on
test data and give me 10 records this is
very simple and we can extract 10
records from 150 records which are
stored in the file on sdfs now we have
learned on Pig we have learned some
questions on hive you could always look
more in books like programming in Hive
or programming in pick and look for some
more examples and try out these examples
on a existing Hadoop setup now let's
learn on hbase which is a nosql database
now edgebase is a four dimensional
database in comparison to your rdbms
which usually are two dimensional so
rdbms have rows and columns but hbase
has four coordinates it has row key
which is always unique column family
which can be any number column
qualifiers which can again be any number
per column family and then you have a
version so these four coordinates make H
base a four dimensional key value store
or a column family store which is unique
for storing huge amount of data and
extracting data from hbase there is a
very good link which I would suggest
everyone can look at if you would want
to learn more on edgebase and you could
just say hbase mapper and this basically
brings up a documentation which is from
mapper but then that's not specific to
map R and you can look at this link
which will give you a detailed
explanation of HP is how it works what
are the Architectural Components and how
data is stored and how it makes edgebase
a very powerful nosql database so let's
learn on some of the important or
critical questions on hbase which might
be asked by the interviewer in an
interview when you are applying for a
Big Data admin or a developer position
role so what are the key components of
edgebase now as I said this is one of
the favorite questions of interviewers
where they would want to understand your
knowledge on different components for a
particular service edgebase as I said is
a nosql database and that comes as a
part of service with Cloudera or
hortonworks and with Apache Hadoop you
could also set up edgebase as an
independent package so what are the key
components of hbase edgebase has a
region server now edgebase follows the
similar kind of topology like Hadoop now
Hadoop has a master process that is name
node and slave processes such as data
nodes and secondary name node in the
same way edgebase also has a master
which is Edge master and the slave
processes are called region servers so
these region servers are usually
co-located with data nodes however it is
not mandatory that if you have 100 data
nodes you would have 100 region servers
so it purely depends on admin so what
does this region server contain so
region server contains hbase tables that
are divided horizontally into regions or
you could say group of rows is called
regions so in edgebase you have two
aspects one is group of columns which is
called column family and one is group of
rows which is called regions now these
regions or these rows are grouped based
on the key values or I would say row
Keys which are always unique when you
store your data in edgebase you would
have data in the form of rows and
columns so group of rows are called
regions or you could say these are
horizontal partitions of the table so a
region server manages these regions on
the Node where a data node is running a
region server can have up to thousand
regions it runs on every node and
decides the size of region so region
server as I said is a slave process
which is responsible for managing HPS
data on the Node each region server is a
worker node or a worker process
co-located with data node which will
take care of your read write update
delete request from the clients now when
we talk about more components of
edgebase as I said you have HP H master
so you would always have a connection
coming in from a client or an
application what does H Master do it
assigns regions it monitors the region
servers it assigns regions to region
servers for load balancing and it cannot
do that without the help of Zookeeper so
if we talk about components of hbase
there are three main components you have
zookeeper you have etch master and you
have region server region server being
the slave process your Edge Master being
the master process which takes care of
all your table operations assigning
regions to the region servers taking
care of read and write requests which
come from client and for all of this
Edge Master will take in help of
Zookeeper which is a centralized
coordination service so whenever a
client wants to read or write or change
the schema or any other metadata
operations it will contact H Master Edge
Master internally will contact zookeeper
so you could have edgebase setup also in
high availability mode where you could
have a active Edge master in a backup
Edge Master you would have a zookeeper
Quorum which is the way zookeeper works
so zookeeper is a centralized
coordination service which will always
run with a quorum of processes so
zookeeper would always run with odd
number of processes such as 3 5 and 7
because zookeeper works on the concept
of maturity consensus now zookeeper
which is a centralized coordination
service is keeping a track of all the
servers which are alive available and
also keeps a track of their status for
every server with zookeeper is
monitoring zookeeper keeps a session
alive with that particular server Edge
Master would always check with zookeeper
which region servers are available alive
so that regions can be assigned to the
region server at one end you have region
server which are sending their status to
the Zookeeper indicating if they are
ready for any kind of read or write
operation and at other end Edge Master
is querying the Zookeeper to check the
status now zookeeper internally manages
a meta table now that meta table will
have information of which regions are
residing on which region server and what
rookies those regions contain so in case
of a read activity Edge Master will
zookeeper to find out the region server
which contains that meta table once etch
Master gets the information of meta
table it can look into the meta table to
find out the row keys and the
corresponding region servers which
contain the regions for those row Keys
now if we would want to understand row
key and column families in hbase let's
look at this and it would be good if we
could look this on an Excel sheet so row
key is always unique it acts as a
primary key for any hbase table it
allows a logical grouping of cells and
make sure that all cells with the same
row key are co-located on the same
server so as I said you have four
coordinates for hbase you have a row key
which is always unique you have column
families which is nothing but group of
columns and when I say column families
one column family can have any number of
columns so when I talk about hbase H
base is four dimensional and in terms of
H base it is also called as a column
oriented database which basically means
that every Row in one column could have
a different data type now you have a row
key which uniquely identifies the row
you have column families which could be
one or many depending on how the table
has been defined and a column family can
have any number of columns or I could
say for every row within a column family
you could have different number of
columns so I could say for my Row 1 I
could just have two columns such as name
and City within the column family for my
Row 2 I could have name City age
designation salary for my third row I
could have thousand columns and all that
could belong to one column family so
this is a horizontally scalable database
so column family consists of group of
columns which is defined during table
creation and each column family can have
any number of column qualifiers
separated by a delimiter now a
combination of row key column family
column qualifier such as name City age
and the value within the cell is makes
the hbase a unique four dimensional
database for more information if you
would want to learn on hbase please
refer this link which is hbase mapper
and this gives a complete edgebase
architecture that has three components
of name node three components that is
name node region servers and zookeeper
how it works how Edge base Edge Master
interacts with zookeeper what zookeeper
does in coordination how are the
components working together and how does
hbase take care of read and write coming
back and continuing why do we need to
disable a table so there are different
table operations what you can didn't do
in hbase and one of them is disabling a
table now if you would want to check the
status of table you could check that my
is disabled and giving the table name or
is enabled and the table name so the
question is why do we need to disable a
table now if we would want to modify a
table or we are doing some kind of
Maintenance activity in that case we can
disable the table so that we can modify
or changes settings when a table is
disabled it cannot be accessed through
the scan command we have reached the end
of this session on Advanced Big Data
full you have mastered essential tools
and Concepts including hdfs cluster
architecture Hadoop ecosystem yarn usage
Edge page insights Big And Hive
introductions and even record Hadoop
interview questions
and with this knowledge you are ready to
excel in the world of Big Data keep
exploring Innovation and applying your
skills to reshape Industries your
journey doesn't end here it's the
beginning of your impactful data driven
future
all right with that we have come to the
end of this video If you like this video
please give it a thumbs up I hope it
will help you all thanks for watching
stay safe and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign