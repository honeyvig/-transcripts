welcome to Simply learn today we get to
start on a transformative journey into
the world of data science with our
comprehensive fill course in this
immersive learning experience you will
Master the art of extracting insights
and making datadriven decisions from the
fundamentals of data analysis and
statistics to Advanced machine learning
and data visualization techniques our
course equips you with the essential
tools and knowledge to excel in this
rapidly evolving field you will dive
deep into the real world projects
gaining hands-on experience that will
set you on a part to become a profession
you're in the wise or looking to enhance
your skills our course is your gateway
to unlocking the power of data that's
said if these are the type of videos
you'd like to watch then hit that like
And subscribe button and the bell icon
to get notified if you are a
professional with minimum one year of
experience and an aspiring data
scientist looking for online training
and certifications from the prestigious
universities and in collaboration with
leading experts to enhance your
credibility then search no more simply
La postgraduate program and data science
offered by Caltech University in
collaboration with IBM is just what you
need for more details head straight to
our homepage and search for data science
postgraduate program or simply click on
the link in the description box below
now without further delay over to our
training experts do you know how Netflix
decides which movies suit your taste
better or how Google Maps can provide
routs with low traffic at given moment
in seconds this is all possible thanks
to data science the popularity of data
science exploded in the 2010s causing an
approximate 15% year on-year growth in
the job market and is expected to grow
even higher in the coming decade now
what exactly is data science data
science is the field of study that works
with massive amounts of data utilizing
relevant tools and techniques to derive
valuable data the day-to-day work of a
data scientist involves collecting
analyzing and interpreting this data to
help businesses achieve their goals
companies across various Industries
generate vast amounts of data so its
application range all the way from
education healthare and entertainment to
finance and marketing when it comes to
the sub fates of data science it it
comprises many Technologies like
artificial intelligence machine learning
natural language processing and deep
learning each of them fulfilling
different purposes and having varied
functionalities the role of a data
scientist was voted the sexiest job of
the 21st century and it is for a good
reason the demand for professionals who
can make sense of vast amount of data is
more than ever in this day and AG
companies like Microsoft Google and
Amazon hire data scientists as they deal
with problem solving models involving
massive amounts of data daily this is
why data science professionals are in
high demand with average salaries going
as high as
$1,864 perom with 2.5 million terabytes
of data being transferred over the
internet on a daily basis the data
science Industry is related to grow even
higher promoting further Innovation and
job
opportunities Google had gathered five
exabytes of data between the beginning
of time 2003 this amount of data started
to be produced every 2 Days in 2010 and
every 40 minutes by 2021 the
responsibility of a data scientist is to
gather clean and present data and have a
keen business sense and analytical
abilities let us have a discussion about
it in the upcoming slides how can you
become a data scientist as a beginner I
guarantee you after watching this video
you will have a clear understanding on
how to drive your career as a data
scientist hey guys welcome to Simply
learn before proceeding please make sure
you subscribe to Simply learn's YouTube
channel and press the Bell icon to never
miss any updates today we are going to
cover significance of data scientist in
Industries after that prerequisites and
Technologies required for a data
scientist and finally salary offered
data scientist I have a query for you
which technology is used by Google Maps
to predict traffic jams deep learning
machine learning natural language
processing data structure please leave
the answer in the comment section below
and stay tuned to get the answer
significance now we will see how top
industries are involved in the field of
data science by 2025 the data science
Industry is anticipated to grow to a
value of $16 billion there is an AB AB
of data science jobs all over the world
now let's list out crucial areas where
data science is used media and
entertainment the major player in the
media and entertainment sector such as
YouTube Netflix hotstar Etc have begun
to use data science to better understand
their audience and provide them with
recommendations that are both relevant
and personalized e-commerce data science
has aided retail companies in better
meeting their expectations as they a
unique combination of deep data
knowledge technology skills and
statistical experience data scientists
are in high demand in the retail
industry top recruiters are Amazon
Flipkart Walmart mintra Etc digital
marketing large volumes of data are
currently being fetched from its users
through search Pages social networks
online traffic display networks movies
web pages Etc a high level of business
intelligence is needed to analyze such
large amount of data and this can only
be done with the proper use of data
science approaches top recruiters are
Amazon Flipkart Facebook Google Etc
cyber security data science and AI are
now being used by the cyber security
industry to prevent the growing usage of
algorithms for harmful purposes top
recruiters includes IBM Microsoft
Accenture Cisco and many more before
moving forward what is the resp response
to the Google Map question that I asked
answer is machine learning coming to
prerequisites now that we no
significance of data science in
Industries let us explore the
prerequisites and Technologies required
for a data scientist seeing the demand
of data scientist in every industry it
is obvious that the scope of a data
scientist is very high so how to start
there is no necessity that you should be
knowing any technology or programming
language AG you can be a Layman too data
scientists typically have a variety of
educational and professional experiences
most should be proficient in four
crucial areas important skill is
mathematical expertise three concepts
like linear algebra multivariable
calculus and optimization technique are
crucial because they aid in our
understanding of numerous machine
learning algorithms that are crucial to
data science similar to that knowing
statistics is crucial because they are
used in data analysis additionally
important to statistics probability is
regarded as a must for mastering machine
learning next is computer science in the
field of computer science there is a lot
to learn but one of the key inquiries
that arises in relation to programming
is r or Python language there are many
factors to consider when deciding which
language to choose for data science
because both have a comprehensive
collection of libraries to implement
complex machine learning learning
algorithms in addition to learning a
programming language you should learn
the following computer science skill
fundamentals of algorithm and data
structures distributed computing machine
learning deep learning Linux SQL mongodb
Etc domain expertise most individuals
wrly believe that domain expertise is
not crucial to data science yet it is
consider the following scenario if you
are interested in working as a data
scientist in the banking Industries and
you already know a lot about it for
instance you are knowledgeable about
stock trading Finance Etc this will be
very advantageous for you and the bank
itself will favor you over other
applicant and finally communication
skill it covers both spoken and written
Communication in a data science project
the project must be explained to others
when finding from the analysis have been
reached this can occasionally be a
report that you provide to your team or
employer at work sometimes it might be a
blog entry it is frequently a
presentation to a group of co-workers
regardless a data science project always
involves some form of communication of
the project findings therefore having a
good communication skill is a
requirement for being a data scientist
apart from all this practicing is very
important keep using different tools
also start reading blocks on data
science start building projects on data
science which can be added to your
resume also you can find many
interesting courses on data science by
simply learn salary reward is the result
of good work now we shall discuss
salaries that a data scientist will get
it should come as no surprise that data
scientist may add significantly to a
business every step of the process from
data processing to data cleansing
requires persistence a lot of athematic
and statistics as well as scattering of
engineering skills one of the most
important factors in a data scientist
salary is experience and at the beginner
level a data scientist can make
$95,000 annually the typical annual
compensation for a mid-level data
scientist is between
$130,000 and
$195,000 a seasoned data scientist
typically earns between
$165,000 and
$250,000 per year in India at the
beginner level a data scientist can make
9 lakh 40,000 rupees on average per year
at mid level data scientist will get 20
lakhs rupees perom and if you are at the
advanced level you will get paid an
average of rupees 25 lakhs annually this
salary will vary in different countries
the top hiring businesses in the US that
provide the highest salaries for data
scientist are apple with
$180,000 perom next is Twitter with $170
perom meta technology
$170 annually
LinkedIn
$160,000 perom pipo technology provides
150,000 perom IBM provides 14 lakhs
perom and accenter will provide you with
19 lakhs perom and finally American
Express will provide an average of 13
lakhs per if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below deep learning deep
learning was first introduced in the
1940s deep learning did not develop
suddenly it developed slowly and
steadily over seven decades many thesis
and discoveries were made on deep
learning from the 1940s to to 2000
thanks to companies like Facebook and
Google the term deep learning has gained
popularity and may give the perception
that it is a relatively New Concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that imitates how
humans gain certain types of knowledge
deep learning includes statistics and
predictive modeling deep learning makes
processes quicker and simpler which is
advantageous to data scientists to
gather and analyze and interpret massive
amounts of data having the fundamentals
discussed let's move into the different
types of deep learning neural networks
are the main component of deep learning
but neural networks comprise three main
types which contain artificial neural
networks orn convolution neural networks
or CNN and recurrent neural networks or
RNN artificial neural networks are
inspired biologically by the animal
brain com evolutional neural networks
surpass other neural networks when given
inputs such as images Voice or audio it
analyzes images by processing data
recurrent neural networks uses
sequential data or series of data
convolutional neural networks and
recurrent neural networks are used in
natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren mccullock and logician Walter
pitz attempted to quantitatively map out
how humans make decisions and carry out
thinking processes therefore the term
machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
system's accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using labeled data machines predict
output based on this data now coming to
unsupervised learning models are not
supervised using a training data set it
is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in
robotics now coming to the evolution of
AI the potential of artificial
intelligence wasn't explored until the
1950s although the idea has been known
for centuries the term artificial
intelligence has been around for a
decade still it wasn't until British
polymath Allen Turing posed the question
of why machines couldn't use knowledge
like humans do to solve problems and
make decisions we can Define artificial
intelligence as a technique of turning a
computer-based robot to work and act
like humans now let's have a glance at
the types of artificial intelligence
weak AI performs only specific tasks
like Apple Siri Google assistant and
Amazon's Alexa you might have used all
of these Technologies but the types I am
mentioning after this are under
experiment General AI can also be
addressed as artificial general
intelligence it is equivalent to human
intelligence hence an AGI system is
capable of carrying out any task that a
human can strong AI aspires to build
machines that are indistinguishable from
the human mind both General and strong
AI are hypothetical right now rigorous
research is going on on this matter
there are many branches of artificial
intelligence which include machine
learning deep learning natural language
processing robotics
expert systems fuzzy logic therefore the
correct answer for which is not a branch
of artificial intelligence is option a
data
analysis now that we have covered deep
learning machine learning and artificial
intelligence the final topic is data
science Concepts like deep learning
machine learning and artificial
intelligence can be considered a subset
of data science let us cover the
evolution of data science the phrase
data science was coined in the early
1960s to characterize a new profession
that would enable the comprehension and
Analysis of the massive volumes of data
being gathered at the time since its
Beginnings data science has expanded to
incorporate ideas and methods from other
fields including artificial intelligence
machine learning deep learning and so
forth data science can be defined as the
domain of study that handles vast
volumes of data using modern tools and
techniques to find unseen patterns
derive meaningful information
and make business decisions therefore
data science comprises machine learning
artificial intelligence and deep
learning there are a lot of areas where
data science can be used one of the very
common one is fraud detection or fraud
prevention there are a lot of fraudulent
activities or transactions primarily on
the Internet it's very easy to commit
fraud and therefore we can use data
science to either prevent or detect
fraud there are certain algorithms
machine learning algorithms that can be
used like for example some outlier
techniques clustering techniques that
can be used to detect fraud and prevent
fraud as well so who is a data scientist
rather it is actually a very generic
role that defines somebody who is
working with data is known as a data
scientist but there can be very specific
activities and the roles can be actually
much more specific what exactly a person
does within the area of data science can
be much more specific but broadly
anybody working in the area of data
science is known as a data scientist so
what does that data scientists do these
are some of the activities data
acquisition data preparation data mining
data modeling and then model maintenance
we will talk about each of these in a
great detail but at a very high level
the first step obviously is to get the
raw data which is known as data
acquisition it can be all kinds of
format and it could be multiple sources
but obviously that raw data cannot be
used as it is for performing data mining
activities or data modeling activities
so the data has to be cleaned and
prepared for using in the data models or
in the data mining activity so that is
the data preparation then we actually do
the data mining which can also include
some exploratory activities and then if
we have to do stuff like machine
learning then you need to build a
machine learning model and test the
model get insights out of it and then if
um the model is fine you deploy it and
then you need to maintain the model
because over a period of time it is
possible that you need to tweak the
model because of change in the process
or change in the data and so on so that
all comes under the model maintenance so
let's take deeper look at each of these
activities let's start with data
acquisition so the stage of data
acquisition basically the data scientist
will collect raw data from all possible
sources so this could be typically an
rdbms which is a relational database or
it can also be a non rdbms or could be
flat files or unstructured data and so
on so we need to bring all that data
from different sources if required we
need to do some kind of homogeneous
formatting so that it all fits into in
in a looks at least format from a format
perspective it looks homogeneous so that
may be requiring some kind of
transformation very often this is loaded
into what is known as data Warehouse so
this can also be sometimes referred to
as ETL or extract transform and load so
a data warehouse is like a common place
where data from different sources is
brought together so that people can
perform data science activities like
reporting or data mining or statistical
analysis and so on so data from various
sources is put in a centralized place
which is known as a data warehouse so
that is also known as ETL and in order
to do this there can be data scientist
can take help of some ETL tools there
are some existing tools that a data
scientist can take help of like for
example data stage or Talent OR
Informatica these are pretty good tools
for performing these ETL activities and
getting the data the next stage now that
you have the raw data into a data
warehouse you still probably are not in
a position to straightaway use this data
for performing the data mining
activities so that is where data
preparation comes into play and there
are multiple reasons for that one of
them could be the data is dirty there
are some missing values and so on and so
forth so a lot of time is actually spent
in this particular state so a data
scientist spends a lot of time almost 60
to 70% of the time in this part of the
project or the process which is data
preparation so there are again within
this there can be multiple sub
activities starting from let's say data
cleaning you will probably have missing
values the data there is some columns
the values are missing or the values are
incorrect or there are null values and
so on and so forth so that is basically
the data cleaning part of it then you
need to perform certain Transformations
like for example normalizing the data
and so on right so you could probably
have to modify a categorical values into
numerical values and so on and so forth
so these are transformational activities
then we may have to to handle outliers
so the data could be such that there are
a few values which are Way Beyond the
normal behavior of the data for whatever
reason either people have keyed in wrong
values or for some reason some of the
values are completely out of range so
those are known as outliers so there are
certain ways of handling these outliers
and detecting and handling these
outliers so this is a part of what is
known as exploratory analysis so you
quickly explore the data to find are
there so and you can use visual tools
like plots and identify what are the
outliers and see how we can get rid of
the outliers and so on then the next
part could be data Integrity data
Integrity is to validate for example if
there are some primary keys that all the
primary keys are populated there are
some foreign Keys then at least most of
the foreign Keys should be populated and
otherwise when we are trying to query
the data you may get wrong values and so
on so that is the data Integrity part of
of it and then we have what is known as
data reduction sometimes we may have
duplicate values we may have columns
that may be duplicated because they're
coming from different sources the same
values are there and so on so a lot of
this can be done using what is known as
data reduction and thereby you can
reduce the size of the data drastically
because very often this could be written
and data which can be removed and so on
so let's take a look at what are the
various techniques that are used for
data cleaning so we need to ensure that
the data is valid and it is consistent
and uniform and accurate so these are
the various parameters that we need to
ensure as a part of the data cleaning
process now what are the techniques that
that are use for data cleaning or so we
will see what each of these are in this
particular case and uh so what is the
data set that we have we have uh data
about a bank and its customer details so
let's take take an example and see how
we go about cleaning the data and in
this particular example we're assuming
we are using python so let's assume we
loaded this data which is the raw file.
CSV this is how the customer data looks
like and um we will see for example we
take a closer look at the geography
column we will see that there are quite
a few blank spaces so how do we go about
when we have some blank spaces or if it
is a string value then we put a empty
string here or we just use a space or
empty string if they are numerical
values then we need to come up with a
strategy uh for example we put the mean
value so wherever it is missing we find
the mean for that particular column so
in this case let's assume we have credit
score and we see that quite a few of
these values are missing so what do we
do here we find the mean for this column
for all the existing values and we found
that the mean is equal to
6386 so we kind of write a piece of code
to replace wherever there are blank
values na an is basically like null and
uh we just go ahead and say fill it with
the mean value so this is the piece of
code we are writing to fill it so all
the blanks or all the null values get
replaced with the mean value now one of
the reasons for doing this is that very
often if you have some such situation
many of your statistical functions may
not even work so that's the reason you
need to fill up these values or either
get rid of these records or fill up
these values with something meaningful
so this is one mechanism which is
basically using a mean there are few
others as we move forward we can see
what are the other ways for example we
can also say that any missing value in a
particular row if even one column the
value is missing you just drop that
particular row or delete all rows where
even a single column has missing values
so that is one way of dealing now the
problem here can be that if a lot of
data has let's say one or two columns
missing and uh we Dro many such rows
then overall you may lose out on let's
say 60% of the data as some value or the
other missing 60% of the rows then it
may not be a good idea to delete all the
rows like in that manner because then
you're losing pretty much 60% of your
data Therefore your analysis won't be
accurate but if it is only 5 or 10% then
this will work another way is only to
drop values where or rather dropped rows
where all the columns are empty which
makes sense because that means that
record is of really no use because it
has no information in it so there can be
some situations like that so we can
provide a condition saying that drop the
records where all the columns are blank
or not applicable we can also specify
some kind of a threshold let's say you
have 10 or 20 columns in a row you can
specify that maybe five columns are
blank or null then you drop that record
so again we need to take care that such
a condition such a situation the amount
of data that has been removed or
excluded is not large if it is like
maybe 5% maximum 10% then it's okay but
by doing this if you're losing out on a
large chunk of data then it may not be a
good idea you need to come up with
something better what else we need to do
next is is so the data preparation part
is done so now we get into the data
mining part so what exactly we do in
data mining primarily we come up with
ways to take meaningful decisions so
data mining will give us insights into
the data what is existing there and then
we can do additional stuff like maybe
machine learning and so on to get
perform Advanced analytics and so on so
the one of the first steps we do is what
is known as data Discovery and uh which
is basically like exploratory analysis
so we can use tools like tblo for doing
some of this so let's just take a quick
look at how we go about that so tblo is
excellent data mining or actually more
of a reporting or a bi tool and you can
download a trial version of tblo at
tableau.com or there is also tblo public
which is free and you can actually use
and play around however if you want to
use it for Enterprise uh purpose purp
then it is a commercial software so you
need to purchase license and you can
then run some of the data mining
activities say your data source your
data is in some Excel sheet so you can
select the source as Microsoft Excel or
any other format and the data will be
brought into the tblo environment and
then it will show you what is known as
dimensions and uh measures so dimensions
are all the descriptive columns so and
Tableau is intelligent enough have to
actually identify these dimensions and
measures so measures are the numerical
value so as you can see here customer ID
gender geography these are all
Dimensions non- numerical values whereas
age balance credit score and so on are
numeric values so they come under
measures so you got your data into tblo
and then you want to let's say build a
small model and you want to let's say
solve a particular problem so what is
the problem statement all right let's
say we want to analyze why customers are
leaving the bank which is known as exit
and we want to analyze and see if what
are some of the factors for exiting the
bank and we want to Let's assume
consider these uh three of them like
let's say gender credit card and
geography these as a criteria and
analyze if these are in any way
impacting or have some bearing on the
customer exiting or the customer exit
Behavior okay so let's uh use tblo and
very quickly we will be able to find out
how these parameters are affecting all
right so let's see so this is our
customer data so from a Excel sheet we
have data set about let's say 10,000
rows and we want to find out what is the
criteria let's start with gender let's
say we want to first use gender as a
criteria so table really offers an easy
drag and drop kind of a mechanism so
that makes it really really easy to
perform this kind of analysis so what we
need to do is exited says whether the
customer has exited or not so it has a
value of zero and one and then of course
you have gender and so on so we will
take these two and simply drag and drop
okay so exited and then we will put
gender and if we drag and drop into the
analysis side of of TBL Loop all right
so here what we are doing is we are
showing male female as two different
columns here and zero for people who did
not exist and one for people who exited
and that is color coded so the blue
color means people who did not exit and
uh this yellow color means people who
did exit all right so now if we pull the
data here create like bar graphs this is
how it would look uh so what is yellow
let's go back so yellow is uh who exited
and uh for the male only
16.45 person have exited and we can also
draw a reference line that will help us
or can provide aliases so these are a
lot of fancy stuff that is um provided
by tblo you can create aliases and um so
that it looks good rather than basic
labels and you can also add a reference
line so you add a reference line
something like this from here we can
make out that on an average female
customers exit more than the male
customers right so that is what we are
seeing here on an average so we have uh
analyzed based on gender we do see that
there is some difference in the male and
female Behavior now let's take the next
criteria which is the credit card so
let's see if having a credit card has
any impact on the customer exit Behavior
so just like before we drag and drop the
credit card has credit card a column if
we drag and drop here and then we will
see that there is pretty much no
difference between people having credit
card and not having credit card
20.8% of people who have no credit card
have exited and similarly 20 18% of
people who have credit card have also
exited so the credit card is not having
much of an impact that's what this piece
of analysis shows last we will basically
go and check how the geography is
impacting so once again we can drag and
drop geography column onto this side and
uh if we see here there are geographies
like I think there are about three
geographies like France Germany and uh
Spain and um we see that there is some
kind of impact with the geography as
well okay so what we derive from this is
that the credit card is really we can
ignore the credit card variable or
feature from our analysis because that
doesn't have any impact but gender and
geography we can keep and do further
analysis okay all right so what are some
of the advantages of data mining bit
more detailed analysis can help us in
predicting the future Trends and it also
helps in identifying customer Behavior
patterns okay so you can take informed
decisions because the data is telling
you or providing you with some insights
and then you take a decision based on
that if there is any fraudulent activity
data mining will help in quickly
identifying such a fraud as well and of
course it will also help us in
identifying the right algorithm for
performing more Advanced Data Mining
activities like machine learning and so
on all right so the next activity now
that we have the data we have prepared
the data and performed some data mining
activity the next step is model building
let's take a look at model building so
what is a model building if we want to
perform a more detailed data mining
activity like maybe perform some machine
learning then you need to build a model
and how do you build a model first thing
is you need to select which algorithm
you want to use to solve the problem on
hand and also what kind of data that is
available and so on and so forth so you
need to make a choice of the algorithm
and based on that you go ahead and
create a model train the model and so on
now machine learning is kind of at a
very high level classified into
supervised and unsupervised so if we
want to predict a continuous value could
be a price or a temperature or or a
height or a length or things like that
so those are continuous values and if
you want to find some of those then you
use techniques like regression linear
regression simple linear regression
multiple linear regression and so on so
these are the algorithms on the other
hand there will be situations or there
may be situations where you need to
perform unsupervised learning case of
unsupervised learning you don't have any
historical labeled data so to learn from
so that is when you use unsupervised
learning and uh some of the algorithms
in unsupervised learning are clustering
K means clustering is the most common
algorithm used in unsupervised learning
and similarly in supervised learning if
you want to perform some activity on
categorical values like for example it
is not measured but it is counted like
you want to classify whether this image
is a cat or a dog whether you want to
classify whether this customer will buy
the product or not or you want to
classify whether this email is Spam or
not spam so these are examples of
categorical values and uh these are
examp examples of classification then
you have algorithms like logistic
regression K nearest neighbor or KNN and
support Vector machine so these are some
of the algorithms that are used in this
case and similarly in case of
unsupervised learning if you need to
perform on categorical values you have
some algorithms like Association
analysis and hidden Marco model okay so
in order to understand this better let's
take an example and uh take you through
the whole process and then we will we'll
also see how the code can be written to
perform this now let's take our example
here where we want to perform a
supervised learning which is basically
we want to do a multi-linear regression
which means there are multiple
independent variables and then we want
to perform a linear regression to
predict certain value so in this
particular example we have World
happiness data so this is a data about
the happiness quotient of people from
various countries and we are trying to
predict and see whether our how our
model will perform so what is the
question that we need to ask first of
all how to describe the data and then
can we make a predictive model to
calculate the happiness score right so
based on this we can then decide on what
algorithm to use and what model to use
and so on so variables that are
available or used in this model this is
a list of variables that are available
there is a happiness rank I'll load the
data and or I'll show you the data in a
little bit so it becomes clear what are
these so there is what is known as
happiness rank happiness score which is
happiness score is more like a absolute
value whereas rank is what is the
ranking and then which country we are
talking about and within that country
which region and what kind of economy
and whether the family which family and
health details and freedom trust
generosity and so on and so forth so
there are multiple variables that are
available to us and uh the specific
details probably are not required and
there can be um in another example the
variables can be completely different so
we don't have to go into the details of
what exactly these variables are but
just enough to understand that we have a
bunch of these variables and now we need
to use either all or some of these
variables and then which we also
sometimes refer to as features and then
we need to build our model and train our
model all right so let's assume we will
use python in order to perform this
analysis or perform this machine
learning activity and I will actually
show you in our lab in in a little bit
this whole thing we will run the Live
code but quickly I will run you through
the slides and then we will go into the
lab so what are we doing here first
thing we need to do is import a bunch of
libraries in Python which are required
to perform our analysis most of these
are for manipulating the data preparing
the data and then psyit learn or SK
learn is the library which you will use
actually for this particular machine
learning activity which is linear
regression so we have numpy we have
pandas and so on and so forth so all
these libraries are imported and then we
load our data and the data is in the
form of a CSV file and there are
different files for each year so we have
data for 2015 16 and 17 and uh so we
will load this data and then combine
them concatenate them to prepare a
single data frame and uh here we are
making an assumption that you are
familiar with python so it becomes
easier if you are familiar with python
programming language or at least some
programming language so that you can at
least understand by looking at the code
so we are reading the file each of these
files for each year and this is
basically we are creating a a list of
all the names of the columns we will be
using later on you will see in the code
so we have loaded 2015 then
2016 and then also 2017 so we have
created um three data frames and then we
concatenate all these three data frames
this is what we are doing here then we
add identify which of these columns are
required which for example some of the
categorical values do we really need we
probably don't then we drop those
columns so that we don't unnecessarily
use all the columns and make the
computation complicated we can then
create some plots using plotly library
and it has some powerful features
including creation or creation of maps
and so on just to understand the pattern
the happiness quotient or how the
happiness is across all the countries so
it's a nice visualization we can see
each of these countries how they are in
terms of their happiness score this is
the legend here so the lighter colored
countries have lower ranking and so
these are the lower ranking ones and
these are higher ranking which means
that the ones with this dark colors are
the happiest ones so as you can see here
Australia and maybe this side uh us and
so on are the happiest ones okay the
other thing that we need to do is the
correlation between the happiness score
and happiness rank we can find a
correlation using a scatter plot and we
find that yes they are kind of inversely
proportion which is obvious so if the
score is high happiness score is high
then they are ranked number one for
example highest is scored as number one
so that's the idea behind this so the
happiness score given here and the
happiness rank is actually given here so
they are inversely proportional because
the higher the score the the absolute
value of the rank will be lower right
number one has the highest value of the
score and so on so they are inversely
correlated but there is a strong what
this graph shows is that there is a
strong correlation between happiness
Rank and happiness score and then we do
some more plots to visualize this we
determine that probably Rank and score
are pretty much conveying the same
message so we don't need both of them so
we will kind of drop one of them and uh
that is what we are doing here so we
drop the happiness rank and similarly so
this is one example of how we can remove
some columns which are not adding value
so we will see in the code as well how
that works moving on this is a
correlation between pretty much each of
the columns with the other columns so
this is a correlation you can plot using
plot function and uh we will see here
that for example happiness score and
happiness score are correlated strongest
correlation right because every variable
will be highly correlated to itself so
that's the reason so the darker the
color is the higher the correlation and
as so the and correlation in numerical
terms goes from 0 to one so one is the
highest value and it can only be between
zero and one correlation between two
variables can be only have a value
between 0 and one so the numerical value
can go from 0 to one and one here is
dark color and um zero is kind of dark
but it is blue color from Red it goes
down the dark blue color indicates
pretty much no correlation so the from
this heat map we see that happiness and
economy and family are probably also
help probably are the most correlated
and then it keeps decreasing after
Freedom kind of keeps decreasing and
coming to pretty much uh zero all right
so that is a correlation graph and then
we can probably use this to find out
which are the columns that need to be
dropped which do not have very high
correlation and uh we take only those
columns that we will need so this is the
codee for dropping some of the columns
once we have prepared the data when we
have the required columns then we use
psyit learn to actually split the data
first of all this is a normal machine
learning process you need to split the
data into training and test data set in
this case we are splitting into 8020 so
80 is the training data set and 20 is
the test data set so that's what we are
doing here so we use uh train test split
method or function so you have all your
training data u in xcore train the
labels in Yore train similarly xcore
test has the test data the inputs
whereas the labels are in Yore test so
that's is how and this value whether it
is 80/20 or 50/50 that is all individual
preference so in our case we are using
8020 all right and uh then the next is
to create a linear regression instance
so this is what we are doing we're
creating an instance of linear
regression and then we train the model
using the fit function and uh we are
passing X and Y which is the x value and
the label data regular input and the
label data label information then we do
the test We Run The or we perform the
evaluation on the test data set so this
is what we are doing with the test data
set and then we will evaluate how
accurate the model is and using the
psychic learn functionality itself we
can also see what are the various
parameters and what are the various
coefficients because in linear
regression you will get like a equation
of like a straight line Y is equal to
Beta 0 plus beta 1 X1 plus beta 2 X2
those beta 1 beta 2 Beta 3 are known as
the coefficients and beta 0 is The
Intercept after the training you can
actually get these information of the
model what is The Intercept value what
are the coefficients and so on by using
these functions so let's take quickly go
into the lab and take a look at our code
okay so this is my lab this is my
Jupiter notebook where the code I have
the actual code and I will take you
through this code to run this linear
regression on the world happiness data
so we will import a bunch of libraries
numpy pandas plot plotly and so on also
yeah psychic learn that's also very
important so that's the first step step
then I will import my data and uh the
data is in three parts there are three
files one for each year 2015 2016 and
2017 and it is a CSV file so I've
imported my data let's take a look at
the data quickly glance at data so this
is how it looks we have the country
region happiness Rank and then happiness
score there are some standard errors and
then what is a per capita family and so
on so and then we will keep going we
will create a list of all these column
names we will be using later so for now
just I will run this code and no need of
major explanation at this point we know
that some of these columns probably are
not required so you can use this drop
functionality to remove some of the
columns which we don't need like for
example region and standard error will
not be contributing to our model so we
will basically drop those values out
here so we use the drop and then we
created a Vector of with these names
column names that's what we are passing
here instead of giving the names of the
columns here we can pass a vector so
that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also
2017 and then we will concatenate this
data so let's do that so we have now
data frame called Happiness which is a
concatenation of both both all the three
files let's take a quick look at the
data now so most of the unwanted columns
have been removed and you have all the
data in one place for all the three
years and this is how the data looks and
if you want to take a a look at the
summary of The Columns you can say
describe and uh you will get this
information for example for each of the
columns what is the count what what is
the mean value standard deviation
especially the numeric values okay not
the categorical values so this this is a
quick way to see how the data is and uh
initial little bit of exploratory
analysis can be done here so what is the
maximum value what's the minimum value
and so on for each of the columns all
right so then we go ahead and create
some visualizations using plotly so let
us go and build a plot so if we see here
now this is the relation corelation
between happiness Rank and happiness
score this is what we have seen in the
slides as well we can see that there is
a tight correlation between them only
thing is it is inverse correlation but
otherwise they are very tightly
correlated which also says that they
both probably provide the same
information so there is not not much of
value add so we'll go ahead and drop the
happiness rank as well from our columns
so that's what we're doing here and now
we can do the creation of the
correlation heat map let us plot the
correlation heat map to see how each of
these columns is correlated to the
others and we as we have seen in the
slides this is how it looks so happiness
score is very highly correlated so this
is the legend we have seen in the slide
as well so blue color indicates pretty
much zero or very low correlation deep
red color indicates very high
correlation and the value correlation is
a numeric value and the value goes from
0 to one if the two items or two
features or columns are highly
correlated then they will be as close to
one as possible and two columns that are
not at all correlated will be as close
to zero as possible so that's how it is
for example here happiness score and
happiness score every column or every
feature will be highly correlated to
itself so it is like between them there
will be correlation value will be one so
that's why we see deep red color but
then others are for example with higher
values are economy and then health and
then maybe family and freedom so these
are generosity and Trust are not very
highly correlated to happiness score so
that is uh one quick exploratory
analysis we can do and uh therefore we
can drop the country and the happiness
rank because they also again don't have
any major impact on the analysis on our
analysis so now we have prepared our
data there was no need to clean the data
because the data was clean but if there
were some missing values and so on as we
have discussed in the slides we would
have had to perform some of the data
cleaning activities as well but in this
in case the data was clean all we needed
to do was just the preparation part so
we removed some unwanted columns and we
did some exploratory data analysis now
we are ready to perform the machine
learning activity so we use psychic
learn for doing the machine learning
psychic learn is python library that is
available for performing our uh machine
learning once again we will import some
of these libraries like pandas and numpy
and um also psychic learn first step we
will do is split the data in uh 2080
format so you have all the test data
which is 20% of the data is test data
and 80% is your training data so this
test size indicates how much of it is in
the what is the size of the test data so
remaining which is point here we are
saying 02 therefore that means training
is8 so training data is 80% all right so
we have executed that split the data and
now we create an instance of the linear
regression model so LM is our linear
regression model and we pass X and Y the
training data set and call the function
fit so that the model gets trained so
now once that is done training is done
training is completed and now what we
have to do is we need to predict the
values for the test data so the next
step is using so you see here fit will
basically run the Training Method
predict will actually predict the values
so we are passing the input values which
is the independent variables and we are
asking for the values of the dependent
variable which is which we are capturing
in Yore PR and we use the predict method
here lm. predict so this will give us
all the predicted y values and remember
we already have Yore test has the actual
values which are the labels so that we
can use these two to compare and find
out how much of it is error so that's
what we are doing here we are trying to
find the difference between the
predicted value and the actual value
Yore test is the actual value for the
test data and Yore predict is the
predicted value we just found out the
predicted value so we will run that and
we can do a quick check as to how the
data looks how is the difference so in
some cases it is positive some cases it
is negative but in most of the cases I
think the difference is very small this
is exponential to the power of 0 minus
04 and so on so looks like our model has
performed reasonably well we can now
check some of the parameters of our
model like the intercept and the
coefficients so that's what we are doing
here so these are the coefficients of
the various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variable so
the coefficients against the
corresponding independent variable so
1.51 is the coefficient for economy
99983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of psychic learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that the root mean square error
is pretty low which is a good sign and
uh which is one of the measures of U how
well our model is performed forming we
can do one more quick plot to just see
how the actual values and the predicted
values are looking and once again you
can see that as we have seen from the
root mean square error the root mean
square error is very very low that means
that the actual values and the predicted
values are pretty much matching up
almost matching up and this plot also
shows the same so this line is going
through the predicted values and the
actual values and the difference is very
very low so again this is actual data
this is one example where where the
accuracy is high and the predicted
values are pretty much matching with the
actual values but in real life you may
find that these values are slightly more
scattered and you may get the error
value can be relatively On The Higher
Side the root me Square okay so this was
a good and quick example of uh the code
to perform data science activity or
machine learning or data mining activity
in this case we did what is known as
linear regression so let's go back to
our slide es and see what else is there
so we saw this these are the
coefficients of each of the features in
our code and uh we have seen the root
mean square error as well and uh with we
can take just few hundred countries of
certain values and actually predict to
see if how the model is performing and I
think we have done this as well and in
this case as we have seen pretty much
the predicted values and the actual
values are pretty much matching which
means our model is almost 100% accurate
as I mentioned real life it may not be
the case but in this particular case we
have got a pretty good model which is
very good also subsequently we can
assume that this is how the equation in
linear regression the model is nothing
but an equation like Y is equal to Beta
0 plus beta 1 X1 plus beta 2 X2 plus
beta 3 X3 and so on so this is what we
are showing here so this is our
intercept which is beta 0 and then we
have beta 1 into economy value beta beta
2 into the family value beta 3 into
health value and so on so that is what
is shown here okay so I think the next
step once we have the results from the
data mining or machine learning activity
the next step is to communicate these
results to the appropriate stakeholders
so that is what we will see here now so
how do we communicate usually you take
these results and then either prepare a
presentation or put it in a document and
then show them these actionable results
orable insights and uh you need to find
out who are your target audience and put
all the results in context and uh maybe
if there was a problem statement you
need to put those results in the context
of the problem statement what was our
initial goal that we wanted to achieve
so that we need to communicate here
based on you remember we started off
with what is the question and what is
the data and so on and then what is the
answer so we we need to put the results
and then what is the methodology that we
have used all that has to be put and
clearly communicated in business terms
so that the people understand very well
from a business perspective so once the
model building is done once the results
are published and communicated the last
part is maintenance of this model now
very often what can happen is the model
may have to be subsequently updated or
modified because of multiple reasons
either the the data has changed the way
the data comes has changed or the
process has changed or for whatever
reason the accuracy may keep changing
once you have trained the model the for
example we got a very high accuracy but
then over a period of time there can be
various factors which can cause that so
from time to time we need to check
whether the model is performing well or
not the accuracy needs to be tested once
in a while and if required you may have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model with the latest data
that you have and then you deploy it you
build the model train it and then you
deploy it so that is like the
maintenance cycle that you may have to
take the model data analyst versus data
engineer versus data scientist which one
to choose this is one of the most
popular questions asked by learners
looking for a career in data and
analytics I'm sure YouTu would have come
across these job roles in the ever
growing data science landscape though
they all deal with data these jobs are
not the same there are significant
differences between what a data analyst
data engineer and a data scientist does
we will look at these job roles and the
differences in
detail
first let's look at some data analytics
and data science
Trends the analytics and data science
Market is thriving data analytics data
engineering and data science are the key
trends in today's accelerating Market as
per status.com the global big data
analytics Market Revenue will grow at a
cagr of 30% with Revenue reaching over
68 billion US by
2025 according to technavio the
Enterprise data management Market is
expected to increase by
64.8 billion US by
2025 as per marketsand markets.com the
big data market size is projected to
grow from
$62.65
73.4 billion US in
2026 now another report from Research
Drive says that the data science
platform Market is estimated to reach
224 .3 billion by
2026 so with so much data available and
companies making huge Investments to
drive business insights the job
opportunities for data analysts data
engineers and data scientists are going
to increase in 2022 and over the coming
years now let's learn the major
differences between data analyst versus
data engineer versus data
scientist so who are
they a data analyst analyzes and
interprets vast volumes of data in order
to extract meaningful information out of
it they find solutions to a business
problem and make critical business
decisions the insights provided by data
analysts are important to companies that
want to understand the needs of their
end
customers talking about who a data
engineer is a data engineer on the other
hand builds infrastructure and scalable
pipelines to manage the flow of data and
prepare it for
analysis so basically they optimize the
systems that enable data analyst and
data scientists to perform their job
efficiently data scientists are
professionals who analyze and visualize
existing data and use algorithms to
build predictive models for making
future decisions they also engage with
Business Leaders to understand their
needs and present complex
findings with that let's look at the
primary roles and responsibilities of
these three job
roles data analysts are responsible to
collect clean store and process data
they discover hidden patterns from data
by performing exploratory data analysis
and visualize data by creating charts
and graphs acquiring data from primary
and secondary sources is one of their
key tasks the build reports and
dashboards and also maintain
databases
now talking about the roles and
responsibilities of a data engineer a
data engineer performs data acquisition
that design build and test data as well
to develop and maintain data
architecture data Engineers are tasks
with testing integrating managing and
optimizing data from a variety of
sources so they integrate data into
existing data pipelines prepare data for
modeling and perform various ETL
operations now talking about the roles
and responsibilities of a data
scientist so data scientists develop
machine learning models to identify
Trends in data for making decisions they
develop hypothesis and use the knowledge
of Statistics data visualization and
machine learning to forecast the future
for the business data scientists
visualize data and use storytelling
techniques and also write programs to
automate data collection and
processing now move on to the skills
possessed by data analysts data
engineers and data
scientists to become a data analyst you
need to have good hands-on experience
with writing SQL queries you should have
excellent Microsoft Excel skills for
analyzing data data analysts are also
good at programming and they need to
know how to visualize data solve
business problems and possess domain
knowledge data Engineers should have a
solid understanding of SQL mongodb and
programming
they need to have a good command of data
architecture scripting data warehousing
and ETL data Engineers are also good at
Hadoop based
analytics now talking about the skills
for a data scientist so a data scientist
should have experience with programming
in Python and R they should have a very
good understanding of mathematics and
statistics as well data scientists need
to possess analytical thinking and data
visualization skills as well
machine learning deep learning and
decision making are other critical
skills every data scientist should
have now we look at the salaries of a
data scientist a data analyst as well as
a data
engineer so a data analyst in the United
States earns over $70,000 perom while in
India a data analyst can earn nearly 7
lakh 25,000 rupees
perom a data engineer in the United
States can earn over $112,500 per year
and in India you can earn over 9 lakh
rupees
perom talking about the salary of a data
scientist a data scientist in the United
States earns over
$117,000 perom and in India a data
scientist can earn over 11 lakh rupees
perom coming to the final section of
this video we'll look at the top
companies hiring for data analysts data
engineers and data
scientists so we have the first company
is Google then we have Tesla next we
have the e-commerce giant Amazon the
internet giant Facebook or the social
media giant Facebook we have the tech
giant Oracle we also have Verizon and
afbn B so these are some of the top
companies that hire for the three roles
if if you are an aspiring data scientist
who's looking out for online training
and certification in data science from
the best universities and Industry
experts then search no more simply
learns post-graduate program in data
science from calch University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below now let's talk
about the life cycle of a data science
project okay the first step is the
concept study in this step it involves
understanding the business problem
asking questions get a good
understanding of the business model meet
up with all the stakeholders understand
what kind of data is available and all
that is a part of the first step so here
are a few examples we want to see what
are the various specifications and then
what is the end goal what is the budget
is there an example of this kind of a
problem that has been maybe solved
earlier so all this is a part of the
concept study and another example could
be a very specific one to predict the
price of a 1.35 karat diamond and there
may be relevant information inputs that
are available and we want to predict the
price the next step in this process is
data preparation data Gathering and data
preparation also known as data monging
or sometimes it is also known as data
manipul infation so what happens here is
the raw data that is available may not
be usable in its current format for
various reasons so that is why in this
step a data scientist would explore the
data he will take a look at some sample
data maybe pick there are millions of
Records pick a few thousand records and
see how the data is looking are there
any gaps is the structure appropriate to
be fed into the system are there some
columns which are probably not adding
value may not be required for the
analysis very often these are like names
of the customers they will probably not
add any value or much value from an
analysis perspective the structure of
the data Maybe the data is coming from
multiple data sources and the structures
may not be matching what are the other
problems there may be gaps in the data
so the data all the columns all the
cells are not filled if you're talking
about structured data there are several
blank records or blank columns so if you
use that data directly you'll get errors
or you'll get inaccurate results so how
do you either get rid of the data or how
do you fill this gaps with something
meaningful so all that is a part of data
monging or data manipulation so these
are some additional subtopics within
that so data integration is one of them
if there are any conflicts in the data
there may be data may be redundant yeah
data res redundancy is another issue
there may be you have let's say data
coming from two different systems and
both of them have customer table for
example customer information so when you
merge them there is a duplication issue
so how do we resolve that so that is one
data transformation as I said there will
be situations where data is coming from
multiple sources and then when we merge
them together they may not be matching
so we need to do some transformations to
make sure everything is set similar we
may have to do some data reduction if
the data size is too big you may have to
come up with ways to reduce it
meaningfully without losing information
then data cleaning so there will be
either wrong values or you null values
or there are missing values so how do
you handle all of that a few examples of
very specific stuff so if there are
missing values how do you handle missing
values or null values here in this
particular slide we are seeing three
types of issues one is missing value
then you have null value you see the
difference between the two right so in
the missing value there is nothing blank
null value it says null now the system
cannot handle if there are null values
similarly there is improper data so it's
supposed to be numeric value but there
is a string or a non-numeric value so
how do we clean and prepare the data so
that our system can work flawlessly so
there are multiple ways and and there is
no one common way of doing this it can
vary from Project to project it can vary
from what exactly is the problem you're
trying to solve it can vary from data
scientist to data scientist organization
to organization so these are like some
standard practices people come up with
and and of course there will be a lot of
trial and error somebody would have
tried out something and it worked and
will'll continue to use that mechanism
so that's how we need to take care of
data cleaning now what are the various
ways of doing you know if if values are
missing how do you take care of that now
if the data is too large and um only a
few records have some missing values
then it is okay to just get rid of those
entire rows for example so if you have a
million records and out of which 100
records don't have full data so there
are some missing values in about 100
records so it's absolutely fine because
it's a small percentage of the data so
you can get rid of the entire records
which have missing values but that's not
a very common situation very often you
will have multiple or at least you know
large number of data set for example out
of million records you may have 50,000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so how
do you handle such situations so there
are again multiple ways of doing it one
is you can probably if a particular
values are missing in a particular
column you can probably take the mean
value for that particular column and
fill all the missing values with the
main value so that first of all you
don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taking the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation
you need to split the data into training
and test data set the reason being if
you try to test with a data set which
the system has already seen as a part of
training then it will tend to give
reasonably accurate results because it
has already seen that data and that is
not a good measure of the accuracy of
the system so typically you take the
entire data set the input data set and
split it into two parts and again the
ratio can vary from person to person
individual preferences some people like
to split it into 50/50 some people like
it as
63.33% testing so you split the data
perform the training with the 80% and
then use the remaining 20% for testing
all right so that is one more data
preparation activity that needs to be
done before you start analyzing or
applying the data or putting the data
through the model then the next step is
model planning now these models can be
statistical models this could be machine
learning models so you need to decide
what kind of models you're going to use
again it depends on what is a problem
you're trying to solve if it is a
regression problem you need to think of
a regression algorithm and come up with
a regression model so it could be linear
regression or if you're talking about
classification then you need to pick a
an appropriate classification algorithm
like logistic regression or decision
tree or svm and then you need to train
that particular model so that is the
model building or model planning process
and the cleaned up data has to be fed
into the model and apart from cleaning
you may also have to in order to
determine what kind of model you will
use you have to perform some exploratory
data analysis to understand the
relationship between between the various
variables and U see if the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data analysis
so what exactly is exploratory data
analysis is basically to as the name
suggests you're just exploring you just
received the data and you're trying to
explore and uh find out what are the
data types and what is the is the data
clean in in each of the columns what is
the maximum minimum value so for example
example there are out of the box
functionality available in tools like R
so if you just ask for a summary of the
table it will tell you for each column
it will give some details as to what is
the mean value what is the maximum value
and so on and so forth so this exercise
or this exploratory analysis is to get
an understanding of your data and then
you can take steps to during this
process you find that a lot of missing
values you need to take steps to fix
those you will also get an idea about
what kind of model to be be used and so
on and so forth what are the various
techniques used for exploratory data
analysis typically these would be
visualization techniques like you use
histograms uh you can use box plots you
can use cater plots so these are very
quick ways of identifying the patterns
or a few of the trends of the data and
so on and then once your data is ready
you you've decided on the model what
kind of model what kind of algorithm
you're going to use if you're trying to
do machine learning you need to pass
your 80% the training data or rather you
use that training data to train your
model and the training process itself is
iterative so the training process you
may have to perform multiple times and
once the training is done and you feel
it is giving good accuracy then you move
on to test so you take the remaining 20%
of the data remember we split the data
into training and test so the test data
is now used to check the accuracy or how
well our model is performing and if if
there are further issues let's say and
model is still during testing if the
accuracy is not good then you may want
to retrain your model or use a different
model so this whole thing again can be
iterative but if the test process is
passed or if the model passes the test
then it can go into production and it
will be deployed all right so what are
the various tools that we use for model
planning R is an excellent tool in a lot
of ways whether you're doing regular
statistical analysis or machine learning
or any of these activities are in along
with our studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detail analysis and
machine learning and so on and so forth
then of course you can write python
programs python offers a rich library
for performing data analysis and machine
learning and so on matlb is a very
popular tool as well especially during
education so this is a very easy to
learn tool so math lab is another uh
tool that can be used and then last but
not least SAS SAS is again very powerful
it is a reparatory tool and it has all
all the components that are required to
perform very good statistical analysis
or perform data science so those are the
various tools that would be required for
or that that can be used for model
building and uh so the next step is
model building so we have done the
planning part we said okay what is the
algorithm we are going to use what kind
of model we going to use now we need to
actually train this model or build the
model rather so that it can then be
deployed so what are the various uh ways
or what are the various types of model
building activities so it could be let's
say in this particular example that we
have taken you want to find out the
price of 1.35 karat diamond so this is
let's say a linear regression problem
you have data for various carats of
diamond and you use that information you
pass it through a linear regression
model or you create a linear regression
model which can then predict your price
for 1.35 karat so this is one example of
model building and then little bit
details of how linear regression works
so linear regression is basically coming
up with a relation between an
independent variable and a dependent
variable so it is pretty much like
coming up with equation of a a straight
line which is the best fit for the given
data so like for example here Y is equal
= mx + C so Y is the dependent variable
and X is the independent variable we
need to determine the values of M and C
for our given data so that is what the
training process of uh this model does
at the end of the training process you
have a certain value of M and c and um
that is used for predicting the values
of any new data that comes all right so
the way it works is we use use the
training and the test data set to train
the model and then validate whether the
model is working fine or not using test
data and uh if it is working fine then
it is taken to the next level which is
put in production if not the model has
to be retrained if the accuracy is not
good enough then the model is retrained
maybe with more data or you come up with
a newer model or algorithm and then
repeat that process so it is an
iterative process once the training is
completed training and test then this
model is deployed and we can use this
particular model to determine what is
the price of 1.35 karat diamond remember
that was our problem statement so now
that we have the best fit for this given
data we have the price of 1.35 karat
diamond which is 10,000 so this is one
example of how this whole process works
now how do we build the model there are
multiple ways you can use Python for
example and use libraries like pandas or
numpy to build a model and implement it
this will be available as a separate
tutorial a separate video in this
playlist so stay tuned for that moving
on once we have the results the next
step is to communicate this results to
the appropriate stakeholders so it is
basically taking this results and
preparing like a presentation or a
dashboard and communicating these
results to the concerned people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and present it to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to overcome this problem
problem or solve this problem so that is
the pretty much once that is accepted
and the last step is to operationalize
so if everything is fine your data
scientists presentations are accepted
then they put it into practice and
thereby they will be able to improve or
solve the problem that they stated in
step one okay so quick summary of the
life cycle you have a concept study
which is basically understanding the
problem asking the right questions and
and trying to see if there is uh enough
data to solve this problem and then even
maybe gather the data then data
preparation the raw data needs to be
manipulated you need to do data monging
so that you have the data in a certain
proper format to be used by the model or
our analytics system and then you need
to do the model planning what kind of a
model what algorithm you will use for a
given problem and then the model
building so the exact execution of that
model happens in step four and you
implement and execute that model and uh
put the data through the analysis in
this step and then you get the results
this results are then communicated
packaged and presented and communicated
to the stakeholders and once that is
accepted that is operationalized so that
is the final let's begin this lesson by
defining the term statistics statistics
is a mathematical science pertaining to
the collection presentation analysis and
interpretation of data it's widely used
to understand the complex problems of
the real world and simplify them to make
well-informed decisions several
statistical principles functions and
algorithms can be used to analyze
primary data build a statistical model
and predict the
outcomes an analysis of any situation
can be done in two ways statistical
analysis or a non-statistical
analysis statistical analysis is the
science of collecting exploring and
presenting large amounts of data to
identify the patterns and Trends
statistical analysis is also called
quantitative analysis non-statistical
analysis provides generic information
and includes text sound still images and
moving images non-statistical analysis
is also called qualitative analysis
although both forms of analysis provide
results statistical analysis gives more
insight and a clearer picture a feature
that makes it vital for
businesses there are two major
categories of Statistics descriptive
statistics and inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set
suppose you want to study the height of
students in a classroom in the
descriptive statistics you would record
the height of every person in the
classroom and then find out the maximum
height minimum height and average height
of the
population inferential statistics
generalizes the larger data set and
applies probability Theory to draw a
conclusion it allows you to infer
population parameters based on the
sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the inner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential statistics
you would categorize height as tall
medium and small and then take only a
small sample from the population to
study the height of students in the
classroom the field of Statistics
touches our lives in many ways from the
daily routines in our homes to the
business of making the greatest cities
run the effective statistics are
everywhere there are various statistical
terms that one should be aware of while
dealing with Statistics population
sample variable quantitative variable
qualitative variable discret variable
continuous
variable a population is the group from
which data is to be
collected a sample is a subset of a
population a variable is a feature that
is characteristic of any member of the
population differing in quality or
quantity from another member a variable
differing in quantity is called a
quantitative variable for example the
weight of a person number of people in a
car a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an
accident a discrete variable is one
which no value can be assumed between
the two given values for example the
number of children in a
family a continuous variable is one in
which any value can be assumed between
the two given values for example the
time taken for a 100 meter run typically
there are four types of statistical
measures used to describe the data they
are measures of frequency measures of
central tendency measures of spread
measures of position
let's learn each in detail frequency of
the data indicates the number of times a
particular data value occurs in the
given data set the measures of frequency
are number and
percentage central tendency indicates
whether the data values tend to
accumulate in the middle of the
distribution or toward the end the
measures of central tendency are mean
median and
mode spread describes how similar or
varied the set of observ values are for
a particular variable the measures of
spread are standard deviation variance
and quartiles the measure of spread are
also called measures of
dispersion position identifies the exact
location of a particular data value in
the given data set the measures of
position are percentiles quartiles and
standard scores statistical analysis
system or SAS provides a list of
procedures to perform descriptive
statistics they are as follows proc
print proc contents proc means proc
frequency proc Univar proc
gchart proc boxplot proc
gplot proc print it prints all the
variables in a SAS data set proc
contents it describes the structure of a
data
set proc means it provides data
summarization tools to compute
descriptive statistics for VAR Ables
across all observations and within the
groups of
observations proc frequency it produces
oneway to inway frequency and cross
tabulation tables frequencies can also
be an output of a SAS data
set proc univariate it goes beyond what
proc means does and is useful in
conducting some basic statistical
analyses and includes high resolution
graphical
features proc G chart
the g- chart procedure produces six
types of charts block charts horizontal
vertical bar charts Pi donut charts and
Star Charts these charts graphically
represent the value of a statistic
calculated for one or more variables in
an input SAS data set the Tred variables
can be either numeric or
character proc box plot the box plot
procedure creates side bys side box and
whisker plots of measurements organ ized
in groups a box and whisker plot
displays the mean quartiles and minimum
and maximum observations for a
group proc gplot gplot procedure creates
two-dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axis plots
against the second vertical axis bubble
plots and logarithmic plots in this demo
you'll learn how to use descriptive
statistics statistics to analyze the
mean from the electronic data set let's
import the electronic data set into the
SAS console in the left plane rightclick
the electronic. xlsx data set and click
import
data the code to import the data
generates automatically copy the code
and paste it in the new
window the proc me means procedure is
used to analyze the mean of the imported
data
set the keyword data identifies the
input data set in this demo the input
data set is
electronic the output obtained is shown
on the
screen note that the number of
observations mean standard deviation and
maximum and minimum values of the
electronic data set are
obtained
this concludes the demo on how to use
descriptive statistics to analyze the
mean from the electronic data set so far
you have learned about descriptive
statistics let's now learn about
inferential
statistics hypothesis testing is an
inferential statistical technique to
determine whether there is enough
evidence in a data sample to infer that
a certain condition holds true for the
entire population to understand the
characteristics of the general
population we take a random sample and
analyze the properties of the sample we
then test whether or not the identified
conclusions correctly represent the
population as a whole the population of
hypothesis testing is to choose between
two competing hypotheses about the value
of a population
parameter for example one hypothesis
might claim that the wages of men and
women are equal while the other might
claim that women make more than
men hypothesis testing is formulated in
terms of two hypothesis
null hypothesis which is referred to as
H null alternative hypothesis which is
referred to as
H1 the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary the alternative hypothesis
is assumed to be true when the null
hypothesis is proven false let's
understand the null hypothesis and
alternative hypothesis using a general
example null hypothesis attempts to show
that no variation exists between
variables and alternative hypothesis is
any hypothesis other than the null for
example say a pharmaceutical company has
introduced a medicine in the market for
a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe if the medicine is proved to be
safe then it is referred to as null
hypothesis to reject null hypothesis we
should prove that the medicine is unsafe
if the null hypothesis is rejected Ed
then the alternative hypothesis is
used before you perform any statistical
tests with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types they are categorical or nominal
variables ordinal variables interval
variables and ratio
variables nominal variables are ones
which have two or more categories and
it's impossible ible to order the values
examples of nominal variables include
gender and blood group ordinal variables
have values ordered logically however
the relative distance between two data
values is not clear examples of ordinal
variables include considering the size
of a coffee cup large medium and small
and considering the ratings of a product
bad good and best interval variables are
similar to ordinal variables except that
the values are measured in a way way
where their differences are meaningful
with an interval scale equal differences
between scale values do have equal
quantitative meaning for this reason an
interval scale provides more
quantitative information than the
ordinal scale the interval scale does
not have a true zero point a true zero
point means that a value of zero on the
scale represents zero quantity of the
construct being assessed examples of
interval variables include the
Fahrenheit scale used to measure
temperature and distance between two
compartments in a
train ratio scales are similar to
interval scales in that equal
differences between scale values have
equal quantitative meaning however ratio
scales also have a true zero point which
give them an additional property for
example the system of inches used with a
common ruler is an example of a ratio
scale there is a true zero point because
0o Ines does in fact indicate a complete
absence of length
in this demo you'll learn how to perform
the hypothesis testing using
SAS in this example let's check against
the length of certain observations from
a random
sample the keyword data identifies the
input data
set the input statement is used to
declare the Aging variable and cards to
read data into
SAS
let's perform a t test to check the null
hypothesis let's assume that the null
hypothesis to be that the mean days to
deliver a product is 6
days so null hypothesis equals 6 Alpha
Al value is the probability of making an
error which is 5% standard and hence
Alpha equals
0.05 the variable statement names the
variable to be used in the
analysis the output is shown on the
screen note that the P value is greater
than the alpha value which is
0.05 therefore we fail to reject the
null
hypothesis this concludes the demo on
how to perform the hypothesis testing
using
SAS let's now learn about hypothesis
testing procedures there are two types
of hypothesis testing procedures they
are parametric tests and non-parametric
tests in statistical inference or
hypothesis testing the traditional tests
such as test and an NOA are called
parametric tests they depend on the
specification of a probability
distribution except for a set of free
parameters in simple words you can say
that if the population information is
known completely by its parameter then
it is called a parametric test if the
population or parameter information is
not known and you are still required to
test the hypothesis of the population
then it's called a nonparametric test
non-parametric tests do not require any
strict distributional assumptions there
are various parametric tests they are as
follows T Test Anova chai squared linear
regression let's understand them in
detail T Test a t test determines if two
sets of data are significantly different
from each other the T test is used in
the following
situations to test if the mean is
significantly different than a
hypothesized value to test if the mean
for two independent groups is
significantly different to test if the
mean for two dependent or paired groups
is significantly
different for example let's say you have
to find out which region spends the
highest amount of money on shopping it's
impractical to ask everyone in the
different regions about their shopping
expenditure in this case you can
calculate the highest shopping
expenditure by collecting sample
observations from each region with the
help of the T Test you can check if the
difference between the regions are
significant or a statistical
fluke an NOA a NOA is a generalized
version of the T Test and used when the
mean of the interval dependent variable
is different to the categorical
independent variable when we want to
check variance between two or more
groups we apply the Anova
test for example let's look at the same
example of the T Test example now you
want to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical
fluke chai
square chai square is a statistical test
used to compare observed data with data
you would expect to obtain according to
a specific
hypothesis let's understand the CH
Square test through an example you have
a data set of male Shoppers and female
Shoppers let's say you need to assess
whether the probability of females
purchasing items of $500 or more is
significantly different from the
probability of males purchasing items of
$500 or more linear
regression there are two types of linear
regression simple linear regression and
multiple linear regression simple linear
regression is used when one wants to
test how well a variable predicts
another variable multiple linear
regression allows one to test how well
multiple variables or independent
variables predict a variable of interest
when using multiple linear regression We
additionally assume the predictor
variables are
independent for example finding
relationship between any two variables
say sales and profit is called Simple
linear
regression finding relationship between
any three variables say sales cost
telemarketing is called multiple linear
regression some of the non-parametric
tests are W coxen rank sum test and
crustal Wallace h test will coxen rank
sum test the wi coxen signed rank test
is a non-parametric statistical
hypothesis test used to compare two
related samples or matched samples to
assess whether or not their population
mean ranks differ in W coxen rank sum
test you can test the null hypothesis on
the basis of the ranks of the
observations crusco Wallis h test crusco
Wallace h test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples the advantages of parametric
tests are as follows provide information
about the population in terms of
parameters and confidence
intervals easier to use in modeling
analyzing and for describing data with
Central Tendencies and data
Transformations Express the relationship
between two or more
variables don't need to convert data
into rank order to
test the disadvantages of parametric
tests are as follows
only support normally distributed data
only applicable on variables not
attributes let's Now list the advantages
and disadvantages of non-parametric
tests the advantages of non-parametric
tests are as follows simple and easy to
understand do not involve population
parameters and sampling Theory make
fewer
assumptions provide results similar to
parametric
procedures the disadvantages of
non-parametric tests are as follows not
as efficient as parametric tests
difficult to perform operations on large
samples manually we'll discuss the types
of distribution in
statistics but before we move ahead
let's have a brief introduction on what
is probability distribution a
probability distribution is a list of
all of the possible outcomes of a random
variable along with the corresponding
probability
values and it is used in many fields but
we rarely do explain what they are so in
this video we'll discuss the three main
types of probability distribution that
is normal binomial and poison
distribution so let's move
ahead so what is normal
distribution normal distribution is a
continuous probability density that has
a probability density function which
gives us a symmetrical bell curve now
data can be distributed or spread out in
different ways but there are many cases
where the data tends to be around a
central value with no bias to the left
or right which means that it doesn't
show any particular spikes towards the
left or the right and it gets close to a
normal distribution half of the data
will fall on the left of the mean and
the other half will fall on the right
now let's take a look at a graph which
shows the height distribution in a
glass as you can see the average he
height is in the middle and the data to
the left of the average height
represents the short people and the data
to the right of it represents the taller
people the y axis shows us the
likelihood of any of these Heights
occurring the average height has the
most distribution or it has the most
number of cases in the class and as the
height decreases or increases the number
of people who have that height also
decreases this kind of a distribution is
called a normal distribution where the
average or the mean is always the
highest point and any other point after
that or before that is significantly
lower the resulting data gives us a bell
curve and as you can see there is no
abrupt bias or spike in the data
anywhere except for the average height
so this kind of a curve is called a bell
curve and it's usually seen in a normal
distribution the reason we call this a
normal distribution is because the data
is normally distributed with the average
being the highest and all the other data
points having a lower
likelihood now we came across two terms
which are associated with normal
distribution continuous probability
density and probability density function
what is continuous probability density
continuous probability density is a
probability distribution where the
random variable X can take any given
value because there are infinite values
that X could assume the probability of X
taking on any specific value zero for
example let's say you have a continuous
probability density for men's height
what is the probability that a man will
have the exact height of 70
in it is impossible to find this out
because the probability of one man
measuring exactly 70 in is very low it
is more probable that he will measure
around 70.1 in or maybe 69. 97 in and it
doesn't stop
there the fact is that it's impossible
to exactly measure any variable that's
on a continuous scale and because of
this it's impossible to figure out the
probability of one exact measurement
which is occurring in a continuous
probability
density next we have the probability
density function it's nothing but a
function or an expression which is used
to define the range of values that a
continuous random variable can take an
example of this would be to gge the risk
and reward of a
stock a probability density function is
a statistical measure which is used to
gge the likelihood of a discrete value a
discrete variable can be measured
exactly while a continuous variable can
have infinite
values however for both continuous as
well as discrete variables we can Define
a
function which gives us the range of
values within
which these variables will fall and that
function is known as the probability
density
function now let's take a look at
standard
deviation what is standard deviation
standard deviation is used to measure
how the values in your data differ from
one another or how spread out your data
is a standard deviation is statistic
that measures the dispersion of a data
set relative to its mean the standard
deviation is calculated as a square root
of variance by determining each data
Point's deviation relative to the mean
if the data points are further from the
mean that means that there's a higher
deviation within the data set and then
the data is set to be more spread
out this leads to a higher standard
deviation to let's take an example
example of income in rural and urban
areas in rural areas let's say such as
farming areas the income doesn't differ
that much more or less everyone earns
the same because of this a bell curve
has a very low standard deviation and it
has a very narrow
Peak however in urban areas the well
distribution is very uneven some people
can have very high incomes and can be
earning a lot while other people can
have very low incomes the furthermore
the data distribution between these two
income points is going to be more spread
out because there are a lot more people
living there who work in various fields
and who have various incomes because of
this our standard deviation is more
spread out and our bell curve will also
have a wider
Peak now how can we find the standard
deviation standard deviation is obtained
by subtracting each data value from the
mean and finding the squared average of
these values let let's look at how we
can do this with the help of an example
these values correspond to the height of
various
dogs we can find the mean by finding the
average of all these values which is
nothing but adding all the values and
dividing it by the total number of
values the mean that we get is
394 this means that the average height
of a dog is 394
mm to find the standard deviation first
we need to subtract the height from the
mean this will tell us how far from the
mean our data points actually
are next we will square up all of these
differences and add them up and again
divide it by the total number of values
that we have this is called the
variance the variance that we get in
this case is
21704 finally when we find the square
root of this value we will get the
standard deviation the standard
deviation here is 147 the standard
deviation will tell us how our data
points differ from the average
and it gives us a basic value suggesting
how spread out our data is from the very
middle or from the mean so when we plot
these values this value 147 will mean
that a curve will have a width of 147
points around the
mean now what is the standard normal
distribution the standard normal
distribution is a type of normal
distribution that has a mean of zero and
a standard deviation of one this means
that the normal distribution has its
Center at zero and it has intervals
which increase by one all normal
distributions like the standard normal
distribution are unimodel and
symmetrically distributed with a
bell-shaped curve however a normal
distribution can take on any value as
its mean and standard deviation in the
standard normal distribution however the
mean and standard deviation are always
fixed when you standardize a normal
distribution the mean becomes zero and
the standard deviation becomes one this
allows you to easily calculate the
probability of certain values occurring
in your distribution or to compare data
sets with different mean and standard
deviations the curve shows a standard
normal distribution as you can see again
the data is centered at
zero this does not mean that the data
necessarily starts at zero this means
that after standardizing this point is
where a mean will lie in a standard
normal distribution the standard
deviation is one so all the data points
will increase or decrease in steps of
one let's better understand a standard
normal distribution with the help of an
example again as you can see the data is
centered around zero which is nothing
but the
mean let's again consider the weights of
students in class 8th the average weight
here is around 50 kgs and the data
increases and decreases in steps of five
the the data over here in this curve is
evenly distributed along these steps
this is what a standard normal
distribution will look
like we already know that the mean of
our data is 50 and because the data is
increasing and decreasing in equal steps
we can just standardize it and take it
to mean that the data is increasing and
decreasing in steps of one this is what
a standard normal distribution look
looks like and when you have a data
which looks like this you can always
standardize it and convert it into a
standard normal distribution now
standard normal distribution has a
couple of properties which makes
calculation comparatively
easy the first one is that 68% of the
values fall within the first standard
deviation which means that 68% of all
data values on this Curve will fall
between the range of minus1 to 1 or the
first interval ranging from minus1 to
1 the second property is that 95% of the
rest of the values are within the second
standard deviation or from the second
negative point to the second positive
point and
finally
99.7% of the values fall within the
third standard deviation or from the
third negative point to the third
positive point this makes calculations
on standard normal distribution fairly
easy you can compare scores on different
distributions with different means and
standard deviations you can normalize
scores for statistical decision making
using standard normal distribution you
can find the probability of observations
in a distribution which fall above or
below a given value and find you can
find the probability that a mean
significantly differs from a population
mean now let's take a look at
zcore so what is a zcore a zcore is used
to tell plus how far from the mean our
data point actually
is it is calculated using the mean and
standard deviation so it can be said
that the Z score is how many standard
deviations below the mean our data is
basically by using the Z score we can
get an approximate location of where our
data point lies on the graph with
regards to the mean now the Z score is
given by subtracting the data point from
the mean and dividing it by standard
deviation
this can also be written as x - mu /
Sigma now any normal distribution can be
standardized by converting its values
into Z scores the Z score will tell you
how many standard deviation from the
mean each values
lie while data points are referred to as
X in a normal distribution they are
called Zed or Z scores in the Zed
distribution a z score is a standard
score that will tell you how many
standard deviations away from the mean
an individual point when lie a positive
Z score will mean that your x value is
greater than the mean and a negative Z
score will mean that your x value is
less than the mean a z Square of0 will
mean that your x value is equal to the
mean and again to standardize a value
from a normal distribution all we have
to do is convert it to a z score by
subtracting the mean from our individual
value and dividing it by the standard
deviation now let's see how we can find
the Z score from data points with the
help of a solved example
let's do a case
study in this case study we'll be taking
the summary of daily travel time of a
person who's commuting to and from work
all these values are in minutes and
using these values we have to calculate
the mean the standard deviation and the
Z score these values are as shown as we
can see there are 13 values in total
let's start by finding the mean the mean
is the average and it can be gotten by
adding all of these values and and
dividing it by the total number of
values this gives us a value of
38.6 the mean tells us the average of
all our data points which means on an
average he travels for 38.6 minutes to
reach work next let's subtract the
individual values from our mean and
calculate the variance and standard
deviation the values on the left give us
the values that we get after subtracting
it from the mean and the variance can be
calculated by squaring all of these
values adding up all of the squared
values and dividing it by the total
number of values at the end of the day
we get a variance of
140 to calculate the standard deviation
all we have to do is take a square root
of the variance which gives us a value
of
11.8 now the mean signifies the average
of our values and we already know this
it gives us the average time which is
taken to travel but the standard
deviation will tell us the average
average value of how much our data
points differ from the mean it tells us
the deviation within our own data and it
tells
us how far away on an average a point is
from the mean now the value that we get
is 11.8 which means that on an average a
single data point is around 11.8 data
points away from the
mean now let's calculate the Z
score the Z score is given by
subtracting individual data points from
the mean and dividing it by the standard
deviation we know that we have a
standard deviation of 11.8 and a mean of
38.6 using these values we can calculate
the Z scores for individual X values now
we know that a negative Z score means
that our x value is lower than our mean
but what does the number 1.06 mean this
means that the Z score for 26 is
1.06 standard
deviations away from the mean the
negative symbol here means that our x
value is less than the mean and by how
less 1.06 * the standard deviation now
we know that the negative value of a z
score means that our x value is less
than our mean but what does the number
1.06 mean this means that the Z score is
1 .06 * the standard deviation less than
the mean the same thing can be said for
the Z score of 33 it is 0.47 * the
standard deviation
less than the
mean the Z square of 65 is 2.23 * the
standard deviation more than the mean
that means it has to be added to the
mean the reason that we know it's more
than the mean is because this has a
positive value so this mean means that
using Z scores we can know where our
data points fall relative to other
points on the graph the Z score will
tell us how far away from the mean a
point is in steps of our standard
deviation Basics and
terminology the first one is
outcome whenever we do an experiment
like flipping a coin or rolling a dice
we get an outcome for example if we flip
a coin we get an outcome of heads or
tails
and if we roll a die we get an outcome
of 1 2 3 4 5 or
six random experiment a random
experiment is any well- defined
procedure that produces an observable
outcome that could not be perfectly
predicted in advance a random experiment
must be well defined to eliminate any
vagueness or
surprise it must produce a definite
observable outcome so that you know what
happened after the random experiment is
run random events
consider a simple
example let us say that we toss a coin
up in the air what can happen when it
gets back it either gives a head or a
tail these two are known as outcome and
the occurrence of an outcome is an event
does the event is the outcome of some
phenomenon the last one is sample
space a sample space is a collection or
a set of possible outcomes of a random
experiment the sample space is represent
using the symbol
S the subset of all possible outcomes of
an experiment is called events and a
sample space may contain a number of
outcomes that depends on the experiment
if it contains a finite number of
outcomes then it is known as a discrete
or finite sample
spaces now let's discuss what is random
variable a random variable is a
numerical description of the outcome of
a statistical experiment a random
variable that may assume only a finite
number of values is set to be discrete
one that may assume any value in some
interval on the real number line is set
to be
continuous let's see an example let X be
a random variable defined as a sum of
numbers when two dices are
rolled X can assume the values 2 3 4 5 6
7 8 9 10 11 and 12 notice there's no one
here because the sum of the two dice can
never be one now that we know the basics
let's move on to binomial
distribution the binomial distribution
is used when there are exactly two
mutually exclusive outcomes of a trial
these outcomes are appropriately labeled
success and failure the B distribution
is used to obtain the probability of
observing X successes in N number of
trials with the probability of success
on a single trial denoted by P the Bal
distribution assumes that P is fixed for
all the
trials here's a real life example of a
bomal distribution suppose you purchase
a lottery ticket then either you are
going to win the lottery or not in other
words the outcome will be either success
or failure that can be proved through
bomal
distribution there are four important
conditions that needs to be fulfilled
for an experiment to be a binomial
experiment the first one is there should
be a fixed number of end trials carried
out the outcome of a given tral is only
two that is either success or a failure
the probability of success remains
constant from trial to trial it does not
changes from one trial to another and
the trials are independent the outcome
of a trial is not affected by the
outcome of any other
trial to calculate the binomial
coefficient we use the formula which is
NCR into p ^ R into 1us P to the^ n
minus r where R is the number of success
in N number of Trials and P is the
probability of sues 1 - P denotes the
probability of a failure now let's use
this formula to solve an
example suppose a die is toss three
times what is the probability of No 5
turning up 15 and 35s turning up to
calculate the No 5 turning up here R is
equal to 0 and N is equal to 3
substituting the value in the formula we
have 33 0 into 1X 6 ^ 0 into 5x 6 ^ 3
where 1X 6 is the probability of success
and 5x 6 is the probability of failure
calculating this equation we'll get the
value to be 0.578
7 in a similar manner to calculate the
probability of 15 turning up we'll
replace r with 1 and N will be 3 so P X1
will be equal to 3 C1 into 1X 6 ^ 1 into
5x 6 ^ 2 which will come out to be
0.347 and for 35 turning up we
substitute Ral 3 and the formula will
remain the same and we'll get the value
to be
0.46 now that we are done with the
concepts of bomal probability
distribution here's a problem for you to
solve post your answers in the comment
section and let us
know a PO distribution is a probability
distribution used in statistics to show
how many times an event is likely to
happen over a given period of time to
put it another way it's a
distribution poison distribution are
frequently used to comprehend
independent event at a constant rate
over a given interval of
time the poison distribution was
developed by French mathematician Simon
Denis poison in
1837 a poison distribution is used in
cases where the chances of any
individual event being a success is very
small the number of defective pencils
per box of a 6,000 pencil the number of
plane crashs in India in one year or the
number of printing mistakes in each page
of a book all of these example can have
use of poison
distribution the poison distribution can
be used to calculate How likely it is
that something will happen X number of
times a random variable X has a poison
distribution with parameter Lambda and
the formula for that is e ^ minus Lambda
into Lambda ^ x / X factorial where X
can be the number of times the event is
happening the value of e is taken as
also have an application for poon
distribution let's discuss one example
to see how we can calculate the poon
distribution suppose on an average a
cancer kills five people each year in
India what is the probability that one
person is killed this year we'll assume
all these events are independent random
events so by the
formula we have x equal to 1 because we
have to calculate the probability of one
person that is killed this year so p x =
1 will be equal to e^ - 5 into 5 ^ 1 / 1
factorial which will come out to be 0
033 which will be near to
3.3% so the probability that only 1
person is skill this year due to cancer
is 3.3% if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learn postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box
below hello everyone welcome to another
session by simply learn today we are
going to discuss the base theorem an
important subtopic that comes under
probability Theory we'll start this
video by talking about probability and
condition probability after that we'll
move on to the base theorem and
understand its formula and a real life
example where the base teror can be used
so let's get started what is
probability probability is the branch of
mathematics concerning numerical
descriptions of How likely an event is
to occur or How likely it is that a
proposition is true the probability of
an event is a number between zero and
one
well roughly speaking zero indicates the
impossibility of the event and one
indicates
certainty the higher the probability of
an event the more likely is that the
event will occur let's look at an
example a simple example is the tossing
of a fair unbiased coin since the coin
is fair the outcome that is heads and
the tails are both equally probable the
probability of heads equals the
probability of the Tails and since no
other outcomes are possible the
probability of either heads or tails can
be said to be 1X two which is also 50%
the probability of an event can be
calculated by number of ways it can
happen divided by the total number of
outcomes now that we know about the
probability let's see if you can answer
this question what is the probability of
drawing a Jack and a queen consecutively
from a deck of 52 cards without
replacement here are your
options post your answers in the comment
section and let us know now let's move
on to conditional probability
let A and B be the two events associated
with a random experiment then the
probability of A's occurrence under the
condition that B has already occurred
and probability of B is not equal to
zero is called the condition probability
it is denoted by P
A/B thus we can say that p a/ b is equal
to p a intersection B / P of B where P
A/B is the probability of occurrence of
a given that B has already occurred and
P B is the probability of occurrence of
B to know more about conditional
probability you can check our previous
video which is specifically
unconditional probability now let's move
on to base
theorem the base theorem is a
mathematical formula for calculating
condition probability in probability and
statistics in other words it is used to
figure out how likely an event is
associated on its proximity to
another base law or base rule are the
other names of this theorem the formula
for the base theorem can be written in a
variety of ways the most common version
is p a/ b is equal to P of b/ a into P
of a / P of B where P A/B is the
conditional probability of event a
occurring given that b is true and P A
and B of B are the probabilities of A
and B occurring independently of one
another let's solve a problem using the
base theorem to understand it better
there is a cricket match tomorrow and in
recent years it has rained only 5 days
each year unfortunately the meterologist
has predicted the rain for
tomorrow now when it rains the
meteorologist correctly forecast rain
90% of the time and when it doesn't rain
he incorrectly forecast rain 10% of the
time let's calculate what is the
probability that it will rain on the
match day so the two sample spaces here
are the events that it rains and it does
does not rain additionally a third event
is also there that meterologist predicts
the rain so the notation for these
events appear below event A1 is equal to
it Reigns on the match Day event A2 that
it does not rain on the match day and
event B is the meterologist predicting
the rain now in terms of probability we
know the following probability of A1 is
5X 365 that it rains 5 days in a year
which will come out to be
0.0136 P A2 is 360 by 365 that is no
days for 360 days in an year which will
come out to be
986 B b/ A1 is
.9 this signifies when it rains the
meteorologist predicts the rain 90% of
the time in a similar manner PB by A2 is
0.1 that it does not rain the
meterologist predicts the rain 10% of
the
time combining all this we can calculate
P A1 /b that is the probability It Will
Rain on the given match day given a
forecast of Rain by meterologist the
answer can be determined using the base
theorem as shown below so here's the
formula of the base theorem and putting
all the values that we have calculated
in the previous Slide the probability
that it will rain on the match day given
a forecast of the rain by meterologist
will come out to be
0.111 which will be equal to
11.11% so there's an 11% chance that it
will rain on the match day given that
the meteorologist has predicted the rain
I hope this example is clear to you it's
a weekend and John decided to watch the
latest movie recommended by Netflix at
his friend's place before heading out he
asked Siri about the weather and
realized it would rain so he decided to
take his Tesla for the long journey and
switch to autopilot on the highway after
coming home from the eventful day he
started wondering how technology has
made his life easy he did some research
on the internet and found out that
Netflix Siri and Tesla are all using AI
so what is ai ai or artificial
intelligence is nothing but making
computers based machines think and act
like humans artificial intelligence is
not a new term John McCarthy a computer
scientist coined the term artificial
intelligence back in
1956 but it took time to evolve as it
demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistance broadly classifying
there are three types of AI artificial
narrow intelligence also called weak AI
is the stage where machines can perform
a specific task Netflix Siri chatbots
facial recommendation systems are all
examples of artificial narrow
intelligence next up we have artificial
general intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human can we are halfway
into successfully implementing this
space IBM's Watson supercomputer and
gpt3 fall under this category and lastly
artificial
superintelligence it is the stage where
machines surpass human intelligence you
might have seen this in movies and
imagined how the world would be if
machines occupy it fascinated by this
John did more research and found out
that machine learning deep learning and
natural language processing are all
connected with artificial
intelligence machine learning a subset
of AI is the process of automating an
enhancing how computers learn from their
experiences without human health machine
learning can be used in email spam
detection medical diagnosis Etc deep
learning can be considered a subset of
machine learning it is a field that is
based on learning and improving on its
own by examining computer algorithms
while machine learning uses simpler
Concepts deep learning works with
artificial neural networks which are
designed to imitate the human brain this
technology can be applied in face
recognition speech recognition and many
more
applications natural language processing
popularly known as NLP can be defined as
the ability of machines to learn human
language and translate it chatbots fall
under this category artificial
intelligence is advancing in every
crucial field like healthcare education
robotics banking e-commerce and the list
goes on like in healthcare AI is used to
identify diseases helping healthcare
service providers and their patients
make better treatment in lifestyle
decisions coming to the education sector
AI is helping teachers automate grading
organizing and facilitating parent
Guardian
conversations in robotics AI powered
robots employ realtime updates to detect
obstructions in their path and
instantaneously design their routes
artificial intelligence provides
Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance with this
growing gr demand for AI more and more
Industries are looking for AI Engineers
who can help them develop intelligent
systems and offer them lucrative
salaries going north of
$120,000 the future of AI looks
promising with the AI Market expected to
reach $190 billion by
2025 we know humans learn from their
past experiences and machines follow
instructions given by
humans but what if humans can train the
machines to learn from their past data
and do what humans can do and much
faster well that's called machine
learning but it's a lot more than just
learning it's also about understanding
and reasoning so today we will learn
about the basics of machine learning so
that's Paul he loves listening to new
songs he either likes them or dislikes
them Paul decides this on the basis of
the song's Tempo JRE intensity and the
gender of voice for Simplicity let's
just use St Tempo and intensity for now
so here Tempo is on the x-axis ranging
from relaxed to fast whereas intensity
is on the y- AIS ranging from light to
Soaring we see that Paul likes the song
with fast tempo and soaring intensity
while he dislikes the song with relaxed
Tempo and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's name it as song a song
a has fast Tempo and a soaring intensity
so it lies somewhere here looking at the
data can you guess whether Paul will
like the song or not correct so Paul
likes this song by looking at Paul's
past choices we were able to classify
the unknown song very easily right let's
say now Paul listens to a new song Let's
label it as song b so song b lies
somewhere here with medium Tempo and
medium intensity neither relaxed nor
fast neither light nor soaring now can
you guess whether Paul likes it or not
not able to guess whether Paul will like
it or dislike it are the choices unclear
correct we could easily classify song A
but when the choice became complicated
as in the case of song b yes and that's
where machine learning comes in let's
see how in the same example for song b
if we draw a circle around the song b we
see that there are four votes for like
whereas one vote for dislike if we go
for the majority votes we can say that
Paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called K
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become complicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
proect for it more the data better the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you 1 million
coins of three different currencies say
one rupee 1 euro and 1 dirham each coin
has different weights for example a coin
of 1 rupe weighs 3 g 1 euro weighs 7 G
and 1 Dam weighs 4 G your model will
predict the currency of the coin here
your weight becomes the feature of coins
while currency becomes the label when
you feed this data to the machine
learning model it learns which feature
is associated with which label for
example it will learn that if a coin is
of 3 G it will be a 1 rupee coin let's
give a new coin to the machine on the
basis of the weight of the new coin your
model will predict the currency hence
supervised learning uses labeled data to
train the model here the machine knew
the Fe features of the object and also
the labels associated with those
features on this note let's move to
unsupervised learning and see the
difference suppose you have Cricket data
set of various players with their
respective scores and wickets taken when
we feed this data set to the machine the
machine identifies the pattern of player
performance so it plots this data with
the respective wickets on the x-axis
while runs on the Y AIS by looking at
the data you'll clearly see that there
are two clusters the one cluster are the
players who scored High runs and took
less wickets while the other cluster is
of the players who scored less runs but
took many wickets so here we interpret
these two clusters as batsmen and
Bowlers the important point to note here
is that there were no labels of batsmen
and Bowlers hence the learning with
unlabeled data is unsupervised learning
so we saw supervised learning where the
data was labeled and the unsupervise
learning where the data was unlabeled
and then there is reinforcement learning
which is reward-based learning or we can
say that it works on the principle of
feedback here let's say you provide the
system with an image of a dog and ask it
to identify it the system identifies it
as a cat so you give a negative feedback
to the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it'll be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to a machine
learning model which then gives the
output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario one Facebook recognizes your
friend in a picture from an album of
tagged
photographs scenario 2 Netflix
recommends new movies based on someone's
Past movie
choices scenario three analyzing Bank
data for suspicious transactions and
flagging the fraud transactions think
wisely and comment below your answers
moving on don't you sometimes wonder how
is machine learning possible in today's
era well that's because today we have
humongous data available everybody's
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis also the memory handling
capabilities of computers have largely
increased which helps them to process
such huge amount of data at hand without
any delay and yes computers now have
great computational Powers so there are
a lot of applications of machine
learning out there to name a few machine
learning is used in healthcare where
Diagnostics are predicted for doctor's
review the sentiment analysis that the
tech Giants are doing on social media is
another interesting application of
machine learning fraud detection in the
finance sector and also to predict
customer turn in the e-commerce sector
while booking a gap you must have
encountered search pricing often where
it says the fair of your trip has been
updated continue booking yes please I'm
getting late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad feather
Rush R Etc so they use a searge pricing
model to ensure that those who need a
cab can get one also it uses predictive
modeling to predict where the demand
will be high with a goal that drivers
can take care of the demand and search
pricing can be minimized great hey Siri
can you remind me to book a cab at 600
p.m. today okay I'll remind you thanks
no problem if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
industry experts then search no more
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below let's dive in a
little deeper and see how machine
Learning Works let's say you provide a
system with the input data that carries
the photos of various kinds of fruits
now you want the system to figure out
what are the different fruits and group
them accordingly so what the system does
it analyzes the input data then it tries
to find patterns patterns like shapes
size and
color based on these patterns the system
will try to predict the different types
of fruit and segregate them finally it
keeps track of all such decisions it
took in the process to make sure it's
learning the next time you ask the same
system to predict and segregate the
different types of fruits it won't have
to go through the entire process again
that's how machine learning
works now let's look into the types of
machine learning machine learning is
primarily of three types first one is
supervised machine learning as the name
suggests you have to supervise your
machine learning while you train it to
work on its own it requires labeled
training data next up is unsupervised
learning wherein there will be training
data but it won't be
labeled finally there's reinforcement
learning wherein the system learns on
its own let's let's talk about all these
types in detail let's try to understand
how supervised Learning Works look at
the pictures very very carefully the
monitor depicts the model or the system
that we are going to train this is how
the training is done we provide a data
set that contains pictures of a kind of
a fruit say an apple then we provide
another data set which lets the model
know that these pictures wear that of a
fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as well
that's how supervis Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used into
filtering spam mails from your email
account as well yes surprised aren't you
so let's move on to un supervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you
honestly speaking this is my favorite
type of machine learning out of all the
three and this type has been widely
shown in most of the Sci-Fi movies
lately let's find out how it works
imagine a newborn baby you put a burning
candle in front of the baby the baby
does not know that if it touches the
flame its fingers might get burned so it
does that anyway and gets hurt the next
time you put that candle in front of the
baby it will remember what happened the
last time and would not repeat what it
did that's exactly how reinforcement
learning works we provide the machine
with a data set wherein we ask it to
identify a particular kind of a fruit in
this case an Apple so what it does as a
response it tells us that it's a mango
but as we all know it's a completely
wrong answer so as a feedback we tell
the system that it's wrong it's not a
mango it's an apple what it does it
learns from the feedback and keeps that
in mind when the next time when we ask a
same question it gives us the right
answer it is able to tell us that it's
actually an apple that is a reinforced
response so that's how reinforcement
learning works it learns from his
mistakes and experiences this model is
used in games like Prince of Persia or
Assassin Creed or FIFA where in the
level of difficulty increases as you get
better with the games just to make it
more clear for you let's look at a
comparism between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this is actually an
apple that is called label data so the
system learns from the label data and
makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case case of supervised learning that is
once you get the output the system tends
to remember that and uses it for the
next operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised
learning now let's talk about a question
that everyone needs to answer before
building a machine learning model
what kind of a machine learning solution
should we
use yes you should be very careful with
selecting the right kind of solution for
your model because if you don't you
might end up losing a lot of time energy
and processing cost I won't be naming
the actual Solutions because you guys
aren't familiar with them yet so we will
be looking at it based on supervised
unsupervised and reinforcement learning
so let's look into the factors that
might help us select the right kind of
machine learning solution first factor
is the problem statement describes the
kind of model you will be building or as
the name suggests it tells you what the
problem is for example let's say the
problem is to predict the future stock
market prices so for anyone who is new
to machine learning would have trouble
figuring out the right solution but with
time and practice you will understand
that for a problem statement like this
solution based on supervised learning
would work the best for obvious reasons
then comes the size quality
and nature of the data if the data is
cluttered you go for unsupervised if the
data is very large and categorical we
normally go for supervised learning
Solutions finally we choose the solution
based on their
complexity as for the problem statement
wherein we predict the stock market
prices it can also be solved by using
reinforcement learning but that would be
very very difficult and time consuming
unlike supervised
learning algorithms are not types of
machine learning
in the most simplest language they are
methods of solving a particular problem
so the first kind of method is
classification which falls under
supervised learning classification is
used when the output you are looking for
is a yes or a no or in the form a or b
or true or false like if a shopkeeper
wants to predict if a particular
customer will come back to his shop or
not he will use a classification
algorithm the algorithms that fall under
classification are decision tree knife
base random Forest logistic regression
and
KNN the next kind is regression this
kind of a method is used when the
predicted data is numerical in nature
like if the shopkeeper wants to predict
the price of a product based on its
demand it would go for
regression the last method is
clustering clustering is a kind of
unsupervised learning again it is used
when the data needs to be organized
most of the recommendation system used
by flip cart Amazon Etc make use of
clustering another major application of
it is in search engines the search
engines study your old search history to
figure out your preferences and provide
you the best search
results one of the algorithms that fall
under clustering is K
means now that we know the various
algorithms let's look into four key
algorithms that are used widely we will
understand them with very simple
examples the four algorithms that we
will try to understand are K nearest
neighbor linear regression decision tree
and knife Pace let's start with our
first machine learning solution K
nearest neighbor K nearest neighbor is
again a kind of a classification
algorithm as you can see on the screen
the similar data points form
clusters the blue
one the red one
and the green one there are three
different clusters now if we get a new
and unknown data point it is classified
based on the cluster closest to it or
the most similar to it k in KNN is the
number of nearest neighboring data
points we wish to compare the unknown
data with let's make it clear with an
example let's say we have three clusters
in a cost to durability graph first
cluster is of
footballs this second one is of tennis
balls and the third one is of
basketballs from the graph we can say
that the cost of footballs is high and
the durability is less the cost of
tennis balls is very less but the
durability is high and the cost of
basketballs is as high as the durability
now let's say we have an unknown data
point we have a black spot which can be
one kind of the balls but we don't know
what kind of it is so what we'll do
we'll try to classify this using KN andn
so if we take K is equal to 5 we draw a
circle keeping the unknown data point is
the center and we make sure that we have
five balls inside that Circle in this
case we have a football a basketball and
three tennis balls now since we have the
highest number of tennis balls inside
the circle the classified ball would be
a tennis
ball so that's how K nearest neighbor
classification is done linear regression
is again a type of supervised learning
algorithm this algorithm is used to
establish linear relationship between
variables one of which would be
dependent and the other one would be
independent like if we want to predict
the weight of a person based on his
height weight would be the dependent
variable and height would be
independent let's have a look at it
through an
example let's say we have a graph here
showing a relation relationship between
height and weight of a person let's put
the y- axis as
Ag and the x-axis as
weight so the green dots are the various
data points these green dots are the
data points and D is the mean squared
error that is the perpendicular
distances from the line to the data
points are the error
values this error tells us how much the
predicted values vary from the original
value Let's ignore this blue line for a
while so let's say if this is our
regression line you can see the distance
from all the data points from this line
is very
high so if we take this line as a
regression line the error in the
prediction will be too
high so in this case the model will not
be able to give us a good prediction
let's say we draw another regression
line here like this even in this case
you can see that the perpendicular
distance of the data points from the
line is very high so the error value
will still come as high as the last one
so this model will also not be able to
give us a good
prediction so what to
do so finally we draw a line which is
this blue line so here we can see that
the distance of the data points from the
line is very less relatives to the other
two lines we
drew so the value of D for this line
will be very less so in this case if we
take any value on the x- axis the
corresponding value on the y- AIS will
be our
prediction and given the fact that the D
is very low our prediction should be
good
also this is how regression works we
draw a line a regression line that is in
such a way that the value of D is the
least eventually giving us good
predictions this algorithm that is
decision tree is a kind of an algorithm
you can very strongly relate to it uses
a kind of a branching method to realize
the problem and make decisions based on
the conditions let's take this graph as
an example imagine yourself sitting at
home getting bored you feel like going
for a swim what you do is you check if
it's sunny outside so that's your first
condition if the answer to that
condition is yes you go for a swim if
it's not Sunny then the next question
you would ask yourself is if it's
raining outside so that's condition
number two if it's actually raining you
cancel the plan and stay indoors if it's
not raining then you would probably go
outside and have a walk so that's the
final node that's how decision tree
algorithm works you probably use this
every day it realizes a problem and then
takes the decisions based on the answer
aners to every
conditions nbis algorithm is mostly used
in cases where a prediction needs to be
done on a very large data set it makes
use of conditional
probability conditional probability is
the probability of an event say a
happening given that another event B has
already happened this algorithm is most
commonly used in filtering spam mails in
your email account let's say you receive
a mail the model goes through your old
spam mail
records then it uses space theorem to
predict if the present male is a spam
male or not so PC of a is the
probability of event C occurring when a
has already occurred B A of C is the
probability of event a occurring when C
has already occurred and P C is the
probability of event C occurring and Pa
is the probability of event a occurring
let's try to understand KN base with a
better example
night base can be used to determine on
which days to play cricket based on the
probabilities of a day being rainy windy
or sunny the model tells us if a match
is possible if we consider all the
weather conditions to be event a for
us and the probability of a match being
possible event
C so the model applies the probabilities
of event A and C into the base theorem
and predicts if a game of cricket is
possible on a particular day or not in
this case if the probability of C of a
is more than 0.5 we can be able to play
a game of cricket if it's less than 0.5
we won't be able to do that that's how
Nas algorithm Works we're going to cover
reinforcement learning today and what's
in it for you we'll start with why
reinforcement learning we'll look at
what is reinforcement learning we'll see
what the different kinds of learning
strategies are that are being used today
in computer models under supervised
versus un supervised versus
reinforcement we'll cover important
terms specific to reinforcement learning
we'll talk about markov's decision
process and we'll take a look at a
reinforcement learning example well
we'll teach a tic-tac-toe how to play
why reinforcement learning training a
machine learning model requires a lot of
data which might not always be available
to us further the data provided might
not be reliable learning from a small
subset of actions will not help expand
the vast realm of solutions that may
work for a particular problem and you
can see here we have the robot learning
to walk um very complicated setup when
you're learning how to walk and you'll
start asking questions like if I'm
taking one step forward and left what
happens if I pick up a 50 pound object
how does that change how a robot would
walk these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expand the Val
realm of solutions that may work for a
particular
problem and we'll see here it learned
how to walk this is going to slow the
growth that technology is capable of
machines need to learn to perform
actions by themselves and not just learn
off
humans and you see the objective climb a
mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most of the models the
non-reinforcement models in computer um
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated to see
how it goes is what we're talking about
with reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains a model to return
an Optimum solution for a problem by
taking a sequence of decisions by itself
consider a robot learning to go from one
place to another the robot is given a
scenario and must arrive at a solution
by itself the robot can take different
paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important is
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different
systems or different model we have
supervised versus unsupervised versus
reinforcement learning and with the
supervised learning that is probably the
most controlled environment uh we have a
lot of different supervised learning
models whether it's linear regression
neural networks um there's all kinds of
things in between decision trees the
data provided is labeled data with
output values specified and this is
important because when we talk about
supervised learning you already know the
answer for all this information you
already know the picture has a
motorcycle in it so you're supervised
learning you already know that um the
outcome for tomorrow for you know going
back a week you're looking at stock you
can already have like the graph of what
the next day looks like so you have an
answer for
it and you have labeled data which is
used you have an external supervision
and solves Problems by mapping labeled
input to know one output so very
controlled unsupervised learning and
unsupervised learning is really
interesting because it's now taking part
in many other models they start with an
you can actually insert an unsupervised
learning model um in almost either
supervised or reinforcement learning as
part of the system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering
output uh so you can look at this and
you can think um some of these things go
with each other they belong together so
it's looking for what connects in
different ways and there's a lot of
different algorithms that look at this
um when you start getting into those
there some really cool images that come
up of what unsupervised learning is how
it can pick out say uh the area of a
donut one model will see the area of the
donut and the other one will divide it
into three sections based on this
location versus what's next to it so
there's a lot of stuff that goes in with
unsupervised learning and then we're
looking at reinforcement learning
probably the biggest industry in today's
market uh in machine learning or growing
Market it's very in it's very infant
stage uh as far as how it works and what
it's going to be capable of the machine
learns from its environment using
rewards and errors used to solve
reward-based
problems no predefined data is used no
supervision follows Trail and error
problem solving approach uh so again we
have a random at first you start with a
random I try this it works and this is
my reward doesn't work very well maybe
or maybe doesn't even get you where
you're trying to get it to do and you
get your reward back and then it looks
at that and says well let's try
something else and it starts to play
with these different things finding the
best route so let's take a look at
important terms in today's reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know we have
the agent uh agent is the model that is
being trained via reinforcement learning
so this is your actual U entity that has
however you're doing it whether using a
neural network or a q table or whatever
combination thereof this is the actual
agent that you're using this is the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who's trying to
get get a chest full of gems or whatever
and that's the output and then you have
your action this is all possible steps
that can be taken by the model and it
picks one action and you can see here
it's picked three different uh routes to
get to the chest of diamonds and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here uh then enironment is a
whole game board so if you're playing
one of those Mobius games you might have
the whole game board going on uh but
then you have your current position
where are you on that game board what's
around that what's around you um if you
were talking about a robot the
environment might be moving around the
yard where it is in the yard and what it
can see what input it has in that
location that would be the current
position condition returned by the model
and then the reward uh to help the model
will move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what would what is your action
that you're you're going to take what's
the policy you're using to have an
output from your agent
one of the reasons they separate uh
policy as its own entity is that you
usually have a prediction um of a
different options and then the policy
well how am I going to pick the best
based on those predictions I'm going to
guess at different options and we'll
actually weigh those options in and find
the best option we think will work uh so
it's a little tricky but the policy
thing is actually pretty cool how it
works let's go ahe and take a look at a
reinforcement learn learning example and
just in looking at this we're going to
take a look uh consider what a dog um
that we want to train uh so the dog
would be like the agent so you have your
your puppy or whatever uh and then your
environment is going to be the whole
house or whatever it is where you're
training them and then you have an
action we want to teach the dog to
fetch so action equals
fetching uh and then we have a little
biscuit so we can get the dog to perform
various actions by offering incentives
such as the dog biscuit as a
reward the dog will follow a policy to
maximize this reward and hence will
follow every command and might even
learn new actions like begging by itself
uh so you have B you know so we start
off with fetching it goes oh I get a
biscuit for that it tries something else
and you get a handshake or begging or
something like that and it goes oh this
is also reward-based and so it kind of
explores things to find out what will
bring it as biscuit and that's very much
like how a reinforced model goes is it
uh looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also will want to run
around and play and explore its
environment uh this quality of model is
called exploration so there's a little
Randomness going on in
Exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh markov's
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state or agent our environment
interaction and so even though the
environment kind of contains everything
um that you you really when you're
actually write the program your
environment's going to put out a reward
in state that goes into the agent uh the
agent then looks at this uh state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or I didn't get rewarded and
then looks at the state then it comes
back and if you remember from policy the
policy comes in um and then we have a
reward the policy is that part that's
connected at the bottom and so it looks
at that policy and it says hey what's a
good action that will probably be
similar to what I did or um uh sometimes
they're completely random but what's a
good action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straightforward as far as once
you start seeing how it works uh you can
see your environment send it says hey
this is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent so in the diagram
shown we need to find the shortest path
between Noe A and D each path has a
reward associated with it and the path
with a maximum reward is what we want to
choose the nodes AB b c d denote the
nodes to travel from node uh A to B is
an action reward is the cost of each
path and policy is each path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or you can go right to D and if you
explored all three of these uh you would
find that a going to D was a zero reward
um a going to C and D would generate a
different reward or you could go AC b d
there's a lot of options here um and so
when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good at um finding an answer they end up
trying almost all the different
directions you see
and so they take up a lot of work uh or
a lot of processing time for
reinforcement learning they're right now
in their infant stage and they're really
good at solving simple problems and
we'll take a look at one of those in
just a minute in a tic tac toe game uh
but you can see here uh once it's gone
through these and it's explored it's
going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning
demo uh in this demo we're going to use
reinforcement learning to make a tic TCT
toe game you will be playing this game
Against the Machine learning
model and we'll go ahead we're doing it
in Python so let's go ahead and go
through I always uh not always actually
have a lot of python tools let's go
through um Anaconda which will open up a
Jupiter notebook seems like a lot of
steps but it's worth it to keep all my
stuff separate and it's also has a nice
display when you're in the Jupiter
notebook for doing
python so here's our Anaconda Navigator
I open up the notebook which is going to
take me to a web page and I've gone in
here and created a new uh python folder
in this case I've already done it and
enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import nump as NP we'll go ahead
and import pickle numpy of course is our
number array and then uh pickle is just
a nice way sometimes for storing uh
different information uh different
states that we're going to go through on
here uh and so we're going to create a
class called State we're going to start
with
that and there's a lot of lines of code
to this uh class that we're going to put
in here don't let that scare you too
much there's not as much here um it
looks like there's going to be a lot
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it uh
we have our board um it's a tic teac toe
board so we're only dealing with nine
spots on the board uh we have player one
player
two uh is in we're going to create a
board hash uh we'll look at that in just
a minute we're just going to stir some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we're going to get the
information from the board on there
which is uh columns and rows we want to
know when a winner occurs uh so if you
get three in a row that's what this
whole section here is for uh let me go
ahead and scroll up a little bit and you
can get a copy of this code if you send
a note over to Simply learn we'll send
you over um this particular file and you
can play with it yourself and see how
it's put together I don't want to spend
a huge amount of time on this uh because
this is just some real General python
coding
uh but you can see here we're just going
through all the rows and you add them
together and if it equals three three in
a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie so you got to add up all the
numbers on there anyway just in case
they're all filled up and next we also
need to know available positions um
these are ones that don't no one's ever
used before this way when you try
something or the computer tries
something uh it's not going to give it
an illegal move that's what the
available positions is doing uh then we
want to update our state and so you have
your position going in we're just
sending in the position that you just
chose and you'll see there's a little
user interface we put in there you p
pick the row and column in
there and again I mean this is a lot of
code uh so really it's kind of a thing
you'd want to go through and play with a
little bit and just read through it get
a copy of it uh great way to understand
how this works and here is a given
reward um so we're going to give a
reward result equals self- winner this
is one of the hearts of what's going on
here uh is we have a result self winner
so if there's a winner then we have a
result if the result equals one here's
our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a a 3X3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um is going to have to change a
little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually get a reward in this case you
get the reward at the end of the game
and I'm spending just a little bit of
time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first reward is going to be
weighed a little bit less than the last
reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case though
you can see right here that they give a
um a 0.1 and a 0. five
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning because it's going to try
out all these different things while
it's learning it's going to do it by
trial and error so you have to keep
resetting it and then of course there's
the play we want to go ahead and play uh
rounds equals 100 depends on what you
want to do on here um you can set this
different you obviously set that to
higher level but this is just going to
go through and you'll see in here uh
that we have player one and player two
this is this is the computer playing
itself uh one of the more powerful ways
to learn to play a game or even learn
something that isn't a game is to have
two of these models that are basically
trying to beat each other and so they
they keep finding explore new things
this one works for this one so this one
tries new things it beats this we've
seen this in um chess I think was a big
one where they had the two players in
chess with reinforcement learning uh was
one of the ways they train one of the
top um computer chess playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the the more
it try stuff um the more we're going to
record the hash we actually have a board
hash where they self get the hash setup
on here where it stores all the
information and then once you get to a
win one of them wins it gets the reward
uh then we go back and reset and try
again and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above it
gives it a reward for its things um or
sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
oops Play One More up the reward system
is really the heart of this how do you
reward the different uh setup and the
other one is when it's playing it's got
to take an
action and so what it chooses for an
action is also the heart of
reinforcement learning how do we choose
that action and those are really key to
right now where reinforcement learning
is um in today's uh technology is uh
figuring this out how do we reward it
and how do we guess the next best action
so we have our uh environment and you
can see the environment is we're going
to be or the state uh which is kind of
like what's going on we're going to
return the state depending on what
happens and we want to go ahead and
create our agent uh in this CL our
player so each one is let go and grab
that and so we look at a class player um
this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state uh that it can
look at and a reward uh so we want to
take a look at this we have a name uh
self State this is class player and when
you say class player we're not talking
about a human player we're talking about
um just a uh the computer players and
this is kind of interesting so remember
I told you depending on what you're
doing there's going to be a Decay gamma
um explor rate uh these are what I'm
talking about is how do we train it um
as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more than the
first step uh and here's our uh we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
uh so once you have your training going
on we have to do a little Randomness and
you can see right here is our NP random
um uniform so it's picking out a random
number take a random action this is
going to just pick which row and which
column it is um and so choosing the
action this one you can see we're just
doing random States um choice length of
positions action position and then it
skips in there and takes a look at the
board uh for p and positions you get
it's actually storing the different
boards each time you go through so it it
has a record of what it did so it can
properly weigh the values and this
simply just a pins a hash date what's
the last date pin it to the uh um to our
states on here here's our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times a reward this is where as it goes
through each step and this is really
important this is this is kind of the
heart of this of what I was talking
about earlier uh you have step
one and this might have a a reward of
two you have step two I should probably
should have done ABC
this has a step three uh step
four so on till you get to step in and
this might have a reward of
10 uh so reward of
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here before 10
was um let's say it's also 10 that just
makes the the math easy so we had 10 and
10 uh we had 10 this is 10 and 10 n
whatever it is but it's time it's 0. n
uh so instead of putting a full 10 here
we only do nine that's uh 0.9
times
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this ABC it would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our s uh set up on
here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points um and the first
step gets the least amount of points so
you're really training this on almost in
Reverse you're training you're training
it from the last place where you have
like it says okay this is now where I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh uh play on the mind when
you're trying to figure this stuff
out and of course we want to go ahead
and reset the board down here uh and
save the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human
player and the human player is going to
be a little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get an action that actually works and
then we're going to go ahead and append
to the hash state which uh we don't need
to worry about because it Returns the
action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagate and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to
do is set up uh P1 player one uh P2
player two and then we're going to send
our players to our state so now it has
P1 P2 and it's going to play and it's
going to play 50,000 rounds now we can
probably do a lot less than this and
it's not going to get the full results
in fact you know what uh let's go ahead
and just do five um just to play with it
because I want to show you something
here oops some in there I forgot to load
something there we go I must have forgot
to run this
run oops forgot a reference there for
the board rows and columns
3x3 um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to train
five times and the reason I did that is
we're going to uh come in and actually
play it and then I'm going to change
that and we can see how it differs on
there there we go and I didn't even make
it through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're we going to come in
here and we're going to do uh player one
is going to be the computer experience
rate zero load policy one human player
human and we're going to go ahead and
play this and I remember I only went
through it um uh just one round of
training in fact minimal training and so
it puts an X there and I'm going to go
ahead and do row zero column one you can
see this is very uh basic on here and so
I put in my zero and then I'm going to
go Z block it 0 0 and you can see right
here it let me win uh just like that I
was able to win
Z two and wo human winds so I only
trained it five times we're going to run
this again and this time uh instead of
five let's do 5,000 or 50,000 I think
that's what the guys in the back had and
this takes a while to train it
this is where reinforcement learning
really falls apart look how simple this
game is we're talking about uh 3x3 set
of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very careful what you apply it to it
sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one so every time
it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there's
a whole layer there which is really fun
to play
with and then it has an output well
think of all those processes I mean that
is just a huge amount of work it's going
to do uh let's go ahead and Skip ahead
here give it a moment it's going to take
a a minute or two to go ahead and
run now to train it uh we went ahead and
let it run and it took a while this this
took um I got a pretty powerful
processor and it took about 5 minutes
plus to run it and we'll go ahead and uh
run our player setup on here oops I
brought in the last whoops I brought in
the last round so give me just a moment
to redo the policy save there we go I
forgot to save the policy back in
there and then go ahead and run our
player again so we we've saved the
policy and then we want to go ahead and
load the policy for P1 as a computer and
we can see the computer's gone in the
bottom right corner I'm going to go
ahead and go uh one one which is the
center and it's gone right up the top
and if you have ever played tic tactoe
you know the computer has me uh but
we'll go ahead and play it out row zero
column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 2 no 01 there we go and column 0
that's where I wanted oh and it says I
okay you your action there we go boom uh
so you can see here we've got a didn't
catch the win on this it said Tai um
kind of funny that didn't catch the win
on
there but if we play this a bunch of
times you'll find it's going to win more
and more the more we train it the more
the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforcement
learning will be one of the more
powerful uh packages evolving over the
next decade or two in fact I would even
go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple Tic-Tac
toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being a to attach environment and
context and all those things together is
going to require reinforcement learning
to
do so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do um you know test it for
different uh uh values you can switch
from P1 computer uh where we loaded the
policy one to load the policy 2 and just
see how it varies there's all kinds of
things you can do on there supervised
learning uses label data to train
machine learning models label data means
that the output is already known to you
the model just needs to map the inputs
to the outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal below
you can see we have a trained model that
identifies the picture of a cat
unsupervised learning uses unlabeled
data to train machines unlabel data
means there is no fixed output variable
the model learns from the data discovers
patterns and features in the data and
Returns the output here is an example of
an unsupervised learning technique that
uses the images of vehicles to classify
if it's a bus or a truck so the model
learns by identifying the paths of a
vehicle such as the length and width of
the vehicle the front and rear end
covers roof hoods the types of Wheels
used Etc based on these features the
model classifies if the vehicle is a bus
or a
truck reinforcement learning trains a
machine to take suitable accents and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different parts
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algorithms that come
under these learning techniques some of
the commonly used supervised learning
algorithms are linear regression
logistic regression support Vector
machines K nearest neighbors decision
tree random forest and knife
base examples of unsupervised learning
algorithms are kin's clustering
hierarchical clustering DB scan
principal component analysis and others
choosing the right algorithm depends on
the type of problem you're trying to
solve some of the important
reinforcement learning algorithms are Q
learning Monte Carlo sarsa and deep Q
Network now let's look at the approach
in which these machine learning
techniques
work so supervised learning takes
labeled inputs and Maps it to known
outputs which means you already know the
target
variable unsupervised learning finds
patterns and understands the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning me
methods so supervised learning methods
need external supervision to train
machine learning models and hence the
name
supervised they need guidance and
additional information to return the
result unsupervised learning techniques
do not need any supervision to train
models they learn on their own and
predict the output similarly
reinforcement learning methods do not
need any supervision to train machine
learning models and with that let's
focus on the types of problems that can
be solved using these three types of
machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next
slide and unsupervised learning is used
for clustering and Association problems
while reinforcement learning is
reward-based so for every task or for
every step completed there will be a
rewarded by the agent and if the task is
not achieved correctly there will be
some penalty
used now let's look at a few
applications of supervised unsupervised
and reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock PR analysis or identifying
if a cancer cell is malignant or
benign now talking about the
applications of unsupervised learning we
have customer segmentation So based on
customer Behavior likes dislikes and
interests you can segment in cluster
similar customers into a group another
example where unsupervised learning
algorithms are used is customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks if you are
an aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from calch University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
often professionals want to know if
there is a relationship between two or
more variables for instance is there a
relationship between the grade on the
third French exam a student takes and
the grade on the final exam if yes then
how is it related and how strongly
regression can be used here to arrive at
a conclusion this is an example of by
variant data that is two variables
however statisticians are mostly
interested in multivariate data
regression analysis is used to predict
the value of one variable the dependent
variable on the basis of other variables
the independent variables in the
simplest form of regression linear
regression you work with one independent
variable the formula for simple linear
regression is shown on the screen in the
next screen we'll look at a few examples
of regression
analysis regression analysis is used in
several situations such as those
described on the screen in example one
using the data given on the screen you
have to analyze the relation between the
size of a house and its selling price
for a realer in example two you need to
predict the exam scores of students who
study for 7.2 hours with the help of the
data shown on the slide a couple more
examples are given on the screen in
example three based on the expected
number of customers and the previous day
data given you need to predict the
number of burgers that will be sold by a
KFC Outlet in example four you you have
to calculate the life expectancy for a
group of people with the average length
of schooling based on the data
given let's look at the two main types
of regression analysis simple linear
regression and multiple linear
regression both of these statistical
methods use a linear equation to model
the relationship between two or more
variables simple linear regression
considers one quantitative and
independent variable X to predict the
other quantitative but dependent
variable y
multiple linear regression considers
more than one quantitative and
qualitative variable to predict a
quantitative and dependent variable y
we'll look at the two types of analyses
in more detail in the slides that follow
in simple linear regression the
predictions of the explained variable y
when plotted as a function of the
explanatory variable X from a straight
line the best fitting line is called the
regression line the output of this model
is a function to predict the dependent
variable on the basis of the values of
the independent variable the dependent
variable is continuous and the
independent variable can be continuous
or discrete let's look at the different
kinds of linear and nonlinear analyses
list of linear techniques are simple
method of least squares coefficient of
multiple determination standard error of
the estimate dummy variable and
interaction similarly there are many
nonlinear techniques available such as
polinomial log Ari mic square root
reciprocal and exponential to understand
this model we'll first look at a few
assumptions the simple linear regression
model depicts the relationship between
one dependent and two or more
independent variables the assumptions
which justify the use of this model are
as follows linear and additive
relationship between the dependent and
independent variables multivariate
normality little or no collinearity in
the data little or no autocorrelation in
the data homocedasticity that is
variance of Errors same across all
values of X the equation for this model
is shown on the screen a more
descriptive graphical representation of
simple linear regression is given on the
screen beta not represents the slope a
slope with two variables implies that
one unit changes in X result in a two
unit change in y beta 1 represents the
estimated change in the average value of
y as a result of 1 unit change in X
Epsilon represents the estimated average
value of y when the value of x is
zero this demo will show the steps to do
simple linear regression in
r in this demo you'll learn how to do
simple linear
regression let's use X and Y vectors
that we have created in the previous
demo
we also ensured there exists a
relationship between X and Y visually by
plotting a
graph to build simple linear regression
model let's use the LM
function
to see how the linear model fits into X
and Y let's plot the linear line by the
ab line
function let's use the predict function
to test or predict the linear model we
can pass a known variable to predict the
unknown
variables
let's look at an example of a common use
for linear regression profit estimation
of a company if I was going to invest in
in a company I would like to know how
much money I could expect to make so
we'll take a look at a venture
capitalist firm and try to understand
which companies they should invest in so
we'll take the idea that we need to
decide the companies to invest in we
need to predict the profit the company
makes and we're going to do it based on
the company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing
uh we might have the location we might
have what kind of administration it's
going through based on all this
different information we would like to
calculate the profit now in actuality
there's usually about 23 to 27 different
markers that they look at if they're a
heavy duty investor we're only going to
take a look at one basic one we're going
to come in and for Simplicity let's
consider a single variable R&D and find
out which companies to invest in based
on that so when we take a R&D and we're
plotting The Profit based on the R&D
expenditure how much money they put into
the research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we can draw a line right
through the data when you look at that
you can see how much they invest in the
R&D as a good marker as to how much
profit they're going to have we can also
note that companies spending more on R&D
make good profit so let's invest in the
ones that spend a higher rate in their
R&D what's in it for you first we'll
have an introduction to machine learning
followed by Machine learning algorithms
these will be specific specific to
linear regression and where it fits into
the larger model then we'll take a look
at applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rainfall how much would be the crop
yield so here we have our crops we have
our rainfall and we want to know how
much we're going to get from our crops
this year so we're going to introduce
two variables independent and dependent
the independent variable is a variable
whose value does not change by the
effect of other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does control control
the crop so when we talk about the
independent variable controlling the
dependent variable let's Define
dependent variable as a variable whose
value change when there is any
manipulation in the values of the
independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numeric ercal and categorical values
let's take our data coming in and this
is kind of random data from any kind of
project we want to divide it up into
numerical and categorical so numerical
is numbers age salary height where
categorical would be a description the
color a dog's breed gender categorical
is limited to very specific items where
numerical is a range of information now
that you've seen the difference between
numerical and categorical data let's
take a look at some different machine
learning definitions when we look at a
different machine learning algorithms we
can divide them into three areas
supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're we just looking at
supervised because that's where linear
regression fits in in supervised data we
have our data already there and our
answers for a group and then we use that
to program our model and come up with an
answer the two most common uses for that
is through the regression and
classification now we're doing linear
regression so we're just going to focus
on the regression side and in the
regression we have SIMPLE linear
regression we have multiple linear
regression and we have polom linear
regression now on these three simple
linear regression is the examples we've
looked at so far where we have a lot of
data and we draw a straight line through
it multiple linear regression means we
have multiple variables remember where
we had the rainfall and the crops we
might add additional variables in there
like how much food do we give our crops
when do we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model model used to predict
the relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 - y1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some python script
that this math that you're going to see
here is already done automatically for
you you don't have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was 1 we
got Y = 2 when X was uh 2 y was 4 and so
on and so on if we go ahead and plot
this data on a graph we can see how it
forms a nice line through the middle you
can see where it's kind of grouped going
upwards to the right the next thing we
want to know is what the means is of
each of the data coming in the X and the
Y the means doesn't mean anything other
than the average so we add up all the
numbers and divide by the total so 1
plus 2 plus 3 + 4 + 5 over 5 = 3 and the
same for y we get four if we go ahead
and plot the means on the graph we'll
see we get 3 comma 4 which draws a nice
line down the middle a good estimate
here we're going to dig deeper into the
math behind the regression line now
remember before I said you don't have to
have all these formulas memorized or
fully understand them even though we're
going to go into a little more detail of
how it works and if you're not a math
whz and you don't know if you've never
seen SE the sigma character before which
looks a little bit like an e that's
opened up that just means summation
that's all that is so when you see the
sigma character it just means we're
adding everything in that row and for
computers this is great because as a
programmer you can easily iterate
through each of the XY points and create
all the information you need so in the
top half you can see where we broken
that down into pieces and as it goes
through the first two points it computes
the squared value of x the squared value
of y and x * Y and then it takes all of
X and adds them up all of Y adds them up
all of X2 adds them up and so on and so
on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x * Y where the sum equals
66 this all comes from our formula for
calculating a straight line where y
equals the slope * X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is in just a
minute and where that information comes
from it's called the square means error
but we'll go into that in detail in a
few minutes all you need to do is look
at the formula and see how we've gone
about Computing it line by line instead
of trying to have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and on the top
part if you read through the brackets
you have the number of data points times
the sum of x * Y which we computed one
line at a time there and that's just the
66 and take all that and you subtract it
from the sum of x times the sum of Y and
those have both been computed so you
have 15 * 20 and on the bottom we have
the number of lines times the sum of X2
easily computed as 86 for the sum minus
I'll take all that and subtract the sum
of X squ and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where M = 6 and C = 2.2 we're going to
take these values and we're going to go
ahead and plot them we're going to
predict them so y = 6 * or X = 1 + 2.2 =
2.8 so on and so on and here the Blue
Points represent the actual y values and
the brown points represent the predicted
y values based on the model we created
the distance between the actual and
predicted values is no one as residuals
or errors the best fit line should have
the least sum of squares of these errors
also known as e s if we put these into a
nice chart where you can see X and you
can see y what the actual values were
and you can see y predicted you can
easily see where we take Yus y predicted
and we get an answer what is the
difference between those two and if we
square that Yus y prediction squared we
can then sum those squared values that's
where we get the 64 plus 36 + 1 all the
way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least Square distance between
the data points and the regression line
now we only looked at the most commonly
used formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc what you want to take take away from
this is whatever formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic mult multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multiple
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attached to it as you can
see here we have M1 M2 M3 and we still
just have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y = M1 * X1 + M2 * X2
so on all the way to m to the nth x to
the nth and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing in Python if you remember
before we were looking at a company and
just based on its R&D trying to figure
out its profit we're going to start
looking at the expenditure of the
company we're going to go back to that
we're going to predict its profit but
instead of predicting it just on the R&D
we're going to look at other factors
like Administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company's going to be
to start our coding we're going to begin
by importing some basic libraries and
we're going to be looking through the
data before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it CU it's
such a visual to look at and so easy to
use um just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going to
import numpy as NP and then I want you
to skip on line and look at import
pandas as PD these are very common tools
that you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your SK learn toolbox you always import
that right off the beginning pandas
although you don't have to have it for
your sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
we usually like to use the pandas when
we can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplot library. pyplot as PLT and then
caborn as SNS Seaborn works with the
matplot library so you have to always
import matplot library and then caborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the caborn is so easy to use it
just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign matap plot
library in line that is only because I'm
doing an inline IDE my interface in the
an Ona Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in the slide you'll
see companies equals pd. read CSV and it
has a long line there with the file at
the end 1,000 companies CSV you're going
to have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the iocation and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandis data set
but we can easily extract the data from
our pandas data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the pandas sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have r& D spend
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of
$2,261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
notebook and see what we got going and
so we have the visualization and again
we're using SNS which is pandas as you
can see we imported the matplot library.
pyplot as PLT which then the caborn uses
and we imported the caborn as SNS and
then that final line of code helps us
show this in our um inline coding
without this it wouldn't display and you
can display it to a file and other means
and that's the matte plot library in
line with the Amber sign at the
beginning so here we come down to the
single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns and
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's that's
what this line of code does here x
equals and then it has the colon comma 3
in Brackets the first part the colon
comma means that we're going to look at
all the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer's automatically transformed
our header into the setup and it's
automatically transformed all these
different variables so when we did the
in enoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have xal X
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different uh models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our r& dpin or Administration or
marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals .2 that basically means 20% So
20% of the rows are going to be tested
we're going to put them off to the side
so since we're using a th000 lines of
data that means that 200 of those lines
we're going to hold off to the side to
test for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so it's setting up
our variables but the next step is the
next step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ah and put
that code in there and walk through it
so here here we go we're going to paste
it in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the SK learn. linear
model we're going to import linear
regression now I don't know if you
recall from earlier when we were doing
all the math let's go ahead and flip
back there and take a look at that do
you remember this where we had this long
formula on the bottom and we were doing
all this suiz and then we also looked at
setting it up with the different lines
and then we also looked all the way down
to multiple linear regression where
we're adding all those formulas together
all of that is wrapped up in this one
section so what's going on here is I'm
going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor do fet in this case we do X
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that that math for us
so within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy
xals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has
the coefficients for us and we can
simply just print regressor do
coefficient uncore when I run this
you'll see our coefficients here and if
we can do the regressor coefficient we
can also do The regressor Intercept
let's run that and take a look at that
this all came from the multiple
regression model and we'll flip over so
you can remember where this is going
into and where it's coming from you can
see the formula down here where y = M1 *
X1 + M2 * X2 and so on and so on plus C
the coefficient so these variables fit
right into this formula y = slope 1 *
column 1 variable plus slope 2 * column
2 variable all the way to the m into the
n and x to the N plus C the coefficient
or in this case you have -
8.89 to the^ of 2 etc etc times the
first column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r r squ value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the r squared
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us so we know our true
the Y predict of those 200 values is
what we think it was true and when we go
ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh on higher
shows that this is a very valid
prediction based on the R2 score and if
r squar value of 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model let's take a look at what we went
over today and take a look at our key
takeaways first if you are an aspiring
data scientist who's looking out for
online training and certification in
data science from the best universities
and Industry experts then search no more
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below what is logistic
regression let's say we have to build a
predictive model or a machine learning
model to predict whether the passengers
of the Titanic ship have survived or not
the ship rck so how do we do that so we
use logistic regression to build a model
for this how do we use logistic
regression so we have the information
about the passengers their ID whether
they have survived or not their class
and name and so on and so forth and we
use this information where we already
know whether the person has survived or
not that is the labeled information and
we help the system to train based on
this information based on this labeled
data this is known as labeled data and
during the process of building the model
we probably we'll remove some of the
non-essential parameters or attributes
here we only take those attributes which
are really required to make this
predictions and once we train the model
we run new data through it whereby the
model will predict whether the passenger
has survived or not so let's see what we
will learn in this video we will talk
about what is supervised learning and we
will go into details about
classification which is one of the
techniques for supervised learning and
then we will further focus on logistic
regression is which is one of the
algorithms for performing classification
especially binary classification then we
will compare linear and logistic
regression and what are some of the
logistic regression applications and
finally we will end with the use case or
a demo of actual python code for doing
logistic regression in jupyter no all
right so let's start with what is
supervised learning supervised learning
is one of the two main types of machine
learning methods here we use what is
known as labeled data to help the system
learn this is very similar to how we
human beings learn so let's say you want
to teach a child to recognize an apple
how do we do that we never tell the
child okay this is an apple has a
certain diameter on the top certain
diameter at the bottom and this has a
certain RGB color no we just show an
apple to the child and tell the child
this is Apple and then next time when we
show an apple child immediately
recognizes yes this is an apple
supervised learning works very similar
on the similar lines so where does
logistic regression fit into the overall
machine learning process machine
learning is divided into two types
mainly two types there is a third one
called reinforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
uses techniques like classification and
regression now supervised learning is
used when you have labeled data you have
historical data then you use supervised
learning when you don't have labeled
data then you used unsupervised learning
it's in supervised learning there are
two types of techniques that are used
classification and regression based on
what is the kind of problem we are
solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict the share price or you want to
predict the temperature that will be the
what will be the temperature tomorrow
that is where you use regression whereas
classification are discrete values is
will the customer buy the product or
will not buy the product will you get a
promotion or you will not get a
promotion I hope you're getting the idea
or it could be multiclass classification
as well let's say you want to build an
image classification model so the image
classification model would take an image
as an input and classify into multiple
classes whether this image of a cat or a
dog or an elephant or a tiger so there
are multiple classes so not necessarily
binary classification so that is known
as multiclass classification so we are
going to focus on classification because
logistic regression is one of the
algorithms used for classification now
the name may be a little confusing in
fact whenever people come across
logistic regression it always causes
confusion because the name has
regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you're wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we going to to focus
on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few decision trees are
quite popular and very easy to
understand and therefore they used for
classification then we have K nearest
neighbors this is another gorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into a little bit
of details about logistic regression all
right what is logistic regression as I
mentioned earlier logistic regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if it it's going to to break down in the
near future so this is like a
classification problem find out whether
your car will break down or not so how
are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y AIS X is the number of years since the
last service was performed and Y is the
probability of your car breaking down
and let's say this information was this
data rather was collected from several
car users it's not just your car but
several car users so that is our labeled
data so the data has been collected and
um for for the number of years and when
the car broke down and what was the
probability and that has been plotted
along X and Y AIS so this provides an
idea or from this graph we can find out
whether your car will break down or not
we'll see how so first of all the
probability can go go from 0 to 1 as you
all aware probability can be between 0
and 1 and as we can imagine it is
intuitive as well as the number of years
are on the Lower Side maybe one year two
years or 3 years still after the service
the chances of your car breaking down
are very limited right so for example
chances of your car breaking down on the
probability of your car breaking down
within 2 years of your last service are
0.1 probability similarly 3 years is
maybe3 and so on but as the number of
years increases let's say if it was six
or 7even years there is almost a
certainty that your car is going to
break down that is what this graph shows
so this is an example of a application
of the classification algorithm and we
will see in little details how exactly
logistic regression is applied here one
more thing needs to be added here is
that the dependent variables outcome is
discrete so if we are talking about
whether the car is going to break down
or not so that is a discrete value the
why that we are talking about the
dependent variable that we are talking
about what we are looking at is whether
the car is going to break down or not
yes or no that is what we are talking
about so here the outcome is discrete
and not a continuous value so this is
how the logistic regression curve looks
let me explain a little bit what exactly
and how exactly we are going to uh
determine the class at the outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any any value of the
probability below 0.5 is considered to
be zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you're going to
find a continuous value you're going to
determine a continuous value it could be
the price of a real estate property it
could be your hike how much hike you're
going to get or it could be a stock
price these are all continuous values
these are not discrete compared to a yes
or a no kind of a response that we are
looking for in logistic regression so
this is one example of a linear
regression let's say the HR team of a
company tries to find out what should be
the salary hike of an employee so they
collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on that
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is a label data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors th root means
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clicked
at 0o and one but still the straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odds so the probability
of an event happening divided by the
probability of the event not happening
so p is it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from
0er to Infinity so when this probability
is zero then the odds will the value of
the odds is equal to Z and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line look so Y
is equal to Beta 0 plus beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of P X by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 + beta 1 x which is the
equation of the straight line and now
from here if you want to find out the
value of PX you will see we can take the
exponential on both sides and then if we
solve that equation we will get the
equation of PX like this PX is = 1 by 1
+ e ^ of - beta 0 + beta 1 x and recall
this is nothing but the equation of the
line which is equal to y y is equal to
Beta 0 + beta 1 x so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression Al all right and if
this is plotted this is how the sigmoid
curve is obtained so let's compare
linear and logistic regression how they
are different from each other let's go
back so linear regression is solved or
used to solve regression problems and
logistic regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have have to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or dis
creete in nature and the linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's n shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and U show the
code but before that let me through a
couple of slides to explain what we're
trying to do so let's say you have an 8
by8 image and the the image has a number
1 2 3 4 and you need to train your model
to predict what this number is so how do
we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model list with the test
data which means that like any machine
learning process we split our initial
data into two parts training set and
test set with the training set we train
our model and then with the test set we
we test the model till we get good
accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks so first
thing we do is import all the libraries
and then we basically take a look at the
images and see what is the total number
of images we can display using mat plot
lip some of the images or a sample of
these images and um then we split the
data into training and test as I
mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model model
with the training set and then we test
it with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so to the most important
part here is that the model will be most
accurate when we have the max maximum
numbers in its diagonal like in this
case that's why it has almost 93 94%
because the diagonal should have the
maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four
four and out of which 34 have been
predicted correctly as number four and
one has been predicted as number eight
and another one has been predicted as
number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like 100%
accurate model okay so that's uh gist of
how to use this Matrix how to use this
confusion Matrix I know the name uh is a
little funny sounding confusion Matrix
but actually it is not very confusing
it's very straightforward so you just
plotting what has been predicted and
what is the labeled information or what
is the actual data that's also known as
the ground truth sometimes okay these
are some fancy terms that are used so
predicted label and the actual label
that's Sol letter is okay yeah so we are
showing a little bit more information
here so 38 have been predicted and here
you will see that all of them have been
predicted correctly there have been 38
zeros and the the predicted value and
the actual value is is exactly the same
whereas in this case right it has there
are I think 37 + 5 yeah 42 have been fed
the images 42 images are of Digit three
and uh the accuracy is only 37 of them
have been accurately predicted three of
them have been predicted as number seven
and two of them have been predicted as
number8 and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and M plot and we will take a look
at uh some of the sample images that we
have uh loaded so so this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the way it works is the
pixel information so as you can see here
this is an 8 by 8 pixel kind of a image
and uh the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn uh for
which number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and test so that the training
data set is used to train the system so
we pass this probably multiple times uh
and then we test it with the test data
set and the split is usually in the form
of there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
202 3 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xor train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is labeled data for each
of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train ycore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is
1383 by 64 and ycore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations
in test and 1383 observations in train
so that's basically what these four
lines of code are are saying okay then
we import the logistic regression
library and uh which is a part of psyit
learn so we we don't have to implement
the logistic regression process itself
we just call these the function and uh
let me go ahead and execute that so that
uh we have the logistic regression
Library imported now we create an
instance of logistic regression right so
logistic RR is a is an instance of
logistic regression and then we use that
for training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as a
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion underscore Matrix method takes
two parameters one is the Yore test and
the other is the predictions so what is
a Yore test these are the labeled values
which we already know for the test data
set and predictions are what the system
has predicted for the test data set okay
so this is known to us and this is what
the system has uh the model has
generated so we kind of create the
confusion Matrix and we will print it
and uh this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point point out
here is that the accuracy of the model
is determined by how many numbers are
there in the diagonal the more the
numbers in the diagonal the better the
accuracy is okay and first of all the
total sum of all the numbers in this
whole Matrix is equal to the number of
observations in the test data set that
is the first thing right so if you add
up all these numbers that will be equal
to the number of observations in the
test data set and then how out of that
the maximum number of them should be in
the diagonal that means the accuracy is
predic good if the the numbers in the
diagonal are less and in all other
places there are a lot of numbers uh
which means the accuracy is very low the
diagonal indicates a correct prediction
this means that the actual value is same
as the predicted value here again actual
value is same as the predicted value and
so on right so the moment you see a
number here that means the actual value
is something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that is basically how we read the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 Plus 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
all the values are here most of the
values so which means reasonable this
seems to be reasonably accurate and yeah
basically the accuracy score is 94% this
is calculated as I mentioned by adding
all these numbers divided by the total
test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this is also
been predicted correctly N9 and actual
value is N9 and this is a im of one and
again this has been predicted correctly
as like the actual value okay so this
was a quick demo of logistic regression
how to use logistic regression to
identify
images so we put them side to side we
have our linear regression which is a
predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on this that
the output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the number is coming out linear
usually means we're looking for that
line versus a different model and it's
used in business domain forecasting
stocks uh it's used as a basis of all of
most um uh predictions with numbers so
if you're looking at a lot of numbers
you're probably looking at a a linear
regression
model
uh for instance if you do just the high
lows of the stock exchange and you're
you're going to take a lot more of that
if you want to make money off the stock
you'll find that the linear regression
model fits uh probably better than
almost any of the other models even you
know high-end neural networks and all
these other different machine learning
and AI models because they're numbers
they're just a straight set of numbers
you have a high value low value volume
uh that kind of thing so when you're
looking at something that straight
numbers um and are connected that way
usually you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of um like a
window of things so you could take a
window of stock history and you could
CLA generate classifications based on
that and separate the data that way if
it's going to be that this particular
pattern occurs is going to be upward
trending or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a Zer one do I care or do I even want to
look at it so let's do a demo so you can
get a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to to find the
poisonous and nonpoisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda Navigator in this one
I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
log
git um is one of the one of the names
that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet rows and columns
it's a nice way of viewing the data and
then we have uh we're going to be
bringing in our pre-processing label en
Cod coder I'll show you what that is um
when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is
and then we have our uh logistic
regression model our categorical one and
then we have our linear regression model
these are the two these right here let
me just um um clear all that there we go
uh these two right here are what this is
all about logistic versus uh linear is
it categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it measure
that we expect it to be you know if
you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this has
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
that's what we're starting with uh we're
loading that with our pandas and it
prints it in a nice format where you can
see the age sex uh body mass index
number of children smoker so this might
be something that the insurance company
gets from the doctor it says hey we're
going to this is what we need to know to
give you a quote for what we're going to
charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 2 3 4 5 6 7 so
there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn
um what to expect them to charge on
there uh so is this going to be you know
is this person going to cost me uh
$16,814.23
3,866 how do we guess that so that we
can guess what the minimal charge is for
their
insurance and then there's one other
thing you really need to notice on this
data um and I mentioned it before but
I'm going to mention it again because
pre-processing data is so much of the
work in data science um sex well how do
you how do you deal with female versus
male um are you a smoker yes or no what
does that mean region how do you look at
Region it's not a number how do you draw
a line between Southwest and
Northwest um you know they they're
objects it's either your Southwest or
your Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this work
oops there we go okay so let's take a
look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. sex. drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at um male or female and so it
usually just converts it to a01 cuz
there there's only two choices on
here same thing with the smoker it's
zero or one so we're going to transfer
the trans change the smoker uh 01 on
this and then finally we did region down
here region does it a little bit
different we'll take a look at that and
um it's I think in this case it's
probably going to do it because we did
it on this label transform um with this
particular setup it gives each region a
number 01 2 3 so let's go a and take a
look and see what that looks like go and
run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
0 1 2 three so it's three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region South West
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set dropped the chart
charges because that's where we're going
to predict we're predicting charges
right here so we don't want that as our
input for our features and our y output
is charges that's what we want to guess
we want to guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the data and
then finally the big ones uh this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from and this
comes from the py kit uh learn and this
is the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows
this model will do as good as any other
model out there for do if you're doing
just the very basic high lows and
looking for a linear fit a regression
model fit um and what you one of the
things when I am looking at this is I
look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about earlier
and so we go in this and we go ahead and
run this you'll see it loads the linear
regression model and just has a nice
output that says hey I loaded the linear
regression model and then the second
part is we did the fit and so this model
is now trained our linear model is now
trained on the training
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so when
you're looking at the back end of the
data you remember we had that formula uh
BX X1 plus bxx2 plus the plus the uh
intercept uh and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know what uh smoker look how smoker
impacts the cost um it's just massive uh
so this is a flag that hey the value of
the smoker really affects this model and
then you can see here with the body mass
index uh so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor um and then you can see
down here we have uh sex is than a
factor also and it just it changes as
you go in there negative number it
probably has its own meaning on there
again it gets really complicated when
you dig into the um workings and how the
linear model works on that and so um we
can also look at the intercept this is
just kind of fun U so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that and so you
can see right here we can go back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out and if
you're going to create a model to
predict something uh we'll go ahead and
predict it here's our y prediction value
linear model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom data
and so in this you can see here we we've
run our basic model we've built our
coefficients you can see the intercept
the backend you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this take a look at
the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little pre coding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder is just
switching it to 0 1 two 3 and giving it
an integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns and the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this gets
shifted from uh xbxx K to 01 2 3 4 5 or
whatever it is class is 01 one being
poisonous zero looks like it's editable
and so forth on here so we're just
encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring at 0 1 2 3 4 five um
in this particular example it's not
going to make that big of a difference
how we encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our U train and
test
and then you'll see here on this next
setup um this is where we create our
model all the magic happens right here
uh we go ahead and create a logistics
model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because that that's what it
does is it goes through the math and it
goes hey can we minimize the ER and it
keeps finding a lower and lower error
and it still is changing that number so
that means it hasn't conversed yet it
hasn't find the lowest amount of error
it can and the default is 100 uh there's
a lot of settings in here so when we go
in here to let me pull that up from the
SK learn uh so we pull that up from the
SK learn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and I'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our SK learn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
went ahead and done a predict here also
just like I was showing you earlier
uh so here is where we actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
um I think I just showed you a graph and
we can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model score
and it says that it's pretty accurate
we're getting a roughly 95% accuracy
well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here U
let's see I believe it's poisonous was
one uh 0 is edible so let's go ahead and
run that 01 zero is good so here is um a
confusion Matrix and this is if you're
not familiar with these we have true
true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 1 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh start e
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as um the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science I'm talk about your machine
learning
code classification is probably one of
the most widely used tools in machine
learning in today's world it is also one
of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithms and there are
a lot out there so we're only going to
touch briefly on a variety of them so
you can see how they the different
flavors work and we'll have some
Hands-On Demos in Python embedded
throughout the tutorial classific ation
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and where you
end when you start running these um
machine learning algorithms and
classification and the classification
algorithms is a little Black Box in a
lot of respects and we'll look into that
you can see what I'm talking about when
we start swapping in and out different
models let's say we are given the task
of classifying a given bunch of fruits
and vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be
grouped together and so we have a data
set we'll call it the bunch is divided
into clusters one of which consists of
the fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out in images
to see what is malignant what is benign
very popular one can you classify
flowers the iris uh data set uh
certainly in Wildlife can you classify
different animals and track where
they're going classification is really
the bottom starting point or the
Baseline for a lot of machine learning
and setting it up and trying to figure
out how we're going to break the data up
so we can use it in a way that is
beneficial so here the fruits and the
vegetables are grouped into clusters and
each clusters has a specific
characteristic I.E whether they are a
fruit or a vegetable and you can see we
have a pile of fruits and vegetables we
feed it into the algorithm and the
algorithm separates them out and you
have fruits and vegetables so some
important terminologies before we dig
into how it sorts them out and what that
all means uh when we look at the
terminologies you have a classifier
that's the algorithm that is used to map
the input data to a specific category
the classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times the
classifier is part of the classification
model um and then you choose which
classifier you use after you choose
which model you're using where features
are what goes in labels are what comes
out so your classifier models right in
the middle of that that's that little
black box we were just talking about
clusters they are a group of data points
which have some common characteristics
binary classification it is a
classification condition with two
outcomes which are either true or false
multi-label classification this is a
classification condition where each
sample is assigned to a set of labels or
targets multiclass classification the
classification with more than two
classes here each sample is assigned to
one and only one label when we look at
this group of terminologies a few
important things to notice uh going from
the top clusters when we cluster data
together we don't necessarily have to
have an end goal we just want to know
what features cluster together these
features then are mapped to the outcome
we want in many cases the first step
might not even care about the outcome
only about what data connects with other
data and there's a lot of clustering
algorithms out there that do just the
clustering part binary classification it
is a classification condition with two
outcomes which are either true or false
we're talking usually um it's a uh it's
either a cat or it's not a cat it's
either a dog or it's not a dog um that's
the kind of thing we talk about binary
classification and then that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multiclass classification where
each one's a binary uh you can either be
a cat or a dog you can't be both a cat
and a dog real world applications so to
make sense of this uh of course the
challenge is always in the details is to
understand how we apply this in the real
world so in real world applications we
use this all the time we have email spam
classifier so you have your email inbox
coming in uh it goes through the email
filter that we usually don't see in the
background and it goes this is either
valid email or it's a Spam and it puts
it in the spam filter if that's what it
thinks it is uh Alexa's voice classifier
Google Voice any of the voice
classifiers they're looking for points
so they try to group words together and
then they try to find those groups of
words trigger a CL a classifier so it
might be that the classifier is to open
your tasks program or open your text
program so you can start sending a text
sentimental sentiment analysis is really
big uh when we're tracking products
we're tracking marketing trying to
understand uh whether something is liked
or disliked is huge uh that's like one
of the biggest driving forces in sales
nowadays and you almost have to have
these different filters going on if
you're running a large business of any
kind fraud detection uh you can think of
banks uh they find different things on
your bank statement and they detect that
there's something going on there they
have algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned the cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with a classification
when the wild animals in our area are
out like foxes we actually have a
mountain lion that lives in the area so
it's nice to know when he's here
handwriting prediction uh classifying A
B C D and then classifying words to go
with that so let's go ahead and uh roll
our sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms uh let's go
back and take a look at our definitions
we have a classifier and a
classification model so we're looking at
the classifier an algorithm that is used
to map the input data to a specific
category one of those algorithms is a
logistic regression the logistic
regression is a classification algorithm
used to model the probability of a
certain class or event existing such as
pass fail or win lose Etc it provides
its output using the logistic function
or sigmoid function to return the
probability value that can then be
mapped to two or more discret classes a
sigmoid function is an activation
function that fits the variable and
limit the output to a range between zero
and one a standard sigmoid function or
logistic function is represented by the
formula FX = 1/ 1 + eus X where X is the
equation of the line and E is the
exponential just taking a quick look at
this um you can think of this as being a
point of uncertainty and so as we get
closer and closer to the middle of the
line it's either um activated or not and
we want to make that just shoot way up
uh so you'll see a lot of the activation
formulas kind of have this nice s curb
where it approaches one and approaches
zero and based on that there's only a
small region of error and so you can see
in the sigmoid logistic function uh the
1 over 1 + e- x to the- X you can see if
fam that that nice s curb uh we also can
use a tangent variation there's a lot of
other different uh models here as far as
the actual algorithm this is the most
commonly used one let's go ahead and
roll up our sleeves and take a look at a
demo that is going to use a logistic
regression so we're going to have the
activation formula and the model because
you have to have you have to have both
for this we will go into our Jupiter
notebook now I personally use the
Anaconda navigator to open up the
Jupiter notebook to set up my IDE as a
webbased it's got some advantages that
it's very easy to display in uh but it
also has some disadvantages in that if
you're trying to do multi- threads and
multi-processing you start running into
a single git issues with python and then
I jump to uh py charm really depends on
whatever ID you want just make sure
you've installed uh numpy and the
sklearn modules into your Python and
whatever environment you're working in
so that you'll have access to that for
this demo now the team in the back has
prepared my code for me which I'll start
bringing in one section at a time so we
can go through it before we do that it's
always nice to actually see where this
information is coming from and what
we're working with uh so the first part
is we're going to import our packages
which you need to install into your
Python and that's going to be your numpy
um we usually use numpy as NP and then
from sklearn the learn model we're going
to use a logistic regression and from
sklearn metrics we're going to import
the classification report confusion
Matrix and if we go ahead and open up S
uh the site k-ar org and go under their
API you can see all the different um
features and models they have and we're
looking at the linear model uh logistic
regression one of the more common
classifiers out there and if we go go
ahead and go into that and dig a Little
Deeper you'll see here where they have
the different settings and it even says
right here note the regularization is
applied by default so by default that is
the activation formula being used now
we're not going to spend we might come
back to this look at some of the other
models it's always good to see what
you're working with but let's go ahead
and jump back in here and we have our
Imports we're going to go ahead and run
those uh so these are now available to
us as we go through our Jupiter notebook
script and they put together a little
piece of dat for us this is simply um
going through uh 0 to one actually let's
go ahead and print this out over here
we'll go ahead and print X just so you
can see what we're actually looking for
and when we run this you can see that we
have our X is 01 2 through 9 we reshaped
it the reason for this is just looking
for a row of data usually we have
multiple features we just have the one
feature which happens to be 0o through
nine and then we have our 10 answers
right down here uh 0 1 0 00111111 you
can bring in a lot of different data
depending on what you're working with uh
you can make your own you can instead of
having this as just a single you could
actually have like multiple features in
here but we just have the one feature
for uh this particular demo and this is
really where all the magic happens right
here uh and I told you it's like a black
box that's the part that is is is kind
of hard to follow and so if you look
right here we have our model we talked
about the model right there and then we
went ahead and set it for Library linear
there was a showed you earlier that's
actually default so it's not that
important uh random State equals zero
this stuff you don't worry too much
about and then with the S kit learn
you'll see the model fit this is very
common to S kit they use similar stuff
in a lot of other different packages but
you'll you'll see that that's very
common you have to fit your data and
that means we're just taking the data
and we're fitting our X right here which
is our features that's our X and here's
y these are the labels we're looking for
so before we were looking at is it fraud
is it not is it cat is it not um that
kind of thing and this is looking at 01
so we want to have a binary setup on
this and we'll go ahead and run this uh
you can see right here it just tells us
what we loaded it with as our defaults
and that this model has now been created
and we've now fit our data to it and
then comes the fun part you work really
hard to clean your data to um bake it
and cook it there's all kinds of I don't
know why they go with the cooking terms
as far as how we get this data formatted
then you go through and you pick your
model you pick your solver and you have
to test it to see hey which one's going
to be best and so we want to go ahead
and evaluate the model and you do this
is that once you've figured out which
one is going to work the best for you
you want to evaluate it so you can
compare it to your last Model and you
can either update it to create a new one
or maybe change the um solver to
something else I mentioned tangent
that's one of the other common ones
that's commonly used with language for
for some reason the tangent even though
it looks almost to me identical to the
one we're using with the sigmoid
function uh it for some reason it
activates better with language even
though it's a very small shift in the
actual math behind it we already looked
at the um data early but we'll going and
look at it again just you can see we
look at we have our rows of zero one row
only has one entity and we have our
output that matches these rows and these
do have to match you'll get an error if
you put in something with a different
shape so if you have um 10 rows of data
and nine answers it's going to give you
an error because you need to have 10
answers for it a lot of times you
separate this too when you're doing
larger models uh but for this we're just
going to take a quick look at that the
first thing we want to start looking at
is uh The Intercept one of the features
inside our linear regression model we'll
go ahead and run that and print it uh
you'll see here we have an intercept of
minus 1.51 6 and if we're going to look
at the uh intercept we should also look
at our um coefficients and if if you run
that you'll see that we get a we get the
um our coefficient is the 7035 you can
just think of this as your eidan
geometry for very basic model like this
where it intercepts the Y at some point
and we have a coefficient multiplying by
it little more complicated in the back
end but that is the just of this simple
model with just the one feature in there
and we'll go ahead and uh reprint the Y
because I want to put them on top of
each other with the Y predict and so
these were the Y values we put in and
this is the Y predict we had coming out
and you can see um yeah here we go uh
there's the Y actual and there's what
the prediction comes in uh now keep in
mind that we used the actual complete
data as part of our training uh that is
if you're doing a real model a big
stopper right there because you can't
really see how good it did unless you
split some data off to test it on this
is the first step is you want to see how
your model actually tests on the data
you trained it with and you can see here
there is this point right here where it
has it wrong and this point right here
where it also has it wrong and it makes
sense because we're going our input is 0
1 0 through 9 and it has to break it
somewhere and this is where the break is
uh so it says this half the data is
going to be zero because that's what it
looked like to me if I was looking at it
without an algorithm and this data is
probably going to be one and I I didn't
I forgot to point this out so let's go
back up here I just kind of glanced over
this window here where we did a lot of
stuff let's go back and and just take a
look at that what was done here is we
ran a prediction uh so this is where our
predict comes in is our model. predict
so we had a model fit we created the
model we programmed it to give us the
right answer uh now we go ahead and
predict what we think it's going to be
there's our model. predict probability
of X and then we have our y predict
which is very similar but this is has to
do more with the probability numbers so
if you remember down below we had the
setup where we're looking at uh that
sigmoid function that's what this is
returning and the Y predict is returning
a zero or a one and then we have our um
confusion Matrix we'll look at that and
we have our report which just basically
Compares our y to our y predict which we
just did it's kind of nice as simple
data so it's really easy to see what
we're doing that's why we do use the
simple data this can get really
complicated when you have a lot of
different features and things going on
and splits uh so here we go we've had
our I printed out our actual and our
prediction so this is the actual data
this is what the predict ran um and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that uh this is great
if you have a lot of data to look at but
you can see right here a confusion
Matrix says uh if you remember from the
confusion Matrix we have the two this is
two correct one two and and uh it's been
a while since I looked at a a confusion
Matrix there's the two and then we have
this one which is our six that's where
the six comes from and then we have this
one which is the um one false this is
the two ones so we have this one here
and this one here which is
misclassified this really depends on
what data you're working with as to what
your is important um you might be
looking at this model and if this model
this confusion Matrix comes up and says
that uh you've
misclassified even one person as being
non malignant cancer that's a bad model
uh I wouldn't want that classification
I'd want this number to be zero I
wouldn't care if this false positive was
off by a little bit more long as I knew
that I was correct on the important
factor that I don't have cancer so you
can see that this confusion Matrix
really aims you in the right direction
of what you need to change in your model
how you need to adjust it uh and then
there's of course a report reports are
always nice um if you notice we
generated a report earlier we'll go and
just print the report up and you can
remember this is our report it's a
classification report y comma y predict
so we're just putting in our two values
basically what we did here visually with
our actual and our predicted value and
we'll go ahead and run the report and
you can see it has the Precision uh the
recall your F1 score your support uh
translated into a accuracy macro average
and weighted average so it has all the
numbers a lot of times when working with
um clients or with the shareholders in
the company this is really where you
start because it has a lot of data and
they can just kind of stare at it and
try to figure it out and then you start
bringing in like the confusion Matrix I
almost do this in Reverse as to what
they show I would never show your
shareholders The Intercept of the
coefficient that's for your internal
team only working on machine language uh
but the confusion Matrix and the report
are very important important those are
the two things you really want to be
able to show on these uh and you can see
here we did a decent job of um
classifying the data managed to get a
significant portion of it correct uh we
had our was it accuracy here is a 080 F1
score uh that kind of thing so you know
it's a pretty accurate model of course
this is pretty goofy it's very simple
model and it's just splitting the model
between uh ones and zeros so that was
our demo of the U UNL listic regression
on there let's go take a look at K
nearest neighbors uh this one is another
very highly used and important algorithm
to understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based on the similarity measure the K
nearest neighbor finds out the class of
the new data point by finding its
nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the the new
data point then the KN andn classifies
the new data point as Class A the K and
K nearest neighbors is the number of
nearest neighbors we are looking for I
I.E if we say k equals 3 this means that
we are looking for nearest three
neighbors of unclassified data point
usually we take the K value between 3 to
10 as it leads to a better result a
smaller value of K means that noise will
have a bigger influence on the result
and a larger value of K makes it compet
Ally expensive hence the data scientists
prefer the range of K between 3 and 10
when we talk about noise remember the
data we just looked at was 0 1 1 0 0 it
had some some values where cut it and
said everything to the right is a one
everything to the left is a zero but it
had some ones and zeros mixed in there
that's called noise that's what they're
talking about is there's some things
that are right in the middle in the
classification which makes it very hard
to classify so suppose we're trying to
find the class for a new point indicated
by the red color and you can see it's
kind of right between the cat right
between the dogs let k equal three so we
are finding the 3n in for the red data
point by looking at the plot on the
right we can see that the red data point
belongs to the class dogs is it has two
votes for class dog and one vote for
class cat and if you ask the question
well what are you measuring the distance
what is that distance um it could be the
measurements of the ears whether they're
pointed or floppy that might be one of
the features you're looking at is how
floppy the ears are um another one might
be the whiskers versus the nose um and
then you take those measurements and
using uh one of the most common things
in K means measurement is the ukian
geometry you can figure out the distance
between those points there's a lot of
different algorithms for that but you
can think about it that you do have to
have some kind of solid data to measure
and so we can conclude that the new data
point belongs to the class dog so let's
go ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our jupyter notebook and open
up a new um Python Programming script
page of course once we're in here we'll
want to look at the uh ssy kit learn um
I did just a quick search for SK
neighbors Ken neighbors classifier um
this actually is the older version .0 no
023 is the one we want and you'll see
here that we have all their defaults in
Neighbors equals 5 at defaults we were
talking about that between three and 10
there's different ways to weigh it
there's an algorithm based on it I
mentioned ukian geometry finding the
distance there's other algorithms for
figuring out what that distance is and
how to weight those uh and there's a lot
of other parameters you can adjust for
the most part the K means uh basic setup
is a good place to start and just let
the defaults go uh we might play with
some of those we'll see what the guys in
the back did and from here we're going
to import numpy we're going to use
pandas if you haven't been running
pandas pandas is our data frame which
sits on top of numpy uh data frames are
you know numes is our number array
pandas is our data frame map plot
library because we're going to plot some
graphs everybody likes some pretty
pictures it makes it a lot easier to see
what's going on when you have a nice
display and that's also what the Seaborn
is in here in the uh setup that sits on
top of the map plot Library the ones we
really want to look at right here are
the what we're bringing in from sklearn
these ones right here uh so from sklearn
we're going to load I mentioned the
breast cancer that's a very popular one
because it has I believe it's 36
measurements so there's 36 features and
unless you're a expert you're not going
to know what any of those features
really mean you can sort of guess what
their special measurements they take of
when they take a image uh and of course
our confusion Matrix so that we can take
a look and see what the data looks like
and how good we did
uh and then we have our knen neighbors
classifier on here uh and then I
mentioned that uh whenever you do
training and testing you want to split
the data up you don't want to train the
data and then test it on the same data
that just tells you how good your
training model is it doesn't tell you
whether it actually works on unknown
data and so this just splits it off so
that we can train it and then we can
take a look at data we don't have in
there and see how good it did and we'll
go ahead and load our data up so here's
our our setup on that oops there we go
so we're going to go ahead and load the
data up uh we have our x value and
that's going to come from our breast
cancer. dat and column breast cancer
feature names so there's our actual um
all our different features we'll print
that out here in a second and then we
have our um mean area mean compactness
so I guess we're going to take the data
and we're only going to use a couple of
the columns this just makes it easier to
read um of course when you actually were
going to do this you'd want to use all
your columns and then we have our Y and
this is simply um whether it's either
malignant or B9 and then um we want to
go ahead and drop the first line because
that's how it came in on there and we'll
go ahead let's just take a look at this
a little closer here let's go and run
this real quick and just because I like
to see my data before I run it we can
look at this and we can look at the
original features remember we're only
going to use two features off of here
just to make it a little easier to
follow and here's the actual data and
you can see that this is just this
massive stream of data coming in here
it's going to just skip around because
there's so much in there to set up I
think there's like 500 if I remember
correctly and you can see here's all the
different measurements they take but we
don't we don't really need to see that
on here we're just going to take a look
at just the two columns and then also
our solution we'll go and just do a
quick uh print y on here so you can see
what the Y looks like and it is simply
just
000000 you know B9 00001 so a one means
it's B9 a zero means it's malignant uh
is what we're looking at on that and cut
that out of there the next stage is to
go ahead and split our data I mentioned
that earlier uh we'll just go ahead and
let them do the splitting forest for us
uh we have XT train X test y train y
test and so we go ahead and train test
split XY random State equals 1 makes it
nice and easy for us we'll go and run
that and so now we have our training and
our testing train means we're going to
use that to train the model and then
we're going to use the test to test to
see how good our model does and then
we'll go ahead and create our model
here's our KNN model the K neighbors
classifier n neighbors equals 5 the
metrics is ukian remember I talked about
ukian uh this is simply your c^2 = A2 +
B2 + u u A2 = b plus C S Plus c s + d^2
and then you take the square root of all
that that's what they're talking about
here it's just the length of the
hypotenuse of a triangle but you can
actually do that in multiple Dimensions
just like you do in two Dimensions with
a regular triangle and here we have our
fit this should start to look familiar
since we already did that in our last
example that's very standard for s s kit
and any other one although sometimes the
fit algorithms look a little bit more
complicated because they're doing more
things on there especially when you get
into neural networks uh and then you
have your K neighbors it just tells you
we created a um a k neighbor setup they
kind of wanted us to reformat the Y but
it's not that big of a deal for this uh
and it comes out and shows you that
we're using the ukian uh metric for our
measurement so now we've created a model
here's our live model we fitted the data
to it we say hey here's our training
data uh let's go ahead and predict it so
we're going to take our y predict equals
KNN predict y test so this is data we
have not this model has not seen this
data and so we're going to create create
a whole new set of data off of there now
before we look at our prediction in fact
let's um I'm going to bring this down
and and put it back in here later let's
take a look at our X test data versus
the Y test what does it look like and so
we have our mean area we're going to
compare it to our mean compactness we're
going to go ahead and run that and we
can see here the data if you look at
this just eyeballing it let me put it in
here we have a lot of blue here and we
have a lot lot of orange here and so
these dots in the middle especially like
this one here and these here these are
the ones that are going to give us false
negatives and so we should expect this
is your noise this is where we're not
sure what it is and then B9 is in this
case is done in blue and malignant is
done in one uh so if you look at it
there's two points based on these
features which makes it really hard to
have a 100% where the 100% is down here
um or up here that's kind of the thing
I'd be looking for when we're talking
about cancer and stuff like that where
you really don't want any uh false
negatives you want everything you false
positive great you're going to go in
there and have another setup in there
where you might get a get a op Toops or
something like that done on it again
that's very data specific on here uh so
now let's go ahead and pull in and and
get our um our prediction in here and
we'll create our y prediction we'll go
and run that so now this is loaded with
what we think the unknown data is going
to be and we can go ahead and take that
and go Ahad and plot it because it's
always nice to have some pretty pictures
and when we plot it we're going to do
the mean area versus mean compactness
again you look at this map and you can
see that there's some clear division
here we can clearly say on some of the
stuff that are why prediction if we look
at this map up here and this map down
here we probably got some pretty good
deal it looks pretty good like they
match a lot this is of course just
eyeballing it really you don't want to
eyeball these things you want to show
people the pictures so that they can see
it and you can say hey this is what it
looks like uh but we really want the
confusion Matrix and when we do the Y
test and the Y predict we can see in the
confusion Matrix here it did pretty good
um and we'll just go ahead and point
that out real quick um here's our 42
which is positive and our
79 and if I remember correctly I'd have
to look at the data which one of these
is a false negative I believe it's the
nine
that's scary I would not want to be one
of those nine people told that I don't
have cancer and then suddenly find out I
do uh so we would need to find a way to
sort this out and there is different
ways to do that uh a little bit past
this but you can start messing with the
actual ukian geometry and the activation
uh measurements and start changing those
and how they interact but that's very
Advanced there's also other ways to
classify them or to create a whole
another class right here of we don't
knows those are just a couple of the
solutions you might use for that but for
a lot of things this works out great uh
you can see here you know maybe you're
trying to sell something well if this
was not uh life dependent and this was
if I display this ad 42 of these people
are going to buy it and if I display
this other ad if I don't display it 79
people are going to go a different
direction or whatever it is so maybe
you're trying to display whether they're
going to buy something if you add it on
to the website in which case that's
really a good numbers you've just added
a huge number Sals to your company so
that was our K nearest neighbors uh
let's go ahead and take a look at
support Vector machines so support
Vector machines uh is the main objective
of a support Vector machine algorithm is
to find a hyperplane in an N dimensional
space N is a number of features that
distinctly classifies the data points
and if you remember we were just looking
at those nice graphs we had earlier in
fact Let Me Go a and flip back on over
there if we were looking at this data
here we might want to try to find a nice
line through the data and that's what
we're talking about with uh this next
setup so the main objective of support
Vector machine algorithm is to find a
hyperplane in an N dimensional space N
is a number of features that distinctly
classifies the data points to separate
the two classes of data there are many
hyperplanes that can be chosen our
objective is to find the plane that has
the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyper
plane depends on the number of features
if there are two input features then the
hyper plane is just a line if there are
three features then the hyper plane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support vector classifier
or hard margin and that's why I just
showed you on the other data you can see
here we look for a line that splits the
data evenly the problem with hard margin
is that it doesn't allow outliers and
doesn't work with non-linearly separable
data and we just were looking at that
let me flip back on over here and when
we look at this setup in here and we
look at this data here we go look how
many outliers are in here these are all
with all these blue dots on the wrong
side of the line would be considered an
outlier and the same with the red line
and so it becomes very difficult to
divide this data unless there's a clear
space there we go therefore we introduce
soft margins which accept the new data
point point and optimize a model for
nonlinear data points soft margins pass
through the data points at the border of
the classes the support Vector machine
can be used to separate the two classes
of shapes here we can see that although
triangles and diamond shapes have pointy
edges but we are able to classify them
in two categories using a support Vector
machine let's go ahead and see what that
looks like in a demo and flip back on
over to our Jupiter notebook we always
want to start with taking a look at the
SK learn learn uh API in this case the
svm SVC there is a significant number of
parameters because SK the um svm has
been a lot of development in the recent
years and it's become very popular if we
scroll all the way down to methods
you'll see right here is our fit and our
predict that's what you should see in
most of the um site kit learn packages
so this should look very familiar to
what we've already been working on and
we'll go ahead and bring in our import
and run that uh we should already have
pandas numpy our M plot Library which
we're going to be using to graph some of
the things a little bit different right
here you'll see that we're going to go
ahead and bring in um one of the fun
things you can do with test data is uh
make circles circles are really hard to
classify you have a ring on the inside
and a ring on the outside and you can
see where that can cause some issues and
we'll take a look at that a little
closer in a minute here's our svm uh
setup on there and then of course our
metrics cuz we're going to use that to
to take a closer look at things so we go
and run this oops already did once we've
gone ahead and done that we're going to
go ahead and start making ourselves some
data and this part probably a little bit
more complicated uh than we need I'm not
going to go too much in detail on it uh
we're going to make a mesh grid and
we'll see what that looks like in a
minute where we're defining the minimums
you can see in here create a mesh gr of
points here's our parameters of x y and
H it's going to return XX y y you can
also send a note to us make sure you get
a copy of this if you want to dig deeper
into this particular code especially and
we have a y Min yaax uh Y Man y Max plus
one here's our mesh grid we're actually
going to make XX and y y plot the
Contours all the way through and this is
just a way of creating data it's kind of
a fun way to create some data we go plot
Contours ax LF XX y y and return it out
add some perimeters here so that when
we're doing it we have our setup on
there train property and then we'll go
ahead and make our data just throw that
right in there too in the same setup and
run that uh so now we have X and Y we're
going to make our circles we have n
samples equals samples in this case
we're going to have 500 samples we went
ahead and gave it uh noise of 0.005
random State 123 these are all going
into oops let's go back up here we go
make our mesh grid make circles there it
is uh so this is going into our make
circles up here and this is part of that
setup on this and then once we've gone
ahead and make the circle let's go ahead
and plot it uh that way you can see what
we're talking about right now what I'm
saying is really confusing because
without a visual it doesn't really mean
very much what we're actually doing so
I'm going go ahe and run this with the
plot and let's go back up here and just
take a look we've made our Circle we
have our in samples equals samples so
we're going to have 500 we're going to
have training property point8 here's our
data frame we go and load it into the
data frame so we can plot it groups DF
Group by label uh this just kind of a
fun way if you have multiple columns you
can really quickly pull whatever setup
is in there and then we go ahead and
plot it and you can see here we have two
rings that are formed and that's all
this is doing is just making this data
for us this is really hard data to
figure out um a lot of programs get
confused in this because there's no
straight line or anything like that but
we can add add planes and different
setups on here and so you can see we
have some very interesting data we have
our zero is in blue and our one's in the
yellow in the middle and the data points
are um an XY coordinate plot on this one
of the things we might want to do on
here is go ahead and find the Min to Max
ratio uh set up in there and we can even
do let's just do a print X so you can
see what the data looks like that we're
producing on this there we go so uh xal
x - Xmen over x max minus X Min all
we're doing is putting this between zero
and one whatever this data is we want um
a 0o to one setup on here if you look at
this all our data is
0.8.5 that's what this particular line
is doing that's a pretty common thing to
do in processing data especially in
neural networks uh neural networks don't
like big numbers they create huge biases
and cause all kinds of problems and
that's true in a lot of our models some
of the models it doesn't really matter
matter but when you're doing enough of
these you just start doing them uh you
just start putting everything between
zero and one there's even some
algorithms to do that in the SK learn
although it's pretty as you can see it's
pretty easy to do it here so uh let's go
ahead and jump into the next thing which
is a linear kind of setup Cal 1.0 this
is the svm regularization parameter
we're going to use models here's our svm
and let's go and scroll up just a notch
there we go now we so here we are with
our model we're going to create the the
setup with the SVC kernel is linear and
we'll come back to that cuz that's an
important uh setup in there as far as
what our kernel is and you'll you'll see
why that's so important here in a minute
because we're going to look at a couple
of these and so this one is we're
actually going to be changing how that
processes it uh and then our C here's
our one our 0.0 and then the rest of
this is uh plotting we're just adding
titles making the Contours so it's a
pretty graph you can actually spend this
would be a whole class just to do all
the cool things you can do with Scatter
Plots and regular plots and colors and
things like that in this case we're
going to create a graph with a nice
white line down the middle so that you
can see what's going on here and when we
do this you can see that as it as it
split the data uh the linear did just
that it drew a line through the data and
it says this is supposed to be blue and
this is supposed to be red and it
doesn't really fit very well that's
because we used a linear division of the
data and it's not very linear data on
this uh is anything but linear so when
we when we look at that it's like well
okay that didn't work what's the next
option well there's a lot of choices in
here one of them is just simply we can
change this from the kernel being linear
to poly and we'll just go back up here
and use the same chart U oops here we go
uh so here we go uh linear kernel we'll
change this to poly and then when we
come in here and create our model here's
our model up here linear we can actually
just go right in and change this to the
poly model and if you remember when we
go back over here to the SBC and let's
scroll back up here there's a lot of
different options oops even further okay
so when we come up here and we start
talking about the kernel here's the
kernel there's linear there's poly RBF
sigmoid precomputed there's a lot of
different ways to do this setup um and
their actual default is RBF very
important to note that uh so when you're
running these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the spvm the kernel is so important
you really need to know that and we
switched our kernel to poly and when we
run this uh you can see it's changed a
little bit we now have quite an
interesting looking diagram and you can
see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is uh we do a a polyfit uh you
can see if you have a split in data
where there's a group in the middle this
one kind of data and the groups on the
outside are different the polyfit or the
poly kernel is what's going to be fit
for that so uh if that doesn't work then
what are we going to use well uh they
have the RBF kernel and let's go ahead
and take a look and see what the RBF
looks like and uh let me go and turn
there we go turn my drawing off and the
RBF kernel oops there we go RBF and then
of course for our title it's always nice
to have it match with the RBF kernel and
we go ahe and run this and you can see
that the RBF kernel does a really good
job uh it actually has divided the data
up on here and this is the kind of what
you expect here was that ring here's our
inner ring and an outer ring of data and
so the RBF fits this data package quite
nicely um and that when we talk about
svm really is powerful in that it has
this kind of sorting feature to it in
its algorithms this is something that is
really hard to get the SK uh means to do
or the K means uh setup and so when you
start looking at these different machine
learning algorithms understanding your
data and how it's grouped is really
important it makes a huge difference as
far as what you're Computing and what
you're doing with it so that was a demo
on the support Vector uh certainly you
could have done continued on that and
done like a confusion Matrix and all
that kind of fun fun stuff to see how
good it was and split the data up to see
how it uh vectorizes the visual on that
so important it makes a big difference
just to see what it looks like and that
giant donut and why it it does Circle so
well or your poly version or your linear
version so we've looked at some very
numerical kind of uh setups where
there's a lot of math involved ukian
geometry um that kind of thing a totally
different machine learning algorithm for
approaching this is the decision trees
and there's also Forest that go with the
decision trees they're based on multiple
trees combined the decision tree is a
supervised learning algorithm used for
classification it creates a model that
predicts the value of a Target variable
by learning simple decision rules
inferred from the data features a
decision tree is a hierarchal tree
structure where an internal node
represents features or tribute the
branch represents a decision Rule and
each Leaf node represents the outcome
and you can see here where they have the
first one uh yes or no and then you go
either left or right and so forth one of
the coolest things about decision trees
um is and I'll see people actually run a
decision tree even though their final
model is different because the decision
tree allows you to see what's going on
you can actually look at it and say why
did you go right or left what was the
choice where's that break uh and that is
really nice if you're trying to share
that information with somebody else as
to why when you start getting into the
why this is happening decision trees are
very powerful so the topmost note of a
decision tree is known as the root node
it learns to partition on the basis of
the attribute value it partitions the
tree in a recursive manner so you have
your decision node if you get yes you go
down to the next Noe that's a decision
node and either yes you go to if it ends
on a leaf node then you know your answer
uh which is yes or no so there's your
there's your in classification set up on
there here's an example of a decision
tree that tells whether whether I'll
sleep or not at a particular evening
mine would be depending on whether I
have the news on or not do I need to
sleep no okay I'll work uh yes is it
raining outside yes I'll sleep no I'll
work so I guess if it's uh not raining
outside it's harder to fall asleep where
they have that nice uh rain coming in
and again this is really cool about a
decision tree is I can actually look at
it and go oh I like to sleep when it
rains outside so when you're looking at
all the data you can say oh this is
where the switch comes in when it rains
outside I'll sleep really good if it's
not raining or if I don't need sleep
then I'm not going to sleep I'm going to
go work so let's go ahead and take a
look at that that looks like in the code
just like we did before we go ahead and
open up the site kit setup just to say
you what the decision tree classifier
has you have your parameters which we'll
look a little bit more in depth at as we
write the code but it has uh different
ways of splitting it the strategy used
to choose a split at each node uh
Criterion Max depth remember the tree
how far down do you want it do you want
it to take up the space of your whole
computer with and and map every piece of
data or you know the smaller that number
is the smaller the level the tree is and
the less processing it takes but it's
also more General so you're less likely
to get as in-depth an answer um and then
of course minimal samples you need for
it to split samples for the leaf there's
a lot of things in here as far as what
how big the tree is and how to define it
and when do you define it and how to
weight it and they other different
attributes which you can dig deeper into
uh that can be very important if you
want to know the why of things uh and
then we go down here to our methods and
you'll see just like everything else we
have our fit method very important uh
and our predict uh the two main things
that we use what is what we're going to
predict our X to be equal to and we'll
go ahead and go up here and start
putting together the code uh we're going
to import our numpy our pandas there's
our confusion Matrix our train test
Split Decision tree classifier that's
the big one that we're actually working
with uh that's the line right here where
we're going to be oops decision tree
there it is decision tree classifier
that's the one I was looking for and of
course we want to know the accuracy and
the classification report on here and
we're going to do a little different
than we did in the other examples and
there's a reason for this let me go and
run this and load this up here uh we're
going to go ahead and build things on
functions and this is when you start
splitting up into a team this is a kind
of thing you start seeing a lot more
both in teams and for yourself because
you might want to swap one data to test
it on a different data depending on
what's going on uh so we're going to
have our import data here um the data
set length the balance and so forth um
this just returns balance data let me
just go ahead and print because I'm
curious as to what this looks like
import data and it's going to return the
balance data so if I run that uh if we
go ahead and print this out here and run
that you can see that we have a whole
bunch of data that comes in there and
some interesting setup on here has a
let's see BR RR I'm not sure exactly
what that represents on here uh 111 112
and so forth so we have a different set
of data here the shape is uh five
columns 1 2 3 4 five uh seems to have a
number at the beginning which I'm going
to guess uh B RL a letter I mean and
then a bunch of numbers in there 1111
let's see down here we got 555 uh set up
on this and let's see balance data and
since it said balance data I'm going to
guess that uh b means balanced R means
you need to move it right and L means
it's um needs to be moved left or skewed
to the left I'm not sure which one uh
let's go and close that out and we'll go
ahead and create a function to split the
data set uh X balance data equals data
values y balance equals data values of
zero there's that letter remember left
right right and balance then we're
looking for the values of 1 through five
and we go ahead and split it just like
you would XT train y train sat random
State 100 test size is.3 so we're taking
30% of the data and it's going to return
your X your y your y train your uh your
X train your X test your y train your um
y test again we do this because if
you're running a lot of these you might
want to switch how you split the data
and how you train it I tend to use a
bfold method I'll take a third of the
data and I'll train it on the other two
thirds and test it on that third and
then I'll switch it I'll switch which
third is a test data and then I can
actually take that information and
correlate it and it gives me a a really
uh robust package for figuring out what
the complete accuracy is uh but in this
case we're just going to go ahead this
is our function for splitting data and
this is where kind of gets interesting
because remember we were talking a
little bit about uh the different
settings in our model and so uh in here
we're going to create a decision tree
but we're going to use the Gen Genie
setup and where did that come from uh
what's the genie on here uh so if we go
back to the top of their page and we
have what uh Criterion are we going to
use we're going to use Genie they have
Genie and entropy those are the two main
ones that they use for the decision tree
uh so this one's going to be Genie and
if we're going to have a function that
creates the Genie model and it even goes
down here and here's our fit train of
the Genie model uh we'll probably also
want to create one for entropy sometimes
I even just um I might even make this
just one function with the different
setups and I know one of my one of the
things I worked on recently I had to
create a one that tested across multiple
models and so I would send the
parameters to the models or I would send
this part right here where it says
decision tree classifier that whole
thing might be what I send to create the
model and I know it's going to fit we're
going to have our xtrain and we're going
to have our predict and all that stuff
is the same so you can just send that
model to your function uh for testing
different models again this just gives
you one of the ways to do it and you can
see here we're going to change train
with the genie and we're also going to
chain train with the entropy to see how
that works and if you're going to have
your models going two separate models
you're sending there we'll go ahead and
create a prediction this simply is our y
predict equals our
uh whatever object we sent whatever
model we sent here the clf object and
predict against our X test and you can
see here print y predict and return y
predict set up on here we'll load that
definition up and then if you're going
to have a function that runs the predict
and print some things out uh we should
also have our accuracy function so
here's our calculate the accuracy what
are we sending we're sending our y test
data this could also be y actual and Y
predict and then we'll print out a
confusion Matrix uh then we'll print out
the accuracy of the U score on here and
print a report classification report
bundle it all together there so if we
bring this all together we have um all
the steps we've been working towards
which is importing our data by the ways
you'll spend 80% of your time importing
data in most machine learning setups and
cooking it and burning it and getting it
formatted so that it it uh works with
whatever models you're working with with
the decision tree has some cool features
in that if you're missing data it can
actually pick that up and just skip that
and says I don't know how to split this
there's no way of knowing whether it
rained or didn't rain last night so I'll
look at something else like whether you
watched uh TV after 8:00 you know that
blue screen thing uh so we have our
function importing our data set we bring
in the data we split the data so we have
our X test test and Y train and then we
have our different models our clf Genie
so it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy uh and
then once we have that we have our
function for making the prediction and
we have our function for calculating the
accuracy uh and then if we're going to
have that we should probably have our
main code involved here this probably
looks more familiar if you're depending
on what you're working on if you're
working on like a pie charm then you
would see this in throwing something up
real quick in jupyter Notebook uh so
here's our our main data import which
we've already defined uh we get our
split data we create our Genie we create
our entropy so there's our two models
going on here there's our two models so
these are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie and we want to go ahead
with the genie and print our um our
predictions why X test to the genie and
calculate the accuracy on here and then
we want to print the results using
entropy so this is just the same thing
coming down like we did with the genie
we're going to put out our y predict
entropy and our calculations so let's go
ahead and run that and just see what
this uh piece of code does uh we do have
like one of our data needs to be is
getting a warning on there there nothing
major because it's just a simple warning
probably an update of a new version's
coming out uh and so here we are we have
our data set it's got 625 you can
actually see an example of the data set
B meaning balanced I guess and here's
our five data points 1111 means it's
balanced it's skewed to the right with
1112 uh and so forth on here and then
we're going to go ahead and predict from
our prediction whether it's to the right
or to the left you can think of a
washing machine that's SK that's banging
on one side of the thing or maybe it's
an automated car where we're down the
middle of the road that's imbalance and
it starts going veering to the right so
we need to correct for it uh and when we
print out the confusion Matrix we have
three different variables r l and B so
we should three the three different
variables on here and you have as far as
whether it predicts in this case the
balance there's not a lot of balance
loads on here and didn't do a good job
guessing whether it's balanced or not
that's what I took from this first one
uh the second one I'm guessing is the
right so it did pretty good job guessing
the right balance you can see that a
bunch of them came up left unbalanced um
probably not good for an automated car
as it tells you 18 out of the 18 miss
things and tells you to go the wrong
direction and here we are going the
other way uh 19 to 71 and of course we
can back that up with an accuracy report
on here and you can see the Precision
how well the left and right balance is
79% 79% precision and so forth and then
we went and used the entropy and let me
just see if we can get so we can get
them both next to each other here's our
entropy of our um the first setup our
first model which is the Genie model
67181 19 71
6322
2070 pretty close the two models you
know that's not a huge difference in
numbers this second one of entropy did
slightly it looks like slightly worse
because it did one better as far as the
right balance and did what is this four
worse on the left balance or whatever uh
so slightly worse if I was guessing
between these I'd probably use the first
one they're so close though that
wouldn't be
it wouldn't be a Clear Choice as to
which one worked better and there's a
lot of numbers you can play with here
which might give better results
depending on what the data is going in
now uh one of the takeaways you should
have from the different category
routines we ran is that they run very
similar you you certainly change the
perimeters in them as to whether you're
using what model you're using and how
you're using it and what data they get
applied to but when you're talking about
the S kit learn package it justes such
an awesome some job of making it easy uh
you split your data up you train your
data and you run the prediction and then
you see what kind of accuracy what kind
confusion confusion Matrix It generates
so um we talk about algorithm selection
logistic regression K nearest neighbors
uh logistic regression is used when we
have a binomial outcome for example to
predict whether an email spam or not
whether the tumor is malignant or not
the logistic regression works really
good on that you can do it in a k
nearest neighbors also the question is
which one will it work better in um I
find the logistic regression models work
really good in a lot of raw numbers so
if you're working with say the stock
market is this a good investment or a
bad investment um so that's one of the
things it handles the numbers better K
nearest neighbors are used in scenarios
where
nonparametric no fixed number of
perimeters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection
uh so K means really good in finding the
patterns um I've seen that as a
pre-processor to a lot of other
processors where you use the K nearest
neighbors to figure out what data groups
together very powerful package support
Vector machines uh support Vector
machines are used whenever the data has
higher Dimensions the human genome
microarray svms are extensively used in
the hard handwriting recognition models
and you can see that we were able to
switch between the parabolic and the
circular setup on there where you can
now have that donnut kind of data and be
able to filter that out with the support
Vector machine and then decision trees
are mostly used in operational
researches specifically decision
analysis to help identify a strategy
most likely to reach any goal they are
pre preferred where the model is easy to
understand I like that last one it's a
good description is it easy to
understand so you have data coming in
when am I going going to go to bed you
know is it raining outside you can go
back and actually look at the pieces and
see those different decision modes takes
a little bit more to dig in there and
figure out what they're doing uh but you
can do that and you can actually help
you figure out why um people love it for
the why Factor so uh strings and
limitations big one on all of these the
strengths and limitations we talk about
logistic regressions uh the strings are
it is easy to implement and efficient to
train it is relatively easy to
regularize the data points remember how
we put everything between zero and one
when you look at logistic regression
models uh you don't have to worry about
that as much limitations it has a high
Reliance on proper representation of
data it can only predict a categorical
outcome with the K nearest neighbors it
doesn't need a separate training period
new data can be added seamlessly without
affecting the accuracy of the model uh
kind of an interesting thing because you
can do partial training uh that can
become huge if you're running across
really large data sets or the data is
coming in you can continually uh do a
partial fit on the data with the K
nearest neighbors and continue to adjust
that data uh it doesn't doesn't work on
high dimensional and large data sets we
were looking at the breast cancer uh 36
different features what happens when you
have 127 features or a million features
and you say well what do you have a
million features in well if I was
analyzing uh logic um the legal
documents I might have a tokenizer that
splits a words up to be analyzed and
that tokenizer might create 1 million
different words available that might be
in the document for doing weights uh
sensitive to noisy data outliers and
missing values that's a huge one with K
nearest neighbors they really don't know
what to do with a missing value how do
you compute the the distance if you
don't know what the value is uh the svm
uh Works more efficiently on
high-dimensional data it is relatively
memory efficient so it's able to create
those planes with only a few different
variables in there as opposed to having
to store a lot of data for different uh
features and things like that it's not
suitable for a large data sets uh the
svm you start running this over
gigabytes of data causes some huge
issues underperforms if the data has
noise or overlapping that's a big one we
were looking at that where the SPM
splits it and it creates a soft buffer
but what happens when you have a lot of
stuff in the middle metal uh that's hard
to sort out and doesn't know what to do
with that causes SPM to start crashing
or not perform as well decision trees
handles non-linear perimeters and
missing values efficiently the missing
values is huge uh I've seen this in uh
was it the wine tasting data sets where
they have three different data sets and
they share certain features uh but then
each one has some features that aren't
in the other ones and it has to figure
out how to handle those well the
decision tree does that automatically in
instead of having to figure a way to
fill that data in before processing like
you would with the other models uh it's
easy to understand and has less training
period so it trains pretty quickly uh
comes up there and just keeps forking
the tree down and moving the parts
around and so it doesn't have to go
through the data multiple times guessing
and adjusting it just creates the tree
as it goes overfitting and high variants
are the most annoying part of it that's
that's an understatement uh that has to
do with how many leavs and how many
decisions you have it do the more you
have the more overfit it is to the data
it also uh just in making the choices
and how the choices come in it might
overfit to a specific feature because
that's where it started at and that's
what it knows and it really um is
challenged with large data sets they've
been working on that with the data
Forest but it's not suitable for large
data sets it's really something you'd
probably run on a single machine and not
across um not across a uh data pool or
anything if you are an aspiring data
scientist who who is looking out for
online training and certification in
data science from the best universities
and Industry experts then search no more
simply learns postgraduate program in
data science from calch University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below so the decision
tree one of the many powerful tools in
the machine learning library begins with
a problem I think I have to buy a car so
it making this question you want to know
how do I decide which one to buy and
you're going to start asking questions
is a mileage greater than 20 is a price
less than 15 will it be sufficient for
six people does it have enough airbag
anti-lock brakes all these questions
come up then as we feed all this data in
we make a decision and that decision
comes up oh hey this seems like a good
idea here's a car so as we go through
this decision process using a decision
tree we're going to explore this maybe
not in buying a car but in how to
process data what's in it for you let's
start by finding out what is machine
learning and why we even want to know
about it for processing our data and
we'll go into the three basic types of
machine learning and the problems that
are used by Machine learning to solve
finally we'll get into what is a
decision tree what are the problems a
decision tree solves what are the
advantages and disadvantages of using a
decision tree and then we want to dig in
a little deep into the mechanics how
does the decision tree work and then
we'll go in and do a case loan repayment
prediction where we actually going to
put together some python code and show
you the basic python code for generating
a decision tree what is machine learning
there are so many different ways to
describe what is machine learning in
today's world and illustrate it we're
going to take a graphic here and uh
making decisions or trying to understand
what's going on and really underlying
machine learning is people want to wish
they were smarter wish we could
understand the world better so you can
see a guy here who's uh saying hey how
can I understand the world better and
someone comes up and says let's use
artificial intelligence machine learning
is a part of artificial intelligence and
now he's get a big smile on his face cuz
now he has artificial intelligence to
help him make his decisions uh and they
can think in new ways so this brings in
new ideas so what is machine learning
this is a wonderful graph here you can
see where we have learn predict decide
these are the most basic three premises
of machine learning in learning we can
describe the data in new ways and able
to learn new aspects about what we're
looking at and then we can use that to
predict things and we can use that to
make decisions so maybe it's something
that's never happened before but we can
make a good guess whether it's going to
be a good investment or not it also
helps us categorize stuff so we can
remember it better so it's easier to
pull it out of the catalog we can
analyze data in new ways we never
thought possible and then of course
there's the uh very large growing
industry of recognize we can do facial
recognition driver recognition automated
car recognition all these are part of
machine learning going back to our guy
here who's in his ordinary system and
would like to be smarter make better
choices what happens with machine
learning is an application of artificial
intelligence wherein the system gets the
ability to automatically learn and
improved based on experience so this is
exciting because you have your ordinary
guy who now has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you'd already have a
list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learning tool and that lets you
predict on the next person whether
they're going to be able to make their
payments are not on their loan if you
have one category where you already know
the answers the next one would be you
don't know the answers you just have a
lot of information coming in
unsupervised learning allows you to
group liked information together so if
you're analyzing photos it might group
all the images of trees together and all
the images of houses together without
ever knowing what a house or a tree is
which leads us to the third type of
machine learning the third type of
machine learning is reinforcement
learning unlike supervised or
unsupervised learning you don't have the
data prior to starting so you get the
data one line at a time and then whether
you make a good choice or a bad choice
the machine learning tool has to then
adjust accordingly so you get a plus or
minus feedback you can liken this to the
way a human learns we experience life
one minute at a time and we learn from
that and either our memories is good or
we learn to avoid something problems in
machine learning to understand where the
decision tree fits into our machine
learning tools we have to understand the
basics of some some of the machine
learning problems and three of the
primary ones fall underneath
classification problems with categorical
Solutions like yes or no true or false
one or zero this might be does it belong
to a particular group yes or no then we
have regression problems where there's a
continuous value needs to be predicted
like product prices profit and you can
see here this is a very simple linear
graph uh you can guess what the next
value is based on the first four it kind
of follows a straight line going up and
in this is problems whereing the data
needs to be organized to find specific
patterns like in the case of product
recommendation they group all the
different products that people just like
you viewed on a shopping site and say
people who bought this also bought this
the most commonly used for the decision
tree is for classification for figuring
out is it red or is it not is it a fruit
or is it a vegetable yes or no true
false left or right 01 and so when we
talk about classification we're going to
look at the basic machine Lear learning
these are the four main tools used in
classification there's a Nave Bay
logistic regression decision tree and
random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or a curve
through the data they work Wonderful in
a lot of problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest but
today we're just going to focus on the
decision
tree what is a decision tree let's go
through a very simple example before we
dig in deep decision tree is a
tree-shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction let's start with
a simple question how do identify a
random vegetable from a shopping bag so
we have this group of vegetables in here
and we can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
it's going to be a what looks to be a
red chili and if it's true it's going to
be a bell pepper from the capsicum
family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each each split is made based on the sum
of squared error before we dig deeper
into the mechanics of the decision tree
let's take a look at the advantages of
using a decision tree and we'll also
take a glimpse at the disadvantages the
first thing you'll notice is that it's
simple to understand interpret and
visualize it really shines here because
you can see exactly what's going on in a
decision tree little effort is required
for data preparation so you don't have
to do special scaling there's a lot of
things you don't have to worry about
when using a decision tree it can handle
both numerical and categorical data as
we discovered earlier and nonlinear
parameters don't affect its performance
so even if the data doesn't fit an easy
curved graph you can still use it to
create an effective decision or
prediction if we're going to look at the
advantages of a decision tree we also
need to understand the disadvantages of
a decision tree the first disadvantage
is overfitting overfitting occurs when
the algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is information
gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
uh deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went to
the different parts of our tree and they
call the leaf node Leaf node carries the
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with the gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
inter is high in this case so let's look
at a training set or a training data set
and we're looking at color we're looking
at height and then we have our different
animals we have our elephants our
giraffes our monkeys and our tigers and
they're of different colors and shapes
let's see what that looks like and how
do we split the data we have to frame
the conditions that split the data in
such a way that the Information Gain is
the highest note gain is the measure of
decrease in entropy after splitting so
the formula for entropy is the sum
that's what this symbol looks like that
looks like kind of like a uh e funky e
of K where I equal 1 to k k would
represent the number of animal the
different animals in there where value
or P value of I would be the percentage
of that animal times the log base 2 of
the same the percentage of that animal
let's try to calculate the entropy for
the current data set and take a look at
what that looks like and don't be afraid
of the math you don't really have to
memorize this math just be aware that
it's there and this is what's going on
in the background and so we have three
giraffes two tigers one monkey two
elephants a total of eight animals
gathered and if we plug that into the
formula we get an entropy that equals 3
over 8 so we have three giraffes a total
of 8 times the log usually they use base
two on the log so log base 2 of 3 over 8
plus in this case let's say it's the
elephants 2 over 8 2 elephants over
total of 8 * log base 2 2 over 8 plus
one monkey over total of 8 log base 2 1
over 8 and plus 2 over 8 of the Tigers
log base 2 over 8 and if we plug that
into our computer our calculator I
obviously can't do logs in my head we
get an entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we'll try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the conditions that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decid to split both
the nodes using height as a condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with
100% accuracy that was easy use case
loan repayment prediction let's get into
my favorite part and open up some Python
and see what the programming code and
the scripting looks like in here we're
going to want to do a prediction and we
start with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and run it in so let's flip on
over and here I'm using the anac Honda
Jupiter notebook now you can use any
python IDE you like to run it in but I
find the Jupiter notebook's really nice
for doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with uh mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spread sheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this this very lengthy setup
on here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn so let's just go
ahead and take a look at this and use
this as a uh opportunity to understand
what's going on here if you read the
warning it says the cross validation is
depreciated so it's a warning on it's
being removed and it's going to be moved
in favor of the model selection so if we
go up here we have sklearn Doc
crossvalidation and if you research this
and go to sklearn site you'll find out
that you can actually just swap it right
in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 4
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet sheet so you can see
what that looks like and here I've
opened it up in the open Office calc
which is pretty much the same as um
Excel and zoomed in and you can see
we've got our columns and our rows of
data little easier to read in here we
have a result yes yes no we have initial
payment last payment credit score house
number if we scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
one 1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um to too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lin function from Python and when
I run this you'll see that it's a th
long and that's what we expected there's
a thousand lines of data in there if you
subtract the column head and this is one
of the nice things when we did the uh
balance data from the panda read CSV
you'll see that the header is row zero
so it automatically removes the
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about the shape is not
only does it give me the length of the
data we have a th lines it also tells me
there's five columns so we were looking
at the data we had five columns of data
and then let's take one more step to
explore the data using Python and now
that we've taken a look at the length
and the shape let's go ahead and use the
uh pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data doad and this is a
Panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
our print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as the head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all grunch together and hard to read you
can now get a nice open view of what
you're working with we're working with a
shape of a th000 long five wide so we
have five columns and we do the full
datae you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember we always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data
and we have to know in the end what the
Y was supposed to be and that's where
this train test split comes in that we
loaded earlier in the modules this does
it all for us and you can see they set
the test size equal to .3 so it's
roughly 30% will be used in the test and
then we use a random state so it's
completely random which rows it takes
out of there and then finally we get to
actually build our decision tree and
they've called it here clf entropy
that's the actual decision tree or
decision tree classifier and in here
they've added a couple variables which
we'll explore in just a minute and then
finally we need to fit the data to that
so we take our clf entropy that we
created and we fit the X train and since
we know the answers for X train are the
Y train we go aad and put those in and
let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf inop equal decision
tree classifier it automatically prints
out what's in that decision tree there's
a lot of very Ables you can play with in
here and it's quite beyond the scope of
this tutorial to go through all of these
and how they work but we're working on
entropy that's one of the options we've
added that it's completely a random
state of 100 so 100% And we have a max
depth of three now the max depth if you
remember above when we were doing the
different graphs of animals means it's
only going to go down three layers
before it stops and then we have minimal
samples of leavs is five so it's going
to have at least five leavs at the end
so I'll have at least three splits or
have no more than three layers and at
least five end leaves with the final
result at the bottom now that we've
created our decision tree classifier not
only created it but trained it let's go
ahead and apply it and see what that
looks like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here and before we run it let's just
take a quick look at what it's doing
here we have a variable y predict that
we're going to do
and we're going to use our variable clf
entropy that we
created and then you'll see do predict
and that's very common in the SK learn
modules that their different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
would be the new loans you're putting in
here to guess whether person's going to
be uh pay them back or not in this case
so we need to test out the data and just
see how good our sample is how good of
our tree does at predicting the loan
payments and finally since Anaconda
jupyter notebook is works as a command
line for python we can simply put the Y
predict e in to print it I could just as
easily have put the
print and put brackets around y predict
to print it out we'll go ahead and do
that it doesn't matter which way you do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th000 so he
should have about 300 answers in here
and this tells you which each one of
those lines of our uh test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from sklearn metrics import
accuracy score and you could probably
run a script make your own script to do
this very easily how accurate is it how
many out of 300 do we get right and so
we put in our y test that's the one we
ran the predict on and then we put in
our y predict that's the answers we got
and we're just going to multiply that by
100 because this is just going to give
us an answer as a decimal and we want to
see it as a percentage and let's run
that and see what it looks like and if
you see here we got an accuracy of 93.
66667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not so we've had
a lot of fun learning about decision
trees so let's take a look at the key
takeaways that we've covered today what
is machine learning we covered up some
different as aspects of machine learning
and what that is utilized in your
everyday life and what you can use it
for for predicting for describing for
guessing what the next outcome is for
storing information we looked at the
three main types of machine learning
supervised learning unsupervised
learning and reinforced learning we
looked at problems in machine learning
and what it solves classification
regression and clustering finally we
went through how does the decision tree
work where we looked at the hunter he's
trying to sort out the different animals
and what kind of anal animal AR and then
we rolled up our sleeves and did our
Python cating and actually applied it to
a data set for random Forest currently
today it's used in remote sensing uh for
example they're used in the etm devices
if you're a space buff that's the
enhanced thermatic mapper they use on
satellites which see uh far outside the
human Spectrum for looking at land
masses and they acquire images of the
Earth's surface the accuracy is higher
and training time is less than many
other machine learning tools out there
also object detection multiclass object
detection is done using random Forest
algorithms a good example is a traffic
where you're trying to sort out the
different cars buses and things and it
provides a better detection in
complicated environments they very
complicated up there and then we have uh
another example connect and let's take a
little closer look at connect connect
they use a random Forest as part of the
game console and what it does is it
tracks the body movements and it
recreates it in the game and let's see
see what that looks like uh we have a
user who performs a step in this case it
looks like Elvis Presley going there
that is then recorded So the connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have uh prints going on this one
from Elvis Presley to Prince it's great
uh so it marks user baseed on the
accuracy if we look at that a little
closer we have a training set to
identify body parts where are the hands
where are the feet uh what's going on
with the body that then goes into a
random Forest classifier that learns
from it once we've trained the
classifier it then identifies the body
parts while the person's dancing it's
able to represent that in a computer
format and then based on that it scores
the game and how accurate you are as
being Elvis Presley or prince in your
dancing let's take an overview of what
we're going to cover today what's in it
for you we're going to start with is
what is machine learning we're not going
to going to go into detail on that we're
going to specifically look how the
random Force fits in the machine
learning hierarchy then we're going to
look at some applications of random
Forest what is classification which is
its primary use why use random Forest
what's the benefits of it and how does
it actually come together what is random
forest and then we'll get into random
forest and the decision tree how all
that's like the final step and how it
works and finally we'll get some python
code in there and we'll use the case the
iris flower anal is now if you don't
know what any of these terms mean or
where we're going with this don't worry
we're going to cover all the basics and
have you up and running and even having
doing some basic script in Python by the
end let's take a closer look at types of
machine learning specifically we're
going to look at where the decision tree
fits in with the different machine
learning packages out there we'll start
with the basic types of machine learning
there's supervised learning where you
have lots of data and you're able to
train your models there's unsupervised
learning where it has to look at the
data and then divide it based on its own
algorithms without having any training
and then there's reinforcement learning
where you get a plus or negative if you
have the answer correct this particular
tool belongs to the supervised learning
let's take a closer look at that what
that means in supervised learning uh
supervised learning falls into two
groups classification and regression
we'll talk about regression a little
later and how that differs this
particular format goes underneath
classification so we're looking at
supervised learning and classification
in the machine learning tools
classification is a kind of problem
wherein the outputs are categorical in
nature like yes or no true or false or
zero or one in that particular framework
there's the KNN where the NN stands for
nearest neighbor Nave Baye the decision
tree which is part of the random Forest
that we're studying today so why random
Forest It's always important to
understand why we use this tool over the
other ones what are the benefits here
here and so with the random Forest the
first one is there's no overfitting if
you use of multiple trees reduce the
risk of overfitting training time is
less overfitting means that we have fit
the data so close to what we have as our
sample that we pick up on all the weird
parts and instead of predicting the
overall data you're predicting the weird
stuff which you don't want high accuracy
runs efficiently in large database for
large data it produces highly accurate
prediction
in today's world of uh Big Data this is
really important and this is probably
where it really shines this is where why
random Forest really comes in it
estimates missing data data in today's
world is very messy so when you have a
random Forest it can maintain the
accuracy when a large proportion of the
data is missing what that means is if
you have data that comes in from five or
six different areas and maybe they took
one set of Statistics in one area and
they took a slightly different set of
Statistics in the other so they they
have some of the sh same shared data but
one is missing like the uh number of
children in the house if you're doing
something over demographics and the
other one is missing the size of the
house it will look at both of those
separately and build two different trees
and then it can do a very good job of
guessing which one fits better even
though it's missing that data let us dig
deep into the theory of exactly how it
works and let's look at what is random
for us random for us or random decision
Forest is a method that operates by
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and let's uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree too where we have
that picture of the fruit goes in and
this one decides that it's a limit and
the decision 3 tree gets another image
and it decides it's an apple and then
this all comes together in what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision tree and looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagrammed used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes hm
orange or red well if it's true then it
comes out as the orange and if it's
false that leaves apples so in this
example it sorts out the fruit in the
bowl or the images of the fruit a
decision tree these are very important
terms to know because these are very
Central to understanding the decision
tree and when working with them the
first is entropy everything on the
decision tree and how it makes those
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set uh then
they also have Information Gain the leaf
node the decision node and the root node
we'll cover these other four terms as we
go down the tree but let's start with
entropy so starting with entropy we have
here a high amount of Random
what that means is that whatever is
coming out of this decision if it was
going to guess based on this data it
wouldn't be able to tell you whether
it's a lemon or an apple it would just
say it's a fruit uh so the first thing
we want to do is we want to split this
apart and we take the initial data set
we're going to S create a data set one
and a data set two we just split it in
two and if you look at these new data
sets after splitting them the entropy of
each of those sets is much less so for
the first one whatever comes in there
it's going to sort that data and it's
going to say okay if this data goes this
direction it's probably an apple and if
it goes into the other direction it's
probably a lemon so that brings us up to
Information Gain it is the measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that information gain is increased
greatly in this example and so you can
find that the information grain simply
equals uh decision E1 minus E2 as we're
going down our list of uh definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision so we look down here to the
leaf node we finally get to our set one
or our set two when it comes down there
and it says okay this object's gone into
set one if it's gone into set one it's
going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right it either be an apple
or lemons those Leaf nodes or those
final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more branches and you
can see here where we have the uh five
apples and one lemon and in the other
case the five lemons and one apple they
have to make a choice of which tree It
Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information so far we've looked at a
very general image um with the fruit
being split let's look and see exactly
what that means to split the data and
how do we make those decisions on there
uh let's go in there and find out how
does a decision tree work so let's try
to understand this and let's use a
simple example and we'll stay with the
fruit we have a bowl of fruit and so
let's create a problem statement and the
problem is we want to classify the
different types of fruits in the bowl
based on different features the data set
in the bowl is looking quite messy and
the entropy is high in this case so if
this bow was our decision maker it would
know what choice to make it has so many
choices which one do you pick Apple
grapes or lemons and so we look in here
we're going to start with a dra a
training set so this is our data that
we're training our data with and we have
a number of options here we have the
color and under the color we have red
yellow purple uh we have a diameter uh
331 331 and we have a label Apple lemon
Grapes apple lemon grapes and how do we
split the data we have to frame the
conditions to split the data in such a
way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call call it entropy there's our
entropy after splitting and now we'll
try to choose a condition that gives us
the highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 2 3 four 3es
that's a pretty high selection so let's
say the condition gives us the maximum
gain of three so we have the most pieces
fall into that range so our first split
from our decision node is we split the
data based on the diameter is it greater
than or equal to three if it's not
that's false it goes into the great bowl
and if it's true it goes into a bowl
fold of lemon and apples the entropy
after splitting has decreased
considerably so now we can make two
decisions if you look at there very uh
much less chaos going on there this node
has already attained an entropy value of
zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
in our choices of color and diameter too
and if the color is yellow it's going to
go to the right bowl and if it's false
it's going to go to the left Bowl so the
entropy in this case is now zero so now
we have three bowls with zero entropy
there's only one type of data in each
one of those bowls so we can predict a
lemon with 100% accuracy and we can
predict the Apple also with 100%
accuracy along with our grapes up there
so we've looked at kind of a basic tree
in our forest but what we really want to
know is how does a random Forest work as
a whole so to begin our um random Forest
classifier let's say we already have
built three trees and we're going to
start with the first tree that looks
like this just like we did in the
example this tree looks at the diameter
if it's greater than or equal to three
it's true otherwise it's false so one
side goes to the smaller diameter one
side goes to larger diameter and if the
color is orange Orange it's going to go
to the right true we're using oranges
now instead of lemons and if it's red
it's going to go to the left false and
we build a second tree very similar but
split differently instead of the first
one being split by a diameter uh this
one when they created it if you look at
that first Bowl it has a lot of red
objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false false
or true and so on and so on and tree
three is the diameter equal to one and
it came up with this because there's a
lot of cherries in this bowl so that
would be the biggest split on there is
is the diameter equal to one that's
going to drop the entropy the quickest
and as you can see it splits it into
true if it goes false and they've added
another category does it grow in the
summer and if it's false it goes off to
left if it's true it goes off to the
right let's go ahead and bring these
three trees so you can see them all in
one image so this would be three
completely different trees categorizing
a fruit and let's take a fruit now let's
try this and this fruit if you look at
it we've blackened it out you can't see
the color on it so it's missing data
remember one of the things we talked
about earlier is that a random Forest
works really good if you're missing data
if you're missing pieces so this fruit
has an image but maybe a person had a
black and white camera when they took
the picture and we're going to take a
look at this and it's going to have um
they put the color in there so ignore
theor color down there but the diameter
equals 3 we find out it grows in the
summer equals yes and the shape is a
circle and if you go to the right you
can look at what one of the decision
trees did this is the third one is a
diameter greater than equal to three is
a color orange well it doesn't really
know on this one but it if you look at
the value it say true and it go to the
right tree 2 classifies it as cherries
is a color equal red is the shape a
circle true it is a circle so this would
look at it and say oh that's a cherry
and then we go to the other classifier
and it says is the diameter equal one
well that's false does it grow in the
summer true so it goes down and looks at
as oranges so how does this random
Forest work the first one says it's an
orange the second one said it was a
cherry and the third one says H it's an
orange and you can guess that if you
have two oranges and one says it's a
cherry uh when you add that all together
the majority of the vote says orange so
the answer is it's classified as an an
orange even though we didn't know the
color and we're missing data on it I
don't know about you but I'm getting
tired of fruit so let's switch and I did
promise you we'd start looking at a case
example and get into some python coding
today we're going to use the case the
iris flower
analysis o this is the exciting part as
we roll up our sleeves and actually look
at some python coating before we start
the python coating we need to go ahead
and create a problem statement wonder
what species of Iris do these flowers
belong to let's try to predict the
species of the flowers using machine
learning in Python let's see how it can
be done so here we begin to go ahead and
Implement our python code and you'll
find that the first half of our
implementation is all about organizing
and exploring the data coming in let's
go ahead and take this first step which
is loading the different modules into
Python and let's go ahead and put that
in our favorite editor whatever your
favorite editor is in this case I'm
going to be using the Anaconda Jupiter
notebook which is one of my favorites
certainly there's notepad++ and eclipse
and dozens of others or just even using
the python terminal window any of those
will work just fine to go ahead and
explore this python coding so here we go
let's go ahead and flip over to our
Jupiter notebook and I've already opened
up a new page for Python 3 code and I'm
just going to paste this right in there
and let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the SK learn.
dat sets import load Iris now this isn't
the actual data this is just the module
that allows us to bring in the data the
load Iris and the iris is so popular
it's been around since 1936 when Ronald
Fischer published a paper on it and
they're measuring the different parts of
the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import a random forest
classifier from the sklearn module so SK
learn. ensemble import random force
classifier and then we want to bring in
two more modules um and these are
probably the most commonly used modules
in Python and data science with any of
the um other modules that we bring in
and one is going to be pandas we're
going to import pandas as PD PD is the
common term used for pandas and pandas
is basically creates a data format for
us where when you create a pandas data
frame it looks like an Excel spreadsheet
and you'll see that in a minute when we
start digging deeper into the code panda
is just wonderful because it plays nice
with all the other modules in there and
then we have numpy which is our numbers
Python and the numbers python allows us
to do different mathematical sets on
here we'll see right off the bat we're
going to take our NP and we're going to
go ahead and Seed the randomness with it
with zero so np. random. seed is seeding
that as zero this code doesn't actually
show anything we're going to go ahead
and run it cuz I need to make sure I
have all those loaded and then let's
take a look look at the next module on
here the next six slides including this
one are all about exploring the data
remember I told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our Jupiter notebook and
here we go we've gone ahead and uh run
the import and I'm going to paste the
code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells it how to
get the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so it's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what what's going on
here the first thing you want to notice
is that our DF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. data down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
all of Iris and see what that looks like
so when I print oliv virus I get this
long list of information and you can
scroll through here and see all the
different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of seel length seel
width pedal length petal width these are
the different names they have for the
four different columns and if you scroll
down far enough you'll also see data
down here oh goodness it came up right
towards the top and uh data is equal to
the different data we're looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go ahead and set
that back to the head and this is one of
the neat features of pandas and Panda
data frames is when you do DF doad or
the panda datf frame. head it'll print
the first five lines of the data set in
there along with the headers if you have
them in this case we have the column
headers set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero 1 2 3 4 not 1 2 3 4 5
so now we've got our Iris data imported
into a data frame let's take a look at
the next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have a DF
species
and this is interesting cuz right here
you'll see where I have DF species in
Brackets which is uh the key code for
creating another column and here we have
iris. Target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are um acceptable let's go ahead
and run this code and see how this
changes and what we've done is is we've
added the target from the iris data set
as another column on the
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what aosa is or a verac
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here and we're going to
run done that and let's talk a little
bit about what we're doing now we're
exploring data and one of the challenges
is knowing how good your model is did
your model work and to do this we need
to split the data and we split it into
two different parts they usually call it
the training and the testing and so in
here we're going to go ahead and put
that in our database so you can see it
clearly and we've set it DF remember you
can put brackets this is creating
another column is train so we're going
to use part of it for training and this
equals NP remember that stands for numpy
random. uniform so we're generating a
random number between zero and one and
we're going to do it for each of the
rows that's where the length DF comes
from so each row gets a generated number
and if it's less than 75 it's true and
if it's greater than 75 it's false this
means we're going to take 75% of the
data roughly because there's a
Randomness involved and we're going to
use that to train it and then the other
25% we're going to hold off to the side
and use that to test it later on so
let's flip back on over and see what the
next step is so now that we've labeled
our database for which is training and
which is testing let's go ahead and sort
that into two different variables train
and test and let's take this code and
let's bring it into our project and here
we go let's paste it on down here and
before I run this let's just take a
quick look at what's going on here is we
have up above we created remember
there's our def. head which prints the
first five rows and we've added a column
is train at the end and so we're going
to take that we're going to create two
variables we're going to create two new
data frames one's called train one's
called test 75% in train 25% in test and
then to sort that out we're going to do
that by doing DF or M original data
frame with the iris data in it and if DF
is train equals true
it's going to go in the train and if DF
is train equals false it goes in the
test and so when I run this we're going
to print out the number in each one
let's see what that looks like and
you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75% in one and
25% for us to test our model on
afterward so let's jump back to our code
and see where this goes in the next two
steps we want to do one more thing with
our data and that's make it readable to
humans um I don't know about you but I
hate looking at zeros and ones so let's
start with the features and let's go
ahead and take those and make those
readable to humans and let's put that in
our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sea length sea width
pedal length and pedal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first four one I
want you to notice here is that when
you're in a command line whether it's
Jupiter notebook or you're running
command line in the uh terminal window
if you just put the name of it it'll
print it out this is the same as doing
print
features and the Shand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we set y equal to
pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is Factor doing I'm going to come
back to that in just a second let's look
at what train species is and why we're
looking at the group zero on there and
let's go up here and here is our
species remember this on that we created
this whole column here for species and
then it has Sosa Sosa Sosa Sosa and if
you scroll down enough you'd also see
virginica and Vera color we need to
convert that into something the computer
understands zeros and ones so the train
species of zero because this is in the
format of a of an array of arrays so you
have to have the zero on the end and
then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work work to get to two
lines of code but there is a lot in
these two lines of code so let's take a
look and see what's going on here and
put this into our full script that we're
running and let's paste this in here and
let's take a look and see what this is
we have we're creating a variable CF and
we're going to set this equal to the
random forest classifier and we're
passing two variables in here and
there's a lot of variables you can play
with as far as these two are concerned
they're very standard in jobs all that
does is to prioritize it not something
to really worry about usually when
you're doing this on your own computer
you do in jobs equals 2 if you're
working in a larger or big data and you
need to prioritize it differently this
is what that number does is it changes
your priorities and how it's going to
run across the system and things like
that and then the random state is just
how it starts zero is fine for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more
cf. fit so we're fitting we're training
it we are actually creating our random
Forest classifier right here this is the
code that does everything and we're
going to take our training set remember
we kept our test off to the side and
we're going to take our training set
with the features and then we're going
to go ahead and put that in and here's
our Target the Y so the Y is 0 1 and two
that we just created and the features is
the actual data going in that we put
into the training set let's go ahead and
run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals 2 you can see the random State
equals Zer those are the two that we
sent in there you would really have to
dig deep to find out all these different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like Max features
is auto so all the features we're
putting in there it's just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process and leaf nodes that's
the end nodes remember we had the fruit
and we're talking about the leaf nodes
like I said there's a lot in this we're
looking at a lot of stuff here so you
might have uh in this case there's
probably only think three leaf nodes
maybe four you might have thousands of
leaf nodes at which point you do need to
put a cap on that and say okay it can
only go so far and then we're going to
use all of our resources on processing
this and that really is what most of
these are about is limiting the process
and making sure we don't uh overwhelm a
system and there's some other settings
in here again we're not going to go over
all of them warm start equals false warm
start is if you're programming it one
piece at a time externally since we're
not we're not going to have like we're
not going to continually to train this
particular Learning Tree and again like
I said there's a lot of things in here
that you'll want to look up more detail
from the
sklearn and if you're digging in deep
and running a major project on here for
today though all we need to do is fit
our train our features and our Target
why so now we have our training model
what's next if we're going to create a
model we now need to test it remember we
set aside the test feature test group
25% of the data so let's go ahead and
take this code and let's put it into our
uh script and see what that looks like
okay here we go and we're going to run
this and it's going to come out with a
bunch of zeros ones and twos which
represents the three type of flowers the
Sosa the virginica and the Versa color
and what we're putting into our predict
is the test features and I always kind
of like to know what it is I am looking
at so real quick we're going to do
test features and remember features is
an
array of SEO length SEO width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this case we're going to go ahead and
put those into our predict so we're
going to put each one of these lines of
data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 00001 2
11222 and and uh again this is the
flower type satos of virginica and Versa
color so now that we've taken our test
features let's explore that let's see
exactly what that data means to us so
the first thing we can do with our
predicts is we can actually generate a
different prediction model when I say
different we're going to view it
differently it's not that the data
itself is different so let's take this
next piece of code and put it into our
script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means so when
we do the predict underscore proba for
probability it generates three numbers
so we had three leaf nodes at the end
and if you remember from all the theory
we did this is the predictors the first
one is predicting a one for satsa it
predicts a zero for virginica and it
predicts a zero for versacolor and so on
and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets 0.5 votes
versal gets 0.5 vs but let's just go
with the virginica since these two are
equal and so on and so on down the list
you can see how they vary on here so now
we've looked at both how to do a basic
predict of the features and we've looked
at the predict probability let's see
what's next on here so now we want to go
ahead and start mapping names for the
plants we want to attach names so that
it makes a little more sense for us and
that's what we're going to do in these
next two steps we're going to start by
setting up our our predictions and
mapping them to the name so let's see
what that looks like and let's go ahead
and paste that code in here and run it
and this goes along with the next piece
of code so we'll skip through this
quickly and then come back to it a
little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we' been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're going
ahead and run it again cf. predict test
features remember that Returns the zeros
the ones and the twos and then we're
going to set that equal to predictions
so this time we're actually putting it
in a variable and when I run
this it distributes and it comes out as
an array and the array is satsa satsa
stosa stosa satsa we're only looking at
the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts mapping it to all the
different flower types the Versa col and
the virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and went ahead and did uh
predictions through 25 let's just do
five and so we have cetosis cetosis
ptosis cetosis that's what we're
predicting from our test model and then
we come down here and we look at test
species I remember I could have just
done test. species. head and you'll see
it says Sosa Sosa Sosa Sosa and they
match so the first one is what our
forest is doing and the second one is
what the actual data is now is we need
to combine these so that we can
understand what that means we need to
know how good our forest is how good it
is at predicting the features so that's
where we come up to the next step which
is lots of fun we're going to use a
single line of code to combine our
predictions and our actuals so we have a
nice chart to look at and let's go ahead
and put that in our script in our
Jupiter notebook here let's see let's go
ahead and paste that in and then I'm
going to because I'm on the Jupiter
notebook I can do a control minus so you
can see the whole line there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
Sosa versus color vinica and the actual
species setosa Versa color vinica and so
the the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have Sosa where they meet you have
verol where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ided by 32 and if we multiply by
100 we can say that it is 93% accurate
so we have a 93% accuracy with our model
I did want to add one more quick thing
in here on our scripting before we wrap
it up so let's flip back on over to my
script in here we're going to take this
uh line of code from from up above I
don't know if you remember it but
predicts equals the iris. target names
so we're going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that so you need the double
brackets and I could actually create
data maybe let's let's just do two
flowers so maybe I'm process processing
more data coming in and we'll put two
flowers in here and then uh I actually
want to see what the answer is so let's
go ahead and type in PRS and print that
out and when I run this you'll see that
I've now predicted two flowers that
maybe I measured in my front yard as
versacolor and
versacolor not surprising since I put
the same data in for each one this would
be the actual uh end product going out
to be used on data that you don't know
the end answer
for so that's going to conclude our
scripting part of this and let's just go
ahead and take a look at the key
takeaways with today's tutorial we have
Solutions under classification so we
looked at where the random Forest fits
in in the bigger model as far as
supervised learning and part of the
machine learning class and in this case
it's in classification and why a random
Forest the three main points it has very
little overfitting if any it has a high
accurac
and in my opinion one of the most
powerful tools is it estimates missing
data we saw that with the missing color
of the fruit we talked about what is a
random Forest versus a tree and then we
went into how does a decision tree work
how does a random Forest work we put all
those trees together and then we took a
look at some basic python coding in the
iris example so what is gin's clustering
K me's clustering is an unsupervised
learning algorithm in this case you you
don't have labeled data unlike in
supervised learning so you have a set of
data and you want to group them and as
the name suggests you want to put them
into clusters which means objects that
are similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically is a
number so we need to tell the system how
many clusters we need to perform so if K
is equal to 2 then will be two clusters
if K is equal to three three clusters
and so on and so forth that's what the k
stands for and of course there is a way
of finding out what is the best or
Optimum value of K for a given data we
will look at that so that is K means
cluster so let's take an example C's
clustering is used in many many
scenarios but let's take an example of
Cricket the game of cricket let's say
you received data of a lot of players
from maybe all all over the country or
all over the world and this data has
information about the runs scored by the
people or by the player and the wickets
taken by the player and based on this
information we need to Cluster this data
into two clusters batsmen and Bowlers so
this is an interesting example let's see
how we can perform this so we have the
data which consists of primarily two
character itics which is the runs and
the wickets so the bowlers basically
take wickets and the batsmen score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsmen who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Bowlers
so how does this work let's say this is
how the data is so there are information
there is information on the y axis about
about the Run scored and on the x-axis
about the wickets taken by the players
so if we do a quick plot this is how it
would look and um when we do the
clustering we need to have the Clusters
like shown in the third diagram out here
so we need to have a cluster which
consists of people who have scored High
runs which is basically the batsman and
then we need a cluster with people who
have taken a lot of wickets which is
typically the the Bowers there may be a
certain amount of overlap but we will
not talk about it right now so with C
in's clustering we will have here that
means K is equal to two and we will have
two clusters which is batsman and
Bowlers so how does this work the way it
works is the first step in cayman's
clustering is the allocation of two
centroids randomly so two points are
assigned as socalled centroids so in
this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the centroid okay so these points
though in our presentation here we have
shown them one point closer to these
data points and another closer to these
data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
centroid this point is taken and the
distance is found from this centroid and
this Center and so on and so forth so
for every point the distance is measured
from both the centroids and then
whichever distance is less that point is
assigned to that centroid so for example
in this case visually it is very obvious
that all these data points are assigned
to this centroid and all these data
points are assigned to this centroid and
that's what is represented here in blue
color and in this yellow color the next
step is to actually
determine the central point or the
actual centroid for these two clusters
so we have this one initial cluster this
one initial cluster but as you can see
these points are not really the centroid
centroid means it should be the central
position of this data set Central
position of this data set so that is
what needs to be determined as the next
step so the central point or the actual
Cent is determined and the original
randomly allocated ated centroid is
repositioned to the actual centroid of
this new clusters and this process is
actually repeated now what might happen
is some of these points may get
reallocated in our example that is not
happening probably but it may so happen
that the distance is found between each
of these data points once again with
these centroids and if there is if it is
required some points may be reallocated
we will see that in a later example but
for now we will keep it simple so this
process is continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the Cent doesn't need
any more repositioning and that means
our algorithm has converged convergence
has occured and we have the cluster two
clusters we have the Clusters with a
cide so this process is repeated the
process of calculating the distance and
repositioning the centroid is repeated
till the repositioning stops which means
that the algorithm has converged and we
have the final cluster with the data
points and the cids so this is what
you're going to learn from this session
we will talk about the types of
clustering what is K me's clustering
application of K means clustering k
clustering is done using distance
measure so we will talk about the common
distance measures and then we will talk
about how kain's clustering works and go
into the details of kain's clustering
algorithm and then we will end with the
demo and a use case for C's clustering
so let's begin first of all what are the
types of clustering there are primarily
two categories of clustering
hierarchical clustering and then
partitional clustering and each of these
categories are further subdivided into
agglomerative and divisive clustering
and K means and fuzzy c means clustering
let's take a quick look at what each of
these types of clustering are in
hierarchical clustering the Clusters
have a Tre like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottomup
approach we begin with each element as a
separate cluster and merge them into
successively larger clusters so for
example we have a b CDE e f we start by
combining BNC form one cluster d and e
form one more then we combine de and f
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top- down
approach we begin with the whole set and
proceed to divide it into successively
smaller clusters so we have a bcde e f
we first take that as a single cluster
and then break it down into a b c d e
and f then we have partitional
clustering split into two subtypes k
means clustering and fuzzy c means in K
means clustering the objects are divided
into the number of clusters mentioned by
the number K that's where the K comes
from so if we say k is equal 2 the
objects are divided into two clusters C1
and C2 and the way it is done is the
features or characteristics are compared
and all objects having similar
characteristics are club together so
that's how K means clustering is done we
will see it in more detail as we move
forward and fuzzy c means is very
similar to K means in the sense that it
clubs objects that have similar
characteristics together but while in K
means clustering two objects cannot
belong to or any object a single object
cannot belong to two different clusters
in c means objects can belong to more
than one cluster so that is the primary
difference between K means and fuzzy c
means so what are some of the
applications of K means clustering K
means clustering is used in a variety of
examples or variety of business cases in
real life starting from academic
performance diagnostic system search
engines and wireless sensor networks and
many more so let us take a little deeper
look at each of these examples academic
performance So based on the scores of
the students students are categorized
into a b c and so on clustering forms a
backbone of search engines when a search
is performed the search results need to
be grouped together the search engines
very often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in a its
respective cluster so clustering
especially K means clustering uses
distance measure so let's take a look at
what is distance measure so while these
are the different types of clustering in
this video we will focus on K means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measure
Mees there is ukian distance there is
Manhattan distance then we have squared
ukian distance measure and cosine
distance measure these are some of the
distance measures supported by K means
clustering let's take a look at each of
these what is ukian distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square < TK of Y2
- y1 2 + X2 - X1 s so this is an
extension of that formula so that is the
ukian distance between two points what
is the squared ukian distance measure
it's nothing but the square of the ukian
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x- axis and the y-
axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
K means now let's go and check how
exactly K means clustering works okay so
this is how kain's clustering works this
is like a flowchart of the whole process
there is a starting point and then we
specify the number of clusters that we
want now there are couple of ways of
doing this we can do by trial and error
so we specify a certain number maybe is
equal to 3 or four or five to start with
and then as we progress we keep changing
until we get the best clusters or there
is a technique called elbow technique
whereby we can determine the value of K
what should be the best value of K how
many clusters should be formed so once
we have the value of K we specify that
and then the system will assign that
many centroids so it picks randomly that
to start with randomly that many points
that are considered to be the centroids
of these clusters and then it measures
the distance of each of the data points
from these centroids and assigns those
points to the corresponding centroid
from which the distance is minimum so
each data point will be assigned to the
centroid Which is closest to it and
thereby we have K number of initial
clusters however this is not the final
clusters The Next Step it does is for
the new groups for the Clusters that
have been formed it calculates the mean
position thereby calculates the new
centroid position the position of the
centroid moves compared to the randomly
allocated one so it's an iterative
process once again the distance of each
point is measured from this new centroid
point and if required the data points
are reallocated to to the new centroids
and the mean position or the new
centroid is calculated once again if the
centroid moves then the iteration
continues which means the convergence
has not happened the clustering has not
converged so as long as there is a
movement of the centroid this iteration
keeps happening but once the centroid
stops moving which means that the
cluster has converged or the clustering
process has converged that will be the
end result
so now we have the final position of the
centroid and the data points are
allocated accordingly to the closest
centroid I know it's a little difficult
to understand from this simple flowchart
so let's do a little bit of
visualization and see if we can explain
it better let's take an example if we
have a data set for a grocery shop so
let's say we have a data set for a
grocery shop and now we want to find out
how many many clusters this has to be
spread across so how do we find the
optimum number of clusters there is a
technique called the elbow method so
when these clusters are formed there is
a parameter called within sum of squares
and the lower this value is the better
the cluster is that means all these
points are very close to each other so
we use this within sum of squares as a
measure to find the optimum number of
clusters that can be formed for a given
data set so we create clusters or we let
the system create clusters of a variety
of numbers maybe of 10 10 clusters and
for each value of K the within SS is
measured and the value of K which has
the least amount of within SS or WSS
that is taken as the optimum value of K
so this is the diagrammatic
representation so we have on the y- AIS
the within sum of squares or WSS and on
the x-axis we have the number of
clusters so as you can imagine if you
have K is equal to 1 which means all the
data points are in a single cluster the
withness value will be very high because
they are probably scattered all over the
moment you split it into two there will
be a drastic fall in the Within s s
value and that's what is represented
here but then as the value of K
increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either two or three
or at the most four but beyond that
increasing the number of clusters is not
not dramatically changing the value in
WSS because that pretty much gets
stabilized okay now that we have got the
value of K and let's assume that these
are our delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned to them but now in this
step the actual centroid will be
calculated which may be for each of
these data set somewhere in the middle
so that's like the mean point that will
be calculated and the centroid will
actually be positioned or repositioned
there same with C2 so the new centroid
for this group is C2 this new position
and C1 is in this new position once
again the distance of each of the data
points is calculated from these cids now
remember it's not necessary that the
distance Still Remains the or each of
these data points still remain in the
same group by recalculating the distance
it may be possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is a repetitive
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these data
points as a part of each cluster so I
hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 n number of points
as our inputs and we want to split this
into K clusters or we want to create K
clusters so the first step is to
randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroids C1
C2 C3 up to CK and find which is the
lowest distance and assign X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and uh so there are K groups
all these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the main position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of these newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as a part of step
four we repeat step two and three so we
again calculate the distance of X1 from
the the centroid C1 C2 C3 and then see
which is the lowest value and assign X1
to that calculate the distance of X2
from C1 C2 C3 or whatever up to CK and
find whichever is the lowest distance
and assign X2 to that croid and so on in
this process there may be some
reassignment X1 was probably assigned to
Cluster C2 and after doing this
calculation maybe now X1 is assigned to
C1 so that kind of reallocation may
happen so we repeat the steps two and
three till the position of the centroids
don't change or stop changing and that's
when we have convergence so let's take a
detail look at at each of these steps so
we randomly pick K cluster centers we
call them centroids because they are not
initially they are not really the
centroids so we let us name them C1 C2
up to CK and then step two we assign
each each data point to the closest
Center so what we do we calculate the
distance of each x value from each C
value so the distance between X1 C1
distance between X1 C2 X1 C3 and then we
find which is the lowest value right
that's the minimum value we find and
assign X1 to that particular centroid
then we go next to X2 find the distance
of X2 from C1 X2 from C2 X2 from C3 and
so on up to CK and then assign it to the
point or to the centroid which has the
lowest value and so on so that is Step
number two in Step number three We Now
find the actual centroid for each group
so what has happened as a part of Step
number two we now have all the points
all the data points grouped into K
groups because we we wanted to create K
clusters right so we have K groups each
one may be having a certain number of
input values they need not be equally
distributed by the way based on the
distance we will have K groups but
remember the initial values of the C1 C2
were not really the centroids of these
groups right we assigned them randomly
so now in step three we actually
calculate the centroid of each group
which means the orig origal point which
we thought was the centroid will shift
to the new position which is the actual
centroid for each of these groups okay
and we again calculate the distance so
we go back to step two which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in step two here okay so that might
change then we perform step three once
again to find the new centroid of this
new group so we have again a new set of
clusters new centroids and new ass
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of uh K mean's
clustering we will actually see some
live Demos in uh python notebook using
python notebook but before that let's
find out what's the problem that we are
trying to solve the problem statement is
let's say Walmart wants to open a chain
of stores across the State of Florida
and uh it wants to find the optimal
store locations now the issue here is if
they open too many stores close to each
other obviously the they will not make
profit but if they if the stores are too
far apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use
Gans clustering to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
libraries like numpy matplot Leb and so
on and we are loading the data that is
available in the form of let's say the
addresses for Simplicity sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups already being
formed so you can actually get an idea
about how the cluster would look and how
many many clusters what is the optimal
number of clusters and then starts the
actual K means clustering process so we
will assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct
clusters and um which is we can say that
this is how the population is probably
distributed across Florida State and uh
the centroids are like the location
where the store should be the optimum
location where the store should be so
that's the way we determine the best
locations for the store and that's how
we can help Walmart find the best
locations for the stores in Florida so
now let's take this into python notebook
let's see how this looks when we are
learning running the code live all right
so this is the code for K means
clustering in Jupiter notebook we have a
few examples here which we will
demonstrate how K means clustering is
used and even there is a small
implementation of C's clustering as well
okay so let's get started okay so this
block is basically importing the various
libraries that are required like M plot
lip and numpy and so on and so forth
which would be used as a part of the
code then we are going and creating
blobs which are similar to clusters now
this is a very neat feature which is
available in psychic learn make blobs is
a nice feature which creates clusters of
data sets so that's a wonderful
functionality that is readily available
for us to create some test data kind of
thing okay so that's exactly what we are
doing here we are using make blobs and
we can specify how many clusters we want
so centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us and this is how it
looks as you can see visually also we
can figure out that there are four
distinct classes or clusters in this
data set and that is what make blobs
actually provides now from here onwards
we will basically run the standard K
means functionality that is readily
available so we really don't have to
implement K means itself the C
functionality or the the function is
readily available you just need to feed
the data and we'll create the Clusters
so this is the code for that we import K
means and then we create an instance of
K means and we specify the value of K
this ncore clusters is the value of K
remember K means in K means K is
basically the number of clusters that
you want to create and it is a integer
value so this is where we are specifying
that so we have K is equal to 4 and so
that instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training kind of
thing but that's the call okay so we are
calling fit and what we are doing here
we are just passing the data so X has
these values the data that has been
created right so that is what we are
passing here and uh this will go ahead
and create the Clusters and uh then we
are
using after doing uh fit We Run The
predict which basically assigns for each
of these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or will actually
start from zero cluster 0o 1 2 and three
maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y K
means when we call predict that is what
it does and we can take a quick look at
these uh Yore K means or the cluster
numbers that have been assigned for each
observation so this is the cluster
number assigned for observation one
maybe this is for observation two
observation three and so on so we have
how many about I think 300 samples right
so all the 300 samples there are 300
values here each of them the cluster
number is given and the cluster number
goes from 0 to 3 so there are four
clusters so the numbers go from 0 1 2 3
so that's what is seen here okay now so
this was a quick example of generating
some dummy data and then clustering that
okay and this can be applied if you have
proper data you can just load it up into
X for example here and then run the C so
this is the central part of the km's
clustering program example so you
basically create an instance and and you
mention how many clusters you want by
specifying this parameter ncore clusters
and that is also the value of K and then
pass the data to get the values now the
next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the K
means algorithm so we will just walk you
through I will walk you through the code
uh at each step what it is doing and
then we will see a couple of more
examples of how K me means clustering
can be used in maybe some real life
examples real life use cases all right
so in this case here what we are doing
is basically implementing K means
clustering and there is a function for a
library calculates for a given two pairs
of points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what K means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to that Cent so
that is basically available as a
standard function and we will be using
that here so as explained in the slides
the first step that is done in case of
kin's clustering is to randomly assign
some cides so as a first step we
randomly allocate a couple of centroids
which we call here we're calling as
centers and then we put this in a loop
and we take it through an iterative
process for each of the data points we
first find out using this function
pairwise distance argumen for each of
the points we find out which one which
Center or which randomly selected
centroid is the closest and accordingly
we assign that data or the data point to
that particular centroid or cluster and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
the center position right so we
calculate the new centroid and then we
check if the new centroid is the
coordinates or the position is the same
as the previous croid the positions we
will compare and if it is the same that
means means the process has converged so
remember we do this process till the
centroids or the centroid doesn't move
anymore right so the centroid gets
relocated each time this relocation is
done so the moment it doesn't change
anymore the position of the centroid
doesn't change anymore we know that
convergence has occurred so till then so
you see here this is like an infinite
Loop while true is an infinite Loop it
only breaks when the centers are the
same the new center old Center positions
are the same and once that is uh done we
return the centers and the labels now of
course as explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in the
change will be very minor so in that
case also that is actually convergence
right so for example the change is 01 we
can consider that as convergence
otherwise what will happen is this will
either take forever or it will be never
ending so that's a small flaw here so
that is something additional checks may
have to be added here but again as
mentioned this is not the most
sophisticated implementation this is
like a kind of a rough implementation of
the K means clustering okay so if we
execute this code this is what we get as
the output so this is the definition of
this particular function and then we
call that find underscore clusters and
we pass our data X and the number of
clusters which is four and if we run
that and plot it this is the output that
we get so this is of course each cluster
is represented by a different color so
we have a cluster in green color yellow
color and so on and so forth and these
big points here these are the centroids
this the final position of the centroids
and as you can see visually also this
appears like a kind of a center of all
these points here right similarly this
is like the center of all these points
here and so on so this is the example or
this is an example of a implementation
of K means clustering and uh next we
will move on to see a couple of examples
of how K means clustering is used in
maybe some real life scenarios or use
cases in the next example or demo we're
going to see how we can use K means
clustering to perform color compression
we will take a couple of images so there
will be two examples and uh we will try
to use C's clustering to compress the
colors this is a common situation in
image processing when you have an image
with millions of uh colors but then you
cannot render it on some devices which
may not have enough memory uh so that is
the scenario where where something like
this can be used so before again we go
into the python notebook let's take a
look at quickly the the code as usual we
import the libraries and then we import
the image and uh then we will flatten it
so the reshaping is basically we have
the image information is stored in the
form of pixels and if the imag is like
for example 427 by 640 and it has three
colors so that's the overall
dimension of the of the initial image we
just reshape it and um then feed this to
our algorithm and this will then create
clusters of only 16 clusters so this
this colors there are millions of colors
and now we need to bring it down to 16
colors so we use K is equal to 16 and U
this is how when we visualize this is
how it looks there are these are all
about 16 million possible colors the
input color space has 16 million
possible colors and we just some
compress it to 16 colors so this is how
it would look when we compress it to 16
colors and this is how the original
image looks and after compression to 16
colors this is a the new image looks as
you can see there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit so this is an example which
we are going to now see in Python
notebook let's go into the python
notebook and once again as always we
will import some libraries and load this
image called flower. jpg okay so let we
load that and this is how it looks this
is the original image which has I think
16 million colors and uh this is the
shape of this image which is basically
what is the shape is nothing but the
overall size right so this is 427 pixel
by 640 pixel and then there are three
layers which is this three basically is
for RGB which is red green blue so color
image will have that right so that is
the shape of this now what we need to do
is data let's take a look at how data is
looking so let me just create a new cell
and show you what is in data basically
we have captured this
information so data is what let me just
show you
here all right so let's take a look at
China what are the values in China and
if you see here this is how the data is
stored this is nothing but the pixel
values okay so this is like a matrix and
each one has about for for this 427 by
640 P pixels all right so this is how it
looks now the issue here is these values
are large the numbers are large so we
need to normalize them to between 0er
and one right so that's why we will
basically create one more variable which
is data which will contain the values
between zero and one and the way to do
that is divide by 255 so we divide China
by 255 and we get the new values in data
so let's just run this piece of code and
this is the shape so we now have also
yeah what we have done is we changed
using reshape we converted into the
three-dimensional into a two-dimensional
data set and let us also take a look at
how let me just
insert probably a cell here and take a
look at how data is looking all right so
this is how data is looking and now you
see this is the values are between zero
and one right so if you earlier noticed
in case of china the values were large
numbers now everything is between zero
and one this is one of the things we
need to do all right so after that the
next thing that we need to do is to
visualize this and uh we can take random
set of maybe 10,000 points and plot it
and check and see how this looks so let
us just plot this and so this is how the
original the color the pixel
distribution is these are two plots one
is red against Green and another is red
against Blue and this is the original
distribution of the color so then what
we will do is we will use K's clustering
to create just 16 clusters for the
various colors and then apply that to
the image now what will happen is since
the data is large because there are
millions of colors using regular K means
maybe a little time consuming so there
is another version of K means which is
called mini batch gaming so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are the this is
how the 16 colors uh would look so this
is red against Green and this is red
against Blue there is uh quite a bit of
similarity between this original color
schema and the new one right so it
doesn't look very very completely
different or anything like that now we
apply this the newly created colors to
the image and we can take a look how
this is uh looking now we can compare
both the images so this is our original
image and this is our new image so as
you can see there is not a lot of
information that has been lost uh it
pretty much looks like the original
image yes we can see that for example
here there is a little bit uh it appears
a little dlish compared to this one
right because uh we kind of took off
some of the finer details of the color
but overall the high level information
has been maintained at the same time the
main advantage is that now this can be
this is an image which can be rendered
on a device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the Summer Palace in China and we repeat
the same process this is a
highdefinition color image with millions
of colors and also uh
three-dimensional uh now we will reduce
that to 16 colors using K means
clustering and um we do the same process
like before we reshape it and then we
cluster the colors to 16 and then we
render the image once again and we will
see that the color the quality of the
image slightly deteriorates as you can
see here this has much finer details in
this which are probably missing here but
then that's the compromise because there
are some devices which may not be able
to handle this kind of high density
images so let's run this code in Python
notebook all right so let's apply the
same technique for another picture which
is uh even more intricate and has
probably much complicated color schema
so this is the image now once again uh
we can take a look at the shape which is
427 by 640 by 3 and this is the new data
would look somewhat like this compared
to the flower image so we have some new
values here and we will also bring this
as you can see the numbers are much big
so we will much bigger so we will now
have to uh scale them down to values
between 0 and one and that is done by
dividing by 255 so let's go ahead and uh
do that and reshape it okay so we get a
two- dimensional Matrix and uh we will
then as the next step we will go ahead
and visualize this how it looks the the
16 colors and this is basically how it
would look 16 million colors and now we
can create the Clusters out of this the
16 cin clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
ahead and uh apply this and visualize
how it is looking for with the with the
new just the 16 color so once again as
you can see this looks much richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done uh using
kins clustering and we have also seen in
the previous examples of how to
implement K means the code to L how to
implement K me's clustering and we use
some sample data using blob to just
execute the cin clustering all right so
with that let's move on so let's
summarize what we have learned in this
video we started with an example of how
we can apply C's clustering taking the
cricet example then we understood what
are the types of clustering two major
categories hierarchical clustering and
partitional cl clustering which in turn
had two subcategories agglomerative and
divisive and then K means and fuzzy C
then we understood the distance measures
what are the different types of distance
measures supported by K means clustering
and we focused on kain's clustering we
talked about its applications and how
exactly the process flow works for the
kain's clustering and then finally we
ended up with a a demo and a couple of
use cases
if you are an aspiring data scientist
who's looking out for online training
and certification in data science from
the best universities and Industry
experts then search no more simply
learns postgraduate program in data
science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below have you ever
wondered how your mail provider
implements spam filtering or how online
news channel channels perform news text
classification or how companies perform
sentimental analysis of Their audience
on social media all of this and more is
done through a machine learning
algorithm called naive Bay classifier
welcome to naive Bay tutorial my name is
Richard kersner I'm with the simplylearn
team that's www.s simplylearn tocom get
certified get ahead what's in it for you
we'll start with what is naive Bay a
basic overview of how it works we'll get
into naive bays and machine learning
where it fits in with our other machine
learning tools why do we need naive bays
and understanding naive Bay classifier a
much more in-depth of how the math works
in the background finally we'll get into
the advantages of the Nave Bay
classifier in the machine learning setup
and then we'll roll up our sleeves and
do my favorite part we'll actually do
some python coding and do some text
classification using the naive base what
is naive Bay let's start with a basic
introduction to the Baye theorem named
after Thomas baz from the 1700s who
first coined this in the western
literature naive Bay classifier works on
the principle of conditional probability
as given by the Bas theorem before we
move ahead let us go through some of the
simple Concepts and the probability that
we will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head heads they come
up as head tail tell head and tell tail
when doing the math on prob ability we
usually denote probability as a p a
capital P so the probability of getting
two heads equals 1/4 you can see on our
data set we have two heads and this
occurs once out of the four
possibilities and then the probability
of at least one tail occurs 3/4 of the
time you'll see on three of the coin
tosses we have tails in them and out of
four that's 3/4s and then the
probability of the second coin being
head given the first coin is tail is 1/2
and the probability of getting two heads
given the first coin is a head head is
1/2 we'll demonstrate that in just a
minute and show you how that math works
now when we're doing it with two coins
is easy to see but when you have
something more complex you can see where
these Pro these formulas really come in
and work so the base theorem gives us
the conditional probability of an event
a given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event a we
want to know what that is so we're going
to label that a since that's our focus
and then given another event B has
occurred in the Baye theorem as you can
see on the left the probability of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred times the probability of a
over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B times
probability of b equals a probability of
B given a * probability of a you can
easily move that around and multiply it
and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with tells in
them giving us three quarters or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins because you can easily understand
the math the probability of throwing a
tail doesn't matter what comes before it
and the same with the head so still
going to be 50% or 1/2 but when that
come when that probability gets more
complicated let's see you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is Tails again we
reversed it because we want to know what
the second event going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
Time the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being Tails given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1/2
* 1/2 over 1/2 or 1 12 = .5 or 1/4 so
the base theorem basically calculates
the conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understanding naive Bay
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
Baye fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also regression
but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is naive
Bay used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's all also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
elements and news classification you
look at the Google news and it says well
is this political or is this world news
or a lot of that's all done with the
naive Baye understanding naive Bay
classifier now we already went through a
basic understanding with the coins and
the two heads and two tails and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Bay classifier is
based on the Baye theorem which gives a
conditional probability of in event a
given event B and that's where the
probability of a given b equals the
probability of B given a Time
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we can multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easily come up with a new formula for
the probability of B to me staring at
these algebraic functions kind of gives
me a slight headache it's a lot better
to see if we can actually understand how
this data fits together in a table and
let's go ahead and start applying it to
some actual data so you can see what
that looks like so we're going to start
with the shopping demo problem statement
and remember we're going to solve this
first in uh table form so you can see
what the math looks like and then we're
going to solve it in Python and in here
we want to predict whether the person
will purchase a product are they going
to buy or don't buy very important if
you're running a business you want to
know how to maximize your profits or at
least maximize the purchase of the
people coming into your store and we're
going to look at a specific combination
of different variables in this case
we're going to look at the day the
discount and the free delivery and you
can see here under the day we want to
know whether it's uh on the weekday you
know somebody's working they come in
after work or maybe they don't work
weekend you can see the bright colors
coming down there celebrating not being
in work or holiday and did we offer a
discount that day yes or no did did we
offer free delivery that day yes or no
and from this we want to know whether
the person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead and
take a look at the data set we have a
small sample data set of 30 rows we're
showing you the first 15 of those rows
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like when we put that into
tables also note in today's world we're
not usually looking at three different
variables in 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free to dely yes or no on those days how
many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to this table
for our Baye theorem let the event buy
ba a now remember when we looked at the
coins I said we really want to know what
the outcome is did the person buy or not
and that's usually event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so is 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right
you can see that N9 out of 24 purchases
are made on the weekday 7 out of 24
purchases on the weekend and eight out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the week dates
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out 11 out of 30 so out of
the 30 people who came into the store
throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or2 so there's a 20% chance that
they're not going to make a purchase no
matter what day of the week it is and
finally we look at the probability of
BFA in this case we're going to look at
the probability of the weekday and not
buying two of the no buys were done out
of the weekend out of the six people who
did not make purchases so when we look
at that probability of the week day
without a purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making
making a purchase at all doesn't matter
what day of the week is roughly 02 or
20% and the probability of a weekday no
purchase is roughly two out of six so
two out of six of our no purchases were
made on the weekday and then finally we
take our P of a b if you looked we've
kept the symbols up there we got P of
probability of B probability of a
probability of B if a we should remember
that the probability of a if B is equal
to the first one time the probability of
no per buys over the probability of the
weekday so we could calculate it both
off the uh table we created we can also
calculate this by the formula and we get
the 367 which equals or 33 * 2 over 367
which equals. 179 or roughly uh 17 to
18% and that'd be the probability of no
purchase done on the weekday and this is
important because we can look at this
and say as the probability of buying
buying on the weekday is more than the
probability of not buying on the weekday
we can conclude that customers will most
likely buy the product on a weekday now
we've kept our chart simple and we're
only looking at one aspect so you should
be able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we're actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday did they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in
just a second in the formula times the
full probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the prop probability of a no
buy is across all the data so that
that's where we get the 6 out of 30 we
divide that out by the probability of
each category over the total number so
we get the 20 out of 30 had a discount
23 out of 30 had a yes for free delivery
and 11 out of 30 were on a holiday we
plug all those numbers in we get
178 so in our probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase did we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 * 21 over 24 * 8
over 24 time the P of a 24 over 30
divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 * 11 over 30
and that gives us our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 986 we have a
probability of no purchase equals. 178
so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals 0986 86 plus. 178 and that equals
the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is
84.7% and the likelihood of no purchase
is
15.29% given these three different
variables so if it's on a holiday if
it's uh with a discount and has free
delivery then there's an 84.7 one%
chance that the customer is going to
come in and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand why you want to why you want
to use the naive Bays what what are the
advantages of naive bays and then once
we understand those advantages we're
just look at that briefly then we're
going to dive in and do some python
coding advantages of naive Bay
classifier so let's take a look at the
six advantages of the naive Bay
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probab ilities and I certainly
mess those up as I put them on you know
is it on the top or the bottom you got
to really pay close attention to that
when you put it into python it's really
nice because you don't have to worry
about any of that you let the python
handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amounts of data this
is great powerful tool for that handles
both continuous and discrete data it's
highly scalable with number of
predictors and data points so as you can
see you just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast it
can be used in realtime predictions this
is so important this is why it's used in
a lot of our predictions on online
shopping carts uh referrals spam filters
is because there's no time delay as it
has to go through and figure out a
neural network or one of the other mini
setups where you're doing classification
and certainly there's a lot of other
tools out there in the machine learning
that can handle these but most of them
are not as fast as the naive Bay and
then finally it's not sensitive to
irrelevant features so it picks up on
your different probabilities and if
you're short on date on one probability
you can kind of it automatically adjusts
for that those formulas are very
automatic and so you can still get a
very solid predictability even if you're
missing data or you have overlapping
data for two completely different areas
we see that a lot in doing census and
and studying of people and habits where
they might have one study that covers
one aspect and another one that overlaps
and because the two overlap they can
then predict the unknown for the group
that they haven't done the second study
on or vice versa so it's very powerful
in that it is not sensitive to the
irrelevant features and in fact you can
use it to help predict features that
aren't even in there so now we're down
to my favorite part we're going to roll
up our sleeves and do some actual
programming we're going to do the use
case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bays so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bay classifier now we're at my favorite
part we're actually going to write some
python script roll up our sleeves and
we're going to start by doing our
Imports these are very basic Imports
including our news group and we'll take
a quick quick glance at the Target names
then we're going to go ahead and start
training our data set and putting it
together we'll put together a nice graph
CU it's always good to have a graph to
show what's going on and once we've
trained it and we've shown you a graph
of what's going on then we're going to
explore how to use it and see what that
looks like now I'm going to open up my
favorite editor or inline editor for
python you don't have to use this you
can use whatever your editor that you
like whatever uh interface IDE you want
this just happens to be the Anaconda
Jupiter notebook and I'm going to going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
uh and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly it
should also work in a lot of the 2x you
just have to make sure all the the
versions of the modules match your
python version and in here you'll notice
the first line is your percentage met
plot library in line now three of these
lines of code are all about plotting the
graph
this one lets the notebook notes and is
the inline setup that we want the graphs
to show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
idees don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop up and
the graph pops up on there so you have a
that setup also but for this we want the
mat plot library in line and then we're
going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP nump as NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our map plot
library. pyplot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import caborn as SNS and we're
going do the SNS doet now caborn sits on
top of pip plot and it just makes a
really nice heat map it's really good
for heat maps and if you're not familiar
with heat maps that just means we give
it a color scale the term comes from the
brighter red it is the hotter it is in
some form of data and you can set it to
whatever you want and we'll see that
later on so those you'll see that those
three lines of code here are just
importing the graph function so we can
graph it and as a data scientist you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything and then from the SK learn. dat
sets we're going to import the fetch 20
news groups very common one for
analyzing tokenizing words and setting
them up and exploring how the words work
and how do you categorize different
things when you're dealing with
documents and then we set our data equal
to fetch 20 news groups so our data
variable will have the data in it and
we're going to go ahead and just print
the target names data. Target names and
let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually here going to go ahead and get
it get the data all set up and take a
look at our data and let's move this
over to our Jupiter notebook and let's
see what this code does first we're
going to set our categories now if you
noticed up here I could have just as
easily set this equal to data. Target
names because it's the same thing but we
want to kind of spell it out for you so
you can see the different categories it
kind of makes it more visual so you can
see what your data is looking like in
the background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 news groups subset
equals test and categories equals
categories let's go down one size so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates train.
dat and we're just going to look at data
piece number five and let's go ahead and
run that and see what that looks like
and you can see when I print train. dat
number five under train it prints out
one of the Articles this is article
number five you can go through and read
it on there and we can also go in here
and change this to test which should
look identical because it's splitting
the data up into different groups train
and test and we'll see test number five
is a a different article but another
article in here and maybe you're curious
and you want to see just how many
articles are in here we could do length
of train. data and if we run that you'll
see that the training data has
11,314 articles so we're not going to go
through all those articles that's a lot
of articles but um we can look at one of
them just so you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs VT
line 58 lines 58 in article uh Etc and
scroll all the way down and see all the
different parts to there now we've
looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you waight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bay and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we had
the print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction. text we're going to
import tfidf vectorizer I told you we're
going to throw a module at you we can't
go too much into the math behind this or
how it works you can look it up the
notation for the math is usually TF IDF
and that's just a way of weighing the
words and it weighs the words based on
how many times they're used in a
document how many times or how many
documents they're used in and it's a
well-used formula it's been around for a
while it's a little confusing to put
this in here uh but let's let it know
that it just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you put a weit on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value in telling you what
this document's about same with words
like in in article in articles in cost
of un maybe cost might or where words
like criminal weapons destruction these
might have a heavier weight because you
describe a little bit more what the
article is doing well how do you figure
out all those weights in the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our sklearn naive bays and
that's our multinomial NB multinomial
naive Bay pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline import
make pipeline now the make pipeline is
just a cool piece of code because we're
going to take the information we get
from the tfidf vectorizer and we're
going to pump that into the multinomial
INB so a pipeline is just a way of
organizing how things flow it's used
commonly you probably already guess what
it is if you've done any businesses they
talk about the sales pipeline if you're
on a work crew or project manager you
have your pipeline of information that's
going through where your projects and
what has to be done in what order that's
all this pipeline is we're going to take
the tfid vectorizer and then we're going
to push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model.fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train.
target is what category they already
categorized that that particular article
as and what's Happening Here is the
train data is going into the tfid
vectorizer so when you have one of these
articles that goes in there it weights
all the words in there so there's
thousands of words with different
weights on them I remember once running
a model on this and I literally had 2.4
million tokens go into this so when
you're dealing like large document bases
you can have a huge number of different
words it then takes those words gives
them a weight and then based on that
weight based on the words and the
weights and then puts that into the
multinomial in B and once we go into our
naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial INB
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um Talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
used the term predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how it is and then we move on to the
next step to find out what happened to
do this we're going to go ahead and
create a confusion Matrix and a heat map
so the confusion Matrix which is
confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual truth was either
agreed with it or was something
different and it's going to add up those
numbers so we can take a look and just
see how well it worked and we're going
to set a variable mat equal to confusion
Matrix we have our test Target our test
data that was not part of the training
very important in data science we always
keep our test data separate otherwise
it's not a valid model if we can't
properly test it with new data and this
is the labels we created from that test
data these are the ones that we predict
it's going to be so we go in and we
create our SN heat map the SNS is our
caborn which sits on top of the PIP plot
so when we create a SNS do heat map we
take our confusion Matrix and it's going
to be uh matt. T and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 00001 format D and C
equals false have to do with the uh
format if you take those out you'll see
that some things disappear and then the
X tick labels and the Y tick labels
those are our Target names and you can
see right here that's the alt atheism
comp Graphics comp osms windows.
miscellaneous and then finally we have
our pltx label remember the SNS or the
Seaborn sits on top of our map plot
Library our PL T and so we want to just
tell that X label equals a true is is
true the labels are true and then the Y
label is prediction label so when we say
a true this is what it actually is and
the prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way we
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides because they have a
black background they put in there that
helps it shine a little bit better so
you can see the graph a little bit
easier so in reading this graph we want
to look at is how the color scheme has
come out and you'll see a line right
down the middle diagonally from upper
left to bottom right what that is is if
you look at the labels we have our
predicted label on the left and our true
label on the right those are the numbers
where the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk politics
miscellaneous social religion Christian
versus Alt atheism it mislabeled some of
those and those are very similar topics
so you could understand why it might
mislabel them but overall it did a
pretty good job if we're going to create
these models we want to go ahead and be
able to use them so let's see what that
looks like to do this let's go ahead and
create a definition a function to run
and we're going to call this function
let me just expand that just a notch
here there we go I like mining big
letters predict categories we want to
predict the category we're going to send
it s a string and then we're sending it
train equals train we have our training
model and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model.
predict s so it's going to send whatever
string we send to to it it's going to
push that string through the pipeline
the model pipeline it's going to go
through and uh tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
it'll put that through our naive Bay and
from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train. Target names predict of zero and
remember that the train. target names
that's just cat categories I could have
just as easily put uh categories in
there. predict of zero so we're taking
the prediction which is a number and
we're converting it to an actual
category we're converting it from um I
don't know what the actual numbers are
but let's say zero equals alt atheism so
we're going to convert that zero to the
word or one maybe it equals comp
Graphics so we're going to convert
number one into comp Graphics that's all
that is and then we got to go ahead and
and then we need to go ahead and run
this so I load that up and then once I
run that we can start doing some
predictions I'm going go ahead and type
in predict category and let's just do
predict category Jesus Christ and it
comes back and says it's social religion
Christian that's pretty good now note I
didn't put print on this one of the nice
things about the Jupiter notebook editor
and a lot of inline editors is if you
just put the name of the variable out is
returning the variable train. Target
names it'll automatically print that for
you in your own
you might have to put in print let's see
where else we can take this and maybe
you're a space science buff so how about
sending load to
International Space
Station and if we run that we get
science space or maybe you're a uh
automobile buff and let's do um oh they
were going to tell me AI is better than
BMW but I'm going to do BMW is better
than an Audi so maybe your car buff and
we run that and you'll see it says
recreational I'm assuming that's what
recc stands for Autos so I did a pretty
good job labeling that one how about uh
if we have something like a caption
running through there president of India
and if we run that it comes up and says
talk politics
miscellaneous so when we take our
definition or our function and we run
all these things through Kudos we made
it we were able to correctly classify
texts into different group groups based
on which category they belong to using
the naive base classifier now we did
throw in the pipeline the TF IDF
vectorizer we threw in the graphs those
are all things that you don't
necessarily have to know to understand
the naive base setup or classifier but
they're important to know one of the
main uses for the naive Bay is with the
TF IDF tokenizer vectorizer where it
tokenizes the word and adds labels and
we use the pipeline because you need to
put push all that data through and it
makes it really easy and fast you don't
have to know those to understand naive
Bays but they certainly help for
understanding the industry and data
science and we can see our categorizer
our naive Bas classifier we were able to
predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our train
model let's go ahead and wrap it up and
let's just go through what we covered we
gave you an introduction to naive B and
how it's used to perform basic
classification as a classifier we went
through the basic formula the
probability of a given B and the
probability of B given a the basics of
the naive Bay we touched a little bit on
some of the different uses for the naive
Bays we also went over the advantages of
it and where it really shines especially
when we talk about real-time processing
naive Bays is very fast we talked about
the shopping demo remember that if you
want to try that on your own send an
note down below let us know and we'll
get you that data set and we also went
through the python my favorite part the
text classification so we learned all
kinds of things in there and walking
through a real life scenario where you'd
use the naive Bay at welcome to the
session on deep learning my name is
Mohan and in this video we are going to
talk about what deep learning is all
about some of you may be already
familiar with the image recognition how
does image recognition
work you can train a application or your
machine to recognize whether a given
image is a cat or a dog and this is how
it works at a very high level it uses
artificial neural network it is trained
with some known images and during the
training it is told if it is recognizing
correctly or not and then when new
images are submitted it recognizes
correctly based on the accuracy of
course so a little quick understanding
about artificial neural networks so this
is the way it does is you provide a lot
of training data also known as labeled
data for example in this case these are
the images of dogs and the network
extracts some features that makes a dog
a dog right so that is known as feature
extraction and based on that when you
submit a new image of dog the basic
features remain pretty much the same it
may be a completely different image but
the features of a dog still remain
pretty much the same in various
different images let's say compared to a
cat and that's the way artificial neural
network works we'll go into details of
this uh very shortly and once the
training is done with training data we
then test it with some test data to
which is basically completely new data
which which the system has not seen
before unlike the training data and then
we find out whether it is predicting
correctly or not thereby we know whether
the training is complete or it needs
needs more training so that's how at a
very high level artificial neural
network works so this is what we're
going to talk about today our agenda
looks something like this what is deep
learning why do we need deep learning
and then what are the applications of
deep learning one of the main components
the secret Source in deep learning is
neural networks so we're going to talk
about what is neural network and um how
it works and some of its components like
for example the activation function the
gradient descent and so on and so forth
so that as a part of working of a neural
network we will go into little bit more
details how this whole thing works so
without much further Ado let's get
started so deep learning is considered
to be a part of machine learning so this
diagram very nicely depicts what deep
learning is at a very high level you
have the all-encompassing artificial
intelligence which is more a concept
rather than a technology or a technical
concept right so it is it is more of a
concept that very high level artificial
intelligence under the her is actually
machine learning and deep learning and
machine learning is a broader concept
you can say or a broader technology and
deep learning is a subset of machine
learning the primary difference between
machine learning and deep learning is
that deep learning uses neural networks
and it is suitable for handling large
amounts of unstructured data and the
last but not least one of the major
differences between machine learning and
deep learning is that in machine
learning the feature extraction or the
feature engineering is done by the data
scientists manually but in deep learning
since we use neural networks the feature
engineering happens automatically so
that's a little bit of a a quick
difference between machine learning and
deep learning and this diagram very
nicely depicts the relation between
artificial intelligence machine learning
and deep learning now why do we need
deep learning machine learning was there
for quite some time and it can do a lot
of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
Rec nition and we need to process
analyze images and voice that's the
reason deep learning is required
compared to let's say traditional
machine learning it can also perform
complex uh algorithms more complex than
let's say what machine learning can do
and it can achieve best performance with
large amounts of data so the more you
have the data let's say reference data
or label data the better the system will
do because the training process will be
that much better and last but not least
with deep learning you you can really
avoid the manual process of feature
extraction those are some of the reasons
why we need deep learning some of the
applications of deep learning deep
learning has made major inroads and it
is a major area in which deep learning
is applied is healthare and within
healthare particularly oncology which is
uh basically cancer related stuff one of
the issues with cancer is that a lot of
cancers today are curable they can be
cured they are detected early on and the
challenge with that is when a
Diagnostics is performed let's say an
image has been taken off a patient to
detect whether there is cancer or not
you need a specialist to look at the
image and determine whether it is the
patient is fine or there is any onset of
cancer and the number of Specialists are
limited so if we use deep learning if we
use automation here or if we use
artificial intelligence here then the
system can with a certain amount of the
good amount of accuracy determine
whether a particular patient is having
cancer or not so the prediction or the
detection process of a disease like
cancer can be expedited the detection
process can be expedited can be faster
without really waiting for a specialist
we can obviously then once the
application once the artificial
intelligence detects or predicts the
that there is an onset of cancer this
can be cross checked by a doctor but at
least the initial screening process can
be automated and that is where the
current focus is with respect to deep
learning in healthcare what else
robotics is another area deep learning
is majorly used in robotics and you must
have seen nowadays robots are everywhere
humanoids the industrial robots which
are used for manufacturing process you
must have heard about sopia who got C
citizenship with Saudi Arabia and so on
there are multiple such robots which are
knowledge oriented but there are also
industrial robots are used in Industries
in the manufacturing process and
increasingly in security and also in
defense for example image processing
video is fed to them and they need to be
able to detect objects obstacles and so
on and so forth so that's where deep
learning is used they need to be able to
hear and make sense of the sounds that
they are hearing that needs deep
learning as well well so robotics is a
major area where deep learning is
applied then we have self-driving cars
or autonomous cars you must have heard
of Google's autonomous car which has
been tested for millions of miles and
pretty much incident free there were of
course a couple of incidents here and
there but it is uh considered to be
fairly safe and there are today a lot of
Automotive companies in fact pretty much
every automotive company worth its name
is investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these will be in production and they
will be used extensively in real life
right now they are all in R&D and in
test phases but pretty soon these will
be on the road so this is another area
where deep learning is used and how is
it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars in the
side will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
weight so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens in a deep learning
environment so again a lot of scope here
to use deep learning a couple of other
applications are mission translation
today we have a lot of information and
very often this information is in one
particular language and more
specifically in English and people need
information in in various parts of the
world it is pretty difficult for human
beings to translate each and every piece
of information or every document into
all possible languages there are
probably at least hundreds of languages
or if not more to translate each and
every document into every language is uh
pretty difficult therefore we can use
deep learning to do pretty much like a
realtime translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the Fly for
example you go to somewhere like China
and you want to know what is written on
a signboard now it is impossible for for
somebody to translate that and put it on
the web or something like that so you
have an application which is trained to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to St stud that document
or in that case you need to translate
that so therefore deep learning is used
in such situations as well and that is
again on demand so it is not like you
have to translate all these documents
from other languages into English in one
sh and keep it somewhere that is again
pretty much an impossible task but on a
need basis so you have systems that are
trained to translate on the flag so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it it needs a lot
of effort to train such neural land it
has been proved that it is possible so
this is a relatively new area and on the
same lines colorization of images so
these two images on the left hand side
is a grayscale image or a black and
white image this was colored by a neural
end or a deep learning application as
you can see this done a very good job of
applying the colors and obviously this
was trained to do this uh colorization
but yes this is one more application of
deep learning now one of the major
Secret Sauce of deep learning is neural
network deep learning works on neural
network or consists of neural network so
let us see what is neural network neural
network or artificial neural network is
des designed or based on the human brain
now human brain consists of billions of
small cells that are known as neurons
artificial neural networks is in a way
trying to simulate the human brain so
this is a quick diagram of biological
Neuron a biological neuron consists of
the major part which is the cell nucleus
and then it has some tentacles kind of
stuff on the top called dendrite and
then there is like a long tail which is
known as the axon further again at the
end of this axon are what are known as
synapses these inter are connected to
the dendroid of the next neuron and all
these neurons are interconnected with
each other therefore there are like
billions of them sitting in our brain
and they're all active they're working
they based on the signals they receive
signals as inputs from other neurons or
maybe from other parts of the body and
and based on certain criteria they send
signals to the neurons at the other end
so they they get either activated or
they don't get activated based on so it
is like a binary Gates so they get
activated or not activated based on the
inputs that they receive and so on so we
will see a little bit of those details
as we move forward in our artificial
neuron but this is a biological neuron
this is the structure of a biological
neuron and artificial neural network is
based on the human brain the smallest
component of artificial neural network
is an artificial neuron as shown here
sometimes is also referred to as
perceptron now this is a very high level
diagram the artificial neuron has a
small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as X1 X2 and so on each of the
inputs are multiplied by what is known
as as weights which are represented as
W1 W2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like X1
into W1 plus X2 into W2 and so on the
products are then added and then there
is a bias that is added to that in the
next uh slide we will see that passes
through an activation function and the
output comes as a y which is the output
and based on certain criteria area the
cell gets either activated or not
activated so this output would be like a
zero or a one binary format okay so we
will see that in a little bit more
detail but let's do a quick comparison
between biological and artificial neuron
just like a biological neuron there are
dendrites and then there is a cell
nucleus and synapse and and an axon we
have in the artificial neuron as well
these inputs come like the dendrite if
you will act like the dendrites there is
a like a central unit which performs the
summation of these uh weighted inputs
which is basically W1 X1 W2 X2 and so on
and then a bias is added here and then
that passes through what is known as an
activation function okay so these are
known as the weights W1 W2 and then
there is a bias which will come out here
and that is added the bias is by the way
common for a particular neuron so there
won't be like B1 B2 B3 and so on only
weights will be one per input the bias
is common for the entire neuron it is
also common for or the value of the bias
Remains the Same for all the neurons in
a particular layer we will also see this
as we move forward and we see deep
neural network where there are multiple
neurons so that's the output now the
whole exercise of training a neuron is
about changing these weights and biases
as I mentioned artificial neural network
will consist of several such neurons and
as a part of the training process
process these weights keep changing
initially they are assigned some random
values through the training process the
weights the whole process of training is
to come up with the optimum values of W1
W2 and WN and then the B for or the bias
for this particular neuron such that it
gives an accurate output as required so
let's see what exactly that means so the
training process this is how it happens
it takes the inputs each input is
multiplied by a weight and these weights
during training keep changing so
initially they're assigned some random
values and based on the output whether
it is correct or wrong there is a
feedback coming back and that will
basically change these weights until it
starts giving the right output that is
represented in here as Sigma I going
from 1 to n if there are n inputs wi
into XI so this is the product of W1 X1
W2 X2 and so on right and there is a
bias that gets added here and that
entire thing goes to what is known as an
activation function so essentially this
is Sigma of
wixi plus a value of bias which is a b
so that entire thing goes as an input to
an activation function now this
activation function takes this as an
input gives the output as a binary
output it could be a zero or a one there
are of course to start with let's assume
it's a binary output later we will see
that there are different types of
activation functions so it need not
always be binary output but to start
with let's keep Simple so it decides
whether the neuron should be fired or
not so that is the output like a binary
output zero or one all right so again
let me summarize this so it takes the
inputs so if you're processing an image
for example the inputs are the pixel
values of the image X1 X2 up to xn there
could be hundreds of these so all of
those are fed as so these are some
values and these pixel values again can
be from 0 to 56 each of those pixel
values are then multiplied with what is
known as a weight this is a numeric
value can be any value so this is a
number W1 similarly W2 is a number so
initially some random values will be
assigned and each of these weights are
multiplied with the input value and
their sum this is known as the weighted
sum so that is performed in this kind of
the central unit and then a bias is
added remember the bias is common for
each neuron so this is not the bias
value is not one bias value for per
input so just keep that in mind the bias
value there is one bias per neuron so it
is like this summation plus bias is the
output from this section this is not the
complete output of the neuron but this
is a bias for output for step one that
goes as an input to what is known as
activation function and that activation
function results in an output usually a
binary output like a zero or a one which
is known as as the firing of the neuron
okay good so we talked about activation
function so what is an activation
function an activation function
basically takes the weighted sum which
is we saw W1 X1 W2 X2 the sum of all
that plus the bias so it takes that as
an input and it generates a certain
output now there are different types of
activation functions and the output is
different for different types of
activation functions moreover why is an
activation function required it is
basically required to bring in
nonlinearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today reu has kind of
taken over so reu is by far the most
popular activation function that is used
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0
and one threshold function is the value
will be zero up to a certain value and
beyond that this is also known as a step
function and beyond that it will be one
in case of sigmoid there is a gradual
increase but in case of threshold it's
like also known as a step function
there's a rapid or instantaneous change
from 0 to one whereas in sigmoid we will
see in the next slide there is a gradual
increase but the value in this case is
between 0 and one as well Now reu
function on the other hand it is equal
to basically if the input is zero or
less than zero then the output is zero
whereas if the input is greater than
zero then the output is equal to the
input I know it's a little confusing but
in the next slides where we show the reu
function it will become clear similarly
hyperbolic tangent this is similar to
sigmoid in terms of the shape of the
function however while sigmoid goes from
0 to 1 hyperbolic tangent goes from
minus1 to 1 and here again the increase
or the change from -1 to 1 is gradual
and not like threshold or step function
where it happens instantaneous l so
let's take a little detailed look at
some of these functions so let's start
with the sigmoid function so this is the
equation of a sigmoid function which is
1 by 1 + e ^ of - x so X is the value
that is the input it goes from 0 to -1
so this is sigmoid function the equation
is 5X = 1 by 1 + e ^ Min - x and as you
can see here this is the input on the x-
axis as X is the value is coming from in
fact it can also go negative this is
negative actually so this is the zero so
this is the negative value of x so as X
is coming from negative value towards
zero the value of the output slowly as
it is approaching zero it it slowly and
very gently increases and actually at
the point let me just use a pen at the
point here it is it is .5 it is actually
0.5 okay and slowly gradually it
increases to one as the value of X
increases but then as the value of X
increases it tapers off it doesn't go
beyond one so that is the speciality of
sigmoid function so the output value
will remain between 0er and 1 it will
never go below zero or above one okay
then so that is sigmoid function now
this is threshold function or this is
also referred to as a step function and
here we can also set the threshold in
this case it is that's why it's called
called the threshold function normally
it is zero but you can also set a
different value for the threshold now
the difference between this and the
sigmoid is that here the change is Rapid
or instantaneous as the x value comes
from negative up to zero it remains zero
and at zero it pretty much immediately
increases to one okay so this is a
mathematical representation of threshold
function 5x is equal to 1 if x is
greater than equal to 0 and 0 if x is
less than zero so for all negative
values it is zero since we have set the
threshold to be zero so as soon as it
reaches zero it becomes one you see the
difference between this and the previous
one which is basically the sigmoid where
the increase from0 to one is gradual and
here it is instantaneous and that's why
this is also known as a step function
threshold function or step function this
is a reu reu is one of the most popular
activation functions today this is the
definition ation of reu 5x is equal to
Max of X comma 0 what it says is if the
value of x is less than 0 then 5x is um
zero the moment it increases goes beyond
zero the value of 5x is equal to X so it
doesn't stop at one actually it goes all
the way so as the value of X increases
the value of y will also increase
infinitely so there is no limit here
unlike your sigmoid or threshold old or
the next one which is basically
hyperbolic tangent okay so in case of
reu remember there is no upper limit the
output is equal to either zero in case
the value of x is negative or it is
equal to the value of x so for example
here if the value of x is 10 then the
value of y is also 10 right okay so that
is reu and there are several advantages
of relu and it is much more efficient
and uh provides much more accuracy
compared to other activation functions
like sigmoid and so on so that's the
reason it is very popular all right so
this is hyperbolic tangent activation
function the function looks similar to
sigmoid function the curve if you see
the shape it looks similar to sigmoid
function but the difference between
hyperbolic tangent and sigmoid function
is that in case of sigmoid the output
goes from 0 to 1 whereas in case of
hyperbolic tangent it goes from minus1
to 1 one so that is the difference
between hyperbolic tangent and sigmoid
function otherwise the shape looks very
similar there is a gradual increase
unlike the step function where there was
an instant increase or instant change
here again very similar to sigmoid
function the value changes gradually
from minus1 to 1 so this is the equation
of hyperbolic tangent activation
function yeah so then let's move on this
is a diagrammatic representation of the
activation function and how the overall
data or how the overall progression
happens from input to the output so if
we get the input from the input layer by
the way the neural network has three
layers typically there will be three
layers there is an input layer there is
an output layer and then you have the
hidden layer so the inputs come from the
input layer and they get processed in
the hidden layer and then you get the
output in the output layer so let's take
a little bit of a detailed look in into
the working of a neural network so let's
say we want to classify some images
between dogs and cats how do we do this
this is known as a classification
process and we are trying to use neural
networks and deep learning to implement
this classification so how do we do that
so this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two hidden layers and what we
do is we provide labeled training data
which means these images are fed to the
network with the label saying that okay
this is a cat the neural network is
allowed to process it and come up with a
prediction saying whether it is a cat or
a dog and obviously in the beginning
there may be mistakes a cat may be
classified as a dog so we then say that
okay this is wrong this output is wrong
but every time it predicts correctly we
say yes this output is correct so that
learning process so it will go back make
some changes to its weights and biases
we again feed these inputs and it will
give us the output we will check whether
it is correct or not and so on so this
is a iterative process which is known as
the training process so we are training
the neural network and what happens in
the training process these weights and
biases you remember there were weights
like W1 W2 and so on so these weights
and biases keep changing every time you
feed these which is known as an Epoch so
there are multiple iterations every
iteration is known as an Epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so once again what
is the input this input could be like
thousand images of cats and dogs and
they are labeled because we know which
is a cat and which is a dog and we feed
those thousand images the neural network
will initially assign some weights and
biases for each neuron and it will try
to process extract the features from the
images and it will try to come up with a
prediction for each image and that
prediction that is calculated by the
network is compared with the actual
value whether it is a cat or a dog and
that's how the error is calculated so
let's say there are th images and in the
first run only 500 of them have been
correctly classified that means we are
getting only 50% accuracy so we feed
that information back to the network
further update these weights and biases
for each of the neurons and we run this
these inputs once again it will try to
calculate extract the features and it
will try to predict which of these is
cats and dogs and this time let's say
out of, 700 of them have been predicted
correctly so that means in the second
iteration the accuracy has increased
from 50% to 70% all right then we go
back again we feed this maybe for a
third iteration fourth iteration and so
on and slowly and steadily the accuracy
of this network will keep increasing and
it may reach probably you never know 90%
95% and there are several parameters
that are known as Hyper parameters that
need to be changed and tweaked and that
is the overall training process and
ultimately at some point we say okay you
will probably never reach 100% accuracy
but then we set a limit saying that okay
if we receive 95% accuracy that is good
enough for our application and then we
say okay our training process is done so
that is the way training happens and
once the training is done now with the
training data set the system has let's
say seen all these thousand images
therefore what we do is the next step
like in any normal machine learning
process we do the testing where we take
a fresh set of images and we feed it to
the network the fresh set which it has
not seen before as a part of the
training process and this is again
nothing new in deep learning this was
there in machine learning as well so you
feed the test images and then find out
whether we are getting a similar
accuracy or not so maybe that accuracy
May reduce a little bit while training
you may get 98% and then for test you
may get 95% but there shouldn't be a
drastic drop like for example you get
98% in training and then you get 50% or
40% with the test that means your
network has not learned you may have to
retrain your network so that is the way
neural network training works and
remember the whole process is about
changing these weights and biases and
coming up with the optimal values of
these weights and biases so that the
accuracy is the maximum possible all
right so little bit more detail about
how this whole thing works so this is
known as forward propagation which is
the data or the information is going in
the forward Direction the inputs are
taken weighted summation is done bias is
added here and then that is fed to the
activation function and then that is
that comes out as an output so that is
forward propagation and the output is
compared with the actual value and that
will give us the error the difference
between them is the error and in
technical terms that is also known as
our cost function and this is what we
would like to minimize there are
different ways of defining the cost
function but one of the simplest ways is
mean square error so it is nothing but
the square of the difference of the
errors or the sum of the squares of the
difference of the errors and this is
also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
square error now there are different
ways of defining cost function it need
not always be the mean square error but
the most common one is this so you
define this cost function and you ask
the system to minimize this error so we
use what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate of the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate is very small
then it will probably take very long to
train whereas if the learning rate is
very high then it will appear to be
faster but then it will probably never
what is known as converge now what is
Convergence now we are talking about a
few terms here conver convergence is
like this this is a representation of
convergence so the whole idea of
gradient descent is to optimize the cost
function or minimize the cost function
in order to do that we need to represent
the cost function as this curve we need
to come to this minimum value that is
what is known as the minimization of the
cost function now what happens if we
have the learning rate very small is
that it will take very long to come to
this point on the other hand if you have
large Higher Learning rate what will
happen is instead of stopping here it
will cross over because the learning
rate is high and then it has to come
back so it will result in what is known
as like an oscillation so it will never
come to this point which is known as
convergence instead it will go back and
forth so these are known as Hyper
parameters the learning rate and so on
and these have to be those numbers or
those values we can determine typically
using trial and error out of experience
we we try to find out these value so
that is the gradient descent mechanism
to optimize the cost function and that
is what is used to train our neural
network this is another representation
of how the training process works and
here in this example we are trying to
classify these images whether they are
cats or dogs and as you can see actually
each image is fed in each time one image
is fed rather and these values of X1 X2
up to xn are the pixel values within
this image okay so those values are then
taken and for each of those values a
weight is Multiplied and then it goes to
the next layer and then to the next
layer and so on ultimately it comes as
the output layer and it gives an output
as whether it is a dog or a cat remember
the output will never be a named output
so these would be like a zero or a one
and we say Okay zero corresponds to dogs
and one corresponds to cats so that is
the way it typically happens this is a
binary classification we have similar
situations where there can be multiple
classes which means that there will be
multiple more neurons in the output
layer okay so this is once again a quick
representation of how the forward
propagation and the backward propagation
works so the information is going in
this direction which is basically the
forward propagation and at the output
level we find out what is the cost
function the difference is basically
sent back as part of the backward
propagation and gradient descent then
adjust the weights and biases for the
next iteration this happens iteratively
till the cost function is minimized and
that is when we say the whole the
network has converged or the training
process has converged and there can be
situ ations where convergence may not
happen in rare cases but by and large
the network will converge and after
maybe a few iterations it could be tens
of iterations or hundreds of iterations
depending on what exactly the number of
iterations can vary and then we say okay
we are getting a certain accuracy and we
say that is our threshold maybe 90%
accuracy we we stop at that and we say
that the system is trained the train
model is then deployed for production
and so on so that is the way the neural
network training happens okay so that is
the way classification Works in deep
learning using neural network and this
slide is an animation of this whole
process as you can see the forward
propagation the data is going forward
from the input layer to the output layer
and there is an output and the error is
calculated the cost function is
calculated and that is fed back as a
part of backward propagation and that
whole process repeats once again okay so
remember in neural networks the training
process is nothing but the finding the
best values of the weights and biases
for each and every neuron in the network
that's all training of neural network
consists of finding the optimal values
of the weights and biases so that the
accuracy is maximum Star Wars fans would
be familiar with the golden life-sized
Hospital ality robot C3PO while Star
Wars might be set in a galaxy far far
away the reality of having machines talk
and respond to us in a human-like manner
is already a reality which keeps getting
more and more realistic with every
passing day the people you ask for
queries on websites your smart
assistants even calls made over the
internet all of them have one thing in
common none of them are actually human
now you must be thinking if they are not
human how do they manage to sound and
seem so human life how do they respond
to me so intelligently and how are they
so articulate this my friends is the
magic of natural language processing
what is NLP natural language processing
or NLP refers to the branch of
artificial intelligence that gives the
machines the ability to read understand
and derive meaning from Human languages
NLP combines the field of linguistics
and computer science to decipher
language structure and guidelines and to
make models which can comprehend break
down and separate significant details
from text and speech every day humans
interact with each other through public
social media transferring vast
quantities of freely available data to
each other this data is extremely useful
in understanding human behavior and
customer habits data analysts and
machine learning experts utilize this
data to give machines the ability to
mimic human linguistic Behavior this
helps save millions in terms of Manpower
and time as you don't need to always
have a person present at the other end
of a phone NLP is also a lot more
widespread than you may realize you use
it every day in seemingly normal and
insignificant situations don't know how
to correctly spell a word autocorrect
has you covered need to see if your
article or thesis will get flagged for
copyright violations that's okay a
plagiarism Checker will search through
the web and find any cases of published
documents which may match your work line
by line while NLP seems really cool yet
a cutting Edge and complicated
technology concept it is actually pretty
easy to learn you start off with a
document or an article to make your
algorithm understand what is going on in
it you need to process it into a form
which is easily comprehensible by the
machine this is no different than making
a child learn to read for the first
time you start off by performing
segmentation which is to break the
entire document down into its
constituent sentences you can do this by
segmenting the article along its
punctuation like full stops and commas
for the algorithm to understand these
sentences we get the words in a sentence
and to explain them individually to our
algorithm so we break down our sentence
into its constituent words and store
them this is called tokenizing where
each word is called a token we can make
the learning process Faster by getting
rid of non-essential Words which do not
add much meaning to our statement and
are just there to make our statement
sound more cohesive these words such as
are and the the are called stop wordss
now that we have the basic form of our
document we need to explain it to our
machine we first start off by explaining
that some words like skipping skips
skipped are the same word with added
prefixes and suffixes this is called
stemming we also identify the base words
for different word tense mood gender Etc
this is called litiz stemming from the
base word LMA now we explain the concept
of nouns verbs articles and other parts
of speech to the Machine by adding these
tags to our words this is called part of
speech tagging next we introduce our
machine to pop culture references and
everyday names by flagging names of
movies important personalities or
locations Etc that may occur in the
document this is called named entity
tagging once we have our base wordss in
tags we use a machine learning algorithm
like naive Bas to teach our model humans
sentiment and speech at the end of the
day most of the techniques used used in
NLP are simple grammar techniques that
we have been taught in school here is a
question for you which of these NLP
techniques is used to obtain words from
sentences a stemming b
tokenization c
litiz d
segmentation give it a thought and leave
your answers in the comment section
below three lucky winners will receive
Amazon gift vouchers with the increasing
demand for automated language Solutions
companies are looking for NLP experts to
join them and are prepared to offer
highly lucrative salaries as well if you
want to learn more about NLP you can
check out Simply learn's postgraduate
program in Ai and machine learning in
collaboration with IBM in this program
you will learn about Frameworks like
Caris and tensorflow and get hands-on
experience in deep learning to become a
truly experienced AI engineer python is
the most widely used programming
language today when it comes to solving
data science tasks and challenges python
never ceases to surprise the its
audience most data scientists out there
are already leveraging the power of
python every day hi I'm AA from Simply
learn and well after some thought and a
bit more research I was finally able to
narrow down my choice of top python
libraries for data science what are they
let's find out so let's talk about this
amazing Library tens oflow which is also
one of my favorites so tensorflow is a
library for high performance numerical
computations with around 35,000 GitHub
comets and a Vibrant Community of around
1,500 contributors and it's used across
various scientific domains it's
basically a framework where we can
Define and run computations which
involves tensors and tensors we can say
are partially defined computational
objects again where they will eventually
produce a value that was about
tensorflow let's talk about the features
of tensor flow so tensorflow is majorly
used in deep learning models and neural
networks where we have other libraries
like to watch and theano also but
tensorflow has hands down better
computational graphical visualizations
when compared to them also tensorflow
reduces the error largely by 50 to 60%
in neural machine translations it's
highly parallel in a way where it can
train multiple neural networks and
multiple gpus for highly efficient and
scalable models this parel Computing
feature of tensorflow is also called
pipelining also tensorflow has the
advantage of seamless performance as
it's ped by Google it has quicker
updates frequent new releases with the
latest of features now let's look at
some applications tensorflow is
extensively used in speech and image
recognition text based applications time
series analysis and forecasting and
various other applications involving
video detection so favorite thing about
tensorflow that it's already popular
among the machine learning community and
most are open to trying it and some of
us are already using it now let's look
at an example of a tensorflow model in
the this example we will not dive deep
into the explanation of the model as it
is beyond the scope of this video so
here we're using amus data set which
consists of images of handwritten digits
handwritten digits can be easily
recognized by building a simple
tensorflow model let's see how when we
visualize our data using matplot lib
Library the inputs will look something
like this then we create our tensorflow
model to create a basic tensorflow model
we need to initialize the variables and
start a session then after training the
model we can validate the data and then
predict the accuracy this model has
predicted 92% accuracy let's see which
is pretty well for this model so that's
all for T of flow if you need to
understand this tutorial in detail then
you can go ahead and watch our deep
learning tutorial from Simply learn as
shown in the right corner interesting
right let's move on to the next library
now let's talk about a common yet a very
powerful python Library called numi numi
is a fundamental package for numerical
competion in Python it stands for
numerical python Pyon as the name
suggests it has around 18,000 comments
on GitHub with an active community of
700 contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numai addresses the slowness problem
partly by providing these
multi-dimensional arrays that we talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now let's talk about
the features of numi it's very easy to
work with large arrays and mattresses
using numi numi fully supports
objectoriented approach for example
coming back to ND array once again it's
a class possessing numerous methods and
attributes ND array provides for larger
and repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods I
always wanted to get rid of loops and
vectorization of numai clearly helps me
with that now let's talk about the
applications of numi numai along with
pandas is extensively used in data
analysis which forms the basis of data
science it helps in creating the
powerful nend dimensional array whenever
we talk about numai the mention of the
array we cannot do it without di mention
of the powerful end dimensional array
also NBI is extensively used in machine
learning when we are creating machine
learning models as in where it forms the
base of other libraries like ccii pyit
learn Etc when you start creating the
machine learning models in data science
you will realize that all the models
will have their Bas as numai or pandas
also when numai is used with scipi and
matte plot lip it can be used as a
replacement of matad lab now let's look
at a simple example of an array in numai
as you can see here there are multiple
array manipulation routines like there
are basic examples where you can copy
the values from one array to another we
can give a new shape to an array from
maybe onedimensional to we can make it
as a two-dimensional array we can return
a copy of the array collapsed into one
dimension now let's look at an example
where this is a Jupiter notebook and we
will just create a basic array and uh
for detailed explanation you can watch
our other videos which Targets on these
explanations of each libraries so first
of all whenever we using any library in
Python we have to import it so now this
NP is the alas which we will be using
let's create a simple
array let's look what is the type of
this
array so this is an ND array type of
array Also let's look what's the shape
of this
array so this is a shape of the array
now here we saw that we can expand the
shape of the
array so this is where you can change
the shape of the array using all those
functions now let's create an array
using arrange functions if I give
arrange 12 it will give me a on day
array of 12 numbers like this now we can
reshape this
array to 3A 4 or we can write it here
itself so this is how arrange function
and the reshape function works for numi
now let's discuss the next Library which
is scipi so this is another free and
open-source python Library extensively
used in data science for high level
competions so this Library as the name
suggest stands for Scientific Python and
it has around 19,000 comits on GitHub
with an active community of 600
contributors it is extensively used for
scientific and Technical computations
also as it extends numai it provides
many userfriendly and efficient routines
for scientific calculations now let's
discuss about some features of scipi so
scipi has this collection of algorithms
and functions which is built on the numi
extension of python secondly it has
various highlevel commands for data
manipulation and visualization also the
ND image function of scipi is very
useful in multi-dimensional image
processing and it includes built-in
functions for solving differential
equations linear algebra and many more
so that was about the features of CPI
now let's discuss its applications so
CPI is used in multi-dimensional image
operations it has functions to read
images from disk into num by arrays to
write arrays to disc as images resize
images Etc solving differential
equations forar transforms then
optimization algorithms linear algebra
Etc let's look at a simple example to
learn what kind of functions are there
in CPI here I'm importing the constants
package of CPI Library so in this
package it has all the
constants so here I'm just mentioning C
or H or n Na and this Library already
knows what it has to fetch like speed of
light planks constant Etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science life
cycle yes pandas is the most popular and
widely used python library for data
science along with numai and mat plot
lip the name itself stands for python
data analysis with around 17,000 comits
on GitHub and an active community of
1200 contributors it is heavily used for
data analysis and cleaning as it
provides fast flexible data structures
like data frames cies which are designed
to work with structured data very easily
and intuitively now let's talk about
some features of pandas so pandas offers
this eloquent syntax and Rich
functionalities like there are various
methods in pandas like Drop na fill na
which gives you the freedom to deal with
missing data also panas provides a
powerful apply function which lets you
create your own function and run it
across a series of data now forget about
writing those four Loops while using
pandas also this library's highlevel
abstraction over lowlevel numpy which is
written in pure C then it also contains
these high level data structures and
manipulation tools which makes it very
easy to work with pandas like there data
structures and series now let's discuss
the applications of pandas so pandas is
extensively used in general data
wrangling and data cleaning then pandas
also Finds Its usage in ETL jobs for
data transformation and data storage as
it has excellent support for loading CSV
files into its data frame format then
pandas is used in a variety of academic
and Commercial domains including
statistics Finance Neuroscience conomics
web analytics Etc then pandas is also
very useful in Time series specific
functionality like date range generation
moving window linear regression date
shifting Etc now let's look at a very
simple example of how how to create a
data frame so data frame is a very
useful data structure in pandas and it
has very powerful functionalities so
here I'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
I'm using Jupiter notebook again and in
this before using pandas here I'm
importing the Panda's Library let me go
and run this so in data frame we can
import a file a CSV file Excel file so
there are many functions doing these
things and uh we can also create our own
data and put it into Data frame so here
I am taking random data and putting in a
data frame also I'm creating an index
and then also giving the column names so
PD is the alas we've given for pandas
random data of 6x4 index which is taking
a range six numbers and column name I'm
giving as ABCD now let's go ahead and
look at it so here it has created a data
frame with my column names ABCD my list
as six numbers 0 to 5 and a random data
of
6x4 so data frame is just another table
with rows and columns where you can do
various functions over it also I can go
ahead and describe this data frame to
see so it's giving me all these
functionalities where count and mean and
standard deviation Etc okay so that was
about panda now let's talk about next
library and the last one so matplot Li
for me is the most fun Library out of
all of them why cuz it has such powerful
yet beautiful visualizations we'll see
in the coming slides plot and mat plot
lip suggest that it's a plotting library
for python it has around 26,000 comets
on GitHub and a very Vibrant Community
of 700 contributors and because of such
graphs and plots that it produces it's
majorly used for data visualization and
also because it provides an
objectoriented API which can be used to
embed those plots into our applications
let's talk about the features of matplot
lip the pyplot module of matplot lip
provides Matlab like interface so
matplot lip is designed to be as usable
as matlb with an advantage of being free
and open source also it supports dozens
of backends and output types which means
you can use it regardless of which
operating system you're using or which
output format you wish pandas itself can
be used as wrappers around m BL lips API
so as to drive mat BL Li via cleaner and
more modern apis also when you start
using this Library you will realize that
it has a very little memory consumption
and a very good runtime Behavior now
let's talk about the applications of
matte blot lip it's important to
discover the unknown relationship
between the variables in your data set
so this Library helps to visualize the
correlation analysis of variables also
in machine learning we can visualize 95%
confidence interval of the model just to
communicate how well our model fits the
data then mat mod Finds Its application
and outly detection using scatter plot
Etc and to visualize the distribution of
data to gain instant insights now let's
make a very simple plot to get a basic
idea I've already imported the libraries
here so this function matte plot lip in
line will help you show the plots in the
Jupiter notebook this is also called a
magic function I won't be able to
display my plots in the jupyter notebook
if I don't use this function I using
this function in uh nump to fix random
state for reproducibility now I'll take
my n as 30 and we'll assign random
values to my variables so this function
is generating 30 random numbers here I'm
trying to create a scatter plot so I
want to decide the area let's put this
so just multiplying 30 with random
numbers to the^ two so that we get the
area of the plot which we will see in
just a minute so using the scatter
function and the areas of mat plot lip
as PLT I've created this if I don't use
this and I have very small circles as my
scatter plot it's colorful it's nice so
that's one very easy plot I suggest that
you Explore More of matte plot lib and
I'm sure you will enjoy it let's create
a histogram so I'm using my the style is
gigg plot and assigning some values to
these variables any random
values now we are assigning bars and
colors and Alignment to the plot and
here we get the graph so we can create
different type of visualizations and
plots and then work upon them using
matplot lip and it's just that simple so
that was about the leading python
libraries in the field of data science
but along with these libraries data
scientists are also leveraging the power
of some other useful libraries for
example like tens of flow kasas is
another popular Library which is
extensively used for deep learning and
neural network modules k WS both
tensorflow and theano backends so it is
a good option if you don't want to dive
into details of tensorflow then psychic
learn is a machine learning library it
provides almost all the machine learning
algorithms that you need and it is
designed to interpolate with numai and
CI then we have caborn which is another
library for data visualization we can
say that caborn is an enhancement of
matplot lib as it introduces additional
plot types if you are an aspiring data
scientist who's looking out for online
training and certific ation in data
science from the best universities and
Industry experts then search no more
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below let's now start
this lesson by defining what data
visualization is data visualization is
the technique to present the data in a
pictorial or graphical format it enables
stakeholders and decision makers to
analyze data visually the data in
graphical format allows them to identify
new trends and patterns easily well you
might think why data visualization is
important let's explain with an example
you are a sales manager in a leading
Global organization the organization
plans to study the sales details of each
product across all regions and countries
this is to identify the product which
has the highest sales in a particular
region
and up the production This research will
enable the organization to increase the
manufacturing of that product in the
particular region the data involved for
This research might be huge and complex
the research on this large numeric data
is difficult and timec consuming when it
is performed
manually when this numeric data is
plotted on a graph or converted to
charts it's easy to identify the
patterns and predict the result
accurately the main benefits of of data
visualization are as follows it
simplifies the complex quantitative
information it helps analyze and explore
Big Data easily it identifies the areas
that need attention or Improvement it
identifies the relationship between data
points and variables it explores new
patterns and reveals hidden patterns in
the
data there are three major
considerations for data visualization
they are Clarity accuracy and efficiency
first ensure the data set is complete
and relevant this enables the data
scientist to use the new patterns yield
from the data in the relevant places
second ensure using appropriate
graphical representation to convey the
right message third use efficient
visualization technique which highlights
all the data points there are some basic
factors that one would need to be aware
of before visualizing the data visual
effect coordination system system data
types and scale informative
interpretation visual effect includes
the usage of appropriate shapes colors
and size to represent the analyzed data
the coordinate system helps to organize
the data points within the provided
coordinates the data types and scale
choose the type of data such as numeric
or
categorical the informative
interpretation helps create visuals in
an effective and easily interpretable
manner using labels title Legends and
pointers so far you have learned what
data visualization is and how it helps
interpret results with large and complex
data with the help of the Python
programming language you can perform
this data
visualization you'll learn more about
how to visualize data using the Python
programming language in the subsequent
screens many new python data
visualization libraries are introduced
recently such as Matt plot Library vispy
bokeh Seaborn pigle folium and networks
the matte plot library has emerged as
the main data visualization Library
let's now learn about this matte plot
library in
detail Matt plot library is a python
two-dimensional plotting library for
data visualization and creating
interactive Graphics or plots using
Python's matte plot Library the data
visualization of large and complex data
becomes easy there are several
advantages of using matte plot library
to visualize data they are as follows
it's a multi-platform data visualization
tool built on the numpy and scipi
framework therefore it's fast and
efficient it possesses the ability to
work well with many operating systems
and graphic
backends it possesses high quality
graphics and plots to print and view for
a range of graphs such as histograms bar
charts pie charts Scatter Plots and heat
maps with Jupiter notebook integration
the developers have been free to spend
their time implementing features rather
than struggling with cross-platform
compatibility it has large community
support and crossplatform support as it
is an open-source tool it has full
control over graph or plot Styles such
as line properties fonts and AIS
properties let's now try to understand a
plot a plot is a graphical
representation of data which shows
relationship between two variables or
the distribution of data look at the
example shown on the screen this is a
two-dimensional line plot of the random
numbers on the y- AIS and the range on
the
xaxis the background of the plot is
called grid the text first plot denotes
the title of the plot and text line one
denotes the
legend you can create a plot using four
simple steps import the required
libraries Define or import the required
data set set the plot parameters display
the created
plot let's consider the same example
plot used earlier follow the steps below
to obtain a this
plot the first step is to import the
required libraries here we have imported
numpy and Pi plot and style from Matt
plot library numpy is used to generate
the random numbers and Pi plot which is
built in Python library is used to plot
numbers and style class is used for
setting the grid style Matt plot Library
inline is required to display the plot
within Jupiter
notebook the second step is to Define or
import the required data set here we
have defined the data set random number
using numpy random method note that the
range is 10 we have used the print
method to view the created random
numbers the third step is to set the
plot parameters in this step we set the
style of the plot labels of the
coordinates title of the plot The Legend
and the line width in this example we
have used ggplot as the plot Style the
plot method is used to plot the graph
against the random numbers in the plot
method the word g denotes the plot line
color as green label denotes the legend
label and it's named as line one also
the line width is set to two note that
we have labeled the x-axis as range and
the Y AIS as labels and set the title as
first
plot the last step is to display the
created plot use the legend method to
plot the graph based on the set
conditions and the show method to
display the created
plot let's now learn how to create a
two-dimensional plot consider the
following example a neutri worldwide
firm wants to know how many people visit
its website at a particular time this
analysis helps it control and monitor
the website traffic this example
involves two variables namely users and
time therefore this is a two-dimensional
or 2D plant take a look at the program
that creates a 2d plant object web
customers is a list on the number of
users and time hours indicates the time
from this we understand that there are
123 customers on the website at 7:00
a.m. 645 customers on the website at
8:00 a.m. and so on the GG plot is used
to set the grid style and the plot
method is used to plot the website
customers Against Time don't forget to
Matt plot library in line to display or
view the plot on the Jupiter
notebook the website traffic curve is
plotted and the graph is shown on the
screen it's also possible to change the
line style of the plot to change the
line style of the plot use Define the
line style as dashed in the plot method
observe the output graph changes to a
dashed line also note that the color is
defined as blue using matte plot Library
it's also possible to set the desired
axis to interpret the required result
use the axis method to set the axis in
this example shown on the screen the
x-axis is set to range from 6.5 to
17.5 and the y- AIS is set to range from
50 to 2,000 let's Now understand how to
set the transparency level of the line
and to annotate a plot Alpha is an
attribute which controls the
transparency of the line lower the alpha
value more transparent the line here the
alpha value is defined as
0.4 the annotate method is used to
annotate the graph the Syntax for
annotate method is shown on the screen
the keyword Max is the attribute that
denotes The annotation text ha indicates
the horizontal alignment VA indicates
the vertical alignment XY text indicates
the text position and XY indicates the
arrow position the keyword Arrow props
indicates the properties of the arrow in
this example the arrow property is
defined as the green color the output
graph is shown on the screen so far
you've learned how to set line width
title x-axis and y- AIS label title of
the plot Legend line color and annotate
the graph for a single plot the plot we
created for website traffic in the
previous screens is for only one day
let's now learn how to create multiple
plots say for 3 Days using the same
example the data set number of user for
Monday Tuesday and Wednesday is defined
with respect to its time
distribution use different color and
line width for each day to distinguish
the plot in this example we have used
red for Monday green for Tuesday and
blue for Wednesday the output graph is
shown on the screen
a subplot is used to display multiple
plots in the same window with a subplot
you can arrange plots in a regular grid
all you need to do is specify the number
of rows columns and plot the Syntax for
subplot is shown on the screen it
divides the current window into an M
byin grid and creates an axis for a
subplot in the position specified by P
for example subplot
212 creates two subplots which are
stacked vertically on a grid if you want
to plot four graphs in one window then
the syntax used should be subplot 2 4
layout and spacing adjustment are two
important factors to be considered while
creating subplots use PLT subplots
adjust method with the parameters h
space and w space to adjust the
distances between the subplot and move
them around on the
grid in this demo you can see how to
create two subplots that will display
side by side in a single frame two
subplots stacked one on top of the other
or vertically split in a single frame
and four subplots displayed in a single
frame first import matte plot lib P plot
and
style Ty type percentage matplot lib in
line to view the plot in Jupiter
notebook Define the parameters such as
temperature wind humidity precipitation
data and time
data you can see the data being typed
here next to create two subplots to be
displayed side by side in a given frame
for 1 121 and one 122 specify the figure
size subplot space title the color for
time and temperature data which is blue
here and line style and
width
similarly specify the color for wind
which is red its line style and
width
you can see the temperature and wind
subplot charts displayed side by side in
a given frame here to create subplots
for 211 and
212 specify the
parameters
this will create two subplots stacked
one on top of the other or vertically
split in a given
frame let's use humidity and
precipitation data to plot the
graphs specify the title color line
style and line width for both the graphs
you can see the two sub plots stacked
one on top of the other with two
different colors indicating
precipitation and humidity here the two
graphs are
separate finally let's draw four
subplots for 2 2 1 2
22 2 2 3 and
224 that will display in a given
frame
specify the title subplot data color
line style and line width for all four
subplots
you can see the four subplots displayed
in a single
frame in this demo you learned how to
create subplots displayed side by side
vertically split subplots and four
subplots displayed in a single frame
using matplot
Liv you can create different types of
plots using matplot Library histogram
scatter plot heat map High chart error
bar
histograms histograms are graphical
representations of a probability
distribution in fact a histogram is a
kind of bar chart using Matt plot
library and its bar chart function you
can create histogram charts a histogram
chart has several advantages some of
them are as follows it displays the
number of values within a specified
interval it's suitable for large data
sets as they can be grouped within the
intervals Scatter Plots a scatter plot
is used to graphically display the
relationship between variables a basic
plot can be cre created using the plot
method however if you need more control
of a plot it's recommended that you use
the scatter method provided by Matt plot
Library it has several advantages it
shows the correlation between variables
it's suitable for large data sets it's
easy to find clusters it's possible to
represent each piece of data as a point
on the plot in this demo you'll learn
how to generate a histogram and scanner
plot using matplot lib
let's import a data set called Boston
data set which we will use to create the
histogram and scanner plot from the scit
learn
Library let's import matplot lib P
plot typ percentage mat plot lib in line
to view the plot in Jupiter
notebook let's use the data in Boston
real estate data set to create the
histogram and scanner plot load this
data you can view this data by using the
print
command
Now define the x-axis for the data which
is Boston real estate
data likewise Define the y- AIS for the
data which is Boston real estate data
with the target
extension specify the plot style figure
style number of bins and labels of the
x-axis and Y AIS use the show method to
display the histogram created by
you
specify the style size data sets and
labels of the scatter plot that you want
to create use the show method to display
the scatter plot created by
you
hey heat Maps a heat map is a better way
to visualize two-dimensional data using
heat maps you can gain deeper and
quicker insight into Data than those
afforded by other types of plots it has
several advantages it draws attention to
the risky prone area it uses the entire
data set to draw bigger and more
meaningful
insights it's used for cluster analysis
and can deal with large data
sets
in this demonstration you'll learn how
to generate a heat map for a data set
using matplot
lib let's import the required libraries
matplot lib pip plot and
caborn type percentage plot lib inline
to view the plot in Jupiter
notebook let's load the flights data set
from the built-in data sets of Seaborn
Library use head to view the top five
records of the data
set we have to arrange the columns to
generate the heat map let's use use the
pivot method to arrange the columns
month year and
passengers let's view the flight data
set that's now ready to generate the
heat
map let's use the heat map method and
pass light data as an
argument
this will generate the heat map which
you can see here in this demo you
learned how to create and display a heat
map pie charts pie charts are typically
used to show percentage or proportional
data Note that usually the percentage
represented by each category is provided
next to the corresponding slice of the
pie Matt plot Library provides the pi
method to make Pi charts it has several
advantages it summarizes a large data
set in visual form it displays the
relative proportions of multiple classes
of
data the size of the circle is made
proportional to the total
quantity in this demonstration you'll
learn how to create a pie chart and
display
it first import mat plot lib pip
plot typeint percentage matplot lib in
line to view the plot in Jupiter
notebook type the job data within
parentheses using single quotes
separated by
commas specify the labels as it Finance
marketing admin HR and
operations
specify the slice it to
explode use the show method to display
the pie
chart
you can see the pie chart with the
slices labels and it the largest
slice error bars an error bar is used to
show the graphical representation of the
variability of data it's used mainly to
point out errors it builds confidence
about the data analysis by unleashing
the statistical differences between the
two groups of data it has several
advantages it shows the variability in
data and indicates the errors it depicts
the Precision in the data analysis it
demonstrates how well a function and
model are used in the data analysis it
defines the underlying data Seaborn is a
python visualization Library based on
Matt plot Library it provides a
highlevel interface for drawing
attractive statistical Graphics it was
originally developed at Stanford
University and is widely used for
plotting and visualizing data there are
several advantages it possesses built-in
themes for better visualizations it has
tools built-in statistical functions
which reveal hidden patterns in the data
set it has functions to visualize
matrices of data which become very
important when visualizing large data
sets now in order to learn tblo the
basic first step is to import a sample
data so in our case what we have done is
we have imported a sample Superstore
which is in Excel format a sample
superstore. excl which has three
worksheets in it orders people and
returns so by importing this data into
tabl first of all we will create
relationships between these sheets in
order to identify who all have placed
orders and how many people have returned
the orders we will do some analysis is
on the orders placed by certain set of
people and Order returned by certain set
of people now as we have imported uh the
sheet we will make certain joins so the
first step is to drag the orders table
the order sheet on the relationship
canvas here okay and you can see the
data sample data the first 100 rows over
here okay then now we need to create an
inner join with people's table between
order and people okay so if you
see it has automatically detected the
field names on which the inner joint has
to be created so on the order side you
have region and on the right hand side
which is the people data you have also a
region okay so both these columns are
common and that's how we have made a
join between orders and people
data so if I close this box and go and
check the people's
data
open
so see the region and the person these
two columns from the people table have
now been joined with the orders table
right so
it means that these are the orders in a
particular region which has been placed
by in this
region let me show you the sample
Superstore Excel file now this is the
structure of the file you have a sample
list of transactions basically the
orders which are placed by
customers across multiple regions south
west of USA South Region west region
then you have a list of order IDs which
have been returned so
basically the order ID in the returns
sheet matches with those orders in the
orders table and then you have the
people uh sheet in which you have
region and a person associated with that
region the sales person associated with
that region okay so basically when we
are combining joining orders with people
we are joining
that which
orders belongs to which region and who's
the salesperson associated with it so
what we have done over here is we have
have made a inner join means all the
orders should belong to particular
region and that region is in the
people's sheet and then in the second
step now we will make a left join
between returns and orders not in a join
we'll make a left join between returns
and orders and we will make a join using
the order
ID
okay so just edit
this click on left and select order ID
as the join column now what does left
join means left join means is that
consider all the orders from the orders
table and only consider the orders from
the returns table which have data means
which are returned otherwise show null
for the order IDs which are not returned
so if you see this is the these are the
two columns from the returns table and
these are null because this is relevant
to the order IDs which are not returned
okay which has been accepted by the
customer but these are the orders for
which you see data in the returned and
Order column it means that these have
been return now with these joins in
place please save your book and now we
have our relations created in the
um in the tblo now we are ready to
create certain reports and extract
certain kpis using this relationship
model now we'll move to sheet
one
okay and first we will
place
State and and person on the rose sheet
okay then I'll go to
my numbers
and put the profit or D so per state per
person how much profit I am making as a
company okay this is my goal to check
now sought by highest to lowest so
California is giving me the maximum
profit of 76,6 381 then New York then
Washington so this is the sorted order
in which I
have listed my profit in descending
order now I can also check what are the
number of
orders
placed
and check the distinct
count so out of 120 out of the total
orders of 127 okay so this is the number
of total number of orders which have
been returned for California is 127
it's 16 29 so the sorted order is as per
the revenue as per the profit and this
is the details of the orders which have
been returned per state so if you see
for conc tiut for a canas there are zero
returns
so you can also extract
data table to refresh his orders and
identify new rows using order date so as
in when new data is being added you can
refresh it now say
extract and now you can save this
information
profit by
state and click save so this is the
extraction of this particular report
which is possible in
table so this is the first exercise
which we have completed
for reviewing and analyzing the profit
per state highest to lowest and within
that per state what are the number of
orders which have been returned by all
the customers the distinct count of
order IDs which have been
returned now let's start our second
exercise on creating calculated fields
in tblo now in this exercise we will be
doing
certain activities like we will be
creating a set to show the states which
have more than 100 customers then we
will be creating a calculated field to
show an average sales per customer okay
then we will create a calculated field
to show the sales goals and then show
emerging and developing stage so these
are the four kpis which we have to
derive now the first thing we have our
sample super store data already imported
and the relationships created inner join
with people and left join with
returns now we have our sheet two in
which we will create the uh States a
list of states which has more than 100
customers so what we have to do is we
have to click right click on the
customer name and click create
set
okay now we have to give the name as
states with 100 plus
customers and then go to the condition
tab select by field and then apply
condition as
count of customer
name greater than equal
to 100 and click
okay now we have this set created states
with 100 plus
customers now to determine average sales
by customer we have to now create a
calculated field so go to the analysis
and click on create calculate
field
okay now name it as
average sales per
customer and now we will say
average we will use
a
okay so we are saying that per customer
we are using a level of definition
function include which means that per
customer what is my average sales right
we've already used a function aggregated
function called average so we are saying
per customer give me the total and then
give me the average per customer so
we're going to click
okay now create another calculated field
you can also create from here and name
is as name it as sales
goal now in this we are going to type
the formula if
minimum
states with 100 plus customers equal to
true
then sum of
sales into
1.3
else average sales per customer
into 100 so we we are saying
that if the customer belongs to the set
of states with 100 plus customers then
the sales Target should be 1.3 times the
actual sales as of today else it should
be 100% of the average sales per
customer now let's create another
calculated field which we call as
emerging or developing
state if distinct
count of customer
name is greater than equal to
100
then the state is tagged as developing
state
else it is called
as emerging
State okay so we have now three
calculated Fields average sales per
customer emerging or developing State
and sales goals now we will use this in
our reporting so we will drag sales goal
under the
columns and then I'll drop my state so
now this is the statewise sales goal
depending whether the state has 100 plus
customers or
not then add your customer
name make the measure as count
distinct and make it as
distri okay so if you see
this we have the count of customers per
state and the the sales goal for that
particular
State and
now I'll put my
sum of sales the total sales which I
want per which is there per
state now go to show me and
select this particular chart bullet
graph
now to bring sales goals to column right
click on the sales AIS and select swap
reference line
Fields now now from the your left hand
panel Dragon drop emerging or developing
straight on the color
panel okay so emerging state is the
orange one and the developing state is
the blue
one and save the sheet as developing and
the emerging
States so if you see this it's an
emerging State because it's count is
less than the customer count is less
than
100 its sales goal is
57384 but the actual sales is 1
19511
okay so now this is a developing State
its count is greater than equal to 100
and it sales goal and its sales is
exactly the same it matches so that's
why you are saying the bar and the blue
bar is ending exactly where the vertical
bar
is so what we are trying to depict is
that whether the a state is going Beyond
its Target sales goal or it's behind it
and you can see that using this
particular vertical bar like for example
Michigan its sales goal is $719 52 but
its actual sales is
76270 average sales so that's why it is
be above its
Target and it's a developing
State because it has more than 100
customers so you can even
sort by the count of the uh customers
higher to lower so all your developing
state will group from at the top and the
emerging States Will Group at the
bottom or you can sort
by the sales
goal so the orange bar is the sales goal
or the blue bar so depending what sales
goal is
being derived for each
state let's go ahead and take a look at
uh building a resume always exciting
putting ourselves out selling ourselves
and if you looked at some of our other
videos dealing with resums you'll see a
trend here uh the top part is so
different than what rums were in the90s
uh in the 2000 to 2010 they've evolved
they've really evolved it used to be
20120 maybe LinkedIn maybe one other
reference now you're going to going to
see that we want those references this
is a sales tactic which now has come
into resums used to be that if you're a
real estate agent every real estate
agent I I knew I used to deal with real
estate software for uh the real estate
industry back in the 90s every real
estate agent won their picture on their
business card they wanted them their
picture if they could put on their
contracts they wanted people to see the
face so that's really a big change is to
make sure that it stands out you stand
out here's a picture of somebody so
you're more than just a couple letters
and a name name and of course you need
your contact information should always
be at the top uh you have your summary
what are you focusing on be a little
careful with your summary because if you
have everything in your summary and then
they scroll down to experience and
education and skills they're going to
stop the second you repeat yourself in
your resume that usually means to the
reader hey this person doesn't have
anything more to offer me I'm done uh so
be a little careful how you word your
summary most companies appreciate when
you come in here and you've adjusted
this summary to be both about you and
how you're going to serve that company
so it's worth researching that company
to find out how those connect and put
that in the summary take some time to do
that that's that's actually a pretty big
deal and the references are huge also
especially in data science uh when
you're talking about any of the
programming or data science data
analytics having a place to go where
they can look it up and scroll down and
see different things you're doing
whether it's LinkedIn in this case which
is the business profile most commonly
used GitHub where have stuff published
Facebook I'm always hesitant because
that tends to push more towards uh
social media type jobs and other jobs U
but certainly there's people who have
Facebook um uh who do marketing and
stuff like that but these links having
these basic links here is important uh
people are starting to look for that for
some other uh setup maybe you have a
personal website this is a good place to
put that so that they now have a
multitude of links that go back to you
and highlight who you are and then the
next part or next four parts so for the
next four parts we have a combination of
experience education skills
certifications and you can see they're
organized if you have you know a lot of
people like to see what kind of degree
you have they want to know where it came
from and if you just got out of college
you're going to put education at the top
and then maybe you'll put skills after
that and then your experience at the
bottom if you've been in the field for
years um you know my degree just give
you my age goes back to the N early '90s
so I usually put education at the very
bottom and then because a lot of the
stuff I'm trying to sell myself on right
now is my skills I actually put that at
the top and I'll put my education my
certifications at the bottom my skills
and then my experience is since it's a
huge part of my resume goes next you can
organize these in whatever order you
want that's going to work best for you
and sell you so remember you're selling
yourself this is your image probably
don't wear
uh an old tie-dye T-shirt with holes in
it you know something nice CU it is
professional uh and of course your
summary you're and then what do you have
to offer the company and again when I
put out resums I haven't done a resume
in a while you go in there and you can
take this and reorganize this so if the
company's looking for something specific
you might put the experiences specific
to that company um you might even take
experience uh if you have like a long
job history like I do I've gone into a
lot of different things you might leave
out those companies or those experiences
it had nothing to do with data science
because it just becomes overwhelming
resume should only take about 30 seconds
to glance over maybe a minute tops
because after that point you've lost the
person's interest and if they want to
dig deeper they now have links they have
your website they have LinkedIn and they
can now take this and they come back to
it and they go okay let's look at this
person a little closer so quick overview
this is your sales sheet selling you to
the company uh so always tie it to the
company so that you have that what am I
going to give this company what are they
going to get from me if you are an
aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box
below before we dive in and start going
through the questions one at a time
we're going to start with some of the
logical kind of concept that's enters in
a lot of interviews in this one you have
two buckets one of 3 l and the other of
5 L you're expected to measure exactly 4
L how will you complete the task and not
you only have the two buckets you don't
have a third bucket or anything like
that just the two buckets and the object
of the question like this is to see how
well you are thinking outside the box in
this case you're in a larger box you
have two buckets and also the pattern
which you go on and what that means is
if you look at the two buckets and we'll
show you their answer in just a second
you have a bucket with 3 l and a bucket
with 5 L and the first thought is what
happens if you go from left to right so
we have a direction and what happens if
you pour the 3 l into the 5 L bucket
well if you pour the 3 l into the 5 L
bucket you have an empty bucket of 3 l
and what's really important here is out
thinking outside the box you realize
that you have a 5 L bucket that has
three lers in it and two empty liters so
you have two additional lers you can
fill up if we continue that process we
can comp pour from the left to right
from the small bucket to the large
bucket you can now measure into
additional liters into the 5 L bucket
and 3 - 2 is 1 and you can keep doing
that you can empty the 5 L bucket in
pour those 3 l in that one liter in and
then you can pour 3 lers in what's cool
about these questions as you explore
them is you realize there's multiple
ways usually to solve them I went from
small bucket to big bucket the simply
learned team their solution that they
pulled out was you fill the 5 L bucket
and empty it into the 3 l bucket now
you're left with 2 L in the 5 L bucket
so that's great we can empty the 3 l
bucket so now we're going from large to
small remember we went from small to
large so you can go both either way but
you have to go one way or the other it
turns out and you can empty the 3 l
bucket and pour the contents of the 5 L
bucket in it so the 3 l bucket now has 2
L and if it has 2 L that means it has an
empty one lit and by now you probably
have guessed that if you have an that
empty space you can start using that
empty space of 1 lit as a measuring so
we fill the 5 L bucket again and we pour
the water in the 3 l bucket it already
has the 2 L and so we're only pouring
one liter in there and 5 - 1 is 4 so
interview questions they break up into
all kinds of different patterns we have
logic like this one which is a lot of
fun we have questions that come up that
are more vocabulary list the difference
between supervised and unsupervised
learning probably one of the fundamental
breakdowns in uh data science and
supervised learning uses no one in
labeled data as input supervised
learning has a feedback mechanism most
commonly used supervised learning
algorithms are decision tree logistic
regression support Vector machine and
you should know that those are probably
the most common used right now and there
certainly are so many coming out so
that's a very evolving thing and be
aware of a lot of the different
algorithms that are out there outside of
the deep learning because a lot of these
work faster on Raw data and numbers than
they do than a deep neural network would
unsupervised learning uses unlabeled
data as input unsupervised learning has
no feedback mechanism most commonly used
unsupervised learning algorithms are K
means clustering hierarchial clustering
the aoral algorithm and there certainly
are more um I'm going to say k means
definitely is at the top of the list and
the hierarchial clustering those two are
used so many times so really important
to understand what those are and how
they're used and most important is
understand that supervised learning is
you have your data set where you have
training data and you have um all those
different pieces moving around but you
you're able to train it you know the
answers and unsupervised we're just
grouping things together that look like
they go together how is logistic
regression done logistic regression
measures the relationship between the
dependent variable our label what we
want to predict and the one or more
independent variables are features by
estimating probability using its
underlying logistic function sigmoid and
whenever I draw these charts I always
end up drawing them the right hand side
first because you want to know what your
output is what is you want out of here
and the left hand side what do you have
going in so you have your in and out you
can see we have a nice labeled image
here to help you remember this we have
our inputs we have our linear model we
have our probabilities what are the
probabilities of it being a certain way
based on these features coming in the
sigmoid function and it's important to
note that the sigmoid function is maybe
the most commonly used but it's only one
of a number of functions that are out
there and the sigmoid function turns our
probabilities into a value between 0o
and one or very close to zero and very
close to one between 0.1 and 9 and based
on that we generate an answer in this
case is z or 1 how is logistic
regression done so last time we talked
about the sigmoid function generally
depending on what's your interview and
level of math and an expertise you're
going in for the market you'll have to
understand that formula of the
probability equal 1/ 1 + e the- y and
that's e to the base 2 so you have your
probability function or your sigmoid
function which pushes it um as you can
see we have a nice visual of that and
that helps a lot to have that visual on
the sigmoid function you definitely
should know your y = m * x + C your base
eidan geometry of forming a line and the
slope plus the um intercept the Y
intercept and then you have your natural
log and the natural log is to the e as
opposed to a base 2 or base 10 so your
natural log to the E of the probability
over 1 minus a probability equals your M
* x + C or your ukian line that helps a
lot as far as the graphing and
understanding the sigmoid function so
we'll just keep pushing on to question
number three explain the steps in making
a decision tree and I know notice last
time we brought up the decision tree and
the forest a lot of questions came up
what is the difference so let's go
through that when you make a decision
tree you're going to take the entire
data set as input you're going to
calculate entropy of the target variable
as well as the predictor attributes and
remember entropy is just how chaotic is
it so if you have like you know banana
and grapes and oranges if you're mixing
in fruit and that's your data coming in
you have all these different objects
that are so separate from each other and
the more they become uniform the lower
the entropy and we call that information
gain so we gain information on sorting
different objects from each other so you
have your entropy you have to calculate
your information gain of all attributes
and then you choose the attribute with
the highest Information Gain as the root
node so if you can separate your group
and each group chaos and each group is
uh lowered whichever split lowers the
chaos the most that's where you split it
and that's your root node at that point
you repeat the same procedure on every
Branch till the decision node of each
branch finalized so understanding that
setup is pretty important important as
far as decision trees and you can see
here we have a nice visual of decision
tree for example if you want to build a
decision tree to decide whether we
should accept or decline a job offer
since these are interview questions
that's a good one to ask and just as a
tip you should be pretty aware of the
formula for entropy and Information Gain
so you need to look those up if you
don't remember those and the salary if
it's greater than 50,000 no decline the
offer yes it's got a good salary the
commute is greater than an hour yes
decline the offer no offers incentives
yes accept the offer no incentives
decline the offer so we use decision
tree pretty much for everything if you
want and if you have a decision tree
then you also should understand how do
you build a random forest model and
remember that a a random Forest is
buildup of a number of decision trees so
if you split your data up into a lot of
different packages and you do a decision
Tree on each of those different groups
of data the random Forest is bringing
all those trees together so how do you
build a random forest model randomly
select K features from a total of of M
features where K is less than M among
the K features calculate the node D
using the best split Point split the
node into daughter nodes using the best
split repeat steps two and three steps
until Leaf nodes are finalized build
Force by repeating steps 1 to four for
in number times to create in number of
trees so you can see it's got the same
build pattern as the tree but instead
you're building a number of different
trees little small trees so it all have
an end Leaf note random Forest has a
vote at the end and whoever gets the
most votes wins that's the answer how
can you avoid overfitting of your model
very important question in any kind of
mathematical scientific data science
setup in any of them there are three
main methods to avoid overfitting and
you should really understand overfitting
overfitting means that your model is
only set for a very small amount of data
and ignores the bigger picture keep the
model simple take into account fewer
variables thereby removing some of the
noise in the training data good advice
for any programming at all use cross
cross validation techniques such as K
folds cross validation use
regularization techniques such as lasso
that penalize certain model parameters
if they're likely to cause overfitting
and you should also be well aware that
your cross validation techniques that's
like a pre- DAT or your lasso and your
regularization techniques are usually
during the process so when you're
prepping your data that's when you're
going to do a cross validation such as
like splitting your data into three
groups and you train it on two groups
and test it on one and then switch which
two groups you test it on that kind of
thing so can you solve oop another one
of these I love these things there are
nine balls out of which one ball is
heavy in weight and the rest are of the
same weight and how many minimum
weighings will you find the heavier ball
and when we say weighing think of a
scale where you can put objects on one
side and the other and you can see which
side is heavier and you want to minimize
that you want to split the balls up in
such a way that you're going to do as
few measurements as you can you will
need to perform two wangs so you can get
it down to just two wangs and I always
think if there's nine balls I'm going to
divide them into three groups of three
Place three balls on each side so you
can just randomly pick six of the balls
and three on one side three on the other
and if they balance out both sides are
equal then you know the heavy weight
isn't in any of those so out of the
remaining three balls from step one take
two balls and place one ball on each
side a little tricky there cuz I always
want to put all three I want to put two
on one side and one on the other but no
just take randomly pick two of those put
one on each side if they balance out
then the left out ball the one you
didn't measure will be the heavier one
otherwise you'll see it in the balance
you'll see which one's heavier because
it'll take one of the balls down now we
go to scenario B where they did not
balance out so now we know which side
has a heavier ball in it and it's just
very similar to what we did before if
the balls in Step One do not balance out
then take those three balls that had the
heavier s side on them and reproduce
step two to find out the heavier ball
difference between univariate B variate
and multivariate Analysis and hopefully
if you know a little Latin you'll kick
in there that you have uni and you have
bu and you have multi because the answer
is in the words themselves so the first
one this type of data contains only one
variable so that's a univariate the
purpose of the univariate analysis is to
describe the data and find patterns
exist within it so when you only see one
one variable coming in in this case
we're using height of students you're
limited as far as what you can do with
that data so you can come up and draw
different patterns and conclusions from
those patterns using the means the
median the mode dispersion range minimum
maximum so we're describing the data so
all those words would describe the data
and that's about all you can do with um
data like that there's no correlation
there's nothing to go beyond that as far
as guessing or predicting anything so we
move into by variate you know uni means
one by means two by variate this type of
data involves two different variables
the analysis of this type of data deals
with causes and relationships and the
analysis is done to find out the
relationship among the two variables and
and this is always a favorite one
because everybody loves ice cream in the
summer when it's hot and very few people
go for ice cream in the winter when it's
really cold so it's easy to see the
correlation in the data the temperature
and ice cream cells in summer season and
you can see here where the temperature
goes from 20 to 35 and as a temperature
goes up so does the cells of ice cream
it goes from 2,000 I'm not sure 2,000
whats I'm guessing it's a very large
chain cuz if they're selling 2,000 ice
cream cones and they have a lot of
business good for them you know a little
vendor on the corner selling 2,000 ice
cream cones a day and 3100 the next day
here the relationship is visible from
the table that temperature and cells are
directly proportional to each other so
the hotter the temperature we can
predict an increase in cells so the word
prediction should come up so we have
description and prediction when the data
involves three or more variables it is
categorized under multivariate it is
similar to bivariate but contains more
than one dependent variable in this
example another really common one the
data for house price prediction the
patterns can be studied by drawing
conclusions using mean median in mode
dispersion or range minimum maximum Etc
and so you can start describing the data
that's what all that was and then using
that description to guess what the price
is going to be so this is very good if
you're in the market and you have
already looked at the area and you
already know that a two-bedroom zero
floor 900t house is usually runs about
40,000 you can guess what the next one
that looks similar to it is and I'll
just throw in another word in there I
don't see very often unless you're
really a hardcore data science we talked
about describing the data descriptive we
talked about predictive and there's also
postcript postcript means we're going to
change the variables to try to guess
what the outcome is if we change what's
going on so that would be the next step
but that usually doesn't show up unless
you're dealing with some really hardcore
data science groups what are the feature
selection methods to select the right
variables there are two main methods for
feature selection there's filter methods
and wrapper methods and when you're
filtering your before we discuss the two
methods real quick the best analogy for
U selecting features is bad data in bad
answer out so when we're limiting or
selecting our features it's all about
cleaning up the data coming in so it's
cleaner and is more representative of
what we're trying to predict filter
method filter methods as they come in we
have linear discrimination analysis A
NOA chai squared chai square is probably
the most common one and these are all
part of pre-processing we're taking out
all the outliers all the things that
have a difference that is very different
from the data we're looking at the odd
ones and sometimes you take the odd ones
out and then you analyze them separately
to see why they're odd but remember your
filter methods you want to pull all that
weird stuff out wrapper methods on the
other hand are forward selection
backward selection recursive feature
elimination and one of the most
important things to remember about
wrapper methods is they're very labor
intensive you have to have some pretty
high-end computers if you're doing a lot
of data analysis with the wrapper method
and um just quick quickly forward
selection means you have all your
different features they're off to the
side and we test just one feature at a
time and we keep adding them in until we
get a good fit backward as we have all
the features and we start we run a test
on that to see how well it does and then
we start removing features to see what
works and recursive which is the most
processing hungry algorithm out there
goes through and just recursively looks
through all the different features and
how they pair together but again we have
filter method and wrapper method and
it's important to understand that we're
sorting the data out and finding out
which Fe features are going to represent
the data the best and which ones are not
going to really add any value to our
models all right let's jump number eight
in your choice of language write a
program that prints the numbers from 1
to 50 but for multiples of three print
Fizz instead of the number and for the
multiples of five print Buzz for numbers
which are multiples of both three and
five print Fizz buzz and this really is
testing your knowledge in iterating over
data very important my sister who runs
at the university the data science team
is in charge of their Department it's
the first question she asks in her
interview of anybody who comes in is how
do they iterate through
data so if if this question comes up a
lot and it's very important you have an
understanding and there's actually a
slight error on this code which I'll
point out in just a second the concept
is we have fizzbuzz in range and you
have range 51 which in this case goes
from 0 to 51 and I'm going to challenge
you to see if you can catch the error
and I'll tell you at the endend of the
code where the error is what that means
is that we're going to go through all
the numbers 0 1 2 3 4 and we're going to
process through this Loop if the
remainder fizzbuzz divided 3 equals 0
and fizzbuzz ided 5 also equals 0 then
print fizzbuzz continue and else if
fizzbuzz ID 3 equals 0 then print Fizz
print Fizz continue else if fizzbuzz
divided 5 equal 0 print Buzz continue
and print FS buzz you fit the print the
answer in this case FSB Buzz is either
going to be the number we generated
which is zero or will be the Fizz Buzz
Fizz or Buzz that's a mouthful now if
you didn't catch the error in the code
which is always a fun game find the
error it has to do with the range and
it's important to remember the range
here says range to 51 that's 0 to 51
which is is correct we want to go to 51
because it stops it gets to 50 and it
stops so that's 0 to 50 but if you
remember the question asked from 1 to 50
so the range should be 1 comma 51 not
just 51 which does 0 to 51 in this
particular script in Python you can
leave out the continue but the continue
in this script skips the next lsf so it
doesn't keep processing it going down
and in programming a lot of scripts you
don't need the continue in so would
depend on what script you chose and
there's probably some other ways to do
this it's a lot of fun and you can see
here from the output we end up with fiz
Buzz for zero which shouldn't be there
one two Fizz four Buzz Fizz seven eight
Fizz Buzz 11 Fizz and so on sounds like
a drinking game from my college days so
long ago many decades ago you are given
a data set consisting of variables
having more than 30% missing values how
will you deal with them oh the joy of
messy data coming in ways to handle
missing data values data set is huge we
can just simply remove the rows with
missing data values is the quickest way
I.E we use the rest of the data to
predict the values you just go in there
and say any row of our data that has a
na in it get rid of it that does doesn't
work with smaller data so smaller data
you start running into problems because
you lose a lot of data and so we can
substitute missing values with the mean
or average of the rest of the data using
Panda's data frame in Python there's
different ways to do this obviously in
different languages and even in Python
there's different ways to do this but in
Python it's real easy you can do the DF
do mean so you get the mean value so if
you set mean equal to that then you can
do a DF do fillna with the mean value
very easy to do in a python Panda script
and if you're using python you should
really know pandas and numpy number
Python and pandas data frames for the
given point how will you calculate the
ukian distance in Python so back to our
basic algebra from high school ukian
distance is the line on the triangle and
so if we're given the points plot 1 = 1A
3 plot 2 = 2A 5 we know that from this
we can take the difference of each one
of those points Square them and then
take the square root of everything so
you pan distance equals the Square t of
plot 1 0 - plot 2 of 0 2ar plus plot 1
of 1 - plot 2 of 1 squar mouthful there
and you can remember if you have
multiple Dimensions that go past two
Dimensions you could have plot three can
simply be the distance from plot one you
only need to do one side of that or plot
two you can do either way and square
that and take the square root of that
another mind Bender how to uh calculate
some how to figure out the solution to
something what is the angle between the
hour and minute hands of a clock when
the time is half past 6 so you want to
kind of imagine that clock where the
large hand is pointed down to the 30 and
the other half is going to be right
between the six and the Seven cuz it's
half past 6 there's actually a couple
ways to solve this but let's take a look
and see how they did it note a clock is
a complete circle having
360 in 1 hour the hour hand covers 360
over 12 so it equals 30 for each hour
in 1 minute the minute hand covers 360
over 60 Minutes Or 6 per minute the
minute hand has traveled for 30 minutes
so it has covered 30 * 6 which equal
180 so we know that's 180 from the 12
the hour hand is traveled for 6.5 hours
6 and 1/2 6.5 so it's covered 6.5 * 30
which equal
195 the difference between the two will
give the angle between the two hands
thus the required angle equals 195 - 180
= 15 and this is nice the way they
solved it cuz you can now punch in any
kind of time within reason the hard part
is on the hours is you have to be able
to convert the hours into decimals
explain dimensionality reduction and
list its benefits Dimension reduction
refers to the process of converting a
set of data having vast Dimensions into
Data with lesser Dimensions fills to
convey similar information concisely it
helps in data compressing and reducing
the storage space it reduces computation
time is less Dimensions lead to less
computing it removes redundant features
for example there's no point in storing
a value in two different un units meters
and inches and I certainly run into a
lot with this with text analysis I've
been known to run a text analysis over a
series of documents ends up with over
1.4 million different features that's a
lot of different words being used and if
you do what they call bu connect them
you connect two words together now
you're up to 4.8 million different
features and you start to figure ways to
bring that down what can we get rid of
that kind of thing so you can see where
that can get really high in on
processing and learning how to reduce
the list Dimensions is very important
how will you calculate igen values and
igen vectors of a 3X3 Matrix and what
they're really looking for here is when
you write it out for the igen is that
you know that you're going to use the
Lambda that's the most common one
obviously you can use any symbol you
want but Lambda is usually what they use
and that you do it down the middle
Diagon and so when you take that Matrix
and you take the characteristic equation
you end up with the determinant and
that's the min-2 minus Lambda - 4 2 - 2
1 - Lambda 2 4 2 5 - Lambda and that's
what they're looking for and you know
that's equal to zero so when you're
doing a matrix in the igen setup with
the igen vectors that's all going to
come out equal to zero and then you can
go ahead and write the whole equation
out so we can expand the determinant as
you can see right here the - 2 - Lambda
time it's it's a mouthful I'll leave it
up here for a second so you can look at
it when you break it down into the
algebraic functions you end up with
minus Lambda cubed + 4 Lambda 2 + 27
Lambda - 90 = 0 so now we have a nice
algebraic equation built from the igen
vectors and always remember you can hit
the pause button and you can also send a
note send a note to Simply learn if you
have more questions on vectors or on
this definitely have that resource
available to you um or post down in
below on the um YouTube video comments
and so when we calculate the igen values
and igen vectors of a 3X3 Matrix as we
continue on down the math of this and to
be honest I really don't like working
with matrixes like this it's important
to understand the math behind it and
it's important to know the code just
enough so that you're not lost when
someone's explaining it or it comes up
when I'm working on different data
science models um of course if you're
dealing with a highend math side of it
then you better know this first is by
hit and trial so you try in different
variables to solve for Z and you can
come in here and you'll find that if we
put in the three in there we end up with
the zero at the end and substitute the
three hence we end up with Lambda minus
3 is one of the factors and you can do
the math going out on that where we have
Lambda cubed - 4 Lambda 2 - 27 Lambda +
90 equal Lambda - 3 * Lambda 2 - Lambda
- 30 so I values based on that one are 3
- 5 and 6 and then from there we can
calculate the igen vector 4 Lambda = 3
and you can see here where the Matrix as
we write it out is the - 5 - 4 2 - 2 - 2
2 4 2 2 that was from the beginning put
in the X Y and Z equal 0 0 0 and so when
we put in those numbers and we calculate
them out we have for x = 1 we have the -
5 - 4 y + 2 Z = 0 - 2 - 2 y + 2 Z = 0
and subtracting the two equations we
just had we get 3 + 2 y = 0 y = -3 3/ 2
and Z = -1 /2 that's going back to the
first equation and similarly we can
calculate the igen vectors for -5 and
six how should you maintain your
deployed model ooh distribution time my
favorite I spent 10 years in software
distribution so first thing and this is
true not just of your data science model
but of any computer code going out there
this is basic setup can work although
usually there's a little added steps in
there first we're going to monitor it so
we have a constant monitoring of all the
model is needed to determine the
performance accuracy of the models so
yeah we want to just keep an eye on it
we want to make sure they're accurate we
want to make sure that whatever they're
supposed to predict or I threw in that
bonus word postcript where you change
something and you want to figure out how
your changes are going to affect things
we need to monitor it make sure it's
doing what it's supposed to do
evaluation metrics of the current model
is calculated to determine if new
algorithm is needed and then we compare
it the new models are compared against
each other to determine which model
performs the best and then we do a
rebuild the best performing model is
rebuilt on the current state of data and
this is interesting I found this out
just recently if you're in weather
prediction the really big weather areas
have about seven or eight different
models depending on what's going on and
so you actually have almost a little
Forest going on there where they're like
which model is going to fit best and
this is what we're going to use to
predict a weather with so not only do
you you don't necessarily get rid of the
models but you figure out which models
fit data of what's going on or the
current state of data what are
recommender systems most commonly used
nowadays in marketing so very big
industry understanding recommender
systems predicts the rating or
preference a user would give to a
product and they they're split into two
different areas one is collaborative
filtering and a good example of that is
the Last.fm recommends tracks that are
often played by other users with similar
interests so people who if you're on
Amazon people who bought this also
bought that it's got me a few times and
then there's content based filtering and
we're looking at content it's instead of
looking at who else is listening to the
music these example Pandora which uses
the properties of a song to recommend
music with similar properties so you
have collaborative filtering and
content-based filtering how to find rmse
and MSE in linear regression model
hopefully you remember what the two
acronyms mean cuz that is like half the
answer we have the root mean square
error and the mean square error in
linear regression model so we're looking
for error the rmse and the msse are the
two of the most common measures of
accuracy for a linear regression model
and you can see here we have the root
mean square error rmsse equals and this
is the square root of the sum of the
predicted minus the actual squared over
the total number so we're just looking
for the average mean so we're looking
for the average over the n and the
reason you need to know about the
difference between rmse versus MSC is
when you're doing a lot of these models
and you're building your own model why
do you need to take the square root of
of it it doesn't change the value as far
as the way you're using it cuz you're
looking us to see whether the error is
greater or less than so why add that
extra computation in so a lot of models
use the msse which indicates the mean
square error or the average error and
it's the same formula minus the square
root at the end or across the whole
thing oh another uh riddle to solve if
it rains on Saturday with a probability
of 6 and it rains on Sunday with a
probability 2 what is the probability
that it rains this weekend and the trick
and probabilities on this case is we're
not we need to know what is the
probability of it not raining what is it
not what's the chance of it not raining
on Saturday and if it doesn't rain on
Saturday we want to take that and
combine that with the chance of it not
raining on Sunday the total probability
which in this case we're just going to
use one minus the probability that it
will not rain on Saturday so that's 1
min-6 we're going to take that as a
union which we simply just multiply them
together of the probability that it will
not rain on Sunday and it's important to
recognize the union here or the uh and
you can see by the formula down here we
end up with 68 or 68% chance that it
will rain on the weekend and there are a
couple other ways to solve this but this
is probably the most traditional way of
doing that how can you select k for K
means so first you better understand
that what K means is and that K is the
number of different groupings and most
commonly we use is the elbow method to
select k for K means the idea the elbow
method is to run K means clustering on
the data set where K is the number of
clusters within the sum of squares WSS
is defined as the sum of the squared
distance between each member of the
cluster and its centroid and you should
know all the terms for your K means on
there and with the elbow point and again
here's our iteration in our code we
talked about that earlier you iterate
starting with um usually you don't start
right at one but least might start with
two three or four and you just see where
it comes out and you can see the nice
elbow there which is easy to see
graphically where the number of K
clusters and the WSS value drops and
then it just kind of flattens out and
there's no reason to take the K means
any further what is the significance of
P value oh good one especially if you're
dealing with r because that's the first
thing that pops up P value typically
less than or equal to 0.05 indicates a
strong evidence against the null
hypothesis and you should know what a
difference why we use null hypothesis
instead of the hypothesis so you reject
the null hypothesis very important that
term null hypothesis in any scientific
setup and also in data science it
doesn't mean that it's true it means
that there's a high correlation that
it's true so if your null hypothesis
means it's not true your hypothesis is
has a high correlation that it's
probably true and if the P value is
typically greater than 005 it indicates
a weak evidence against the null
hypothesis so you failed to reject the
whole null hypothesis and if you reject
that then your actual hyp hypothesis is
probably not true the correlation of
your data with what you think it's
saying is is probably Incorrect and if
you're right at the cut off of 0.05 it's
considered to be marginal could go
either way and again you can use that P
value on different features to decide
whether you're going to include your
features as far as something worth
exploring in your data science model how
can outlier values be treated o good one
you can drop outliers only if it is a
garbage value so sometimes you end up
with like one outlier that just is
probably someone's measurements way off
height of an adult equals ABC feet this
cannot be true as height cannot be a
string value in this case outliers can
be removed if the outliers have extreme
values they can be removed for example
if all the data points are clustered
between 0 to 10 but one point lies at
100 then we can remove this point and
again sometimes you just look for the
outlier so you can see what's going on
if there's something unusual there so
maybe the equipment's not calibrated
correctly if you cannot drop outliers
you can try the following try a
different model data detected as
outliers by linear model can be fit by
nonlinear model so we be sure you are
choosing the right model so if it has
like more of a curved look to it instead
of a straight line you might need to use
something other than just a straight
line linear model try normalizing the
data this way the extreme data points
are pulled to a similar range if you can
use algorithms which are less affected
by outliers example random Forest so
there is another solution is you can
come up with the random Forest which a
lot of times completely bypasses your
outliers how can you say that a Time
series data is stationary oh that's an
interesting term stationary mean it's
not moving but it's a Time series we can
say that a Time series is stationary
when the variance and mean of the series
is constant with time and this the
graphic example is very easy to see we
have our um the variance is constant
with time so we have our first variable
y and x and x being the time factor and
Y being the variable as you can see goes
through the same values all the time
it's not changing in the long long
period of time so that's stationary and
then you can see in the second example
the waves get bigger and bigger so it's
nonstationary here the variance is
changing with time again we have Y which
stays constant so that if you look at
the bigger picture it's the same wave
over and over again and then of course
we have uh where the wave is growing in
size going up it can also go down so it
also be non-stationary how can you
calculate accuracy using confusion
Matrix oh great one uh confusion
matrixes are so useful when you're
taking that first look at data and also
when you're showing the shareh holders
and you want to ask them for money how
can you calculate accuracy using
confusion Matrix so you have your total
data that we're looking at is 650 and
you have your predicted values and your
actual values and you have your
predicted p and your actual p and so
when you look at this you'll note that
if the predicted p and the actual P are
262 but our predicted P also had 15 that
weren't correct so you can see there's a
false positive there 15 and the same
thing with the the N you can see where n
predicts n and it has a false negative
of 26 out of the total number of n
values in there and so we can do an
accuracy on there the true positive plus
the true negative is our total
observations so you have a total of 093
accuracy or 93% and just a quick note on
this this is so important because it's
one thing if someone is being diagnosed
with uh say cancer you know this is life
death or is my nuclear reactor going to
blow up Suddenly if the p is the
probability of it blow blowing up and
this's say you have 15 that's a lot less
than say the 26 chances of it blowing up
you know so the actual domain of your
data is very important so if you're
non-positive you don't really care about
the predicted value having non-positive
as positive because they're going to go
do a biopsy on the cancer or whatever
anyway but you're very interested if you
have a positive um an actual positive
value which is looked at as negative a
false negative that's really important
in that domain depending on what domain
you're in write the equation and
calculate precision and recall rate and
so continuing with our um confusion
Matrix I was just talking about the
different domains we have the Precision
equals 262 over 277 so your Precision is
the true positive over the true positive
plus false positive and the recall rate
is your true positive over the total
positive plus false negative and you can
see here we have that 262 over 277 equal
a 94% and the recall over here is the
262 over 280 which equals 9.9 or 90% And
oh good we're going to take a pause for
another brain teaser if a drawer
contains 12 Red Socks 16 blue socks and
20 white socks how many must pull out to
be sure of having a matching pair the
last time I went through these kind of
brain teaser things was like 20 years
ago and I had six people sitting across
the table waiting for my answer that's
kind of mind-numbing when you're in an
interview like that hopefully you're not
stuck in an interview like that but uh
on this you need to ask yourself how
many different colors of socks are there
so they've thrown a lot of extra data in
here that you don't need to solve the
answer the answer is four an example
your first pick is white your second
pick is red third pick is blue so no
pairs yet and that means when you get to
the fourth pick there's 100% chance
you're going to have a match so the most
is going to be four that you ever have
to pull out of your drawer if it was
four colors the answer would be five and
so on it doesn't matter how many white
socks you have or how many red socks or
blue socks different pairs you have it's
the different colors a number of
different colors people who bought this
also bought recommendations seen on
Amazon as a result of which algorithm Oo
we covered this earlier recommendation
engine is done with collaborative
filtering collaborative filtering
exploits the behavior of other users and
their purchase history in terms of
ratings selection Etc it makes
predictions on what you might interest a
person based on the preference of many
other users in this algorithm features
of the items are not known and we have a
nice example here where they took a
snapshot of a sales page it says uh for
example suppose X number of people buy a
new phone and then also buy tempered
glass with it next time when a person
buys a phone he'll be recommended to buy
tempered glass along with it and if you
remember the um vocabulary words we
covered earlier this is the
recommendation this is collaborative the
other word was content based so looking
at things with similar content versus
collaborative which is similar people
and remember you know you're not going
to know every vocabulary word but it
also doesn't hurt to get your 3x5 cards
out and make yourself a vocabulary stack
of cards by an app on your phone for it
SQL query I remember back in the 9s it
was so important to know SQL query and
only a few people got it nowadays it's
just part of your kit you have to know
some basic SQL so write a basic SQL
query to list all orders with customer
information and you can kind of make up
your own name for the database and you
can pause it here if you want to write
that down on a paper and let's go ahead
and look at this we have uh to list all
orders with customer information and so
usually you have an order table and a
customer table and you have an order ID
a customer ID order number total amount
and then from your customer table you
have ID first name last name City County
and so if we're going to write in SQL
with this we're going to select keyword
there for SQL selecting order number
total amount first name last name City
Country so that's the columns we're
going to look at we're going to do that
from our order where we're going to join
it with our customer and we're going to
join it on the order customer ID equals
the customer ID so very basic SQL query
that's going to return a table of data
for us you are given a data set on
cancer detection you've built a
classification model and achieved an
accuracy of 96% woo 96% why shouldn't
you be happy with your model performance
what can you do about it that's an
interesting one cuz this comes up that's
one of the standard data sets on there
is for cancer detect detection cancer
detection results in inbalanced data in
an imbalanced data set accuracy should
not be based as a measure of performance
because it is important to focus on the
remaining 4% which are the people who
were wrongly diagnosed we talked a
little bit about this earlier you have
to know your domain you know this is the
medical cancer domain versus weather
domain you know Weather Channel they get
by with 50% wrong in cancer you don't
want 4% of the people being wrongly
diagnosed wrong diagnosis is of a major
concern concerned because there can be
people who have cancer but we're not
predicted so in an imbalanced data set
accuracy should not be used as a
measurement performance which of the
following machine learning algorithm can
be used for inputting missing values of
both categorical and continuous
variables so we have a couple choices
here we have K means clustering we have
linear regression we have the K Inn
nearest neighbor and decision tree and
which of the following machine learning
algorithms can be used for inputting
missing values of both categor IAL and
continuous variables now certainly you
can use some pre-processing to do some
of that but you should have gone with
the K nearest neighbor because it can
compute the nearest neighbor and if it
doesn't have the value it just computes
the nearest neighbor based on all the
other features where when you're dealing
with K means clustering or linear
regression you need to do that in your
pre-processing otherwise it'll crash
decision trees also although there's
some variance on that too can you solve
another riddle always fun ones given a
box of matches and two ropes not necess
necessarily identical measure a period
of 45 minutes and in this particular
setup the ropes are not uniform in
nature and the Rope takes exactly 60
Minutes to completely burn out so each
rope takes up to 60 Minutes to burn out
and there's actually a couple different
solutions to this but let me go ahead
and one of the things is they're not
uniform in nature so even though they
take 60 minutes anyways let's go ahead
and see what they did to solve it and
then we can also look at different
options we have two ropes A and B B
light a from both ends and B from one
end okay when a is finished burning we
know that 30 minutes have elapsed and B
has 30 minutes remaining now light the
other end of B also so that the
remaining part of B will burn taking 15
minutes to burn this we have gotten 30 +
15 = 45 minutes excellent solution mine
which I like was to take one rope fold
it in two so we know it's a half hour
take the other rope fold it in four
places so we know that that one's 15
minutes and then you can just connect
the two and burn it straight across I
think they're trying to to cover that by
saying they're not regular the ropes are
have some irregularities maybe that's
what they meant by that you couldn't do
something like that that's my solution
below are the eight actual values of
Target variable in the train file so we
have a training file not to be confused
with the Train on the tracks we have
0000 0 11111 what is the entropy of the
target variable we mentioned earlier
that you you should know your entropy
and how to calculate the entropy what is
the entropy of the target variable so we
have a couple options here we have Min -
5 over 8 lth of 58 + 3 over8 logarithm
of 3 over8 okay let's just see where
they got those numbers from we have uh
one which is going to be five ones and
three Zer and then we have a total of
eight okay and then we have the option
of five uh which is number of ones 58
logarithm of 58 plus 38 logarithm of 38
and we also have 38 logarithm of 5/8
plus 58 logarithm of 38 and then we kind
of reverse those numbers around and
let's see what you're going to get here
which one did you think it was you
should have checked the first one so
what is the entropy of the target
variable the key there is the target
variable so we're looking at the Target
in this case is going to be one usually
that's what you're looking for and so
the entropy of that one we want to
subtract out the entropy of the
non-target variable oops I had that
backwards we want to we're looking at
zero so we want to subtract out the 5/8
from there so 58 logarithm 58 or
negative 58 logarithm 58 plus 38
logarithm 38 and they have the hint on
the bottom entropy equals I of P of n so
we have a netive p plus p and n * the
logarithm base 2 of p over p + n minus
the N / p+ n * logarithm 2 of n / p+ n
we want to predict the probability of
death from heart disease based on three
risk factors age gender and blood
cholesterol level what is the most
appropriate algorithm for this case so
we have three features and we want to
know the predictability of death okay A
little morbid there choose the right
algorithm do we want to use logistic
regression for this linear regression K
means clustering or the aoria algorithm
and if you selected logistic regression
then you probably got the right answer
linear regression remember deals with
like you take your line and draw a line
through the data and of course you don't
necessarily have to use a straight line
there's other means for that but you're
dealing with a lot of numbers and K
means means we're just going to Cluster
objects together with the logistic
regression though you can mix those
things together in buckets so really the
logistic regression is what you want to
use in that model would be the most app
fit after studying the behavior of a
population you have identified four
specific individual types who are
valuable to your study you would like to
find all users who are most similar to
each individual type which algorithm is
most appropriate for this study
certainly identifying census and just
about a lot of different mark is common
so maybe they have a census or whatever
means but let's take a look at some of
the different algorithms we might use on
this we have K means clustering linear
regression Association rules and
decision treats and uh I'll give you a
hint we're looking for grouping people
together by similarities and uh by four
different similarities so very specific
they gave you one of the values
specifically the K value so K means
clustering would be great for this
particular problem you have run the
association rule rules algorithm on your
data set and the two rules banana apple
is associated with grape and apple
orange is associated with grape have
been found to be relevant what else must
be true so this would challenge you to
understand Association rules you could
picture in this particular one you're
going shopping and uh you almost always
see somebody who has bananas they
usually have grapes in their bag also
and somebody who has apples usually has
grapes in their bags and then apples and
oranges is also associated with grapes
and let's go a and take a look at that
and we have a couple different options
here first one is banana apple and grape
orange must be a frequent item set not
so much banana apples oranges must be
relevant rule grape is common with
banana apple must be a relevant Rule and
how about grape apple must be a frequent
item set let's go back and take a look
at that and we notice that we have
bananas apples to grapes we have apple
orange to Grape boy there's a lot of
grapes and a lot of apples in there and
so if you said the last one one grape
and apple must be a frequent item set
then you got it correct your
organization has a website where
visitors randomly receive one of two
coupons it is also possible that
visitors to the website will not receive
a coupon you have been asked to
determine if offering a coupon to
visitors to your website has any impact
on their purchase decision which
analysis method should you use and so
let's go ahead and start by giving you
another hint and give you some limiting
your selection we have a oneway an NOA K
means clustering a Association rules and
student T Test so obviously you should
know what each one of these means but
let's take a look at the question again
so you want to know which method should
you use to see if the coupon valid for
their purchase well we're not clustering
and we're not associating things
together we want to know the end result
student T Test also drawing that little
T in boxes and switch them around
there's really only one answer that
works in here and that's a oneway Anova
and with that we have reached to the end
of the session on the complete data
science code course should you need any
assistance PPD or any other resources
used in this session please do let us
know in the comment section below and
our team of experts we'll be happy to
help you as soon as possible until next
time thank you and keep learning stay
tuned for more from simp Lon staying
ahead in your career requires continuous
learning and upscaling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here