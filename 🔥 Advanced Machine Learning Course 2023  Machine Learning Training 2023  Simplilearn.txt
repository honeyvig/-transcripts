hello and welcome to Simply land today
we have a session on the advanced
concepts of machine learning in today's
rapidly volume technological landscape
machine learning has emerged as a
powerful tool that's transforming
Industries and reshaping our daily lives
whether you are aspiring data scientist
a business professional or simply
curious about this fascinating field you
have come to the right place I am
abhisar welcome to the advanced machine
learning full course by simply learn and
I will be your guide as you embark on
this journey to domestify the
fundamentals of machine learning so what
exactly is machine learning how does it
work and why is it so crucial today
let's find out machine learning is a
branch of artificial intelligence that
focuses on developing algorithms and
models capable of learning and making
predictions or decisions without being
explicitly programmed it enables
computers to automatically analyze and
interpret vast amounts of data and cover
patterns and extract valuable insights
in this session we'll explore the key
Concepts that form the foundation of
machine learning we'll start by
understanding the types of machine
learning and their characteristics will
dive into supervised learning where
models learn from label data and
unsupervised learning where models
identify patterns in unlabeled data
we'll also touch upon reinforcement
learning which involves training models
through interaction with an environment
furthermore we'll discuss the crucial
steps of machine learning algorithms
from linear regression to new base we
will also address project topics like
fake news detection image
classifications using CNN and object
detection and many more which will help
you build your resume by the end of this
session you will have a solid
understanding of the advanced concepts
of machine learning enabling you to
comprehend its application and
challenges better whether you want to
pursue a Korean in data science
collaborate with machine learning
experts or simply be an informed
participant in today's data driven world
this knowledge will undoubtedly prove
invaluable according to recent studies
Ai and machine learning rated job
postings have increased by 344 in the
past five years companies across the
globe are actively seeking professionals
who can harness the power of data and
build intelligent systems the average
salary is 150 000 in U.S and 15 lakhs
per annum in India hey everyone welcome
to simple lens YouTube channel but
before we dive into that don't forget to
like subscribe and share
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that costs everything from the
fundamentals to Advanced Techniques for
that accelerate your career in Ai and ml
with a comprehensive postgraduate
programming Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
your prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three Capstone
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you'll gain practical
experience master classes by Caltech
faculty and IBM experts and show
top-notch education simply learns job
assist helps you notice by Leading
companies this program calls statistics
python supervised and unsupervised
learning NLP neural networks computer
vision gns kras tensorflow and many more
skills enroll now and then look exciting
Ai and ml opportunities the course link
is mentioned in the description box
below so without any further delay let's
get started so why machine learning why
do we even care about having these
computers can and be able to do all
these new things for us well because
machines can now drive your car for you
still very in the infant stage but it's
just exploding as we see with Google's
waymo and then Uber had their program
which unfortunately crashed they know
that this is huge this is going to be
the huge industry to change our whole
Transportation infrastructure machine
learning is now usually detect over 50
eye diseases do you know how amazing
that is to have a computer that double
checks for the doctor for things they
might miss that's just huge in the
health industry pretty soon they
actually do already have that within
some areas where maybe not for eyes but
for other diseases where they're using
the camera on your phone to help
pre-diagnose before you go in and see
the doctor and because the machine can
now unlock your phone with your face I
mean this is cool having it being able
to identify your face or your voice and
be able to turn stuff on and off for you
depending on where you're at and what
you need talking about an ultimate
automation or world we live in and as we
dig in deeper we have a nice example of
Facebook as you can see here they have
the Facebook post with Halloween comment
yes if you want it order here nobody
likes spam post on Facebook that annoy
them into interacting with likes shares
comments and other actions I remember
the original ones were all if you don't
click on here you will have bad luck or
some kind of Fear Factor well this is a
huge thing in a social media when people
are getting spammed and so this tactic
known as engagement bait takes advantage
of Facebook's news feed algorithm by
choosing engagement in order to get the
greater reach to eliminate engagement
bait the company reviewed and
categorized hundreds of thousands of
posts to train a machine learning model
that detects different types of
Engagement bait so in this case we have
we're using Facebook but this is of
course across all the different social
media they have different tools for
building and the Facebook scroll GIF
will be replaced kind of like a virus
coming in there and notices that there's
a certain setup with Facebook and it's
able to replace it and they have like
vote baiting react baiting share baiting
they have all these different these are
kind of General titles but there
certainly are a lot of way of baiting
you to go in there and click on
something so they fed all this this data
was fed into the machine and then they
have the new post the new post comes up
that Takes Over part of the Facebook
setup and that's what you're looking at
you're looking at this new post that's
replaced like a virus has replaced that
so what Facebook did to eliminate this
is they started scanning for keywords
and phrases like this and checks the
click-through rate so it starts looking
for people who are clicking through it
without even looking at it or clicking
through it and it's not something that
normally would be clicked through once
Facebook has scanned for these keywords
and phrases it is now able to identify
the spam coming in and this makes your
life easier so you're not getting
spammed it's not like walking through an
airport in a lot of countries you have
like hundreds of people trying to sell
you time share come join us sign up for
this eliminates that annoyingness so now
you can just enjoy your Facebook and
your cat pictures or maybe it's your fan
pictures mine is family certainly people
like their cat pictures too another good
example is Google's Deep Mind project
alphago a computer program that plays a
board game go has defeated the world's
number one go player and I hope I say
his name right key G the ultimate go
challenge game of three of three was on
May 27 2017. so that's just last year
that this happened and what makes this
so important is that you know go is just
is a game so it's not like you're
driving a car or something in our real
world but they are using games to learn
how to get the machine learning program
to learn they wanted to learn how to
learn and that is a huge step a lot of
this is still in its infant stage as far
as development as we saw what happened
with the as I referred to earlier the
Uber cars they lost their whole division
because they jumped ahead too fast so
still an infant stage but boy is this
like the beginning of just an amazing
world that is automated in ways we can't
tomorrow's going to look like we've
looked at a lot of examples of machine
learning so let's see if we can give a
little bit more of a concrete definition
what is machine learning machine
learning is a science of making
computers learn and act like humans by
feeding data and information without
being explicitly programmed we see here
we have a nice little diagram where we
have our ordinary system and your
computer nowadays you can even run a lot
of stuff on a cell phone because cell
phones advance so much and then with
artificial intelligence and machine
learning it now takes the data and it
learns from what happened before and
then it predicts what's going to come
next and then really the biggest part
right now in machine learning that's
going on is it improves on that how do
we find a new solution so we go from
descriptive words learning about stuff
and understanding how it fits together
to predicting what it's going to do to
post scripting coming up with a new
solution and when we're working on
machine learning there's a number of
different diagrams that people have
posted for what steps to go through a
lot of it might be very domain specific
so if you're working on Photo
identification versus language versus
medical or physics some of these are
switched around a little bit or new
things are put in they're very specific
to The Domain this is kind of a very
general diagram first you want to Define
your objective very important to know
what it is you're wanting to predict
then you're going to be collecting the
data so once you've defined an objective
you need to collect the data that
matches you spend a lot of time in data
science collecting data in The Next Step
preparing the data you got to make sure
that your data is clean going in there's
the old saying bad data in bad answer
out or bad data out and then once you've
gone through and we've cleaned all this
stuff coming in then you're going to
select the algorithm which algorithm are
you going to use you're going to train
that algorithm in this case I think
we're going to be working with svm the
support Vector machine then you have to
test the model does this model work is
this a valid model for what we're doing
and then once you've tested it you want
to run your prediction you want to run
your prediction or your choice or
whatever output it's going to come up
with and then once everything is set and
you've done lots of testing then you
want to go ahead and deploy the model
remember I said domain specific this is
very general as far as the scope of
doing something a lot of models you get
halfway through and you realize that
your data is missing something and you
have to go collect new data because
you've run a test in here someplace
along the line you're saying hey I'm not
really getting the answers I need so
there's a lot of things that are domain
specific that become part of this model
this is a very general model it's a very
good model to start with and we do have
some basic divisions of what machine
learning does that's important to know
for instance do you want to predict a
category well if you're categorizing
thing that's classification for instance
whether the stock price will increase or
decrease so in other words I'm looking
for a yes no answer is it going up or is
it going down and in that case we'd
actually say is it going up true if it's
not not going up it's false meaning it's
going down this way it's a yes no zero
one do you want to predict a quantity
that's regression so remember we just
did classification now we're looking at
regression these are the two major
divisions and what data's doing for
instance predicting the age of a person
based on the height weight health and
other factors So based on these
different factors you might guess how
old a person is and then there are a lot
of domain specific things like do you
want to detect an anomaly that's anomaly
detection this is actually very popular
right now for instance you want to
detect money withdrawal anomalies you
want to know when someone's making a
withdrawal that might not be their own
account we've actually brought this up
because this is really big right now if
you're predicting the stock whether to
buy stock or not you want to be able to
know if what's going on in the stock
market is an anomaly use a different
prediction model because something else
is going on you got to pull out new
information in there or is this just the
norm I'm going to get my normal return
on my money invested so being able
detect anomalies is very big in data
science these days another question that
comes up which is on what we call
untrained data is do you want to
discover structure in unexplored data
and that's called clustering for
instance finding groups of customers
with similar Behavior given a large
database of customer data containing
their demographic and past buying
records and in this case we might notice
that anybody who's wearing certain set
of shoes go shopping at certain stores
or whatever it is they're going to make
certain purchases by having that
information it helps us to Market or
group people together so that we can now
explore that group and find out what it
is we want to Market to them if you're
in the marketing world and that might
also work in just about any Arena you
might want to group people together
whether they're based on their different
areas and Investments and financial
background whether you're going to give
them a loan or not before you even start
looking at whether they're a valid
customer for the bank you might want to
look at all these different areas and
group them together based on unknown
data so you're not you don't know what
the data is going to tell you but you
want to Cluster people together that
come together
let's take a quick DeTour for quiz time
oh my favorite so we're gonna have a
couple questions here under a quiz time
and we'll be posting the answers in
these part two of this tutorial so let's
go ahead and take a look at these quiz
times questions and hopefully you'll get
them all right it'll get you thinking
about how to process data and what's
going on can you tell what's happening
in the following cases of course you're
sitting there with your cup of coffee
and you have your checkbox and your pen
trying to figure out what's your next
step in your data science analysis so
the first one is grouping documents into
different categories based on the topic
and content of each document very big
these days you know you have legal
documents you have maybe it's a Sports
Group documents maybe you're analyzing
newspaper postings
but certainly having that automated is a
huge thing in today's world B
identifying handwritten digits and
images correctly so we want to know
whether they're writing an A or capital
A B C what are they writing out in their
hand their handwriting C behavior of a
website indicating that the site is not
working as designed D predicting salary
of an individual based on his or her
years of experience with HR hiring uh
set up there so stay tuned for part two
we'll go ahead and answer these
questions when we get to the part two of
this tutorial or you can just simply
write at the bottom and send a note to
Simply learn and they'll follow up with
you on it back to our regular content
and these last few bring us into the
next topic which is another way of
dividing our types of machine learning
and that is with supervised unsupervised
and reinforcement learning supervised
learning is a method used to enable
machines to classify predict objects
problems or situations based on labeled
data fed to the machine and in here you
see we have a jumble of data with
circles triangles and squares and we
label them we have what's a circle
what's a triangle what's a square we
have a model training and it trains it
so we know the answer very important
when you're doing supervised learning
you already know the answer to a lot of
your information coming in so you have a
huge group of data coming in and then
you have a new data coming in so we've
trained our model the model now knows
the difference between a circle a square
a triangle and now that we've trained it
we can send in in this case a square and
a circle goes in and it predicts that
the top one's a square and the next
one's a circle and you can see that this
is uh being able to predict whether
someone's going to default on a loan
because it's talking about Banks earlier
supervised learning on stock market
whether you're going to make money or
not that's always important and if you
are looking to make a fortune in the
stock market keep in mind it is very
difficult to get all the data correct on
the stock market it is very it
fluctuates in ways you really hard to
predict so it's quite a roller coaster
ride if you're running machine learning
on the stock market you start realizing
you really have to dig for new data so
we have supervised learning and if you
have supervised we should need
unsupervised learning in unsupervised
learning machine learning model finds
the hidden pattern in an unlabeled data
so in this case instead of telling it
what the circle is or what a triangle is
and what a square is it goes in there
looks at him and says for whatever
reason it groups them together maybe
they'll group it by the number of
corners and it notices that a number of
them all have three corners a number of
them all have four corners and a number
of them all have no corners and it's
able to filter those through and group
them together we talked about that
earlier with looking at a group of
people who are out shopping we want to
group them together to find out what
they have in common and of course once
you understand what people have common
maybe you have one of them who's a
customer at your store or you have five
of them or customer at your store and
they have a lot in common with five
others who are not customers at your
store how do you Market to those five
foreign customers at your store yet they
fit the demographs of who's going to
shop there and you'd like them to shop
at your store not the one next door of
course this is a simplified version you
can see very easily the difference
between a triangle and a circle which
might not be so easy in marketing
reinforcement learning reinforcement
learning is an important type of machine
learning or an agent learns how to
behave in an environment by performing
actions and seeing the result and we
have here where the in this case a baby
it's actually great that they used an
infant for this slide because the
reinforcement learning is very much in
its infant stages but it's also probably
the biggest machine learning demand out
there right now or in the future it's
going to be coming up over the next few
years is reinforcement learning and how
to make that work for us and you can see
here where we have our action I mean the
action in this one it goes into the fire
hope the baby didn't it was just a
little candle not a giant fire pit like
it looks like here when the baby comes
out and the new state is the baby is sad
and crying because they got burned on
the fire and then maybe they take
another action the baby's called the
agent because it's the one taking the
actions and in this case they didn't go
into the fire they went a different
direction and now the baby's happy and
laughing and playing reinforcement
learning is very easy to understand
because that's how as humans that's one
of the ways we learn we learn whether it
is you know you bring yourself on the
stove don't do that anymore don't touch
the stove in the big picture being able
to have a machine learning programming
or an AI be able to do this is huge
because now we're starting to learn how
to learn that's a big jump in the world
of computer and machine learning and
we're going to go back and just kind of
go back over supervise versus
unsupervised learning understanding this
is huge because this is going to come up
in any project you're working on we have
in supervised learning we have labeled
data we have direct feedback so someone
has already gone in there and said yes
that's a triangle no that's not a
triangle and then you predict an outcome
so you have a nice prediction this is
this this new set of data is coming in
and we know what it's going to be and
then with unsupervised training it's not
labeled so we really don't know what it
is there's no feedback so we're not
telling it whether it's right or wrong
we're not telling it whether it's a
triangle or a square we're not telling
it to go left or right all we do is
we're finding hidden structure in the
data grouping the data together to find
out what connects to each other and then
you can use these together so imagine
you have an image and you're not sure
what you're looking for so you go in and
you have the unstructured data find all
these things that are connected together
and then somebody looks at those and
labels them now you can take that label
data and program something to predict
what's in the picture so you can see how
they go back and forth and you can start
connecting all these different tools
together to make a bigger picture
everyone knows the algorithm is a
step-by-step is to approach a particular
problem there are numerous examples of
algorithms from figuring out sets of
number to finding Roots through maps to
showing data on a screen let's
understand this by using an example
every algorithm is built on inputs and
outputs Google search algorithm is no
different the input is the search speed
and the output is the page of its result
that appears when you enter a particular
phrase or keyword also known as serp or
search engine result page
Google has a search algorithm so it can
sort results from various websites and
provides the users the best result
when you start a search you will see the
search box will attempt to guess what
you are looking for in order to better
understand what the user is looking for
the algorithm is trying to gather as
many as suggestions from them as
possible
the results from the search field that
best matches the query will be ranked
the choose fit website will Rank and in
what position using more than 200
ranking variables now let's take an
example of coding program and see how
the algorithm works
here we will use a case of computer
program wherein we want to print the
multiplication table of any number let's
take two the algorithm start here and
then it assign a value to a variable the
variable I is having an initial value of
1. the system will read the number the
number in case is 2. now the system has
a condition a condition can now either
be true or false if the value of I
reaches 11 then the loop will end
otherwise value of I will multiply by
the number
the initial value of I is 1 so for the
first time the system output will be 2
now value of I will be increased by 1.
according to the loop condition
the system will then move back and check
for the condition again
the new value of I is 2. which is still
less than 11 the system will again print
2 into Y which is 2 into 2 on this
screen
the new output result will be 4. the
system will keep following the same
procedure repeatedly until the value of
I becomes 11. once the value of I
becomes 11 then only the algorithm will
terminate after discussing how an
algorithm work let's move forward and
see some popular machine learning
algorithms
some popular machine learning algorithms
are
first one is linear regression algorithm
second one is logistic regression
algorithm
and the third one is decision tree
and the fourth one is support Vector
machine algorithm svm
and the fifth one is KNN K nearest
neighbor algorithms
and the sixth one is K means clustering
algorithms the and the seventh one is
random Forest algorithms and the last
two but not the least algorithm is a
priority algorithms let's go through
them in detail one by one
linear regression is one of the most
famous and straightforward machine
learning algorithms utilized for
predictive analysis linear regression
show the linear connection between the
dependent and independent factors
the equation of line is y equals to MX
plus b here y stand for this response
variable or a dependent variable whereas
X is for the Predator variable or an
independent variable it attempts best to
fit line between the dependent and
independent variables and this best bit
line is known as line of regression or
regression line
let's take a real application example in
predicting consumer Behavior businesses
use the linear regression to forecast
things like how much a client is likely
to spend
things like targeted marketing and
product development May benefit from
this
Walmart for instance use linear
regression to forecast which good would
be in high demand Across the Nation
moving forward let's see types of linear
regression
there are two types of linear regression
algorithm
the first one is simple linear
regression and the second one is
multiple linear regression
in simple linear regression if an
independent variable is utilized to
focus the worth of a mathematical
dependent variable then at that point
such a linear regression algorithm is
called Simple linear regression the
equation of line will be y equals to a0
plus a 1 X
and the second one is multiple linear
regression if the dependent variables
declines on the Y and that if
independent variable on the X
then such a relationship is known as
negative linear relationship the line of
equation will be minus of a0 plus a 1 X
moving forward let's see logistic linear
regression
logistic regression is the supervised
machine learning algorithm utilized to
anticipate all the categorical factors
or discrete values it could be very well
used for the grouping issues in machine
learning and the result of the logistic
regression can be either yes or no 0 or
1 men or women and so on it gives the
values which lies between 0 and 1. for
example a credit card business is
interested in knowing whether the
transaction amount and the credit score
have an impact on the probability that a
particular transaction would be
fraudulent the business can use logistic
reduction to determine how these two
Predator values can relate the
probability that a transaction is
fraudulent this response variable in the
model has two possible outcomes
first one is the transaction is
shortland and the second one is the
transaction is not fraudulent
in logistic regression rather than
fitting a regression line we fit an S
form logistic capability which predicts
two greatest value 0 or 1. a logistic
regression equation can be calculated
from linear regression equation the
steps to get logistic regression
equations are the equation of straight
line can be written as y equals to b0
plus B1 X1 plus B2 x 2 till b n x n in
logistic regression y can be between 0
and 1 only so for that let's divide the
above equation by 1 minus y then the
equation will be y upon 1 minus y that
is 0 for y 0 and Infinity for y equals
to 1 but range between minus infinity to
plus infinity then we have to take the
logarithm of equation and now it will
become log of Y upon 1 minus y equals to
b0 plus b 1 x 1 plus b 2 x 2 so on till
B and x n let's move forward and see
types of logistic regression
there are three types of logitative
regression that can be classified first
one is binomial in binomial logistic
regression there can be only two
possible types of dependent variables
like yes or no password man woman and
many more and the second one is
multinomial in multinomial logistic
regression there can be three or more
possible unordered ways of dependent
variable such as horse cow and sheep and
the last one is ordinal in ordinal
logistic regression there can be three
or more possible ordered ways of
dependent variable such as small medium
or large moving forward let's see
decision trees in detail
a decision tree is a tree structured
classifier that could be used for
classification and regression a decision
tree is a tree in which each non-leaf
node is assigned to an attribute
additionally each are contained one of
the available values for its parent node
which is associated with each Leaf node
that is the node from where the arc is
directed
let's see some decision tree terminology
first one is root that contains the
entire data set the next one is node a
test for the data of a certain attribute
and the third one is branch which
connects the node to internal node or
the internal node to Leaf node and the
fourth one is leaf node the terminal
node that predicts the outcome let's
move forward and see decision tree
algorithms the first one is select the
best attribute to use the current node
in the tree the second one is for each
possible values select the attributes
the third one is partition the examples
using the possible values of this
attribute and assign these disjoint
subset or the example to the appropriate
child node recursively generate each
child node until ideally all examples
for a node have the same label like
class moving forward let's understand
this recently for building a decision
tree
Step One is Select an attribute then
split the data into its children in a
tree continue splitting with available
attributes
and keep splitting until late node are
pure like only one class remains a
maximum depth is released a performance
metric is achieved let's move forward
and see svm algorithm support Vector
machine algorithms
a support Vector machine is a well-known
supervised machine learning model it is
utilized for both information
classification and regression
it is regularly utilized for the
grouping issues we can involve it in
different life care system and we can
involve it in typically happy or sad
local Arrangements we can involve it in
filters if we make specific Loops it
would add the particular filter
according to the expression the scope of
articulation lies between happy and sad
support Vector machine helps them to
recognize and return characters use
widely like checks continue to reach a
significant part of the majority of
non-cash transaction and are frequently
written by the pupil the current check
processing system in many developing
nations involves a bank employee to read
and manually enter the information in a
check while also verifying the data like
signature and date
a handwritten tax recognition system can
reduce expenses and labor hours because
a bank must handle several checks each
day
moving forward let's see the algorithm
of svm
the objective of support Vector machine
is to make the best line or Choice limit
that can isolate and dimensional space
into classes so we can undoubtedly put
the new data of interest in the right
category later on this batch decision
boundary is known as a hyperplate let's
move forward and see types of support
Vector machine support Vector machine
can be of two types first one is linear
svm second one is non-linear SPM let's
move forward and see linear sphere
linear svm is utilized for linearly
detachable information
which implies if a data set can be
ordered into two classes by utilizes a
straight line then such information are
named linearly separable information and
the classifier is utilized called linear
XVM classifier
moving forward let's see non-linear svm
non-linear SPM is utilized for
non-dietly isolated information and that
implies in the event that a data set
can't be categorized by utilizing a
straight line
such information is named non-directed
information
and the classifier utilize is called a
non-linear svm classifier moving forward
let's see k n algorithm in detail
KNN is a supervised learning technique
Canon classifies new data into our
targeted classes depending on the
features of its neighboring points and
also we use for the regression problems
it is an instance based learning
algorithm and a bit lazy learning
algorithm k n calculation stores every
one of its accessible information and
orders another information Point based
on the likeliness this means that when
new data information appears it usually
tends to the successfully categorized
into a good suit classes using the k n
algorithm let's imagine we have an image
of animal that resembles a cow or Ox
however here we are not sure if it is a
cow or Ox as k n method is based on a
likeness Matrix it will identify the
properties of new data that are related
to the image of coworks and based on
those quality it will classify the data
is belonging to either cow or Ox group
moving forward let's see how does KNN
work
the steps to implement k n algorithms
are step one decide on the neighbor's K
numbers Step 2 calculate the euclidean
distance between K Neighbors in step 2.
third one is based on the determined
euclidean distance select the K closest
neighbors step 4 is count the numbers of
data points in each category between
these connectors step 5 assign the fresh
data points in the category where the
highest neighbors count and then k n
model is ready
Let's see we need to add a new data
point to the vital category at first we
will decide on the numbers of neighbors
therefore we will pick k equals to 5.
then the euclidean distance between the
data points and then can be determined
the distance between two points known as
the euclidean distance can be determined
by under root of X2 minus X when Ka
whole square plus Y2 minus y1 whole
Square then we determine the closest
neighbors by calculating the euclide
distance
there are three closest Neighbors in
category a and two closest neighbor in
category B
this new data point Must Fall with
category a because as we can see its
three closest neighbors are also from
group a after understanding k n
algorithm let's move forward and see
k-means algorithms in detail
the K means is the cluster falls under
that is announced supervised learning
algorithm it is used to address machine
learning clusting problems and utilize
to tackle the grouping issues in machine
learning
it permits us to Bunch the information
into various Gatherings it is a helpful
method for finding the classification of
groups in the unlabeled data set without
the requirement of any training this
k-means algorithm groups the data into
similar classes let's see some
application of k-means clustering let's
see some applications of k-means
clustering diagnostic system the medical
profession uses k-means cursing in
creating smarter medical decision
support system especially in the
treatment of liver alignments the second
one is search engines clustering forms a
backbone of search engine when a search
engine is performed the search result
need to be grouped and the search
engines very open use clustering to do
this moving forward let's see how
k-means algorithm works
his steps to implement k-means
algorithms are step one select the
number key to set the number of clusters
Step 2 select a random K points or
centroid step 3 assign each data point
the closest centroid data forms to
predefined K cluster step 4 determine
the variance and a certain new Gravity
points for each cluster step 5 repeat
the third step this means reallocating
each data point to the new closest
centroid cluster step 6 if I
reassignment occurs go to step 4
otherwise go to exit step 7 the model is
ready to use
so now we have a clear understanding of
how k-means algorithm work let's move
forward to see the graphical
representation of k-means algorithm
consider that there are two variables M1
and M2 this is a scatter plot of these
two variables along the X and Y axis we
should accept the number of K of punches
that is k equals to 2 to recognize the
data set and to place them into various
groups it implies here we will attempt
to Bunch these data set into two unique
groups people really want to pick an
irregular key point or centroid to frame
this group
these centroids can be either the focus
of the data set or some of other points
thus here we are choosing under two
points as capoints which are not the
piece of our data set
we will assign every data of interest in
this scatter plot to the nearest K point
or centroid we will register it by
applying some math that we consider to
find the distance between two points
that is euclidean distance thus we will
draw a median between both the centroids
from the graph the left half of the line
are close points to the right and the K1
centroid green is near the orange
centroid we should variety them as green
and orange for Clear representation as
the need might arise to track down the
nearest group we will repeat the cycle
by picking another centroid
to pick the new centroid we will figure
out the center point of gravity of the
centroid and will track down new
centroids then we will reassign every
piece of information to highlight new
centroid for this we will repeat a
similar course of tracking down a middle
line the middle will be like as seen in
the picture one orange point is on the
left half of the line and two green
points are on the right thus these three
points will be appointed to the new
centroids as reassignment has occurred
we will again go to step 4 tracking down
new centroids or k points we will repeat
the cycle by tracking down the center
point of gravity of centroid so the new
centroids will be displayed as like this
we now have new centroid so once more
defined the middle boundary and reassign
the data of Interest by this graph there
are no unique pieces of information on
one of the other side of the line
implying our model is shaped by the
previous graph there are no unique
pieces of information on one or the
other line
implying our model is shaped as our
k-means model is ready and the two plus
groups will be displayed as like these
now we have a clear understanding of how
k-means clustering algorithm works now
let's move forward to understand random
Forest algorithm
random Forest is an adaptable simple to
utilize machine learning algorithm that
produces even without the hyper boundary
tuning and extraordinary outcome more
often that Norm it is likewise quite
possibly the most utilized algorithm
because of its effortness and variety
like its tend to be utilized for both
grouping and classification tasks random
Forest is a classifier that contains
various Choice trees on different
subsets of the given data set and takes
the normal to work on the present
exactness of the data set instead of
depending on the choice tree the random
Forest takes the forecast from each tree
and in light of the larger part of
boards of expectation it predicts the
final result
now let's move forward and see how does
random Forest work
we should see the random forest in order
since the arrangement is now and again
thought to be the structured block of
machine learning this is what a random
Forest would look like with two trees
the random Forest has a similar hyper
meter to their decision tree or a paging
classifier luckily there is a compelling
reason need to consolidate a decision
tree with a paging classifier
since you can undoubtedly utilize the
classifier classes of random words with
random Forest you can likewise manage
tasks using the algorithm regression
random forests add extra arbitness to
the model while tableving the trees
rather than looking for the man element
while parting a node
it looks to the best component among an
irregular subset of highlights these
outcomes in a wide variety often result
in a superior model
subsequently in a random Forest just a
random subset of the element is thought
about the algorithms you might make
trees more random by involving random
edges for each component instead of
looking for the most ideal limits like a
typical Choice tree does
let's move forward and see some
application of random Forest algorithms
random Forest is involved at work by
researchers in numerous Ventures
including banking stock exchanging
medication and many more it is utilized
to predict things which assist these
businesses with running productively
like client activity patient history and
safety in banking random Forest is used
to identify clients who are more likely
to pay back their debts on schedule
additionally it is utilized to forecast
who will make more frequent use of Bank
Services even fraud detection uses it
the robin out of algorithms indeed
random Forest is a tool used by stock
Trader to forecast future stock Behavior
retail businesses utilize it to make
product recommendation and forecast
client satisfaction random forage can be
used in healthcare industry to examine a
patient medical history and detect
disorders random Forest is a tool used
by pharmaceutical expert to determine
the idle mix of ingredients in treatment
or to forecast drug sensitivity by
seeing application of random Forest
algorithm let's move forward and see
some difference between decision trees
and Endo Forest
let's see the difference between random
forest and decision tree the first one
is
while building a random Forest the
number of rows is selected randomly
in decision trees it builds several
decision trees and find out the output
the second one is it combines two or
more decision trees together in decision
trees whereas the decision is a
collection of variables or data sets or
attributes the third one is it gives
accurate results whereas it gets less
accurate results the fourth one is by
using random Forest it reduces the
chance of overlifting whereas decision
trees it has the possibility of
overlifting the fifth one is random
Forest is more complicated to
interpreters whereas the decision tree
is simple it is easy to read and
understand
after seeing what is random Forest how
it works let's move forward to see a
priority algorithms in detail
the upper ID algorithm utilize standard
item sets to create calculation rules
and is intended to chip away the
information basis containing exchanges
with the assistance of these application
rules it decide how firmly orphibly two
objects are associated
this algorithm utilize a blood pressure
and history to work out the items at
Association effectively
it is the iterative interaction for
finding out the successive item set from
the huge data set
let's move forward and see steps for a
priority algorithms the steps for a
priority algorithms are Step One is
establish minimal support and confidence
for item set in the transactional
database the second one is take all
transaction supports with a greater
support value the minimum or chosen
support value in step two the third one
is track down all the rules in these
subsets with confidence valuator then
the threshold value the fourth one is
arrange the rules to lower the lift at
last we will see some advantages and
disadvantages of a priority algorithm
the advantages of a priority algorithms
are easy to understand an algorithm and
the second one is the join and prune
steps of the algorithms can be easily
implemented on the large data set the
disadvantages of a priority algorithms
are they are priority algorithms work
slowly as compared to the other
algorithm and the second one is the
priority algorithms times and space
complexity are o of 2D which is very low
compared to the other ones let's move
forward and see some Hands-On lab demo
so we will see a handsole lab demo on
linear regression as we know linear
regression is a way to find or to
predict the relationship between two
variables generally we use X and Y so
first we will open command prompt to
write command to open Jupiter notebook
so we will write Jupiter
notebook
and then press enter
so this is the landing page of Jupiter
notebook and select open
new python file
so this is how jupyter notebook UI looks
like so at first we will import some
major libraries of python which will
help us in mathematical functioning so
the first one is numpy
Nampa is a python Library used for
working with arrays it also has
functions for working in domains like
linear algebra and matrices it is in
open source project and you can use it
freely
numpy stand for numerical python
so we will write like import
numpy S and B here NP is used for
denoting numpire so we will import the
next libraries pandas pandas is a
software Library written for Python
programming
for like data manipulation and Analysis
in particular it offers data structures
and operation for manipulating numerical
labels and Time series
so we will write import pandas
as PD
here it is used for denoting pandas so
our next library is matplotlib
is a graph plotting library in Python
that serves as a visualization utility
mat.lib is a open source so we can use
it freely and it is most written in
Python a few segments in C and
JavaScript for platform compatibility
for importing Metro lip you have to
write import matplotlib
dot Pi plot
as PLT
so after importing the libraries we will
move ahead and import data set so for
importing data set we have to write data
set
equals to PD Dot rate underscore CSV and
here we have to give the path of the
file
data set
dot CSV
here PD is for pandas library and read
is used for reading the data set from
the machine and CSV CSV is used for the
type of file which you want to read
so after reading let's see our data so
we will write data set
dot hat
and press enter
here head is used for retrieving the
first five lines from the data so our
data is coming properly so moving ahead
now let's first Define X and Y axis for
X we have to write x equals to data set
Dot ilock
bracket column comma colon again
-1
dot values for accesses and for y axis
we have to write y equals to
data set
dot I log
bracket colon comma 1
dot values
so if we will use dataset.ilog for -1
values for x axis it will select till
the second last column of the data frame
instead of the last column
and I know this is as the second last
column value and the last column value
for the row is different
whereas if you will use for the y-axis
values return a series Vector a vector
does not have a column size
so moving ahead let's see the value for
x and y axis first we will see the value
for x like just you have to write X and
press enter so
these are the arrays value for x and for
y axis you have to type Y and then enter
so these are arrays value for y axis so
after this now let's split the data set
into training and testing separating
data into training and testing set is an
important part for evaluating data
mining models typically when you
separate a data set into a training set
and a testing set most of the data set
is used for training and a similar
portion of data set is used for testing
so we will split it into 70 and 30 ratio
so for splitting we need to import some
more libraries for this process so we
will write from
SK learn
dot model
underscore selection
selection
import
train
underscore test
underscore split
is most useful and robust library for
machine learning in Python it provides a
selection of efficient tools for machine
learning and statical modeling including
classification regression and clustering
so after importing left side code for
the splitting data so we will write here
X underscore
train
comma
X underscore test
comma y underscore train
comma y underscore test
equals to
10
underscore test
underscore split
in Brackets we have to write X comma y
comma
test
underscores size
equals to
0.3
comma
random
underscore state
equals to zero
random basically
which random State equals to zero we get
the same Trend and test sets across
different execution so after this let's
see the values together
so we will write X underscore train
comma X underscore test
comma y underscore
train
comma y underscore
test
so these are the values of array X and
array y together moving ahead now let's
work with regression first we need to
import the library for regression so we
will use from
SQ learn
dot linear
underscore
model
import
linear
regression
we already discussed SQL is used for
machine learning and linear regression
is a major part of machine learning so
after this let's make a function for
regression as RSG for easy use so we
will write Reg
equals to
linear
regression
and brackets first we try to train and
then test and compare so we will write
here
Reg
Dot
bit
X underscore brain
comma y underscore
change
now let's predict values but prediction
values are always different so we will
predict values for y first so we will
write y
underscore predict
equals to Reg
dot predict
sorry predict
X underscore test
so like y underscore
reading
and enter
this is when the predict functions come
into the picture python predict function
enables us to predict the labels of the
data values on the basis of training
model the predict function accept only a
single argument which is X underscore
test here usually the data to be tested
so when you will check the value for y
test you will see the different values
like
White
underscore
test
so you can see there are totally
different values from x axis now let's
display on graph for training data set
so we have to write PLT dot scatter
X underscore drain
comma y underscore train
comma color
equals to
you can choose by yourself I will choose
red
then Realty
Dot Plot
X underscore train
comma integration point for predicting
values
for X underscore
train
comma color
equals to
blue
so this color is for the regression line
and the PLT dot title
linear
regression
depression
salary
versus experience
and I will prefer this size would be
80.
let's set the labels for x and y axis so
X label
year of employee
common size will be
I will prefer size will be 50.
then for y-axis we will write
here
right salaries
of
employee
and I will prefer this same size 15.
let's show the plot by writing PLT dot
show
then
hopefully we are fortunate everything is
going to be fine
perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
work for training data set let's see how
it will work for testing data set
now let's predict for test data set you
can copy for the same and like you can
change it
so we can copy here
so we can paste here
so we have to just change here then to
test data set
just here and then again
test here
test here
everything everything you will
reduce some size
to
so
[Music]
perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
works for testing data set what is the
site kit learn it's simple and efficient
tool for data mining and data analysis
it's built on numpy scipy and matplot
Library so it interfaces very well with
these other modules and it's an open
source commercially usable BSD license
BSD originally stood for Berkeley
software distribution license but it
means it's open source with very few
restrictions as far as what you can do
with it another reason to really like
the site kit learn setup so you don't
have to pay for it as a commercial
license versus many other copyrighted
platforms out there what we can achieve
using the scikit learn we use class the
two main things or classification and
regression models classification
identifying which category an object
belongs to for one application very
commonly used is Spam detection so is it
a Spam or is it not a Spam yes no in
banking it might be is this a good loan
bad loan today we'll be looking at wine
is it going to be a good wine or a bad
wine and regression is predicting an
attribute associated with an object one
example is stock prices prediction what
is going to be the next value if the
stock today sold for 23 dollars and five
cents a share what do you think it's
going to sell for tomorrow and the next
day and the next day so that would be a
regression model same thing with weather
weather forecasting any of these are
regression models where we're looking at
one specific prediction I want a tribute
today we will be doing classification
like I said we're gonna be looking at
whether a wine is good or bad but
certainly the regression model which is
in many cases more useful because you're
looking for an actual value is also a
little harder to follow sometimes so
classification is a really good place to
start we can also do clustering and
model selection clustering is taking an
automatic grouping of similar objects
into sets customer segmentation is an
example so we have these customers like
this they'll probably also like this or
if you like this particular kind of
features on your objects maybe you like
these other objects so it's a referral
is a good one especially on Amazon.com
or any of your shopping networks models
section comparing validating and
choosing parameters and models now this
is actually a little bit deeper as far
as a site kit learned we're looking at
different models for predicting the
right course or the best course or
what's the best solution today like I
said we're looking at Wines it's going
to be well how do you get the best wine
out of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and fine-tuning
now this is only part one so we're not
going to do too much tuning on the
models we're looking at but I'll Point
them out as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and I'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
I like to use the Jupiter notebook and I
use it out of the Anaconda Navigator so
if you install the addacana Navigator by
default it will come with the jupyter
notebook or you can install the jupyter
notebook by itself this code will work
in any of your python setups I believe
I'm running an environment of 3.7 setup
on there I have to go in here and
environments and look it up for the
python setup but it's one of the three
x's and we go ahead and launch this and
this will open it up in a web browser so
it's kind of nice it keeps everything
separate and in this and conda you can
actually have different environments
different versions of python different
modules installed in each environment so
it's a very powerful tool if you're
doing a lot of development and jupyter
notebook is just a wonderful visual
display certainly you can use I know
spider is another one which is installed
with the Anaconda I actually use a
simple notepad plus plus when I'm doing
some of my python script any of your
Ides will work fine jupyter notebook is
iron python because it's designed for
the interface but it's good to be aware
of these different tools
and when I launched the Jupiter notebook
I'll open up like I said a web page in
here and we'll go over here to New and
create a new python setup like I said I
believe this is python37 but any of the
three this the site kit learn works with
any of the three X's there's even two
seven versions so it's been around a
long time so it's very big on the
development side and then the guys in
the back guys and gals developed they
went ahead and put this together for me
and let's go ahead and import our
different packages now if you've been
reading some of our other tutorials
you'll recognize pandas as PD pandas
library is pretty widely used it's a
data frame setup so it's just like
columns in rows and a spreadsheet with a
lot of different features for looking
stuff up the Seaborn sits on top of
matplot libraries this is for a graphing
and we'll see that how quick it is to
throw a graph out there to view in the
Jupiter notebook for demos and showing
people what's going on and then we're
going to use the random Forest the SVC
or support vector vector classifier and
also the neural network so we're going
to look at this we're actually going to
go through and look at three different
classifiers that are most common some of
the most common classifiers and let's
show how those work in the scikit learn
setup and how they're different and then
if you're going to do your setup on here
you'll want to go ahead and import some
metrics so the sklearn.metrix on here
and we're going to use the confusion
metrics and the classification report
out of that and then we're going to use
from the sklearn pre-processing the
standard scalar and label encoder
standards scalar is probably the most
commonly used pre-processing there's a
lot of different pre-processing packages
in the sklearn and then model selection
for splitting our data up it's one of
the many ways we can split data into
different sections and the last line
here is our percentage matplot library
in line some of the Seaborn and matplot
Library will go ahead and display
perfectly in line without this and some
won't it's good to always include this
when you're in the jupyter notebook this
is jupyter notebook so if you're you're
an IDE when you run this it will
actually open up a new window and
display the graphics that way so you
only need this if you're running it in a
editor like this one with the
specifically Jupiter notebook I'm not
even familiar with other editors that
are like this but I'm sure they're out
there I'm sure there's a Firefox version
or something Jupiter notebook just
happens to be the most widely used out
there and we can go ahead and hit the
Run button and this now has saved all
this underneath the packages so my
packages are now all loaded I've run
them whether you read it on top we run
it to the left and all the packages are
up there so we now have them all
available to us for our project we're
working on and I'm just going to make a
little side note on that when you're
playing with these and you delete
something out and add something in even
if I went back and deleted this cell and
just hit the scissors up here these are
still loaded in this kernel so until I
go under kernel and restart or restart
and clear or restart and run all I'll
still have access to pandas
important to know because I've done that
before I've loaded up maybe not a module
here but I've loaded up my own code and
then changed my mind and wondering why
does it keep putting out the wrong
output and then I realize it's still
loaded in the kernel and you have to
restart the kernel just a quick side
note for working with a jupyter notebook
and one of the troubleshooting things
that comes up and we're going to go and
load up our data set we're using the
pandas so if you haven't yet go look at
our pandas tutorial a simple read the
CSV with the separation on here so let
me go ahead and run that and that's now
loaded into the variable wine and let's
take a quick look at the actual file I
always like to look at the actual data
I'm working with in this case we have
wine quality Dash red I'll just open
that up I have it in my open Office
setup separated by semicolons that's
important to notice
and we open that up you'll see we have
go all the way down here it looks like
1600 lines of data minus the first one
so 15
1599 lines and we have a number of
features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it five six seven it's not really I'm
not sure how how high of a level it goes
but I don't see anything over a seven so
it's kind of five through seven is what
I see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with I didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates pH density total
sulfur dioxide and so on those are all
the features we're going to be looking
at and since this is a pandas we'll just
do wine head and that prints our first
five rolls rows of data that's of course
a pandas command and we can see that
looks very similar to what we're looking
at before we have everything across here
it's automatically assigned an index on
the left
just does if you don't give it an index
and for the column names it has assigned
the first row so we have our first row
of data pulled off the our comma
separated variable file in this case
semicolon separated and it shows the
different features going across and we
have what one two three four five six
seven eight nine ten eleven features 12
including quality but that's the one we
want to work on and understand and then
because we're in Panda's data frame we
can also do wine dot info and let's go
ahead and run that this tells us a lot
about our variables we're working with
you'll see here that there is
1599 that's what I said from the
spreadsheet so that looks correct
non-null float 64. this is a very
important information especially the
non-nullness there's no null values in
here that can really trip us up in
pre-processing and there's a number of
ways to process non-null values one is
just to delete that data out of there so
if you have enough data in there you
might just delete your non-null values
another one is to fill that information
in with like the average or the most
common values or other such means but
we're not going to have to worry about
that but we'll look at another way
because we can also do wine is null and
sum it up and this will give us a
similar it won't tell us that these are
float values but it will give us a
summation oops there we go let me run
the he'll give us a summation on here
how many null values in each one so if
you wanted to you know from here you
would be able to say okay this is a null
value but she doesn't tell you how many
are null values this one would clearly
tell you that you have maybe five null
values here two null values here and you
might just if he had only seven null
values and all that different data you'd
probably just delete them out where if
ninety percent of the data was null
values you might rethink either a
different data collection setup or find
a different way to deal with the null
values we'll talk about that just a
little bit in the models too because the
models themselves have some built-in
features especially the forest model
which we're going to look at at this
point we need to make a choice and to
keep it simple we're going to do a
little pre-processing of the data and
we're going to create some bins and bins
we're going to do is 2 comma 6.5 comma
8. what this means is that we're going
to take those values if you remember up
here let me just scroll back up here we
had our quality the quality comes out
between two and eight basically or one
and eight we have five five five six you
can see just in the just in the first
five lines of variation in quality we're
going to separate that into just two
bins of quality and so we've decided to
create two bins that we have bad and
good it's going to be the labels on
those two bins we have a spread of 6.5
and an exact index of eight the exact
index is because we're doing zero to
eight on there the 6.5 we can change we
could actually make this smaller or
greater but we're only looking for the
really good wind we're not looking for
the zero one two three four five six
we're looking for wines with seven or
eight on us so high quality you know
this is what I want to put on my dinner
table at night
I want to taste the good wine not the
semi-good wine or mediocre wine and then
this is a panda so PD remember stands
for pandas Panda's cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bends to comma 6.58 so two different
bins and our labels bad and good and we
can also do let me just do it this way
wine quality since that's what we're
working on and let's look at unique
another pandas command and we'll run
this and I get this lovely error why did
I get an error well because I replaced
wine quality and I did this cut here
which changes things on here so I
literally altered one of the variables
is saved in the memory so we'll go up
here to the kernel restart and run all
it starts it from the very beginning and
we can see here that that fixes the
error because I'm not cutting something
that's already been cut we have our wine
quality unique and the wine quality
unique is a bad or good so we have two
qualities objects bad is less than good
bad is going to be zero and good is
going to be one and to make that happen
we need to actually encode it so we'll
use the label quality equals label
encoder and the label encoder let me
just go back there so this is part of
sklearn that was one of the things we
imported it was a label encoder you can
see that right here from the
sklearn.processing import standard
scalar which we're going to use in a
minute and label encoder and that's what
tells it to use that equals 0 and good
equals one and we'll go ahead and run
that and then we need to apply it to the
data and when we do that we take our
wine quality that we had before and
we're going to set that equal to label
quality which is our encoder and let's
look at this line right here we have dot
fit transform and you'll see this in the
pre-processing these are the most common
used is fit transform and fit transform
because they're so often that you're
also transferring the data when you fit
it they just combine them into one
command and we're just going to take the
wine quality feed it back into there and
put that back in our wine quality setup
and run that and now when we do the wine
and the head of the first five values
and we go ahead and run this you can see
right here underneath quality zero zero
zero I have to go down a little further
to look at the better wines
let's see if we have some that are ones
yeah there we go there's some ones down
here so when you look at 10 of them you
can see all the way down to zero or one
that's our quality and again we're
looking at high quality we're looking at
the seven and the eights or six point
five and up and let's go ahead and grab
our or was it here we go wine quality
and let's take another look at what else
more information about the wine quality
itself and we can do a simple pandas
thing value counts hopefully I type that
in there correctly and we can see that
we only have 217 of our wines which are
going to be the higher quality so 217
and the rest of them fall into the bad
bucket the zero which is uh 1382 so
again we're just looking for the top
percentage of these the top what is that
it's probably about a little a little
under 20 percent on there so we're
looking for our top wines our seven and
eights and let's use our let's plot this
on a graph so we take a look at this and
the S and S if you remember correctly
that is let me just go back to the top
that's our Seaborn Seaborn sits on top
of matplot Library it has a lot of added
features plus all the features of the
matplot library and also makes it quick
and easy to put out a graph we'll do a
simple bar graph and they actually call
it count plot and then we want to just
do count plot the wine quality so let's
put our wine quality in there and let's
go ahead and run this and see what that
looks like and nice in line remember
this is why we did the inline so make
sure it appears in here and you can see
the blue space or the first space
represents low quality wine and our
second bar is a high quality line and
you can see that we're just looking at
the top quality wine here most of wine
we want to just give it away to the
neighbors now maybe if you don't like
your neighbors maybe give them the good
quality wine and I don't know what you
do with the bed quality wine I guess use
it for cooking there we go but you can
see here it forms a nice little graph
for us with the Seaborn on there and you
can see our setup on that so now we've
looked at we've done some pre-processing
we've described our data a little bit we
have a picture of how much of the wine
what we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment if you're in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what in this
case quality what we're looking for and
we can take this we can go wine it's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality I'll make
sure we have our axes equals one if you
left it out it would still come out
correctly just because of the way it
processes on the defaults and then our y
if we're going to remove quality for our
X this is going to be Y and it is just
the quality that we're looking at for y
so we put that in there and we'll go
ahead and run this so now we've
separated the features that we want to
use to predict the quality of the wine
and the quality itself the next step is
if you're going to create a data set in
a model we got to know how good our
model is so we're going to split the
data train and test splitting data and
this is one of the packages we imported
from sklearn and the actual package was
train test split and we're going to do X
Y test size 0.2 random State 42 and this
returns four variables and most common
you'll see is capital x train so we're
going to train our set with capital X
test that's the data we're going to keep
on the side to test it with why train
why remember stands for the quality or
the answer we're looking for so when we
train it we're going to use x train and
Y train and then why test to see how
good our X test does and the train test
split let me just go back up to the top
that was part of the sklearn model
selection import train test split there
is a lot of ways to split data up this
is when you're first starting you do
your first model you probably start with
the basics on here you have one test for
training one for test our test size is
0.2 or 20 percent and random State just
means we just start with a it's like a
random seed number so it's not too
important back there we're randomly
selecting which ones we're going to use
since this is the most common way this
is what we're going to use today there
is and it's not even an sklearn package
yet so someone's still putting it in
there one of the new things they do is
they split the data into thirds and then
they'll run the model on each of they
combine each of those thirds into
two-thirds for training and one for
testing and so you actually go through
all the data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you know have a test site
one test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out
works great so we have our X train our X
test our y train and our y test and then
we need to go ahead and do the scalar
and let's talk about this because this
is really important some models do not
need to have scaling going on most
models do and so we create our scalar
variable we'll call it SC standard
scalar and if you remember correctly we
imported that here wrong with the label
encoder the standard scalar setup so
there's our scalar and this is going to
convert the values instead of having
some values that go from zero if you
remember up here we had some values are
54 60 40 59 102. so our total sulfur
dioxide would have these huge values
coming into our model and some models
would look at that and they'd become
very biased to sulfur dioxide and have
the hugest impact and then a value that
had
0.076.098 our chlorides would have very
little impact because it's such a small
number so when we take the scalar we
kind of level the playing field and
depending on our scalar it sets it up
between 0 and 1 a lot of times is what
it does let's go ahead and take a look
at that and we'll go ahead and start
with our X train and our X train equals
SC fit transform we talked about that
earlier that's an sklearn setup it's
going to both fit and transform our X
train into our X train variable and if
we have an X train we also need to do
that to our test and this is important
because you need to note that you don't
want to refit the data we want to use
the same fit we used on the training as
on the testing otherwise you get
different results and so we'll do just
oops not fit
form we're only going to transform the
test side of the data so here's our X
test that we want to transform and let's
go ahead and run that and just so we
have an idea let's go ahead and take and
just print out our X train oh let's do
first 10 variables very similar to the
way you do with the head on a data frame
you can see here our variables are now
much more uniform and they've scaled
them to the same scale so they're
between certain numbers and with the
basic scalar you can fine tune it I just
let it do its defaults on this and
that's fine for what we're doing in most
cases you don't really need to mess with
it too much it does look like it goes
between like minus probably minus two to
two or something like that that's just
looking at the train variable I'm going
to cut that one out of there so before
we actually build the models and start
discussing the SK learn models we're
going to use we covered a lot of ground
here most of when you're working with
these models you put a lot of work into
pre prepping the data so we looked at
the day data notice that it's separated
loaded it up we went in there we found
out there's no null values that's hard
to say no no values we have there's none
there's none nobody I can't say it
and of course we sum it up if you had a
lot of null values this would be really
important coming in here so is there a
null summary we looked at pre-processing
the data as far as the quality and we're
looking at the bins so this would be
something you might start playing with
maybe you don't want super fine wine you
don't want the seven and eights maybe
you want to split this differently so
certainly you can play with the bins and
get different values and make the bins
smaller or lean more towards the lower
quality so you then have like medium to
high quality and we went ahead and gave
it the labels again this is all pandas
we're doing in here setting up with
unique labels and group names bad good
bad is less than good that could be so
important you don't know how many times
people go through these models and they
have them reversed or something and then
they go back and they're like why is
this data not looking correct so it's
important to remember what you're doing
up here and double check it and we used
our label encoder so that was to set
that up is equality zero one good in
this case we have bad good zero one and
we just double check that to make sure
that's what came up in the quality there
and then we threw it into a graph
because people like to see graphs I
don't know about you but you start
looking all these numbers and all this
text and you get down here and you say
oh yes you know here this is how much of
the wine we're going to label as subpar
not good this is how much we're going to
label as good and then we got down here
to finally separating out our data so
it's ready to go into the models and the
models take X and A Y in this case X is
all of our features minus the one we're
looking for and then Y is the features
we're looking for so in this case we
dropped quality and in the Y case we
added quality and then because we need
to have a training set and a test set so
we can see how good our models do we
went ahead and split the models up X
train X test y train y test and that's
using the train test split which is part
of the sklearn package and we did as far
as our testing size 0.2 or 20 percent
the default is 25 so if you leave that
out it'll default setup and we did a
random State equals 42. if you leave
that out it'll use a random State I
believe it's default one I'd have to
look that back up and then finally we
scaled the data this is so important to
scale the data going back up to here if
you have something that's coming out as
a hundred it's going to really outweigh
something that's 0.071 that's not in all
the models different models handle it
differently and as we look at the
different models I'll talk a little bit
about that we're going to only look at
three models today three of the top
models used for this and see how they
compare and how the numbers come out
between them so we're going to look at
three different setups change my cell
here to mark down there we go and we're
going to start with the random Forest
classifier so the three sets we're
looking at is the random Forest
classifier support vector classifier and
then a neural network now we start with
the random Forest classifier because it
has the least amount of Parts moving
parts to fine tune and let's go ahead
and put this in here so we're going to
call it RFC for random force classifier
and if you remember we imported that so
let me go back up here to the top real
quick and we did an import of the random
fourth classifier from sklearn Ensemble
and then we'll all we also let me just
point this out here's our svm where we
inputted our support Vector classifier
so svm is support Vector model support
vector classifier and then we also have
our neural network and we're going to
from there the multi-layered precipitron
classifier kind of a mouthful for the P
percepatron don't worry too much about
that name it's just it's a neural
Network there's a lot of different
options on there and setups which is
where they came up with the precipitron
but so we have our three different
models we're going to go through one
here and then we're going to weigh them
here's our metric so we're going to use
a confusion metrics also from the SK
learn package to see how good our model
does with our split so let's go back
down there and take a look at that and
we have our RFS equals random forest
classifier and we have n estimators
equals 200. this is the only value you
play with with a random Force classifier
how many forests do you need or how many
trees in the forest and so how many
models are in here that makes it pretty
good as a startup model because you're
only playing with one number and it's
pretty clear what it is and you can
lower this number or raise it usually
start up with a higher number and then
bring it down to see if it keeps the
same value so you have less you know the
smaller the model the better the fit and
it's easier to send out to somebody else
if you're going to distribute it now the
random 4 is classified fire everything I
read says it's used for kind of a
medium-sized data set so you can run it
in on Big Data you can run it on smaller
data obviously but tends to work best in
the mid-range and we'll go ahead and
take our RFC and I just copied this from
the other side dot fit X train comma y
train so we're sending it our features
and then the quality in the Y train what
we want to predict in there and we just
do a simple fit now remember this is
sklearn so everything is fit or
transform another one is predict which
we'll do in just a second here let's do
that now predict our FC equals and it's
our RFC model predict and what are we
predicting on well we trained it with
our train values so now we need our test
our X test so this has done it this is
going to do this is the three lanes of
code we need to create our random Force
variable fit our training data to it so
we're programming it to fit in this case
it's got 200 different trees it's going
to build and then we're going to predict
on here let me go ahead and just run
that and we can actually do something
like oh let's do predict
RFC just real quick we'll look at the
first 20 variables of it let's go ahead
and run that and in our first 20
variables we have three wines that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you can remember correctly
we'll go ahead and take this out of here
this is based on our test so these are
the first 20 values in our test and this
has as you can see all the different
features listed in there and they've
been scaled so when you look at these
they're a little bit confusing to look
at and hard to read but we have there's
a minus 01 so this is 0.36 minus so one
so 0.164 minus
0.09 or no it's still minus one so minus
0.9 all between 0 and 1 on here I think
I was confused earlier and I said 0
between 2 negative two but it's between
minus one and one which is what it
should be in the scale and we'll go
ahead and just cut that out of there run
this we have our setup button here so
now that we've run the prediction and we
have predicted values well one you could
publish them but what do we do with them
well we want to do with them is we want
to see how well our model model
performed that's the whole reason for
splitting it between a training and
testing model and for that remember we
imported the classification report
that was again from the SK learn there's
our confusion Matrix and classification
report and the classification report
actually sits on the confusion Matrix so
it uses that information and our
classification report we want to know
how good our y test that's the actual
values versus our predicted RFC so we'll
go ahead and print this report out and
let's take a look and we can see here we
have a Precision out of the zero we had
about 0.92 that were labeled as a bad
that were actually bad and out of
precision for the quality wines we're
running about 78 percent so you kind of
give us an overall 90 percent and you
can see our F1 score our support set up
on there a recall you could also do the
confusion Matrix sign here which gives
you a little bit more information but
for this this is going to be good enough
for right now we're just going to look
at how good this model was because we
want to compare the random force
classifier with the other two models and
you know what let's go ahead and put in
the confusion Matrix just so you can see
you that on there with Y test and
prediction RFC so in the confusion
Matrix we can see here that we had
266 correct and seven wrong these are
the missed labels for bad wine and we
had a lot of mislabels for good wine so
our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead of cell
and change cell type to mark down and
run that so we have a nice label let's
look at our svm classifier our support
Vector model and this should look
familiar we have our clf we're going to
create what's it we'll call it just like
we call this an RFC and then we'll have
our clf DOT fit and this should be
identical to up above X
train comma y train and just like we did
before let's go ahead and do the
prediction and here is our clf predict
and it's going to equal the clf dot
predict and we want to go ahead and use
x underscore test and right about now
you can realize that you can create
these different models and actually just
create a loop to go through your
different models and put the data in and
that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and I'm just going to copy this right
off of here
they say you shouldn't copy and paste
your code and the reason is is when you
go in here and edit it
you invariably will miss something we
only have two lines I think I'm safe to
do it today and let's go ahead and run
this
and let's take a look how the svm
classifier came out so up here we had a
90 and down here we're running about an
86 percent so it's not doing as good now
remember we randomly split the data so
if I run this a bunch of times you'll
see some changes down here so these
numbers this size of data if I read it
100 times it would probably be within
plus or minus three or four on here in
fact if I ran this 100 times you'd
probably see these come out almost the
same as far as how well they do in
classification and then on the confusion
Matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12.
so it's doing not quite as good that
shows up here 71 percent versus 78
percent and then if we're going to do a
svm classifier we also want to show you
one more and before I do that I'll kind
of tease you a little bit here before we
jump into neural networks the big save
all deep learning because everything
else must be shallow learning that's a
joke let's just talk a little bit about
the svm versus the random Forest
classifier the svm tends to work better
on smaller numbers it also works really
good on because a lot of times you
convert things into numbers and bins and
things like that the random Forest tends
to do better with those at least that's
my brief experience with it or if you
have just a lot of raw data coming in
the svm is usually the fastest and
easiest to apply model on there so they
each have their own benefits you'll find
them again that when you run these like
a hundred times difference between these
two on a data set like this is going to
just go away there's Randomness involved
depending on which data we took and how
they classify them the big one is the
neural networks and this is what makes
the neural networks nice is they can do
they can look into huge amounts of data
so for a project like this you probably
don't need a neural network on this but
it's important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series and the Order of the
words is really important same thing
with starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when I say lots of
data coming in I'm not talking about
just the high lows that you can run an
svm on real easily I'm talking about the
data that comes in where you have maybe
you pulled off the Twitter feeds and
have word counts going on and you've
pulled off the the different news feeds
the business are looking at and the
different releases when they release the
different reports so you have all this
different data coming in and the neural
network does really good with that
pictures picture processing Now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by Google it has a neural
network for doing it's kind of goofy but
you can put Little Star Wars Androids
dancing around your pictures and things
things like that that's all done with
the neural network so it has a lot of
different uses but it's also requires a
lot of data and is a little heavy-handed
for something like this and this should
now look familiar because we've done it
twice before we have our multi-layered
precipitron classifier we'll call it an
mlpc and it's this is what we imported
mlpc classifier there's a lot of
settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then I went
ahead and just did three layers probably
get by with a lot less on this but yeah
I didn't want to sit and play with it
all afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with over fitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500. the default's 200
because I use three layers of 11 each
which is by the way kind of a default I
use I realized that usually have about
three layers going down and the number
of features going across you'll see that
it's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
X train y train and then we're going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict X test
just put that down here dot predict test
and if I run that we've now programmed
it we now have our prediction here same
as before and we'll go ahead and do the
copy print again always be careful with
the copy paste not because you always
run the chance of missing one of these
variables so if you're doing a lot of
coding you might want to skip that copy
and paste and just type it in let's go
ahead and run this and see what that
looks like and we came up with an 88
percent we're going to compare that with
the 86 from our tree our svm classifier
and our 90 from the random forest
classifier and keep in mind random
Forest classifiers they do good on
mid-size data the SPM on smaller amounts
of data although to be honest I don't
think that's initially the split between
the two and these things will actually
come together if you random a number of
times and we can see down here the noun
of good wines mislabeled with the setup
on there it's on par with our random
Forest so it had 22 25 should be
surprised it's identical it just didn't
do as good with the bad wines labeling
what's a bad one and what's not so yeah
because they had 266 and seven we head
down here 260 and 13. so mislabeled a
couple of the bad wines as good wines so
we've explored three of these basic
classifiers these are probably the three
most widely used right now I might even
throw in the random tree if we open up
their website we go under supervised
learning there's a linear model we
didn't do that almost most of data
usually just start with a linear model
because it's going to process the
quickest I'm going to use the least
amount of resources but you can see they
have linear quadratic they have kernel
Ridge there's our support Vector of
stochastic gradient nearest neighbors
nearest neighbors is another common one
that's used a lot very similar to the
svm gaussian process cross decomposition
naive Bayes this is more of an
intellectual one that I don't see used a
lot but it's like the basis of a lot of
other things decision tree there's
another one that's used a lot Ensemble
methods not as much multi-class and
multi-label algorithms feature selection
neural networks that's the other one we
use down here and of course the forest
so you can see there's a in sklearn
there are so many different options and
they've just developed them over the
years we covered three of the most
commonly used ones in here and went over
a little bit over why they're different
neural network just because it's fun to
work in deep learning and not in Shallow
learning as I told you that doesn't mean
that the svm is actually shallow it does
a lot of it covers a lot of things and
same thing with the decision for the
random forest classifier and we notice
that there's a number of other different
classifier options in there these are
just the three most common ones and I'd
probably throw the nearest neighbor in
there and the decision tree which is
usually part of the decision for us
depending on what the back end you're
using and since as human beings if I was
in the shareholders office I wouldn't
want to leave them with a confusion
Matrix they need that information for
making decisions but we want to give
them just one particular score and so I
would go ahead and we have rsk learn
metrics we're going to import the
accuracy score and I'm just going to do
this on the random 4 since that was our
best model and we have our CM accuracy
score and I forgot to print it remember
in jupyter Notebook we can just do the
last variable we leave out there it'll
print and so our CM accurate score we
get is 90 percent and that matches up
here we should already see that up here
in Precision so you can either quote
that but a lot of times people like to
see it highlighted at the very end this
is our Precision on this model and then
the final stage is we would like to use
this for future so let's go ahead and
take our wine if you remember correctly
we'll do one head of 10. we'll run that
remember our original data set we've
gone through so many steps now we're
going to go back to the original data
and we can see here we have our top 10
our top 10 on the list only two of them
make it as having high enough quality
wine for us to be interested in them and
then let's go ahead and create some data
here we'll call it X Nu equals and this
is important this data has to be we just
kind of randomly selected did some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our X Nu
equals
7.3.58 and so on and then it is so
important this is where people forget
this step X Nu equals SC remember SC
that was our standard scalar variable we
created if we go right back up here
before we did anything else we created
an sc we fit it and we transformed it
and then we need to do what transform
the data we're going to feed in
so we're going to go back down here and
we're going to transform our X new and
then we're going to go ahead and use the
where are we at here we go our random
forest and if you remember all it is is
our RFC predict model right there let's
go ahead and just grab that down here
and so our y Nu equals here's our RFC
predict and we can do our X new in and
then it's kind of nice to know what it
actually puts out so according to this
it should print out what our prediction
is for this wine and oh it's a bad wine
okay so we didn't pick out a good wine
for our X new and that should be
expected most of wine if you remember
correctly only a small percentage of the
wine met our quality requirements so we
can look at this and say oh we'll have
to try another wine out which is fine by
me because I like to try out new wines
and I certainly have a collection of old
wine bottles and very few of them match
but you can see here we've gone through
the whole process just a quick rehash we
had our Imports we touched a lot on the
SK learn our random Forest our svm and
our MLP classifier so we had our support
vector classifier
our random forests and we have our
neural network three of the top used
classifiers in the sklearn system and we
also have our confusion Matrix and our
classification report which we used our
standard scalar for scaling it and our
label encoder and of course we need to
go ahead and split our data up in our
implot line train and we explored the
data in here for null values we set up
our quality into bins we took a look at
the data and what we actually have and
put a nice little plot to show our
quality what we're looking at and then
we went through our three different
models and it's always interesting
because you spend so much time getting
to these models and then you kind of go
through the models and play with them
until you get the best training on there
without becoming biased that's always a
challenge is to not over train your data
to the point where you're training it to
fit the test value and finally we went
ahead and actually used it and applied
it to a new wine which unfortunately
didn't make the cut it's going to be the
one that we drink a glass out of and
save the rest for cooking
of course that's according to the random
Forest on there because we use the best
model that it came up with there's no
better time to train yourself in the
exciting field of machine learning if
you're looking for a course that costs
everything from the fundamentals to
Advanced Techniques for that accelerate
your career in a and ml with a
comprehensive postgraduate programming
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and ask me anything sessions by IBM with
three Capstone projects and 25 plus
industry projects using real data sets
from Twitter Uber and more you'll gain
practical experience master classes by
Caltech faculty and IBM experts and show
top-notch education simply learns job
assist helps you notice by Leading
companies this program calls statistics
python supervised and unsupervised
learning NLP neural networks computer
vision gns Keras tensorflow and many
more skills enroll now and then look
exciting Ai and ml opportunities the
course link is mentioned in the
description box below base classifier
have you ever won wondered how your mail
provider implements spam filtering or
how online news channels perform news
text classification or how companies
perform sentimental analysis of Their
audience on social media all of this and
more is done through a machine learning
algorithm called naive Bayes classifier
welcome to naive Bayes tutorial my name
is Richard kirschner I'm with the simply
learned team that's
www.simplylearn.com get certified get
ahead what's in it for you we'll start
with what is naive Bayes a basic
overview of how it works we'll get into
naive Bayes and machine learning where
it fits in with our other machine
learning tools why do we need naive
Bayes and understanding naive Bayes
classifier a much more in depth of how
the math works in the background finally
we'll get into the advantages of the
Nave Bayes classifier in the machine
learning setup and then we'll roll up
our sleeves and do my favorite part
we'll actually do some python coding and
do some text classification using the
naive Bayes what is 9 Bays let's start
with a basic introduction to the Bayes
theorem named after Thomas Bayes from
the 1700s who first coined this in the
western literature naive Bayes
classifier works on the principle of
conditional probability as given by the
Bayes theorem before we move ahead let
us go through some of the simple
Concepts in the probability that we will
be using let us consider the following
example of tossing two coins here we
have two quarters and if we look at all
the different possibilities of what they
can come up as we get that they could
come up as head heads they come up as
head tell tell head and Telltale when
doing the math on probability we usually
denote probability as a p a capital P so
the probability of getting two heads
equals one-fourth you can see in our
data set we have two heads and this
occurs once out of the four
possibilities and then the probability
of at least one tail occurs three
quarters of the time you'll see in three
of the twin tosses we have tails in them
and out of four that's three fourths and
then the probability of the second coin
being head given the first coin is 10 L
is one half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the Bayes
theorem gives us a conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Bayes theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply a
Bayes theorem to our example here we
have our two quarters and we'll notice
that the first two probabilities of
getting two heads and at least one tail
we compute directly off the data so you
can easily see that we have one example
HH out of four one fourth and we have
three with tells in them giving us three
quarters or three-fourths seventy-five
percent the second condition the second
set three and four we're gonna explore a
little bit more in detail now we stick
to a simple example with two coins
because you can easily understand the
math the probability of throwing a tail
doesn't matter what comes before it and
the same with the head so still going to
be fifty percent or one half but when
that com when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
it really comes in handy but let's stick
to the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being tells given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one-half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculate relates to
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
Bayes and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervisors learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease ease or
other elements and news classification
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with a
naive Bayes understanding naive Bayes
classifier now we already went through a
basic understanding with the coins and
the two heads and two tells and head
Telltale heads Etc we're going to do
just a quick review on that and remind
you that the naive Bayes classifier is
based on the Bayes theorem which gives a
conditional probability of event a given
event B and that's where the probability
of a given b equals the probability of B
given a times probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easy come up with a
new formula for the probability of B to
me staring at these algebraic functions
kind of gives me a slight headache as
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy be a now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery and day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
that's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a twenty
percent chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B of A in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 0.33 or 33
percent let's take a look at this at
different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
look we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.36 7 which
equals or 0.33 times 0.2 over 0.367
which equals 0.179 or roughly 17 to 18
percent and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full tape table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we're actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday do they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals a probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that just
a second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of freedom delivery times
the probability is the day equal holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the 6 out of 30. we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 23 3 out of 30 had a
yes for free delivery and 11 out of 30
were on a holiday we plug all those
numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals
0.178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're owning a
shop that's like is the bottom line is
you want to make some money so you can
keep your shop open and have a living
now I promised you that we were going to
be finishing up the math here with a few
pages so we're going to move on and
we're going to do two steps the first
step is I want you to understand why you
went under why you want to use the naive
Bayes what are the advantages of naive
bays and then once we understand those
advantages we just look at that briefly
then we're going to dive in and do some
python coding advantages of naive Bayes
classifier so let's take a look at the
six advantages of the naive Bayes
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top of the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because of
two overlap they can then predict the
unknowns for the group that they haven't
done the second study on or vice versa
so it's very powerful in that it is not
sensitive to the irrelevant features and
in fact you can use it to help predict
features that aren't even in there so
now we're down to my favorite part we're
going to roll up our sleeves and do some
actual programming we're going to do the
use case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bayes so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you a
graphical what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup that we want the graphs
to show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have a
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so those you'll
see that those three lines of code here
are just importing the graph function so
we can graph it and as a data science
test you always want to graph your data
and have some kind of visual it's really
hard just to shove numbers in front of
people and they look at it and it
doesn't mean anything and then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.miscellaneous
and it goes all the way down to talk
politics.miscellaneous talk
religion.missella genius these are the
categories we've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
Links of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article
Etc and you can scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we
have the print module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
is and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model dot predict most of the sklearn
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our jupyter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the in line
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix we have
our test Target our test data that was
not part of the training very important
in data science we always keep our test
data separate otherwise it's not a valid
model if we can't properly test it with
new data and this is the labels we
created from that test data these are
the ones that we predict it's going to
be so we go in and we create our SN heat
map the SNS is our Seaborn which sits on
top of the pi plot so we create an
sns.heat map we take our confusion
Matrix and it's going to be matte dot T
and do we have other variables that go
into the sns.heat map we're not going to
go into detail what all the variables
mean The annotation equals true that's
what tells it to put the numbers here so
you have the 166 the one the zero zero
zero one format d and c bar equals false
have to do with the format if you take
those out you'll see that some things
disappear and then the X tick labels and
the y t labels those are our Target
names and you can see right here that's
the alt atheism comp graphics composms
windows.miscellaneous and then finally
we have our plt.x label remember the SNS
or the Seaborn sits on top of our
matplot library our PLT and so we want
to just tell it X label equals a true is
is true the labels are true and then the
Y label is prediction label so when we
say a true this is what it actually is
and the prediction is what we predicted
and let's look at this graph because
that's probably a little confusing the
way we rattled through it and what I'm
going to do is I'm going to go ahead and
flip back to the slides because they
have a black background they put in
there that helps it shine a little bit
better so you can see the graph a little
bit easier so in reading this graph what
we want to look at is how the color
scheme has come out and you'll see a
line right down the middle diagonally
from upper left to bottom right what
that is is if you look at the labels we
have our predicted label on the left and
our true label on the right those are
the numbers where the prediction and the
true come together and this is what we
want to see is we want to see those lit
up that's what that heat map does is you
can see that it did a good job of
finding those data and you'll notice
that there's a couple of red spots on
there where it missed you know it's a
little confused we talk about talk
religion miscellaneous versus talk
politics miscellaneous social religion
Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this function let me just expand that
just a notch here there we go I like
mine in big letters predict categories
we want to predict the category we're
going to send it as a string and then
we're sending it train equals train we
have our training model and then we had
our pipeline model equals model this way
we don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model dot predict s so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
I'll put that through our naive Bayes
and from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train.target namespredict of zero and
remember that the train.target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual number numbers are let's
say 0 equals alt atheism so we're going
to convert that 0 to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going to go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train.target underscore names
it'll automatically print that for you
in your own ID you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous we're going to
drill down to what is classification I
touched upon all those different aspects
that classification is the yes no you're
classifying it as something and the
reason they spent a lot of time on
classification is because it's one of
the easier ones to understand it's a
little easier to track it's a little
easier to figure out the errors on it
and when we're talking about a confusion
Matrix we're usually talking about
classification and I'll show you why
here in a little bit classification is a
process of classifying data into
different categories based on some of
their common characteristics example a
system classifying spam email
and you can see here we have our email
coming in some of them are spam some of
them are not
in the classifier has to only say yes or
no it's spam or it's not spam
need for confusion matrixes
classification models have multiple
output categories most error measures
will tell us the total error in our
model but we cannot use it to find out
individual instances of errors in our
models so you have your input coming in
you have your classifier it measures the
error and it says oh 53 of these are
correct
but we don't know which 53 percent are
correct is it 53 percent correct on
guessing on the spam is it 23 guessing
on spam and 27 percent guessing on
what's not spam
this is where the confusion Matrix comes
in
so during the classification we also
have to overcome the limitations of
accuracy accuracy can be misleading for
classification problems if there is a
significant class imbalance a model
might predict the majority class for all
cases and have a high accuracy score
and so you can see here we have our
email coming in and there's two spams
the classifier comes in and goes hey it
only catches one of those spams and it
misclassifies one that's not spam
so our model predicted 8 out of 10
instances and we'll have an accuracy of
80 percent but is it classifying
correctly
a confusion Matrix represents a table
layout of the different outcomes of
prediction
and results of a classification problem
and helps visualize its outcomes
and so you see here we have our simple
chart predicted and actual the confusion
Matrix helps us identify the correct
predictions of a model for different
individual classes as well as the errors
so you'll see here that the values
predicted by our classifier are along
the rows this is what we're going to
guess it is or CL our model is guessing
what this is based on its training so
we've already trained the model to guess
whether it's spam or not spam or
whatever it is you're working on
and then the actual values of our data
set are along the columns
so this is the actual value that's
supposed to be
people who can speak English will be
classified as positives so because they
have a remember zero one do you speak
English yes no and you could extend this
that they might have do you speak French
do you speak whatever language is and so
you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
English will be classified as negatives
so there'll be a zero so yeah zero ones
the number of times are actual positive
values are equal to predicted positive
values gives us true positive TP the
number of times are actual negative
values are equal to predictive negative
values gives us true negative TN
the number of times our model wrongly
predicts negative values as positives
gives us a false positive
FP
and you'll see when you're working with
these a lot you know memorizing that
it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the numbers times our model wrongly
predicts positive values as negatives
gives us a false negative FP
now I'm going to do a quick step out
here
let's say you're working in the medical
and we're talking about cancer
uh do you really want a bunch of false
negatives
you want zero under false negative so
when we look at this confusion Matrix if
you have five percent false positives
and five percent false negatives it'd be
much better to even have twenty percent
false positives because they go in and
test it in zero false negatives
let's say it might be true if you're
working on uh say a car driving is this
a safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff
so again when you're working on the
project or whatever it is you're working
on this chart suddenly has huge value we
were talking about spam email how many
important emails say from your banking
overdraft charge coming in that you want
to be a a true false negative you don't
want it to go in the spam folder
likewise you want to get as much of the
spam out of there but you don't want to
miss anything really important
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy precision recall and F1
score the F1 score is the one I usually
hear the most and accuracy is usually
what you put on your chart when you're
sending in front of the shareholders how
accurate is it people understand
accuracy
F1 score is a little bit more on the
math side and so you got to be a little
careful when you're quoting F1 scores in
the when you're sitting there with all
the shareholders because a lot of them
will just glaze over so confusion Matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right it is the sum of all
True Values divided by the total values
and this makes sense again it's one of
those things
I don't want to you know it depends on
what you're looking for are you looking
for uh not to miss any spam mails are
you looking to drive down the road and
not run anybody over
precision is used to calculate the
model's ability to classify positive
values correctly it answers the question
when the model predicts a positive value
how often is it right it is the true
positive divided by the total number of
predicted positive values again in this
one depends on what project you're
working on whether this is what you're
going to be focusing on
so recall it is used to calculate the
model's ability to predict positive
values how often does the model actually
predict the correct positive values
it is a true positive divided by the
total number of actual positive values
and then your F1 score it is the
harmonic mean of recall and precision it
is useful when you need to take both
precision and recall into account
consider the following two confusion
Matrix derived from two different
classifier
to figure out which one performs better
we can find the confusion Matrix for
both of them
and you can see we're back to uh does it
classify whether they can speak English
or are non-speaker they speak some they
don't know the English language and so
we put these two uh confusion matrixes
out here we can go ahead and do the math
behind that we can look up the accuracy
that's a tpn plus TN over the TF plus TN
plus FP plus FN and so we get an
accuracy of 0.8125
and we have a Precision if you do the
Precision which is your TP truth
positive over TP plus FP
we get 0.891
and if we do the recall we'll end up
with the 0.825 that's your TP over TP
plus FN
and then of course your F1 score which
is two times Precision times recall over
Precision plus recall
and we get the 0.857
and if we do that with another model
let's say we had two different models
and we're trying to see which one we
want to use for whatever reason we might
go ahead and compute the same things we
have our accuracy our precision and our
recall and our F1 score
and as we're looking at this we might
look at the accuracy because that's
really what we're interested in is uh
how many people are we able to classify
as being able to speak English I really
don't want to know if I'm you know I I
really don't want to know if they're
non-speakers
um I'd rather Miss 10 people speaking
English instead of 15. and so you can
see from these charts we probably go
with the first model because it does a
better job guessing who speaks English
and has a higher accuracy because in
this case that is what we're looking for
so uh with that we'll go ahead and pull
up a demo so you can see what this looks
like in the python setup in in the
actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda it's a really
good tool to use as far as doing display
in demos
and for quick development as a data
scientist I just love the package
now if you're going to do something
heavier lifting there's some limitations
with anaconda and with the setup but in
general you can do just about anything
in here with your python
and for this we'll go with Jupiter
notebook Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with pi charm if you
work in pi charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up
um I simply learned files I work on and
create a new file called confusion
Matrix demo and the first thing we want
to note is the data we're working with
here I've opened it up in a word pad or
notepad or whatever
uh you can see it's got a row of
headers comma separated and then all the
data going down below
and then I save this in the same file so
I don't have to remember what path I'm
working on of course if you have your
data separated and you're working with a
lot of data you probably want to put it
into a different folder or file
depending on what you're doing
and the first thing we want to do is go
ahead and import our tools we're going
to use the pandas that's our data frame
if you haven't had a chance to work with
the data frame please review Panda's
data frame and go into simply learn you
can pull up the pandas data frame
tutorial on there
and then we're going to use uh the side
kit framework which is all denoted as
sklearn and I can just pull this in you
can see here's the scikit dash learn.org
with the stable version that you can
import into your python
and from here we're going to use the
train test split
for splitting our data we're going to do
some pre-processing we're going to do
use the logistic regression model that's
our actual machine learning model we're
using and then with this core this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report
so let me go ahead and run that and
bring all that information in
and just like we open the file we need
to go ahead and load our data in here so
we're going to go ahead and do our
pandas read CSV
and then just because we're in Jupiter
notebook we can just put data to read
the data in here a lot of times we'll
actually let me just do this I prefer to
do the just the head of the data or the
top part
and you can see we have age sex I'm not
sure what CP stands for test BPS
cholesterol so a lot of different
measurements if you were in this domain
you'd want to know what all these
different measurements mean
I don't want to focus on that too much
because when we're talking about data
science a lot of times you have no idea
what the data means if you've ever
looked up the breast cancer measurement
it's just a bunch of measurements and
numbers unless you're a doctor you're
gonna have no idea what those
measurements mean
but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target so here's our Target value
here
and it's either one or zero
so we have a classifier if you're
dealing with one zero true false what do
you have you have a classifier
and then our X is going to be uh
everything except for the Target so
we're going to go ahead and drop the
target axis equals one remember that's
columns versus the index or rows X is
equal zero would would give you an error
but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have Y data X
data and you can see from the X data we
have the X head and we can go ahead and
just do print
the Y head data
and run that
so this is all loading the data that
we've done so far if there's a confusion
in there go back and rewind the tape and
review it
and then we need to go ahead and split
our data into our X train X test y train
y test and then keep in mind you always
want to split the data before we do the
scalar and the reason is is that you
want the scalar on the training data to
be set on the training data data or fit
to it
but not on the test data think of this
as being out in the field you're not it
could actually alter your results so
it's always important to do make sure
whatever you do to the training data or
whatever fit you're doing is always done
on the training not on the test and then
we want to go ahead and scale the data
now we are working with linear
regression model and I'll mention this
here in a minute when we get to the
actual model
so some sometimes you don't need to
scale when you're working with linear
regression models it's not going to
change your result as much as say a
neural network where it has a huge
impact
uh but we're going to go ahead and take
here's our X train X test y train y test
we create our scalar we go ahead and
scale the scale is going to fit the X
train
and then we're going to go ahead and
take our X train and transform it and
then we also need to take our X test and
transform it based on the scale on here
so that our X is now between that nice
minus one to one and so this is all uh
our pre data setup
and
hopefully all of that looks fairly
familiar to you if you've done a number
of our other classes and you're up to
the setup on here
and then we want to go ahead and do is
create our model and we're going to use
a logistic regression model
and from the logistic regression model
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here
and let's just go ahead and run that and
so now we are we actually have like our
X test and our prediction so if you
remember from
our Matrix we're looking for the actual
versus the prediction and how those
compare
and if I take this back up here you're
going to notice that we imported the
accuracy score the confusion Matrix and
the classification report
and there's of course our logistic
regression the model we're using for
this
and I did mention it's going to talk a
little bit about scalar and the
regression model
the scalar on a lot of your regression
models uh your basic Mass standard
regression models and I'd have to look
it up for the logistic regression model
when you're using a standard regression
model you don't need to scale the data
it's already just built in by the way
the model works
in most cases but if you're in a neural
network and you're there's a lot of
other different setups than you really
want to take this and fit that on there
and so we can go in and do the accuracy
and this is if you remember correctly we
were looking at the accuracy
with the English speaking so this is
saying our accuracy as to whether this
person is I believe this is the heart
data set
um
it's going to be accurate about 85
percent of the time as far as whether
it's going to predict the person's going
to have a heart condition
or the one as it comes up with the zero
one on there
which would mean at this point that you
have an 85 percent being correct on
telling someone they're extremely high
risk for a heart attack kind of thing
and so we want to go ahead and create
our confusion Matrix and let me just do
that
of course the software does everything
for us so we'll go ahead and run this
and you can see right here
um here's our 25
uh
prediction uh correct predictions right
here
and if you remember from our slide I'll
just bring this over so it's a nice
visual we have our true positive false
positive
uh so we had 25 which were true that it
said hey this person is going to be high
risk at heart and we had four that were
still high risk that has said were false
so out of these 25 people or out of
these 29 people and that makes sense
because you have 0.85 out of 29 people
it was correct on 25 of them and so uh
here's our accuracy score we were just
looking at that our accuracy is your
true positive and your true negative
over all of them so how true is it and
there was our accuracy coming up here
0.85 and then we have our nice Matrix
generated from that
and you can see right here is a similar
Matrix we had going for that from the
slide and this starts to let's just
start asking questions at this point
um so if you're in a board meeting or
you're working with this you really want
to start looking at this data here and
saying well
is this good enough is uh this number of
people and hopefully you'd have a much
larger data set in my is my confusion
Matrix showing for the true positive and
false positive is that acceptable for
what we're doing uh and of course if
you're going to put together whatever
data you're putting out you might want
to separate the true negative false
positive false negative true positive
and you can simply do that by doing the
confusion Matrix and then of course the
Ravel part lets you set that up so you
can just split that right up into a nice
Tuple and the final thing we want to
show you here in the coding on this part
is the confusion Matrix metrics
and so we can come in here and just use
the Matrix equals classification report
the Y test and the predict
and then we're going to take that
classification report and go ahead and
print that out
and you can see here it does a nice job
uh giving you your accuracy your micro
average your weighted average you have
your Precision your recall your F1 score
and your support all in one window so
you can start looking at this data and
saying oh okay our precision's at 0.83
uh 0.87 for getting a a positive and
0.83 for the negative site for a zero
and we start talking about whether this
is a valid information or not to use
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that costs everything from the
fundamentals to Advanced Techniques for
that accelerator Credit in a and ml with
a comprehensive postgraduate programming
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and ask me anything sessions by IBM with
three caption projects and 25 plus
industry projects using real data sets
from Twitter Uber and more you'll gain
practical experience master classes by
Caltech faculty and IBM experts and show
top-notch education simple learns job
assist helps you notice by Leading
companies this program calls statistics
python supervised and unsupervised
learning NLP neural networks computer
vision gns Keras tensorflow and any more
skills and all now and then look
exciting Ai and ml opportunities the
course link is mentioned in the
description box below machine learning
has improved our lives in a number of
wonderful ways today let's talk about
some of these I'm Rahul from Simply
learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could help them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record what are
you saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever notice
that these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to Lord's cricket ground the first thing
I would probably do is to get on Google
Maps so search it
and let's put it here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicate that
they are slightly congested and red
means they are heavily congested so
let's look at the map a different
version of the same map and here as I
told you before red means heavily
congested yellow means slow moving and
blue means clear
so how exactly is Google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
with the help of machine learning and
with the help of two important measures
first is the average time that's taken
on specific days at specific times on
that route the second one is the
real-time location data of vehicles from
Google Maps and with the help of sensors
some of the other popular map services
are Bing Maps maps.me and here we go
next up we have social media
personalization so say I want to buy a
drone and I'm on Amazon and I want to
buy a DJI mavic Pro the thing is it's
close to one lap so I don't want to buy
it right now but the next time I'm on
Facebook I'll see an advertisement for
the product next time I'm on YouTube
I'll see an advertisement even on
Instagram I'll see an advertisement so
here with the help of machine learning
Google has understood that I'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what spam and what's not spam
so Gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data Gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that Gmail uses is content
filters header filters General Blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes is that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags
the one known size and duration now this
is used to predict stock market trends
assistive Medical Technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice chemic stock lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and science that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence of sequence learning which is
the same thing that's used with chatbots
here the image recognition happens using
convolutional neural networks and the
text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other so the need for
feature selection to train a model we
collect huge quantities of data to help
the machine learn better consider a
table which contains information on old
cars the model decides which cars must
be crushed for spare parts and when we
talk about huge quantities of data they
save everything from people's favorite
cat pictures to and you can imagine
there's so much data out there even in a
company they'll save all these little
pieces of information about people and
companies and corporations you need some
way to sort through because if you try
to run your models on all of it
you'll end up with these very clunky
models and they might have issues which
we'll talk about later but in this case
we're talking about cars and crushing
but not all this data will be useful to
us some classes are part of the data may
not contribute much to our model and can
be dropped and you can see right here we
have who was the owner of the car in our
data a car will not be crushed based on
its previous owner so yeah that's kind
of a clear cut you can see that why
would I care who owned the car before
once it's in the junkyard and we're
crushing the cars we're not going to
really care about that
so here we have dropped the owner column
as it does not contribute to the model
having too much unnecessary data can
cause the model to be slow the model may
also learn from this irrelevant data and
be inaccurate so feature selection is a
process of reducing the input variable
to your model by using only relevant
data and getting rid of the noise in the
data consider the database given below a
library wants to donate some old books
to make place in their shells for new
books we want to train a model to
automate this task
in this case the color of the book does
not matter and keeping it can cause a
model to learn to donate books based on
color we can remove this as a feature
using feature selection we can optimize
our model in several ways and so the
number one is to prevent learning from
noise and overfitting that's actually
the huge one because we we don't want it
to give us a wrong prediction and that
means also improved accuracy so instead
of giving us a ROM prediction we also
want it to be as close to the right
answer as we can get and we want to
reduce the training time it's an
exponential growth in some of these
models so that each feature you add in
increases that much
more training time we talk about feature
selection methods we put together a nice
little flow chart that shows the various
methods used for feature selection
and you have your basic feature
selection and then there is supervised
and unsupervised under supervised
there's intrinsic wrapper method filter
method
so when we talk about unsupervised
feature selection refers to the method
which does not need the output label
class for feature selection and that was
you can see here under super
unsupervised we don't have I mean that's
really a growing market right now
unsupervised learning and so feature
selection is the same thing supervised
feature selection refers to the method
which uses the output label class for
the feature selection and if you
remember we looked at three different we
have intrinsic wrapper and filter method
so we're going to start with the filter
method on this now remember we know what
the output is so we're going to be
looking at that output to see how well
it's doing versus the features in this
method features are dropped based on
their relation to the output or how they
are correlating to the output
and you can see here we have a set of
features selecting best feature learning
algorithm and then performance and so we
want to find out which feature
correlates to the performance on the
output consider the example of book
classifier here we drop the color column
based on simple deduction and that kind
of sums it up in the in a nutshell is we
want to filter out things that clearly
do not go with what we're looking for if
we look at the wrapper method in the
wrapper method we split our data into
subsets and train a model using this
based on the output of the model we add
and subtract features and train the
model again and you can see here in the
wrapper method we have a set of features
we generate a subset we run it through
the algorithm and we see how each one of
those subset of features performs
consider the book data set by using the
wrapper method we would use a subset of
different features to train the machine
and adjust the subset according to the
output and so you can see here let's say
we take a name and number of times red
and we run just those and we look at the
output and if we looked at them with all
four inputs and look at the output we'd
see quite a different variation in there
and we might say you know what condition
of the book and color really doesn't
affect what we're looking for and you
can see here we've run it on condition
of the book and color
depending on the output of the model we
will choose our final set of features
these features will give us the best
result for our model
and it might come up that the name
number of times red is probably pretty
important the intrinsic method this
method combines the qualities of both
filter and wrapper method to create the
best subset
the model will train and check the
accuracy of different subsets and select
the best among them we kind of looked at
a little overview of some of the stuff
some of the common feature selection
algorithms based on which method they
belong to are given below
and you'll see it's primarily under
supervised there's not like I said a lot
of unsupervised methods and the ones
that are usually used these methods and
finds a way to create a supervised
connection between the data
and we talk about supervised methods we
have our filter method which we talked
about and it uses like the Pearson's
coefficient Chi Squared Anova
coefficient those are all under the
filter method and in the wrapper method
recursive feature elimination so
remember we're choosing a subset and we
want to go through there and look at
each one so you're just doing a lot of
Loops or recursive calculations to see
which one works best and which ones
don't have an impact on the output
and there's a lot of genetic algorithms
to go with this too on the wrapper
method and how they evaluate it and with
the intrinsic method there's the two
main ones we're looking at is the lasso
regularization the lasso algorithms are
basically your standard regression model
so it's finding out how these different
methods fit together and which ones have
the best add together to have the least
amount of error
the other one used in the intrinsic
method is a decision tree it says hey if
this one is this one produces this
result this one produces this result yes
no which way do we go based on the input
and the output variables we can choose
our feature selection model so you have
your numeric input coming in you have
your numeric output if you use the
Pearson's correlation coefficient or
spearman's rank coefficient you can then
select what features you're going to
feed into that specific model and you
maybe have a numerical input and a
categorical input so we're going to be
looking more at Anova correlation
coefficient or Kindles rank coefficient
and if you have a core categorical input
and a numerical output we might be
looking at an over correlation
coefficient in Kindle's rank coefficient
So based on the input and the output
variables we can choose our feature
selection model and you can see we have
categorical to categorical we might be
looking at the chi-squared test
contingency tables and mutual
information let's go and take a look and
see in the python code what we're
talking about here
and I'm going to go ahead and use for my
IDE the Jupiter notebook in the and they
always launch it out of anaconda on here
and we'll go ahead and go up here and
create a new Python 3 module
call it
feature
select
and since we're in Python we're going to
be working mainly with your numpy your
pandas your matplot library so we have
our number array our data frame setup
which goes with the number array the
numpy the pandas data frame and then we
want to go ahead and graph everything so
we're going to import these three
modules
and then we put down other some data
we're going to read this in it's Kobe
Bryant I guess he's a basketball player
our guys in the back we have a number of
them guys both we have a lot of men and
women so it's probably a misnomer our
team in the back they have a some of
them have a liking for basketball and
they know who Kobe Bryant is and they
want to learn a little bit more about
Kobe Bryant what's going in for
whatever's going on with his game in
basketball so we're going to take a look
at him
and once we import the data we can see
what columns are available original
features count so we can see how many
features there are
um the length of it and we'll actually
have a list of them and then print just
to the data head the top five rows
and so when we do this
we can see from the CSV file we have 25
original features
our original features are your action
type combined shot type game event ID
and so forth there's a lot of features
in here that they're recorded on all of
his shots this is what we talk about
like a massive amount of data I mean
imagine people are sitting there and
they record all this stuff and they
import this stuff for different reasons
but depending on what we want to look at
do we really want all those features
maybe the question we're going to ask is
what's the chance if you're making any
one specific shot
um in right from the beginning we can
look at the some of these things and say
team name team name probably I don't
know maybe it does matter because the
other team might be really good at
defense uh game day maybe we don't
really want to look at the game date
team ID definitely not of importance in
any of this so we look at this we have
25 features and some of these features
just really don't matter to us we also
have location X location y latitude and
longitude I'm guessing that's the same
data we've actually imported the the
very similar data maybe they're slightly
zoned differently but as far as our
program we don't want to repeat data
some of the models when you repeat data
into them and this is true for most
models create a huge bias they weigh
that data over other data
so just at a glance these are the things
we're looking at we want to find out
well how do we get this these features
down and get rid of this bias and all
these extraneous features that we don't
really want to spend time running our
models on and programming on
and as I pointed out there's a location
x a location y latitude and longitude
let's just take a look at that and see
what we're looking at here we'll go
ahead and create a plot of these
and we'll just plot we'll do a scatter
plot of location X and location Y and
then we'll do a scatter plot of data
lawn data lat which is probably
longitude and latitude
and the scatter plot is going to
actually put a little Title Here
location and Scatter on there and we'll
just go ahead and plot these and when
you look at this
coming in
these two graphs are pretty identical
except they're flipped and so when we
look at the location from which they're
shooting from they're probably the same
and at this point we can say okay we can
get rid of one of these sets of datas we
don't need both X and Y and latitude and
longitude because it's the same data
coming in and as we look at this
particular data the latitude longitude
we might also ask does it really make a
difference which side of the Court
you're on whether you're on the left
side or the right side
and so we might go ahead and explore
instead of looking at this as
X Y we might look at it as a distance
and an angle and we can easily compute
that and you can see we can create our
data distance equals location X plus the
location y squared standard euclidean
geometry or triangular geometry
hypotenuse squared equals the each side
squared
and then once we've done that
we can also compute the angle so the
data angle is based on the arc tangent
and so forth on here so this is all this
is is we're just going to compute the
angle here and then set that up pi over
2 to get our angle
and we'll go ahead and run that
and you'll see some errors Run come up
and that's because when we took slices
over here we took a slice of a slice
there's ways to fix that but it's really
not important for this example so if you
do see that you want to start looking up
here for
instead of data location X of
not location x0 this would be like I
believe the term is ilo.ilocation if
this was yeah this is in pandas so
there's different things in there but
for this it doesn't really matter these
are just warnings that's all they are
and then let's combine our remaining
minutes and seconds column into one
there's another one so if you remember
up here we're trying to get rid of these
columns
do we really need let me see if I can
find it on here there we go there's our
minutes remaining
um and then they had
what was it it was uh minutes remaining
and seconds column so there's also a
seconds column on here let me see if I
can find that one this is where it
really gets kind of crazy because here's
our seconds remaining so you can see
that here's our minutes remaining this
gets crazy when you're looking at
hundreds of these features and you can
see that if if I'm going to say
write a model that's going to predict a
lot of this and I want it to run in this
case it's a basketball and how good his
shots are as the data comes in let's say
I want to have it run on your phone if
I'm running it across hundreds of
features it's going to just hang up on
your phone where if I can get it down to
just a handful
we'll actually be able to come in here
and run it on a smaller device and not
use up as much memory
or processing power so we'll go ahead
and take data remaining time here
um and data minutes remaining times 60
plus data seconds remaining so we're
just going to combine those and we'll go
ahead and reprint our data so we can see
what we got
um
coming across we have our action type
combined and this is we do this a lot we
want to take a look at oops I got so so
zoomed in let me see if I can zoom out
just a little bit
there we go
boom
all right so we come up here you can see
that we now have our distance our angle
remaining time which is now just a
number uh that computes both the minutes
and seconds together
and we still have we've been adding
columns I thought you said we're
supposed to subtract columns right
um we're going to delete the obsolete
columns when we get to them so we're
just filtering out and then this is the
filter method we're just filtering
through the things uh that we really
don't need and next let's go ahead and
explore team ID and team name so let me
just go ahead and run that and if you
look at this we have Los Angeles Lakers
and then they have the team ID here and
they're unique it's not that's not
really anything that's going to come up
because that's this particular athletes
works for that team so it's the same on
every line so there's another thing we
can filter out on there team ID and T
name is just useless the whole column
contains only one value each and it's
pretty much useless
let's go ahead and take a look at
matchup and opponent that's an
interesting one and we see here that we
have the Lal versus por and the opponent
is POR and IND
again here's a lot of duplicate
information so
this basically contains the same
information on here again we're
filtering all this stuff out and this is
because we're only looking at one
athlete this might change if you're
looking at multiple athletes that kind
of thing now these are easy to see but
we might have something that looks more
like this we might have something where
we're looking at the distance which we
computed and the shot distance are they
the same thing
and what we can do is we can plot that
and plot them against each other on here
and we see it just draws a nice straight
line and so again we're looking at the
same information so again we're
repeating stuff and we really don't want
to be running our model on repeat
information on here so again it contains
the same information so now let's look
at the shot Zone area shot Zone basic
shot Zone range
so now we're looking at the zones and
what does that mean and we'll go ahead
and do this also in a scatter plot in
this case we're going to just create
three of these side by side so we're
going to create our plot figure side 20
by 10 and then we're going to Define our
scatter plot by category feature and
we're going to do each one setup button
here give it a slightly different color
and so our shot Zone area is going to be
plot subplot 131 scatter one three one
is how that's read by the way
meaning that it's number one
we have three across
and this is the first one down so one
one one all right scatter plot by
category is going to be the shot Zone
area we're going to plot that
and then we're going to do the shot Zone
basic and then the shot Zone range and
each just push them through our
definition so each of those areas go
through and you'll see one three one one
three two one three three again it's a 1
by 3 setup and then it's just a place on
each one and so we look at this we can
see that these shots they map out the
same so it's very again redundant
information that should be intuitive
when we're looking at this in this color
graphs
it kind of helps you start looking at
something you it's very intuitive like
this is and you start to realize that
some of this stuff
um you'll be looking for in data you
might not understand
and you'll see these circular patterns
where they match or they mostly match
and you start to realize when you're
looking at these that they're repetitive
data and then you want to explore them
more closely depending on what domain
you're working in so we we look at these
and we look at them and they look just
like the regions of the court but we
already have stored this information in
angle and distance columns so we've seen
this image before let me go back up here
and here's our the similar image
and repeating that image is down here
and so let's go ahead and drop some of
this in the stuff so now let's drop all
the useless columns and we can drop the
shot ID team ID team name shot Zone area
shot Zone range shot Zone basic the
match up the longitude and latitudes are
putting that into distance seconds
remaining minutes remaining because we
combine that into one column
shot distance because we have just
distance on there location X location y
the game event ID game ID all this stuff
is just being dropped on here and we'll
just go ahead and loop through our drops
and this is a nice way of doing this
because as you're playing with this
um this kind of data
putting your list into one setup helps
uh because then you're just running it
through an iteration
and you can come back and change it you
might be playing with different models
and do this with models you might be
looking at all kinds of different things
that you can drop and add in as you test
these out and again we're working in the
filter method so this is a lot of human
interaction with the data and it takes a
lot of critical thinking to look at this
stuff and say what matches and what
doesn't and so we look at the remaining
features let me go ahead and just run
this
the original to the new count we had 25
features now we have 11 features you can
see that right there let me just circle
that there's our 25 and there's old old
new now we're down to 11. so we've cut
it down to less than half
and you can just see the actual
different information on here and the
remaining time at this point we filtered
it through and then we'd move into the
next process which would be to run our
model on this
and maybe we would drop some of the
features and see if it runs better or
worse and what happens
uh that's kind of would be the next step
on there versus this is the filter setup
and that would be one of the other
setups depending on which algorithm you
use cross validation in machine learning
my name is Richard kirschner with the
simply learned team that's
www.simpylearn.com get certified get
ahead
what is in it for you we're going to
cover the need for cross validation what
is cross-validation
steps in Cross validation and types of
cross-validation and then of course
we'll do a nice Hands-On demo so you can
see what it looks like in Python code
let's start by exploring the need for
cross validation
while choosing machine learning models
we need to compare models to see how
different models perform on our data set
this goes beyond just comparing
different models
but also in explaining this to the
shareholders or whoever you're working
with you really need to know how valid
is your data set
or your models that you're working with
on the data set so the need for cross
validation comes up when the data is
usually limited and training and testing
on the same portion of data does not
give us an accurate view of how our
model performs and this just makes sense
if I'm going to program a model to out
guess something I don't want to use the
same data I trained it with I want to
see how it performs on the next data
otherwise I'm just kind of like giving
myself my own juice saying hey my model
did really good because this is what
it's based on
self-fulfilling prophecy and that is
still important to do because you still
need to run it on your own data to see
how well your set fits your data versus
how well it fits new data coming in so
training a model in the same data means
that the model will eventually learn
well for only that data and fail on new
data this is called overfitting
so you might write a wonderful script it
works great for predicting the failure
on loans for a bank
but it only works for the data you
already have not the new data coming in
and that again is overfitting so it's
not going to give you a proper you're
not going to get a good value as to how
good your model is going to fit the new
data coming in so what is cross
validation cross validation is a
technique which is used to train and
evaluate our model on a portion of our
database before reportioning our data
set and eventually using it on the new
portions
consider the block shown to be our
complete data set and you can see we've
already taken the data set and divided
into four sections and here we've cut
out one section highlighted in yellow
when we partition the data set into
training and testing data
and you'll see that in this particular
example we've taken 75 percent of the
data or three quarters of it
and we said okay we're going to use that
to train our model
and the testing data set will be used by
our model to predict on unseen data it
is used to evaluate our model's
performance
and this is what we're talking about is
you want to be able to train it on one
set of data come back and say hey this
is how well it performs on data it's
never seen before
now there's an interesting relationship
between these and it's important to also
see how well it performs on the train
data we're not going to talk about too
much on that one it's usually used in
higher end neural networks and it's one
of the ways they know when the training
data is done and the testing data and
the predictions kind of match that's
when they start looking at okay you've
trained it enough
and you can see here we take the first
partition and we go ahead and we map how
good it did and we partitioned the data
set into training and testing data we
then choose a different portion of the
data for training and testing and
re-evaluate the model performance to get
more accurate results and you can see
here we've colored it in a nice pretty
yellow for you and so we've tested on
the first section and now we're testing
it on the second section so we're still
using three quarters of the data to
train our model and one quarter to test
it and we just Loop through all the
different quarters to test it
that's what we're talking about we talk
about cross validation is we want to be
able to get a picture of how it works on
the data it hasn't seen
and we look at the steps in Cross
validation the steps involved in
cross-validation begin with step one
split the date into train and test sets
and evaluate models performance since we
measure one
we measure two
we measure three and we measure four
and each one of these generates its own
performance rate based on the other 75
percent being used to validate that data
how well does it perform on the Unseen
data
and so we have actually created four
separate models
and then we just take those measurements
uh one two three four we add them
together divide by four or however many
divisions you use and you get a total
model performance
for myself when I first start doing a
new data set I'll start with just
partitioning it into three because it
runs faster it's quick you only have to
run three models
that kind of setup a lot of this depends
on how much data you have if you have a
lot of data three works just fine if
you're very limited in how much data is
coming in you might want to divide it
into more validation sets when you're
first testing out your models and your
data sets
and there's other steps in here we'll
look at in just a moment types of cross
validation now this is important because
we looked at one type because actually
we're going to put down as a number two
type
one of the ways we can do this is what
they call leave one out cross validation
this is where the entire data set is
used for training and One Singular data
point is kept as the testing data
consider a data set with endpoints n
minus 1 will be training set at one
point will be testing set
leave one out cross validation is then
we go to another point and it'll be
chosen as a testing data and the rest of
the points will be the training
this will repeat for the rest of the
data set in times and yes we're creating
if your data is in the used in minus 1
in so we'll do it n minus 1 times and
you create n minus 1 models that you're
testing across your data set to see how
good your model does
the final performance measure will be
the average of the measures for all n
iterations and so you add up all those
different measures and divide by n minus
one now keep in mind at the end you just
run it across all the data and that's
your final model that you're going to
actually push out
this is to validate it this gives it an
N minus 1 validation you can imagine if
you're dealing with big data and you
have terabytes of data doing it this way
would probably be a very bad idea you
might have to wait a month before you
get an answer
we're dividing it into smaller portion
works so again we're dealing with
smaller amounts of data usually when you
do this level of cross validation
k-fold cross validation this is what our
example initially was and this is what
most people look at is you divide it
into in this case four sections so the
data is divided into K number of
different sections and like I said I
like to use three if I'm just initially
looking at the data it gives a decent
training model versus a faster run so at
least I know I'm using 66.66 percent of
the data
one section is for testing and the rest
for training the number of sections K is
the selected depending on size of data
set
again if you have a very small data set
you want your K to be much larger
because you just you really can't divide
out most your data you lose so much in
the training and so if you divide it out
enough you end up with leave one out
cross validation we divide it all the
way to each data point gets its own run
and this will generate if we do k k
equals 5 it will generate five different
models for you to evaluate on
and so of course we're generating each
one the final performance measure will
be the average of the output measures of
the K iterations so we add them all
together and we divide by K so we're
going to have five models there's going
to be five measurements and we'll divide
that by five and that gives us a good
guess you know good estimate of what the
measure is on this particular cross
validation going on and how good our
model is and then you train the model
with all the data in the end because you
don't want to leave one section out
now one of the things and I'm going to
point that out on this page before we
get into the next validation
what happens if your measurements are
all over the board you've divided into
five sections one measurement is really
bad one is really good that means
there's something going on with the way
you divided your data and how the data
is set up and we want to start looking
at stratified k-fold cross validation
the data is split so that each portion
has the same percentage of all the
different classes that exist in the data
set
consider a data set which has two
classes of data as shown
in normal cross validation the data is
divided without keeping in mind the
distribution of individual classes the
model thus cannot properly predict for
minority classes and you could think of
this in that if you were let's say you
had data set that was looking at drivers
how good do drivers do when driving
their car and one of the features is the
color of the car now my guess it
probably wouldn't have a huge
impact as far as the data but let's face
it there's the most common car out there
is probably white there's probably Brown
and you get into like fire engine red it
only has a few red cars
at that point if you train your data on
only cars that aren't red you're going
to run into a problem when the red car
comes up because it might have one model
that says red cars are bad drivers or
one model that says red cars are good
drivers and at that point it's going to
weigh the data and create a bias so you
need to divide up all those different
features
this comes out extremely heavily in
doing population statistics
where we might pull an area out run the
statistics but because we're basing our
statistics just on that area and the
people they live in there that area
might be predominantly one group of
people and then you have a minority that
is part of that group but there there's
a bias as far as how they fit into the
data and how it weighs their particular
choices and then you start running into
problems with the bias on there that's
why using stratified k-fold
cross-validation is a very important
thing to add into your toolkit when you
start looking at the different data
structures and validating your models
stratified k-folds overcomes this by
maintaining the same percentage of data
classes in all the folds models can be
trained even on minority classes and you
can see here where even though we have
five separate splits in the data we used
k equals five each one has a
representation of all the different
features in there so that we help get
rid of that bias and help each one train
equally across the data choose from over
300 in-demand skills and get access to 1
000 Plus hours of video content for free
visit skillup by simply learn click on
the link in the description to know more
so with that let's roll up our sleeves
we're going to do a demo where we use
adult census data to predict whether a
person is making above 500 000 a year
I'm going to go ahead and limb and guess
that that is not U.S money because
that's a half a million dollars in the
U.S but it's still a good money then we
use cross validation on our model to see
which one is performing better who is
earning less than 500k that is a
question we're going to ask here for our
demo we're going to be using Python 3
which you can use in whatever IDE you
want I myself am going to be using
Anaconda notebook
which opens up a nice web page
and from here we'll just go ahead and
create ourselves a new data set
for a new python scripting page
and we'll start by bringing in our
different modules in here and you'll see
in here let me go ahead and change this
to error it's fine
and that will go with black uh we'll see
up here at the top we have our pandas
and numpy this is your numpy array and
your pandas data frame which we always
like to use because they look so nice
and easy to use and View and of course
down here here's our
data frame here we're going to load up
our data and take a look at it in just a
minute
and then from our SK learn module
which you can see on the next set of
lines here we have our logistic
regression model K neighbors classifier
and then our cross valve score
stratified fold k-fold train test split
and k-fold and so this is really uh
what we're looking at right here we're
looking at how to validate our different
models and it looks like we're going to
be doing this in logistic request
regression and K neighbors classifier so
we can have something to compare on
there
and then of course once we have done
this we need to go ahead and have some
data so we'll go ahead and run this
here's our data set and this is actually
let's see we're using pandas so this is
a data frame
and then the first thing we're going to
look to see if we have any null values I
always check your data and see what you
have coming in you just never know
and so we can see here with uh with our
quick dataset.is null and Summit that we
have zero null values in our data set so
we've read our data set in and the first
step in any data is always to take a
look at it so we'll take our data set
and we'll go ahead and do dot head now
remember in Jupiter notebook if this was
not the only line or the last line of my
code on this particular grouping of
cells I'd have to add print around it
otherwise it just automatically prints
it for us and we can see here here's our
actual data and so we have of course a
nice you know the titles up here age
work class different information
education
I guess what level of Education marital
status relationship not in family race
sex Capital Dot Game Capital loss and if
you remember correctly I talked a little
bit about social minorities imagine if
if you were some place where the
predominant area is all in India
someplace where everybody's in is Indian
and there's a small group of people who
are what they call expatriates Who Came
From America and happen to be white well
you're going to run into a problem we
did that cross-validation because if
we're looking at this we might notice
that or in this case this just looking
at the first four things we see
three people who are white one person
who's black this might be a huge
discrepancy in our data and we want to
make sure we split that out or take a
look at that we probably won't get into
that too much today but just be aware
that this is the kind of thing that you
want to start flagging yourself on and
it might be
here we have world class private I'm not
sure what that means there's a lot of
question marks here that's kind of
important to note also and these are
just some of the things I start to look
at when I look at this data on here
native country United States so these
are people coming from the United States
and here's our fifty thousand now
remember we're looking for five hundred
thousand so the average person here is
probably around fifty thousand maybe
we'll take a look at that and see what
kind of uh spread we have on our data
but it's a good place to start
and we'll go ahead and do this as a
print head 10 the reason I wanted to do
that is so we could take just a little
closer look at some of the data we're
looking at
and if we go up here let's uh yeah put a
drawing on there let's take a note here
under sex
we're talking about that probably
convert that to 0 and 1. and we're going
to drop a lot of these columns on here
maybe we'll also just look at marital
status we might want to convert this to
0 and 1 on here
some other things that we're not looking
at is of course a race and this kind of
gets interesting because when you look
at race uh they only show right now I
only show the two variations we look at
how many variations there are
um same thing with male and female if we
label one of these uh zero and one one
is not greater than zero and zero is not
greater than one what that means is that
this is a yes no but as we get more
variety you can't do that you can't do
it yes no you have to start splitting
that up into multiple columns and again
it depends on your data in what you find
out you might come in here like we have
marital status we'll actually look a
little deeper marital status a couple
different options in here one is just to
convert this to married or not married
that's a zero one we might split this
into columns where Widow would be 0 1
divorce would be zero one separated zero
one and so forth again it depends on
what kind of data we're looking at and
how deep we want to get into it so let's
go ahead and clean up our data a little
bit here let's start by cleaning up a
few columns and again we're going to
drop a bunch of these because we don't
want to spend too much time cooking our
data and getting it all set up correctly
um we just want to have a little bit
more time to focus on Cross validation
and you'll see here on this we have our
heroes talking about the sex we're going
to convert that to Mel zero female one
and then we'll do the same thing with
marriage status uh at this point we're
going to create married status Widow is
going to if you're never married divorce
separated Widow it's going to be zero
for single and then we're going to put
this as one if you're married and uh
let's go ahead and convert this into a
type integer so this works really nicely
we'll have 0 and 1 for a single or
married and then there's going to be an
integer that way our our different
models can recognize it most models
don't recognize text unless you encode
it somehow just real quick before we
start looping through the different
models we got to remember to create our
Y which is going to be our income that's
our answer and of course from the data
set drop the Y because we don't want to
use that as part of the processing and
the other thing we want to look at this
is the code for creating our X train y
train X test y test so normally a lot of
times you'll see this where we just
split the data and we think okay good
I've got my test site I've got my y side
but we're going to go ahead and create a
loop so that we can go through this and
then take a look at that as far as
coming up with the results on there with
a cross validation score so let's go
ahead and run this should
and whoops I must have mistyped
something all I need to do is Rerun the
threads because I'd run it twice and so
it didn't like changing things up when
I'd already run it once and I've already
made too many changes on here so we go
ahead and run this and now we have an X
train and a y train we're not actually
going to do anything with this x train
and Y train here this is just a reminder
for what that looks like but we do need
our data set and our Y part of the data
set on here
so the next step
this is the kind of thing I love working
with is we create our models and you can
see where you can start creating really
robust packages to compare different
models
again remember we're dealing with small
data if you did this across Big Data you
can run into some major problems and so
we're going to import the two different
data models logistic regression and K
Neighbors classifier
and we go ahead and pin these both to
models on here
and if we do something like print
models
we'll go ahead and run that you can see
that we have a title which is a string
and then a function
and we can run each of these functions
going through our different setup
and then what we really want to do is we
want to
create
a loop
for the different models and go through
the cross validation score on the
different models
and we'll go ahead and do I'm going to
create a results dictionary one of my
favorite things to do is put everything
in dictionaries
and we're going to go for name and model
in models
so for each row we have the name LR and
then the model so logistic regression
model
and then we'll go ahead and do our
k-fold right here this k-fold is the
heart of the setup on this I think the
random state is being
changed so because it threw a warning on
there and how that's being done uh we'll
take a look at that in just a second the
warning
and then we have our splits we're doing
10 splits on here with our k-fold
and then we have our results
and once we have our results which is
going to be uh 10 we're going to have a
row of 10 pieces of data from each
different fold
and so we want to go ahead and do the
mean on that oops there we go go the
arrow the wrong way
and the standard deviation so how much
do they deviate from each other
how low is our error on here the lower
the better
so let's go ahead and run that
and then of course I have on mine
there we go we see down here is our
future warning says something about
random States I'm not going to worry too
much about that
warnings come up so often when you're
running these different configurations
because they're always making changes on
here
uh and so I've got in here my results
here's my results
um with the name and it's going to be
the means and the standard deviation on
here
oops there we go that
there's our mean standard deviation and
then I always like to do key value just
because this reminds me very much of
doing mapping in larger data usually
have a key value for your setup and
we'll print the key value which in this
case is the name of the model
and the results on here
and we can see down here here's the two
models we ran clearly I could add like a
ton of these different models long as
they are part of the sklearn setup and
they use the same input there are some
models that work slightly differently so
you don't want to use uh you know if
you're doing classification and then you
switch to regression you run into
problems and of course we're doing
regression here and we can see our
results the knnn
did slightly better it had lower errors
and look at the standard deviation the
standard deviation is also lower so if I
was running this model I would go for
I'd be looking at the k n as my primary
model I would then go rerun this model
on all the data
to send out for future predictions the
stuff we don't know and you can see
right here I had it you know it does a
nice job at 0.77 versus 0.79 and we
picked this you could run this across
like I said a lot of times there's like
about six seven different models in
there some of the models have different
settings so you might go in there and
mess with the settings there's all kinds
of ways to go through this and then you
can take your in this case cross
validation with in this case uh what do
we had 10 folds and you can get a good
idea of what kind of setup or which
model is going to work the best and then
compare it and then you go back to your
feedback loop of how to build it and
send it out and of course from this
definition this model you have somebody
new come in and you find out what their
marriage status is their education
this different information that we put
in here you might find out how what
their predicted income is yeah it worked
pretty good I definitely wouldn't
make it the primary goal with the 0.7 9
over here error on there or with this
kind of setup on the means but it would
give you a good idea what you're looking
at for people coming in if that's what
you're trying to evaluate Idols is a
machine learning framework used for
image classification and neural network
processing fibroids is a python based
open source machine learning
implementation based on the torch
Library it is one of the most popular AI
Library among industry and academic
researchers worldwide hey everyone you
are already watching simply learn and in
today's video we're going to be learning
all about what is pytots if you love
watching videos like this do hit the
Subscribe button to never miss any
updates from Simply love if you want to
become a deep learning expert consider
deep learning with kairas and tensorflow
by simplica and master those necessary
skills check out the link in the
description for more details before we
get started further I have one simple
question for you guys
which is an open source machine learning
library for python that is entirely
based on Torch and the options are
tensor pie torch numpy or python if you
know the answer please leave them in the
comment box below and now let's get
started with the agenda for today's
session
first let's start with what is pi torch
falling to that how does pie Dodge work
then Pi dots features moving ahead let
us know by touch benefits and finally
the use cases of Pi torch
so now let's start with what is pi torch
pytots is a free and open source Python
and torch machine learning
implementation deep learning models
based on neural networks are currently
being developed and trained in a variety
of deep learning projects using pytots
because of its ease of use pytorch is
increasingly being used to create
artificial neural network models pytos
is used by data scientists for
artificial intelligence research and
applications
next we have how does pytotch work
to write readable code Python's adhes to
Python's distinct and unique coding
features it allows developers to run and
test the relevant portions of code
immediately rather than waiting for the
entire program to finish furthermore it
eliminates the need for neural network
debuggers machine learning programmers
and deep learning researchers to run the
code in real time without having to wait
to see if it works or not
now we know what pytog is and it's
working so moving ahead let's understand
the features of Pi torch
few mentioned python features are simple
interference tools and libraries tensor
computation Dynamic graph computation
variables parameters modules and
functions so let's understand each in
detail
so like python phytos has a
straightforward user interference it
offers a simple way to use the
application programming interference
eapi the Python's framework is very
simple to use and run
next we have tools and libraries
it is possible to expand Pi torch and
support development in fields like
computer vision and reinforcement
learning by using a wide range of tools
and libraries
next
tensor computation
tensor computation with row burst GPU
graphical Processing Unit acceleration
support equivalent to numpy next
Dynamic graph computation
dynamic graphs are adaptable and let us
examine and change the internals of the
graph at any time the dynamic graph in
Phi torch is used it follows from our
Declaration of variables that the
computational graph is constructed
dynamically so following each training
iterations the graphics rebuilt
next we have variables
for the purpose of holding the gradient
the variables is enclosed outside the
tensor a computational graphs node is
represented by it
next we have parameters
parameters encircle variables they are
used when using a parameter as a tensor
which isn't possible when using a
variable is required
and next comes module
stateful computation module serves as
their building blocks in order to
simplify and facelates this creation of
complex multi-layer neural network
phytots offers a large library of models
next
we have functions
the connections between the two
variables are as follows functions don't
have their memory and don't have any
memory to store the state or a buffer
moving ahead we have phytorch benefits
because its syntax and usage is similar
to python it makes development simple
for python developers to learn and
understand
it has an easy to use interference and
straightforward python coding
it includes a large number of libraries
and well-known tools that make debugging
easier then scalability strong support
on well-known Cloud platform and a small
open source focused communities are all
available next let's have a look at the
use cases of pytorch the Python's
framework is well known for being
convenient and adaptable with example of
its more typical use cases including
reinforcement learning image
classification and neural language
processing so let's learn each in detail
the development of Robotics for
Automation and using pytots in business
strategy plan to benefit from
reinforcement learnings quick and
flexible experimentation capabilities
with powerful GPU acceleration pytouch
makes sensors and dynamic neural
networks available RL machines are
designed to learn from experience to
make wise decision and obtain the best
rewards next we have image
classification
pytouch employs an image classification
algorithm that requires developers to
categorize an image based on its visual
content
a computer vision application for
example can use an algorithm to
determine whether a given image contain
an object such as a car or bicycle
object detection is simple for the human
eye but difficult for computer vision
application
next is natural language processing
NLP is a technology the main component
of NLP are speech recognition optical
character recognition Mission
translation and information retrieval
computers can comprehend spoken and
written languages with the help of NLP
technology in this video we will learn
about overfitting and underfitting in
machine learning we have our experience
instructor Richard who will take us
through this video and help us
understand the basics of overfitting and
underfitting the reasons why it occurs
and finally we look at a demo in Python
over to Richard Now what is overfitting
what is underfitting and those are like
the biggest things right now in data
sciences overfitting and underfitting
what does that mean
so let's go ahead and talk about
overfitting when we talk about
overfitting it's a scenario where the
machine learning model tries to learn
from the details along with the noise
and the data tries to fit each data
point on the curve you can see that
if you plug in your coordinates you're
just going to get the whatever is fitted
every point on the data stream there's
no average there's no two points that
might have that you know y might have
two different answers because uh if the
wind blows a certain way
um and your efficiency of your car maybe
you have a headwind so your car might
alter how efficient it is as it goes and
so there's going to be this variance on
here and this says no you can't have any
variants what's you know the this is
it's going to be exactly this it can't
be any you can't be the same speed or
the same car and have a slightly
different efficiency
so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the prediction
uh so you'll get like a really high
error on here
and so uh reasons for overfitting data
used for training is not cleaned and
contains noise garbage value is in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of something wrong with the
data coming in it needs to be addressed
whether it's the source of the data
maybe they use in medical different
measuring tools
so you now have to adjust for data that
came in from hospital a versus Hospital
b or even off of machine a and machine B
that's testing something and those those
numbers are coming in wrong
the model has a high variance again wind
is a good example I was talking about
that with the car you may have 100 tests
but because the wind's blowing it's all
over the place
size of training data used is not enough
so a small amount of data is going to
also cause this problem you only have a
few points and you try to plot
everything
the model is too complex this comes up a
lot
we put too many pieces together and how
they interact can't even be tracked and
so you have to go back break it up and
find out actually what correlates and
what doesn't
so what is underfitting a scenario where
machine learning models can neither
learn
the relationship between the data points
nor predict or classify a new data point
and you can see here we have our
efficiency of our car and our line drawn
and it's just going to be way off for
both the training and the predicting
data
as the model doesn't fully learn the
patterns it accepts every new data point
during the prediction
so instead of looking for a general
pattern we just kind of accept
everything
data used for training is not cleaned
and contains noise garbage and values
again under fitting and overfitting same
issue you got to clean your data
the model has a high bias
we've seen this in all kinds of things
from
[Music]
the mod the most common is the driving
cars to facial identification or
whatever it is the model itself when
they build it you might have a bias
towards one thing and this would be an
underfitted model but have that bias
because it's averaged it out so if you
have five people from India and 10
people from
Africa and 20 people from the U.S you've
created a bias because it's looking at
the 20 people and you only have a small
amount of data to work with
size of training data used is not enough
that goes with the size I was just
talking about
so we have a model with a high bias we
have size or training data used it's not
enough the model is too simple
again this is one straight line through
all the data when it needs as a slight
shift to it for other reasons
so what is a good fit
a linear curve that best fits the data
is neither overfitting or underfitting
models but is just right
and of course we have the nice examples
here where we have overfitting lines
going up and down every point is trying
to be include included under fitting
the line really is off from where the
data is and then a good fit is got to
get rid of that minimize that error
coming through
so this is all exciting but what does
this look like so we really need to jump
in and put a code together and see what
this looks like when we're programming
for this demo we'll bring up our trusty
anaconda and go into Jupiter notebook
for python
and move myself out of the way here
and so we're going to start off this is
going to be a demo on overfitting and
underfitting using python
and let's start with our Imports
now if you've been through enough of
these tutorials we don't want to spend a
huge amount of time on what we're
bringing in and what we're doing so you
should be up on doing this with python
and how to bring in your different
modules
uh we're going to bring in the SK learn
or the scikit processing sklin
learn.neural Network Import in a MLP
regressor
so there's our regressor model right
there
that's going to be our linear regression
model
and we have our metrics mean absolute
error if you remember we had our that's
how we figure out how well it fits is
how far off that error is based on the
um mean square error value MSE
and then of course numpy because we just
like to work with numpy it's a great
data array we always import it as MP
that's the most common way of doing it
and then we have SK learn model
selection import validation curve so
we're going to look at a validation
curve to see how good our models are
and then we have the data set we'll use
the very famous Iris data set and that's
embedded in the sci kit so the S key
learn data sets have a load Iris in
there
and then we have the matplot library
because if you're doing any kind of demo
or showing this off to your shareholders
we want to have something nice to
display it on
and then we have SK learn model
selection we're going to import import K
fold and we'll talk about that when we
get to it and then we're going to go
ahead and do just for our numpy we're
going to do like a random seed for
random numbers and then for our plot
style we'll use the GG plot that's just
some back end setup you could even
probably leave the plot style out
depending on what version of
you're using of
depending on what version you're using a
matplot library
and then we'll go ahead and run this
it's not going to do anything that we
can visibly see because it's just
loading those modules and then we also
want to load our Iris data in here and
the IRS data has an iris data and Iris
Target
we're going to load that as X and Y
and just so you can have an idea what
we're talking about we're going to go
ahead and print
X
and just the first bit of X we'll just
do the top of X and we'll also print y
so you can see what the top of Y looks
like print to y
and since we're in numpy we're going to
go ahead and do our own thing if this
was uh of course pandas we could just do
the head of it and see what it looks
like and you can see here we've loaded
this up and in X we have these different
measurements that they take of the
flower the iris flowers from this
particular data set and what kind of
flower it is it's going to be a zero one
or a two is actually what the target
comes out of even though that doesn't
show in here
and so we're going to come in here and
we're going to use the k-folds cross
validation with 20 folds
and a good catch that this was a model
selection we're we're going through and
we're selecting different parts of the
data in here here we use k-fold
cross-validation with 20 folds k equals
20 to evaluate the generalization
efficiency of the model within each fold
we will then estimate the training and
test error using the training and test
sets respectfully
so here we have our KF equals KF k-fold
here's our splits on the top
and then we need to go ahead and have
our list training error we're going to
create an array for that we're going to
list our testing error
and for train index and test index in k
f dot splitx X train y uh X train and X
test
we're going to go ahead and split up our
our data our X values and same thing
with the Y values so now we have an X
train and X test a y train and a y test
and then here's our model our
MLP regressor
that's your linear regressor model in
there and we have used a multi-layer
precipitron MLP so this is a neural
network a multi-layer precipitron that's
what the MLP is for regressor means that
it is dealing with numbers we're not
categorizing things
um
and then let's go ahead uh kind of went
off the screen here we'll just go ahead
and bring that down it's a class of feed
forward artificial neural networks and
they kind of loosely call it a n n don't
get caught up in the a n n n
there's NN is neural network and then
everybody puts their own flavor on it
depending on what they're doing uh so if
you see the NN you know you're dealing
with a neural network
so we go ahead and fit our data here's
our model.fit we have X train and Y
train
and the Y train data we're going to
predict equals model dot predict
X train so here's our prediction of what
it's going to be so we've trained it and
we've predicted it we've traded our the
train data and then we have our y train
and then we have our y test
and the Y test equals a model predict X
test
now notice what we did here is we're
going to use our model to predict what
we think y should be but this is the
training set so we've trained it with
this data and now we want to see how
good our model fits our training data
and then we want to see how well it fits
our testing data
so we take our fold training error mean
absolute error why train
why train data predict and we're going
to do our full testing error the mean
absolute error of Y test and why test
data predict
and we do this and here's our mean
absolute error there's our a little bit
different connotation but that's that's
taking the Square value and finding the
in this case it's using the absolute
value so instead of the Square value we
get rid of the minus and pluses by using
an absolute value and we find the
average of that and that works the same
way as doing a squared value
then we take our list training error and
we're going to just append it for each
each one of these runs we go through so
every time we fold the data
think of it like this uh we want to go
ahead and take a piece of data that's
going to be one piece of the data and
we're going to look at
each section and we want to go through
each section to see how well it does and
splits it up this way we have a nice
picture when we're looking at it from a
distance I do this a lot when I do X and
when I split my X train and my y train
I'll take two-thirds of the data and
then one third of the data and then I'll
switch it and I'll do three different
models so I can really see how well it
tests out and how that averages out
this is the same thing but with the with
the k-fold we're doing and we're doing
it across 20 sections
we'll go ahead and run this
and we run this it's not too exciting
because we're just loading up the data
and appending it into our list
and so we want to take with this is
we're going to go ahead and plot it and
this is where we can really see what's
going on this is where it gets exciting
so we take it and we're going to create
a couple subplots that way we have a
nice setup down here we're splitting it
up into a couple different graphs
and let's go ahead and run this and then
we'll walk through it a little bit so
our subplot comes in
um
there's our subplot and then our PLT
plot we're going to do in there range
one we're going to ahead and do the
splits plus one in pra list training
error uh Ravel um this is of course just
a code to how we properly set it up on
there so that it sees it correctly
and then we have our X label which is
our number fold plot the Y label
training error plot the title training
here across Folds
plot the tight layout plot the subplot
so we're going to move on to this is one
two one one two two there's our one just
one and two
it has to do with how it how it layers
it on there for doing multiple plots
because you can do all kinds of cool
things with our plot our PI plot Library
uh and again we're going to go ahead and
do the same thing for the error and we
end up with our training error cross
folds and our testing error across Folds
and so you can see these different folds
how they kind of Spike and how they look
and so we're talking about overfitting
or under fitting we're comparing these
two graphs and if one of them
is more off than the other one
if you're looking at these two graphs
you got to say hey is this one over fit
or under fit and this is always a good
question to ask I mean what do we got
going here is that over fit or is that
underfit
and I would say based on these two
graphs in the training data the training
data is more sporadic than the testing
data
so I would look at this and say hey this
might need to be fit a little bit better
maybe we don't have enough data with the
iris we probably don't
something else is going on here so it's
a little underfit maybe a different
model would fit better I would not use a
neural network model for this I would
actually use just a basic linear linear
model on this
a lot of different choices but this
gives you an idea what we're looking at
is how chaotic are these two is it
getting better or is it getting worse
if at some point the training data gets
so much better than the testing data you
know you've overfit it and that's where
you start running into the overfitting
this to me looks like it's under fit so
that concludes under fitting and
overfitting there's no better time to
train yourself in the exciting field of
machine learning if you're looking for a
course that costs everything from the
fundamentals to Advanced Techniques for
that accelerator Credit in a and ml with
a comprehensive postgraduate programming
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and ask me anything sessions by IBM with
three Capstone projects and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience master classes by
Caltech faculty and IBM experts and show
top-notch education simply lens job
assist helps you notice by Leading
companies this program calls statistics
python supervised NM supervised learning
NLP neural networks computer vision gns
kras tensorflow and many more skills
enroll now and then look exciting Ai and
ml opportunities the course link is
mentioned in the description box below
in this lesson you are going to
understand the concept of text mining
by the end of this lesson you will be
able to explain text mining execute text
processing tasks
so let's go ahead and understand text
mining in detail
let's first understand what text mining
is text mining is the technique of
exploring large amounts of unstructured
Text data and analyzing it in order to
extract patterns from the text to data
it is aided by software that can
identify Concepts patterns topics
keywords and other attributes in the
data it utilizes computational
techniques to extract and summarize the
high quality information from
unstructured textual resources let's
understand the flow of text mining there
are five techniques used in text mining
system information extraction or text
pre-processing this is used to examine
the unstructured text by searching out
the important words and finding the
relationships between them
categorization or text transformation
attribute generation categorization
technique it labels the text document
under one or more categories
classification of Text data is done
based on input output examples with
categorization
clustering or attribute selection
clustering method is used to group text
documents that have similar content
clusters are the partitions and each
cluster will have a number of documents
with similar content clustering makes
sure that no document will be omitted
from the search and it derives all the
documents that have similar content
visualization technique the process of
finding relevant information is
simplified by visualization technique
this technique uses text Flags to
represent a group of documents or a
single document and compactness is
indicated using colors visualization
technique helps to display textual
information in a more attractive way
summarization or interpretation or
evaluation
summarization technique will help to
reduce the length of the document and
summarize the details of the documents
it makes the document easy to read for
users and understand the content at the
moment
let's understand the significance of
text mining
document clustering document clustering
is an important part of text mining it
has many applications in Knowledge
Management and information retrieval
clustering makes it easy to group
similar documents into meaningful groups
such as in newspapers where sections are
often grouped as business Sports
politics and so on pattern
identification text mining is the
process of automatically searching large
amount of text for text patterns and
recognition of features features such as
telephone numbers and email addresses
can be extracted using pattern matches
product insights text mining helps to
extract large amounts of text for
example customer reviews about the
products mining consumer reviews can
reveal insights like most loved feature
most hated feature improvements required
and reviews of competitors products
security monitoring text mining helps in
monitoring and extracting information
from news articles and reports for
national security purposes
text mining make sure to use all of your
available information it is a more
effective and productive knowledge
discovery that allows you to make better
informed decisions automate information
intensive processes gather business
critical insights and mitigate
operational risk
let's look at the applications of text
mining
speech recognition speech recognition is
the recognition and translation of
spoken language into text and vice versa
speech often provides valuable
information about the topics subjects
and concepts of multimedia content
information extraction from speech is
less complicated yet more accurate and
precise than multimedia content this
fact motivates content-based speech
analysis for multimedia Data Mining and
retrieval where audio and speech
processing is a key enabling technology
spam filtering spam detection is an
important method in which textual
information contained in an email is
extracted and used for discrimination
text mining is useful in automatic
detection of spam emails based on the
filtering content using text Mining and
email service provider such as Gmail or
Yahoo mail checks the content of an
email and if some malicious text is
found in the mail then that email is
marked as spam and sent to the spam
folder
sentiment analysis it is done in order
to determine if a given sentence
expresses positive neutral or negative
sentiments sentiment analysis is one of
the most popular applications of text
analytics the primary aspect of
sentiment analysis includes data
analysis of the body of the text for
understanding the opinion expressed by
it and other key factors comprising
modality and mood usually the process of
sentiment analysis works best on text
that has a subjective context than on
that with only an objective context
e-commerce personalization
text mining is used to suggest products
that fit into a user's profile text
mining is increasingly being used by
e-commerce retailers to learn more about
the consumers as it is the process of
analyzing textual information in order
to identify patterns and gain insights
e-commerce retailers can Target specific
individuals or segments with
personalized offers and discounts to
boost sales and increase Customer
Loyalty by identifying customer purchase
patterns and opinions on particular
products
let's look at natural language toolkit
library in detail
natural language toolkit is a set of
Open Source python models that are used
to apply statistical natural language
processing on human language data
let's see how you can do environment
setup of nltk
go to Windows start and launch python
interpreter from Anaconda prompt and
enter the following commands enter
command python to check the version of
python installed on your system
enter import nltk to link you to the
nltk library available to download then
enter
nltk.download function that will open
the nltk download window check the
download directory select all packages
and click on download this will download
nltk onto your python once you have
downloaded the nltk you must check the
working and functionality of it in order
to test the setup enter the following
command in Python idle from
nltk.corpus import brown brown dot word
parenthesis parenthesis
the brown is an nltk Corpus that shows
the systematic difference between
different genres available words
function will give you the list
available words in the genre the given
output shows that we have successfully
tested the nltk installed on python
let's Now understand how you can read a
specific module from nltk corpora if you
want to import an entire module from
nltk corpora use asterisk symbol with
that module name import command enter
the command from
nltk.book import asterisk it will load
all the items available in nltk's book
module now in order to explore Brown
Corpus enter the command
nltk.corpus import Brown this will
import Brown Corpus on the python enter
brown dot categories function to load
the different genres available
select a genre and assign that genre to
a variable using the following syntax
variable name is equal to brown.words
categories is equal to genre name now in
order to see the available words inside
the selected genre just enter the
defined variable name as a command
let's understand text extraction and
pre-processing in detail
so let's first understand the concept of
tokenization
tokenization is the process of removing
sensitive data and placing unique
symbols of identification in that place
in order to retain all the essential
information concerned with the data by
its security it is a process of breaking
running streams of text into words and
sentences it works by segregating words
using punctuation and spaces
text extraction and pre-processing
engrams
now let's look at what n-gram is and how
it is helpful in text mining engram is
the simplest model that assigns these
probabilities to sequences of words or
sentences engrams are combinations of
adjacent words or letters of length and
in the source text so engram is very
helpful in text mining when it is
required to extract patterns from the
text as in the given example this is a
sentence all of these words are
considered individual words and thus
represent unigrams a 2 gram or bigram is
a two-word sequence of words like this
is is a or a sentence and a three gram
or trigram is a three-word sequence of
words like this is a or is a sentence
let's Now understand what stop words are
and how you can remove them
stop words are natural language words
that have negligible meanings such as a
and and or the and other similar words
these words also will take up space in
the database or increase the processing
time so it is better to remove such
words by storing a list of stop words
you can find the list of stop words in
the nltk data directory that is stored
in 16 different languages use the
following command to list the stop words
of English language defined in nltk
Corpus importing nltk will import the
nltk Corpus for that instance enter from
nltk.corpus import Stop words will
import Stop words from nltk Corpus Now
set the language as English so use set
function as set under braces stop words
dot words set genre as English
stop words are filtered out before
processing of natural language data as
they don't reveal much information
so as you can see in the given example
before filtering the sentence the
tokenization of stop word is processed
in order to remove these stop words and
the filtering is applied in order to
filter the sentence based on some
criteria
text extraction and pre-processing
stemming
stemming is used to reduce a word to
stem or base word by removing suffixes
such as helps helping helped and helper
to the root word help
the stemming process or algorithm is
generally called a stemmer there are
various stemming algorithms such as
Porter stemmer Lancaster stemmer
snowball stemmer Etc use any of the
stemmers defined under nltk stem Corpus
in order to perform stemming as shown in
the example here we have used Porter
stemmer When You observe the output you
will see that all of the words given
have been reduced to their root word or
stem
text extraction and pre-processing
limitization
limitization is the method of grouping
the various inflected types of a word in
order that they can be analyzed as one
item it uses vocabulary list or a
morphological analysis to get the root
word it uses wordnet database that has
English words linked together by their
semantic relationship as you can observe
the given example the different words
have been extracted to their relevant
morphological word using limitization
text extraction and pre-processing POS
tagging
let's now look at different part of
speech tags available in the national
language toolkit Library
a POS tag is a special label assigned to
each token or word in a Text corpus to
indicate the part of speech and often
also other grammatical categories such
as tense number either plural or
singular case Etc POS tags are used in
text analysis tools and algorithms and
also in Corpus searches so look at the
given example here Alice wrote a program
is the source text given the POS tags
given are Alice is a noun wrote is a
verb a is an article and program is an
adjective look at the given example to
understand how POS tags are defined so
the given sentence or paragraph contains
different words that represent different
parts of speech we will first use
tokenization and removal of stop words
and then allocate the different POS tags
these are shown with different words in
the given sentence
POS tags are useful for limitization in
building named entity recognition and
extracting relationships between words
text extraction and pre-processing named
entity recognition now let's understand
what named entity recognition is all
about Nar seeks to extract a real world
entity from the text and sorts it into
predefined categories such as names of
people organizations locations Etc many
real world questions can be answered
with the help of name entity recognition
where specified products mentioned in
complaints or reviews
does the Tweet contain the name of a
person does the Tweet contain the
person's address
as you can see in the given example
Google America Larry Page Etc are the
names of a person place or an
organization so these are considered
named entities and have different tags
such as person organization
gpe or geopolitical entity Etc
NLP process workflow
now you have an understanding of all
nltk tools so now let's understand the
natural language processing workflow
Step 1 tokenization it splits text into
pieces tokens or words and removes
punctuation Step 2 stop word removal it
removes commonly used words such as the
is are Etc which are not relevant to the
analysis step 3 stemming and
limitization it reduces words to base
form in order to be analyzed as a single
item step 4 POS tagging it tags words to
be part of speech such as noun verb
adjective Etc based on the definition
and context step 5 information retrieval
it extracts relevant information from
the source
mo1 Brown Corpus problem statement the
Brown University standard Corpus of
present-day American English also known
popularly as brown Corpus was compiled
in the 1960s as a general Corpus in the
field of Corpus Linguistics it contains
500 samples of English language text
totaling roughly 1 million words
compiled from Works published in the
United States in 1961. we will be
working on one of the subset data set
and perform text processing tasks
let us import the nltk library and read
the ca underscore 10 Corpus import nltk
we will have to make sure that there are
no slashes in between hence we will use
the replace function within pandas for
the same
let's have a look at the data once
tokenization after performing sentence
tokenization on the data we obtain
similarly after applying sentence
tokenizer the resulting output shows all
individual words tokens
stop word removal let's import the stop
word library from
nltk.corpus import Stop words
we also need to ensure that the text is
in the same case nltk has its own list
of stop words we can check the list of
stop words using
stopwords.words and English inside the
parenthesis
map the lowercase string with our list
of word tokens
let's remove the stop words using the
English stop words list in nltk we will
be using set checking as it is faster in
Python than a list
by removing all stop words from the text
we obtain
often we want to remove the punctuations
from the documents too since python
comes with batteries included we have a
string dot punctuation
from string import punctuation
combining a punctuation with the stop
words from nltk
removing stop words with punctuation
stemming and limitization we will be
using stemming and limitization to
reduce words to their root form for
example walks walking walked will be
reduced to their root word walk
importing Porter stemmer as the stemming
library from nltk.stem import Porter
stemmer
printing the stem words
import the wordnet limitizer from
nltk.stem
printing the root words
we also need to evaluate the POS tags
for each token
create a new word list and store the
list of word tokens against each of the
sentence tokens in data 2.
for I in tokenized
also we will check if there were any
stop words in the recently created word
list
we will now tag the word tokens
accordingly using the POS tags and print
the tagged output
for our final text processing task we
will be applying named entity
recognition to classify named entities
in text into predefined categories such
as the names of persons organizations
locations expressions of times
quantities monetary values percentages
Etc
now press the tagged sentences under the
chunk parser if we set the parameter
binary equals true then named entities
are just tagged as NE otherwise the
classifier adds category labels such as
person organization and gpe
create a function named as extract
entity names along with an empty list
named as entity names
we will now extract named entities from
a nltk chunked expression and store them
in the empty created above
again we will set the entity names list
as an empty list and we'll extract The
Entity names by iterating over each tree
in chunked sentences
great we have seen how to explore and
examine the Corpus using text processing
techniques let's quickly recap the steps
we've covered so far One Import the nltk
library to perform tokenization three
perform stemming and limitization four
remove stop words five perform named
entity recognition
structuring sentences syntax
let's first understand what syntax is
syntax is the grammatical structure of
sentences in the given example this can
be interpreted as syntax and it is
similar to the ones you use while
writing codes knowing a language
includes the power to construct phrases
and sentences out of morphemes and words
the part of the grammar that represents
a speaker's knowledge of these
structures and their formation is called
syntax
phrase structure rules are rules that
determine what goes into a phrase that
is constituents of a phrase and how the
constituents are ordered constituent is
a word or group of words that operate as
a unit and can be used to frame larger
grammatical units the given diagram
represents that a noun phrase is
determined when a noun is combined with
a determiner and the determiner can be
optional a sentence is determined when a
noun phrase is combined with a verb
phrase a verb phrase is determined when
a verb is combined optionally with the
noun phrase and prepositional phrase and
a prepositional phrase is determined
when a preposition is combined with a
noun phrase
a tree is a representation of syntactics
structure of formulation of sentences or
strings consider the given sentence the
factory employs 12.8 percent of Bradford
County
what can be the Syntax for pairing the
statement let's understand this a tray
is produced that might help you
understand that the subject of the
sentence is the factory the predicate is
employees and the target is 12.8 percent
which in turn is modified by Bradford
County
syntax parses are often a first step
toward deep information extraction or
semantic understanding of text
rendering syntax trees
download the
corresponding.exe file to install the
ghost script rendering engine based on
your system configuration in order to
render syntax trees in your notebook
let's understand how you can set up the
environment variable
once you have downloaded and installed
the file go to the folder where it is
installed and copy the path of the file
now go to system properties and under
Advanced properties you will find the
environment variable button click on
that to open the pop-up box tab of the
environment now open the bin folder and
add the path to the bin folder in your
environment variables
now you will have to modify the path of
the environment variable use the given
code to test the working of syntax tree
after the setup is successfully
installed
structuring sentences chunking and chunk
parsing
the process of extraction of phrases
from unstructured text is called
chunking instead of using just simple
tokens which may not represent the
actual meaning of the text it is
advisable to use phrases such as Indian
team as a single word instead of Indian
and team as separate words the chunking
segmentation refers to identifying
tokens and the labeling refers to
identifying the correct tag these chunks
correspond to mixed patterns in some way
to extract patterns from chunks we need
chunk parsing the chunk parsing segment
refers to identifying strings of tokens
and labeling refers to identifying the
correct chunk type
let's look at the given example you can
see here that yellow is an adjective dog
is a noun and the is the determiner
which are chunked together into a noun
phrase
similarly chunk parsing is used to
extract patterns and to process such
patterns from multiple chunks while
using different parsers
let's take an example and try to
understand how chunking is performed in
Python let's consider the sentence the
little mouse ate the fresh cheese
assigned to a variable named scent using
the word tokenize function under nltk
corpora you can find out the different
tags associated with a sentence provided
so as you can see in the output
different tags have been allocated
against each of the words from the given
sentence using chunking
NP chunk and parser
you will now create grammar from a noun
phrase and will mention the tags you
want in your chunk phrase within the
function here you have created a regular
expression matching the string the given
regular expression indicates optional
determiner followed by optional number
of adjectives followed by a noun you
will now have to parse the chunk
therefore you will create a chunk parser
and pass your noun phrase string to it
the parser is now ready you will use the
parse parenthesis parenthesis within
your chunk parser to parse your sentence
the sentence provided is the little
mouse ate the fresh cheese this sentence
has been parsed and the tokens that
match the regular expressions are
chunked together into noun phrases NP
create a verb phrase chunk using regular
Expressions the regular expression has
been defined as optional personal
pronoun followed by zero or more verbs
with any of its type followed by any
type of adverb you'll now create another
chunk parser and pass the verb phrase
string to it create another sentence and
tokenize it add POS tags to it so the
new sentence is she is walking quickly
to the mall and the POS tag has been
allocated from nltk corpora now use the
new verb phrase parser to parse the
tokens and run the results you can look
at the given tree diagram which shows a
verb parser where a pronoun followed by
two verbs and an adverb are chunked
together into a verb parse
structuring sentences chinking
chinking is the process of removing a
sequence of tokens from a chunk how does
chunking work the whole chunk is removed
when the sequence of tokens spans an
entire chunk if the sequence is at the
start or the end of the chunk the tokens
are removed from the start and end and a
smaller chunk is retained if the
sequence of tokens appears in the middle
of the chunk these tokens are removed
leaving two chunks where there was only
one before
consider you create a chinking grammar
string containing three things chunk
name the regular expression sequence of
a chunk the regular expression sequence
of your here in the given code we
have the chunk regular expression as
optional personal pronoun followed by
zero or more occurrences of any type of
the verb type followed by zero or more
occurrences of any of the adverb types
the regular expression says that
it needs to check for the adverb in the
extracted chunk and remove it from the
chunk inside the chinking block with
open curly braces and closing curly
braces you have created one or more
adverbs
you will now create a parser from
nltk.reg exp parser and pass the
grammar to it now use the new
parser to parse the tokens sent three
and run the results
as you can see the parse tree is
generated while comparing the syntax
tree of the parser with that of
the original chunk you can see that the
token is quickly adverb chinked out of
the chunk
let's understand how to use context-free
grammar
a context-free grammar is a four Tuple
sum ntrs where sum is an alphabet and
each character in sum is called a
terminal and T is a set and each element
in NT is called a non-terminal r the set
of rules is a subset of NT times the set
of sum U and t s the start symbol is one
of the symbols in NT
a context-free grammar generates a
language L capturing constituency and
ordering in CFG the start symbol is used
to derive the string you can derive the
string by repeatedly replacing a
non-terminal on the right hand side of
the production until all non-terminals
have been replaced by terminal symbols
let's understand the representation of
context-free grammar through an example
in context-free grammar a sentence can
be represented as a noun phrase followed
by a verb phrase noun phrase can be a
determiner nominal a nominal can be a
noun VP represents the verb phrase a can
be called a determiner flight can be
called a noun
consider the string below where you have
certain rules when you look at the given
context-free grammar a sentence should
have a noun phrase followed by a verb
phrase a verb phrase is a verb followed
by a noun a verb can either be Saul or
met noun phrases can either be John or
Jim and a noun can either be a dog or a
cat check the possible list of sentences
that can be generated using the rules
use the join function to create the
possible list of sentences you can check
the different rules of grammar for
sentence formation using the production
function it will show you the different
tags used and the defined context-free
grammar for the given sentence
demo 2 structuring sentences problem
statement a company wants to perform
text analysis for one of its data sets
you are provided with this data set
named
tweets.csv which has tweets of six U.S
airlines along with their sentiments
positive negative and neutral the tweets
are present in the text column and
sentiments in Airline underscore
sentiment column
we will be retrieving all tags starting
with at the rate in the data set and
save the output in a file called
references.txt let us first import the
pandas library and read the tweets data
set
extract the features text and Airline
sentiment
we will iterate through the data set
using reg X find the relevant tweets
now we will import the enter tools
module it returns efficient iterators
the result is stored in a file named
references.txt
let's extract all noun phrases and save
them in a file named noun phrases for
left carrot Airline underscore sentiment
right carrotreview.txt
here left carrot Airline underscore
sentiment right carrot has three
different values positive negative and
neutral so three files will be created
now we will iterate all the leaf nodes
and assign them to noun phrases variable
this means that the functions in itter
tools operate on iterators to produce
more complex iterators
using the map function we will get all
the noun phrases from the text
putting it into list
creating a file name in the name of
review.txt
great we have now seen how to explore
and examine the Corpus using text
processing techniques Christopher himmel
I am going to talk this evening about
neural networks
I want to give an introduction uh and
also talk about the structure basic
structure of neural networks
so let's uh let's do it
so if this is a basic agenda for what
we're going to talk about we're going to
first go through the biological
motivation behind artificial neural
networks
we're talking about neurons and the
what they're made of they're going to do
a mathematical approximation of exactly
what we started with which was that
biological breakdown
and then we're going to show this simple
perceptron which was the first time uh
the the neuron was modeled uh you know
many decades ago
at least half a century ago
and uh
and then we're gonna build it up from
there we're going to start with a simple
neuron the perceptron and then we're
going to show how that is actually let's
just take a regression when we start to
uh put multiple uh points together you
know it could be just one neuron but
multiple inputs
uh and we're gonna
show how when we make more complex
neural network uh then we can represent
this in mathematics with a matrices with
some linear regression
and also show uh one part of the neuron
uh the activation functional
organization we're going to show all the
math behind that
so and then at the very end uh we're
going to build a neural network from
scratch start with a single neuron
then we'll make it a layer of neurons
and then we're going to make it multiple
layers which actually is a deep neural
network
so that's what we're going to do tonight
um
so let's start with the neuron structure
biological what is a neuron actually
look like and very shortly you'll
understand why I'm showing you this
uh but uh but yeah so there's really
four major parts to a neuron neuron is a
cell in the brain in the nervous system
uh it's what makes us really smart is
all these neurons so let's look at the
smallest fundamental piece of
a neural network of the brain is is a
neuron neuron consists of the cell body
which you can see here
uh that's and it basically
sends a pulse out if it gets enough
input it sends a pulse out so and we're
going to do that in math shortly
so where does the center pulls out to
sends it out through this Axon
which this axon is actually where the
learning happens
uh the the stronger this axon is the
more of that pulse makes it to makes it
out of the neuron out into this area
which
then goes through these synapses and you
have to imagine okay for a second
another neuron over here
there's other neurons that these
synapses are connected to
so they activate the next neuron and how
do they activate it they activate it
through dendrites which these dendrites
are activated from another neuron at
this end you have to imagine more
neurons
that are sending out their pulse and
coming in through the pulses going
through the axon through the synapse and
then into the next
next one next to neuron through the
dendrites
Okay so
that was basically how a neuron looks
now let's let's take the same pieces the
cell body the axon the synapse the
dendrites and let's look at how
mathematically we would represent that
so the axon is coming from other neurons
and the axon is going to we're going to
represent
that pulse coming from the other neuron
is an X it's that's the variable that is
you know what how much pulse is coming
from the previous neural and
um it's passing through synapse
okay so the synapse is represented by
this W or weight it says how much of
this pulse makes it through to the next
neuron
it's
it's the strength of that
uh that acts on the previous neuron
firing going through to the next neuron
but how does it get into the next neuron
you send it to the next neural through
dendrites
so if we multiply those two together so
our synapse is controlling how much by
multiplication
gets through to the dendrite
and this is when you multiply it
together that's this is showing going
into the next neuron the cell body and
we have multiple of those dendrites
coming from different synapses from
different axons
uh going into this the cell body
and in the cell body it's just adding up
all these inputs okay that's what this
summation is it's adding them all up
Miss bias don't worry about it for now
that's a mathematical
construct to make sure that
if all of your inputs are off okay you
still have something coming in this this
B makes it so that you don't have zero
coming in
and then that this once you add up all
these
and the the bias the B
uh then it goes through this function
and this is going to be the activation
function which we're going to show all
those so we're passing this summation
through we're taking the function f of
that and then that's going to be your
next output which will become your x to
the next neuron
that's the Axon
so here we go so the output
if we if we write what this value ends
up being it's a f of
whatever's in here which this is the
remember this is the sum of all of those
inputs F of all of that that's what goes
into the output which goes to the next
neuron goes to the next axon I should
say that's an axon
all right so really what I did I just
showed you a perceptron
now we're going to show you that math
that I just showed you in a little bit
different flavor which is more the
flavor of
how that
how neurons were developed you know
decades ago
we have all those inputs
all the X's
and a bias
okay we have all the W's we just showed
all the W's those are those are weights
those are strengths or the synapse
then we have the summation and the
activation in the cell body and the
output being an axon so it's really
uh the same same thing we just did
uh drawn a little bit differently here
we have all the inputs
and I'm showing this over and over again
so that you get into your head you know
what this neuron looks like
mathematically
uh and you should be able to recognize
how this looks just like that you know
the the biological part of it
so you have the inputs and the bias and
we have all these weights
okay multiply by the X's
some sum them all together and then pass
through an activation function and we'll
talk more about the activation function
shortly
okay so that was
take converting that biological
piece to the mathematical one neuron
now let's write more math about this a
little bit more and show
how you know another picture of what our
neuron could look like
uh this is just one neuron
just like we had before here it's G
instead of f
okay we have all these thetas which are
multiplied by the X X is all your inputs
um
and if we just write it this way
instead of the thetas we write W's
times all the X's
and then take this summation and just
pass it through a function at a g that's
our activation function
if you if you've done any modeling at
all statistical modeling basic you know
data science machine learning
you'll recognize that this is this is
the congression
uh so our you know one neuron is exactly
the same as our logistic regression
equation
you know passing through uh we have all
these inputs and we have one output the
output is again the activation function
here is an activation function okay it's
a sample one happens to be the most most
popular one but it's only one
where we apply that activation function
to
the summation of all of our weighted
inputs
okay you'll hear that a lot weighted
inputs in neural network talk
okay
and something else about logistic
regression if you remember your feature
importance is something in linear and
logistic regression is all these weights
it says how important is each one of
these inputs how much of them when you
multiply are getting passed through
and then our logistic regression is
actual activation function
Okay so
I just showed you one neuron
let's make this more complicated instead
of one neuron let's have three A1 A2 and
A3
this is these are not neurons okay if we
go back
with this picture these four things are
not inputs remember they're just values
coming in their values coming out of the
previous layer
if we have a previous layer
uh but that's your input to these and
we've got three neurons instead of one
so instead of having
just one neuron and then four inputs
and then four weights we now have four
times three or twelve weights
so if we want to write that same
function that we wrote before
with uh with a matrix you know with
linear algebra
if we want to write it out we would
write it like this where
see these blue lines are for the blue
weights
the green lines are for the green
weights
and then the orange or for the
orange weights for this third one
so we take those write it like this with
a matrix
um right this input is a matrix and then
we add that bias term one bias for each
neuron
we would get something like this you
know when we multiply it out anybody
that's done any linear algebra use you
know when you multiply
matrices together it ends up looking
like this
so uh you take this
right here this will be three values
you know once you add these together
and pass it through the activation
function you now have the output of the
neurons
so this is the way when you have more
than one neuron
it has to be you it's going to be more
complicated the math is getting more
complicated the algebra so you write it
in
in
linear algebra terms matrices
multiplication
and then we say this is our weight our W
this is our X Matrix this is our bias
Matrix and here through activation we
get our outputs
and this is how you'd write it as a
function f of
WX plus b
equals a and then sometimes you write
this as y because Y is an output but in
this case we wrote it as is a
okay so that activation function
when we talked about the biological part
of this okay we said that
the neuron turns on if it gets enough
input so what we're showing here is if
we have enough input so that's on the
input input is along the x-axis and the
output is the y-axis if we have enough
inputs then our output goes High
you know goes all the way on our input
you know if we don't have enough input
if it's if we add up all of our inputs
weighted sum inputs then
we have not enough and we have a zero
coming on the output the at the neuron
is not activated
and this is just the way you would write
that mathematically
uh f of x equals zero for anything where
our input our X is less than zero and
one for anything where we have enough
input
this is the binary step
activation function
and something you need down the road in
the net the next lecture we're going to
talk about
how we how we have our neuron our neural
networks learn we need to take the
derivative of this activation function
so this is how
the derivative would look like
derivative is a slope
for X not equal 0. we have a slope of
zero for X when it's at zero it's a
vertical line which means an infinite
slope so that's why we have a question
mark
don't worry about the derivative
tonight
um
you know just just be aware that that's
if that's the case
all right so another activation function
um is this Logistics sigmoid which is I
showed you that here before 1 over 1
plus e to the minus X
and that's what it ends up looking like
your input
being negative
gives you a small input and the bigger
your input is
the higher or the closer to one it gets
it converges onto one
uh and here right in the middle uh right
where x equals zero we have a 0.5
write and write it half
uh when we take the derivative we get
this we have no question mark you know
we have no vertical line here so we can
take the derivative of this curve
that's why this one's preferable to the
step actually because of that because we
can take the derivative
another
uh very popular activation function
uh where we're remember the activation
function is what we're passing our
weighted inputs weighted sum through
another one is this rlu we're rectified
linear unit
which means that if we have a negative X
Okay negative input then we get zero
our neuron is not activated if our if we
have a positive output then we just get
whatever that is on the output
x equal y equals x f of x equals X for X
greater than or equal to zero
or zero if it's less than zero
and for this the derivative is zero or
when it's less than zero and it's
actually one
if it's greater than or equal to zero
greater than zero at at zero it's
undefined and that's the problem with
this one is it doesn't have a smooth
derivative
Okay so
these different activation functions are
just part of how the neuron works it's
the it's what
output is what you know turns on or off
the neuron
all right so now we're going to take
we're going to build up our neural
network this is the you know we went
through the you know the the basis of
okay where do the neuron neural network
come from
to what is a neural what is the
mathematical part of it now we're going
to start stitching them all together all
those pieces we just built
very we're going to start with the basic
neuron that we started with one neuron
okay and this is called a four by one
neural network which means four inputs
one output
so one neuron with four inputs and it's
our and we call this a layer okay the
inputs even though it's not neurons it's
just a layer of inputs and then we have
our output layer
so this is one layer and one neuron
uh
everything that we've been showing up to
now it looks looks like this so let's
make it a little more complicated
uh now we're going to add some neurons
in the middle
okay originally we had an output and
three inputs now we're going to stick
these ones in the middle
okay and then have our inputs feed that
middle layer
okay in our middle layer
is going to feed our output
so now we have like this buffer in
between
uh and this is called a four by three by
one
neural network
so that means four inputs three in the
hidden layer this middle layer is called
the hidden layer because you keep the
output can't see it you know the input
can't the input people can't see it
because it's it's buffered you know by
the input of the output
so it's hidden uh so how many neurons do
we have
we still have four inputs but now we
have three more neurons plus the one
output neuron so this is four
neurons
basic four neuron two layer
uh neural network and once you go to
this level
okay you've gone from a basic one layer
neural network to deep learning
okay that's all deep learning means is
more than one layer
and what this means is you can represent
a much more complex
model with this because what we're doing
is we're taking in inputs
and coming up with an output so we're
modeling a system
and we can Model A more complex system
with this
do we need how many arrows do we have
one two three four five six seven eight
nine ten eleven twelve so 12 just like
before but we have more
okay these are not the outputs this is
the output
so we have three more
you know synapses weights whatever you
want to call them we have three more
arrows
so we have a total of 15 different
synapses or axons you know dendrites and
the sit-ups in between
okay so now let's look at a wider
Network
uh and the application for a wider
Network
you get much more interesting things out
of it
this is also
a
two-layer neural network with
you know these are the inputs
and these are the hit this is the hidden
layer and this is the output layer
so how many neurons do we have
uh well let's talk about inputs first
how many inputs do we have the way we
drew this
okay with this dot dot dot we have 784
inputs
why I'll explain in a minute
how many do we have in the middle
we say 16 down here there's a dot dot
dot so it's not five
it's five plus whatever is here and what
we're saying is 16. that's an arbitrary
number
the the how well this works depends on
how many hidden layers
on the output how many neurons do we
have
we have 10.
okay because we have this dot dot dot 10
outputs
so what this is is 784 inputs
16 hidden layers and 10 outputs now what
are we going to do with this network
okay but if we take a 28 by 28
grid
okay and draw numbers on it handwritten
numbers
you know it's a grid it's like pixels
everybody understands how to take a
picture that that's a representative of
you know it's represented by pixels
we can basically tell what those
handwritten numbers are and you know
this will be another lecture is actually
developing this with python uh
developing this network and showing how
you can do handwritten numbers and this
is you know decades ago three decades
ago at least
that we were doing this level of
handwriting recognition
so the way this works is we put that we
say
which one of those pixels is lit up
and then we give it a number going in
all the other ones that are not lit up
give them a zero
one of these zero through nine will
light up
depending on
what is on that input what is on those
pixels
and that's how this neural network
recognizes handwritten numbers
so this is just a way of looking at the
count and that data set okay the
handwritten numbers
is a very popular data set the mnis data
set
early image recognition as in like you
know a while ago
and so that's how many arrows do we have
784
you know buy however many you have in
the middle 16
and then 16 by 10 on this layer
so that's how many synapses axons
dendrites yeah
how many neurons do you have 16 plus 10.
is 26. that's it to recognize
handwritten numbers all you need is 26
neurons in exactly the same form I just
presented
that's all you need and you can do
handwritten numbers this is how the
brain works
this is why we are so smart all we need
is 26 neurons and we have 100 billion of
them in our head
to recognize handwriting
it's pretty amazing
is how smart we are
all right so let's talk about
deeper neural networks you know that was
a very basic
deep neural network
let's go and look at deeper neural
networks
so before we had just one hidden layer
okay now let's do
more hidden layers okay now we have two
hidden layers we have the inputs
okay we call that layer but it's not
actual neurons
then we have four neurons in this hidden
layer four neurons in this hidden layer
and then one in the output
so what's the application of this one
I don't know add addition could be the
application of this one and we'll yeah
we would see that we can actually do
that
so this is a very simple
deeper neural network where we just
added one neuron or one layer
uh
uh but
there's also even more
than that we could have you know 100
hidden layers
okay and that's how convolutional
networks have come about
and the layer the neurons are a little
different and just that they're
different activation
that's all they're all the activation
we've been doing up now is the same kind
I showed you before same thing is what's
in
logistic regression
but you you do different kinds like the
relu you and then you can do
uh what's convolutional neural networks
do which is better image recognition
not just like handwritten numbers but
like faces okay much more complex
pictures more pixels
uh there's also recurrent neural
networks okay which is where
instead of just feeding forward all the
way through
you have feeding backwards
or feeding across
so that's just that's all that is auto
encoders are where you have the same
thing on the input as you do on the
output
and you can do some special stuff with
that
okay so uh another
uh so I just talked about these Advanced
neural networks uh like convolutional
neural networks I'll show that structure
in just a second recurrent neural
networks I'll show that one as well and
a very special type of recurrent neural
networks is the lstm long short-term
memory
which you can actually you can do neural
networks with a combination of ease I
know this is Way Beyond what we're
talking about here what I'm doing is
just I'm going to show you extensions of
the basic neural networks
so the CNN looks like this
where you have this very complex picture
at the beginning
feeding through all these layers
all the way to the outputs
and
very basic just neurons throughout all
of these
and you have different types of neurons
neural
layers convolution versus relu versus
pooling
you just one after the other after the
other of all of these
layers and we end up with being able to
tell whether that's a car truck van or
bicycle
okay but it's all the very basic
structure
the structure I just showed you
just add more and more and more and more
of that that's all it is
so RNN recurrent neural network
here's some some uh
I'll stop this in the middle here
um
or I guess oh that's this one I can stop
this one here
so we're current neural network I'll
just run this one out
um takes the output and then puts it
back to the input or puts it to the next
one
um and what you do with the recurrent
neural networks is you you have a time
aspect or an order aspect stored in it
um like this is great with
natural language processing language
because language has an order to it you
have words
made up of symbols these letters and
also then sentences made up of words in
an order so like you put each word on
the input
and then you get the meaning on the
output
so uh that's you know how neural
networks work how our current neural
networks more advanced you know these
are these are going to be more uh
lectures that that will be showing in
the future
and then the combination uh between the
two
like if you have
your CNN structures at the beginning and
then
the long short-term memory
uh structures in the middle and then the
outputs you have just like this the
basic structure that we started with
um and the input being a picture
or words and the output being meaning
feelings is another kind of meaning
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that course everything from the
fundamentals to Advanced Techniques for
that accelerator Credit in a and ml with
a comprehensive postgraduate programming
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and ask me anything sessions by IBM with
three Capstone projects and 25 plus
industry projects using real data sets
from Twitter Uber and more you'll gain
practical experience master classes by
Caltech faculty and IBM experts and show
top-notch education simple learns job
assist helps you notice by Leading
companies this program calls statistics
python supervised and unsupervised
learning NLP neural networks computer
vision gns Keras tensorflow and many
more skills enroll now and then look
exciting Ai and ml opportunities the
course link is mentioned in the
description box below finding a suitable
job in the field of machine learning is
becoming increasingly difficult the
ideal way to display your machine
learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
determination system fake news detection
and many more are the best way to
improve your early programming skills
you may have the knowledge but putting
it to the use what is keep you
competitive here are 10 machine learning
projects that can increase your
portfolio and enable you to acquire a
job as a machine learning engineer at
number 10 we have loan approval
prediction system in this machine
learning project we will analyze and
make prediction about the loan approval
process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe even everything you read in
social media isn't it true that not all
news is true but how will you recognize
fake news ml is the answer you will able
to tell the difference between real and
fake news by practicing this project of
detecting face new this ml projects for
detecting fake news is concerned with
the fake news and the true news on our
data set we create our tfid vectorizer
with SQL the model is then fitted using
a passive aggressive classifier that has
been initialized finally the accuracy
score and the confusion Matrix indicate
how well our model performs the link for
the projects in the description box
below at number 8 we have personality
prediction system the idea is based on
determining an individual personality
using machine learning techniques a
person personality influences both his
personal and professional life nowadays
many companies are shortlisting
applicant based on their personality
which increases job efficiency because
the person is working on what he is good
at rather than what is compelled to do
in our study we are tempted to combine
personality prediction system using
machine learning techniques such as as
SVD name based and Logistics regression
to predict a personal principality and
talent prediction using phrase frequency
method this model or method allows users
to recognize their personality and
Technical abilities easily to learn
about more this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than one million worldwide
each other in this machine learning
project we will develop an svm model
using python modules scikit-learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design and space model and calculate the
model accuracy and at the end we will
check the Parkinson's disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtedly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text-to-speech apis are
available for this project we will
utilize pytt sx3 pydt access 3 is a
python text to speech conversion
liability it operates offline unlike
other libraries and is compatible with
python 2 and python 3. before API
various pre-trained models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number 5 we have speech
recognition system speech recognition
often known as speech to types is the
capacity of a machine or programmed to
recognize and transfer word is spoken
allowed into readable text mlsp's
recognition uses algorithm that models
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acoustic modeling is used
to identify the finance and the
phonetics on the speech for this project
we will utilize pyts X3 pyts X3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and python 3. to learn more about this
project check the link in the
description box below at number 4 we
have sentiment analysis sentiment
analysis also known as opinion mining is
a state forward process of determining
the author's feeling about attacks what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data management
station in this project we will use the
Amazon customer review data set for the
sentiment analysis check the link in the
description box below at number 3 we
have image classification using CNN deep
learning is a booming field currently
most projects and problem statement use
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
conspiration in this project to learn
more about this project check the link
in the description box below at number 2
we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use opencv and face
recognition libraries to create a phase
detection system opencv provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be Discerning
recommendations are often made based on
a viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dataset
generated by the more than 6000 users to
learn how to create movie recommendation
system for yourself or for your loved
ones check the project in the
description box below we will take you
through our hands of lab demo of how to
do image classification using CNN before
we start I hope this screen is clearly
visible and the audio is fine if yes
please type in yes if there are any
issues do let us know in the chat
section so that we can resolve them
let's wait for some more minutes to let
us the people join until then let me
tell you guys that we have regular
updates on multiple Technologies if you
are a tech geek in a continuous hunt for
the latest technological Trends then
consider getting subscribed to our
YouTube channel and press that Bell icon
to never miss an update from Simply
learn great I think we can get started
so in today's session we will discuss
what image classification is and moving
ahead we will discuss what CNN is and at
the end we will do a hands of lab demo
of image classification using CNN
so before we move on to the programming
part let's discuss what image
classification is and proceed further
for the same what is image
classification
the process of classifying an entire
image is known as image classification
images are anticipated to have just one
class per image models for image
classification taken an image as input
and produce a prediction of the class to
which the image belongs
so we can utilize image classification
models when we are not interested in
individual instance of items with
position information or their shape
so let's see what is CNN
machine learning includes convolutional
neural networks also known as convents
or cnns it is a subset of the several
artificial neural network models that
are employed for diverse purpose and
data sets a CNN is a particular type of
network design but deep learning
algorithm that is utilized for tasks
like image recognition and pixel data
processing and so more
okay
although there are different kinds of
neural network in deep learning
cnns are preferred neural architect for
identifying and recognizing object
therefore they are really suited for
computer vision activities and
application where accurate object
recognition is crucial
such as facial and self-driving
automatic system so moving ahead
so dear Learners if you want to upskill
your AI and machine learning skills so
give yourself a chance to Simply and
professions certificate program in Ai
and machine learning which comes with a
completion certificate and in-depth
knowledge of AI and machine learning
check this course out details from the
description box below so now let's move
to our programming part of how to do
image classification using CNN if
getting your learning started is half
the battle what if you could do that for
free visit skillup by simply learn click
on the link in the description to know
more
the first B we will open a command
prompt to write a command to open
Jupiter notebook so here we will write
Jupiter
notebook
press enter
so this is the landing page of Jupiter
notebook so here you can select new
python file
so this is how the kernels look like
okay Jupiter notebook kernels look like
so first we will import some major
libraries of python which will help us
in like analyzing the data okay so in
this file we will classify small images
of cifar 10 data set from tensorflow
Krrish data set there are total 10
classes as shown below so we will use
CNN for the classification purpose okay
so here I will write import
tensorflow
as TF
okay so from
tensorflow
dot Keras
import
data sets
from our layers
of models
okay so we will import
numpy as NP
right and import
matplotlib
as
elt
right okay so here I will let By plot
as PLT
right so tensorflow this one
this
so tensorflow is a free and open source
machine learning and artificial
intelligence software Library
it can be used for variety of
applications
but it focuses on mainly deep neural
network training and the influence
purpose okay got it
and this numpy numpy is a python Library
used for working with the arrays it also
has a function for working with the
domain of lineal algebra and matrices it
is an open source project and you can
use it freely numpy stand for numerical
python
and this third one matplotlib
for Python and its numerical extension
numpy matplotlib is a cross-platform
data visualization and a graphical
charting package as a result it presents
a strong open source substitute for
Matlab the apis application programming
interfaces format plot live allow
programmers to incorporate graphs into
GUI applications
got it
so let's run this
let me change image
classification
using
the yellow
okay
so let's load the data set okay we will
load the data set from the uh
load data function
so here I will write
X
underscore
train
comma why underscore okay
okay
and one for test X
underscore test comma y underscore test
okay then equals to data set
result sets dot we are using Cipher 10.
C for 10.
for 10
dot load
Dot
underscore
data okay
so it will load our data so let's load
the data
so data is loaded let's see X
let's go
test dot shape
okay
yeah
so as you can see
so we have thousand
rows and what what x
underscore train
dot shape
let me run this
so here you can see
like we see like training data
like training images are
50 000 and the test images are
10 000 okay
so this is for testing this is for
training and
so moving ahead we will see for the Y
train
dot shape
1000 and here we will see the array so y
underscore train
five
okay let's send this first let me give
some space yeah
so here y underscore train is a 2d array
like for our classification having 1D
arrays are good enough
so what we will do we will convert this
to now 1D array this is 2D array we will
convert into 1D array okay
for that
we'll write here y underscore train
was to Y underscore train
dot reshape
minus 1 comma
then why underscore train
and again semicolon 5.
let's run this
okay so now this is 1D array
so
y underscore test
equals to Y underscore test
Dot
reshape
n minus 1
here I will write
classes
there are some classes okay in the data
set like airplane
comma
automobile
comma
but
comma
cat
comma
yeah
dog
yeah
dog
course
step
the truck
okay
so these are the some classes
like airplane automobile bird cat deer
so it will be helping classifying the
images
so let's plot some images to see like
what they are what they exactly are okay
so we will what we will do we will
create one function
for that
let me write here
def plot underscore sample
okay then X comma y
comma index
okay
then I will write here plot dot figure
PLT dot figure
then figure size should be
comma 2
equals to
PLT dot show
like image show to IM show
X
index
PLT
dot X label
classes
y index okay
so let's see some samples of the images
so I will write a plot
underscore sample
uh let's go train
comma y underscore train
okay comma let's see the fifth image
okay then enter
would be capital
so as you can see this is a car so it is
showing automobile okay
so let's see once more
like plot underscore sample
X underscore train
comma y underscore test
comma we'll see the tenth one
here it is not quite visible
so we'll go for the 11
okay
okay I am
why it is showing wrong because I use a
test I have to use here train instead of
test
then it will show I think correct yeah
you can see horse the nodes
then what about
201 image
see you can see
so we will see once more
500
Fancy Frog okay it's not quite visible
yeah so you can see here
the proper ship okay
so what we will do now we will normalize
the images to a number from zero to one
image has three channels like RGB colors
so and each value in the channel can
range from 0 to 255. hence to normalize
in 0 to 1 range we need to divide it by
255 okay
so
now what we will do we will normalize
the data
so here what we'll do at X underscore
train
equals to
X underscore train
divided by
255 and 0.
okay
and same for test
question test
divided by
255.0
okay to range between 0 to 1.
we will build simple artificial neural
network for image classification first
okay
so we will write here a n
equals to models Dot
sequential
okay then I will write here
layers
Dot
platen
input underscore
shape equals to
32 comma 32. comma 3.
okay
then again layers dot tens
3000
comma activation
got it
and again for the Thousand so I will
what I will do I will cop it
and paste it here okay
thousand
so let me add one more one more layer
dot tens
then
we'll add 10 comma
activation
softmax
okay
here I will okay let me give for the
better visuals
yeah
so a n n
dot compile
optimizer
okay
it was so SGD
and comma
I will write a loss
equals to
sparse
categorical
cross entropy
okay
so here I will add comma then Matrix
equals to
accuracy
a n
dot fit
underscore train
comma y underscore train
comma epochs
okay
so number I will give you box equals to
five
right
so it will take time to run
it's stunning
okay there is some issue
underscore train
it box equation
five
what it is saying
when user code the file program C
engine okay
you will copy it
and paste it again
run it again
okay now it's working
okay
so it will take time and then I will get
back to you okay
then import
numpy you guys already know what numpa
is
and B
y underscore prediction
NN dot predict
it
X underscore test
okay then y underscore
prediction underscore
classes
equals to
NP Dot
argument Max
element
or
element
in
y underscore prediction
print
classification
both
comma classification
let's go report
then y
underscore test comma y underscore
prediction
underscore
classes
okay
so let me run this
this
capital
yeah
it will take less time
now we will create a graph
okay
like X text is like we have thousand
images so CLA graph will be like messed
up
still let's see
so I'd like to import
c bond as SNS
okay
okay c bond
yeah
let me give some space
here I will write PLT Dot
bigger
because
I should be
14 comma seven
okay then as soon as we will create a
heat map for this
and Y underscore prediction
not question
true
then PLT dot y label
just truth
alt dot X label
fiction
than PLT dot title should be
vision
okay then PLT dot show
let me run this
okay
see why prediction has X text
it's still running
let's wait
so now let's make CNN model okay to
train our images so for that I will add
CNN equals to
models Dot
sequential
okay
yeah so let me run this
so this is our CNN model from which we
will train our images
and CNN like compile
then
optimize
optimizer
equals to
Adam
okay
comma
right here then loss
equals to
ours
categorical
you then cross
entropy
okay
and comma
I will write here Matrix
equals to
curacy
we run this
okay
those
yeah
same goes for you
okay loss
will I do I will rewrite this
okay now let's check CNN model
for the 10 epox
okay
let's see the accuracies increasing or
not
CNN
it's called train
comma y underscore train
comma
box
also
it is started
it will take less time than the previous
one
okay
see
you want this whole code
you can comment down the same
okay
so after completing this I will get back
to you
so it is almost done like 27 seconds
with a 10 epochs
till then let me write CNN dot evaluate
underscore test
comma y underscore test
okay
so with CNN the N5 Epoch accuracy was
around like 70 percent and which is a
significant improvement over a n okay
and then we have like just 49
okay and cnns are the best for image
classification and gives the super
accuracy also computation is the much
less compared to simple a n as Max
pooling reduces the image Dimension
while still uh preserving these features
okay so let me run this
foreign
take some time
till then I will write y underscore
prediction
equals to CNN
Dot
predict
and X underscore test
okay
then I will write a y underscore
prediction
then column five
let me run this
so you can see the accuracy and all the
array
okay
it's
yeah
so let's
class is equals to
NP Dot ERG Max
element
or
element in
y prediction
then why underscore classes
then
these are the number of classes then y
underscore test
is column five
these are the array
so it's converted into array then
now let's see the it is predicting Right
image or wrong image okay by not with
the training data here we predict from
the training data by training extend
okay now we will predict from the test
data so here I will add plot
underscore sample
then
X underscore test comma y underscore
test
and you can write
these random one
so here I will write 60
let's see so you can easily see here
this is odds and it is predicting right
horse
and let me
okay
plot
let's go sample
then X underscore test comma y
underscore test
comma
100
okay
press enter
okay X is capital
yeah
so you can see this is Dr
okay so our model is predicting the
correct
image okay then what we like
let's see it is predicting the right
class or not okay we made the classes
like random classes
okay where are these these so let's see
it is predicting right or wrong
okay for that I have to write classes
y underscore classes
like which number 60. okay
60.
so 60 is not defined because like number
of classes
okay okay so there are one two three
four five six seven
okay zero to nine I can choose
so here what I will do
I will take small one
I get five
okay this is frog
okay
it's from right
so I will take instead of 60 here I will
see frog
okay y class is not defined
by underscore classes
is defined see okay there are three s
yeah
so as you can see frog this is frogs so
our classes depending right so here I
will write it again like
60
it was hot
and let me write here 60.
you can see the right prediction okay
so
this is what how you can you do image
classification using CNN first we will
import some major libraries of python so
here I will write import
pandas as PD
and import
numpy as NP
then
import
c bond
that's SNS
and import
SK learn
dot model selection
port
train underscore
test underscore split
before that
I will import
matplotlib
dot Pi plot
as PLT
okay then
I will write here from SK learn
Dot
Matrix
import
accuracy
or
than from
Escalon
dot matrix
Airport
classification
to report
and port
Ari
then import
string
okay
then press enter
so it is saying
okay
here I have to write from
everything seems good
loading let's see
okay till then Nampa is a python Library
used for working with arrays which also
has function for working with domain of
lineal algebra and matrices
it is an open source project and you can
use it freely
number stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and Time series
then C born
an open source python Library based on
matplotlib is called c-bone it is
utilized for data exploration and data
visualization
with data frames and the pandas Library
c-bond functions with ease
than matplotlib for Python and its
numerical extension numpy
matplotlib is a cross platform for the
data visualization and graphical
charting package
as a result it presents a strong open
source suitable for Matlab
the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications then this train test
split we may build our training data and
the test data with the aid of SQL and
trim test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like hone and test
accuracy score the accuracy score is
used to Gorge the model's Effectiveness
by calculating the ratio of total true
positive to Total to negative across all
the model prediction
this re regular expression
the function in the model allow you to
determine whether a given text fits a
given regular expression or not
which is known as re
okay then string a collection of letters
words or other character is called a
string it is one of the basic data
structure that serves as the foundation
of manipulating data
the Str class is a built-in string class
in Python because python strings are
immutable they cannot be modified after
they have been formed
okay so now let's import the data set
we will be going to import two data set
one for the fake news and one for the
True News or you can say not fake news
okay
so I will write here
EF underscore
big was to
PD Dot
read underscore CSV
or what can I say dear fake okay
it underscore fake
okay
then
pick
dot CSV
you can download this data set from the
description box below
then data Dot
true
plus PD dot read
underscore CSV
sorry CSC
plan
fake news sorry true
dot CHP
then press enter
so these are the two data set
you can download these data set from the
description box below so let's see the
board data set okay
then I will write here data underscore
fake
dot head
so this is the fake data okay then
data underscore true
Dot
and this is the two data
okay this is not fake
so if you want to see your top five rows
of the particular data set you can use
head
and if you want to see the last five
rows of the data set you can use tail
instead of head
okay
so let me give some space for the better
visual
so now we will insert column class as a
Target feature okay then I will write
here data
let's go fake
Plus
equals to zero
then
Theta underscore true
Plus
versus one
okay
then
I will write here data underscore fake
dot shape
and
data underscore true
dot ship
okay then press enter
so the shape method return the shape of
an array the shape is a tuple of
integers these number represent the
length of the corresponding array
dimension in other words a tuple
containing the quantities of entries on
each axis is an array shape dimension
so what's the meaning of shape
in the fake word
in this data set we have two three four
eight one rows and five columns and in
this data set true
we have two one four one seven rows and
five column okay so these are the rows
column rows column for the particular
data set
so now let's move
and let's remove the last 10 rows for
the manual testing okay
then I will write here data underscore
fake
let's go manual
testing
was true data underscore fake
dot tail
was it last 10 rows I have to write here
10.
okay so for I
in range
two three four
eight one
sorry zero
comma 2 3
4 7 0
comma minus 1.
okay
and
TF underscore not DF data
underscore fake
dot drop
one
yeah instead of one I can write here I
comma
this is equals to zero
it plays
equals to true
then
data
not here
data underscore
same I will write for I will copy from
here
and I will paste it here
and I will make the particular changes
so here I can write true
that I can write true
okay
then I have to change a number
two one
six
right
2 1 4 0 6
-1
same
so press enter
X is equals to zero
valid since X maybe you mean double zero
or so of this
okay we will put here double equals
well I'm putting this
egg dot drop i x is equal to zero okay
in place okay
and also write equals to an equation
yeah
so
okay axis is not defined
foreign
let me see
now
did the
underscore
fake dot shape
okay
and data dot true
and
data underscore true
dot ship
as you can see
10 rows are deleted from each data set
yeah
so I will write here data underscore
fake underscore manual
testing
class
equals to zero
and data underscore
true
let's go
manual underscore testing
plus equals to
1.
just ignore this warning
and
let's see
data underscore
fake underscore
manual
testing dot hat
as you can see we have this
and then data dot sorry underscore true
underscore
manual
testing
dot at
10.
this is this is the true data set
so here I will merge data
underscore merge
question
PD Dot
concat
concat is used for the concatenation
data underscore fake
data underscore
comma
axis
equals to zero
then data underscore merge
dot head
the top 10 rows
yeah
as you can see the data is merged
here
okay
first it will come for the fake news and
then with that for the True News
and let's merge true and fake data
frames
okay
we did this and
let's merge the column then data dot
merge
Dot columns or let's see the columns
it is not defined whatever data
underscore much
these are the columns same title text
subject date class okay
now
let's remove those columns which are not
required for the further process
so here I will write data underscore
or request to
data underscore merge
crop
title I don't need
that
subject
we don't need
then
so one
so let's check some null values
it's giving here
because of this
that's good then data
dot is null
Center
so no null values
okay then
let's do the random shuffling of the
data frames okay
for that we have to write here data
equals to
data dot sample
one
then
data
okay
data
dot hat
now you can see here the random
shuffling is done
and one for the
true data reset and zero for the fake
news one okay
then
let me write here data Dot
reset
underscore index
place
to
true
data dot drop
comma X is
equals to 1
then comma in place
true true
okay
then let me see columns now data Dot
columns
so here we have two columns only rest we
have deleted
okay
see data Dot at
yeah
everything seems good
let's proceed further and let's create a
function to process the text okay
for that I will write here
but
okay
you can use any name
text
and text equal to
text Dot lower
okay
and text
for the substring
remove these things
from the
datas
okay
so for that I'm writing here
comma
okay
then text equals to
re Dot substring
comma
comma text
okay
then I have to write text equals to
r e Dot substring
to www
Dot
S Plus
comma
from our text
okay then text equals to
re Dot substring
then
oh
comma
okay
then
text equals to
re Dot substring
and
percentage
as
again percentage or
RG dot SK function
right here string
dot punctuation
comma
and Gamma then text
right
then text equals to re Dot substring
and n
comma
text equal
re Dot
substring
right here
and again d
then again
then comma
and again
texture
okay then at the end after right here
return text
so everything like this these type of
special character will be removed from
the data set
okay let's run this let's see
yeah so here I will add DF sorry not DF
data
data
then
text
pursue
data
okay dot apply
to the function name wordpot word opt
okay
press enter
yeah so now let's define the dependent
and independent variables okay x equals
to
data
text
and Y equals to
data
class
okay
then splitting training and testing data
okay sorry
so here I will write X underscore train
comma X underscore test
uh then y underscore train
comma y underscore test equals to
train underscore test underscore split
then X comma y
comma test
let's go size equals to
0.25
okay press enter
so now let's convert text to vectors
for that I have to write here
that it's X
so here I will write from sqlarn
Dot feature
extraction
Dot text import
t
vectorizer
okay
then vectorization
plus 2 tfid
factorizer
okay
then
it's V
underscore
train
equals to
vectorization
or reset it the ion refactorization dot
fit
that transform
the score train
okay
then XV underscore test equals to
atarization
Dot transform
X underscore test
okay then press enter
[Music]
foreign
logistic regression
so here I will write
from
sqln Dot
linear underscore model
okay import
logistic
regression
then a lot gosu
logistic
regression
and
LR Dot
fit
then XV
Dot
dot dot train
comma
x v underscore test
okay
press enter
dot XV dot train
here I have to write y train
and press enter
will work so here I will write
prediction
underscore
linear regression
l r dot predict
XV underscore test
okay let's see the accuracy score
for that I have to write l r dot score
then XV underscore test
comma y underscore test
okay
let's see the accuracy so here as you
can see accuracy is quite good 98
percent
now let's print
the classification
port
by underscore test comma
prediction of linear regression
okay
so this is you can see Precision score
then F1 is code then support value
accuracy
okay
so now we will do this same for the
decision free gradient boosting
classifier random Forest classifier okay
then we will do model testing then we
will predict this score
so now for the decision tree
classification so for that I have to
import from SK learn
dot tree
import
decision
three
classifier
okay
then at the short form I will write here
I will copy it from here
then
okay
then I have to write the same as this so
I will copy it from here
and
let's change linear regression
to
season 3 classified
okay
then I will write here same
let's go DT
question
DT dot predict
C let's go
test
e
still loading it's it will take time
okay
till then let me write here for the
accuracy
DT DOT score
3 underscore test
comma y
let's wait okay
let's run
the accuracy
so as you can see accuracy is good than
this linear regression
okay logistic regression
okay so let me
show you the
let me predict
print
okay
so this is the accuracy score this is
the all the report
yeah
so now let's move for the gradient
boosting classifier
okay for that I've read from
sqlarn
dot ensemble
port
gradient
boosting
classifier
pacifier
I will write here GB
equals to let me copy it from here
I will give here random
let's go state
equals to zero
wait wait wait wait so I will write here
GB Dot
fit
underscore train
comma
y underscore trade okay then press enter
so here I will write predict
underscore GB
also
GB Dot
wait sorry
Reddit
three
Dot test
dot dot underscore test
till then it's loading so I will write
here uh it's for the score then I will
add GB
DOT score
then
three underscore test
comma
y underscore test
okay
so let's wait it is running this part
till then let me write for the
printing this
okay it's taking time
taking time still taking time
what if I will run this
it's not coming because of this
yeah it's done now so you can see the
accuracies
not good then
decision tree but yeah it is also good
99
.4 something okay so
now let's check for the last one random
Forest
first I will do
for the random for us we have to write
from sqlarn Dot
symbol
import
random
Forest
classifier
okay
and here I will write RF
equals to
right I will copy it from here
then
random
date
question
RF Dot
fit
three underscore train
comma y underscore train
okay then press enter
and predict
underscore
RC
or F
equals to
RF dot predict
three underscore test
okay
till then I will write here still
loading it will take time
so till then I will write for this score
score accuracy score
XV underscore test comma y underscore
test
okay
then I will write here till then print
classification
port
and Y underscore test
comma
it will take time little bit
so
it run the accuracy score is 99 it is
also good
so now I will write the code for the
model testing so I will get back to you
but after writing the code so
so I have made two functions one for the
output label and one for the manual
testing okay
so it will predict
the all the from the all models from the
repeat so it will predict
the the news is fake or not from all the
models okay
so for that
let me write
here news
pursue string
what
okay
then I will write a manual underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want
so I'm just copying from the internet
okay from the Google
the news
which is not fake okay I'm adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is showing
okay
string input object is not callable okay
let me check this first
okay I have to give here Str only
yeah let's check
okay I have to add here again the Escape
yeah
manual testing is not defined
let me see manual testing
okay I have to edit something
it is just GB and it is just RF
and GPS is not defined okay okay
so what I have to do
I have to remove this
this
everything seems sorted
now
as I said to you
I just copied this news from the
internet I already know the news is not
fake so it is showing not a fake news
okay so now what I will do I will copy
one fake news from the internet
and let's see it is detecting it or not
okay
so let me run this
and let me add the news for this
so
all the models are predicting right it
is a fake news
or you can add your own script like this
is the fake news okay
I hope you guys understand
till here
so I hope you guys must have understand
how to detect a fake news using machine
learning you can you can copy it any
news from the internet and you can check
it is fake or not there's no better time
to train yourself in the exciting field
of machine learning if you're looking
for a course that costs everything from
the fundamentals to Advanced Techniques
for that accelerate your career in Ai
and ml with a comprehensive postgraduate
programming Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three caption
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Caltech
faculty and IBM experts and show Top
Notch education simple learns job assist
helps you notice by Leading companies
this program called statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns kras
tensorflow and many more skills and now
and then look exciting Ai and ml
opportunities the course link is
mentioned in the description box below
what is time series forecasting
making scientific projection based on
the data with historical timestamps is
known as time series forecasting it
entails creating model through
historical study using them to draw a
conclusion and guide strategic decision
making in the future
the fact that the future result is
wholly unknown at the time of the task
and can only be anticipated through
analysis and evidence-based priors is an
essential distinction in forecasting
give yourself a chance to Simply a
professional certificate program in Ai
and machine learning which comes with
completion certificate and in-depth
knowledge of AI and machine learning
check this course detail from the
description box below
so here is one question for you guys I
will give you exactly one minute for
this you can comment or you can give
your answer in the chat section so I can
see if the answers given by you are
right or wrong
okay
so the question is which type of
programming does python support
I'm repeating again which type of
programming does python support
oriented programming option b structured
programming option C functional
programming and option D all of the
above
so let us know in your answer in the
chat section or in the comment section
so I am starting a timer of one minute
just type your answer in the comment
section or in the chat section
do let me know your answers please
so I am starting the timer of one minute
this type of programming does python
support object oriented structured
programming functional or all of the
above
do let me know your answers please
you can comment or you can give your
answer in the chat section so I can see
if the answer is given by you are right
or wrong
which type of programming does python
support object oriented structured
functional or all of the above
30 seconds meaning
this type of programming does python
support object oriented structured
function or all of the above
let us know your answers in the chat
section or in the comment section below
and seconds more
which type of programming does python
support
five seconds more
so the allotted time is over we will
give a reply to those who gave the
correct answer and for those who didn't
give the correct answer we will give you
reply with the correct answer okay
now let's move to our programming part
so we will open command prompt to write
a command to open Jupiter notebook so
here I will write Jupiter
notebook
that's okay Jupiter
notebook press enter
will take time
so this is the landing page of Jupiter
nodu and here I will select new
python kernel file
so this is how Jupiter kernel look likes
so
here
what we will do we will import some
major libraries of python which will
help us in analyzing the data okay
import numpy
as NP
okay then import
pandas
as PD
then import
C bone
as SNS
the fourth one is from
matplotlib
port
as pld
okay
then
we will import some model libraries so
here I will add from
stats
models
dot TSA
dot API
import
exponential
meeting
and comma then
simple
XP smoothing
then one more
hold
we will write here import sorry from
skill on dot linear
underscore model
import
linear
regression
okay
then import
ance
earnings
then we will already have warnings
Dot
filter warnings
should we ignored
yes there will be no error
or it's still loading yeah
so numpy numpy is a python Library used
for working with arrays it also has a
function for working with the domain of
linear algebra and matrices
it is an open source project and you can
use it freely numpy stand for numerical
python
second is Panda
pandas is a software Library written for
the Python programming language for data
manipulation and Analysis in particular
it offers data structure and operations
for manipulating numerical data and Time
series
then C bone an open source python
Library based on matplotlib is called
c-bone
it is utilized for data exploration and
data visualization
with data frames and the pandas
liability c bond function with ease
matplotlib for Python and its numerical
extension numpy mat problem is a cross
platform data visualization and
graphical charting package
as a result it presents a strong open
source substitute for matpler
the apis format.lib allow programmers to
incorporate graphs into GUI applications
linear regression the machine learning
method regression built on linear
supervised learning
analysis regression is done regression
creates a value for the aim prediction
using independent variables as inputs
it main goal is to investigate the
relationship between factors and
forecasting
exponential is smoothing
the exponential Windows function is a
general method for smoothing time series
data known as exponential smoothing
it contrasts to the ordinary moving
average which weights previous data
quality
exponential function use weights that
decrease exponentially with time
and there is one more simple exponential
smoothing
the simple exponential smoothing classes
use Simple exponential smoothing models
to give very simple time serial analysis
a weight average of the most recent
value and the preceding smooth value
constitute the predicted value
the contribution of older value degrade
exponentially as a result of the
smoothing parameter
okay so after importing libraries let's
import data set
for this we I will write here DF
or you can write data frame PD Dot
read
underscore CSV
here I will write
monthly
underscore CSV
dot CSV
okay this is my file name
you can download this file from the
description box below
and
for seeing the data I will write here DF
Dot
head
okay then press enter
yeah so here PD is for pandas Library
read is used for reading the data set
from the machine and CSV is used for the
type of file which I was using okay or
whichever you want to read
and if you want to see the top 5 rows of
your data set you can use head and if
you want to see the last five rows of
your data set you can use tail instead
of head
okay this one you can write your tail
so moving forward let's see how many
rows and columns are present in our data
set for that I have to write DF dot
shape
okay
then press enter
okay it will give error why because
this yeah
so here you can see 847 rows and two
columns date and price only
so moving forward let's do some Eda
exploratory data analysis okay for that
I have to write here print
data
date range
of
of gold prices
available from
and here I have to give curly brackets
and DF dot location Loc
then colon
comma
date
okay D is capital here so I have to
write
date
then from location 0
to
again same thing DF Dot
Loc
date
to the length of
EF
minus 1
. oh everything seems good yeah
then press enter
okay
we have date range of gold prices
available from this 1950 to 2020. okay
in our data set
and here I will write date
equals to PD Dot
date
range
start from
slash one slash 1950
comma
and equals to
hash
one slash 2020.
comma frequency
media
okay then
then here I will write date
then press enter
yeah
so here you can see date time index
okay
from starting to length these dot dots
and here I will write
DF
and
was to
date
we have dot drop
state
comma axis is one
comma in place
also true
T should be capital
then DF equals to DF
dot set
index
and month
and DF
Dot
head
press enter
so instead of
this state
we have adjusted this month
okay for particular value
so moving forward let's see the graph
different different graphs okay TF Dot
Plot
then figure set should be
plus two
twenty comma 8.
then plot
dot title
is
like gold
prices
monthly
tense
1950
and onwards
okay title should be this
then X level should be PLT dot X label
will be months
then PLT dot y labels
should be
price
and I will write a PLT Dot
grid
okay
press enter
okay title spelling my battery ideally
okay then press enter
or why label it is sorry
here you can see
gold prices monthly since 1950 and
onwards
okay
till from 1950 and from till 2020. okay
this is the price
then moving forward let's see another
graph so I will write here for that
round
DF Dot
describe
comma 3.
and here you can see the count variable
and the average is 416.557 standard
deviation that minimum price value is
this 20 this and the maximum value is
this okay
so
so the average gold price in last 70
years is this Force 16.557 okay
only 25 percent of the time the gold
price is above 447.
the highest gold price ever does this
this one 1840 807.
so we will do visual analysis so here I
will write
ax equals to
PLT Dot subplots
sub plots
okay then figure size
pursue
25
comma r
okay then
sns.box plot
is X
equals to
DF dot index
dot ear
comma y
equals to DF dot values
colon comma 0
comma x equals to X
okay
then same PLT dot title
price
monthly
and in 50.
converts
same graph will come but in the
different format okay
let me remove this
then PLT Dot
X label
table must be here
LT dot y label
ice
and
PLT Dot
X ticks
will be rotation
wasu okay I will give rotation 90.
PLT dot grid
you can write here instead of grid I can
write here directly show so grid is with
this format this box format okay
so press enter
it's loading yeah
so here you can see from 1950 every year
is here till 2020 so how the gold prices
are decreasing and increasing okay
let's see the another graph
I will write here from
ads models
models
Dot graphics
Dot
TSA plots
port
okay
then I will write here figure comma ax
equals to
then PLT Dot
subplots
bigger size
equals to
22 comma 8.
okay
then month plot
TF comma y label
pursue
gold
price
okay comma
then ax equals to ax
and I will give the title PLT Dot
title
so I will copy from here
and PLT Dot
X label
month
and PLT dot y label
the price
PLT dot grid
so here you can see gold prices monthly
since 1950. like for every month like
January February March April so on till
December okay
we will cover one more graph
okay and many many more graphs
so
so we will
go with the next graph
for that I will add here
let's go
comma X
equals to
PLT Dot subplots
and figure size
equals to
22 comma 8
then same SNS
dot box plot
x equals to
DF dot index
dot month name
okay
then
y equals to
DF dot values
comma 0
comma ax
equals to ax
okay then PLT dot title
will put same title so I can copy from
it here
let's copy from here and paste it here
okay
and PLT Dot
X label
and PLT dot y label
price
and PLT dot grid
okay
dot show
this time let's not use grid
okay
okay something videos month name
yes
you can see
for every month
this is another type of graph
okay box plot graph
so why we are creating so much graphs
because we are doing Eda exploratory
data analysis in this we have to see the
multiple different and different
different types of
graph
okay so moving forward let's see average
gold price per year like Trend since
1950.
so for that I have to write here DF
let's go early
let's go sum
DF Dot resample
sample
then
a
dot mean
DF underscore
yearly
underscore sum
Dot Plot
PLT Dot title
and here I will write average
gold price
yearly
since
1950
or you can write onwards from onwards
1950
so here I will add PLT dot X label
here
and PLT dot y label
ice
okay
then PLT dot grid this time we'll use
grid
so here you can see the average gold
price early since 1950. okay this is the
chart
till 2020 like sometimes up sometimes
down okay
and we will see now like average gold
price per quarter
like trends like since 1950
so here I will write DF Dot
not DOT TF dot quarterly
okay
underscore sum
equals to
DF Dot resample
order Q
dot mean
okay
move this
then DF underscore
quarterly
scores sum
Dot Plot
and the same PLT dot title
from here
here I will write average good price
quarterly
quarterly
okay
since 1950
then PLT dot X label
label is now here quarter
PLT dot y label
is price
okay now PLT dot shown
at this time
okay
EF underscore quietly
okay quarterly
yeah
so here you can see the price prediction
okay let me set to the grid only
it's not this
visuals are not good so great
yeah
so here you can see the quarterly
prices prediction
okay average gold price quarterly
prediction
so like moving forward
we will see now average gold price per
decade like per 10 years okay
so from 1950 only so here I will write
TF underscore decade
underscore sum
equals to DF Dot resample
every 10 years okay 10 year
dot mean
we are writing mean because we are like
putting out average
okay
so DF underscore decade
underscore sum
Dot Plot
PLT dot title
average
goal
price
per decade
since
1950 okay
it's since 1950 yeah yeah perfect
so here I will get PLT Dot
X label
then
decade decade is off like every 10 year
PLT dot y label
PLT dot grid
yeah
so here you can see the average gold
price per decade like from 1950 to 1960
like this straight then against it then
up then sometimes down then up and down
okay every 10 years you can see a 1990
2000 2010 and 2020.
so moving forward let's do like analysis
in coefficient of variation the
coefficient of variation CV is a
statical measure of the relative
dispersion of data points in a data
series around the mean
and in finance the coefficient of
variation allows investor to determine
how much mortality
or risk is assumed in comparison of the
amount
like amount to the return expected from
investors
okay the lower the ratio of the standard
deviation to mean return the better risk
return trade-off
okay let's like let us look now the CV
values for each year in gold prices
so CV means coefficient of variation in
prices okay
so here I will write DF underscore one
equals to DF Dot
Group by
F dot index
dot ER
dot mean
dot name
dot columns
equals to
private price
and
mean
then again DF underscore 1 equals to DF
underscore one
dot range
what we can do instead of range VL
Android merge
DF dot Group by
School
DF dot index
dot EO
in the deviation
rename
columns
equals to
price
standard deviation
comma left
index
also true true
then comma right index
was so true
okay
and here DF underscore one
it's cool
first two
EF underscore one
standard division
slash
DF underscore one
two
hundred
like Dot
round figure should be like 2 after
decimal how much
like numbers you want to see
then DF underscore one
dot hit
press enter yeah
so here you can see for every year I
have mean standard deviation
and this coefficient of variation
okay
for every year
so
like moving forward let's see the
average gold price per year again
so for that figure
underscore
figure Dot
comma ax
equals to PLT
dot subplot
figure size
15 comma
10.
okay
and
I will write here DF underscore one
is
COV underscore
Dot Plot
PLT dot title
average goal
price
yearly
since
1950.
okay then PLT dot X label
is here
or PLT
dot y level
by label
coefficient of
variation I am writing CV
in percent
okay
then PLT dot show
okay DF underscore invalid syntax
HTT The Dot Plot
okay
codf underscore one DF underscore one
here
alert syntax
a
so here you can see average gold price
since 1950.
okay
this is like percentage CV in percent
okay
like
good
you can say the chart
so the CV value reached its highest in
1978 like somewhere here like 1980 1978
okay
like near to 25 percent which could have
made the asset as a highly risky but in
2020 the CV value is closer to five
percent which makes the asset variable
via good investment okay
so
now what we will do we will do time
series forecasting
okay we will train model we will build
model different different model we will
train and test split to build time
series forecasting model
so for that let me do like this first
yeah
so here I will write train
was to DF
TF dot index
dot per year
okay
equals to
2015
and for the testing we will write TF
DF dot index
dot ER
2015.
okay for training we are taking till
2015 for and for the testing we are
taking till 2020 from 2015. okay
then I will write here how many columns
present in train or test
so for that I will add print train
dot shape
and print
test dot ship
press enter
792 rows and one column in train
training for training the model and 55
to test the model okay
so now let's see the training data and
testing data
so train
a
like square brackets so
price
Dot Plot
then figure size
equals to
13 comma
5.
and
font size should be
also 15.
ice
Dot Plot
figure size
bigger size
let me give
same 13 comma 5
font size
should be
okay then PLT dot let me add grid
LT dot create
then PLT
Dot agent
training
data
comma
test data
okay then PLT
or
show I will tell you what the legend is
okay
so here you can see the training data in
blue and the testing data okay this is
known as the legend this this portion
and
yeah
so here you can see month wise
okay till 2020.
and from 1950
right these are the prices
and this is the chart so moving forward
let's do model formation now okay
we will do two models lineal
linear regression and the name based one
okay so first we are
first we will go from
linear regression
for that I will write train underscore
time
assume I plus 1
.
four
I in range
brain
test underscore time equals to
I Plus
length like train
one for I in
range then length should be test
okay
so for length
training time
or my length should be tested
press enter
okay this is the training and this for
the testing 792 rows and here 55 rows in
testing
so
LR underscore alarm means linear
regression let me make it capital
thank you
and
equals to
make a copy
underscore
test equals to test copy
dot copy
then LR
okay
so LR
train
time
what's your train time
and LR
underscore test
time
equals to test time
okay
underscore is there
so here I will write LR equals to linear
regression
dot fit to the model
train
and for the time
underscore train
price
values
linear regression is not defined okay my
bad
L should be Capital yeah
so now see the graph
so test
underscore
prediction
let's go model
one
question
Allah Dot
predict
La underscore test
time
test
first two
test underscore predictions
score model
one
okay
let's create the graph figure
size should be
let's do
but in comma 6
.
PLT Dot Plot
print
price
comma
label
should be
train
okay then plot
PLT Dot Plot
test
ice
label
equals to test
PLT Dot Plot
then LR train or tests
podcast
label
equals to
regression on time okay regression on
time
then again PLT dot Legend
best
PLT dot get it
okay
here I have to write predict
loading
one more error
price
okay
is the price
so here you can see regression on time
test data is this green one and this
training Returns the testing data
OK let's find the map now so for this
we'll write here def
me
shall comma prediction
then return
Ed
the dot mean
abs
actual
section
Jewel
I
100
comma 2.
okay
forgot to give the
yeah
so for forgetting the map you have to
write a map underscore
model
let's go test equals to mape
test
price
Dot values
comma
test underscore
model one
okay
then print
if
is
person 3
.3 f
then here after percent
model
one underscore test
comma
percentage
press enter
test
okay
jewel is not defined
web test
real model one test
May test
values
the test
sections
model one
okay here's a is small
so
map here you can see 29.76 0.
so you are a bit confused like what is
Me Maybe is a measure of prediction of
accuracy of a forecasting method in
statical model OKAY is a measure of
prediction accuracy of forecasting
method in static and author
okay
now
results
equals to PD Dot
data frame
test
map
in percent
underscore
model one
comma index
equals to
regression
on time
if then I will print the results
okay D should be capital
real pandas has no attribute data frame
okay
model one is not defined
let's go test
when it's in there perhaps you gotta
forget a comma
colon
model one okay
here you can see the test map regression
on time
so let's do with the name now we have to
perform the same pattern so what I will
do what I I will write the code and get
back to you okay
so I'm done with the code so here you
can see I have same pattern train and
test copy
and this is the name forecast on the
test data so this is the line
and this is the training and this orange
one is testing
the same we have got me like 19.380
okay
so and regression is this
of the name model
so what we'll do we will create now
final model of ours okay
and we will forecast final forecasting
will do
for that I have to write final
model
question
exponential
smoothing
okay DF comma
Trend equals to
iterative
comma seasonal
pursue additive
comma fit
smoothing
level
goes to 0.4
comma
smoothing
trend
0.3
comma smoothing
seasonal
0.6
okay press enter
exponential exponential
so map
go final underscore model is equal to
map
like DF
dice
dot values
comma final
let's go
model
Dot
fitted values
then print
if
comma map
School
final underscore model
okay
so map is 17.24 which is quite good for
the final model
okay
so getting the prediction for the same
number of time stamps at the present
time in the test data so I will write
here predictions
equals to final
underscore model Dot
forecast
steps
equals to length DOT test
Center
now we will compute 95 5 percent of
confidence interval for the predicted
value so
said tkf equals to PD Dot
data frames
did
then
lower
CI
prediction
1.96
into
NP dot standard deviation of final
model
Dot
one
okay
and comma
right here prediction
okay then upper CI
action
Plus
1.96
into
dot standard deviation
the same final model
but
so one
okay
then prediction
underscore DF dot at
so this is lowerci prediction and the
upper CI okay how much it will forecast
or
now
at the end at the final State what we
will do we will plot a graph okay
forecast graph along with the confidence
band
so for that X is equals to DF Dot Plot
label
pursue
actual
comma figure size
16
comma 9
addiction DF
and
Dot Plot
x equals to
this
comma label
underscore DF
it's lower CI
what I will do
instead of this
label equals to
forecast
comma Alpha
equals to 0.5
okay
this
dot bill
between
underscore DF
dot index
comma prediction underscore DF
to 1 underscore CI
comma prediction
underscore DF
this CI
then
color
question
um
comma
Alpha
question
15.
okay
then X is
dot set
underscore
X label
year month
axis
dot set
y label
it's
PLT dot Legend
should be location also
best
then PLT dot grid
PLT dot show
okay this is PLT only
then press enter
okay 16 comma 9 position argument
follows keyword argument
15.
oh
okay okay I have to give you
Oppo CI
alright upper CA
so here you can see our final model
forecasting so till 2020 it is showing
like normal and after that till 2030
okay this will be the forecast as per
the data
okay
so
here you can see
as you can see we have the map is
17.24
okay then here I did the prediction for
the testing data
and here I've created the data frames
these are the data frames lower CI
prediction and upper CI and then I have
created
the and then I have created the final
graph of the forecasting
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that course everything from the
fundamentals to Advanced Techniques for
that accelerate your career in Ai and ml
with a comprehensive postgraduate
programming Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three caption
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you'll gain practical
experience master classes by Caltech
faculty and IBM experts and show Top
Notch education simply learns job assist
helps you notice by Leading companies
this program calls statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns kras
tensorflow and many more skills enroll
now and then look exciting Ai and ml
opportunities the course link is
mentioned in the description box below
intimate analysis sometimes known as
opinion mining is a technique used in
natural language processing NLP to
determine the emotional undertone of a
document
this is a common method used by
organization to identify and group ideas
regarding a certain good service or
concept
text is mined for sentiment and
subjective information using data mining
machine learning and artificial
intelligence
tools for sentiment analysis assist
businesses in extracting information
from unstructured unorganized language
found in online sources like emails blog
posts support tickets webchats forums
and comments algorithm use rule-based
automatic or hybrid technology to
replace manual data processing while
automatic system use machine learning to
learn from their data rule-based system
execute sentiment analysis using
predetermined
lexicon based rules combining the two
method result in hybrid sentiment
analysis
sentiment analysis or opinion mining can
extract the subject opinion holder and
polarity or the degree of positivity and
negative
from the text and identify sentiment
additionally other Scopes including
document paragraph sentence and sub
sentence level can be used for sentiment
analysis after seeing what is sentiment
analysis let's discuss some types of
sentiment analysis
so the first one is fine grain sentiment
analysis by segmenting sentiment each
phrase or clause in a sentence is broken
down and examined in relationship to the
others simply said you can tell who
review a product and what topics a
person is specifically discuss in their
feedback
and the second one is emotion detection
instead of identifying positivity and
negativity emotion detection recognize
particular emotions example could
include shock rage grief frustration and
happiness and the third one is intent
based analysis in addition to identify
opinions in a text the intent based
analysis also identifies behaviors
for instance a frustrated online comment
about changing a battery can motivate
customer care to get in touch to address
that particular problem
and the last one is expect based
analysis collects the precise component
that is being mentioned either favorably
or unfavorably
for instance a client May write in a
product review that the battery life is
too short
the system will then respond that the
battery life is the main complaint and
not the product as a whole
so after seeing types of sentiment
analysis let's see some application of
sentiment analysis
organization can employ sentiment
analysis Technologies for a number of
purpose such as
determining brand popularity reputation
and awareness at a particular period or
overtime
customer service requests into
categories and the next one is
monitoring consumer response to new
improvements or products and the fourth
one is identifying the demographics or
Target Market
and the fifth one is determining the
effectiveness of a marketing effort
so here is one question for you guys I
will give you one minute for this you
can comment or you can give answers in
chat section so I can see if the answers
given by you are right or wrong so the
question is is text analytics also
refers to as text mining
I'm repeating again is text analytics
also refers to as text mining the
options are true
or false
so let us know your answer in comment
section below or in chat section
so I am starting timer of one minute
just type your answer in comment section
or in chat section
do let me know your answers so I'm
starting timer of one minute
I kindly ask that everyone take part in
this to make this live session exciting
so the question is is stacked Analytics
also refers to as text mining
35 seconds are left
do let me know your answers I kindly ask
that everyone take part in this to make
the live session exciting
you want
20 seconds left
you can comment or you can give your
answer in chat section so I can see if
the answer is given by you are right or
wrong
so time up guys
so after the allotted time has passed
Those Who provided the correct response
will receive a response and those Who
provided the incorrect response will
receive one
okay now let's move to a programming
part to perform sentiment analysis using
Amazon Customer data set
so first we will open command prompt to
write command to open Jupiter notebook
so here we will write Jupiter
notebook
press enter
okay it's Jupiter notebook
so this is the landing page of Jupiter
notebook and here you can select
new python file
it's loading
so this is how the Jupiter UI looks like
so at first we will import some bunch of
major libraries of python which will
help us in mathematical functioning so
the first one is numpy
so first I will write numpy import numpy
s NP
I is a python Library used for working
with arrays it also has functions for
working in the domain of linear algebra
and matrices it is an open source
project and you can use it freely
numpy stands for numerical python
so the next one is pandas
so I will write here
import
ance
as PD
so pandas is a software Library written
for a Python programming language for
data manipulation and Analysis in
particular it offers data structures and
operation for manipulating numerical
tables and Time series
after that we will import
nltk
so the natural language toolkit nltk is
a Python Programming environment for
creating application for statical
natural language processing NLP
for parsing classification stemming
tagging and semantic reasoning it
includes text processing libraries
so after that
we will import from nltk
Dot sentiment
dot Vader
import
sentiment
HD
analyzer
okay there is no column
yeah so the Lexicon and rule-based
sentiment analysis tool Vader Vader
means valence aware dictionary and
essential reasoning
is a customized precisely two sentiments
expressed on social media
Vader makes use of variety of a
sentiment lexicon is a collection of
lexical elements such as words
that are often classified as either
positive or negative depending on their
semantic orientation
Vader not only informs us of the
positivity and negativity scores but
also of the sentimentally of each score
so the next one is
import
re
python re module fully supports regular
expression similar to those in Pearl if
a regular expression compilation or use
error occurs the re modules raises the
expression re
two key functions that are used to
manage regular expression will be
covered so re means regular expressions
so the next one is
from
X blob
import
block
okay
so a python two or three package called
text blob is used to process textual
data it offers a straightforward API to
explore typical natural language
processing NLP
tasks like part of speech tagging noun
phrase extraction sentiment analysis
classification translation and more
so the next one is
import
s
from
word
cloud
a word cloud commonly refers to as a tag
cloud is an image of words popular words
and phrases are highlighted using Cloud
creators depending on their frequency
and importance
they give you immediate straightforward
visual insights that can Inspire more
through analysis
and the next one is import
Seaborn
as
SNS
yeah
so like an open source python Library
based on matplotlib is called C bone it
is utilized for data exploration and
data visualization
where data frames and the pandas Library
c-bond function with ease
so the next one is import
matplotlib
Dot
Pi plot
as PLT
okay yeah seems good
for Python and its numerical expression
numpy matplotlib is a cross platform
data visualization and graphical
charting package as a result it presents
a strong open source substitute for
Matlab the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications
and the next one is import
of links
c u f f l i n k s
c f
perfect
another python module called cufflink
links plotly with pandas so that charts
can be easily created on data frames in
essence it function like a plugin
so next inline
at lordlib
at plot clip
so you can enable inline plotting by
using the magic function percentage
map.lib inline which causes the plots
and graphs to appear below
the cell where you plotting commands are
entered similar to a Jupiter notebook it
offers back-end interactivity in front
ends so next we will import
what you can like we can import okay
from
oddly dot offline
import
init underscore notebook
underscore mode
comma
a plot
okay
in it underscore notebook
notebook
mode
gets through yeah
dot go
underscore
offline
so
yeah in order to display the plot inside
the notebook you need to initiate
broadly notebook mode as follows like
this
keep rest of the script as it is like it
and run the notebook cell by pressing
you can shift press enter graph will be
displayed offline inside the notebook so
basically it is used for the offline
graphing you can see okay so next we
will import
um plotly
dot subplots subplots are important
import
make underscore
subplot
okay yeah
the non-leaf nodes in this figure schema
are represented by a hierarchy of python
classes that was automatically
constructed and is found in the plotly
dot graph objects which is normally
imported as go
these class instance are refers to as
graph object the plotly defined
fundamental classes
okay
so here we will import some like
warnings you can say
like to get not error
so we will import let me give a line
space
import
warning
warnings yeah
warnings
dot filter
warning
ignore
warnings dot one
and
this like will not show
and let me add something PD
dot set
option
display Dot Max underscore
plums
key import
words from word cloud
okay there is an error let me check
what's the error
okay import
this
is what cloud
from word cloud
capital
and W is also capital
here will be capital
that's it what is small only
from word cloud
okay my bad here when from word cloud
from word cloud
import word cloud
okay
yeah seems good
so wherever we
yeah or in warnings despite the
notification the software continues to
execute to display warning messages
utilize the one
found in the warning module the python
built-in class inspection is a subclass
of the warning module which is used to
display warning messages import warnings
okay for seeing full data every single
columns like
okay there is error C bone
no module has c bond so
s will be small
got it
okay inline
this only live
something like that a line
okay any errors now
I guess no error
okay
yeah for seeing full data like every
single Columns of PD so here we are
using this PD dot set option
so we can use for it
so moving forward let's import data set
so for importing data set we will write
here
uh
like DF equals to PD Dot
read
CSV
okay
add data set name
so
you can download data set from the
description box below so no worries of
for that
let me run this
okay
so like here is PD is for pandas Library
read is used for reading the data set
from the machine and CSV is used for the
type of file which you want to read
so let's see our data
so here I will write DF Dot
yeah
so if you want to like if you want to
see top five rows of your data set you
can use head and if you want to uh like
see five rows of your data set you can
use tail insert instead of head this one
okay so this is our data set with only
five rows
if you want to see full data set you can
like from starting from 0 and ending it
four nine one four rows okay
four nine one one five rows and 12
columns
it
so let me give something like this for
the pattern visual
yeah
okay so moving ahead let's sort this
Wilson bound column to ascending order
for better analysis so I am talking
about this Wilson lower bound column
this one for it's not zero zero zero
like you know all the rows
so let me show you
so here you have to write like DF equals
to DF dot sort
values
Wilson
spelling should be correct
lower
pound
make it to ascending order
ascending
sorry descending order so
we have dot drop
named
unnamed
colon 0
okay
in place
in base equals to true
comma X is
equals to 1.
okay so let's see our data
so here you can see the data is sorted
in s
in as ascending order okay
so the provided row or column is
eliminated via the drop function the
draft method eliminates the specified
column when the column axis is specified
the drop method eliminates the
designated row when the row axis is
supplied
okay
so I hope you guys understand till here
if you have any queries or like any
queries regarding any code or there is
something quotient till here just put as
in comments our team will shortly
provide you the correct solution
okay
so moving forward let's make a function
for missing values
so
okay
so I will write here like DF
missing
values
analysis
okay it's def or function we use def
yeah
EF
columns
was to
col
for columns
for
col and DF Dot
columns
if
PF
column
is null
dot sum
greater than zero
underscore Miss
equals to
DF
underscore columns
let's go
or some
but with DOT sort values
sort underscore values
like in ascending
ascending
ascending equals to
true
like some
if you like this and this
that's fine I guess
yeah so here I will write ratio
underscore equals to
if
any underscore columns
underscore
Dot
is null
same thing
that sum
divided by
F dot shape
level
100
or sort
values
ending
equals to true
fine
okay okay
that means okay and the TF part
get sort values because you're sending
because it true fine
so
here we will write missing
underscore DF equals to PD
Dot concat
be dot round
issue
underscore comma
two
you're perfect
and access will be 1.
okay
is
equals to
that will give like
total where I will write missing values
missing
values
okay
comma ratio
thing
underscore DF equals to PD
dot data frame
it's called DF
okay so let me return
thing
TF
let's create one more function for like
check data frame so DF
check underscore data frame
okay
it equals to five
comma tail
goes to
5.
so here I will write print
okay
yeah
Dot Center
comma
again for row Sprint
it was
belly braces
dot format
but format
yeah
dot shape
you know
so for column Sprint
dot shape
paint
and types like of types of data
Dot
Center
same as like that 82
comma
end
DF
dot d types
Center
just printing this is like you will see
don't worry
just stay with me you will see the
beautiful result
yeah print
values
Trend missing values analysis uh
this is like DF
okay
then again
you get it
values
like there's many how many duplicate
values are there
so dot same for the center
see
um
again this one
print
F Dot duplicated
don't worry I will explain you like line
by line
just stay with me let me write the whole
code first print
and tiles
like I am printing multiple of things
so just be with me
Dot
enter
82
comma
F Dot
one tile
2.05
comma
one
okay
check
data frame
I'm hoping like there will be no error
yeah
so you can see now like multiple
information regarding data set is
printed
so this shape is for like how many rows
and columns present in the data set
like in types you can say like every
column type like object
N64 float64 and many more are like
present so like you can see review text
is like object and day difference is N64
and Wilson lower bound is float 64.
okay so total missing values
so total missing values are like uh
one from here and one from here like one
in from reviewer name and other is from
reviewer text
and like there is no duplicated values
in our data set
and here you can see so like after that
quantize a quantize defines a particular
part of the data set that is a quantile
determines how many values in the
distribution are above or below a
certain limit
special quantiles are the like quartile
quarter and you can say quintile that
the fifth one and the percentile which
is from 100th okay so quantile is for
like quarter
or the you can say the fifth one
so why I wrote this 83 82 and this for
this purpose for making this something
look cool
that is why
okay and
so moving forward
let's see unique values and in each
column
okay so I will add
check
does
frame
unique
f equals to PD Dot
data
frame
variable
variable
the frame
columns comma
classes
colon
data frame
I
I will write like an unique
unique
UE
or
I in
ETA frame
oh
columns
okay like and unique
you
or DF equals to
dot sort
values
passes
comma ascending
equals to
false
okay
and unique
underscore DF equals to n
unique
F dot reset
index
drop equals to
true
return
and unique
q-u-e and unique
let's go PF
check underscore class
which one DF
so let me run this first
yeah it's running you can see here
okay there is an error classes
okay
check class DF
is saying okay
this is
kses Capital C Small
let me run now yeah
so here you can see the every column
having unique values are sorted in
descending order like review tags have
like more unique values
so it come first and then reviewer name
reviewer time day difference Wilson
lower bound score average than goes to
overall
okay moving forward let's do some
categorical variable analysis for
overall yes so this will be like amazing
something amazing is going to be
happened
so
constraints equals to so I am giving
here like some
color values so
b34
d
2 2.
e b 0 0 C
okay
so you can change you can like write
whatever you want color
s
mistake oh I didn't write hashtag yeah
hashtag
three come on
it's done
0 C
9 to e b
yeah
EB 0
3
and then d
you like five right
okay so d f let's make function Okay
small d f
categorical
variable
okay
DF comma
column
okay categorical variable summary yeah
perfect
so let's make some figures so make
subplots
plots
rows
equals to
1
comma calls
equals to 2.
let me write from here like subplot
metals
equals to
count plot
like one for percentage
percentage
so specs space
equals to
a like
type
colon
colon
that's why
okay
come on
same type
column here
domain
we got dot add
and then Trace
Dot
bar
like one bar and one that pie chart okay
why
goes to DF
columns
column name
values Dot to list
for X you can click
string
yeah Str
I
or I
n
yeah
column
name
dot value
counts
X
equals to DF
Alum name
dot value
counts and then bracket
dot values
dot to list
okay
go for text
font
the size should be
14 is enough I guess
to column
UNESCO name
X position
so I'm doing for like making chart so
stay with me to see that amazing chart
okay
so show Legend
let's do take
colors
goes to constraints
so these are the like colors so I'm
giving here value like very variable
name
cost
trends
comma line
equals to
it's hashtag
DB
e
6 e c
comma
one
Okay small
okay let me just like go over through
everything is correct or not for the
column name
value counts index yeah it's fine
text equals to TF column name
dot value
counts
dot values
to to list okay
text font equals to take size then okay
14 size is enough
name equals to column name and text
position equals to auto
and show Legend false and Mark
predict like color equals to constant
okay fine line equals to color
and this yeah this okay
cool
o equals to 1
comma column equals to 1.
that seems good
like for pie chart we will make figure
dot at
so this is done for the like line chart
not line chart that bar chart and this I
am doing for the pie chart okay
Trace
go Dot
pi
f
columns
column name
is
values
goes to DF
again that column name
what value
count
s
and Dot values
text font
because you did
the size
I will prefer 18 is enough
position
like Auto is fine
so Legend
there's still no need of Legend false
m equals to column name
and your column name
colors
equals to constraints
equals to one
column equals to two like in second
okay you will see and you will get to
know don't worry
so figure dot update
layout
middle
equals to
column
0.9
and 5
come on
X
should be on centered
then y anchor
here on top
let equals to
tally
okay
I plot
figure
let me run this okay like
call equals to two
now let's see the chart of like column
overall one so we will write here
categorical
okay
variable
variable
summary
and then data frame name DF
comma overall
okay let's run this
okay it's loading here
it's loading
yeah
you can see this is this is our bar plot
and this is our pie chart
so
like this is overall
uh from overall you can say that column
so like percentage bar and that bar plot
so this is how our like pie chart and
bar chart looks like I hope you guys
understand till here if you have any
questions or any query regarding any
code
okay like any code any sort of code
so just put as in comments our team will
shortly provide you and the correct
solution
so moving forward our goal is to rank
the com comments by the sentiment
analysis so we don't get hung up on the
other details I have sorted this data
set according to certain rules before
so moving forward we want to clean our
data so let's see the sample data for
cleaning
so DF Dot
review
text
okay dot
head
let me run this so so this is our sample
data looks like so let's see like data
full data for this okay let me see for
this I guess okay
or like any particular row you can say I
can take okay
so I will write review
underscore example
example
equals to DF
Dot
review
text I will take a random two zero three
one I guess
okay so review
underscore example
so this is like at two zero three one
there is one review text okay so in that
the whole data is here like uh like so
my lovely wife bought me a Samsung
Galaxy Tab for father this is this
like to find the exact model number this
is a long you know comment
I got the this this is it works like a
charm okay
so
after seeing this data what we're gonna
do is like we will clean it from
punctuation and numbers using regex
regex what regular expression okay so
here I will write
review underscore example
example okay
equals to re dot sub
and what I will do is
okay I will explain
don't worry
small let
to capital a
to Z
okay
comma then space
comma G view underscore example
exam
but
sorry
yeah
review
so here we used re for regular
expression
okay and
so all the punctuation and numbers are
removed from particular data
so let me check first
okay this A to Z then A to Z okay fine
so like all the punctuation and numbers
are removed from particular data I will
now convert the texture to all lowercase
our machine learning algorithm
recognized words that start with the
capital letters as a different word and
we will convert them to lowercase so
thus our machine learning algorithm will
not perceive words that start with the
capital letter as a different word so
what I will do is
here I will write review
scample
sample
equals to G View
again example
dot lower
I will split
split
okay
your example
so here you can see we have converted
all the words in the lowercase letter by
using lower function and why they are
coming in new lines because we are
splitting them using split function okay
if you see only did lowercase for single
row like we did for the like two zero I
guess two zero three one okay yeah so
let's make all rows same in lowercase so
I will write here RT
equals to Lambda
X
Google
Str
f
U
text
is capital text
DF
View
text
okay equals to DF
View
yeah dot lower
F Dot
add
see all the data is in like lowercase
so I printed just top 10 rows and in
like in reviews text column all the are
in the lower case now so I hope you guys
understand till here if you have any
question or any query regarding any code
or question till here like any code just
put as in comments our team will shortly
provide you the correct solution and if
you need this full code you can comment
down below okay
so now the time has come to do sentiment
analysis because all the data is sorted
so what
will I write
let me do it little bit up
yeah
so I will write
from
yeah
from Vader
sentiment
dot Vader
element
report
sentiment
intensity
analyzer
polarity
subjectivity
is objectivity
let's do DF
if you
text
dot apply
da
PD
dot series
text
blob
and again text
Dot
like sentiment
let me run for Loop for index
comma row
in DF
G View
text
but either
items
or you
intimate
density
analyzer
or polarity
scores
row
negative I mean score
or neutral I will write score
a for positive
if
this is something crazy if negative is
greater than positive
then DF dot location
index
comma
sentiment
equals to
negative
negative
okay it's done
else if
positive is greater than negative
then in this case
d f Dot
log
index
comma
sentiment
equals to
positive
P capital
else
e f dot LSC
X
intimate
let's do
neutral
okay
so like this is for sentiment analysis
and text block
this thing like text blob this blob okay
it's blocked
a text blob will return polarity and
subjectivity polarity indicates your
mood that is whether it is positive
or negative or neutral it returns a
value between 0 and 1. the closer to one
the more positive and the closer to zero
is more negative
okay let me run this it will take
a little bit time
because the sentiment okay it is getting
error
like model Panda has no attribute series
okay which one
text sentiment
okay here what is the thing s is capital
let me run now
it is executing
like fit for a while wait
so it will take time to run because like
because of this sentiment intensity
analyzer
it take times
again sentiment intensity
analyzer okay spelling mistake again
y index index is not defined
sorry again
decks
it will run I guess because everything
seems good
negative positive and this else
still loading so I have to wait like for
like two minutes more I guess
still loading guys
I take time
sentiment intensity analyzer
so let me tell you again sentiment
analysis like this text blob
text blob will return polarity and
subjectivity
so polarity indicates your mood that is
whether it is positive it is negative or
it is it will be neutral
it returns a value between 0 and 1. the
closer to one the more
positive the closer to zero more
negative okay
I guess it's still
running
so
it will take time so let me write code
for that 20 interpretation I repeat
moving forward let's identify the 20
interpretation now we can include the
positive negative and neutral status of
the command
okay
then DF
DF
sentiment
what's equals to
positive
ought
lose
Sun
lower
ending
equals to
false
yeah
dot head
it's five
if I will run it may
if errors to me because it's still
running
still running here it's still loading
so let let me write code for that
unbalanced data problem so like we will
category that data into positive
negative and neutral
Okay so
categorical
variable
summary
TF
event
yeah
it came like this because of this
wait I will tell you
yeah so here you can see the pie chart
distributed in negative positive and
neutral emotions okay or you can say
ratings and here you can see the bar
chart also there's no better time to
train yourself in the exciting field of
machine learning if you're looking for a
course that costs everything from the
fundamentals to Advanced Techniques for
that accelerate your career in a and ml
with a comprehensive postgraduate
programming Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three caption
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Caltech
faculty and IBM experts and show Top
Notch education simple learns job assist
helps you notice by Leading companies
this program called statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns kras
tensorflow and many more skills enroll
now and then look exciting Ai and ml
opportunities the course link is
mentioned in the description box below
so what is Myers prediction
to better understand personality
characteristics in the broader
population the mbti examination was
created although there is no such thing
as Better or Worse personality prevents
the MBA examination can Aid individuals
in understanding their strength weakness
and potential difference from the others
so there are some types of mbti
indicators
there is no best or better personality
type than another
it is not a device made to search for
abnormalities and dysfunction
it is sole purpose is to assist you in
discovering mode about yourself
so there are four separate skills make
up the actual questionnaire
first is
extrovert and introversion
second is sensing and intuition
third is thinking and feeling
for this judging and perceiving
So based in these four there are some
types so there are 16 types
istj istp isfj isfp isfj like this ENFP
e and FJ entp and entg
so what is meaning of isdj
I means introversion s means sensing t
means thinking and J means judging like
this
so dear Learners if you want to upskill
your AI and machine learning skills so
give yourself a chance to Simply learn a
professional certificate program in Ai
and machine learning which comes with
completion certificate and in-depth
knowledge of AI and machine learning
check this course detail from the
description box below
so now let's move to our programming
part
so first we will open command prompt
first we will open command prompt so
here I will add Jupiter
enter
so this is landing page of Jupiter
notebook so here I will go new
python file
so this is how the python kernels look
like
so here I will change name
personality prediction
okay
so at first we will import some major
libraries of python which will help us
analyzing the data
so here I will write
lab
report Rd
or numpy as NP
import
pandas as PD
support matplotlib
Pi plot
as PLT
Port e bone
SNS
than thought
lordly
dot offline
and
okay then here import
antly
dot graph
and this
vgs object as go
I'll import
shortly
dot tools
as TLS
um
bs4
and board
beautiful
then here I will write from sqlarn dot
model
underscore selection
port
because underscore validate
okay
then again let me copy from here
till import
ant
then here I will import straight
ified
so
yes so here I will import sqlarn
dot linear
model
quote
Majestic regression
so from this
we'll write here s k learn
okay
learn
dot model underscore selection
port
key learn
Dot
port
Rait
is
okay so here I will write more from
learn dot decomposition
board
truncated
yeah
so here I will write from
SK learn
Dot
underscore
section
text import
okay
then from sqlarn
dot pipeline
both pipeline
from let's get on
dot is
multinomial
binomial NB
and
p y Dot
in it
underscore
and let's go
connected
is equal to
by the python Library used for working
with array it also has a function for
working with domain of lineal algebra
and matrices it is open source project
and you can use it freely numpy stand
for numerical python so panda is a
software Library written for the pandas
programming language
for data manipulation and Analysis in
particular it offers data section and
operation for manipulating numerical
tables and Time series
an open source python Library based on
matplotlib is called sibo it is utilized
for data exploration and data
visualization with data frames and the
pandas Library c-bond function with
these
matplotlib for Python and its numerical
extension numpy Mac proclip is a cross
platform data visualization and
graphical charting project
as a result it presents a strong open
source substitute for Matlab the api's
format plotlib allows programmers to
incorporate graphs into GI application
beautiful soup python software called
Beautiful soup is called parse HTML and
XML text
for process Pages it generates a pass
tree that can be used to extract HTML
data for web scripting
pipeline the pipeline goal is to put
together several phases that can be
cross-validated while using different
parameters
moving forward let's import some
different data that you can download
these data set from the description box
below so here I will write train
equals to PD Dot
speed underscore CSV
and then here
PTI
one
Dot
and second one is
equals to
PD Dot
see it
underscore
users
let's see
okay
then there is one more
equals to
PD Dot
p
SV
with that
forum
suggests
so here I will write mbti
question
a
K value
so I'm writing a keyword
version
comma
then okay
with
confusions
uh
Victor
[Music]
then t means thinking
f means
healing
J means
judging
the loading
so it's still loading for data set
three different data set mbti1 cfp users
and Forum messages you can download
these data set from the description box
below
or you can find this link
for the Drive Link
so these are like large data sets so it
will take time sometime
importing is data that is done so
let's use shape function to see how many
rows columns are present in trained data
set so here I will write train
dot shape
as you can see 865 8675 rows and two
columns
okay so let's move forward
so with to take a class
I repeat
so we take a look to classes so it look
like very unbalanced data set so here I
will write CNT
dot dot is equal to
train
type
Leo counts
so here I will write plot dot figure
guys
let's do
12 comma four
to
sns.bar plot
ENT
that is
dot values
to 0.8
so plot dot y label
and says
guys
types different types then font size
13
LT dot show
okay
so here you can as you can see different
types of Personality infj infj INTP intj
so what
okay
so these are number of occurrence and
the types
so moving forward let's group user ID
and messages so I will write here
was
like a visual
yeah so here I will write PS
message
uh fill any
yes
Dot
underscore join
equals to PS dot Group by
post
write
message
Dot
create
here I will write
the
call
ing
alone
that
loading
moving forward let's check accuracy with
different classifiers and models
so first we will check with extra tree
classifier
okay
so for that
right here like each extra three
classifier equals to
trees
classifier
than underscore
estimators
equals to 20
comma Max
equals to 4
oops
equals to
minus
so here I will write TF IDF
equals to
let me copy it from here
and gram
underscore range
assume
comma one
that
I'll plugs
SVD
it was
it
components
does
first two
pipeline
DF
I
right DFI
I will write here
that
foreign
and here I will write NP Dot
okay sorry
NP dot random
dot read
ing
issue
Gracie
vlog
in
Dennis type
the red scoring
stops equals to minus one
it is still loading
okay
looking
now we will print
so here I will write print
TV
run 0.4
f
and I will write
Plus
minus
0.4 f
Dot
B Dot
mean
result
position
results
copy
at all and the chip
accordingly
so now F1 is score what happened score
we will write here the F1
same
okay
then here here write NP dot mean
that is a
security here you will write test F1
underscore
micro
okay and same here
test F1
again
for the negative laws
right
love
good
here I will write negative
in this log underscore loss
if
underscore log
underscore
run it
so
wait let me make it
so here you can see the accuracy and
it's it is not that good with xrt
classifier as the data set is very
unbalanced F1 score is a better metric
than accuracy
so we will check with some alternative
models so let's try if we can find a
more accuracy with model another model
although we haven't got a lot of data
so here I will write TF
clean
next
okay
so here I will write text
to
beautiful
ml
every dot sub
scribe
r
Oma
okay then text was to
re Dot so
http
instead
button
text
here I will add train
posts
to
train
course
start
apply
today okay done so let's check accuracy
with first name base okay
oh I'm done with the code because the
code are mostly same so I just copied it
from
my documents okay so let me enter
so here you can see uh the accuracy is
far better than
this x30 classifier
okay
so we will check from one mode we will
let's check for logistic regression
so let me write the code first then I
will get back to you
I'm done with the code let's see the
accuracy of logistic regression
so let me press enter
loading you can see
yeah so accuracy is far good than the
both
so is this model overfitting could we
have a pattern model with more data
let's see the learning curve okay so
here I will write train
underscore sizes
comma train
underscore scores
command
pursue
learning underscore girl
done
post
okay
type
apples
jobs
wisdom
pudding
equals to
here I will write F1 underscore
train
sizes
goes to NP
Dot
base
1 comma one Prince
num
state
two one
and press enter
okay it is K Falls
is dispatcher
so for the learning curve it is a long
code
so let me write it for you again it's
still loading
I don't know why
so I'm done with the code so let me run
it
now let's see the Curve
so for that that's that import
or we don't
have that plot learning
I think of
chain
posts
comma train
type
then
under sizes
again underscore scores
common
test
underscore scores
comma y
0.0
figure size should be
okay
elt dot show
here as you can see
training school cross validation Score
Learning graph
so it's look like that with more data
the model gets better and better so that
is not overfitting
so what we will do
moving forward let's see kaggle user
personality let's apply our model to
whole kaggle user commands let's see
what is the most common casual user
personality
so for that I will write here BS
underscore join
then
comments
equals to
underscore join
message
Dot
apply
text
foreign
model
underscore learning dot fit
train
posts
brain
type
all equals to model
it
join
it's loading here
so moving forward what what we will do
let's create a bar graph to see kaggle
user personality okay
loading
it will show the warning something like
let me write the code for the paragraph
okay it's a long code so I will write
first then I will get back to you
so I wrote the code so let me run it and
see the personality
so here you can see e s f b is the most
used personality
so let's see the count personality count
percentage count for that I will write
here
let's go
DF
percent
to prediction underscore TF
count
and underscore DF then again count
where I will add prediction underscore
TF
description
personality
reply
yeah
X
joint
PTI
right but
just
so here you can see the percentage count
for every personality
okay with description extroversion
sensing feeling perceiving ESF
so
finally at the end we will see the
personality distribution using pie chart
okay so here I will write labels equals
to prediction
underscore TF
I will write description
yeah
sizes
equals to prediction underscore DF
extent
is equal to go
Dot
I
equals
equals to labels
comma
values
plus two sizes
a out should be
layout equals to go
dot layout
return
was true
Kegel user
personalities
data
equals to
Trace
that
oh
Dot
data
I should data
divide Dot
I plot
okay
let me turn it
let me run it
so here you can see the pie chart
so like most fifty percent is e s f p
okay then 23 is introversial institution
then
so on okay so everything is written here
and in every color
so apparently the most common user
personality is esfp this is 50.6 percent
okay
so extra version sensing feeling
perceiving but we are getting this
conclusion based on users command it is
reasonable to think that user who
participate more writing comments are
more extrovert our sample data don't
come from the kaggle users population it
is only based on the comments opencv
open source computer vision library is
an open source computer vision and
machine learning software Library it is
written in C plus plus but has a binding
for various programming languages such
as python Java Matlab opencv was
designed with a goal of providing a
common infrastructure for computer
vision applications and to accelerate
the use of machine learning perception
in commercial product opencv is widely
used in a variety of Industries
including robotics automotive and
Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide supports to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide supports to its users so now
let's see what is object detection
object detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name
so here I will write object
detection
demo
okay
so
first we will import some major Library
like opencv so for that we will write
import CV2 and the next one is import
matplotlab
dot Pi plot
as PLT
so why we are writing PLT because we
can't write again and again matplotlib
dot pipelot okay it's a long one so we
can write a short form PLT
so yeah
so let's run this
so what is opencv opencv is an open
source software library for computer
vision and machine learning the opencv
full form is open source computer vision
Library it was created to provide a
shared infrastructure for application
for computer vision and to speed up the
use of machine learning perception in
consumer products opencv has a PSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and opencv is one of them
second one is matte broadly matplotlib
is a easy to use and an amazing
visualized library in Python it is built
on numpy array and designed to work with
broader sci-fi stack and consists of
several plots like line bar scatter
histogram and many others okay so moving
forward we will import our file okay so
here I will write config
file
equals to this is our file name SSD
underscore
mobilenet
to V3
large
Coco
22
14 dot BB
okay
so you can find this file on the
description box below
chosen
model
question
I'll explain you
every single thing
inference
graph
dot PB okay
so let me run it first
mobilenet as a name applied the mobile
net model is designed to use in mobile
application and its tensorflow first
mobile computer vision model mobile net
used depth wise separable convolutions
it significantly reduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the heads this result in the
lightweight of the deep neural network
so mobilenet is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the state of the
art of neural network its versatility as
a multi-purpose scene variations of best
to train a computer vision model and
Benchmark its performance okay so what
is Coco the common object and context is
one of the most popular large scale
labeled images data available for public
use it represents a handful of objects
we encounter on a daily basis and
contains Imaging notations in 80
categories I will show you the
categories I have with over 1.5 million
object instances okay so modern day AI
driven solution are still not capable of
producing absolute accuracy and result
which comes down to the fact that Coco
dataset is a major Benchmark for CV to
train test and polish refine models for
faster scaling of The annotation
Pipeline on the top of that the Coco
data set is a supplement to transfer
learning where the data used for one
model serves as a starting point for the
another so what is frozen Insurance
graph like freezing is the process to
identify and save all the required
graphs like weights and many others in a
single file that you can usually use a
typical tensorflow model contains four
file and this com contains a complete
graph okay so forward let's create one
model here I will add modern
DNN
model
chosen
and then
config file
so here I am giving the parameters two
parameters like frozen model and config
file
score here yeah you run it first
okay there is error
okay so either is cv2.dnn detection
model return as result
with an exception set
the question comes what is the meaning
of detection model or DNN detection
model for this class represent the high
level API for object detection networks
detection models allows to set
parameters for pre-processing input
image detection model creates net from
file and with trained weights and config
sets in processing input runs forward
pass and return the result detections
okay moving forward let's set the class
labels
class
labels
file name
txt
I will put this file on the description
box below you can download from there
open
file name
pass labels
the strip
so here I created one array of name
class labels so this is the file name
what I am doing I am putting this label
file into this class label okay so here
if I will print
class labels
so these are the 80
categories in the Coco data set
okay this person bicycle car motorbike
airplane bus train these all are the 80
categories
I will put this file label.txt in the
description box below you can download
from them
okay fine
so let's print the length of the Coco
data set or you can see class labels
this is 80 as you can see I have already
told you
the length will be 80.
so here let's
set this some model input size scaling
mean and all so I will write here model
Dot
set
put
eyes
T20 comma 20.
set
input
scale
1.0
slash 127
0.5
okay I will explain you don't worry
model dot set
import
me
127.5
comma 127.5
comma 127.5
okay and then model
dot set
to web
B
will be
what is set input size
okay
so set input size is a size of new frame
the shape of the new blob Less Than Zero
okay so this is the size of the new
frame the second one is set input scale
so set input scale is a scale factor of
the value for the frame
or you can say the perimeter will be the
multiplier of the frame values or you
can say multiplier for the frame values
okay so at input mean so it set the mean
value for the frame the frame in which
the photo will come the video will come
on my webcam will come so it set the
mean value for the frame or the four
parameters mini scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swipe I'll be flagged
which indicates the swipe first and the
last channels so moving forward we will
put one image
M read
y dot jpg
Dot
I am sure
so this is the size of 320 by 320 okay
so first thing is
you can download this the random picture
from the Google I do from Google itself
so now what we will do
we will set the class index
the confidence value
box is the boundary box which I will
create for that particular person cycle
motorbike and the car
okay
because to model
that
the confidence threshold
threshold is used for
if my model will confirm it's the
particular image which is detecting is
correct it will print the name
okay
so let me print
and
pass
class index is coming one two three four
okay so one means person
two means bicycle
three means car and four means motorbike
this is the class index index for
particular level what I will do I will
print the boxes
font
okay
equals to 3
and the font equals to CV2 Dot
font Hershey
again
for
class
index and then
confidence and the box is
and
x dot pattern
confidence
pattern
box the boundary box
okay
and I will write here CV2 dot rectangle
make the rectangle
set the image
and obsess
I've
comma 0 comma zero
this is the color of the box and this
will be the thickness
okay
then I will write CV2 dot put
text
image then
first labels
I will write class
index minus one
because always index start with 0.
that's fine and the boxes
zero
and
comma boxes
and
body
okay
I want
comma
scale
pursue font
scale
okay color
question
this will be the text column
0 comma 255
comma 0
and the thickness
two three
let me run it
ah no Adder okay now PLT dot I am sure
then CV2 dot CBT
color
then if
my CV t two
dot color
me
hello and BGR
to prg
that is why we wrote
swap RB equals to true because every
time we will convert BGR to brg
sorry
GB
RGB so we don't have to write again and
again it will convert all the files into
RGB okay run it
okay as you can see the motorbike is
coming
bicycle is coming the person is coming
the car will come the car is coming okay
so it's detecting the right
for the image now we will do this for
the video and for the webcam
we are done with this image one and then
now I will write here
okay
so this is will do for the video
for the video I will write here cap
equals to capture
you can write any name so CV2 Dot
video
capture
so you can take any random video I do
this pixels
George
you can comment down
share the link
for
F dot is open
so here I will add cap
equals to
if you do
sorry CV2
dot video
capture
0.
and if not
cap
dot is open
then
and raise
input output
error
and
open the video
and open the video
here everything will be the same font
scale
equals to 3
okay I want
equals to CV2 Dot
font
foreign
okay
so here I will write y true
from a frame
equals to cap dot sheet
this for the reading of the file
the same I will write class index
comma confidence
from a boundary box
question model
dot detect
game
and the confidence
threshold
0.55
okay everything is the same
we did before so here I will print
class
index
okay
so here I will write if
and
the class index
does not equals to zero
and what to perform is
here I have to add 4
class index
comma confidence
got my boxes
and zip
us
x dot
flatten
flatten is the layers
okay
go on for this
flatten
e-box
and if
class
index is greater than equals to 80.
then
what to do
here
okay the same thing I have to write here
so here I will write CV2 Dot
I am sure
this will be the return in the frame
object detection
by simply learn
and frame
so if CV2 Dot
weight key
to
and 0
f of x
equals to
Ord
Q okay
then
here I will write break
will you break when
and get into two the weight key will be
2.
okay I will tell you what is the weight
key
so here I will write cap dot release
and CV2 Dot
destroy
or Windows
okay
so now let me run
it's here there will be another okay
CB Python programming language modules
we run it again
activate keys
okay
so video is here
the video is here as you can see see
bicycle
the person the person the bus car
traffic light the person person so it
our object detection for the video is
coming right
okay person
okay person traffic light bus
this is how you can do for the video
okay
so now let's we will do for the webcam
live
so this is for the video so
if
we want to do for the webcam with
okay so we need to just change
one one thing only we have to change
instead of giving the file we have to
write one here
okay the rest will be the same
got it so I have to just shut down my
webcam
so
let me shut down the webcam and get back
to you
thank you
so as you can see
this is a 320 by 320 box so
so this is coming right okay so I if I
will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that costs everything from the
fundamentals to Advanced Techniques for
that accelerate your career in Ai and ml
with a comprehensive postgraduate
programming Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three caption
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Caltech
faculty and IBM experts and show Top
Notch education simple learns job assist
helps you notice by Leading companies
this program calls statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns kras
tensorflow and many more skills enroll
now and then look exciting Ai and ml
opportunities the course link is
mentioned in the description box below
hello everyone welcome to this session
I'm Mohan from Simply learn and today
we'll talk about interview questions
now this
will probably help you when you're
attending interviews or machine learning
positions and the attempt here is to
probably consolidate 30 most commonly
asked questions and and to help you in
answering these questions we tried our
best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine but what is
important is you would probably be
better of emphasizing that there are
actually two main types of initial
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you are trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact
so that is what is supervised learning
when you have existing labeled data
which you then use to train your model
that is known as supervised learning and
unsupervised learning is when you don't
have this labeled data so you have data
it is not labeled so the system has to
figure out a way to do some analysis on
this okay so that is unsupervised
learning and you can then add a few
things like what are the ways of
performing a supervised learning and
unsupervised learning what are some of
the techniques so supervised learning we
we perform or we do
regression and classification and
unsupervised learning video clustering
and clustering can be of different types
similarly regression can be of different
types but you don't have to probably
elaborate so much if they are asking for
just the different types you can just
mention these and just at a very high
level but if they want you to elaborate
give examples then of course I think
there is a different question
then the third so we have supervised and
we have unsupervised and then
reinforcement you need to provide a
little bit of information around as well
because it is sometimes a little
difficult to come up with a good
definition for reinforcement
so you may have to little bit elaborate
on how reinforcement learning
right so reinforcement learning works in
in such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it
cheap and every time it is moving in the
direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal then it is rewarded and every
time it is going in a direction where it
is away from the goal then it is
punished so that is the way you can
little bit explain and this is used
primarily or very very impactful or
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphaco used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
co-world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting maybe a
little different
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes
explain this now let's say you're
teaching a child about recognizing some
fruits or something like that
and you're teaching this child about
recognizing let's say three fruits
apples oranges and pineapples okay so
this is a small child and for the first
time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples okay and you take this basket
to this child and there may be let's say
hundreds of these fruits so you take
this basket to this child and keep
showing each of this root and then first
time obviously the child will not know
what it is so you show an apple and you
say hey this is Apple then you show
maybe an orange and say this is orange
and so on and then again you keep
repeating that right so till that basket
is over this is basically how training
work in machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then the process
what has happened the child has pretty
much memorized these so even before you
finish that basket right by the time you
are halfway through the child has
learned about recognizing the Apple
orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100 accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple
that means it is kind of memorized this
this now let's say you bring another
basket of fruits and it will have a mix
of maybe apples which were already there
in the previous set but it will also
have in addition to Apple it will
probably have a banana or maybe another
fruit like a jackfruit right so this is
an equivalent of your test data set
which the child has not seen before some
parts of it it probably has seen like
the apples it has seen but this banana
and Jackfruit it has not seen so then
what will happen in the first round
which is an equivalent of your training
data set towards the end it has 100 it
was telling you what the fruits are
right Apple was accurately recognized
orange or I was accurately recognized
and pineapples were accurately
recognized right so that is like a
hundred percent accuracy but now when
you get another a fresh set which were
not a part of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the Jackfruit or the banana
will not be recognized by the child
right so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach 100
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much hundred percent whereas when
you use testing there will be a huge
error which means the loss will be
pretty high and therefore the accuracy
will be also low okay this is known as
overfitting this is basically a process
where training is done training
processes it goes very well almost
reaching 100 accuracy but while testing
it really drops down now how can you
avoid it so that is the extension of
this question there are multiple ways of
avoiding overfitting there are
techniques like what you call
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
drop out in case of neural networks and
a few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be sufficient
so so there will be some questions where
the interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stop you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set machine learning model and how
is the split done the question can be
can be so in machine learning when we
are trying to train the model so we have
a three-step process we train the model
and then we test the model and then once
we are satisfied with the test only then
we deploy the model so what happens in
the train and test is that you remember
the labeled data so let's say you have
thousand records with labeling
information now one way of doing it is
you use all the Thousand records for
training and then maybe right which
means that you have exposed all this
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
obviously it has become familiar with
the entire data so when you are taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may ask this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times till the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that your that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like a realistic situation
because when the model is deployed that
is what will happen it will receive some
new data not the data that it has
already seen right so this is a
realistic test so you put some new data
so this data which you have set aside is
for the model it is new and if it is
able to accurately predict the values
that means your training has worked okay
the model got trained properly but let's
say while you're testing this with this
test data you're getting lot of errors
that means you need to probably either
change your model or retrain with more
data and take
now coming back to the question of how
do you split this what should be the
ratio there is no fixed number again
this is like individual preferences some
people split it into 50 50 test and 50
training Some people prefer to have a
larger amount for training and a smaller
amount for test so they can go by either
60 40 or 70 30 or some people even go
with some odd numbers like 65 35 or
63.33 and 33 which is like one third and
two thirds so there is no fixed rule
that it has to be something that you has
to be this you can go by your individual
preferences all right then you may have
questions around data handling data
manipulation or
data management preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there are a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were this many
rows where the data was missing that
would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling this data or corrupt data
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like a 10 percent
okay now between these how do you so
let's assume that this is not a mission
critical situation and in order to fix
this 10 percent of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50 of your data is like that
because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10 percent
of the records column one is missing and
another 10 percent column two is missing
another 10 percent column three is
missing and so on and so forth so it
adds up to maybe half of your data set
so you cannot completely remove half of
your data set then the whole purpose is
lost
how do you handle
then you're with ways of filling up this
data with some meaningful value right
that is one way of handling so when we
say meaningful value what is that
meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for a task right there may be situations
where if the missing values instead of
putting mean may be a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this
okay so then the next question can be
how can you choose a classifier based on
a training set data size so again this
is one of those questions where you
probably do not have like a one size
fits-all answer first of all you may not
let's say decide your classifier based
on the training set size maybe not the
best way to decide the type of the
classic
and even if you have to there are
probably some thumb rules which we can
use but then again every time so in my
opinion the best way to respond to this
question is you need to try out few
classifiers irrespective of the size of
the data and you need to then decide on
your particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just buy if somebody defines a
problem to you and somebody even if they
show the data to you or tell you what is
the data or even the size of the data I
don't think there is a way to really say
that yes this is the classifier that
will work here no that's not the right
way so you need to still you know test
it out get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have questions around confusion Matrix
so the question can be explained
confusion Matrix right so confusion
Matrix I think the best way to explain
it is by taking an example and drawing
like a small diagram otherwise it can
really become tricky so my suggestion is
to take a piece of pen and paper and
explain it by drawing a small Matrix and
confusion Matrix is about to find out
this is used especially in
classification learning process and when
you get the results when the our model
predicts the results you compare it with
the actual value and try to find out
what is the accuracy okay so in this
case let's say this is an example of a
confusion Matrix and it is a binary
Matrix so you have the actual values
which is the labeled data right and
which is so you have how many yes and
how many you know so you have that
information and you have the predicted
values how many yes and how many you
know right so the total actual values
the total yes is 12 plus 130 and they
are shown here and the actual value
those are 9 plus 3 12 okay so that is
what this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nodes okay so this is
the way to look at this confusion Matrix
okay and uh out of this what is the
meaning converting so there are two or
three things that needs to be explained
out right the first thing is for a model
to be accurate the values across the
diagonal should be high like in this
case right that is one number two the
total sum of these values is equal to
the total observations in the test data
set so in this case for example you have
12 plus 3 15 plus 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix the numbers is equal to
the size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 12 21 and divide
it by the total so in this case what
will it be let me take a pen so your
your diagonal values is equal to if I
say t is equal to 12 plus 9 so that is
21 right and the total data set is equal
to right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85 percent right
so this is 85 percent so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate these numbers on the Fly Right
so a couple of hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and then
otherwise it will be pretty difficult to
so we use the same example of the
confusion Matrix
and we can explain that so A confusion
Matrix looks somewhat like this and when
we just
which looks more like this and we
continue with the previous example where
this is the actual value this is the
predicted value and in the actual value
we have 12 plus 1 13 yeses and 3 plus 9
12 nose and the predicted values there
are 12 plus 3 15 yeses and 1 plus 9 10
loss okay now this particular case which
is the false positive what is a false
positive first of all the second word
which is positive okay is referring to
the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay you consider this row so this
is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay
and then the first term is for the
actual so true positive so true in case
of actual is yes right so true positive
is this one okay and then in case of
actual the negative now we are talking
about let's say true negative true
negative negative is this one and the
true comes from here so this is true
negative right 9 is true negative the
actual value is also negative and the
predicted value is also negative okay so
that is the way you need to explain this
the terms false positive false negative
and true positive true negative then uh
you might have a question like what are
the steps involved in the machine
learning process or what are the three
steps in the process of developing a
machine learning model right so it is
around the methodology that is applied
so basically the way you can probably
answer in your own words but the way the
model development of the machine
first of all you try to understand the
problem and try to figure out whether it
is a classification problem or a
regression problem based on that you
select a few algorithms and then you
start the process of training these
models
so you can either do that or you can
after due diligence you can probably
decide that there is one particular
algorithm which is more suitable usually
it happens through trial and error
process but at some point you will
decide that okay this is the model we
are going to use okay so in that case we
have the model algorithm and the model
decided and then you need to do the
process of training the model and
testing the model and this is where if
it is supervised learning you split your
data the label data into training data
set and test data set and you use the
training data set to train your model
and then you use the test data set to
check the accuracy whether it is working
fine or not so you test the model before
you actually put it into production
right so once you test the model you're
satisfied it's working fine then you go
to the next level which is putting it
for production and then in production
obviously new data will come and
the inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values is regression
you know so this can be an iterative
process so it is not a straightforward
process where you do the training
through the testing and then you move it
to production now so during the training
and test process there may be a
situation where because of either
overfitting or not things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be a an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so let net this is a continue us
process of tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so it can be as
simple as what is a deep click so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then obviously the
question would be then what is the
difference right so deep learning you
need to mention there are two key parts
that interviewer will be looking for
when you are defining deep learning so
first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
scope and deep learning is one one part
of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
future engineering is done manually what
do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and this data has several let's say if
it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict a
height weight and so on and so forth so
these are all features of human beings
let's say we have census data and we
have all this so those are the features
now that may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many teachers and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically
this is a explanation these are two key
things probably will add value to your
response all right so the next question
is what is the difference between or
what are the differences between machine
learning and deep learning so here this
is a quick comparison table between
machine learning and deep learning and
in machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on low
end system so you don't need a large
machine and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural networks
so here in deep learning we use neural
line so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines because it needs a lot
of computing power and the Machine
learning features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end-to-end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of then you
might get a question around the uses of
machine learning or some real life
applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
or actually supervised machine learning
it could be a very specific question
around supervised decision learning so
this is like give examples of supervised
machine learning use of supervised
machine learning in modern business so
that could be the next so there are
quite a few examples or quite a few use
cases if you will for supervised
definition learning the very common one
is email spam detection so you want to
train your application or your system to
detect between spam and non-spam so this
is a very common business application of
a supervised machine learning so how
does this work the way it works is that
you obviously have historical data above
of your emails and they are categorized
as spam and not spam so that is what is
the labeled information and then you
feed this information or the all these
emails as an input to your model right
and the model will then get trained to
detect which of the females are to
detect which is Spam and which is not
spam so that is the training process and
this is supervised machine learning
because you have labeled data you
already have emails which are tagged as
spam or not and then you use that to
train your model right so this is one
example now there are a few industry
specific applications for supervised
machine learning one of the very common
ones is in healthcare Diagnostics in
healthcare Diagnostics you have these
images and you want to train models to
detect whether from a particular image
whether it can find out if the person is
sick or not whether a person has cancer
or not right so this is a very good
example of supervised machine learning
here the way it works is that existing
images it could be x-ray images it will
be MRI or any of these images are
available and they are saying that okay
this x-ray image is deflective of the
person has an illness or it could be
cancer whichever illness right so it is
stacked as defective or clear or good
image and effectively something like
that so we come up with a binary or it
could be multi-class as well saying that
this is defective to 10 percent this is
25 and so on but let's keep it simple
you can give an example of just a binary
classification that would be good enough
so you can say that in healthcare
Diagnostics using which we need to
detect whether a person is ill or
whether
cancer or not so here the way it works
is you feed labeled images and you allow
the model to learn from that so that
when New Image is fed it will be able to
predict whether this person is having
that illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
uh unsupervised and so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the person suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
art of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labor now
this is a big problem in many Industries
or in many under many situations getting
the label data is not that easy because
there's a lot of effort in labeling this
data let's take an example of diagnostic
images we just let's say take extra
images now there are actually millions
of x-ray images available all over the
world but the problem is they are not
labeled so their images are there but
whether it is effective or whether it is
good and information is not available
along with it right in a form that it
can be used by a machine which means
that somebody has to take a look at
these images and usually it should be
like a doctor and then say that okay yes
this image is clean and this image is
cancerous and so on and so forth now
that is a huge effort by itself so this
is where semi-supervised learning comes
into play so what happens is there is a
large amount of data maybe a part of it
is labeled then we try some techniques
to label the remaining part of the data
so that we get completely labeled data
and then we train our model so I know
this is a little long-winding
explanation but unfortunately there is
no quick and easy definition for
semi-supervised machine learning this is
the only way probably to explain this
concept
we may have another question as what are
unsupervised machine learning technique
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together there are
different ways of finding similar
objects so their characteristics can be
and if they have in most of the
characteristics if they are similar then
they can be put together
analysis frustrating clustering then
Association you can I think the best way
to explain Association is with an
example in case of Association you try
to find out how the items are linked to
each other for example if somebody
bought maybe a laptop or the person has
also purchased a mouse so this is more
in an e-commerce scenario for example so
you can give this as an example so
people who are buying and laptops are
also buying the mouse so that means
there is an association between laptops
and
people who are buying bread are also
buying butter so that is the association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and
unsevised and in case of supervised and
again here probably the key word that
the person may be wanting to hear is
labeled data now very often people say
we have historical data and if we run it
it is supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for suppose
so it is it's very key to understand
that we put in that keyword labeled so
when we have labeled data for training
our model then we can use supervised
learning and if we do not have label
data then we use unsupervised learning
and there are different algorithms
available to perform both of these
so there can be another question a
little bit more theoretical and
conceptual in nature this is about
inductive machine learning and
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
ready they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
training inductive training is induced
by somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it what can be an
example so we can probably tell the
person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw a
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or you throw
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
probably that explanation
sufficient the next question is are knnn
and K means clustering similar to one
another or are they same right because
the letter K is kind of common between
them okay so let us take a little while
to understand what these two are one is
KNN another is K means a KNN stands for
K nearest neighbors and K means of
course is the clustering mechanism now
these two are completely different
except for the letter K being common
between them KN is completely different
K means clustering is
KNN is a classification process and
therefore it comes under supervised
learning whereas k-means clustering is
actually a unsupervised okay when you
have K and N when you want to implement
k n n which is basically K nearest
neighbors the value of K is a number so
you can say k is equal to 3 you want to
implement k n n with K is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular
but it is possible that they may be from
two or three different classes okay so
let's say they are from two classes and
then they are from two classes now
usually you take a odd number you assign
odd number two so if there are three of
them and two of them belong to one class
and then one belongs to another class so
this new object is assigned to the class
to which the two of them belong now the
value of K is sometimes tricky whether
should you use three should you use five
should you use seven it can be tricky
because the ultimate classification can
also vary so it's possible that if
you're taking K as3 the object is
probably in one particular class but if
you take K is equal to 5 maybe the
object will belong to a different class
because when you're taking three of them
probably two of them belong to a class
one and one belong to class two whereas
when you take five of them it is
possible that only two of them belong to
class one and the three of them belong
to Class 2 so which means that this
object will belong to class 2 right so
you see that so it is the class
allocation can vary depending on the
value of K now K means on the other hand
is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to some of their features
okay and but the similarity of course is
the the letter K and in case of K means
also we specify its value and it could
be three or five or seven there is no
technical limit as such but it can be
any number of clusters that you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects there
is a similarity to that extent that K is
a number in both the cases but actually
these two are completely different
processes
we have what is known as naive based
classifier and people often get confused
thinking that naive base is the name of
the person who found this classifier or
who developed this classifier which is
not 100 True base is the name of the
person bais is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of this particular
classifier an ibase classifier is a
probability based classifier and it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but even if we go with this assumption
this whole algorithm works very well
even with this assumption and that is
the good side of it but the term comes
from there so that so that is the
explanation that you can then there can
be question around reinforcement
learning it can be paraphrased in
multiple ways one could be can you
explain how a system can play a game of
chess using reinforcement learning
the best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either it is related to game then the
goal could be that you have to score
very high score
or it could be that your number of lives
should be as high as possible don't lose
life so this could be some of them a
more advanced examples could be for
driving the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
that
well it is rewarded and every time it
takes a step which goes against that
core right contrary or in the reverse
Direction it is penalized okay so it is
like a carrot and asked
now how do you use this to create a game
of chess so to create a system to play a
game of chess now the way this works is
and this could probably go back to this
alphago example where alphaco defeated a
human Champion so the way it works is in
reinforcement learning the system is
allowed for example if in this case
we're talking about Chess so we allow
the system to first of all watch playing
a game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being being played the game because this
is reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent it works on
that so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you are playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to play chess and and learn
on its own over a period of repetition
so I think you can probably explain it
to this much to this extent and I
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SPM you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is
okay so we have to try out a bunch of
algorithms see which one gives us the
best performance best accuracy accuracy
and then decide to go with that
particular algorithm so in machine
learning a lot of it happens through
trial and error there is no real
possibility that anybody can just by
looking at the problem or understanding
the problem tell you that okay in this
particular situation this is exactly the
algorithm that you should
then the questions may be around
application of machine learning and this
question is specifically around how
Amazon is able to recommend other things
to buy so this is around recommendation
engine how does it work how does the
recommendation engine work so this is
basically the question is all about so
the recommendation engine again Works
based on various inputs that are
provided obviously something like uh you
know Amazon a website or e-commerce site
like Amazon collects a lot of data
around the customer Behavior who is
purchasing what and if somebody is
buying a particular thing they are also
buying something else so this kind of
Association right so this is the
unsupervised learning we talked about
they use this to associate and Link or
relate items and that is one part of it
so they kind of build association
between items saying that somebody
buying this is also buying this that is
one part of it then they also profile
the users right based on their age their
gender their geographic location they
will do some profiling and then when
somebody is logging in and when somebody
is shopping kind of the mapping of these
two things are done they try to identify
obviously if you have logged in then
they know who you are and and your
information is available like for
example your age maybe your agenda and
where you're located what you purchased
earlier right so all this is taken and
the recommendation engine basically uses
all this information and comes up with
recommendations for a particular user so
that is how the recommendation engine
work all right then the question can be
something very basic like when will you
go for classification versus regression
right when do you do classification
instead of regression or when will you
use classification instead of regression
now yes so so this is basically going
back to the understanding of the basics
of classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values another is the
continuous values so for regression if
we are trying to find some continuous
values you use regression whereas if you
are trying to find some discrete values
you use classification and then you need
to illustrate what are some of the
examples so classification is like let's
say there are images and you need to put
them into classes like cat dog elephant
tiger something like that so that is the
classification problem or it can be that
is a multi-class classification no
problem it could be binary
classification problem like for example
whether a customer will buy or he will
not buy that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again a combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not it's a classification
problem it's a binary classification
right whether it's going to rain or not
rain however you also have to predict
what is going to be the temperature
tomorrow right now temperature is a
continuous value you can't answer the
temperature in a yes or no kind of a
response right so what will be the
temperature tomorrow so you need to give
a number which can be like 20 degrees 30
degrees or whatever right so that is
where you use regression one more
example is stock price prediction so
that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continuous so
the next question is more about a little
bit of a design related question to
understand your Concepts
and so it is how will you design a spam
filter so how do you basically design or
Developers so I think the main thing
here is he's looking at probably
understanding your Concept in terms of
what is the algorithm you will use or
what is your understanding about
difference between classification and
regression
regression and things like that right
and the process of course the
methodology and the process so the best
way to go about responding to this is we
say that okay this is a classification
problem because we want to find out
whether an email is a spam or not spam
so that we can apply the filter
accordingly so first thing is to
identify what type of a problem it is so
we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector machines for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you use your training data set
to train your model that or your
algorithm that you have used or rather
the model actually so and you actually
will have three models let's say you are
trying to test out three algorithms so
you will obviously have three models so
you need to try all three models and
test them out as well see which one
gives the best accuracy and then you
decide that you will go with that model
okay so training and test will be done
and then you zero in on one particular
model and then you say okay this is the
model will you use we will use and then
go ahead and Implement that or put that
in production so that is the way you
design a spam the next question is about
random Forest what is random form so
this is a very straightforward question
however the response you need to be
again a little careful while we all know
what is random Forest explaining this
can sometimes be tricky so one thing is
random Forest is kind of in one way it
is an extension of decision trees
because it is basically nothing but you
have multiple decision trees and trees
will basically we will use for doing if
it is classification mostly it is
classification you will use the the
trees for classification and then you
use voting for finding the final class
so that is the underlyings but how will
you explain this how will you respond to
this so first thing obviously we will
say that random Forest is one of the
algorithms and the more important thing
that you need to probably the
interviewer is is waiting to hear is
Ensemble learner right so this is one
type of Ensemble learner what is
Ensemble learner Ensemble learner is
like a combination of algorithms so it
is a learner which consists of more than
one algorithm or more than one or maybe
models okay so in case of random Forest
the algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a random Forest is an ensemble
there are other types of Ensemble
Learners where we have like reuse
different algorithms itself so you have
one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to hear this
word Ensemble Learners and so then you
go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees and each
observation you pass through this forest
and each observation let's say it is a
classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and ten of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way random Forest works for
classification is a regression problem
it's somewhat similar but the only thing
is instead of what what we will do is
sorry in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
hundred trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the 100 degrees right so
this is also a way of reducing the error
so maybe if you have only one tree and
that one tree makes a header it is
basically hundred percent wrong or 100
right right but if you have on the other
hand if you have a bunch of trees you
are basically medicating that reducing
that error okay so that is the way
random Forest works so the next question
is considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to outright say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a
one-size-fits all so we need to first of
all you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithm
right so for example if it's a class
session you cannot use linear regression
algorithm or if it is a regression
problem you cannot use svm or maybe now
you can use svm but maybe a logistic
regression right so to that extent you
can probably shorten the list but still
you will not be able to 100 decide on
saying that this is the exact algorithm
that I am going to use so the way to go
about is you choose a few algorithms
based on what the problem is you try out
your data you train some models of these
algorithms check which one gives you the
lowest error or the highest accuracy and
based on that you choose that particular
algorithm
all right then they can be questions
around bias and variants so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual value so that
is the bias okay and whereas they are
all all the values are probably they are
far off but they are very near to each
other though the predicted values are
close to each other right while they are
far off from the actual value but they
are close to each other you see the
difference so that is bias and then the
other part is your variance now variance
is when the predicted values are all
over the place right so the variance is
high that means it may be close to the
Target but it is kind of very scattered
so the points the predicted values are
not close to each other right in case of
bias the predicted values are close to
each other but they are not close to the
Target but here they may be close to the
Target but they may not be close to each
other so they are a little bit more
scattered so that is what in case of a
variance okay then the next question is
about again related to bias and variance
what is the trade-off between bias and
variance yes I think this is a
interesting question because these two
are heading in different directions so
for example if you try to minimize the
bias variance will keep going high and
if you try to minimize the variance bias
will keep going high and there is no way
you can minimize both of them so you
need to have a trade-off saying that
okay this is the level at which I I will
have my bias and this is the level at
which I will have variance so the
trade-off is that pretty much attack you
you decide what is the level you will
tolerate for your bias and what is the
level you will tolerate for variance and
a combination of these two in such a way
that your final results are not way off
and having a trade-off will ensure that
the results are consistent right so that
is basically the output is consistent
and which means that they are close to
each other and they are also accurate
right that means they are as close to
the Target as possible right so if
either of these is high then one of them
will go off the track define precision
and Recall now again here I think it
would be best to draw a diagram and take
up in the confusion Matrix and it is
very simple the definition is like a
formula your Precision is true positive
by group positive plus false positive
and your recall is true positive by true
positive plus false negative okay so
that's you can just show it in a
mathematical way that's pretty much you
know
that's the easiest way to define so the
next question can be about decision tree
what is decision tree pruning and why is
it so basically decision trees are
really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and conditions can become very
it and this can also lead to overfitting
which is basically that during training
you will get 100 accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose or the
reason for doing decision tree pruning
is to reduce overfitting or to cut down
on our fitting and what is decision trip
rolling it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing those internal
nodes all right so the next question can
be what is logistic regression uh so
basically logistic regression is one of
the techniques used for performing
classification especially binary
classification now there is something
special about logistic regression and
there are a couple of things you need to
be careful about first of all the name
is a little confusing it is called
logistic regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the to the URL if it's
really you know quiet and they can also
ask this like a request
request so that is one part second thing
is the term logistic has nothing to do
with the usual Logistics that we talk
about but it is derived from the log so
that the mathematical derivation
involves log and therefore the name
logic
logic so what is logistic regression and
how is it used so logistic regression is
used for binary classification and the
output of a logistic regression is
either a zero or a one and it varies so
it's basically it calculates a
probability between 0 and 1 and we can
set a threshold that can vary typically
it is 0.5 so any value above 0.5 is
considered as 1 and if the probability
is below 0.5 it is considered as zero so
that is the way we calculate the
probability of the system calculates the
probability and based on the threshold
it sets a value of 0 or 1 which is like
a binary classification zero or one okay
then we have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is the K nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the K is a
number it is an an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
want to find out which class this
belongs right so you go about as the
name suggests it you go about finding
the nearest neighbors right the points
which are closest to this and how many
of them you will find that is what is
defined by K now let's say our initial
value of K was 5 okay so you will find
find the K the five nearest data points
so in this case as it is Illustrated
these are the five nearest data points
but then all five do not belong to the
same class or cluster so there are one
belonging to this cluster one the second
one belonging to this cluster two three
of them belonging to this third cluster
okay so how do you decide that's exactly
the reason we should as much as possible
try to assign an odd number so that it
becomes easier to assign this so in this
case you see that the majority actually
if there are multiple classes then you
go with the majority so since three of
these items belong to this class we
assign which is basically the in in this
case the green or the tennis or the
third cluster as I was talking
right so we assign it to this third
class so in this case it is that's how
it is decided okay so K nearest neighbor
so first thing is to identify the number
of neighbors that are mentioned as K so
in this case it is K is equal to 5 so we
find the five nearest points and then
find out out of these five which class
has the maximum number in that kit and
and then the new data point is assigned
to that class okay so that's pretty much
how K nearest neighbors work with that
we've reached the end of this session if
you have any questions please feel free
to comment and we'll have it answered
for you as soon as possible until next
time thank you for watching stay safe
keep learning and get help staying ahead
in your career requires continuous
learning and upskilling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs in Cutting
Edge domains including data science
cloud computing cyber security AI
machine learning or digital marketing
designed in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click