foreign
tastic video on the machine learning
expert course by simply learn in this
video we will cover your doubts
regarding the best roles salaries
projects skills and certification you
can choose to enter or Advance your
career in machine learning but before we
begin if you enjoy watching these videos
and find them interesting then subscribe
to our YouTube channel as we bring you
the best videos daily also hit the Bell
icon to never miss any update from
Simply none so let's get started in this
video first you will learn about what is
machine learning and types of machine
learning following that you will come
across the fundamentals of machine
learning like machine learning roadmap
skills required to become a machine
learning engineer machine learning
projects and machine learning
engineering salary then you will be
introduced to AI versus ml versus deep
learning after that we will see some
significant applications of machine
learning after covering all these we
will see some mathematics for machine
learning like stat and probability after
that you will learn about machine
learning algorithm and how it works you
will learn then about NLP natural
language processing after covering
fundamental and core topics we will come
into hands-on experience where you will
learn about build amazing projects and
then you can add them to your resume in
the end you will see the top machine
learning interview question that will
help you to build your career in machine
learning by the end of this video I can
show that all your machine learning
related questions and doubts will have
been cleared also accelerate your career
in Ai and ml with our conference
postgraduate program in Ai and machine
learning boost your career with this Ai
and ml course delivered in collaboration
with Purdue University and IBM learn
in-demand skills such as machine
learning deep learning and LP come to
Vision reinforcement learning generative
AI prompt engineering charitability and
many more you will receive a prestigious
certificate and ask me anything session
by IBM with five Capstone in different
domain using real data set you will gain
practical experience master classes by
party faculty and IBM expert ensure
top-notch education simply learns
jobages help you get notified by Leading
companies this program covers statistics
python supervisor and supervised
learning NLP neural network computer
vision gen square tensorflow and many
more other skills so why wait enroll now
and unlock exciting Ai and ml
opportunities the link is in the
description box below should we have our
um it looks a little bit like
Frankenstein or Frankenstein looking
robot today let me tell you what is
machine learning machine learning works
on the development of computer programs
that can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct a house in two days this was
back in July 29th
2016. so that's pretty impressive this
amount of time to continue to grow in
its development and it's smart enough to
leave spaces in the brickwork for wiring
and plumbing and can even cut and shape
bricks to size
Amazon Echo relies on machine learning
and with more data it becomes more
accurate play your favorite music order
pizza from Domino's voice control your
home request rides from Uber have you
ever wondered the difference between AI
machine learning and deep learning
artificial intelligence a technique
which enables machines to mimic human
behavior this is really important
because this is how we are able to gauge
how well our computations or what we're
working on works is the fact that we're
mimicking human behavior we're using
this to replace human work and make it
more efficient and make it more
streamlined and more accurate and so the
center of artificial intelligence is the
big picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which the
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer ibf Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks alphago natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we
would have a very hard time tracking and
it's able to come up with an answer from
that now let's see how machine learning
works first we start with training the
data once we've trained the data the
train we go into the machine learning
algorithm which then puts the data into
a processing which then goes down to
machine another machine learning
algorithm and then we take new data
cause you have to test whatever you did
to make sure it works correctly and we
put that into the same algorithm once we
do that we check our prediction we check
our results and from the prediction if
we've set aside some training date and
we find that it didn't do a good job
predicting it and it gets a thumbs down
as you see then we go back to the
beginning and we retrain the algorithm
and a lot of times it's not just about
getting the wrong answer it's about
continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can
now let's look into the types of machine
learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggests you have
to supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well yes surprise
aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed these data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it this kind of a
model is used by Flipkart to figure out
the products that are well suited for
you
honestly speaking this is my favorite
type of machine learning out of all the
three and this type has been widely
shown in most of the Sci-Fi movies
lately let's find out how it works
imagine a newborn baby
you put a burning candle in front of the
baby the baby does not know that if it
touches the flame its fingers might get
burnt so it does that anyway and gets
hurt the next time you put that candle
in front of the baby it will remember
what happened the last time and would
not repeat what it did that's exactly
how reinforcement learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask our same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from his mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA wherein the level of
difficulty increases as you get better
with the games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learn
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to find out hidden
patterns or structures in data
I think this would have made a lot of
things clear for you regarding
supervised and unsupervised learning
machine learning has occupied every
field be it e-commerce education
personal assistant self-driving cars and
what not it is everywhere now what
exactly is machine learning machine
learning in general is a branch of
artificial intelligence that uses data
patterns to generate judgments or
predictions it allows computers to adapt
to repeating processes and results based
on past performance without requiring
them to be explicitly programmed in
advance as with standard algorithms in
simple English it basically allows
robots to learn from predict and adjust
to Prior Behavior it is a method of
generating artificial intelligence
without needing to predetermine all the
guidelines and procedures now that we
have learned the basics of machine
learning it's time to focus on subjects
that will help you to become a machine
learning expert it's time to list out
all the technologies that we need to
learn firstly computer science a
complete machine learning project that
is successful will produce working
software as a result since they need
strong software engineering abilities to
produce usable software ml Engineers are
expected to have a solid understanding
of core computer science concept next we
need programming language learning a
programming language is the next and
most obvious step you must learn how to
develop software if a machine learning
engineer's output is deliverable
software an understanding of a
programming language is needed for that
the most used language for machine
learning is python some employers may
require you to be knowledgeable in
additional languages such as Java and C
plus plus depending on where you work
next comes data structure and algorithms
data structures and algorithms are
frequently disregarded when discussing
machine learning although this
passionate depicts their significance
DSA addresses answers to Common problems
in detail and gives us a clear idea of
how effective each solution is
additionally it teaches us scientific
methods for assessing an algorithms
Effectiveness enabling us to select the
optimal answer to our problem from a
range of options this is crucial for a
machine learning engineer because we
might need to develop our algorithms
sometimes so having a solid
understanding of DSA is crucial next
comes Basics mathematics and statistics
there is a lot of arithmetic required
for machine learning our algorithms
ability to find patterns in data and use
those patterns to inform decisions is
made possible by math in our ml careers
we may need to revisit various math
ideas from time to time to comprehend
various technology systems and
Architects to get us started machine
learning requires data without data
there can be no machine learning it's
safe to assume that most of the data
utilized for machine learning are
structured this by the fields expansion
into other fields that involve
unstructured data and finally machine
learning algorithms learning machine
learning is crucial to become a machine
learning engineer machine learning
algorithms are classified into four
types which includes supervised learning
unsupervised learning semi-supervised
learning and reinforcement learning
learn it in detail it would help you if
you consider the options for several
programming languages IDs and platforms
when creating your machine learning
models it would also help you next begin
studying and improving each machine
learning approach although the subject
is Broad each topic can be taught in a
few hours if you focus on them it would
be best if you thought about one subject
at a time mastered it put it to practice
and applied the associated algorithms in
your chosen language machine learning
has been the Talk of the Town lately
every organization has realized the
potential of machine learning in
improving their business objectives and
attaining the Enterprise goals this
expanding demand has led to a lot of
people applying for machine learning
jobs and upskilling themselves in the
field of machine learning you can take
up this growing opportunity in the field
of machine learning and utilize it to
lend yourself a very challenging
fulfilling and high paying job in this
video we will be breaking down in
complete detail each and every skill
that you would need in order to correct
the machine learning engineer job
interview well ml is not just a passing
Trend it's a seismic shift that is
reshaping a world and creating new
avenues for Innovation and Discovery so
by embracing a career in ml you become
part of dynamic field that thrives on
solving complex problem pushing
boundaries and making a profound impact
on society so the demand for ML
professional is skyrocketing across
industries from Healthcare and finance
to entertainment and transportation
organizations are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but but what skill does it takes
to become an ml engineer how can you
embark on this thrilling Journey we have
the answers to all your questions also
accelerate your career in Ai and ml with
our comprehensive postgraduate program
in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and ask me anything
sessions by IBM with three Capstone
projects and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Caltech
faculty and IBM experts ensure top-notch
education simply learns job assist help
you get noticed by Leading companies
this program covers python supervised
and unsupervised learning NLP neural
networks computer Visions gns Keras
tensorflow and many more other skills so
enroll now and unlock exciting Ai and
MLM opportunities the link is in the
description box below so without any
further delay let's get started to
become a machine learning engineer you
need a combination of technical skills
non-technical skills and some bonus
skills so here are some essential skills
required to pursue a career as an ml
engineer so first we will talk about
some technical skills to become a ml
engineer so first one in the list is
programming languages strong programming
skills are essential you should be
proficient in at least one programming
languages such as python or R python is
widely used in the ml Community due to
its Rich libraries that is numpy pandas
tensorflow and Pi torch that supports ml
tasks and the second on the list is
machine learning algorithms and
techniques you should have a solid
understanding of various ml algorithms
such as linear regression logistic
regression decision trees random forests
neural network and deep learner
familiarize yourself with the principles
Behind These algorithms their pros and
cons and when to use them so third one
on the list is data preprocessing ml
models require were clean and well
prepared data you should know how to
handle missing data deal with normalized
and standardize data and the finalists
perform feature engineering
understanding data preprocessing
technique is crucial for Effective ml
model training and the fourth one is
data manipulation and Analysis data is
the foundation of ml model you should be
skilled in the data manipulation and
Analysis using libraries like numpy and
pandas this includes cleaning and
transforming data exploratory data
analysis which is Eda and understanding
the statical properties of the data and
the fifth one is machine learning
libraries and Frameworks familiarity
with popular ml libraries and framework
is essential some some commonly used one
include numpa pandas tensorflow and Pi
dot this Library provide pre-implemented
ml algorithms neural network
architectures and tools for model
training and evaluation now that we have
seen the Technical Machine learning
engineering skills let us have a look at
the non-technical machine learning
skills
so the first one is industry knowledge
machine learning projects that
effectively tackle genuine challenges
are likely to achieve great success
regardless of the industry you are
involved in it is crucial to have a
comprehensive understanding of its
operation and identify ways to optimize
business outcomes and the second one on
the list is effective communication
effective communication plays a crucial
role in facilating these interactions
companies seeking skill ml engineer
value candidates who can effectively
convey technical discoveries to
non-technical terms like marketing or
sales demonstrating Clarity and fluency
in their explanation moving forward
let's see some bonus skills to become a
ml engineer the first one on the list is
reinforcement learning in 2023
reinforcement learning emerge as a
catalyst for numerous captivating
advancement in deep learning and
artificial intelligence to pursue a
career in robotics self-driving cars or
any other AI related feed it is crucial
to comprehend this concept and the
second on the list is computer vision
computer vision and machine learning are
fundamental branches of computer science
that can independently fuel highly
Advanced system relying on CV and ml
algorithm however their combination has
a potential to unlock the greater even
possibilities and achievements so
remember that the field of ml is
constantly evolving through continuous
learning and staying updated with the
latest development and the research
papers are essential to be the Practical
finding a suitable job in the field of
machine learning is becoming
increasingly difficult the ideal way to
display your machine learning skill is
in the form of portfolio of data science
and machine learning projects a solid
portfolio of projects will illustrate
that you can utilize those machine
learning skills in your profile as well
projects like movie recommendation
system fake news detection and many more
are the best way to improve your early
programming skills you may have the
knowledge but putting it to the use what
is keep you competitive here are 10
machine learning projects that can
increase your portfolio and enable you
to acquire a job as a machine learning
engineer at number 10 we have loan
approval prediction system in this
machine learning project we will analyze
and make prediction about the loan
approval process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
production system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting face new this ml projects for
detecting fake news is concerned with
the fake news and the true news on our
data set we create our tfid vectorizer
with SQL the model is then fitted using
a passive aggressive classifier that has
been initialized finally the accuracy
score and the config Fusion Matrix
indicate how well our model performs the
link for the projects in the description
box below at number 8 we have
personality prediction system the idea
is based on determining an individual
personality using machine learning
techniques a person personality
influences both his personal and
professional life nowadays many
companies are shortlisting applicants
based on their personality which
increases job efficiency because the
person is working on what he is good at
rather than what is compelled to do in
our study we are tempted to combine
personality prediction system using
machine learning techniques such as SPD
name based and logistic regression to
predict a personal personality and
talent prediction using phrase frequency
method this model or method allow users
to recognize their personality and
Technical abilities easily to learn
about more this project check the link
in the description box below at number 7
we have Parkinson disease system
percussion disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than one million worldwide
each other in this machine learning
project we will develop an svm model
using python modules scikit-learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the feature split the data set
design and NCS model and calculate the
model accuracy and at the end we will
check the Parkinson's disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtedly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text-to-speech apis are
available for this project we will
utilize pytt sx3 pydt access 3 is a
python text to speech conversion
liability it operates offline unlike
other libraries and is compatible with
python 2 and python 3. before API
various pre-trained models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number 5 we have a speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or programmed to
recognize and transfer word is spoken
allowed into readable text mlsp's
recognition uses algorithm that models
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acoustic modeling is used
to identify the finance and the
phonetics on the speech for this project
we will utilize pyts X3 pyts X3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and python 3. to learn more about this
project check the link in the
description box below at number 4 we
have sentiment analysis sentiment
analysis also known as opinion mining is
a state forward process of determining
the author's feeling about attacks what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data management
station in this project we will use the
Amazon customer review data set for the
sentiment analysis check the link in the
description box below at number 3 we
have image classification using CNN deep
learning is a booming field currently
most projects and problem statement use
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
conspiration in this project to learn
more about this project check the link
in the description box below at number 2
we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use opencv and face
recognition libraries to create a phase
detection system opencv provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be Discerning
recommendations are often made based on
a viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dataset
generated by the more than 6000 users
machine learning is transforming
Industries and driving Innovation by
enabling systems to learn from data and
make Intelligent Decisions this has
resulted in a skyrocketing demand for
machine learning Engineers these
professionals possess the skills to
develop Advanced algorithms well
predictive models and extract valuable
insights from vast amounts of data the
era of Big Data necessates professionals
who can Drive insights from vast
information and machine learning
Engineers possess the skills to analyze
complex data sets extract valuable
knowledge and build predictive models
for accurate forecast process Automation
and optimized decision making Industries
such as Healthcare Finance e-commerce
and autonomous vehicles heavily rely on
machine learning to enhance processes
and drive growth as businesses
increasingly recognize the power of
data-driven insights the demand for
skilled machine learning Engineers
continues to search that instrumental in
revelaging the potential of machine
learning to optimize operations improve
efficiency and gain a Competitive Edge
with the market value of machine
learning expected to grow exponentially
now is the time to enter into the field
of machine learning and become an ml
engineer who are at the Forefront of
technological advancements and Bank
lucrative career opportunities in this
Dynamic field that opens doors to a
world of endless possibilities and
Promises of fulfilling an impactful
career on that note hello everyone
welcome to Simply learn today we will
explore a profession that has witnessed
a huge surge in demand in recent years
which is going to be the next big thing
in the future can you guess which
profession we are talking about well you
got it right that's machine learning
engineer having said that if you want to
Embark your career as an ml engineer
then our postgraduate program in a and
ml can be the right option for you this
AR ml postdocet program by simply learn
in collaboration with Purdue University
covers a wide range of topics and
provides hands-on experience in
developing Ai and machine learning
solutions for professionals who want to
upgrade their skills and people to a
career in the field of AR machine
learning this course features master
classes by Purdue faculty and IBM
experts exclusive hackathons to help you
master various skills like statistics
python supervised learning neural
networks NLP and much more by covering
tools and techniques like numpy pandas
python sci-fi along with amazing
industry projects so what is stopping
you from building your future in a and
ml enroll now course link is added in
the description box below make sure to
check that out without any further Ado
let's get started meet John who has been
working as a software developer in the
IIT industry for over five years now
John has been hearing a lot about
machine learning lately and how it has
been revolutionizing the tech industry
he wonders whether learning machine
learning would be worth it and that's
when his colleague Harry helps him out
hey harry I've been reading a lot about
machine learning engineering lately what
are your thoughts absolutely machine
learning is booming right now really
quite intriguing but how can I become an
ml engineer well with the right skill
set you can my friend possibilities are
endless wow that's impressive looks like
machine learning can be a regarding
career with great opportunities and
lucrative salaries absolutely John so
let me tell you more about the working
aspects of machine learning engineer in
detail we'll start this tutorial with a
quick introduction to who ml engineer is
and next we'll understand what exactly
does a machine learning engineer do next
we'll dive into the roles and
responsibilities of a machine learning
engineer after that we'll discuss some
of the most important skills that animal
engineer should possess and finally
we'll talk about the salary and job
aspects of a machine learning engineer
so firstly who is a machine learning
engineer a machine learning engineer is
a skilled professional who combines
expertise in programming mathematics and
machine learning to develop and deploy
intelligent systems that can learn from
data and make accurate predictions or
decisions the role of a machine learning
engineer involves working with large
data sets pre-processing and cleaning
the data selecting appropriate machine
learning algorithms and training models
using this data they often collaborate
with data scientists and domain experts
like software Engineers to integrate
machine learning Solutions into
production environments for optimizing
and fine-tuning models for accuracy and
efficiency as well as evaluating their
performance and finally they perform a
thorough effective data exploration and
visualization techniques in order to
contribute for better understanding of
the data and identifying patterns making
inform decisions throughout the machine
learning pipelines so what are the roles
and responsibilities of a machine
learning engineer firstly problem
understanding and solution design which
machine learning Engineers work closely
with stakeholders domain experts to
understand the business problem or
objective that machine learning can
address they analyze requirements
identify suitable data sources and
propose machine learning based Solutions
this involves translating business
problems into technical specifications
defining measurable goals and designing
the overall architecture of the machine
Learning System data preparation and
Analysis machine learning Engineers need
to have a deep understanding of the data
they are working with this involves
cleaning pre-processing and transforming
raw data into a format suitable for
training and evaluating ml models
exploratory data analysis techniques are
often used to gain insights and
understand the underlying patterns and
relationships within the data model
development and evaluation ml Engineers
are responsible for developing training
and fine-tuning machine learning models
this involves selecting the appropriate
algorithms architectures and techniques
based on the problems at hand you will
need to implement an experiment with
different models such as the decision
trees neural networks support Vector
machines and much more and finally
monitoring maintenance and iterative
Improvement after deployment ml
Engineers monitor the performance of
deployed models and address any sort of
issues that arise they design and
Implement monitoring systems to track
model performance detect any kind of
anomalies and identify data drift or
concept drift regular model maintenance
is necessary to retrain and refine
models using new data to maintain
accuracy and adaptability so these were
some of the day-to-day roles and
responsibilities of an ml engineer well
let us now talk about the skill set that
a ml engineer should persist firstly
they should have a strong knowledge on
programming tools now strong programming
skills are essential for machine
learning Engineers programming languages
like R which is a free software
environment for statistical Computing
and Graphics can surely help but mainly
they use python python is widely used in
the machine Learning Community due to
its simplicity extensive libraries and
strong ecosystem so you need to
familiarize yourself with python
fundamentals data manipulation libraries
like numpy and pandas and machine
learning Frameworks such as tensorflow
pytotch and scikit-learn also
familiarize yourself with programming
Concepts data structures and algorithms
and additionally knowledge of other
languages like R and Java can be
beneficial secondly machine learning and
deep learning Concepts ml Engineers
should have a deep understanding of
various machine learning algorithms
including supervised learning
unsupervised learning and reinforcement
learning familiarize yourself with
Concepts like KNN linear regression
decision trees support Vector machines
neural networks Ensemble methods and
much more not just that furthermore ml
Engineers need a solid foundation in
mathematics and statistics this includes
knowledge of linear algebra calculus
probability Theory and statistical
analysis understanding these Concepts is
crucial for this designing and
implementing machine learning algorithms
now in addition to these technical
skills machine learning Engineers should
also pose a strong analytical and soft
skills to excel in their roles they need
to think critically and out of the box
to understand problems break them down
into manageable components and develop
appropriate machine learning solutions
they should be able to analyze data
identify patterns and make informed
decisions based on the results and
finally communication skills now
effective communication is crucial for
machine learning Engineers they need to
extend complex technical Concepts to
non-technical stakeholders as well while
collaborating with cross-functional
teams and present their findings and
recommendations strong written and
verbal communication skills will surely
help convey information clearly and
concisely which not only enhance their
professional growth but also contribute
to their overall effectiveness as a
machine learning engineer with all that
having said let us now discuss some of
the salary and job prospects of a
machine learning engineer now the salary
of machine learning engineer can vary
significantly based on various factors
in United States of America machine
learning Engineers are in high demand
particularly in technology hubs like
Silicon Valley in California and Pickups
like New York according to report by
Glassdoor the average salary of a
professional ml engineer is around 109
000 per year now salaries in the U.S
tend to be higher due to the cost of
living and fierce competition for talent
and entry-level machine learning
Engineers can expect salaries ranging
from 80 000 to 120 000 per year while
mid-level professionals with a few years
of experience can around 120 000 to 180
000 per year and finally a senior level
or an experienced machine learning
engineer can comment salaries exceeding
two hundred thousand dollars per year as
well especially in Top tire companies or
leadership positions on the other hand
in India the salary range for machine
learning Engineers is relatively lower
compared to the US but it's important to
consider the lower cost of living as
well now the average salary for machine
learning engine engineer in India is
around 8.5 lakhs while entry-level
machine learning engineers in India can
expect salaries ranging from 4 lakhs to
8 lakhs per year and mid-level
professionals can earn on somewhere
between 8 lakhs to 15 lakhs per year and
finally senior level machine learning
Engineers with extensive experience and
expertise can earn salaries ranging from
15 lakhs to 30 lakhs per year now
additionally other benefits and
compensation components such as bonuses
stock options health care benefits and
retirement plans should also be
considered but let's talk about reality
it's crucial to note that these figures
are approximate and can vary based on
various such factors so here are some of
the salary deciding factors of a machine
learning engineering firstly company and
location now the location of the
employee company can impact a machine
learning engineer's salary large
technology companies or organizations
with a strong focus on a and ml tend to
offer competitive compensation packages
to attract top talent Additionally the
cost of living in different Geographic
areas can affect salary ranges which
side is often being higher in Tech hubs
or cities with a higher cost of living
work experience experience plays a
significant role in determining the
salary of a ml engineer generally
Engineers with more years of experience
tend to command higher salaries this is
because experience indicates a deeper
understanding of machine learning
Concepts techniques and real world
applications which makes experienced
Engineers more valuable to the employers
out there skill set and expertise now
specific skills and expertise possessed
by a machine learning engineer can
impact their salaries as well ml
Engineers proficient and popular
programming languages like python are
Java and Frameworks like tensorflow and
pytouch and libraries such as
scikit-learn are often in high demand
and can negotiate higher salaries
additionally specialized knowledge in
areas like deep learning natural
language processing computer vision can
also be valuable and potentially result
in higher compensation and finally the
industry that you're working on and the
current demand prints now the industry
in which an machine learning engineer
Works can influence their salary as well
industries that heavily rely on machine
learning and AI applications such as
Finance Healthcare e-commerce and
autonomous vehicles offer higher salary
packages to machine learning Engineers
due to the high demand for their
expertise also it's important to note
that these relative importance of these
factors may vary depending on the
specific circumstances and job market
conditions salaries can also be
influenced by economic factors market
demand and individual negotiation skills
overall machine learning engineers in
both U.S India and the rest of the world
can earn competitive salaries and it's
essential to assess the overall
compensation package growth
opportunities and Company culture when
evaluating job opportunities
human versus artificial intelligence
humans are amazing let's just face it
we're amazing creatures we're all over
the planet we're exploring every Nick
Chinook we've gone to the Moon we've
gotten into outer space we're just
amazing creatures we're able to use the
available information to make decisions
to communicate with other people
identify patterns and data remember what
people have said adapt to new situations
so let's take a look at this so you can
get a picture you're a human being so
you know what it's like to be human
let's take a look at artificial
intelligence versus the human artificial
intelligence develops computer systems
that can accomplish texts that require
human intelligence
so we're looking at this one of the
things that computers can do is they can
provide more accurate results this is
very important recently I did a project
on cancer whereas identifying markers
and as a human being you look at that
and you might be looking at all the
different images and the data that comes
off of them and say I like this person
so I want to give them a very good
outlook and the next person you might
not like so you want to give them a bad
Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolve it's very quick and easy and
affordable to do this
what is machine learning and deep
learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world
and you're sitting by the road and you
have a whole lot of and a whole lot of
time passes by there's a few hours
and suddenly you wonder
how many cars buses trucks and so on
passed by in the six hours
now chances are you're not going to sit
by the road for six hours and count
buses cars and trucks unless you're
working for the city and you're trying
to do City Planning and you want to know
hey do we need to add a new truck route
maybe we need a bicycle length we have a
lot of bicyclists here that kind of
thing so maybe City Planning would be
great for this
machine learning well the way machine
Learning Works is we have labeled data
with features okay so you have a truck
or a car a motorcycle a bus or a bicycle
and each one of those are labeled it
comes in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle this is a little bit more
in depth on this and the model here it
actually the features we're looking at
would be like the tires someone sits
there and figures out what a tire looks
like it takes a lot of work if you try
to try to figure the difference between
a car tire a bicycle tire a motorcycle
tire uh so in the machine learning field
this could take a long time if you're
going to do each individual aspect of a
car and try to get a result on there and
that's what they did do that was a very
this is still used on smaller amounts of
data we figure out what those features
are and then you label them
deep learning so with deep learning one
of our Solutions is to take a very large
unlabeled data set
and we put that into a training model
using artificial neural networks and
then that goes into the neural network
itself when we create a neural network
and you'll see the arrows are actually
kind of backward but uh which actually
is a nice point because when we train
the neural network
we put the bicycle in and then it comes
back and says if it's a truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the model so you need
to have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
is a truck a motorcycle as a motorcycle
let's just take a little closer look at
that
determining what objects are present in
the data so how does deep learning do
this and here we have the image of the
bike it's 28 by 28 pixels that's a lot
of information there could you imagine
trying to guess that this is a bicycle
image by looking at each one of those
pixels and trying to figure out what's
around it and we actually do that as
human beings it's pretty amazing we know
what a bicycle is and even though it
comes in as all this information and
what this looks like is the image comes
in
it converts it into a bunch of different
nodes in this case there's a lot more
than what they show here and it goes
through these different layers and
outcomes and says okay this is a bicycle
a lot of times they call this the magic
Black Box why because as we watch it go
across here all these weights and all
the math behind this and it's not it's a
little complicated on the math side you
really don't need to know that when
you're programming or doing working with
the Deep learning but it's like magic
you don't know you really can't figure
out what's going to come out by looking
what's in each one of those dots and
each one of those lines are firing and
what's going in between them so we like
to call it the magic box so that's where
deep learning comes in
and in the end it comes up and you have
this whole neural network it comes up
and it says okay we fire all these
different pixels and we connect all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning
machine learning we're going to take a
step into machine learning here and
you'll see how these fit together in a
minute the system is able to make
predictions or take decisions based on
past data that's very important for
machine learning is that we're looking
at stuff and based on what's been there
before we're creating a decision on
there we're creating something out of
there we're coloring a beach ball we're
telling you what the weather is in
Chicago
what's nice about machine learning is a
very powerful processing capability it's
quick and accurate outcomes so you get
results right away once you program the
system the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are petabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so because a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do
problem solved in an end-to-end method
so instead of having to break it apart
and you have the first piece coming in
and you identify tires and the second
piece is identifying labeling handlebars
and then you bring that together that if
it has handlebars and tires it's a
bicycle and if it has something that
looks like a large Square it's probably
a truck the neural networks does this
all in one network you don't really know
what's going on in all those weights and
all those little bubbles but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own
best features are selected by the system
and it this is important they kind of
put it's on a bullet on the side here
it's a subset of machine learning this
is important we talk about deep learning
it is a form of machine learning there's
lots of other forms of machine learning
data analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test you put in your information
you then have your group of tests and
then you held some aside you see how
does it do it's very quick to test it
and see what's going on with your deep
learning and your neural network
are they really all that different
AI versus machine learning versus deep
learning concepts of AI so we have a
concepts of I I you'll see natural
language processing machine learning and
approach to create artificial
intelligence so it's one of the subsets
of artificial intelligence knowledge
representation automated reasoning
computer vision robotics machine
learning versus AI versus deep learning
or Ai and machine learning and deep
learning
so we look at this we have ai with
machine learning and deep learning and
so we're going to put them all together
we find out that AI is a big picture we
have a collection of books it goes
through some deep learning the Digital
Data is analyzed text mining comes
through the particular book you're
looking for maybe it's a genre books is
identified and in this case we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens so
we'll look at some of the pieces here
this information goes into uh there's as
far as this example the translation of
the handwritten printed data to digital
form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we used a deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the book
so here's a nice summation of what we
were just talking about AI with machine
learning and deep learning deep learning
is a subset of machine learning which is
a subset of artificial intelligence so
you can look at artificial intelligence
as the big picture how does this compare
to The Human Experience in either doing
the same thing as a human we do or it
does it better than us and machine
learning which has a lot of tools is
something that learns from data past
experiences it's programmed it comes in
there and it says hey we already had
these five things happen the sixth one
should be about the same and then
there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together
real life examples
artificial intelligence news generation
very common nowadays as it goes through
there and finds the news articles or
generates the news based upon the news
feeds or the back end coming in and it
says okay let's give you the actual news
based on this there's all the different
things Amazon Echo they have a number of
different Prime music on there of course
there's also the Google command and
there's also Cortana there's tons of
smart home devices now where we can ask
it to turn the TV on or play music for
us that's all artificial intelligence
from front to back you're having a human
experience with these computers and
these objects that are connected to the
processing machine learning spam
detection very common machine learning
doesn't really have the human
interaction part
so this is the part where it goes and
says okay that's a Spam that's not a
Spam and it puts it in your spam folder
search engine result refining another
example of machine learning whereas it
looks at your different results and it
Go and it is able to categorize them as
far as I said the most hits this is the
least viewed this has five stars you
know however they want to weight it all
exam good examples of machine learning
and then the Deep learning deep learning
another example is as you have like a
exit sign in this case is translating it
into French sortie I hope I said that
right
neural network has been programmed with
all these different words and images and
so it's able to look at the exit in the
middle and it goes okay we want to know
what that is in French and it's able to
push that out in France French and learn
how to do that
and then we have chat Bots I remember
when Microsoft first had their little
paper clip
um boy that was like a long time ago
they came up and you would type in there
and chat with it these are growing you
know it's nice to just be able to ask a
question and it comes up and gives you
the answer and instead of it being were
you just doing a search on certain words
it's now able to start linking those
words together and form a sentence in
that chat box
types of AI and machine learning
types of artificial intelligence this in
the next few slides are really important
so one of the types of artificial
intelligence is reactive machines
systems that only react they don't form
memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to re-sitter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
short-lived when we're talking about
this and you look at like a neural
network that's been programmed to
identify cars it doesn't remember all
those pictures it has no memory as far
as hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as a
final output for that neural network we
looked at
so when they talk about limited memory
this is what they're talking about
they're talking about I've created this
based on all these things but I'm not
going to remember anyone specifically
theory of Mind systems being able to
understand human emotions and how they
affect decision making to adjust their
behaviors according to their human
understanding
this is important because this is our
page mark this is how we know whether it
is an artificial intelligence or not is
it interacting with humans in a way that
we can understand
without that interaction is just an
object so we talk about theory of mind
we really understand how it interfaces
that whole if you're in web development
user experience would be the term I
would put in there so the theory of mine
would be user experience how's the whole
UI connected together and one of the
final things is as we get into
artificial intelligence is systems being
aware of themselves understanding their
internal States and predicting other
people's feelings and act appropriately
so as artificial intelligence continues
to progress we see ones they're trying
to understand well what makes people
happy how would they increase our
happiness how would they keep themselves
from breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time to help your
life and help you grow that'd be the
self-awareness is being able to know all
those different things
types of machine learning and like I
said on the last slide this is very
important this is very important if you
decide to go in and get certified in
machine learning or know more about it
these are the three primary types of
machine learning the first one is
supervised learning systems are able to
predict future outcome based on past
data requires both an input and an
output to be given to the model for it
to be trained
so in this case we're looking at
anything where you have a hundred images
of a bicycle
and those hundred images you know are
bicycle so they're preset someone
already looked at all hundred images and
said these are pictures of bicycles and
so the computer learns from those and
then it's given another picture
and maybe the next picture is a bicycle
and it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
it can look at an image and start
pulling the different pieces of the
image out because they aren't the same
the human all the parts of the human are
not the same as a fuzzy tree behind them
because it's slightly out of focus which
is not the same as the beach ball it's
unsupervised because we never told it
what a beach ball was we never told it
what the human was and we never told it
that those were trees all we told it was
hey separate this picture by things that
don't match
and things that do match and come
together
and finally there's reinforcement
learning systems are given no training
it learns on the basis of the reward
punishment it received for performing
its Last Action it helps increase the
efficiency of a tool function or a
program reinforced learning a
reinforcement learning is kind of give
it a yes or no yes you gave me the right
response no you didn't and then it looks
at that and says oh okay so based on
this data coming in what I gave you was
a wrong response so next time I'll give
you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
were grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning
they're usually not talking about huge
amounts of data we're talking about
maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe you have like the case
of one of the things 500 points of data
and 30 different fields it starts
getting really confusing there and
artificial intelligence or machine
learning and the Deep learning aspect
really shines when you get to that
larger data that's really complex
works well on a low end systems so a lot
of the machine learning tools out there
you can run on your laptop with no
problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data
we're talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts
and the problem is solved in an
end-to-end manner so you only have one
neural network or two neural networks
that is bringing the data in and putting
it out it's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem I have a solving one
piece of the puzzle
with regular machine learning emotional
machine learning tools out there they
take longer to test and understand how
they work and with the Deep learning is
pretty quick once you build that neural
network you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthcare
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization in so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant machine
learning has improved our lives in a
number of wonderful ways today let's
talk about some of these I'm Rahul from
Simply learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decoded with
the help of machine learning in neural
networks and then provide you with an
output so if you've ever noticed that
these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to large cricket ground the first thing
I would probably do is to get on Google
Maps so search it
and let's put it here
so here we have the path you should take
to get to launch cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicate that
they are slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as I told you
before red means heavily congested
yellow means slow moving and blue means
clear
so how exactly is Google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
the help of machine learning and with
the help of two important meshes first
is the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps.me and here we go next up
we have social media personalization so
say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what spam and what's not spam
so Gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data Gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that Gmail uses is content
filters header filters General Blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes is that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags
the one known size and duration now this
is used to predict stock market trends
assist to Medical Technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice chemic stock lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and science that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence of sequence learning which is
the same thing that's used with chatbots
here the image recognition happens using
convolutional neural networks and the
text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other also accelerate
your career in Ai and ml with our
conference postgraduate program in Ai
and machine learning both your career
with this Ai and ml course delivered in
collaboration with Purdue University and
IBM so why wait enroll now and unlock
exciting Ai and ml opportunities the
link is in the description box below
they already knows the individual pieces
of factual information collected from
various sources it is stored process and
later used for analysis
and so we see here just a huge grouping
of information a lot of tech stuff money
dollar signs numbers and then you have
your performing analytics to drive
insights and hopefully you have a nice
share your shareholders gathered at the
meeting and you're able to explain it in
something they can understand
so we talk about data's types of data we
have in our types of data we have a
qualitative categorical
you think nominal or ordinal and then
you have your quantitative or numerical
which is discrete or continuous
and let's look a little closer at those
data type vocabulary always people's
favorite is the vocabulary words okay
not mine
uh but let's dive into this what we mean
by nominal nominal they are used to
label various uh label our variables
without providing any measurable value
country gender race hair color
Etc it's something that you either mark
true or false this is a label it's on or
off either they have a red hat on or
they do not so a lot of times when
you're thinking nominal data labels
think of it as a true false kind of
setup and we look at ordinal this is
categorical data with a set order or a
scale to it and you can think of salary
range as a great one movie ratings Etc
you see here the salary range if you
have ten thousand to twenty thousand
number of employees earning that rate is
150. twenty thousand to thirty thousand
a hundred and so forth some of the terms
you'll hear is bucket this is where you
have 10 different buckets and you want
to separate it into something that makes
sense into those 10 buckets and so when
we start talking about ordinal a lot of
times when you get down to the breast
bones again we're talking true false so
if you're a member of the 10 to 20K
range so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket
quantitative numerical data falls into
two classes discrete or continuous and
so data with a final set of values which
can be categorized class strength
questions answered correctly and runs
hit and Cricket a lot of times when you
see this you can think integer and a
very restricted integer I.E you can only
have 100 questions on a test so you can
it's very discreet I only have a hundred
different values that it can attain so
think usually you're talking about
integers but within a very small range
they don't have an open end or anything
like that
uh so discrete is very solid simple to
count set number continuous on the other
hand continuous data can take any
numerical value within a range so water
pressure weight of a person Etc usually
we start thinking about float values
where they can get phenomenally small in
their in what they're worth and there's
a whole series of values that falls
right between discrete and continuous
you can think of the stock market you
have dollar amounts it's still discrete
but it starts to get complicated enough
when you have like you know jump in the
stock market from
525.33 to
580.67 cents there's a lot of Point
values in there it'd still be called
discrete but you start looking at it as
almost continuous because it does have
such a variance in it now we talk about
no we did we went over nominal and
ordinal almost true false charts and we
looked at quantitative and numerical
data data which we'll start to get into
numbers discrete you can usually a lot
of times discrete will be put into it
could be put into true false but usually
it's not so we want to address this
stuff and the first thing we want to
look at is the very basic which is your
algebra so we're going to take a look at
linear algebra you can remember back
when your euclidean geometry we have a
line well let's go through this we have
a linear algebra as the domain of
mathematics concerning linear equations
and their representations in Vector
spaces and through matrixes I told you
we're going to talk about Matrix is uh
so a linear equation is simply
2x plus 4y minus 3z equals 10. very
linear 10x plus 12.4 y equals z and now
you can actually solve these two
equations by combining them unless we're
talking about a linear equation
and the vectors we have a plus b equals
c now we're starting to look at a
direction and these values usually think
of an x y z plot so each one is a
direction and the actual distance of
like a triangle a b is C and then your
Matrix can describe all kinds of things
I find matrixes confuse a lot of people
not because they're particularly
difficult but because of the magnitude
and the different things they're used
for
and a matrix is a chart or a you know
think of a spreadsheet but you have your
rows and your columns and you'll see
here we have a times b equals c very
important to know your counts uh so
depending on how the math is being done
what you're using it for making sure you
have the same rows and number of columns
or a single number there's all kinds of
things that play in that that can make
matrixes confusing but really it has a
lot more to do with what domain you're
working in are you adding in multiple
polynomials where you have like uh a x
squared plus b y plus you know you start
to see that can be very confusing versus
a very straightforward Matrix and let's
just go a little deeper into these
because these are such primary this is
what we're here to talk about is these
different math mathematical computations
that come up
so we're looking at linear equations
let's dig deeper into that one an
equation having a maximum order of one
is called a linear equation
so it's linear because when you look at
this we have ax plus b equals c which is
a one variable
we have two variable ax plus b y equals
c a X plus b y plus z c z equals D and
so forth but all of these are to the
power of one you don't see x squared you
don't see X cubed so we're talking about
linear equations that's what we're
talking about in their addition if you
have already dived into say neural
networks you should recognize this ax
plus b y plus c z
setup plus The Intercept which is
basically your your neural network each
node adding up all the different inputs
and we can drill down into that most
common formula is your y equals MX plus
c
so you have your y equals the M which is
your slope your X Value Plus C which is
your y-intercept it kind of labeled it
wrong here
threw me for a loop but the the C would
be your y-intercept so when you set x
equal to 0 y equals c and that's that's
your y-intercept right there uh and
that's they they just had a reverse
value of y when x equals zero equals the
y-intercept which is C and your slope
gradient line which is your M so these
are y equals two X plus three and
there's lots of easy ways to compute
this this way this is why we always
start with the most basic one when we're
solving one of these problems and then
of course the uh one of the most
important takeaways is the slope
gradient of the line so the slope is
very important that M value in this case
we went ahead and solved this
if you have y equals two X plus three
you can see how it has a nice line graph
here on the right
so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and rows
so we're talking M rows by n columns
here a11 is denotes the element of the
first row in the First Column similarly
a 12 and it's really pronounced a11 in
this particular setup so it's a row one
column one a 12 is a of Row one column
two first row and second column and so
on
and there's a lot of ways to denote this
I've seen these as like a capital letter
a smaller case a for the top row or I
mean you can see where they can go all
kinds of different directions as far as
the value
you just take a moment to realize there
needs to be some designation as far as
what row it's in and what column it's in
and we have our basic operations we have
addition so when you think about
addition you have uh two matrixes of two
by two and you just add each individual
number in that Matrix and then when you
get to the bottom you have in this case
the solution is 12 10 plus 2 is 12 5
plus 3 is 8 and so on and the same thing
with subtraction
now again your counting Matrix is you
want to check your dimensions of the
Matrix the shape you'll see shape come
up a lot in programming so we're talking
about Dimensions we're talking about the
shape if the two shapes are equal this
is what happens when you add them
together or subtract them
and we have multiplication when you look
at the multiplication you end up at the
very a slightly different setup going
now if we look at our last one where
we're like why this always gets to me
when we get to matrixes because they
don't really say why you multiply
matrixes
um you know my first thought is one
times two four times three but if you
look at this we get one times two plus
four times three one times three plus
four times five
uh six times two plus three times three
six times three plus three times five if
you're looking at these matrixes uh
think of this more as an equation and so
we have if you remember when we back up
here for our multiple line equations
let's just go back up a couple slides
where we were looking at uh two
variables so this is a two variable
equation ax plus b y equals c
and this is a way to make it very quick
to solve these variables and that's why
you have the Matrix and that's why you
do
the multiplication the way they do and
this is the dot product of one times two
plus four times three
one times three plus four times five
six times two plus three times three
six times three plus three times five
and it gives us a nice little uh 14 23
21 and 33 over here which then can be
used and reduced down to a sample
formula as far as solving the variables
as you have enough inputs uh and then in
Matrix operations when you're dealing
with a lot of matrixes now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so we're talking about multiplication
we're talking about solving for
equations when you're finding the
product you are just finding one times
two keep that in mind because that does
come up I've had that come up a number
of times where I'm altering data and I
get confused so what I'm doing with it
uh transpose flipping the Matrix over
its diagonal comes up all the time where
you have you still have 12 but instead
of it being a 12 8 it's now 12 14 8 21.
you're just flipping the columns in the
rows and then of course you can do an
inverse changing the signs of the values
of across this main diagonal and you can
see here we have the inverse a to the
minus 1 and ends up with a set of 12 8
14 12 is now minus 22 minus 12. vectors
Vector just means we have
a value and a Direction
and we have down four numbers here on
our vector
in mathematics a one-dimensional matrix
is called a vector so if you have your X
plot and you have a single value that
value is along the x axis and it's a
single Dimension if you have two
Dimensions you can think about putting
them on a graph you might have X and you
might have y and each value denotes a
direction and then of course the actual
distance is going to be the hypothesis
so that triangle and you can do that
with three dimensionals X Y and Z and
you can do it all the way to nth
Dimensions so when they talk about the K
means for categorizing and how close
data is together they will compute that
based on the Pythagorean theorem so you
would take the square of each value add
them all together and find the square
root and that gives you a distance as
far as where that point is where that
Vector exists or an actual point value
and then you can compare that point
value to another one it makes a very
easy comparison versus comparing
50 or 60 different numbers and that
brings us up to I Gene vectors and I
Gene values uh I Gene vectors the
vectors that don't change their span
while transformation
and hygiene values the scalar values
that are associated to the vectors
conceptually you can think of the vector
as your picture you have a picture it's
um two Dimensions X and Y
and so when you do those two dimensions
and those two values or whatever that
value is
um that is that point but the values
change when you skew it and so if we
take and we have a vector a
and that's a set value uh B is your is
your you have a and b which is your I
Gene Vector 2 is the I Gene value so
we're altering all the values by two
that means we're maybe we're stretching
it out One Direction making it tall if
you're doing picture editing
um that's one of the places this comes
in but you can see when you're
transforming uh your different
information how you transform it is then
your hygiene value and you can see here
a vector after line Transit transition
we have 3A a a is the I Gene Vector
three is the iogene value so a doesn't
change that's whatever we started with
that's your original picture and three
is skewing in One Direction and maybe uh
B is being skewed another Direction and
so you have a nice tilted picture
because you've altered it by those by
the hygiene values
so let's go ahead and pull up a demo on
linear algebra and to do this I'm going
to go through my trusted Anaconda into
my Jupiter notebook
and we'll create a new uh notebook
called linear algebra
since we are working in Python we're
going to use our numpy I always import
that as NP or numpy array probably the
most popular
module for doing matrixes and things in
given that this is part of a series I'm
not going to go too much into numpy we
are going to go ahead and create two
different variables a for a numpy array
10 15 and b 29
. we'll go ahead and run this and you
can see there's our two arrays 10 15 29
and I went and added a space there in
between
so it's easier to read
and since it's the last line we don't
have to put the print statement on it
unless you want we can simply but we can
simply do a plus b so when I run this we
have 10 15 29 and we get 30 24 which is
what you expect 10 plus 20 15 plus 9 you
could almost look at this addition as
being
um
just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do a DOT t
plus b dot T remember that t flips them
and so if we do that we now get them we
now have 30 24 going the other way we
could also do something kind of fun
there's a lot of different ways to do
this
uh as far as a plus b I can also do a
plus b dot t
and you're going to see that that will
come out the same the 3024 whether I
transpose A and B are transpose them
both at the end
and likewise we can very easily subtract
two vectors I can go a minus B
and we run that and we get minus 10 6.
now remember this is the last line in
this particular section that's why I
have to put the print around it
and just like we did before
we can transpose either the individual
or we can transpose the main setup and
then we get a minus 10 6 going the other
way
now we didn't mention this in our notes
but you can also do a scalar
multiplication
and just put down the scalar so you can
remember that
and what we're talking about here is I
have this array here U and if I go a
times U
H we'll take the value 2 we'll multiply
it by every value in here so 2 times 30
is 60 2 times 15
and just like we did before
this happens a lot because when you're
doing matrixes you do need to flip them
you get 60 30 coming this way
so in numpy uh we have what they call
Dot product
and uh with this this is in a
two-dimensional vectors it is the
equivalent of two matrix multiplication
remember we were talking about matrix
multiplication uh where it is the
well let's walk through it
we'll go ahead and start by defining two
numpy arrays we'll have uh 10 20 25 6 or
our U and our V and then we're going to
go ahead and do if we take
the values and if you remember correctly
an array like this would be 10 times 25
plus 20 times 6.
we'll go ahead and print that
there we go
and then we'll go ahead and do the NP
dot dot of U comma
V
and we'll find when we do this we go
ahead and run this we're going to get
370
370. so this is a strain multiplication
where they use it to solve
linear algebra when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
value for our Dot multiplication
and we did addition earlier which is
just your basic addition and of course
the Matrix you can get very complicated
on these or in this case we'll go ahead
and do let's create two complex matrixes
this one is a matrix of
um you know 12 10 4 6 4 31. we'll just
print out a so you can see what that
looks like here's print a
we print a out you can see that we have
a
two by three layer Matrix for a and we
can also put together always kind of fun
when you're playing with print values we
could do something like this we could go
in here
there we go we could print a we have it
end with equals a run and this kind of
gives it a nice look here's your Matrix
that's all this is comma N means it just
tags it on the end that's all all that
is doing on there and then we can simply
add in what is a plus b and you should
already guess because this is the same
as what we did before there's no
difference we do a simple vector
addition we have 12 plus 2 is 14 10 plus
8 is 18 and so on
and just like we did The Matrix addition
we can also do a minus B and do our
Matrix subtraction
and we look at this we have what 12
minus 2 is 10 10 minus eight uh where
are we
oh there we go eight minus ah
confusing what I'm looking at I should
have reprinted out the original numbers
but we can see here 12 minus 2 is of
course 10 10 minus 8 is 2 4 minus 46 is
minus 42 and so forth so same as its
attraction as before we just call it
Matrix subtraction it's identical
now if you remember up here we had a
scalar addition we're adding just one
number
to a matrix you can also do scalar
multiplication and so simply if you have
a single value a and you have B which is
your array we can also do a times B
when we run that you can see here we
have 2 times 4 is 8 5 times 4 is 20 and
so forth you're just multiplying the 4
across each one of these values and this
is an interesting one that comes up a
little bit of a brain teaser is Matrix
and Vector multiplication
and so we're looking at this
we are let's do a regular arrays it
doesn't necessarily have to be a numpy
array we have a
which has our array of arrays and B
which is a single array and so we can
from here
do the DOT
a b
and this is going to return two values
and the verse value is that it's you
could say it's like uh we're doing the
this array B array
first with a and then with a second one
and so it splits it up so you have a
matrix of vector multiplication and you
can mix and match when you get into
really complicated uh back end stuff
this becomes more common because you're
now you've got layers upon layers of
data and so you you'll end up with a
matrix and a set of Bolt Vector matrices
do you want to multiply
now keep in mind that if you're doing
data science a lot of times you're not
looking at this this is what's going on
behind the scenes so if you're in the
site kit looking at SK learn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on
other times you might find yourself
having to do part of this and manipulate
the data around so it fits right and
then you go back in and you run it
through the psy kit and if we can do
um
up here we did a matrix and Vector
multiplication we can also do Matrix to
matrix multiplication and if we run this
where we have the two matrixes you can
see we have a very complicated array
that of course comes out on there for
our DOT and just to reiterate it we have
our transposer Matrix which is your dot
t
and so if we create a matrix a and then
we do transpose it you can see how it
flips it from 5 10 15 20 25 30 to 5 15
25 10 20 30. rows and columns
and certainly with the math this comes
up a lot it also comes up a lot with X Y
plotting when you put in the pi plot you
have one format where they're looking at
Pairs and numbers and then they want L
of x's and all y's so you know the
transpose is an important tool both for
your math and for plotting and all kinds
of things
another tool that we didn't discuss uh
is your identity Matrix
and this one is more definition
uh the identity Matrix we have here one
where we just did two so it comes down
as one zero zero one one zero zero zero
one zero it creates a diagonal of one
and what that is is when you're doing
your identities you could be comparing
all your different features to the
different features and how they
correlate and of course when you have a
feature one compared to feature one to
itself it is always one where usually
it's between zero one depending on how
well correlates so when we're talking
about identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is and then another thing we can
do to kind of wrap this up we'll hit you
with the most complicated uh piece of
this puzzle here is an inverse a matrix
and let's just go ahead and put the um
that's a lengthy description
let's go and put the description this is
straight out of the
the website for numpy so given a square
Matrix a here's our Square Matrix a
which is two one zero zero one zero one
two one keep in mind three by three it's
Square it's got to be equal it's going
to return the Matrix a inverse
satisfying dot a
um a inverse so here's our matrix
multiplication
um and then of course it equals the dot
yeah a inverse of a with an identity
shape of a DOT shape zero this is just
reshaping the identity
that's a little complicated there uh so
we're going to have our here's our array
uh we'll go ahead and run this and you
can see what we end up with is we end up
with uh an array 0.5 minus 0.5 and so
forth with our two one one going down
two one zero zero one zero one two one
um getting into a little deep on the
math understanding when you need this is
probably really is is what's really
important when you're doing data science
versus uh handwriting this out and
looking up the math and handwriting all
the pieces out you do need to know about
the linear algorithm inverse of a so if
it comes up you can easily pull it up or
at least remember where to look it up
you took a look at the algebra side of
it let's go ahead and take a look at the
calculus side of what's going on here
with the machine learning so calculus oh
my goodness and differential equations
you got to throw that in there because
that's all part of the bag of tricks
especially when you're doing large
neural networks but it also comes up in
many other areas the good news is most
of it's already done for you in the back
end so when it comes up you really do
need to understand from the data science
not data analytics data analytics means
you're digging deep into actually
solving these math equations in a neural
network is just a giant differential
equation
uh so we talk about calculus we're going
to go ahead and understand it by talking
about cars versus time and speed
uh so it helps to calculate the
spontaneous rate of change
so suppose we plot a graph with the
speed of a car with respect to time so
as you can see here going down the
highway probably merged into the highway
from an on-ramp so I had to accelerate
so my speed went way up
stuck in traffic merged into the traffic
traffic opens up and I accelerate again
up to the speed limit and maybe Peter's
off up there so you can look at this as
as
the speed versus time I'm getting faster
and faster because I'm continually
accelerating and if I hit the brakes it
go the other way
so the rate of change of speed with
respect to time is nothing but
acceleration how fast are we
accelerating
the acceleration is the area between the
start point of X and the endpoint of
Delta X
so we can calculate a simple if you had
X and Delta X we could put a line there
and that slope of the line is our
acceleration
now that's pretty easy when you're doing
linear algebra but I don't want to know
it just for that line in those two
points I want to know it across the
whole of what I'm working with that's
where we get into calculus
so we talk about the distance between X
and Delta X it has to be the smallest
possible near to zero in order to
approximate the acceleration
so the idea is that instead of I mean if
you ever did took a basic calculus class
they would draw bars down here and you
would divide this area up let's go back
up a screen you divide this area up of
this time period up into maybe 10
sections and you'd use that and you
could calculate the acceleration between
each one of those 10 sections kind of
thing and then we just keep making that
space smaller and smaller until Delta X
is almost
infinitesimally small
and so we get a function of a equals a
limit as H goes to 0 of a function of a
plus h minus a function of a over H and
that is your Computing the slope of the
line
we're just Computing that slope under
smaller and smaller and smaller samples
uh and that's what calculus is calculus
is the integral you can see down here we
have our nice integral sign looks like a
giant s and that's what that means is
that we've taken this down to as small
as we can for that sampling
so we're talking about calculus we're
finding the area under the slope is the
main process in the integration similar
small intervals are made of the smallest
possible length of X Plus Delta X where
Delta X approaches almost an
infinitesimally small space
and then it helps to find the overall
acceleration by summing up all the links
together so we're summing up all the
accelerations from the beginning to the
end
and so here's our integral we sum of a
of x times D of x equals a plus c
that is our basic calculus here
so when we talk about multivariate
calculus
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations
change in W over change of time equals
change of w over change of Z the
differential of Z to DX differential of
x to DT it gets pretty complicated and
it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f of x or y of D of a equals
the sum from C to D and A to B of f of x
or y d x d y equals the sum of a to B
sum of C to D of f x or y d y d x
understanding the very specifics of
everything going on in here and actually
doing the math is use the calculus one
calculus two and differential equations
so you're talking about three
full-length courses to dig into and
solve these math equations what we want
to take from here is we're talking about
calculus we're talking about summing of
all these different slopes and so we're
still solving a linear uh expression
we're still solving y equals m x plus b
but we're doing this for infinitesimally
small x's and then we want to sum them
up that's what this integral sign means
the sum of a of x d of x equals a plus c
and when you see these very complicated
uh multivariate differentiation using
the chain rule when we come in here and
we have the change of w to the change of
T equals a change of w DZ uh and so
forth that's what's going on here that's
what these means we're basically looking
for the area under the curve which
really comes to how is the change
changing speeds going up how is that
changing and then you end up with a
multiple layer so if I have three layers
of neural networks how is a third layer
changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
multivariate integration with integrals
the good news is we can solve this
mathematically and that's what we do
when you do neural networks and reverse
propagation so the nice thing is that
you don't have to solve this on paper
unless your data analysis and you're
working on the back end of integrating
these formulas and building the script
to actually build them so we talk about
applications of calculus it provides us
the tools to build an accurate
predictive model so it's really behind
the scenes we want to guess at what the
change or the change of the change is
that's a little goofy I I know I just
threw that out there's kind of a meta
term but if you can guess how things are
going to change then you can guess what
the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables
so there's our multiple variables going
in there if one variable is changing how
does it affect the other variable
and then in gradient descent calculus is
used to find the local and Global Maxima
and this is really big we're actually
going to have a whole section here on
gradient descent because it is really I
mean I talked about neural networks and
how you can see how the different layers
go in there but gradient descent is one
of the most key things for trying to
guess the best answer to something
so let's take a look at the code behind
gradient descent and before we open up
the code let's just do real quick
gradient descent
let's say we have a curve like this and
most common
is that this is going to represent your
error oops
error there we go error uh hard to read
there and I want to make the error as
low as possible
and so when I'm looking at it is I want
to find this line here
which is the minimum value so we're
looking for the minimum
and it does that by sampling there
and then it based on this it guesses it
might be someplace here and it goes hey
this is still going down it goes here
and then goes back over here and then
goes a little bit closer and it's just
playing a high low until it gets to that
spot that bottom spot
and so we want to minimize the error in
on the flip note you could also want to
be maximizing something you want to get
the best output of it that's simply
minus the value so if you're looking for
where the peak is this is the same as a
negative for where the valley is I'm
looking for that Valley that's all that
is and this is a way of finding it so
the cool thing is all the heavy
lifting's done I actually ended up
putting together one of these a while
back as when I didn't know about
sidekick and I was just starting boy
it's a long while back and uh is playing
high low how do you play high low not
get stuck in The Valleys figure out
these curves and things like that well
you do that and the back end is all the
calculus and differential equations to
calculate this out
the good news is you don't have to do
those
so instead we're going to put together
the code and let's go ahead
and see what we can do with that
so uh guys in the back put together a
nice little piece of code here which is
kind of fun uh some things we're gonna
note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
gonna find these things are stumbling
blocks the first one is current X where
do we start at keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start and we started at this
because the algorithm starts at x equals
three so we arbitrarily picked five
learning rate is how many bars to skip
going one way or the other I'm in fact
I'm going to separate that a little bit
because these two are really important
if we're dealing with something like
this where we're talking about well
here's our here's the function we're
going to use our gradient of our
function 2 times X plus five keep it
simple so that's a function we're going
to work with so if I'm dealing with
increments of a thousand point one is
going to be a very long time and if I'm
dealing with increments of 0.001
0.1 is going to skip over my answer so I
won't get a very good answer and then we
look at Precision this tells us when to
stop the algorithm so again very
specific to what you're working on if
you're working with money
and you don't convert it into a float
value you might be dealing with 0.01
which is a penny that might be your
Precision you're working with
and then of course the previous step
size Max iterations we want something to
cut out at a certain point usually
that's built into a lot of minimization
functions and then here's our actual
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
itters is less than Max and Max itters
say that 10 times fast
um
we're just saying if it's uh if we're if
we're still greater than our Precision
level we still got to keep digging
deeper and then we also don't want to go
past a thou or whatever this is a
million or 10 000 running that's
actually pretty high I almost never do
Max iterations more than like a hundred
or two hundred rare occasions you make
up to four or five hundred if it's uh
depending on the problem you're working
with uh so we have our previous equals
our current that way we can track time
wise uh the current now equals the
current minus the rate times the formula
of our previous X
so now we've generated our new version
uh previous step size equals the
absolute current previous
so we're looking for the change in x
errors equals iterations plus one that's
how we know to stop if we get too far
and then we're just going to print the
local minimum occurs at axon here and if
we go ahead and run this
uh you can see right here it gets down
to this point and it says hey
um local minimum is minus
3.322 for this particular series we
created and this is created off of our
formula here Lambda x 2 times X plus
five now
when I'm running this stuff you'll see
this come up a lot
in with the sklearn kit and one of the
nice reasons of breaking this down the
way we did is I could go over those top
pieces those top pieces are everything
when you start looking at these
minimization toolkits in built-in code
and so from
um we'll just do it's actually Docs
Dot
scipy.org and we're looking at
the scikit
there we go optimize minimize
you can only minimize one value you have
the function that's going in this
function can be very complicated so we
use the very simple function up here it
could be there's all kinds of things
that could be on there and there's a
number of methods to solve this as far
as how they shrink down and your X
naught there's your there's your start
value so your function your start value
there's all kinds of things that come in
here that you can look at which we're
not going to
optimization automatically creates
constraints bounds some of this it does
automatically but you really the big
thing I want to point out here is you
need to have a starting point you want
to start with something that you already
know is mostly the answer if you don't
then it's going to have a heck of a time
trying to calculate it out
or you can write your own little script
that does this and does a high low
guessing and tries to find the max value
that brings us to statistics what this
is kind of all about is figuring things
out a lot of vocabulary and statistics
ah so statistics well I guess it's all
relative it's definitely not an adult
class so a bunch of stuff going on
statistics statistics concerns with the
collection organization analysis
interpretation and presentation of data
that is a mouthful
um so we have from end to end where
where does it come from is it valid what
does it mean how do we organize it how
do we analyze it and then you got to
take those analysis and interpret it
into something that uh people can use
kind of reduce it to understandable and
nowadays you have to be able to present
it if you can't present it then no one
else is going to understand what the
heck you did
so we look at the terminologies uh there
is a lot of terminologies depending on
what domain you're working in so clearly
if you're working in a domain that deals
with
viruses and T cells and and how does you
know where does that come from and
you're studying the different people
then you can have a population if you
are working with
um
mechanical gear you know a little bit
different if you're looking for the
wobbling statistics to know when to
replace a rotor on a machine or
something like that that can be a big
deal you know we have these huge fans
that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit
uh an interesting project that came up
for our city a while back uh so
population all objects are measurements
whose properties are being observed uh
so that's your population all the
objects it's easy to see it with people
because we have our population in large
but in the case of these sewer fans
we're talking about how the fan units
that's the population of fans that we're
working with
you have a parameter a matrix that is
used to represent a population or
characteristic
you have your sample a subset of the
population studied you don't want to do
them all because then you don't have a
if you come up with a conclusion for
everyone you don't have a way of testing
it so you take a sample sometimes you
don't have a choice you can only take a
sample of what's going on you can't
study the whole population and a
variable a metric of interest for each
person or object in a population
types of sampling we have a
probabilistic approach selecting samples
from a larger population using a method
based on the theory of probability
and we'll go into a little bit more
deeper on these we have random
systematic stratified and then you have
non-probabilistic approach selecting
samples based on the subjective Judgment
of the researcher rather than random
selection it has to do with convenience
trying to reach a quota or snowball
uh and they're very biased that's one of
the reasons you'll see this big stamp on
it says biased uh so you gotta be very
careful on that
so probabilistic sampling uh when we
talk about a random sampling we select
random size samples from each group or
category so we it's as random as you can
get uh we talk about systematic sampling
we're selecting random size samples from
each group or category with a fixed
periodic interval
uh so we kind of split it up this would
be like a Time setup or our different
categories and you might ask your
question what is a category or a group
uh if you look at I'm going to go back a
window let's say we're studying
economics of different of an area
we know pretty much that based on their
culture where they came from they might
need to be separated and so uh and when
I say separated I don't mean separated
from their their place where they live I
mean as far as the analysis we want to
look at the different groups and make
sure they're all represented
so if we had like an 80 uh of a group
that is uh say Hispanic and or Indian
and also in that same area we have 20 20
who are let's call our expatriates they
left America and they're nice and uh
your Caucasian group we might want to
sample a group that is representative of
both uh so we're talking about
stratified sampling and we're talking
about groups those are the groups we're
talking about and that brings us to
stratified sampling selecting
approximately equal size samples from
each group or category
uh this way we can actually separate the
categories and give us an insight into
the different cultures and how that
might affect them in that area
so you can see these are very very
different kind of depends on what you're
working with as far as your data and
what you're studying and so we can see
here just a little bit more we'd have
selecting 25 employees from a company of
250 employees randomly don't care
anything about them what groups are in
which office they're in nothing
uh and we might be selecting one
employee from every 50 unique employees
in a company of 250 employees and then
we have selecting one employee from
every branch in the company office so we
have all the different branches there's
our group or our categories by the
branch and the category could depend on
what you're studying so it has a lot of
variation on there you see this kind of
grouping and categorizing is also used
to generate a lot of misinformation
uh so if you only study one group and
you say this is what it is then
everybody assumes that's what it is for
everybody and so you've got to be very
careful of that and it's very unethical
thing to kind of do
so types of Statistics we talk about
statistics we're going to talk about
descriptive and inferential statistics
there are so many different terms and
statistics to break it up uh so we so
we're talking about a particular
setup
so we're talking about descriptive and
inferential uh statistics the base of
the word describe is pretty solid you're
describing the data what does it look
like with inferential statistics we're
going to take that from the small
population to a large population so if
you're working with a drug company you
might look at the data and say these
people were helped by this drug they did
80 better as far as their health or 80
percent better survival rate than the
people
who did not have the drug so we can
infer that that drug will work in the
greater populace and will help people so
that's where you get your inferential so
we are predicting how it's going to
affect the greater population
so descriptive statistics it is used to
describe the basic features of data and
form the basis of quantitative analysis
of data
so we have a measure of central
Tendencies we have your mean median and
mode
and then we have a measure of spread
like your range your interquartile range
your variance in your standard deviation
and we're going to look at all these a
little deeper here in a second but one
of them you can think of is
how the data difference
differences you know what's the max Min
range all that stuff is your spread and
anything that's just a single number is
usually your central Tendencies measure
of central tendencies
so we talk about the mean it is the
average of the set of values considered
what is the average outcome of
whatever's going on
and then your median separates the
higher half and the lower half of data
so where's the center point of all your
different data points so your mean might
have some a couple really big numbers
that skew it so that the average is much
higher than if you took those outliers
out where the median would by separating
the high from the low might give you a
much lower number you might look at and
say oh that's that's odd Y is the
average so much higher than the median
well it's because you have some outliers
or why is it so much lower
and then the mode is the most frequent
appearing value this is really
interesting if you're studying economics
and how people are doing you might find
that the most common
income like in the U.S was at 1.24
000 a year
where the average was closer to 80 000
and it's like wow what a difference well
there's some people have a lot of money
and so that skews that way up so the
average person is not making that kind
of money and then you look at the median
income and you're like well the median
income is a little bit closer to the
average so it does create a very
interesting way of looking at the data
again these are all uh Central
Tendencies single numbers you can look
at for the whole spread of the data
and we look at the measure of central
Tendencies the mean is the average marks
of a student's in a classroom so here we
have the mean some of the marks of the
students total number of students and as
we talked about the median if we have 0
through 10 and we take half the numbers
and put them on one side of the line
half the numbers on the other side of
the line we end up with five in the
middle and then the mode what Mark was
scored by most of the students in a test
in a simple case where most people
scored like an 82 percent and got
certain problems wrong easy to figure
out uh not so easy when you have
different areas where like you have like
the um oh let's go back to economy a
little bit more difficult to calculate
if you have a large group of scores that
makes 30 000 and a slightly bigger group
that makes twenty six thousand so what
do you put down for the mode certainly
there's a number of ways to calculate
that and there's actually a different
variations depending on what you're
doing so now we're looking at a measure
of spread uh range what's the difference
between the highest and the lowest value
first thing you want to look at you know
it's uh we had everybody in the test
scored between 60 and 100 percent so we
got 100 or maybe 60 to 90 percent it was
so hard that a lot of people could not
get a hundred percent
um you have your interquartile range
quartiles divide a rank ordered data set
into four equal parts
very common thing to do is part of all
the basic packages whether you're
working in uh data frames with pandas
whether you're working in Scala whether
you're working in R you'll see this come
up where they have range your Min your
Max and then it'll have your
interquartile range how does it look
like in each quarter of data variants
measures how far each number in the set
is from the mean and therefore from
every other number in the set
uh so you have like how much turbulence
is going on in this data
and then the standard deviation it is a
measure the variance or the dispersion
of a set of values from the mean
and you'll usually see if I'm doing a
graph I might have the value graphed and
then based on the the error I might grab
graph the standard deviation in the
error on the graph as a background so
you can see how far off it is so
standard deviation is used a lot
so measurement of spread marks of a
student out of 100 we have here from 50
to 63 or 50 to 90 so the range maximum
marks minimum marks we have 90 to 45 and
the spread of that is 45 90 minus 45 and
then we have the interquartile range
using the same marks over there you can
see here where the median is and then
there's a first quarter the second
quarter and the third quarter based on
splitting it apart by those values and
to understand the variance and standard
deviation we first need to find out the
mean so here's our you know calculating
the average there we end up
approximately 66 for the average and
then we look at that in the variance
once we know the means we can do equals
the marks minus the mean squared Y is a
squared because one you want to make
sure it's you don't have like if you if
you're putting all this stuff together
you end up with an error as far as one's
negative one's positive one's a little
higher one's a little lower so you
always see the squared value and over
the total observations and so the
standard deviation equals the square
root of the variance which is
approximately 16. and if you were
looking at a predictable model you would
be looking at the deviation based on the
error how much error does it have that's
again really important to know if you're
if your prediction is predicting
something what's a chance of it being
way off or just a little bit off
now that we've looked at the tools as
far as some of the basics for doing your
statistics what we're talking about
let's go ahead and pull up a little demo
and show you what that looks like in
Python code so you can get some little
Hands-On here for that let's go back
into our Jupiter notebook in Python now
almost all of this you can do in numpy
last time we worked in numpy this time
we're going to go ahead and use pandas
and if you remember from pandas on here
this is basically a data frame rows
columns let's just go ahead and do a
print df.head
and run that
and you can see we have the name Jane
Michael William Rosie Hannah sat in
their salaries on here and of course
instead of having to do all those hand
calculations and add everything together
and divide by the total we can do
something very simple on this uh like
use the command mean in pandas and so if
I go ahead and do this print DF pick our
column salary because we want to find
the means of that calorie
we want to find the means of that column
and we go ahead and print this out and
you can see that the average income on
here is 71
000.
and let's just go ahead and do this
we'll go ahead and put in means
and if we're going to do that we also
might want to find the median
and the median is very similar
except it actually is just median we're
used to means in average it's kind of
interesting that those are they use the
two different words
there can be in some computation slight
differences but for the most part the
means is the average and then the median
oops let's put a
median here do you have salary that way
it displays a little better we can see
the median is 54
000. so the halfway mark is
significantly below the average why
because we have somebody in here who
makes 189 000. darn you Rosie for
throwing off our numbers but that's
something you'd want to notice this is
this is the difference between these is
huge and so is what is the meaning
behind that when you're studying a
populist and looking at the different
data coming in and of course we also
want to find out hey what's the most
common
income that people make
in this little tiny sample and so we'll
go ahead and do the mode
and you can see here with the mode uh
it's at fifty thousand
so this is this is very telling that
most people are making 50 000 the middle
point is at fifty four thousand so half
the people are making more than that
what that tells me is that if the most
common income is way is below the median
then
there's a few there's a you know there's
a lot of high salaries going up but
there's some really low salaries in
there and so this trend which is very
common in statistic and when you're in
analyzing the economy in different
people's income is pretty common and the
bigger difference between these is also
very important when we're studying
statistics and when you hear someone
just say hey the average income was you
might start asking questions at that
point why aren't you talking about the
median income why aren't you talking
about the mode the most common income
what are you hiding and if you're doing
these analysis you should be looking at
these saying hey why is this
discrepancies why are these so different
and of course with any analysis it's
important to find out the minimum
and the maximum so we'll go ahead it's
just simply uh
um dot min so pull up your minimum and
then dot Max pulls up the maximum pretty
straightforward on as far as
translating it and knowing what you're
you know put the your lowest value and
what your highest value is here which
you'll use to generate like a spread
later on and real quick on no mode note
that it puts mode zero like I said
there's a couple different ways you can
compute the mode
um although the standard one's pretty
good we can of course do the range which
is your max minus your Min so now we
have a range of 149 000 between the
upper end and the lower end and you
might want to be looking up the
individual values on all of these but
turns out there is a describe
feature in pandas
and so in pandas we can actually do DF
salary describe and if we do this you
can see we have that there's seven
setups here's our mean our standard
deviation which we didn't compute yet
which would just be a DOT STD and you
gotta be a little careful because when
it computes it it looks for axes and
things like that we have our minimum
value and here's our quartiles
our maximum value and then of course the
name salary uh so these are these are
the basic statistics you can pull them
up and like just describe this is a
dictionary so I could actually do
something like um
and here I could actually go uh count
and run and now it just prints the count
so because this is a dictionary you can
pull any one of these values out of here
it's kind of a quick and dirty way to
pull all the different information and
then split it up and depending on what
you need now if I just walked in and
gave you this information in a meeting
at some point you would just kind of
fall asleep that's what I would do
anyway
um so we want to go ahead and see about
graphing it here we'll go ahead and put
it into a histogram and plot that graph
on it
of the salaries and let's just go ahead
and put that in here so we do our
matplot inline remember that's a
Jupiter's notebook thing a lot of the
new version of the matplot library does
it automatically but just in case I
always put it in there import matplot
library pipelot is PLT that's my
plotting
and then we have our data frame I guess
I really don't need to respell the data
frame maybe we could just remind
ourselves what's in it so we'll go ahead
and just print
DF that way we still have it and then we
have our salary DF salary salary.plot
history title salary distribution color
gray
uh plot axvline salary the mean value so
we're going to take the mean value
color violet line style Dash this is
just all making it pretty uh what color
dashed line line width of two that kind
of thing and the median and let's go
ahead and run this just so you can see
what we're talking about
and so up here we are taking on our plot
so here's the data here's our our data
frame printed out so you can see it with
the salaries we'll look at the salary
distribution and just look at this the
way there the salary is distributed
um you have our in this case we did
Let's see we had red for the median
we have violet
for our average or mean and you can just
see how it really here's our outlier
here's our person who makes a lot of
money here's the average and here's the
median
um and so as you look at this you can
say wow based on the average it really
doesn't tell you much about what people
are really taking home all it does is
tell you how much money is in this you
know what the average salary is
so some of the things you want to take
away in addition to this is that it's
very easy to plot
um
an axv line these are these up and down
lines for your markers
um and as you just just play the data I
mean you can add all kinds of things to
this and get really complicated keeping
it simple is pretty straightforward I
look at this and I can see we have a
major outlier out here we can definitely
do a histogram and stuff like that but
you know pictures worth a thousand words
what you really want to make sure you
take away is that we can do a basic
describe which pulls all this
information out and we can print any of
the individual information from the
describe because this is a dictionary
and so if we want to go ahead and look
up
the mean value we can also do describe
mean so if you're doing a lot of
Statistics being able to
doesn't have the print on there so it's
only going to print the last one which
happens to be the mean you can very
easily reference any one of these and
then you can also if you're doing
something a little bit more complicated
and you don't need just the basics you
can come through and pull any one of the
individual
um
references from the from the pandas on
here so now we've had a chance to
describe our data uh let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you can see here we have a nice little
picture movie ratings and
if we took this group of people and said
hey how many people like the movie
dislike it can't say and then you ask
just a random person who comes out of
the movie who hasn't been in this study
you can infer that 55 chance of saying
liked 35 chance of saying disliked or a
10 or 11 chance of can't say so that's
real basics of what we're talking about
is you're going to infer that the next
person is going to follow these
statistics
uh so let's look at Point estimation it
is a process of finding an approximate
value for a population's parameter like
mean or average from random samples of
the population let's take an example of
testing vaccines for covid-19 vaccines
and flu bugs all that it's a pretty big
thing of how do you test these out and
make sure they're going to work on the
populace
a group of people are chosen from the
population medical trials are performed
results are generalized for the whole
population so here's a protected there's
our small group up here where we've
selected them we run medical trials on
them and then the results work for the
population a nice diagram with the
arrows going back and forth in the very
scary coveted virus in the middle of one
and let's take a look at the
applications of inferential statistics
very Central is what they call
hypotheses testing and the confidence
interval which go with that and then as
we get into
probability we get into our binomial
theorem our normal distribution in
central limit theorem
hypothesis testing hypothesis testing is
used to measure the plausibility of a
hypothesis assumption by using sample
data
now when we talk about theorems Theory
hypothesis keep in mind that if you are
in a philosophy class theory is the same
as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things are up to debate so a hypothesis
is the same as a philosophical class
calling a theory where theory in science
is not the same theory in science says
this has been well proven gravity is a
theory so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collapsing in your
area that is a philosophical debate very
important I've heard people mix those up
and it is a pet peeve of mine when we
talk about hypotheses testing the steps
involved in hypotheses testing is first
we formulate a hypothesis we figure out
the right test to test our hypothesis we
execute the test and we make a decision
 and so when you're talking about a
hypothesis you're usually trying to
disprove it if you can't disprove it and
it works for all the facts then you
might call that a theorem at some point
so in a use case let's consider an
example we have four students we're
given a task to clean a room every day
sounds like working with my kids they
decided to distribute the job of
cleaning the room among themselves they
did so by making four chits which has
their names on it and the name that gets
picked up has to do the cleaning for
that day Rob took the opportunity to
make chits and wrote everyone's name on
it so here's our four people Nick Rob
imlia imlia and summer
now Rick Emily and summer are asking us
to decide whether Rob has done some
Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits
so the probability of Rob not doing work
on day one is three out of four there's
a 0.75 chance that he didn't do work uh
two days three fourths times
three-fourths equals 0.56
three days you have three four three
fours three-fourths which equals 0.42
uh when you get to day 12 it's 0.032
Which is less than 0.05 remember this
0.05 that comes up a lot when we're
talking about certain values when we're
looking at statistics Rob is cheating as
he wasn't chosen for 12 consecutive days
that's a very high probability when on
day 12 he still hasn't gotten the job
cleaning the room
so we come up to our important important
terminologies we have null hypothesis
a general statement that states that
there is no relationship between two
measured phenomenon or no association
among the groups
alternative hypothesis contrary to the
null hypothesis it states whenever
something is happening a new theory is
preferred instead of an old one and so
the two hypotheses go hand in hand so
your null this is always interesting in
in when talking about data science and
the math behind it it's about proving
that the things have no correlation null
hypothesis says these two have zero
relation to each other where the
alternative hypothesis says hey we found
a relation this is what it is
we have p-value the p-value is a
probability of finding the observed or
more extreme results when the null
hypotheses of a study question is true
and the T value it is simply the
calculated difference represented in
units of standard error the greater the
magnitude of T the greater the evidence
against the null hypothesis and you can
look at the T values being specific to
the test you're doing
where the p-value is derived from your T
value and you're looking for what is
called a five percent or the 0.05
showing that it has a high correlation
so digging in deeper let's assume that a
new drug is developed with the goal of
lowering the blood pressure more than
the existing drug and this is a good one
because the null value here isn't that
you don't have any drug then null value
here is it is better than existing drug
the new drug doesn't lower the blood
pressure more than the existing drug
now if we get that that says our null
hypothesis is correct there is no
correlation and the new drug is not
doing its job the alternative hypothesis
the new drug does significantly lower
the blood pressure more than an existing
drug uh yay we got a new drug out there
and that's our alternative hypothesis or
the H1 or h a
and we look at the p-value results from
the evidence like medical trials showing
positive results which will reject the
null hypothesis
and again they're looking for a 0.05 or
5 percent and the T value comparing all
the positive test results and finding
means of different samples in order to
test hypothesis so this is specific to
the test how what percentage of increase
did they have
and this leads us to the confidence
intervals a confidence interval is a
range of values we are sure our true
values of observations lie in
let's say you asked a dog owner around
you and asked them how many cans of food
do you buy for your per year for your
dog
through calculations you got to know
that the on an average around 95 percent
of the people bought around 200 to 300
cans of food hence we can say that we
have a confidence interval of 2 300
where 95 percent of our values lie in
that data spread and this the graph
really helps a lot so you can start
seeing what you're looking at here where
you have the 95 percent you have your
peak in this case it's a normal
distribution so you have the nice bell
curve equal on both sides it's not
asymmetrical and 95 percent of all the
values lie within a very small range and
then you have your outliers the 2.5
percent going each way
so we touched upon hypothesis and we're
going to move into probability so you
have your hypothesis once you've
generated your hypothesis we want to
know the probability of something
occurring probability is a measure of
the likelihood of an event to occur any
event can be predicted with total
certainty and can only be predicted as a
likelihood of its occurrence so any
event cannot be predicted with total
certainty can only be predicted as a
likelihood of its occurrence score
prediction how good you're going to do
in whatever sport you're in weather
prediction stock prediction if you've
studied physics and Chaos Theory even
the location of the chair you're sitting
on has a probability that it might move
three feet over
granted that probability is one in like
uh I think we calculated as under one in
trillions upon trillions so it's
the better the probability the more
likely it's going to happen there are
some things that have such a low
probability that we don't see them so we
talk about random variable a random
variable is a variable whose possible
values are numerical outcomes of a
random phenomena so we have the coin
tossed how many heads will occur in the
series of 20 coin flips probably you
know the on average they're 10 but you
really can't know because it's very
random how many times a red ball is
picked from a bag of balls if there's
equal number of red balls and blue balls
and green balls in there how many times
the sum of digits on two dice result are
five each
so you know there's how often you're
going to roll two fives on your pair of
Dives
so in a use case let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values two three four five six
seven eight nine ten eleven twelve
so we have a random variable in a
combination of dice and instead of
looking at how many times both dice for
roll five let's go ahead and look at
total sum of five and you have in as far
as your random variables you can have a
one four equals five four one two three
three two
so four of those roles can be four if
you look at all the different options
you have four of those random roles can
be a five
and if we look at the total number
which happens to be 36 different options
uh you can see that we have four out of
36 chance every time you roll the dice
that you're going to roll a total of
five you're gonna have an outcome of
five
and uh we'll look a little deeper as to
what that means but you could think of
that at what point if someone never
rolls a five or they always roll a five
can you say hey that person's probably
cheating we'll look a little closer at
the math behind that but let's just
consider this is one of the cases is
rolling two dice and gambling
there's also a binomial distribution it
is the probability of getting success or
failure as an outcome in an experiment
or trial that is repeated multiple times
and the key is is by meaning two
binomial so passing or filling an exam
winning or losing a game and getting
either head or tails so if you ever see
binomial distribution it's based on a
true false kind of setup you win or lose
let's consider a use case and let's
consider the game of football between
two clubs Barcelona and Dortmund the
teams will have to play a total of four
matches and we have to find out the
chances of Barcelona winning the series
so we look at the total games and we're
looking at five different games or
matches let's say that the winning
chance for Barcelona is 75 or 0.75 that
means that each game they have a 75
chance that they're going to win that
game and losing chances are 25 or 0.25
clearly 0.75 plus 0.25 equals one so
that accounts for 100 of the game
probability for getting K wins in in
matches is calculated
and we we're talking like so if you have
five games uh and you want to know if I
play
um how many wins in those five games
should I get what's a percentage on
those and the probability for getting K
wins and N matches is calculated by p x
equals k equals nck P to the k q to the
N minus K here p is a probability of
success and Q is the probability of
failure and so we can do total games of
n equals five where k equals zero one
two three four five P which is the
chance of winning is 0.75 Q the chance
of losing equals 1 minus P which equals
1 minus 0.075 which equals 0.25 the
probability that Barcelona will lose all
of the matches can then just plug in the
numbers and we end up with a point zero
zero zero nine seven six five six two
five so very small chance they're going
to lose all their matches
and we can plug in the value for two
matches probability of the Barcelona
will win at least two matches is 0.0878
and of course we can go on to the
probability of the Barcelona will win
three matches the 0.26 and of course
four matches and so on and it's always
nice to take this information
um and let's find the accumulated
discrete probabilities for each of the
outcomes where Barcelona has won three
or more matches x equals three x equals
four x equals five
and we end up with the p equals 0.264
plus 0.395 plus 237 which equals 0.89
in reality
the probability of Barcelona winning the
series is much higher than 0.75 and it's
always nice to uh
put out a nice graph so you can actually
see the number of wins to the
probability and how that pans out with
our binomial case
continuing in our important terminology
location the location of the center of
the graph depends on the mean value and
this is some very important things so
much of the data we look at and when you
start looking at probabilities almost
always has a normalized look like the
graph in the middle
uh but you do have left skewed where the
data is skewed off to the left and you
have more stuff happening out to the
left and you have right skewed data and
so when this comes up and these
probabilities come up where they're
skewed it's really important to take a
closer look at that mostly you end up
with a normalized set of data but you
got to also be aware that sometimes it's
a skewed data
and then the height height of the slope
inversely depends upon the standard
deviation
so you can see down here the standard
deviation is really large it kind of
squishes it out and if the standard
deviation is small then most of your
data is going to hit right there in the
middle you can have a nice Peak and so
being aware of this that you might have
a probability that fits certain data but
it has a lot of outliers so you're if
you have a really high standard
deviation
if you're doing stock market analysis
this means your predictions are probably
not going to make you much money where
if you have a very small deviation you
might be right on Target and set to
become a millionaire which leads us to
the z-score z-score tells you how far
from the mean a data point is it is
measured in terms of standard deviations
from the mean around 68 percent of the
results are found between one standard
deviation around 95 percent of the
results are found between two standard
deviations and you read the symbols of
course they love to throw some Greek
letters in there we have mu minus two
Sigma mu is just a quick way it's that
kind of funky you it just means the mean
and then the sigma is the standard
deviation and that's the o with that
little arrow off to the right or the
little waggly Tail Going up the o with
it with a line on it so mu minus 2 Sigma
is your 95 of the results are found
between two standard deviations
Central limit theorem this goes back to
the skew if you remember we were looking
at the skew values on this previous
slide have left skewed normalized and
right skewed when we're talking about it
being skewed or not skewed the
distribution of the sample means will be
approximately normally distributed
evenly distributed not skewed if you
take large random samples from the
population with the mean mu and the
standard deviation Sigma with
replacement
and you can see here of course we have
our mu minus two Sigma and the spread
down here the mean the median and the
mode and so you're talking about very
large populations
these numbers should come together and
you shouldn't have a skewed value if you
do that's a flag that something's wrong
that's why this is so important to be
aware of what's going on with your data
where your samples are coming from and
the math behind it and if you're going
to do all this we got to jump into
conditional probability
the conditional probability of an event
a is a probability that the event will
occur given the knowledge that an event
to be has already occurred and you'll
see this as Bayes theorem b-a-y-e-s Bays
and this is red
I mean you have these funky looking
little p brackets a b this is the
probability of a being true while B is
already true
and you have the probability of B being
true when a is already true so P B of A
probability of a being true divided by
the probability of B being true
and we talk about Bayes theorem which
occurred back in the 1800s when he
discovered this this is such an
important formula and it's really it's
not if you actually do the math you
could just kind of do
um
um x y equals J K and then you divide
them out and you're going to see the
same math but it works with
probabilities which makes it really nice
and so if you have a set you might have
uh eight or nine different studies going
on in different areas different people
have done the studies they brought them
together
if we look at today's covet virus the
virus spread certainly the studies done
in China versus the studies the way
they're done in the U.S that data is
different in each of those studies but
if you can find a place where it
overlaps where they're studying the same
thing together you can then compute the
changes that you need to make in one
study to make them equal
and this is also true if you have a
study of one group and you want to find
out more about it so this formula is
very powerful and it really has to do
with the data collection part of the
math and data science and understanding
where your data is coming from and how
you're going to combine different
studies in different groups
and we're going to go into a use case
let's find out the chance of a person
getting lung disease due to smoking and
this is kind of interesting the way they
word this let's say that according to
medical report provided by the hospital
states that around 10 percent of all
patients they treated suffered lung
disease so we have kind of a generic
medical report they further found out by
a survey that 15 percent of the patients
that visit them smoke
so we have 10 percent that are lung
disease and 15 percent of the patients
smoke and finally five percent of the
people continued smoke even when they
had lung disease not the brightest
choice but you know it is an addiction
so it can be really difficult to kick
and so we can look at the probability of
a uh prior probability of 10 people
having lung disease
and then probability B probability that
a patient smokes is 15 percent
uh and the probability of B if B then a
the probability of a patient smokes even
though they have lung disease is five
percent
and probability of a is B probability
that the patient will have lung disease
if they smoke and then when you put the
formulas together you get a nice
solution here you get the probability of
a of B probability that the patient will
have lung disease if they smoke
and you can just plug the numbers right
in and we get a 3.33 percent chance
hence there is a 3.33 chance that a
person who smokes will get a lung
disease
so we're gonna pull up a little python
code and we're always my favorite roll
up the sleeves
keep in mind we're going to be doing
this
um kind of like the back end way
so that you can see what's going on and
then later on we're going to create
um we'll get into another demo which
shows you some of the tools that are
already pre-built for this
let's start by creating a set so we're
going to create a set with curly braces
this means that our set has only unique
values so you have a list you have your
tuples which can never change and then
you have in this case the the set so
four seven you can't create a four seven
comma four it'll delete the four out so
it's only unique values and if you use
dictionaries
quick reminder this should look familiar
because it is a dictionary where you
have a value and that value is assigned
to or that key is assigned to a value
uh so you could have a key value set up
as a dictionary so it's like a
dictionary without the value it's just
the keys and they all have to be unique
and if we run this we have a set of four
seven
we can also take a list of regular setup
and I'm going to go ahead and just throw
in another number in here four
and run it and you can see here if I
take my list one two three four four and
I convert it to a set and here it is my
set from list equals set my list
the result is one two three four so it
just deletes that last four right out of
there
and with the sets you can also go in
there and print here is my set my set
three is in the set and then if you do
three in my set
that's going to be a logic function and
one in my set 6 is not in the set and so
forth if we run this
we get three is in the set true one is
in the set false because three five
seven is another one six is in the Set
uh six is not in the set so not in my
set
you can also use this with a list we
could have just used 357 and it would
have the same response on there is three
and usually you do if three is in but
three in my set is still works on just a
regular list
and we'll go ahead and do a little
iteration we're going to do kind of the
dice one remember
one two three four five six and so we're
going to bring in that iteration tool
and import product as product
and I'll show you what that means in
just a second so we have our two dice we
have dice a
and it's going to be a set of values and
you can only have one value for each one
that's why they put it in a set and if
you remember from range it is up to
seven so this is going to be one two
three four five six it will not include
the seven and the same thing for our
dice B
and then we're going to do is we're
going to create a list
which is the product
of A and B so what's a plus b
and if we go ahead and run this it'll
print that out and you'll see in this
case when they say product because it's
an iteration tool
we're talking about creating a tuple of
the two so we've now created a tuple of
all possible outcomes of the dice where
dice a is one two three one to six and
dice B is one to six and you can see one
to one one to two one to three and so
forth you remember we had a slide on
this earlier where we talked about
um the different all the different
outcomes of a dice
we can play around with this a little
bit uh we can do in dice equals two dice
faces one two three four five six
uh another way of doing what we did
before and then we can create an event
space where we have a set which is the
product of the dice faces repeat equals
end Dice and we're going to just run
this
and you can see here it just again puts
it through all the different possible
variables we can have
and then if we want to take the same set
on here and print them all out like we
had before we can just go through four
outcome and event space outcome and
equals
so the event space is creating
uh sequence and as you can see here when
we print it out it Stacks them versus
going through and putting them in a nice
line
and we'll go ahead and do something
let's go print
since we have the End Printing with a
comma that just means it's just gonna
it's not going to hit the return going
down to the next line and we'll go ahead
and do the links
of our event space uh I'll be an
important variable we're going to want
to know in a minute
and of course if I get carried away with
my typing of length I will print it
twice and it'll give me an error so we
have 36 different possible variations
here
and we might want to calculate something
like
um what about the multiple of three what
if we want to have
the probability of the multiple of three
in our setup
and so we can put together the code for
the outcome and event space of X Y
equals outcome if X Plus y
remainder three so we're going to divide
by 3 and look at the remainder and it
equals zero
then it's a fable outcome we're going to
pop that outcome on the end there
and we'll turn it into a set so the
favor outcome equals a set not necessary
because we know it's not going to be
repeating itself but just in case we'll
go ahead and do that
and if we want to print out the outcome
we can go ahead and see what that looks
like and you can see here these are all
multiples of three one plus two is three
five plus four is nine which divided by
three is three and so forth
and just like we looked up the length of
the one before let's go ahead and print
the length
of our F outcome
so we can see what that looks like
there we go
and of course I did forget to add the
print in the middle because We're
looping through and putting an end on
the on the setup on there so we're going
to put the print in there and if I run
this you can see uh
we end up with 12. so we have 36 total
options
we have 12 that are multiple that add up
to a multiple of three
and we can easily convert the
probability of this by simply taking the
length of our favorable outcome over the
length of the event space
and if we print it out let me put that
in there probability
last line so we just type it in we end
up with the 0.33333 chance
and it's roughly a third
and we might want to make this look nice
so let's go ahead and put in another
line there the probability of getting
the sum which is a multiple of three is
0.33333
we can compute the same thing for five
dice
and if we do this for five dice and go
and run it you can see we just have a
huge amount of choices so just goes on
and on down here and we can look at the
length of the event space
and we have over
776 choices that's a lot of choices
if we want to ask the question like we
did above uh what is the sum where the
sum is a multiple of five but not a
multiple of three
we can go through all of these different
options and then you can see here D1 D2
D3 D4 D5 equals the outcome and if you
add these all together and they're
division by five does not have a
remainder of zero but the remainder is
also of a division by three is not equal
to zero so the multiple of five is equal
to zero but the multiple three is not we
can just append that on here and then we
can look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length
of our favorable outcome
it's always good to see what we're
working with and so we have 904 out of
770
6 and then of course we can just do a
simple division to get the probability
on here what's the probability that
we're going to roll a multiple of five
when you add them together
but not a multiple of three
and so we're just going to divide those
two numbers and you can see here we get
0.116255 or 11.62 percent
and so you can really have a nice visual
that this is not really complicated in
math right here on probabilities it's
just how many options do you have and
how many of those are you possibly going
to be able to come up with with the
solution you're looking for and this
leads us to a confusion Matrix
a confusion Matrix is a table which is
used to describe the performance of a
classification model on a set of test
data for which the True Values are known
and so you'll see on the left we have
the predicted and the actual and we have
a negative false negative positive true
positive
and then we have false positive and true
negative and you can think of this as
your predicted model what does that mean
that means if you divided your data and
you used two-third of us to create the
model
you might then test it against an actual
case for the last third and see how well
it comes out how many times was it true
positive versus uh false positive we get
a false positive response and you can
imagine in medical situations this is a
pretty big deal you don't want to give a
false positive so you might adjust your
model accordingly so you don't have a
false positive say with the covet virus
test it'd be better to have a false
negative when they go back and get
retested then to have 30 percent false
positives where then the test is pretty
much invalid
so in a use case like cancer prediction
let's consider an example where a cancer
prediction model is put to the test for
its accuracy and precision actual result
of a person's medical report is compared
with the prediction made by the machine
learning model
and so you can see here here's our
actual predicted whether they have
cancer or not you know cancer a big one
you don't want to have a false positive
I mean a false negative in other words
you don't want to have it tell you that
you don't have cancer when you do so
that would be something you'd really be
looking for in this particular domain
you don't want a false negative
and this is again you know you've
created a model you have hundreds of
people or thousands of pieces of data
that come in there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many negative results do
you have that are either telling people
they have cancer that don't or telling
people that don't have cancer that they
do and then we can take these numbers
and we can feed them into our accuracy
our precision and our recall
so accuracy precision and recall
accuracy metric to measure how
accurately the results are predicted and
this is your total true where you got
the right results you add them together
the true positive the true negative over
all the results so what percentage of
them were accurate versus what were
wrong
we're talking about Precision is a
metric to measure how many of the
correctly predicted cases are actually
turned out to be positive
uh so we have a Precision on true
positive again if you're talking about
like covid testing with the viruses you
really want this to be a high number you
want this true that to be the center
point where you might have the opposite
if you're dealing with cancer where you
want no false negatives
uh so this is your metric on here
Precision is your test positive true
positive plus false positive
and then your recall how many of the
actual positive cases we were able to
predict quickly with our model uh so
test positive is the test positive plus
the false negative on there also
accelerate your career in Ai and ml with
our conference postgraduate program in
Ai and machine learning boost your
career with this Ai and ml course
delivered in collaboration with Purdue
University and IBM so why wait enroll
now and analog exciting Ai and ml
opportunities the link is in the
description box below
everyone knows the algorithm is a
step-by-step process to approach a
particular problem
there are numerous examples of algorithm
from figuring out sets of number to
finding roots to maps to showing data on
a screen let's understand this by using
an example
every algorithm is built on inputs and
outputs Google search algorithm is no
different the input is the search field
and the output is the page of its result
that appears when you enter a particular
phrase or keyword also known as serp or
search engine result page
Google has a search algorithm so it can
sort results from various website and
provides the users the best result
when you start a search you will see the
search box we'll attempt to guess what
you are looking for
in order to better understand what the
user is looking for the algorithm is
trying to gather as many as suggestions
from them as possible
the results from the search field that
best matches the query will be ranked
the choose fit website will Rank and in
what position using more than 200
ranking variables now let's take an
example of coding program and see how
the algorithm works
here we will use a case of computer
program wherein we want to print the
multiplication table of any number let's
take two the algorithm start here and
then it assign a value to a variable the
variable I is having an initial value of
1. the system will read the number the
number in case is 2. now the system has
a condition a condition can now either
be true or false if the value of I
reaches 11 then the loop will end
otherwise value of I will multiply by
the number the initial value of I is 1
so for the first time the system output
will be 2. now value of I will be
increased by 1. according to the loop
condition the system will then move back
and check for the condition again
the new value of I is 2. which is still
less than 11. the system will again
print 2 into Y which is 2 into 2 on this
screen
the new output result will be 4. the
system will keep following the same
procedure repeatedly until the value of
I becomes 11. once the value of I
becomes 11 then only the algorithm will
terminate after discussing how an
algorithm work let's move forward and
see some popular machine learning
algorithms
some popular machine learning algorithms
are
first one is linear regression algorithm
second one is logistic regression
algorithm
and the third one is decision tree
and the fourth one is support Vector
machine algorithm svm
and the fifth one is KNN K nearest
neighbor algorithms
the sixth one is K means clustering
algorithms the and the seventh one is
random Forest algorithms and the last
but not the least algorithm is a
priority algorithms let's go through
them in detail one by one
linear depression is one of the most
famous and straightforward machine
learning algorithms utilized for
predictive analysis linear regression
show the linear connection between the
dependent and independent factors
the equation of line is y equals to MX
plus b here y stand for this response
variable or a dependent variable whereas
X is for the Predator variable or an
independent variable it attempts best to
fit line between the dependent and the
independent variables and this best bit
line is known as line of regression or
regression line
let's take a real application example in
predicting consumer Behavior businesses
use the linear regression to forecast
things like how much a client is likely
to spend
things like targeted marketing and
product development May benefit from
this Walmart for instance use linear
regression to forecast which good would
be in high demand Across the Nation
moving forward let's see types of linear
regression
there are two types of linear regression
algorithm
the first one is simple linear
regression and the second one is
multiple linear regression
in simple linear regression if an
independent variable is utilized to
focus the worth of a mathematical
dependent variable then at that point
such a linear regression algorithm is
called Simple linear regression the
equation of line will be y equals to a0
plus a 1 X
and the second one is multiple linear
regression if the dependent variables
declines on the Y and that if
independent variable on the X
then such a relationship is known as
negative linear relationship the line of
equation will be minus of a 0 plus a 1 X
moving forward let's see logistic linear
regression
logistic regression is this supervised
machine learning algorithm utilized to
anticipate all the categorical factors
or discrete values it could be very well
used for the grouping issues in machine
learning and the result of the logistic
regression can be either yes or no 0 or
1 men or women and so on it gives the
values which lies between 0 and 1. for
example a credit card business is
interested in knowing whether the
transaction amount and the credit score
have an impact on the probability that a
particular transaction would be
fraudulent the business can use logistic
redirection to determine how these two
Predator values can relate the
probability that a transaction is
fraudulent this response variable in the
model has two possible outcomes
first one is the transaction is
shortland and the second one is the
transaction is not fraudulent
in logistic regression rather than
fitting a regression line we fit an S
form logistic capability which predicts
two greatest value 0 or 1. the logistic
regression equation can be calculated
from linear regression equation the
steps to get logistic regression
equations are the equation of straight
line can be written as y equals to b0
plus B1 X1 plus b 2 x 2 till b n x n in
logitative regression y can be between 0
and 1 only so for that let's divide the
above equation by 1 minus y then the
equation will be y upon 1 minus y that
is 0 for y 0 and Infinity for y equals
to 1 but range between minus infinity to
plus infinity then we have to take the
logarithm of equation and now it will
become log of Y upon 1 minus y equals to
b0 plus B1 X1 plus b 2 x 2 so on till B
and x n let's move forward and see types
of logistic regression
there are three types of logistic
regression that can be classified first
one is binomial in binomial logistic
regression there can be only two
possible types of dependent variables
like yes or no password man woman and
many more and the second one is
multinomial in multinomial logistic
regression there can be three or more
possible unordered ways of dependent
variable such as horse cow and sheep and
the last one is ordinal in ordinal
logistic regression there can be three
or more possible ordered ways of
dependent variable such as small medium
or large moving forward let's see
decision trees in detail
a decision tree is a tree structured
classifier that could be used for
classification and regression a decision
tree is a tree in which each non-leaf
node is assigned to an attribute
additionally each are contained one of
the available values for its parent node
which is associated with each Leaf node
that is the node from where the arc is
directed
let's see some decision tree terminology
first one is root that contains the
entire data set the next one is node
attached for the data of a certain
attribute and the third one is branch
which connects the node to internal node
or the internal node to Leaf node and
the fourth one is leaf node the terminal
node that predicts the outcome let's
move forward and see decision tree
algorithms the first one is select the
best attribute to use the current node
in the tree the second one is for each
possible values select the attributes
the third one is partition the examples
using the possible values of this
attribute and assign these disjoint
subset or the example to the appropriate
child node recursively generate each
child node until ideally all examples
for a node have the same label like
class moving forward let's understand
the decision tree for building a
decision tree
Step One is Select an attribute then
split the data into its children in a
tree continue splitting with available
attributes
and keep splitting until late node are
pure like only one class remains a
maximum depth is
metric is achieved let's move forward
and see svm algorithm support Vector
machine algorithms
a support Vector machine is a well-known
supervised machine learning model it is
utilized for both information
classification and regression
it is regularly utilized for the
grouping issues we can involve it in
different life care system and we can
involve it in typically happy or sad
look Arrangements we can involve
retained filters if we make specific
Loops it would add the particular filter
according to the expression the scope of
articulation lies between happy and sad
support Vector machine helps them to
recognize and return characters use yd
like checks continue to reach a
significant part of the majority of
non-cash transaction and are frequently
written by the pupil the current check
processing system in many developing
nations involves a bank employee to read
and manually enter the information in a
check while also verifying the data like
signature and date
a handwritten text recognition system
can reduce expenses and labor hours
because a bank must handle several
checks each day
moving forward let's see the algorithm
of svm
the objective of support Vector machine
is to make the best line or Choice limit
that can isolate and dimensional space
into classes so we can undoubtedly put
the new data of interest in the right
category later on this batch decision
boundary is known as a hyperplate let's
move forward and see types of support
Vector machine support Vector machine
can be of two types first one is linear
svm second one is non-linear SPM let's
move forward and see linear sphere
linear svm is utilized for linearly
detachable information
which implies if a data set can be
ordered into two classes by utilizes a
straight line then such information are
named linearly separable information and
a classifier is utilized called linear
XVM classifier moving forward let's see
non-linear svm
non-linear SPM is utilized for
non-dietly isolated information and that
implies in the event that a data set
can't be categorized by utilizing a
straight line
such information is named non-directed
information
and the classifier utilize is called a
non-linear SPM classifier moving forward
let's see k n algorithm in detail
KNN is a supervised learning technique
Canon classifies new data into our
targeted classes depending on the
features of its neighboring points and
also be used for the regression problems
it is an instance based learning
algorithm and a bit lazy learning
algorithm k n calculation stores every
one of its accessible information and
orders another information Point based
on the likeliness this means that when
new data information appears it usually
tends to the successfully categorized
into a good suit classes using the k n
algorithm let's imagine we have an image
of animal that resembles a cow or Ox
however here we are not sure if it is a
cow or Ox as k n method is based on a
likeness Matrix it will identify the
properties of new data that are related
to the image of cow or ox and based on
those quality it will classify the data
is belonging to either cow or Ox group
moving forward let's see how does KNN
work
the steps to implement k n algorithms
are step one decide on the neighbor's K
numbers Step 2 calculate the euclidean
distance between K Neighbors in step 2.
third one is based on the determined
euclidean distance select the K closest
neighbors step 4 is count the numbers of
data points in each category between
these scannables step 5 assign the fresh
data points in the category where the
highest neighbors count and then k n
model is ready
Let's see we need to add a new data
point to the vital category at first we
will decide on the numbers of neighbors
therefore we will pick k equals to 5.
then the euclidean distance between the
data points and then can be determined
the distance between two points known as
the euclidean distance can be determined
by under root of X2 minus X1 whole
square plus Y2 minus y1 whole Square
then we determine the closest neighbors
by calculating the euclide distance
there are three closest Neighbors in
category a and two closest neighbor in
category B
this new data point Must Fall with
category a because as we can see its
three closest neighbors are also from
group a after understanding k n
algorithm let's move forward and see
k-means algorithms in detail
the K means is the cluster falls under
that is an unsupervised learning
algorithm it is used to address machine
learning clustering problems and utilize
to tackle the grouping issues in machine
learning
it permits us to Bunch the information
into various Gatherings it is a helpful
method for finding the classification of
groups in the unlabeled data set without
the requirement of any training this
k-means algorithm groups the data into
similar classes let's see some
application of k-means clustering let's
see some applications of k-means
clustering diagnostic system the medical
profession uses k-means casting in
creating smarter medical decision
support system especially in the
treatment of lever alignments the second
one is search engines clustering forms a
backbone of search engine when a search
engine is performed the search result
need to be grouped and the search
engines very open use clustering to do
this moving forward let's see how
k-means algorithm works
the steps to implement k-means
algorithms are step one select the
number key to set the number of clusters
Step 2 select a random K points or
centroid step 3 assign each data point
the closest centroid data forms to
predefined K cluster step 4 determine
the variance and a certain new Gravity
points for each cluster step 5 repeat
the third step this means G allocating
each data point to the new closest
centroid cluster step 6 if I
reassignment occurs go to step 4
otherwise go to exit step 7 the model is
ready to use
so now we have a clear understanding of
how k-means algorithm work let's move
forward to see the graphical
representation of k-means algorithm
consider that there are two variables M1
and M2 this is a scatter plot of these
two variables along the X and Y axis
we should accept the number of K of
punches that is k equals to 2 to
recognize the data set and to place them
into various groups it implies here we
will attempt to Bunch these data set
into two unique groups people really
want to pick an irregular key point or
centroid to frame this group
these centroids can be either the focus
of the data set or some of other points
thus here we are choosing under two
points as cables which are not the piece
of our data set
we will assign every data of interest in
this scatter plot to the nearest K point
or centroid
we will register it by applying some
math that we consider to find the
distance between two points that is
euclidean distance thus we will draw a
median between both the centroids from
the graph the laptop of the line are
close points to the right and the K1
centroid green is near the orange
centroid we should variety them as green
and orange for Clear representation as
the need might arise to track down the
nearest group we will repeat the cycle
by picking another centroid
repeat the new centroid we will figure
out the center point of gravity of the
centroid and will track down new
centroids then we will reassign every
piece of information to highlight new
centroid for this we will repeat a
similar course of tracking down a middle
line the middle will be like as seen in
the picture one orange point is on the
left half of the line and two green
points are on the right thus these three
points will be appointed to the new
centroids as reassignment has occurred
we will again go to step 4 tracking down
new centroids or k points we will repeat
the cycle by tracking down the center
point of gravity of centroid so the new
centroids will be displayed as like this
we now have new centroid so once more
defined the middle boundary and reassign
the data of Interest by this graph there
are no unique pieces of information on
one of the other side of the line
implying our model is shaped by the
previous graph there are no unique
pieces of information on one or the
other line
implying our model is shaped as our
k-means model is ready and the two plus
groups will be displayed as like these
now we have a clear understanding of how
k-means clustering algorithm works now
let's move forward to understand random
Forest algorithm
random Forest is an adaptable simple to
utilize machine learning algorithm that
produces even without the hyper boundary
tuning and extraordinary outcome more
often that Norm it is likewise quite
possibly the most utilized algorithm
because of its effortness and variety
like its tend to be utilized for both
grouping and classification tasks random
4S is a classifier that contains various
Choice trees on different subsets of the
given data set and takes the normal to
work on the present exactness of the
data set instead of depending on the
choice tree the random Forest takes the
forecast from history and in light of
the larger part of boards of expectation
it predicts the final result
now let's move forward and see how does
random Forest work
we should see the random forest in order
since the arrangement is now and again
thought to be the structured block of
machine learning this is what a random
Forest would look like with two trees
the random Forest has a similar
hypermeter to their decision tree or a
paging classifier luckily there is a
compelling reason need to consolidate a
decision trade with a paging classifier
since you can undoubtedly utilize the
classifier classes of random forests
with random Forest you can likewise
manage tasks using the algorithm
regression random forests add extra
orbitness to the model while tableving
the trees rather than looking for the
man element while parting a node
it looks to the best component among an
irregular subset of highlights these
outcomes in a wide variety often result
in a superior model
subsequently in a random Forest just a
random subset of the element is thought
about the algorithms you might make
trees more random by involving random
edges for each component instead of
looking for the most ideal limits like a
typical Choice Traders
let's move forward and see some
application of random Forest algorithms
random Forest is involved at work by
researchers in numerous Ventures
including banking stock exchanging
medication and many more it is utilized
to predict things which assist these
businesses with running productively
like client activity patient history and
safety in banking random Forest is used
to identify clients who are more likely
to pay back their debts on schedule
additionally it is utilized to forecast
who will make more frequent use of Bank
Services even fraud detection uses it
the robin node of algorithms indeed
random Forest is a tool used by stock
Trader to forecast future stock Behavior
retail businesses utilize it to make
product recommendation and forecast
client satisfaction random forage can be
used in healthcare industry to examine a
patient medical history and detect
disorders random Forest is a tool used
by pharmaceutical expert to determine
the idle mix of ingredients in treatment
or to forecast drug sensitivity by
seeing application of random Forest
algorithm let's move forward and see
some difference between decision trees
and random Forest
let's see the difference between random
forest and decision tree the first one
is
while building a random Forest the
number of rows is selected randomly
in decision trees it builds several
decision trees and find out the output
the second one is it combines two or
more decision trees together in decision
trees whereas the decision is a
collection of variables or data sets or
attributes the third one is it gives
accurate results whereas it gets less
accurate results the fourth one is by
using random Forest it reduces the
chance of overlifting whereas decision
trees it has the possibility of
overlifting the fifth one is random
Forest is more complicated to
interpreters whereas the decision tree
is simple it is easy to read and
understand
after seeing what is random Forest how
it works let's move forward to see a
primary algorithm in detail
algorithm utilize standard item sets to
create calculation rules and is intended
to chip away the information basis
containing exchanges with the assistance
of these application rules it decide how
firmly perceively two objects are
associated
this algorithm utilize a breakfast
search and history to work out the item
set Association effectively
it is the iterative interaction for
finding out the successive item set from
the huge data set let's move forward and
see steps for a priority algorithms the
steps for a priority algorithms are Step
One is establish minimal support and
confidence for item set in the
transactional database the second one is
take all transaction supports with a
greater support value the minimum or
chosen support value in Step 2 the third
one is track down all the rules in these
subsets with confidence value greater
than the threshold value the fourth one
is arrange the rules to lower the lift
at last we will see some advantages and
disadvantages of a priority algorithm
the advantages of a priority algorithms
are easy to understand an algorithm and
the second one is the join and prune
steps of the algorithms can be easily
implemented on the large data set the
disadvantages of a priority algorithms
are their priority algorithms work
slowly as compared to the other
algorithm and the second one is the
priority algorithms times and space
complexity are o of 2D which is very low
compared to the other ones let's move
forward and see some Hands-On lab demo
so we will see a handsole lab demo on
linear regression as we know linear
regression is a way to find or to
predict the relationship between two
variables generally we use X and Y so
first we will open command prompt to
write command to open Jupiter notebook
so we will write Jupiter
notebook
and then press enter
so this is the landing page of jupyter
notebook and select open
new python file
so this is how Jupiter notebook UI looks
like so at first we will import some
major libraries of python which will
help us in mathematical functioning so
the first one is numpy
Nampa is a python Library used for
working with arrays it also has
functions for working in domains like
linear algebra and matrices it is in
open source project and you can use it
freely
numpy stands for numerical python
so we will write like import
numpy S and B here NP is used for
denoting numpy so we will import the
next libraries pandas pandas is a
software Library written for Python
programming
for like data manipulation and Analysis
in particular it offers data structures
and operation for manipulating numerical
labels and Time series
so we will write import pandas
as PD
here PD is used for denoting pandas so
our next library is matplotlib mat.lib
is a graph plotting library in Python
that serves as a visualization utility
is open source so we can use it freely
and it is most written in Python a few
segments in C and JavaScript for
platform compatibility
for importing Metro lip you have to
write import matplotlib
dot lip
dot Pi plot
as PLT
so after importing the libraries we will
move ahead and import data set so for
importing data set we have to write data
set
equals to PD Dot delete underscore CSV
and here we have to give the path of the
file data set
dot CSV
here PD is for pandas library and read
is used for reading the data set from
the machine and CSV CSE is used for the
type of file which you want to read
so after reading let's see our data so
we will write data set
dot head
and press enter
here head is used for retrieving the
first five lines from the data so our
data is coming properly so moving ahead
now let's first Define X and Y axis for
X we have to write x equals to data set
Dot ilock
bracket colon comma colon again minus
one
dot values for exercise and for y axis
we have to write y equals to
data set
dot I log
bracket colon comma 1
dot values
so if we will use dataset.ilog for -1
values for x axis it will select till
the second last column of the data frame
instead of the last column
and I know this is as the second last
column value and the last column value
for the row is different
whereas if you will use for the y-axis
values return a series Vector a vector
does not have a column size
so moving ahead let's see the value for
x and y axis first we will see the value
for x like just you have to write X and
press enter so
these are the arrays value for x and for
y axis you have to type Y and then enter
so these are the arrays value for y axis
so after this now let's split the data
set into training and testing separating
data into training and testing set is an
important part for evaluating data
mining models typically when you
separate a data set into a training set
and a testing set most of the data set
is used for training and a similar
portion of data set is used for testing
so we will split it into 70 and 30 ratio
so for splitting we need to import some
more libraries for this process so we
will write from
sqlan
dot model
underscore selection
selection
import
train
underscore test
underscore split
is most useful and robust library for
machine learning in Python it provides a
selection of efficient tools for machine
learning and statical modeling including
classification regression and clustering
so after importing left side code for
the splitting data so we will write here
X underscore
train
comma
X underscore test
comma y underscore train
comma y underscore test
equals to
10
underscore test
underscore split
in Brackets we have to write X comma y
comma
test
underscores size
equals to
0.3
comma
random
underscore state
equals to zero
random basically
which random State equals to 0 we get
the same Trend and test sets across
different execution so after this let's
see the values together
so we will write X underscore train
comma X underscore test
comma y underscore
chain
comma y underscore
test
so
these are the values of array X and
array y together moving ahead now let's
work with regression first we need to
import the library for regression so we
will use from
SQ learn
dot linear
underscore
model
in both
linear
the regression
we already discussed SQL is used for
machine learning and linear regression
is a major part of machine learning so
after this let's make a function for
regression as RSG for easy use so we
will write Reg
equals to
linear
regression
and brackets first we try to train and
then test and compare so we will write
here Reg
Dot
bit
bracket X underscore brain
comma y underscore chain
now let's predict values but prediction
values are always different
so we will predict values for y first so
we will write y
underscore predict
equals to red
dot predict
sorry predict
or X underscore test
so like y underscore
reading
and enter
this is when the predict functions come
into the picture python predict function
enables us to predict the labels of the
data values on the basis of training
model the predict function except only a
single argument which is X underscore
test here usually the data to be tested
so when you will check the value for y
test you will see the different values
like
White
underscore
test
so you can see there are totally
different values from x axis now let's
display on graph for training data set
so we have to write PLT dot scatter
X underscore drain
comma y underscore train
comma color
equals to
you can choose by yourself I will choose
red
then PLT
Dot Plot
X underscore train
comma integration point for predicting
values
or X underscore
train
comma color
equals to
blue
so this color is for the regression line
and the pl tip dot title
linear
regression
depression
salary
versus experience
and I will prefer this size would be
edit
let's set the
labels for x and y axis so
X label
pair of employee
common size will be
available
then for y-axis we will write
here
I sell it is
and I will prefer this same size 15.
let's show the plot by writing PLT dot
show
then
hopefully we are fortunate everything is
going to be fine
perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
work for training data set let's see how
it will work for testing data set
now let's predict what test data set you
can copy for the same and like you can
change it
so we can copy here
so we can paste here
so we have to just change here then to
test data set
just here then again
test here
test here
everything everything in Google
reduce some size to
16 let's see then it will use 12 and so
perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
works for testing data set
in this lesson you are going to
understand the concept of text mining
by the end of this lesson you will be
able to explain text mining execute text
processing task
so let's go ahead and understand text
mining in detail let's first understand
what text mining is text mining is the
technique of exploring large amounts of
unstructured Text data and analyzing it
in order to extract patterns from the
text to data it is aided by software
that can identify Concepts patterns
topics keywords and other attributes in
the data it utilizes computational
techniques to extract and summarize the
high quality information from
unstructured textual resources let's
understand the flow of text mining there
are five techniques used in text mining
system information extraction or text
pre-processing this is used to examine
the unstructured text by searching out
the important words and finding the
relationships between them
categorization or text transformation
attribute generation categorization
technique it labels the text document
under one or more categories
classification of Text data is done
based on input output examples with
categorization
colostrain or attribute selection
clustering method is used to group text
documents that have similar content
clusters are the partitions and each
cluster will have a number of documents
with similar content clustering makes
sure that no document will be omitted
from the search and it derives all the
documents that have similar content
visualization technique the process of
finding relevant information is
simplified by visualization technique
this technique uses text Flags to
represent a group of documents or a
single document and compactness is
indicated using colors visualization
technique helps to display textual
information in a more attractive way
summarization or interpretation or
evaluation summarization technique will
help to reduce the length of the
document and summarize the details of
the documents it makes the document easy
to read for users and understand the
content at the moment
let's understand the significance of
text mining
document clustering document clustering
is an important part of text mining it
has many applications in Knowledge
Management and information retrieval
clustering makes it easy to group
similar documents into meaningful groups
such as in newspapers where sections are
often grouped as business Sports
politics and so on
pattern identification text mining is
the process of automatically searching
large amount of text for text patterns
and recognition of features features
such as telephone numbers and email
addresses can be extracted using pattern
matches
product insights text mining helps to
extract large amounts of text for
example customer reviews about the
products mining consumer reviews can
reveal insights like most loved feature
most hated feature improvements required
and reviews of competitors products
security monitoring text mining helps in
monitoring and extracting information
from news articles and reports for
national security purposes
text mining make sure to use all of your
available information it is a more
effective and productive knowledge
discovery that allows you to make better
informed decisions automate information
intensive processes gather business
critical insights and mitigate
operational risk
let's look at the applications of text
mining
speech recognition speech recognition is
the recognition and translation of
spoken language into text and vice versa
speech often provides valuable
information about the topics subjects
and concepts of multimedia content
information extraction from speech is
less complicated yet more accurate and
precise than multimedia content this
fact motivates content-based speech
analysis for multimedia Data Mining and
retrieval where audio and speech
processing is a key enabling technology
spam filtering spam detection is an
important method in which textual
information contained in an email is
extracted and used for discrimination
text mining is useful in automatic
detection of spam emails based on the
filtering content using text Mining and
email service providers such as Gmail or
Yahoo mail checks the content of an
email and if some malicious text is
found in the mail then that email is
marked as spam and sent to the spam
folder
sentiment analysis it is done in order
to determine if a given sentence
expresses positive neutral or negative
sentiments
sentiment analysis is one of the most
popular applications of text analytics
the primary aspect of sentiment analysis
includes data analysis of the body of
the text for understanding the opinion
expressed by it and other key factors
comprising modality and mood usually the
process of sentiment analysis works best
on text that has a subjective context
then on that with only an objective
context
e-commerce personalization
text mining is used to suggest products
that fit into a user's profile text
mining is increasingly being used by
e-commerce retailers to learn more about
the consumers as it is the process of
analyzing textual information in order
to identify patterns and gain insights
e-commerce retailers can Target specific
individuals or segments with
personalized offers and discounts to
boost sales and increase Customer
Loyalty by identifying customer purchase
patterns and opinions on particular
products
let's look at natural language toolkit
library in detail
natural language toolkit is a set of
Open Source python models that are used
to apply statistical natural language
processing on human language data
let's see how you can do environment
setup of nltk
go to Windows start and launch python
interpreter from Anaconda prompt and
enter the following commands enter
command python to check the version of
python installed on your system
enter import nltk to link you to the
nltk library available to download then
enter
nltk.download function that will open
the nltk download window check the
download directory select all packages
and click on download this will download
nltk onto your python once you have
downloaded the nltk you must check the
working and functionality of it in order
to test the setup enter the following
command in Python idle
from
nltk.corpus import brown brown dot word
parenthesis parenthesis
the brown is an nltk Corpus that shows
the systematic difference between
different genres available words
function will give you the list
available words in the genre the given
output shows that we have successfully
tested the nltk installed on python
let's Now understand how you can read a
specific module from nltk corpora if you
want to import an entire module from
nltk corpora use asterisk symbol with
that module name import command enter
the command from
nltk.book import asterisk it will load
all the items available in nltk's book
module now in order to explore Brown
Corpus enter the command nltk dot Corpus
import Brown this will import Brown
Corpus on the python enter brown dot
categories function to load the
different genres available
select a genre and assign that genre to
a variable using the following syntax
variable name is equal to brown.words
categories is equal to genre name now in
order to see the available words inside
the selected genre just enter the
defined variable name as a command
let's understand text extraction and
pre-processing in detail
so let's first understand the concept of
tokenization tokenization is the process
of removing sensitive data and placing
unique symbols of identification in that
place in order to retain all the
essential information concerned with the
data by its security it is a process of
breaking running streams of text into
words and sentences it works by
segregating words using punctuation and
spaces
text extraction and pre-processing
engrams
now let's look at what n gram is and how
it is helpful in text mining engram is
the simplest model that assigns these
probabilities to sequences of words or
sentences engrams are combinations of
adjacent words or letters of length and
in the source text so engram is very
helpful in text mining when it is
required to extract patterns from the
text as in the given example this is a
sentence all of these words are
considered individual words and thus
represent unigrams a 2 gram or bigram is
a two-word sequence of words like this
is is a or a sentence and a three gram
or trigram is a three-word sequence of
words like this is a or is a sentence
let's Now understand what stop words are
and how you can remove them
stop words are natural language words
that have negligible meaning such as a
and and or the and other similar words
these words also will take up space in
the database or increase the processing
time so it is better to remove such
words by storing a list of stop words
you can find the list of stop words in
the nltk data directory that is stored
in 16 different languages use the
following command to list the stop words
of English language defined in nltk
Corpus importing nltk will import the
nltk Corpus for that instance enter from
nltk.corpus import Stop words will
import Stop words from nltk Corpus Now
set the language as English so use set
function as set under braces stop words
dot words set genre as English
stop words are filtered out before
processing of natural language data as
they don't reveal much information
so as you can see in the given example
before filtering the sentence the
tokenization of stop word is processed
in order to remove these stop words and
the filtering is applied in order to
filter the sentence based on some
criteria
text extraction and pre-processing
stemming
stemming is used to reduce a word to
stem or base word by removing suffixes
such as helps helping helped and helper
to the root word help
the stemming process or algorithm is
generally called a stemmer there are
various stemming algorithms such as
Porter stemmer Lancaster stemmer
snowball stemmer Etc use any of the
stemmers defined under nltk stem Corpus
in order to perform stemming as shown in
the example here we have used Porter
stemmer When You observe the output you
will see that all of the words given
have been reduced to their root word or
stem
text extraction and pre-processing
limitization
limitization is the method of grouping
the various inflected types of a word in
order that they can be analyzed as one
item it uses vocabulary list or a
morphological analysis to get the root
word it uses wordnet database that has
English words linked together by their
semantic relationship as you can observe
the given example the different words
have been extracted to their relevant
morphological word using limitization
text extraction and pre-processing POS
tagging
let's now look at different part of
speech tags available in the national
language toolkit Library
a POS tag is a special label assigned to
each token or word in a Text corpus to
indicate the part of speech and often
also other grammatical categories such
as tense number either plural or
singular case Etc POS tags are used in
text analysis tools and algorithms and
also in Corpus searches so look at the
given example here Alice wrote a program
is the source text given the POS tags
given are Alice is a noun wrote is a
verb a is an article and program is an
adjective look at the given example to
understand how POS tags are defined so
the given sentence or paragraph contains
different words that represent different
parts of speech we will first use
tokenization and removal of stop words
and then allocate the different POS tags
these are shown with different words in
the given sentence
POS tags are useful for limitization in
building named entity recognition and
extracting relationships between words
text extraction and pre-processing named
entity recognition now let's understand
what named entity recognition is all
about Nar seeks to extract a real world
entity from the text and sorts it into
predefined categories such as names of
people organizations locations Etc many
real world questions can be answered
with the help of name entity recognition
where specified products mentioned in
complaints or reviews
does the Tweet contain the name of a
person does the Tweet contain the
person's address
as you can see in the given example
Google America Larry Page Etc are the
names of a person place or an
organization so these are considered
named entities and have different tags
such as person organization
gpe or geopolitical entity Etc
NLP process workflow
now you have an understanding of all
nltk tools so now let's understand the
natural language processing workflow
Step 1 tokenization it splits text into
pieces tokens or words and removes
punctuation Step 2 stop word removal it
removes commonly used words such as the
is are Etc which are not relevant to the
analysis step 3 stemming and
limitization it reduces words to base
form in order to be analyzed as a single
item step 4 POS tagging it tags words to
be part of speech such as noun verb
adjective Etc based on the definition
and context step 5 information retrieval
it extracts relevant information from
the source
mo1 a brown Corpus problem statement the
Brown University standard Corpus of
present-day American English also known
popularly as brown Corpus was compiled
in the 1960s as a general Corpus in the
field of Corpus Linguistics it contains
500 samples of English language text
totaling roughly 1 million words
compiled from Works published in the
United States in 1961. we will be
working on one of the subset data set
and perform text processing tasks
let us import the nltk library and read
the ca underscore 10 Corpus import nltk
we will have to make sure that there are
no slashes in between hence we will use
the replace function within pandas for
the same
let's have a look at the data once
tokenization after performing sentence
tokenization on the data we obtain
similarly after applying sentence
tokenizer the resulting output shows all
individual words tokens
stop word removal let's import the stop
word library from
nltk.corpus import Stop words
we also need to ensure that the text is
in the same case nltk has its own list
of stop words we can check the list of
stop words using
stopwords.words and English inside the
parenthesis
map the lowercase string with our list
of word tokens
let's remove the stop words using the
English stop words list in nltk we will
be using set checking as it is faster in
Python than a list
by removing all stop words from the text
we obtain
often we want to remove the punctuations
from the documents too since python
comes with batteries included we have a
string dot punctuation
from string import punctuation
combining the punctuation with the stop
words from nltk
removing stop words with punctuation
stemming and limitization we will be
using stemming and limitization to
reduce words to their root form for
example walks walking walked will be
reduced to their root word walk
importing Porter stemmer as the stemming
library from nltk.stem import Porter
stemmer
printing the stem words
import the wordnet limitizer from
nltk.stem
printing the root words
we also need to evaluate the POS tags
for each token
create a new word list and store the
list of word tokens against each of the
sentence tokens in data 2.
for I in tokenized
also we will check if there were any
stop words in the recently created word
list
we will now tag the word tokens
accordingly using the POS tags and print
the tagged output
for our final text processing task we
will be applying named entity
recognition to classify named entities
in text into predefined categories such
as the names of persons organizations
locations expressions of times
quantities monetary values percentages
Etc
now press the tagged sentences under the
chunk parser if we set the parameter
binary equals true then named entities
are just tagged as any otherwise the
classifier adds category labels such as
person organization and gpe
create a function named as extract
entity names along with an empty list
named as entity names
we will now extract named entities from
a nltk chunked expression and store them
in the empty created above
again we will set the entity names list
as an empty list and we'll extract The
Entity names by iterating over each tree
in chunked sentences
great we have seen how to explore and
examine the Corpus using text processing
techniques let's quickly recap the steps
we've covered so far One Import the nltk
library to perform tokenization three
perform stemming and limitization four
remove stop words five perform named
entity recognition
structuring sentences syntax
let's first understand what syntax is
syntax is the grammatical structure of
sentences in the given example this can
be interpreted as syntax and it is
similar to the ones you use while
writing codes knowing a language
includes the power to construct phrases
and sentences out of morphemes and words
the part of the grammar that represents
a speaker's knowledge of these
structures and their formation is called
syntax
phrase structure rules are rules that
determine what goes into a phrase that
is constituents of a phrase and how the
constituents are ordered constituent is
a word or group of words that operate as
a unit and can be used to frame larger
grammatical units the given diagram
represents that a noun phrase is
determined when a noun is combined with
a determiner and the determiner can be
optional a sentence is determined when a
noun phrase is combined with a verb
phrase a verb phrase is determined when
a verb is combined optionally with the
noun phrase and prepositional phrase and
a prepositional phrase is determined
when a preposition is combined with a
noun phrase
a tree is a representation of syntactics
structure of formulation of sentences or
strings consider the given sentence the
factory employs 12.8 percent of Bradford
County
what can be the Syntax for pairing the
statement let's understand this a tree
is produced that might help you
understand that the subject of the
sentence is the factory the predicate is
employees and the target is 12.8 percent
which in turn is modified by Bradford
County
syntax parses are often a first step
toward deep information extraction or
semantic understanding of text
rendering syntax trees
download the
corresponding.exe file to install the
ghost script rendering engine based on
your system configuration in order to
render syntax trees in your notebook
let's understand how you can set up the
environment variable
once you have downloaded and installed
the file go to the folder where it is
installed and copy the path of the file
now go to system properties and under
Advanced properties you will find the
environment variable button click on
that to open the pop-up box tab of the
environment
now open the bin folder and add the path
to the bin folder in your environment
variables
now you will have to modify the path of
the environment variable use the given
code to test the working of syntax tree
after the setup is successfully
installed
structuring sentences chunking and chunk
parsing
the process of extraction of phrases
from unstructured text is called
chunking instead of using just simple
tokens which may not represent the
actual meaning of the text it is
advisable to use phrases such as Indian
team as a single word instead of Indian
and team as separate words the chunking
segmentation refers to identifying
tokens and labeling refers to
identifying the correct tag these chunks
correspond to mixed patterns in some way
to extract patterns from chunks we need
chunk parsing the chunk parsing segment
refers to identifying strings of tokens
and labeling refers to identifying the
correct chunk type
let's look at the given example you can
see here that yellow is an adjective dog
is a noun and the is the determiner
which are chunked together into a noun
phrase similarly chunk parsing is used
to extract patterns and to process such
patterns from moldable chunks while
using different parsers
let's take an example and try to
understand how chunking is performed in
Python let's consider the sentence the
little mouse ate the fresh cheese
assigned to a variable named scent using
the word tokenize function under nltk
corpora you can find out the different
tags associated with a sentence provided
so as you can see in the output
different tags have been allocated
against each of the words from the given
sentence using chunking
NP chunk and parser
you will now create grammar from a noun
phrase and will mention the tags you
want in your chunk phrase within the
function here you have created a regular
expression matching the string the given
regular expression indicates optional
determiner followed by optional number
of adjectives followed by a noun you
will now have to parse the chunk
therefore you will create a chunk parser
and pass your noun phrase string to it
the parser is now ready you will use the
parse parenthesis parenthesis within
your chunk parser to parse your sentence
the sentence provided is the little
mouse ate the fresh cheese this sentence
has been parsed and the tokens that
match the regular expressions are
chunked together into noun phrases NP
create a verb phrase chunk using regular
Expressions the regular expression has
been defined as optional personal
pronoun followed by zero or more verbs
with any of its type followed by any
type of adverb you'll now create another
chunk parser and pass the verb phrase
string to it
create another sentence and tokenize it
add POS tags to it so the new sentence
is she is walking quickly to the mall
and the POS tag has been allocated from
nltk corpora now use the new verb phrase
parser to parse the tokens and run the
results you can look at the given tree
diagram which shows a verb parser where
a pronoun followed by two verbs and an
adverb are chunked together into a verb
parse
structuring sentences chinking
chinking is the process of removing a
sequence of tokens from a chunk how does
chunking work the whole chunk is removed
when the sequence of tokens spans an
entire chunk if the sequence is at the
start or the end of the chunk the tokens
are removed from the start and end and a
smaller chunk is retained if the
sequence of tokens appears in the middle
of the chunk these tokens are removed
leaving two chunks where there was only
one before
consider you create a chinking grammar
string containing three things chunk
name the regular expression sequence of
a chunk the regular expression sequence
of your
here in the given code we have the chunk
regular expression as optional personal
pronoun followed by zero or more
occurrences of any type of the verb type
followed by zero or more occurrences of
any of the adverb types the
regular expression says that it needs to
check for the adverb in the extracted
chunk and remove it from the chunk
inside the chinking block with open
curly braces and closing curly braces
you have created one or more adverbs
you will now create a parser from
nltk.reg exp parser and pass the
grammar to it now use the new
parser to parse the tokens sent three
and run the results
as you can see the parse tree is
generated while comparing the syntax
tree of the parser with that of
the original chunk you can see that the
token is quickly adverb chinked out of
the chunk
let's understand how to use context-free
grammar
a context-free grammar is a four Tuple
sum ntrs where sum is an alphabet and
each character in sum is called a
terminal and T is a set and each element
in NT is called a non-terminal r the set
of rules is a subset of NT times the set
of sum U and t s the start symbol is one
of the symbols in NT a context-free
grammar generates a language L capturing
constituency and ordering in CFG the
start symbol is used to derive the
string you can derive the string by
repeatedly replacing a non-terminal on
the right hand side of the production
until all non-terminals have been
replaced by terminal symbols let's
understand the representation of
context-free grammar through an example
in context-free grammar a sentence can
be represented as a noun phrase followed
by a verb phrase noun phrase can be a
determiner nominal a nominal can be a
noun VP represents the verb phrase a can
be called a determiner flight can be
called a noun
consider the string below where you have
certain rules when you look at the given
context-free grammar a sentence should
have a noun phrase followed by a verb
phrase a verb phrase is a verb followed
by a noun a verb can either be Saul or
met noun phrases can either be John or
Jim and a noun can either be a dog or a
cat check the possible list of sentences
that can be generated using the rules
use the join function to create the
possible list of sentences you can check
the different rules of grammar for
sentence formation using the production
function it will show you the different
tags used and the defined context-free
grammar for the given sentence
demo 2 structuring sentences problem
statement a company wants to perform
text analysis for one of its data sets
you are provided with this data set
named
tweets.csv which has tweets of six U.S
airlines along with their sentiments
positive negative and neutral the tweets
are present in the text column and
sentiments in Airline underscore
sentiment column
we will be retrieving all tags starting
with at the rate in the data set and
save the output in a file called
references.txt let us first import the
pandas library and read the tweets data
set
extract the features text and Airline
sentiment
we will iterate through the data set
using reg X find the relevant tweets
now we will import the enter tools
module it returns efficient iterators
the result is stored in a file named
references.txt
let's extract all noun phrases and save
them in a file named noun phrases for
left carrot Airline underscore sentiment
right carrotreview.txt
here left carrot Airline underscore
sentiment right carrot has three
different values positive negative and
neutral so three files will be created
now we will iterate all the leaf nodes
and assign them to noun phrases variable
this means that the functions in itter
tools operate on iterators to produce
more complex iterators
using the map function we will get all
the noun phrases from the text
putting it into list
creating a file name in the name of
review.txt
great we have now seen how to explore
and examine the Corpus using text
processing techniques also accelerate
your career in Ai and ml with our
conference postgraduate program in Ai
and machine learning boost your career
with this Ai and ml course delivered in
collaboration with Purdue University and
IBM so why wait enroll now and unlock
exciting Ai and ml opportunities the
link is in the description box below so
first we will import some major
libraries of python so here I will write
import
pandas as PD
and both
numpy as NP
then
import
c bond
as SNS
and import
SK learn
dot model selection
quote
train underscore
test underscore split
before that
I will import
matplotlib
dot Pi plot
as PLT
okay then
I will write here from SK learn
Dot
Matrix
import
accuracy
for
than from
Escalon
dot matrix
import
classification
to report
at Port
Ari
then import
string
okay
then press enter
so it is saying
okay
here I have to write from
everything seems good
loading let's see
okay till then number is a python
Library used for working with arrays
which also has function for working with
domain of lineal algebra and matrices
it is an open source project and you can
use it freely
number stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and Time series
then Seaborn
an open source python Library based on
matplotlib is called C bone it is
utilized for data exploration and data
visualization with data frames and the
pandas Library c-bond functions with
ease
than matplotlib for Python and its
numerical extension numpy
matplotlib is a cross platform for the
data visualization and graphical
charting package
as a result it presents a strong open
source suitable formatlab
the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications then this train test
split we may build our training data and
the test data with the aid of SQL and
train test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like hone and test
accuracy score the accuracy score is
used to Gorge the model's Effectiveness
by calculating the ratio of total true
positive to Total to negative across all
the model prediction
this re regular expression
the function in the model allow you to
determine whether a given text fits a
given regular expression or not
which is known as re
okay then string a collection of letters
words or other character is called a
string it is one of the basic data
structure that serves as the foundation
of manipulating data
the Str class is a built-in string class
in Python because python strings are
immutable they cannot be modified after
they have been formed
okay so now let's import the data set we
will be going to import two data set one
for the fake news and one for the True
News or you can say not fake news
okay
so I will write here
EF underscore
big was to
PD Dot
read underscore CSV
or what can I say dear fake
okay
it underscore fake
okay
then
pick
dot CSV
you can download this data set from the
description box below
then data Dot
true
PD dot read
underscore CSV
sorry CSC
plan
fake news sorry true
dot CHP
then press enter
so these are the two data set
you can download these data set from the
description box below so let's see the
board data set okay
then I will write here data underscore
fake
dot head
so this is the fake data okay then
data underscore true
Dot
and this is the two data
okay this is not fake
so if you want to see your top five rows
of the particular data set you can use
head
and if you want to see the last five
rows of the data set you can use tail
instead of head
okay
so let me give some space for the better
visual
so now we will insert column class as a
Target feature okay then I will write
here data
let's go fake
Plus
equals to zero
then
Theta underscore true
and
Plus
was one
okay
then
I will write here data underscore fake
dot shape
and
data underscore true
dot ship
okay then press enter
so the shape method return the shape of
an array the shape is a tuple of
integers these number represent the
length of the corresponding array
dimension in other words a tuple
containing the quantities of entries on
each axis is an array shape dimension so
what's the meaning of shape
in the fake word
in this data set we have two three four
eight one rows and five columns
and in this data set true
we have two one four one seven rows and
five column okay so these are the rows
column rows column for the particular
data set
so now let's move
and let's remove the last 10 rows for
the manual testing okay
then I will write here data underscore
fake
let's go manual
testing
was to
data underscore fake
dot tail
for the last 10 rows I have to write
here 10.
okay so for I
in range
two three four
eight one
sorry zero
comma 2 3
4 7 0
comma minus 1.
okay
and
TF underscore not DF data
underscore fake
dot drop
one
yeah instead of one I can write here I
comma
this is equals to zero
it place
equals to true
then
data
not here
data underscore
same I will write for I will copy from
here
and I will paste it here and I will make
the particular changes
so here I can write true
that I can write true
okay
then I have to change a number
two one
six
right
2 1 4 0 6
-1
same
so press enter
X is equal to zero
let's syntax maybe you mean double zero
or of this
okay we will put here double course
and I'm putting this
egg dot drop i x is equal to 0 in place
okay
and also write equals to a question
yeah
so
okay axis is not defined
thank you
now it's working so
let me see
now
did the
underscore
fake dot shape
okay
and data dot true
and
data underscore true
dot shape
as you can see
10 rows are deleted from each data set
yeah
so I will write here data underscore
fake underscore manual
testing
class
equals to zero
and data underscore
true
let's go
manual underscore testing
class equals to
1.
just ignore this warning
and
let's see
data underscore
fake underscore
manual
testing dot hat
as you can see we have this
and then data dot sorry underscore true
underscore
manual
testing
dot at
this is this is the true data set
so here I will merge data
underscore merge
pursue
PD Dot
concat
concat is used for the concatenation
data underscore fake
data underscore
comma
axis
equals to zero
then data underscore merge
dot head
the top 10 rows
yeah
as you can see the data is merged
here
okay
first it will come for the fake news and
then with that for the True News
and let's merge true and fake data
frames
okay
we did this and
let's merge the column then data dot
merge
Dot columns or let's see the columns
it is not defined what are the data
underscore much
these are the column same title text
subject date class okay
now
let's remove those columns which are not
required for the further process
so here I will write data underscore
or request to
data underscore merge
crop
title I don't need
that
subject
we don't need
then
so one
so let's check some null values
it's giving here
because of this
that's good then data
dot is null
dot sum
Center
so no null values
okay then
let's do the random shuffling of the
data frames okay
for that we have to write here data
equals to
data dot sample
one
then
data
okay
data
dot hat
now you can see here the random
shuffling is done
and one for the
true data reset and zero for the fake
news one okay
then
let me write here data Dot
reset
underscore index
place
because you
true
a dot dot drop
comma X is
equals to 1
then comma in place
true true
okay
then let me see columns now data Dot
columns
so here we have two columns only rest we
have deleted
okay
see data Dot at
yeah
everything seems good
let's proceed further and let's create a
function to process the text okay
for that I will write here
but
okay
you can use any name
text
and text equals to
text Dot lower
okay
and text
for the substring
remove these things
from the
datas
okay
so for that I'm writing here
comma
okay
then text equals to
re Dot substring
comma
comma text
okay
then I have to write text equals to
r e Dot substring
www
Dot
S Plus
comma
from our text
okay then text equals to
re Dot substring
then
oh
comma
then
text equals to
re Dot substring
and
percentage
as
again percentage or
RG dot SK function
right here string
dot punctuation
comma
and Gamma then text
right
then text equals to re Dot substring
and n
comma
x equals to
re Dot
substring
right here
and again d
then again
then comma
and again
texture
okay then at the end I have to write
here return text
so everything like this these type of
special character will be removed from
the data set
okay let's run this let's see
yeah so here I will add DF sorry not DF
data
data
then
next
pursue
data
okay dot apply
to the function name what part word opt
okay
press enter
yeah so now let's define the dependent
and independent variables okay x equals
to
data
text
and Y equals to
data
class
okay
then splitting training and testing data
okay sorry
so here I will write X underscore train
comma X underscore test
uh then y underscore train
comma y underscore test equals to
train underscore test underscore split
then X comma y
comma test
let's go size equals to
0.25
okay press enter
so now let's convert text to vectors
for that I have to write here
that it's X
so here I will write from sqlarn
dot feature
extraction
Dot text import
t
vectorizer
okay
then vectorization
plus 2 tfid
factorizer
okay
then
three
underscore
train
equals to
vectorization
refactorization dot fit
that transform
ERS code train
okay
then
XV underscore test equal to
factorization
Dot transform
X underscore test
okay then press enter
[Music]
foreign
logistic regression
so here I will write
from
sqln Dot
linear underscore model
okay import
logistic
regression
then a lot
goes to
logistic
regression
to write here LR Dot
fit
then XV
Dot
dot so dot train
comma
x v underscore test
okay
press enter
dot XV dot train
okay here I have to write y train
and press enter
will work so here I will write
prediction
underscore
linear regression
l r dot predict
XV underscore test
okay let's see the accuracy score
for that I have to write l r dot score
then XV underscore test
comma y underscore test
okay
let's see the accuracy so here as you
can see accuracy is quite good 98
percent
now let's print
the classification
code
by underscore test comma
prediction of linear regression
okay
so this is you can see Precision score
then F1 is code then support value
accuracy
okay
so now we will do this same for the
decision tree gradient boosting
classifier random for this classifier
okay then we will do model testing then
we will predict this score
so now for the decision tree
classification so for that I have to
import from SK learn
Dot 3
import
decision
free
classifier
than at the short form I will write here
I will copy it from here
then
okay
then I have to write the same as this so
I will copy it from here
and
here
let's change linear regression
to
season 3 classified
okay
then I will write here same
let's go DT
question
DT dot product
XV underscore
test
e
still loading it's it will take time
okay
till then let me write here for the
accuracy
DT DOT score
three underscore test
comma y
let's wait okay
let's run
accuracy
so as you can see accuracy is good than
this linear regression
okay logistic regression
okay so let me
show you the
let me predict
trend
okay
so this is the accuracy score this is
the all the report
yeah
so now let's move for the gradient
boosting classifier
okay for that I've read from
sqlarn
dot ensemble
port
gradient
boosting
classifier
pacifier
I will write here GB
equals to let me copy it from here
I will give here random
let's go state
equals to zero
wait wait wait so I will write here GB
Dot
fit
x 3 underscore train
comma
y underscore train okay then press enter
here I will write predict
underscore GB
also
GB Dot
wait sorry
predict
three
Dot test
dot dot underscore test
till then it's loading so I will write
here uh it's for this code then I will
add GB
DOT score
then
three underscore test
comma
y underscore test
okay
so let's wait it is running this part
till then let me write for the
printing this
case taking time
taking time still taking time
45 will run this
it's not coming because of this
yeah it's done now so you can see the
accuracies
not good then
decision tree but yeah it is also good
99
.4 something okay so
now let's check for the last one random
Forest
first I will do
for the random for us we have to write
from sqlarn Dot
symbol
import
random
Forest
classifier
okay
and here I will write RF
equals to
right I will copy it from here
then
random
date
equals to
zero
and
RF Dot
fit
three underscore train
comma y underscore train
okay then press enter
and predict
underscore
RC
or F
equals to
RF dot predict
three underscore test
okay
till then I will write it still loading
it will take time
so till then I will write for the score
score accuracy score
XV underscore test comma y underscore
test
okay
then I will write here till then print
classification
port
and Y underscore test
comma
it will take time little bit
so
it run the accuracy score is 99 it is
also good
so now I will write the code for the
model testing so I will get back to you
but after writing the code so
so I have made two functions one for the
output label and one for the manual
testing okay
so it will predict
the all the from the all models from the
repeat so it will predict
the the news is fake or not from all the
models okay
so for that
let me write
here news
string
what
okay
then I will let him manual underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want
so I'm just copying from the internet
okay from the Google
the news
which is not fake okay I'm adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is showing
okay string input object is not callable
okay let me check this first
a I have to give here Str only
yeah let's check
okay I have to add here again the script
yeah
manual testing is not defined
let me see manual testing
okay I have to edit something
it is just GB and it is just RF
in GPS is not defined okay okay
so what I have to do
I have to remove this
this
everything seems sorted
now
as I said to you
I just copied this news from the
internet I already know the news is not
fake so it is showing not a fake news
okay so now what I will do I will copy
one fake news from the internet
and let's see it is detecting it or not
okay
so let me run this
and let me add the news for this
so
all the models are predicting right it
is a fake news
or you can add your own script like this
is the fake news okay
I hope you guys understand
till here
so I hope you guys must have understand
how to detect a fake news using machine
learning you can you can copy it any
news from the internet and you can check
it is fake or not okay or if your model
is predicting right or not today we will
take you through our hands of lab demo
of how to do image classification using
CNN before we start I hope this screen
is clearly visible and the audio is fine
if yes please type in yes if there are
any issues to let us know in the chat
section so that we can resolve them
let's wait for some more minutes to let
other people join until then let me tell
you guys that we have regular updates on
multiple Technologies if you are a tech
geek in a continuous hunt for the latest
technological Trends then consider
getting subscribed to our YouTube
channel and press that Bell icon to
never miss an update from Simply learn
great I think we can get started
so in today's session we will discuss
what image classification is and moving
ahead we will discuss what CNN is and at
the end we will do a hands of lab demo
of image classification using CNN
so before we move on to the programming
part let's discuss what image
classification is and proceed further
for the same what is image
classification
the process of classifying an entire
image is known as image classification
images are anticipated to have just one
class per image models for image
classification taken an image as input
and produce a prediction of the class to
which the image belongs
we can utilize image classification
models when we are not interested in
individual instance of items with
position information or their shape
so let's see what is CNN
machine learning includes convolutional
neural networks also known as convents
or cnns it is a subset of the several
artificial neural network models that
are employed for diverse purpose and
data sets a CNN is a particular type of
network design but deep learning
algorithm that is utilized for tasks
like image recognition and pixel data
processing and so more
okay
although there are different kinds of
neural network in deep learning
cnns are preferred neural architect for
identify and recognizing object
therefore they are really suited for
computer vision activities and
application where accurate object
recognition is crucial
such as facial and self-driving automile
system so moving ahead
so dear Learners if you want to upskill
your AI and machine learning skill so
give yourself a chance to Simply end
professional certificate program in Ai
and machine learning which comes with
the completion certificate and in-depth
knowledge of AI and machine learning
check this course out details from the
description box below so now let's move
to our programming part of how to do
image classification using CNN if
getting your learning started is half
the battle what if you could do that for
free visit skillup by simply learn click
on the link in the description to know
more so first we will open a command
prompt to write a command to open
Jupiter notebook so here we will write
Jupiter
notebook
press enter
so this is the landing page of jupyter
notebook so here you can select new
python file
so this is how the kernels look like
okay jupyter notebook Corners look like
so first we will import some major
libraries of python which will help us
in like analyzing the data okay so in
this file we will classify small images
of cifar 10 data set from tensorflow
Keras data set there are total 10
classes as shown below so we will use
CNN for the classification purpose okay
so here I will write import
sensorflow
as TF
okay
so from
tensorflow
dot Keras
import
data sets
from our layers
of models
okay
so we will import
numpy as NP
right
and import
matplotlib
as
PLT
right okay so here I will write By plot
as PLT
right so tensorflow this one
this
so tensorflow is a free and open source
machine learning and artificial
intelligence software Library
it can be used for variety of
applications
but it focuses on mainly deep neural
network training and the influence
purpose okay got it
and this numpy numpa is a python Library
used for working with the arrays it also
has a function for working with the
domain of lineal algebra and matrices it
is an open source project and you can
use it freely number stand for numerical
python
and this third one matplotlib
for Python and its numerical extension
numpy matplotlib is a cross-platform
data visualization and graphical
charting package as a result it presents
a strong open source substitute for
Matlab the apis application programming
interfaces for matplotlab allow
programmers to incorporate graphs into
GUI applications
got it
so let's run this
let me change image
classification
using
CNN
okay
so let's load the data set okay we will
load the data set from the uh
load data function
so here I will write
X
underscore
train
comma why underscore
okay
and one for test X
underscore test comma y underscore test
okay then equals to data set
result sets Dot s we are using Cipher 10
C for 10
for 10
dot load
Dot
underscore
data okay
so it will load our data so let's load
the data
so data is loaded let's see X
score
test dot shape
okay
yeah
so as you can see
so we have thousand
rows and what what x
underscore train
dot shape
let me run this
so here you can see
like we see like training data
like training images are 50 000 and the
test images are
10 000. okay
so this is for testing this is for
training and
so moving ahead we will see for the Y
train
dot shape
1000 and here we will see the array so y
underscore train
five
okay let's send this first let me give
some space yeah
so here why underscore train is a 2d
array like for our classification having
1D arrays are good enough
so what we will do we will convert this
to now 1D array this is 2D array we will
convert into 1D array
okay
for that
we'll write here y underscore train
was to Y underscore train
dot reshape
minus 1 comma
okay then y underscore train
and again semicolon 5.
let's send this
okay so now this is 1D array
so
y underscore test
equals to Y underscore test
Dot
reshape
n minus 1
here I will write
classes
question there are some classes okay in
the data set like airplane
comma
automobile
comma
but
comma
cat
comma
yeah
dog
course
yep
the truck
okay
so these are the some classes
like airplane automobile bird cat deer
so it will be helping classifying the
images
so let's plot some images to see like
what they are what they exactly are okay
so we will what we will do we will
create one function
for that
let me write here
Def
plot underscore sample
okay then X comma y
comma index
okay
then I will write here plot dot figure
PLT dot figure
then figure size should be
comma 2
equals to
PLT dot show
image show to IM show
X
index
elt
dot X label
classes
y index okay
so let's see some samples of the images
so I will write a plot
underscore sample
uh let's go train
comma y underscore train
okay comma let's see the fifth image
okay then enter
would be capital
so as you can see this is a car so it is
showing automobile okay
so let's see once more
like plot underscore sample
X underscore train
comma y underscore test
comma we'll see the tenth one
it is not quite visible
so we'll go for the 11
.
okay okay I am
why it is showing wrong because I use a
test I have to use here train instead of
test
then it will show I think correct yeah
you can see horse the nodes
then what about
201 image
see you can see
so we will see once more
500
Fancy Frog okay it's not quite visible
yeah so you can see here
the proper ship okay
so what we will do now we will normalize
the images to a number from zero to one
image has three channels like RGB colors
so and each value in the channel can
range from 0 to 255.
hence to normalize in 0 to 1 range we
need to divide it by 255 okay
so
now what we will do we will normalize
the data
so here what we'll do at X underscore
train
a question
X underscore train
divided by
255 and 0.
okay
and same for test
question test
divided by
255.0
okay to range between 0 to 1.
we will build simple artificial neutral
Network for image classification first
okay
so we will write here a n
equals to models Dot
sequential
okay then I will write here
layers
Dot
platen
input underscore
shape equals to
32 comma 32. comma 3.
okay
then again layers dot dance
3000
comma activation
pursue rialu
got it
and again 40
000 so I will what I will do I will copy
it
and paste it here okay
say
1000.
so uh let me add one more one more layer
dot tense
then we'll add 10 comma
activation
also softmax
there I will okay let me give for the
better visuals
yeah
so a n
and Dot compile
optimizer
okay first two
SGD
and comma
I will write a loss
equals to
sparse
categorical
cross entropy okay
so here I will add comma then Matrix
equals to
accuracy
a n dot fit
underscore train
comma y underscore train
comma epochs
okay
so number I will give you box equals to
five
right
so it will take time to run
it's running
okay there is some issue
underscore train
it pops equation
five
what it is saying
end user code the file program C
engine okay
I will copy it
and paste it again
run it again
okay now it's working
okay
so it will take time and then I will get
back to you okay
then import
numpy you guys already know what numpy
is
NP
y underscore prediction
a n dot predict
X underscore test
okay then y underscore
prediction underscore
classes
equals to
and P Dot
argument Max
element
or
element
in
y underscore prediction
print
classification
both
comma classification
let's go report
then y underscore test comma y
underscore
prediction
underscore
classes
okay
so let me run this
capital
yeah
it will take less time
now we will create a graph
okay
like X text is like we have thousand
images so CLA graph will be like messed
up still let's see
so I'd like to import
c bond as SNS
okay
C born
let me give some space
here I will write
PLT Dot
bigger
because
I should be
14 comma 7.
okay then as soon as we will create a
heat map for this
and Y underscore prediction
not question
true
then PLT dot y level
this truth
alt dot X label
action
then PLT dot title should be
division
okay then PLT dot show
let me run this
okay
see why prediction has X text
it's still running
let's wait
so now let's make CNN model okay to
train our images so for that I will add
CNN equals to
models
Dot
sequential
okay
yeah so let me run this
so this is our CNN model from which we
will train our images
and CNN like compile
then
optimize
optimizer
equals to
Adam
okay
comma
right here then loss
equals to
ours
categorical
you then cross
entropy
okay
and comma
I will write here Matrix
equals to
accuracy
we've done this
okay
those
yeah
same goes for you
okay loss
and I do I will rewrite this
okay now let's check CNN model
for the tenor box
okay
let's see the accuracy is increasing or
not
CNN Dot
let's go train
comma y underscore train
comma
box
awesome
it is started
it will take less time than the previous
one
okay
see
you want this whole code
you can comment down the same
okay
so after completing this I will get back
to you
so it is almost done like 27 seconds
will it 10 a box
till then let me write CNN dot evaluate
underscore test
comma y underscore test
okay
foreign
the N5 Epoch security was around like 70
percent and which is a significant
improvement over a n okay and then we
have like just 49
okay and cnns are the best for image
classification and gives the superb
accuracy also computation is the much
less compared to simple a n as Max
pooling reduces the image Dimension
while still uh preserving these features
okay so let me run this
okay
take some time
till then I will write y underscore
prediction
equals to CNN
Dot
predict
and X underscore test
okay
foreign
then column five
let me run this
so you can see the accuracy and all the
array okay
yeah
so let's
class is equal to
NP Dot
ARG Max
element
or
element in y prediction
then y underscore classes
then
these are the number of classes then y
underscore test
is column five
these are the array
so it's converted into array then
now let's see the it is predicting Right
image or wrong image okay by not with
the training data here we predict from
the training data while training extend
okay now we will predict from the test
data so here I will add plot
underscore sample
then
X underscore test comma y underscore
test
and you can write
the random one
so here I will write 60
let's see so you can easily see here
this is odds and it is predicting right
horse
and let me
okay
plot
let's go sample
then X underscore test comma y
underscore test
comma
100.
okay
press enter
okay X is capital
yeah
so you can see this is Dr
okay so our model is predicting the
correct
image
okay then what we like
let's see it is predicting the right
class or not okay we made the classes
like random classes
okay where are these these so let's see
it is predicting right or wrong
okay for that I have to write classes
why underscore classes
like which number 60. okay
60.
360 is not defined because like number
of classes
okay okay
so there are one two three four five six
seven
okay zero to nine I can choose
so here what I will do
I will take small one
five
okay this is frog
okay
it's from right
so I will take instead of 60 here I will
see frog
okay y class is not defined
by underscore classes
is defined see okay there are three s
yeah
so as you can see frog this is frog so
our class is defining right so here I
will write it again like
60
it was horse
and let me write here 60
you can see the right prediction okay
so
this is what how you can you do image
classification using CNN also accelerate
your career in Ai and ml with our
conference postgraduate program in Ai
and machine learning boost your career
with this AI animal course delivered in
collaboration with Purdue University and
IBM so why wait enroll now and unlock
exciting Ai and ml opportunities the
link is in the description box below
hello everyone welcome to this session
I'm Mohan from Simply learn and today
we'll talk about interview questions
now this video probably help you when
you're attending interviews or machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked questions and to help you
in answering these questions we tried
our best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplementary
or answers with your own experience so
the responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be talking
there are three types of machine
learning but what is important is you
would probably be better of emphasizing
that there are actually two main types
of initial learning which is supervised
and unsupervised and then there is a
third type which is reinforcement learn
so supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you are trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
label built so that will have the right
impact
so that is what is supervised learning
when you have existing labeled data
which you then use to train your model
that is known as supervised learning and
unsupervised learning is when you don't
have this labeled data so you have data
it is not labeled so the system has to
figure out a way to do some analysis on
this okay so that is unsupervised
learning and you can then add a few
things like what are the ways of
performing a supervised learning and
unsupervised learning and what are some
of the techniques so supervised learning
we we perform or we do
regression and classification and
unsupervised learning video clustering
and clustering can be of different types
similarly regression can be of different
types but you don't have to probably
elaborate so much if they are asking for
just the different types you can just
mention these and just at a very high
level equation but if they want you to
elaborate give examples then of course
then I think there is a different
question
then the third so we have supervised
then we have unsupervised and then
reinforcement you need to provide a
little bit of information around as well
because it is sometimes a little
difficult to come up with a good
definition for reinforcement
so you may have to little bit elaborate
on how reinforcement learning
right so reinforcement learning works in
in such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and every time it is moving in
the direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal then it is rewarded and every
time it is going in a direction where it
is away from the goal then it is
punished so that is the way you can
little bit explain and this is used
primarily or very very impactful or
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphaco used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
co-world champion all right this much of
information that would be good enough
then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting Maybe
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes
explain this now let's say you're
teaching a child about recognizing some
fruits or something like that
and you're teaching this child about
recognizing let's say three fruits
apples oranges and pineapples okay so
this is a small child and for the first
time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples
and you take this basket to this child
and there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so and
then again you keep repeating that right
so till that basket is over this is
basically how training work in machine
learning also that's how training works
so till the basket is completed maybe
100 fruits you keep showing this child
and then the process what has happened
the child has pretty much memorized
these so even before you finish that
basket right by the time you are halfway
through the child has learned about
recognizing the Apple orange and
pineapple now what will happen after
halfway through initially you remember
it made mistakes in recognizing but
halfway through now it has learned so
every time you show a fruit it will
exactly 100 accurately it will identify
it will say the child will say this is
an apple this is an orange and if you
show a pineapple it will say this is a
pineapple app so that means it has kind
of memorized this data now let's say you
bring another basket of fruits and it
will have a mix of maybe apples which
were already there in the previous set
but it will also have in addition to
Apple it will probably have a banana or
maybe another fruit like a jackfruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this banana and Jackfruit it has not
seen so then what will happen in the
first round which is an equivalent of
your training data set towards the end
it has 100 it was telling you what the
fruits are right Apple was accurately
recognized orange or I was accurately
recognized and pineapples were
accurately recognized right so that is
like a hundred percent accuracy but now
when you get another a fresh set which
were not a part of the original one what
will happen all the apples maybe it will
be able to recognize correctly but all
the others like the Jackfruit or the
banana will not be recognized by the
child right so this is an analogy this
is an equivalent of overfitting so what
has happened during the training process
it is able to recognize or reach 100
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much hundred percent whereas when
you use testing there will be a huge
error which means the loss will be
pretty high and therefore the accuracy
will be also low okay this is known as
overfitting this is basically a process
where training is done training
processes it goes very well almost
reaching 100 accuracy but while testing
it really drops down now how can you
avoid it so that is the extension of
this question there are multiple ways of
avoiding overfitting there are
techniques like what you call
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
drop out in case of neural networks and
a few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stop you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split
the question can be
can be learning when we are trying to
train the model so we have a three-step
process we train the model and then we
test the model and then once we are
satisfied with the test only then we
deploy the model so what happens in the
train and test is that you remember the
labeled data so let's say you have 1000
records with labeling information now
one way of doing it is you use all the
Thousand records for training and then
maybe right which means that you have
exposed all this thousand records during
the training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results right but
there is a flaw there what is the flaw
this is very similar to human beings it
is like you are showing this model the
entire data as a part of training okay
so obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you'd normally do
is you actually as a part of the
training you may ask this data through
the model multiple times so this
thousand records may go through the
model maybe 10 15 20 times still the
training is perfect till the accuracy is
high till the errors are minimized okay
now so
fine which means that your that is what
is known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like like a realistic
situation because when the model is
deployed that is what will happen it
will receive some new data not the data
that it has already seen right so this
is a realistic test so you put some new
data so this data which you have set
aside is for the model it is new and if
it is able to accurately predict the
values that means your training has
worked okay the model got trained
properly but let's say while you are
testing this with this test data you're
getting lot of errors that means you
need to probably either change your
model or retrain with more data and
takes
now coming back to the question of how
do you split this what should be the
ratio there is no fixed uh number again
this is like individual preferences some
people split it into 50 50 test and 50
training Some people prefer to have a
larger amount for training and a smaller
amount for test so they can go by either
60 40 or 70 30 or some people even go
with some odd numbers like 65 35 or
63.33 and 33 which is like one third and
two-thirds so there is no fixed rule
that it has to be something that doesn't
has to be this you can go by your
individual preference all right then you
may have questions around data handling
data manipulation or what you call data
management or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to
and depending on what exactly is the
problem what kind of data it is how
critical it is what kind of data is
missing and what is the type of
corruption so there are a whole lot of
things this is a very generic question
and therefore you need to be a little
careful about responding to this as well
so probably have to illustrate this
again if you have experience in doing
this kind of work in handling data you
can illustrate with examples saying that
I was on one project where I received
this kind of data these were the columns
where data was not filled or these were
the this many rows where the data was
missing that would be in fact a perfect
way to respond to this question but if
you don't have that obviously you have
to provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or corrupt
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing I and you have pretty much half
of your data having these missing values
in terms of number of rows okay that
could be one situation another situation
could be that you have records or data
missing but when you do some initial
calculation how many records are corrupt
or how many rows or observations as we
call it has this missing data let's
assume it is very minimal like a 10
percent okay now between these two cases
how do we so let's assume that this is
not a mission critical situation and in
order to fix this 10 percent of the data
the effort that is required is much
higher and obviously effort means also
time and money right so it is not so
Mission critical and it is okay to let's
say get rid of these records so
obviously one of the easiest ways of
handling the data part or missing data
is remove those records or remove those
observations from your analysis so that
is the easiest way to do but then the
downside is as I said in as in the first
case if let's say 50 percent of your
data is like that because some column or
the other is missing so it is not like
every in every place in every Row the
same column is missing but you have in
maybe 10 percent of the records column
one is missing and another 10 percent
column two is missing another 10 percent
column three is missing and so on and so
forth so it adds up to maybe half of
your data set so you cannot completely
remove half of your data set then the
whole purpose is lost okay so then how
do you handle then you need to come up
with ways of filling up this data with
some meaningful value right that is one
way of handling so when we say
meaningful value what is that meaningful
right let's say for a particular column
you might want to take a mean value for
that column and fill wherever the data
is missing fill up with that mean value
so that when you're doing the
calculations your analysis is not
completely way off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
a task right there may be situations
where if the missing values instead of
putting mean may be a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this
okay so then the next question can be
how can you choose a classifier based on
a training set data size again this is
one of those questions where you
probably do not have like a one size
fits-all answer first of all you
may not let's say decide your classifier
based on the training set site maybe not
the best way to decide the type of the
classifier and even if you have to there
are probably some thumb rules which we
can use but then again every time so in
my opinion the best way to respond to
this question is you need to try out few
classifiers irrespective of the size of
the data and you need to then decide on
your particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just buy if somebody defines a
problem to you and somebody even if they
show the data to you or tell you what is
the data or even the size of the data I
don't think there is a way to really say
that yes this is the classifier that
will work here no that's not the right
way so you need to still you know test
it out get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have a question around confusion Matrix
so the question can be explained
confusion Matrix
so confusion Matrix I think the best way
to explain it is by taking an example
and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and it
is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many know so you have that
information and you have the predicted
values how many yes and how many know
right so the total actual values the
total yes is 12 plus 130 and they are
shown here and the actual value those
are 9 plus 3 12 okay so that is what
this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nodes okay so this is
the way to look at this confusion Matrix
okay and uh out of this what is the
meaning converting so there are two or
three things that needs to be explained
out right the first thing is for a model
to be accurate the values across the
diagonal should be high like in this
case right that is one number two the
total sum of these values is equal to
the total observations in the test data
set so in this case for example you have
12 plus 3 15 plus 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix numbers is equal to the
size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 12 21 and divide
it by the total so in this case what
will it be let me uh take a pen so your
your diagonal values is equal to if I
say t is equal to 12 plus 9 so that is
21 right and the total data set is equal
to right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85 percent right
so this is 85 percent so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be paired with an example so that it
becomes easy for you don't have to
calculate those numbers on the fly right
so a couple of hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and then
otherwise it will be pretty difficult to
so if we use the same example of the
confusion Matrix
and we can explain that so A confusion
Matrix looks somewhat like this and when
we just
look somewhere like this and we continue
with the previous example where this is
the actual value this is the predicted
value and in the actual value we have 12
plus 1 13 yeses and 3 plus 9 12 nodes
and the predicted values there are 12
plus the 15 yeses and one plus nine ten
loss okay now this particular case which
is the false positive what is a false
positive first of all the second word
which is positive okay is referring to
the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right 9 is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way a you can probably answer in
your own words but the way the model
development of the machine learning
model
first of all you try to understand the
problem and try to figure out whether it
is a classification problem or a
regression problem based on that you
select a few algorithms and then you
start the process of training these
models
so you can either do that or you can
after due diligence you can probably
decide that there is one particular
algorithm which is more suitable usually
it happens through trial and error
process but at some point you will
decide that okay this is the model we
are going to use
so in that case we have the model
algorithm and the model decided and then
you need to do the process of training
the model and testing the model and this
is where if it is supervised learning
you split your data the label data into
training data set and test data set and
you use the training data set to train
your model and then you use the test
data set to check the accuracy whether
it is working fine or not so you test
the model before you actually put it
into production right so once you test
the model you're satisfied it's working
fine then you go to the next level which
is putting it for production and then in
production obviously new data will come
and
the inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values whether it is
you know so this can be an iterative
process so it is not a straightforward
process where you do the training
through the testing and then you move it
to production now so during the training
and test process there may be a
situation where because of either
overfitting or things like that the test
doesn't go through which means that you
need to put that back into the training
process so that can be a an iterative
process not only that even if the
training and test goes through properly
and you deploy the model in production
there can be a situation that the data
that actually comes the real data that
comes with that this model is failing so
in which case you may have to once again
go back to the drawing board or
initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will let you rewrite
so that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on
so net net this is a continuous process
of tweaking the model and testing it and
making sure it is up to date then you
might have question around deep learning
so because deep learning is now
associated with AI artificial
intelligence and so on so it can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then obviously the
question would be then what is the
difference right so deep learning you
need to mention there are two key parts
that interviewer will be looking for
when you are defining deep learning so
first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
scope and deep learning is one one part
of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and this data has several let's say if
it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have census data and we
have all this so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many teachers and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manual
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on low
end system so you don't need a large
machine and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural line
so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines because it needs a lot
of computing power and the Machine
learning features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of then you
might get a question around the uses of
machine learning or some real life
applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
or actually supervised machine learning
it could be a very specific question
around supervised decision learning so
this is like give examples of supervised
machine learning use of supervised
machine learning in modern business so
that could be the next question so there
are quite a few examples or quite a few
use cases if you will for supervised
machine learning the very common one is
email spam detection so you want to
train your application or your system to
detect between spam and non-spam so this
is a very common business application of
a supervised machine learning so how
does this work the way it works is that
you obviously have historical data above
your emails and they are categorized as
spam and not spam
that is what is the labeled information
and then you feed this information or
the all these emails as an input to your
model right and the model will then get
trained to detect which of the emails
are to detect which is Spam and which is
not spam so that is the training process
and this is supervised machine learning
because you have labeled data you
already have emails which are tagged as
spam or not spam and then you use that
to train your model right so this is one
example now there are a few industry
specific applications for supervised
machine learning one of the very common
ones is a healthcare Diagnostics in
healthcare Diagnostics you have these
images and you want to train models to
detect whether from a particular image
whether it can find out if the person is
sick or not whether a person has cancer
or not right so this is a very good
example of supervised machine learning
here the way it works is that existing
images it could be x-ray images it will
be MRI or any of these images are
available and they are saying that okay
this x-ray image is deflective of the
person has an illness or it could be
cancer whichever illness right so it is
stacked as defective or clear or good
image and Effectiveness something like
that so we come up with a binary or it
could be multi-class as well saying that
this is defective to 10 percent this is
25 and so on but let's keep it simple
you can give an example of just a binary
classification that would be good enough
so you can say that in healthcare
Diagnostics using which we need to
detect whether a person is ill or
whether person cancer or not so here the
way it works is you feed labeled images
and you allow the model to learn from
that so that when New Image is fed it
will be able to predict whether this
person is having that illness or not
having cancer or not right so I think
this would be a very good example for
supervised machine learning in modern
business all right then we can have a
question like so we've been talking
about supervised and uh unsupervised
then so there can be a question around
semi-supervised machine learning so what
is semi-supervised machine learning now
semi-supervised learning as the name
suggests it falls between supervised
learning and unsupervised learning but
for all practical purposes it is
considered as a art of supervised
learning and the reason this has come
into existence is that in supervised
learning you need labeled data so all
your data for training your model has to
be labeled now this is a big problem in
many Industries or in many under many
situations getting the able data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of a diagnostic images every
just
three images now there are actually
millions of x-ray images available all
over the world but the problem is they
are not labeled so their images are
there but whether it is effective or
whether it is good and information is
not available along with it right in a
form that it can be used by a machine
which means that somebody has to take a
look at these images and usually it
should be like a doctor and then say
that okay yes this image is clean and
this image is cancerous and so on and so
forth now that is a huge effort by
itself so this is where semi-supervised
learning comes into play so what happens
is there is a large amount of data maybe
a part of it is labeled then we try some
techniques to label the remaining part
of the data so that we get completely
labeled data and then we train our model
so I know this is a little long winding
explanation but unfortunately there is
no quick and easy definition for
semi-supervised machine learning this is
the only way probably to explain this
concept
we may have another question as what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in a
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together
then Association you can I think the
best way to explain Association is with
an example in case of Association you
try to find out how the items are linked
to each other
example if somebody bought maybe a
laptop or the person has also purchased
a mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying and laptops are also buying the
mouse so that means there is an
association between laptops and
people who are buying red are also
buying
buying so that is a Association that can
be created so this is unsupervised
learning one of the techniques
all right then we have very fundamental
question what is the difference between
supervised and unsupervised machine
learning so machine learning these are
the two main types of machine learning
supervised and answers and in case of
supervised and again here probably the
key word that the person may be wanting
to hear is labeled data now very often
people say we have historical data and
if we run it it is supervised and if we
don't have historical data yes but you
may have historical data but if it is
not labeled then you cannot use it for
so it is it's very key to understand
that we put in that keyword labeled so
when we have labeled data for training
our model then we can use supervised
learning and if we do not have labeled
data then we use unsupervised learning
and there are different algorithms
available to perform both of these
so there can be another question a
little bit more theoretical and
conceptual in nature this is about
inductive machine learning and
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
ready they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
exclamation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it what can be an
example so we can probably tell the
person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
a fire can cause damage
right so this is inductive learning
compared to that what is deductive
learning so here you draw conclusion or
the person draws conclusion out of
experience so we will stick to the
analogy so compared to the showing a
video Let's assume a person is allowed
to play with fire right and then he
figures out that if he puts his finger
it's burning or you throw something into
the fire it burns so he is learning
through experience so this is known as
deductive learning okay so you can have
applications or models that can be
trained using inductive learning or
deductible all right I think probably
that explanation will
sufficient the next question is rknn and
K means clustering similar to one
another or are they same right because
the letter K is kind of common between
them okay so let us take a little while
to understand what these two are one is
KNN another is K means k n stands for K
nearest neighbors and K means of course
is the clustering mechanism now these
two are completely different except for
the letter K being common between them
KN is completely different K means
clustering is complete
KNN is a classification process and
therefore it comes under supervised
learning whereas k-means clustering is
actually a unsupervised okay when you
have K and N when you want to implement
k n n which is basically K nearest
neighbors the value of K is a number so
you can say k is equal to 3 you want to
implement KN with K is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular but it is possible that they
may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign odd number two so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K
as3 the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
the three of them belong to Class 2 so
which means that this object will belong
to class 2 right so you see that so it
is the class allocation can vary
depending on the value of K now K means
on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
the similarity of course is the the
letter K and in case of K means also we
specify its value and it could be three
or five or seven there is no technical
limit as such but it can be any number
of clusters that you can create okay so
based on the value that you provide the
system will create that many clusters of
similar objects there is a similarity to
that extent that K is a number in both
the cases but actually these two are
completely different processes
we have what is known as naive base
classifier and people often get confused
thinking that naive base is the name of
the person who found this classifier or
who developed this classifier which is
not 100 True base is the name of the
person bais is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of his particular
classifier an ibase classifier is a
probability based classifier and it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but even if we go with this assumption
this whole algorithm works very well
even with this assumption and uh that is
the good side of it but the term comes
from there so that so that is the
explanation that you can
then there can be question around
reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can
or it can be any
unique the best way to explain this is
again to talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either it is related to game then the
goal could be that you have to score
very high score
or it could be that your number of lives
should be as high as possible don't lose
life so this could be some of them a
more advanced examples could be for
driving the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
that
well it is rewarded and every time it
takes a step which goes against that
core right contrary or in the reverse
Direction it is penalized okay so it is
like a character elastic
now how do you use this to create a game
of chess so to create a system to play a
game on chess now the way this works is
and this could probably go back to this
alphago example where alphaco defeated a
human Channel so the way it works is in
reinforcement learning the system is
allowed for example if in this case we
are talking about Chess so we allow the
system to first of all watch playing a
game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being play the game because this is
reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent it works on
that so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you are playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one two three four or five times
but hundreds of times thousands of times
maybe even when hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to play chess and learn on
its own over a period of repetition so I
think you can probably explain it to
this much to this extent and I
are sufficient
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is
okay so we have to try out a bunch of
algorithms see which one gives us the
best performance and best accuracy and
then decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should
then the questions may be around
application of machine learning and this
question is basically around how Amazon
is able to recommend other things to buy
so this is around recommendation engine
how does it work how does the
recommendation engine work so this is
basically the question is all about so
the recommendation engine again Works
based on various inputs that are
provided obviously something like uh you
know Amazon a website or e-commerce site
like Amazon collects a lot of data
around the customer Behavior who is
purchasing what and if somebody is
buying a particular thing they're also
buying something else so this kind of
Association right so this is the
unsupervised learning we talked about
they use this to associate and Link or
relate items and that is one part of it
so they kind of build association
between items saying that somebody
buying this is also buying this that is
one part of it then they also profile
the users right based on their age their
gender their geographic location they
will do some profiling and then when
somebody is logging in and when somebody
is shopping kind of the mapping of these
two things are done they try to identify
obviously if you have logged in then
they know who you are and your
information is available like for
example your age maybe your agenda and
where you're located what you purchased
earlier right so all this is taken and
the recommendation engine basically uses
all this information and comes up with
recommendations for a particular user
user so that is how the recommendation
engine work all right then the question
can be something very basic like when
will you go for classification versus
regression right when do you do
classification instead of regression or
when will you use classification instead
of regression now yes so so this is
basically going back to the
understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression the use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values another is the
continuous value so for regression if we
are trying to find some continuous
values you use regression whereas if you
are trying to find some discrete values
you use classification and then you need
to illustrate what are some of the
examples so classification is like let's
say there are images and you need to put
them into classes like cat dog elephant
tiger something like that so that is the
classification problem or it can be that
is a multi-class classification problem
it could be binary classification
problem like for example whether a
customer will buy or he will not buy
that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not it's a classification
problem that's a binary classification
right whether it's going to rain or not
rain however you also have to predict
what is going to be the temperature
tomorrow right now temperature is a
continuous value you can't answer the
temperature in a yes or no kind of a
response right so what will be the
temperature tomorrow so you need to give
a number which can be like 20 degrees 30
degrees or whatever right so that is
where you use regression one more
example is stock price prediction so
that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continue so
the next question is more about a little
bit of a design related question to
understand your concept
s so it is how will you design a spam
filter so how do you basically design on
development so I think the main thing
here is he is looking at probably
understanding your Concepts in terms of
uh what is the algorithm you will use or
what is your understanding about
difference between classification and
regret
and things like that right and the
process of course the methodology and
the process so the best way to go about
responding to this is we say that okay
this is a classification problem because
we want to find out whether an email is
a spam or not spam so that we can apply
the filter accordingly so first thing is
to identify what type of a problem it is
so we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector machines for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you your training data set to
train your model that or your algorithm
that you have used or rather the model
actually so and you actually will have
three models let's say you are trying to
test out three algorithms so you will
obviously have three models so you need
to try all three models and test them
out as well see which one gives the best
accuracy and then you decide that you
will go with that model okay so training
and test will be done and then you zero
in on one particular model and then you
say okay this is the model will you use
we will use and then go ahead and
Implement that or put that in production
so that is the way you design a spam
filter the next question is about random
RS what is random form so this is a very
straightforward question however the
response you need to be again a little
careful while we all know what is random
Forest explaining this can sometimes be
tricky so one thing is random Forest is
kind of in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and trees will basically we will
use for doing if it is classification
mostly it is classification you will use
the the trees for classification and
then you use voting for finding the
final class so that is the underlyings
but how will you explain this how will
you respond to this so first thing
obviously we will say that random Forest
is one of the algorithms and the more
important thing that you need to
probably the interviewer is is waiting
to here is Ensemble learner right so
this is one type of Ensemble learner
what is Ensemble learner Ensemble
learner is like a combination of
algorithms so it is a learner which
consists of more than one algorithm or
more than one or maybe models okay so in
case of random Forest the algorithm is
the same but instead of using one
instance of it we use multiple instances
of it and we use so in a way that is a
random Forest is an ensemble there are
other types of Ensemble Learners where
we have like reuse different algorithms
itself so you have one maybe logistic
regression and a decision tree combined
together and so on and so forth or there
are other ways like for example
splitting the data in a certain way and
so on so that's all about on some not
going to that but random Forest itself I
think the interviewer will be happy to
hear this word Ensemble Learners and so
then you go and explain how the random
Forest works so if the random Forest is
used for classification then we use what
is known as a voting mechanism so
basically how does it work let's say
your random Forest consists of 100 trees
and each observation you pass through
this forest and each observation let's
say it is a classification problem
binary classification zero or one and
you have 100 trees now if 90 trees say
that it is a zero and ten of the trees
say it is a one you take the majority
you may take a vote and since 90 of them
are saying zero you classify this as
zero then you take the next observation
and so on so that is the way random
Forest works for classification if it is
a regression problem it's somewhat
similar but the only thing is instead of
what what we will do is sorry in
regression remember what happens you
actually calculate a value right so for
example you're using regression to
predict the temperature and you have 100
trees and each tree obviously will
probably predict a different value of
the temperature they may be close to
each other but they may not be exactly
the same value so these hundred trees so
how do you now find the actual value the
output for the entire Forest right so
you have outputs of individual trees
which are a part of this Forest but then
you need to find the final output of the
forest itself so how do you do that so
in case of regression you take like an
average or the mean of all the hundred
trees right so this is also a way of
reducing the error so maybe if you have
only one tree and that one tree makes a
header it is basically hundred percent
wrong or 100 right right but if you have
on the other hand if you have a bunch of
trees you are basically indicating that
reducing that error okay so that is the
way random Forest works so the next
question is considering the long list of
machine learning algorithms how will you
decide on which one to use so once again
here there is no way to outright say
that this is the algorithm that we will
use for a given data set this is a very
good question but then the response has
to be like again there will not be a
one-size-fits all so we need to first of
all you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithm
right so for example if it's a class
station you cannot use linear regression
algorithm or if it is a regression
problem you cannot use svm or maybe now
you can use svm but maybe a logistic
regression right so to that extent you
can probably shorten the list but still
you will not be able to 100 decide on
saying that this is the exact algorithm
that I am going to use so the way to go
about is you choose a few algorithms
based on what the problem is you try out
your data you train some models of these
algorithms check which one gives you the
lowest error or the highest accuracy and
based on that you choose that particular
algorithm
all right then there can be questions
around bias and variants so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual value so that
is the bias okay and whereas they are
all all the values are probably they are
far off but they are very near to each
other though the predicted values are
close to each other right while they are
far off from the actual value but they
are close to each other you see the
difference so that is bias and then the
other part is your variance now variance
is when the predicted values are all
over the place right so the variance is
high that means it may be close to the
Target but it is kind of very scattered
so the point the predicted values are
not close to each other right in case of
bias the predicted values are close to
each other but they are not close to the
Target but here they may be close to the
Target but they may not be close to each
other so they are a little bit more
scattered so that is what in case of a
variance okay then the next question is
about again related to bias and variance
what is the trade-off between bias and
variance yes I think this is a
interesting question because these two
are heading in different directions so
for example if you try to minimize the
bias variance will keep going high and
if you try to minimize the variance bias
will keep going high and there is no way
you can minimize both of them so you
need to have a trade-off saying that
okay this is the level at which I I will
have my bias and this is the level at
which I will have variance so the
trade-off is that pretty much attack you
you decide what is the level you will
all rate for your bias and what is the
level you will tolerate for variance and
a combination of these two in such a way
that your final results are not way off
and having a trade-off will ensure that
the results are consistent right so that
is basically the output is consistent
and which means that they are close to
each other and they are also accurate
which that means they are as close to
the Target as possible right so if
either of these is high then one of them
will go off the track define precision
and Recall now again here I think it
would be best to draw a diagram and take
up in the confusion Matrix and it is
very simple the definition is like a
formula your Precision is true positive
by true positive plus false positive and
your recall is true positive by true
positive plus false negative okay so
that's you can just show it in a
mathematical way that's pretty much you
know
that's the easiest way to define so the
next question can be about decision tree
what is decision tree pruning and why is
it so basically decision trees are
really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and conditions can become very
complex
and this can also lead to overfitting
which is basically that during training
you will get 100 accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose or the
reason for doing decision tree pruning
is to reduce overfitting or to cut down
on overfitting and what is decision tree
pruning it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing those internal
nodes all right so the next question can
be what is logistic regression uh so
basically logistic regression is one of
the techniques used for performing
classification especially binary
classification now there is something
special about logistic regression and
there are a couple of things you need to
be careful about first of all the name
is a little confusing it is called
logistic regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if it's
really
quiet and they can also ask this like a
trick question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from the log so that the mathematical
derivation was log and therefore the
name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between 0
and 1 and we can set a threshold that
can vary typically it is 0.5 so any
value above 0.5 is considered as 1 and
if the probability is below 0.5 it is
considered as zero so that is the way we
calculate the probability of the system
calculates the probability and based on
the threshold it sets a value of 0 or 1
which is like a binary classification
zero or one okay then then we have a
question around K nearest neighbor
algorithm so explain K nearest neighbor
algorithm so first of all what is the K
nearest neighbor algorithm this is a
classification algorithm so that is the
first thing we need to mention and we
also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you are explaining
I think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
you want to find out which class this
belongs right so you go about as the
name suggests it you go about finding
the nearest neighbors right the points
which are closest to this and how many
of them you will find that is what is
defined by K now let's say our initial
value of K was Phi okay so you will find
the K the five nearest data points so in
this case as it is Illustrated these are
the five nearest data points but then
all five do not belong to the same class
or cluster so there are one belonging to
this cluster one the second one
belonging to this cluster two three of
them belonging to this third cluster
okay so how do you decide that's exactly
the reason we should as much as possible
try to assign an odd number so that it
becomes easier to assign this so in this
case you see that the majority actually
if there are multiple classes then you
go with the majority so since three of
these items belong to this class we
assign which which is basically the in
in this case the green or the tennis or
the third cluster as I was talking
right so we assign it to this third
class so in this case it is uh that's
how it is decided okay so K nearest
neighbor so first thing is to identify
the number of neighbors that are
mentioned as K so in this case it is K
is equal to 5 so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that and and then the new data
point is assigned to that class okay so
that's pretty much how K nearest
neighbors work and with that we have
come to end of this video on machine
learning full course I hope you found it
useful and entertaining please ask any
question about the topics covered in
this video in the comments box below our
experts will assist you in addressing
your problems thank you for watching
stay safe and keep learning with simply
learn staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign