foreign
welcome to this new video on eiml and
deep learning interview questions by
simply learn make sure to subscribe to
our Channel and hit the Bell icon to
never miss any updates from us AIML and
DL are interconnected disciplines aimed
at replicating human intelligence in
machines enabling machines to learn from
data and utilizing deep neural networks
for complex tasks alongside these
advancements the field of AI has
witnessed a surge in Innovative tools
such as chat GPT word and other
generative AI tools leading to the war
of AI tools each continuously evolving
to push the boundaries of Technology
this rapid growth in AI has also
resulted in a significant increase in
job opportunities it is projected by
2023 around 12 million AI related jobs
will be available reflecting the high
demand for skilled professional in this
field according to class or the median
base salary for an AI engineer in United
States is hundred and five thousand
dollars indicating the promising earning
potential and attractiveness of AI
careers with the fusion of AI ML and DL
Industries across various sectors
including Healthcare finance and
autonomous vehicles have undergone
transformative changes embracing the
potential of these Technologies this
ongoing Evolution and Innovation with AI
continued to shape the future Paving the
way for exciting advancements and
opportunities in the coming years so if
you are looking to try your hands on
this wheel then do check simply learn
Sports graduate program in Ai and
machine learning so
features of this course includes post
graduate program certificate and Alumni
Association membership a exclusive
hackathons and ask me anything sessions
by IBM three capstones and 25 plus
projects with industry data sets from
Twitter Uber Mercedes-Benz and many more
master classes delivered by Purdue
faculty and IBM experts simply lands job
assist help you to get noticed by top
hiring companies so what you will learn
with this course this course covers chat
GPT generative AI Expendable AI
generative modeling statistics python
supervised learning unsupervised
learning NLP neural networks computer
vision and much more tools covered in
this course will help you to grab
attention from Top recruiters naming a
few python tensorflow Keras chat GPT
nltk scikit-learn matplot live open AI
gym Amazon Siege maker kubernetes Alexa
and more so hurry up and enroll Now find
the course Link in the description box
all Learners have experienced a huge
success in their career listen to their
experience Listen to What a learner says
about are courses in their own words hey
I'm Sharia Jaleel I live in
Canada I have been in I.T sector for the
past 20 years I recently took the
professional certificate program in
artificial intelligence and machine
learning the course has changed the way
I look at things and helped me back some
amazing freelance projects I started my
career in 1999 and over the years I have
worked with many companies my last tenor
was with IBM Canada my aim was to
restart my career and learn something
that would help accelerate my career
I thought artificial intelligence can
make me Future Ready the course in
artificial intelligence and machine
learning is provided by simply learn in
association with Purdue University which
is why I chose the course I did not have
high expectations from online course but
my experience was simply awesome the
quality of interaction within the course
was simply amazing
course faculty was also very experienced
and knowledgeable
after the course my knowledge has grown
manifold I have immensely benefited from
Python and coding skills I am able to
get some new freelance projects also I
am planning to start an AI based startup
with my friend where I feel that the
learning from the course to be very
helpful I am really delighted and happy
in my free time I try to create
meaningful content on YouTube and I talk
about new technologies and what kind of
courses professionals should be taking
along with many other topic I definitely
recommend this course to everyone after
all when it comes to new skills to
advance your career there should be no
compromise you should always learn from
the band so without any further Ado
let's begin with top AI ML and deep
learning interview questions for 2023 or
to our experts we'll start with a very
general concept of what is deep learning
this is where we take large volumes of
data in this case on cats and dogs or
whatever A lot of times you use a
training setup to train your model
remember it's kind of like a magic Black
Box going on there then we use that to
extract features or extract information
and in this case classify the image of a
cat and a dog so the primary takeaway
we're talking about deep learning is it
learns from large volumes of structures
and even unstructured data and uses
complex algorithms to train neural
network it also performs complex
operations to extract hidden patterns
and features and if we're going to
discuss deep learning in this very
simplified overview and we also have to
go over what is a neural network this is
a common image you'll see of a drawing
of a forward propagation neural network
and it's it's human brain inspired
system which replicate the way humans
learn so this has inspired how our own
neurons and our brain fire but at a much
simplified level obviously it's not
ready to take over the human population
and be our leader yet not for many years
it's very much in its infant stage but
it's inspired by how our brains work and
they use a lot of other Inspirations you
can study brains of moths and other
animals that they've used to figure out
how to improve these neural networks the
most common one consists of three layers
of network and this is generally how you
view these networks is you have an input
you have a hidden layer and an output
and the neural network is broken up into
many pieces but we Focus just on the
neural network it's always on the hidden
layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes that perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN Gan Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework let's start
with what is a multi-layer precipitron
or MLP a lot of time is it referred to
and you'll see these abbreviations I'll
be honest I have to write them down on a
piece of paper and go through them
because I never remember what they all
mean even though I play with them all
the time what is a multi-layer
precipitron well if you look at the
image on the right it's very similar to
what we just looked at you have your
input layer your hidden layer and your
output layer and that's exactly what
this is it has the same structure of a
single layer precipitron with one or
more hidden layers except the input
layer each node in the other layers uses
a non-linear activation function what
that means is your input layer is your
data coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer precipitron can classify
only linear reciprobole classes with
binary output 0 1 but the MLP can
classify by non-linear classes so let's
break this down just a little bit the
multi-layer precipitron with an input
layer and a hidden layer and an output
layer as you see that it comes in there
it has adds up all the numbers and
weights depending on how your setup is
it then goes to the next layer that then
goes to the next hidden layer if you
have multiple thin layers and finally to
the output layer the back propagation
takes the error that it sees so whatever
the output is it says hey this has an
error to it it's wrong and it sends that
error backwards from where it came from
and there's a lot of different functions
used to train this based on that error
and how that error goes backwards in the
notes so forward is you get your answers
back where it is for training you see
this every day even my Google pixel
phone has this it they train the neural
network which takes a lot more data to
train than it does to use and then they
load up that neural network into in this
case I have a pixel 2 which actually has
a built-in neural network for processing
pictures and so it's just a forward
propagation I I use when it processes my
photos but when they were training it
you use a back propagation to train it
with the errors they had we'll be coming
back to different models that are used
for right now though multi-layer
precipitron MLP put that down as your
vocabulary word and of course back
propagation what is data normalization
and why do we need it this is so
important we spend so much time in
normalizing our data and getting our
data clean and setting it up so we talk
about data there's a pre-processing step
to standardize the data so whatever we
have coming in we don't want it to be a
you know one gigabyte file here a two
gigabyte picture here and a three
kilobyte text there even as a human I
can't process those all in the same
group I have to reformat them in some
way that Loops them together so the
standardized format we use this data
normalization in pre-processing to
reduce it and eliminate data redundancy
a lot of times the data comes in and you
end up with two of the same images or uh
the same information in different
formats then we want to rescale values
to fit into a particular range for
achieving better convergence what this
means is with most neural networks they
form a bias we've seen this in recently
in attacks on neural networks where they
light up one pixel or one piece of the
view and it skews the whole answer so
suddenly because one pixel is really
bright it doesn't know what to do but
when we start rescaling it we put all
the values between say minus one and one
and we change them and refit them to
those values it helps get rid of that
bias helps fix for some of those
problems and then finally we restructure
the data and improve the Integrity we
want to make sure that we're not missing
values or we don't have partial data
coming in one way to look at this is bad
data in bad data out so we want clean
data in and you want good answers coming
out one of the most basic models used is
a boltzmann machine so let's address
what is a boltzmann machine and if you
know we just did the ml P multi-layer
precipitron so now we're going to come
into almost a simplified version of that
and in this we have our visible input
layer and we have our hidden layer the
boltzmann machines are almost always
shallow they're usually just two layer
neural Nets that make stochastic
decisions whether a neuron should be on
or off true false yes no first layer is
a visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes are
the same layer connected hence it is
also known as restricted boltzmann
machine now that we've covered a basic
MLP or multi-layer precipitron and we've
gone over the boltzmann machine also
known as the restricted boltzmann
machine let's talk a little bit about
activation formulas and this is a huge
topic that can get really complicated
but it also is automated so it's very
simple so you have both a complicated
and a simple at the same time so what is
the role of activation functions in a
neural network activation function
decides whether a neuron should be fired
or that's the most basic one and that
actually changes a little bit because
it's either weather fired or not in this
case activation function or what value
should come out when it's fired but in
these models we're looking at just the
boltzmann restricted layers so this is
what causes them to fire either they
don't or they do it's a yes or no true
false All or Nothing it accepts a
weighted sum of the inputs the bias as
input to any activation function so
whatever our activation function is it
needs to have the sum of the weights
times the input so each input if you
remember on that model and let's just go
back to that model real quick and then
you always have to add a bias and you
can look at the bias if you remember
from your euclidean geometry you draw a
straight line formula for that line has
a y coordinate at the end it's always c
x plus M or something like that where m
is where it crosses the Y coordinates if
you're doing a straight line with these
weights it's very similar but a lot of
times we just add it in as its own
weight we take it as a node of a 1 value
coming in and then we compute its new
weight and that's how we compute that
bias just like we compute all the other
weights coming in the node which gets
fires depends on the Y value and then we
have a step function in the step
function this is where I remember I said
is going to get complicated and simple
all at the same time we have a lot of
different step functions we have the
sigmoid function we have just a standard
step function we have the relu it's
pronounced like Ray the ray from the Sun
and Lou like a name so relu function and
we have the tangent H function and if
you look at these they all have
something similar they all either force
it to be one value or the other they
force it to be in the case of the first
three is zero or one and in the last one
it's either a minus one or a one and you
can easily convert that to a zero one
yes no true false and on this one of the
most common ones is the step function
itself because there is no middle value
there is no discrepancy that says well
I'm not quite sure but as you get into
different models probably the most
commonly used used to be the sigmoid was
most commonly used but I see the relu
used more often really depending on what
you're doing you just have to play with
these and find out which one works best
depending on the data and your output
the reason to have a non-zero one answer
or something kind of in the middle is
when you're looking at this and it's
coming out you can actually process that
middle ground as part of the answer into
another neural network so it might be
that the relu function says hey this is
only a 0.6 not a one and even though the
one is what's going into the next neural
network or the next hidden layer as an
input the 0.6 value might also be going
in there to let you know hey this is not
a straight up one or straight up zero
someplace in the middle this is a little
uncertain what's coming out here so it's
a very powerful tool up in the basic
neural network you usually just use the
step function it's yes or no let's take
a a big step back and take a kind of an
overview the next function is what is a
cost function that we're going to cover
this is so important because this is
your end result that you're going to do
over and over again and use to decide
whether the model is working or not
whether you need to try a different step
function whether you need to try a
different activation whether you need to
try a fully different model used so what
is the cost function cost function is a
measure to evaluate how good your
model's performance is it is also
referred as loss or error used to
compute the error of the output layer
during back propagation there's our back
propagation where we're training our
model that's one of our keywords mean
squared error is an example of a popular
cost function and so here we have the
cost function C equals half of Y minus y
predicted and then you square that so
the first thing is you know real quick
if you haven't done statistics this is
not a percentage it's not a percentage
of how accurate it is it's just a
measurement of the error and we take
that error if we're training it and we
push that error backwards through the
neural network and we use that at
through the different training functions
depending on what model you're using to
train the neural network so when you
deploy the network you're usually done
training it because it takes a lot of
computational force to train it this is
a very simple model and so you deploy
the trained one but we want to know how
your error is and so how do we do that
well you split your data part of your
data is for trading and part of your
data is for testing and then we can also
test the error on there so it's very
important and then we're going to go one
more step on this we've got to look at
both the local and the global setup it
might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests in the error test as far
as your loss you don't you want to make
sure that you're in a closed environment
when you do initial testing but you also
want to open that up and make sure you
follow up with the testing on the larger
scale of data because it will change it
might not fit the larger scale there
might be something in there in the way
you brought the data in specifically or
the data group you used or any of those
could cause an error so very important
to remember that we're looking at both
the local and the global context of our
error and just one other side note on on
a lot of the newer models of neural
networks by comparing the error we get
on the data our training data with a
portion of the test data we can actually
figure out how good the model is whether
it's over fitted or not we'll go into
that a little bit more as we go into
some of the different models so we have
our output we're able to figure out the
error on it based on the Square means
usually although there's other functions
used so we want to talk about what is
gradient descent another vocabulary word
gradient descent is an optimization
algorithm to minimize the cost function
or to minimize the error aim is to find
the local or Global Minima of a function
determine the direction the model should
take to reduce the error so as we're
looking at this we have our squared
error that we just figured out the cop
based on the cost function it says how
bad is my model fitting the data I just
put through it and then we want to
reduce that error so how do you figure
out what direction to do that in well it
could be that you're looking at just
that line of that line of data coming in
so that would be a local Minima we want
to know the error of that particular
setup coming in and then you have your
Global your Global Minima we want to
minimize it based on the overall data
we're putting through it and with this
we can figure out the global minimum
cost we want to take all those local
minimum costs of each piece of data
coming in and figure out the global one
how are we going to adjust this model to
fit all the data we don't want it to be
biased just on three or four lines of
data coming in we wanted to kind of
extrapolate a general answer for all the
data coming in this of course we
mentioned it briefly about back
propagation this is where it really
comes in handy is training our model
neural network technique to minimize the
cost function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our predicted
y coming out and then we have since it's
a training set we already know the
answer and the answer comes back and
based on case the square means was one
of the functions we looked at one of the
activation functions based on cost
function that cost function then
depending on what you choose for your
back propagation method and there's a
number of them we'll change the weights
it will change the weight going to each
of the one of those nodes in the hidden
layer and then based upon the error
that's still being carried back it'll
change the weights going to the next
hidden layer and then it computes an
error level on that and sends that back
up and you're going to say well if it
computes the error into the first hidden
layer and fixes it why would it stop
there well remember we don't want to
create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so it will have the
correct answers so far we've been
talking about forward propagation neural
networks everything goes forwards goes
left to right but let's let's take a
little detour and let's see what is the
difference between a feed forward neural
network and a recurrent neural network
now this is in the function not when
we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones that has a big highlight nowadays
is the CNN a convolutional neural
network tensorflow the one put out by
Google is probably most known for their
CNN where the information goes forward
it first takes a picture splits it apart
goes through the individual pixels on
the picture so it picks up a different
reading then calculates based on that
goes into a regular feed forward neural
network and then gives your
categorization on there now we're not
covering the CNN today but we do have a
video out that you can look up on
YouTube put out by simply learn the
convolutional neural network wonderful
tutorial check that out learn a lot more
about the convolutional neural network
but you do need to know that the CNN is
a forward propagation neural network
only so it's only moving in One
Direction so we want to look at a
recurrent neural network signals travel
in both directions making it a looped
Network considers the current input
along with the previous received inputs
for generating the output of a layer has
the ability to memorize past data due to
its internal memory and you can see they
have a nice image here we have our input
and for some reason they always do the
recurrent neural network in Reverse from
bottom up in the images kind of a
standard although I'm not sure why your
X goes into your hidden layer and your
hidden layer the answer for part of the
answer from that it generates feeds back
into the hidden layer so now you have an
input of both X and part of the Hidden
layer and then that feeds into your
output now if we go back to the forward
let me just go back a slide and we're
looking at our forward propagation
Network one of the tricks you can do to
use just a forward propagation network
is if you're in a what they call a Time
sequence that's a good term to remember
or a Time series meaning that a
sequential data each term comes after
the other you can trick this by creating
your input nodes as with the history so
if you know that you have values 1 5 and
7 going in and you know what the output
is from one what those outputs are you
can expand the input to include the
history input that's one of the ways to
trick a forward propagation Network into
looking at that but when you deal with
the recurrent neural network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the app
applications of recurrent neural network
the RNN can be used for sentiment
analysis and text mining getting up
early in the morning is good for health
it's a positive sentiment one of the
catches you really want to look at this
when you're looking at the language is
that I could switch this around and
totally negate the meaning of what I'm
doing so it would no longer be positive
so when you're looking at a sentence
knowing the order of the words is as
important as the meaning of the words
you can't just count how many good words
there are versus bad words to get a
positive sentiment you now have to know
what they're addressing and there's lots
of other different uses kids are playing
football or soccer as we call it in the
U.S RN can help you capture an image So
based on previous information coming in
it refeeds that back in and you have a
image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or sell of
products can be solved using an RNN and
this is a really good example you have
whatever your stocks were doing earlier
this month will have a huge effect on
what doing today if you're investing so
having an RNN Model A recurrent neural
network feeding into itself was
happening previously allows it to take
that model and program in that whole
series without having to put in the
whole a month at a time of data you can
only put in one day at a time but if you
keep them in order it will look back and
say oh this because what happened
yesterday I need some information from
that and I'm going to use that to help
predict today's and so on and so on
we're going to go back to our activation
functions remember I told you Rayleigh
was one of the most common functions
used so let's talk a little bit more
about relu and also softmax softmax is
an activation function that generates
the output between 0 and 1. it divides
each output such that the total sum of
the outputs is equal to one it is often
used in the output layers softmax L of
the N equals e to L on the N over the
absolute value of e to the L so what
does this function mean I mean what is
actually going on here so we have our
output notes and our output nodes are
giving us let's say they gave us one
point 2.9 and 0.4 as a human being I
look at that and I say well the greatest
value is 1.2 so whatever category that
is if you have three different
categories maybe you're not just doing
if it's a cat or it's a dog or oh let's
say it's a cow maybe we had cats and
dogs earlier why the cats and dogs are
hanging out with a cow I don't know but
we have a value and it might say 1.2 is
a cat 0.9 is the dog and point four is a
cow for some reason it sinks that
there's a chance of it being any one of
these three items and that's how it
comes out of the output layer well as a
human I can look at 1.2 and say this is
definitely what it is it's definitely a
cat or whatever it is Maybe
different kinds of cars might be a
better whether it's a car truck or a
motorcycle maybe that'd be a better
example well from a computer standpoint
that might be a little confusing because
they're just numbers waving at us and so
with the soft Max we want all those
numbers to always add up to one so when
I add three numbers together I want the
final output to be one on there and so
it goes through this formula changes
each of these numbers in this case it
changes them to 0.46 0.34 and 0.20 they
all add up to one and that's a lot
easier to register because it's very set
it's a set output it's never going to be
more than one it's never going to be
less than zero and so you can see here
that there's probably a pretty high
chance that it's the first one so you're
a human being we have no problem knowing
that but this output can then also go
into say another input so it might be an
automated car that's picking up images
and it says that image in front of us is
probably a big truck we should deal with
it like it's a big truck it's probably
not a motorcycle or whatever those
categories are that's the soft Max part
of it but now we have the relu well what
where's the relu coming from well the
relu is what's generating the 1.2 and
the 0.9 and the 0.4 and so if you
remember our relu stands for rectified
linear unit and is the most widely used
activation function we looked at a
number of different activation functions
including tangent age the step function
remember I said the step function is
really used if that's what your actual
output is because then you know it's a
zero or one but the relube if you have
that as your output you now have a
discrepancy in there and if that's going
into another neural network or another
process having that discrepancy is
really important and it gives an output
of x if x is positive and zero otherwise
so it says my x value is going to be
somewhere between 0 or 1 and then the
usually unless it's really uncertain the
output is usually a one or zero and then
you have that little piece of
uncertainty there that you can send
forward to another Network where you can
look at to know that there's uncertainty
involved and is often used in the hidden
layers this is what's coming out of the
Hidden layers into the output layer
usually or as we reference the
convolutional neural network the CNN
you'd have to go to another video to
review the relu is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the Rayleigh works really
good for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layers going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between 0 1 so it's very
standardized so we have our soft Max we
looked at that let's go back one we
looked at that here where it transforms
the numbers and then we have our relu
function which takes the information and
the summation and puts it between a zero
and a one where it's either clearly a
zero or pending on how confident our
model is it'll go between the zero and
one value what are hyper parameters well
this is a great interview question hyper
parameters when you are doing neural
networks this is what you're playing
with most the time once you've gotten
the data formatted correctly a hyper
parameter is a parameter whose value is
set before the learning process begins
determines how a network is trained and
the structure of the network this
includes things like the number of
hidden units how many hidden layers are
you going to have and how many nodes in
each layer learning rate learning rate
is usually multiplied once you've
figured out the error and how much you
want to change the weights we talked
about or I mentioned it earlier just
briefly you don't want to just make a
huge change otherwise you're going to
have a biased model so you only take
some little incremental changes and
that's what the learning rate is is a
small incremental changes epics how many
times are you going to go through all
the data in your training set so one
Epic is one trip through all the data
and there's a lot of other things
depending on which model you're working
with and which programming scripture
working with like the python sklearn
package will have a slightly different
than say Google's tensorflow package
which will be a little bit different
than the spark machine learning package
so these are just some examples of the
hyper parameters and so you see in here
we have a nice image of our data coming
in and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so and we send it back and we
change our hyper parameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set too low or too high
we have a nice couple graphs here we
have one over here is a learning rate
set to load you can see that it slowly
Works its way down the curve and on the
right you can see a learning rate set
too high it's just bouncing back and
forth when your learning rate is too low
that's what we studied two slides over
this what the learning rate was training
of the model will progress very slowly
as we are making very tiny updates to
the weights we'll take many any updates
before reaching the minimum point so I
just mentioned epic going through all
the data might have to go through all
the data a thousand times instead of 500
times for it to train learning rate too
high causes undesirable Divergent
Behavior to the loss function due to
drastic updates and weights at times it
may fail to converge or even diverge so
if you have your learning rate set too
high and it's training too quickly maybe
you'll get lucky and it trains after one
epic run but a lot of times it might
never be able to train because the
weights are changing too fast they flip
back and forth too easy and you see down
here we've introduced two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and has minimized
the error diverge means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there converge and diverge
are important to know also what is drop
out and batch normalization Dropout is a
technique of dropping out hidden
invisible units of a network randomly to
prevent overfitting of data it doubles
the number of iterations needed to
converge the network so here we have our
standard neural network and then after
applying Dropout now it doesn't mean we
actually delete the node the node is
still there and we're still going to use
that note what it means is that we're
only going to work with a few of the
notes a lot of times I think the most
common one right now used is 20 percent
so you'll drop out 20 percent of the
nodes when you do your training you
reverse propagate your data and then
you'll randomly pick another 20 nodes
the next time you go through an epic
data training so each time you go
through one Epic you will randomly pick
20 of those nodes not to not to mess
with and this allows for less
overfitting of the data so by randomly
doing this you create some I guess it
just kind of pulls some nodes off to the
sides it says we're going to handle the
data later on so we don't over fit batch
normalization is a technique to improve
the performance and stability of neural
network the idea is to normalize the
inputs in every layer so that they have
mean output and activation of zero and
standard deviation of one this question
covers a lot of different things which
is great it's a great interview question
because it pulls in that you have to
understand what the mean value is so a
mean output activation of 0 that means
our average activation is zero so when
you normalize it remember usually we're
going between
-1 and 1 on a lot of these it's a very
standard setup so you have to be very
aware that this is your mean output
activation of zero and then we have our
standard deviation of one so we want to
keep our error down to just a one value
the benefits of this doing a batch
normalization is it provides
regularization it trains faster Higher
Learning rates and weights are easier to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the the entire data
set it takes time to converge because
the volume of data is huge and weights
update slowly so you can look at the
batches a lot of times if you're using
big data batch the data in but you still
go through a full epic you still go
through all the data on there so bash
gradient descent means you're going to
use it to fit all the data and look for
convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise and the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with non-linear models that have
more flexibility when learning a Target
function an example of this would be if
you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and rate on maybe a small test amount of
data but when you go to use it in the
rural World it leaves out a lot and
start and is not very functional outside
of your small area your slow laboratory
data coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model as a bird
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you because it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy k-fold cross
validation having a validation data set
to evaluate the model so we do the
resampling we're randomly going to be
picking out data we'll run it a few
times to see how that works depending on
our random data and how we sample the
data to generate our model and then we
want to go ahead and validate the data
set by having our training data and then
keeping some data on the side testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other
machine learning tools initializing all
weights randomly here the weights are
assigned randomly by initializing them
very close to zero it gives better
accuracy to the model since every neuron
performs different computations and here
we have the weights are set randomly we
have our input layer the hidden layers
and the output layer and W equals in P
random random in layer size L layer size
L minus one this is the most commonly
used is to randomly generate your
weights what are the different layers in
CNN convolutional neural network first
is the convolutional layer that performs
a convolutional operation we have our
other video out if you want to explore
that more so go into detail exactly how
the C the convolutional layer works in
the CNN as far as creating a number of
smaller picture windows to go over the
data the second step is as a Rayleigh
layer relu brings non-linearity to the
network and converts all the negative
pixels to zero output is rectified
feature map so it goes into a mapping
feature there pooling layer pooling is a
down sampling operation that reduces the
dimensionality of the feature map so we
have all our relu layer which is pulling
all these little Maps out of our
convolutional layer it's taking that
picture and a little creating a little
tiny neural networks to look at
different parts of the picture uh then
we need to pull it together and then
finally the fully connected layer so
we've flattened our pulling layer out
and we have a fully connected layer
recognizes and classifies the objects in
the image and that's actually your
forward propagation reverse propagation
training model usually I mean there's a
number of different models out there of
course what is pooling in CNN and how
does it work pooling used to reduce the
spatial dimensions of a CNN performs
down sampling operation to reduce the
dimensionality creates a pooled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide it's important to
know that you have if you see here they
have a rectified feature map and so each
one of those colors like the yellow
color that might be one of the a smaller
little neural network using the relu
you'll look at it'll just kind of go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces and then you have another one
that's also looking at features and it
has a 2785 if each one of those is a map
so it might be the first one might be a
map looking for cat ears and the second
one looking for human eyes when it does
this you then have this rectified
feature map looking at these different
features and the max pooling with the
two by two filters and a strata tube
stride means instead of skipping every
pixel you're going to go every two
pixels you take the maximum values and
you can see over here we look at a pool
feature map one of the features says hey
I had a max value of eight so somewhere
in here we saw a human eye labeled as
eight pretty high label and maybe seven
was a human hand and maybe four was cat
whiskers or something that we thought
might be cat whiskers four is kind of a
low number in this particular case
compared to the other ones so you have
your full pool feature map you can see
the process here is we have our stepping
we look for the max value and then we
create a pulled feature map of the maxed
values how does a lstm network work
that's long short term memory so the
first thing to know is that an lstms are
a special kind of recurrent neural
network okay capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior we did look at
the RNN briefly talked about how the
hidden layer feeds back into itself with
the lstm as a much more complicated
feedback and you can see here we have
the hidden layer of T minus one and the
hidden Larry that's what the H stands
for hidden layer of T and the formula is
going in as we can see here we have the
hidden layers we have T minus 1 and then
h of T where T stands for time so this
is a series remember working with series
and we want to remember the past and you
can see you have your X your input of T
and that might be a frame in a video as
the frame comes in they usually use in
this one the tangent H activation
formula but you also see that it goes
through a couple other formulas the
Omega formula and so when it combines
these that thing goes into the next
layer your next hidden layer that then
goes into the data that's submitted to
the next input so you have your X of t
plus one so when you have that coming in
then you have your H value that's coming
forward from the last process and
depending on how many of these Omega
structures you put in there depends on
how long term the memory gets so it's
important to remember this is more for
your long-term recurrent neural networks
the three steps in an lstm step one
decides what to forget and what to
remember step two selectively update
cell State values So based on what we
want to remember and forget we want to
update those cell values and then decide
what part of the current state make it
to the output so now we have to also
have an output on there what are
Vanishing and exploding gradients this
is a great question to fix all our
neural networks while training an RNN
your slope can become either too small
or too large and this makes the training
difficult when the slope is too small
the problem is known as Vanishing
gradient so our slope we have our change
in X and our change in y when the slope
decreases gradually to a very small
value sometimes negative and makes
training difficult when the slope tends
to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you see a nice graph of
that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between Epic bat and iteration and deep
learning epic an epic represents one
iteration over the entire data set so
that's everything you're going to go
ahead and put into that training model
batch we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have 10
000 images as data and a batch size of
200 then the Epic should run 10 000
times over 200 so that means we have our
total number over the 200 equals 50
iteration so in each epic we're running
over all the data set we're going to
have 50 iterations and each of those
iterations includes a batch of 200
images in this case why tensorflow is
the most preferred library in deep
learning what first tensorflow provides
both C plus plus and python apis that
makes it easier to work on has a faster
compilation time than other deep
learning libraries like Karas and torch
tensorflow supports both CPUs and gpus
Computing devices so right now
tensorflow is at the top of the market
because it's so easy to use for both
programmer side and for Hardware side
and for the speed of getting something
up and running what do you mean by
tensor and tensorflow tensor is a
mathematical object represented as a
raise of higher Dimension these arrays
of data with different dimensions and
ranks that are fed as input to the
neural network are called tensors you
can see here we have a tensor of
Dimensions five comma four so it's a
two-dimensional tensor coming in you can
look at an image like this that each one
of those pixels is a different value if
it's a black and white so it might be
zero and ones and then each one
represents a black and white image in a
color photo you might either find a
different value system or you might have
a tensor value that has the X Y
coordinates as we see here Plus colors
so you might have three more different
dimensions for the three different
images the red the blue and the yellow
coming in and even as you go from one
layer or one tensor to the next these
layers might change we might flatten
them might bring in numerous in the case
of the convergence neural network we
have all those smaller different
mappings of features that come in so
each one of those layers coming through
is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensorflow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf.constant command
example a equals tf.constant 2.0 TF
float32 so it's a tensorflow value of
32. b equals TF constant 3.0 print a b
if we did a print of a b we'd have
tf.constant and then of course B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use
tf.variable command and initial lies
them before running the graph in session
example W equals TF variable 0.3 D type
TF float32 or b equals the TF variable
minus three comma D type float 32
placeholders placeholders allow us to
feed data to a tensorflow model from
outside a model it permits a value to be
assigned later to define a placeholder
we use TF placeholder command example a
equals TF placeholder b equals a times
two with the TF session as sess result
equals session run B comma feed
dictionary equals a 3.0 print result so
we have a nice example of the replace
holder session a session is run to
evaluate the nodes this is called as the
tensorflow runtime so for example you
have a equals TF constant 2.0 b equals
TF constant 4.0 C equals a plus b and at
this point you'd go ahead and create a
session equals TF session and then you
could evaluate the tensor C print
session run C that would input c as an
input into your session what do you
understand by a computational wrath
everything in tensorflow is based on
creating a computational graph it has a
network of nodes where each node
performs an operation nodes represent
mathematical operation and edges
represent tensors since data flows in a
form of a graph it is also called a data
flow graph and we have a nice visual of
this graph or graphic image of a
computational graph and you can see here
we have our input nodes our add multiply
nodes and our multiply node at the end
and then we have the edges where the
data flows so we have from a going to C
A going to D you can see we have a two
flowing four flowing explain generative
adversarial Network along with an
example suppose there's a wine shop that
purchases wine from dealers which they
will resell later dealer point of the
wine our shop owner that then sells it
for a profit but there are some
malfactor dealers who sell fake wine in
this case the shop owner should be able
to distinguish between fake and
authentic wine the forger will try to
different techniques to sell fake wine
and make sure are certain techniques go
past the shop owner's check so here's
our forager fake wine shop owner the
shop owner would probably get some
feedback from the wine experts that some
of the wine is not original the owner
would have to improve how he determines
whether a wine is fake or authentic goal
of forger to create wines that are
indistinguishable from the authentic
ones goal of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we'll refer it
to as a noise Vector coming in we have
our forger who's going to generate fake
wine and then we have our real authentic
wine and of course our shop owner who
has to figure out whether it's real or
fake the generator is a CNN that keeps
producing images that are closer in
appearance to the real images while the
discriminator tries to determine the
difference between real and fake images
the ultimate aim is to make the
discriminator to learn to identify real
and fake images what is an auto encoder
the network is trained to reconstruct
its inputs it is a neural network that
has three layers here here the input
neurons are equal to the output neuron
the Network's Target outside is same as
the input it uses dimensionality
reduction to restructure the input input
image comes in we have our Latin space
representation and then it goes back out
reconstructing the image it works by
compressing the input to a Latin space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are Ensemble techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we have our training
data and our test data very standard
thing to do then we're going to randomly
select data into the bags and train your
model separately so we might have bag
one model one bag two model two bag
three model three and so on in boosting
the emphasis is to select the data
points which give wrong output in order
to improve the accuracy so in boosting
we have our data set again we split it
to test data and train data and we'll
take egg one and we'll train the model
data points with wrong predictions then
go into Bag two and we then train that
model and repeat so if you are looking
to try your hands on this wheel then do
check simply learn Sports graduate
program in Ai and machine learning so
hurry up and roll now find the course
Link in the description box so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine learning but
what is important is you would probably
be better of emphasizing that there are
actually two main types of initial
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you are trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have that
okay so that is what is supervised
learning when you have existing labeled
data which you then use to train your
model that is known as supervised
learning and unsupervised learning is
when you don't have this labeled data so
you have data it is not labeled so the
system has to figure out a way to do
some analysis on this okay so that is
unsupervised learning and you can then
add a few things like what are the ways
of performing a supervised learning and
unsupervised learning and what are some
of the techniques so supervised learning
we we perform or we do
regression and classification and
unsupervised learning video clustering
and clustering can be of different types
similarly regression can be of different
types but you don't have to probably
elaborate so much if they are asking for
just the different types you can just
mention these and just at a very high
level but if they want you to elaborate
give examples then of course then I
think there is a different question
then the third so we have supervised
then we have unsupervised and then
reinforcement you need to provide a
little bit of information around as well
because it is sometimes a little
difficult to come up with a good
definition for reinforcement
so you may have to little bit elaborate
on how reinforcement learning
right so reinforcement learning works in
in such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and every time it is moving in
the direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal then it is rewarded and every
time it is going in a direction where it
is away from the goal then it is
punished so that is the way you can
little bit explain and this is used
primarily or very very impactful or
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphaco used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting Maybe
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes
you can explain this now let's say
you're teaching a child about
recognizing some fruits or something
like that and you're teaching this child
about recognizing let's say three fruits
apples oranges and pineapples okay so
this is a small child and for the first
time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples and you take this basket to
this child and there may be let's say
hundreds of these fruits so you take
this basket to this child and keep
showing each of this fruit and then
first time obviously the child will not
know what it is so you show an apple and
you say hey this is Apple then you show
maybe an orange and say this is orange
and so on and so and then again you keep
repeating that right so till that basket
is over this is basically how training
work in machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then the process
what has happened the child has pretty
much memorized these so even before you
finish that basket right by the time you
are halfway through the child has
learned about recognizing the Apple
orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100 accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple Apple so that means
it has kind of memorized this data now
let's say you bring another basket of
fruits and it will have a mix of maybe
apples which were already there in the
previous set but it will also have in
addition to Apple it will probably have
a banana or maybe another fruit like a
jackfruit right so this is an equivalent
of your test data set which the child
has not seen before some parts of it it
probably has seen like the apples it has
seen but this banana and Jackfruit it
has not seen so then what will happen in
the first round which is an equivalent
of your training data set towards the
end it has 100 it was telling you what
the fruits are right Apple was
accurately recognized orange or I was
accurately recognized and pineapples
were accurately recognized right so that
is like 100 accuracy but now when you
get another a fresh set which were not a
part of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the Jackfruit or the banana
will not be recognized by the child
right so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach 100
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much hundred percent whereas when
you use testing there will be a huge
error which means the loss will be
pretty high and therefore the accuracy
will be also low okay this is known as
overfitting this is basically a process
where training is done training
processes it goes very well almost
reaching 100 accuracy but while testing
it really drops down now how can you
avoid it so that is the extension of
this question there are multiple ways of
avoiding overfitting there are
techniques like what you call
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
drop out in case of neural networks and
a few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stop you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set machine learning model and how
is the split done the question can be
can be so in
learning when we are trying to train the
model so we have a three-step process we
train the model and then we test the
model and then once we are satisfied
with the test only then we deploy the
model so what happens in the train and
test is that you remember the labeled
data so let's say you have thousand
records with labeling information now
one way of doing it is you use all the
Thousand records for training and then
maybe right which means that you have
exposed all this thousand records during
the training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results right but
there is a flaw there what is the flaw
this is very similar to human beings it
is like you are showing this model the
entire data as a part of training okay
so obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may ask this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times still the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that your that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some
yes that is where the real test is now
you have trained the model and now you
are testing the model with some data
which is kind of new that is like a
situation like a realistic situation
because when the model is deployed that
is what will happen it will receive some
new data not the data that it has
already seen right so this is a
realistic test so you put some new data
so this data which you have set aside is
for the model it is new and if it is
able to accurately predict the values
that means your training has worked okay
the model got trained properly but let's
say while you are testing this with this
test data you're getting lot of errors
that means you need to probably either
change your model or retrain with more
data and things like that
coming back to the question of how do
you split this what should be the ratio
there is no fixed number again this is
like individual preferences some people
split it into 50 50 test and 50 training
Some people prefer to have a larger
amount for training and a smaller amount
for test so they can go by either 60 40
or 70 30 or some people even go with
some odd numbers like 65 35 or
63.33 and 33 which is like one third and
two-thirds so there is no fixed rule
that it has to be something that
has to be this you can go by your
individual preference all right then you
may have questions around uh data
handling data manipulation or
data management or Preparation so these
are all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there are a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were this many
rows where the data was missing that
would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling this data or corrupt data
now let's take a few examples now let's
say you have data where some values in
some of the columns are missing and you
have pretty much half of your data
having these missing values in terms of
number of rows okay that could be one
situation another situation could be
that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like 10 percent okay now
between these how do you so let's assume
that this is not a mission critical
situation and in order to fix this 10
percent of the data the effort that is
required is much higher and obviously
effort means also time and money right
so it is not so Mission critical and it
is okay to let's say get rid of these
records so obviously one of the easiest
ways of handling the data part or
missing data is remove those records or
remove those observations from your
analysis so that is the easiest way to
do but then the downside is as I said in
as in the first case if let's say 50
percent of your data is like that
because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10 percent
of the records column one is missing and
another 10 percent column two is missing
another 10 percent column three is
missing and so on and so forth so it
adds up to maybe half of your data set
so you cannot completely remove half of
your data set then the whole purpose is
lost
how do you handle then you
then you know with ways of filling up
this data with some meaningful value
right that is one way of handling so
when we say meaningful value what is
that meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for a task right there may be situations
where if the missing values instead of
putting mean may be a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this
okay so then the next question can be
how can you choose a classifier based on
a training set data size again this is
one of those questions where you
probably do not have like a one size
fits-all answer first of all you you may
not let's say decide your classifier
based on the training set size maybe not
the best way to decide the type of the
classifier and even if you have to there
are probably some thumb rules which we
can use but then again every time so in
my opinion the best way to respond to
this question is you need to try out few
classifiers irrespective of the size of
the data and you need to then decide on
your particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just buy if somebody defines a
problem to you and somebody even if they
show the data to you or tell you what is
the data or even the size of the data I
don't think there is a way to really say
that yes this is the classifier that
will work here no that's not the right
way so you need to still you know test
it out get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have a question around confusion Matrix
so the question can be explained
confusion Matrix right so confusion
Matrix I think the best way to explain
it is by taking an example and drawing
like a small diagram otherwise it can
really become tricky so my suggestion is
to take a piece of pen and paper and
explain it by drawing a small Matrix and
confusion Matrix is about to find out
this is used especially in
classification learning process and when
you get the results when the our model
predicts the results you compare it with
the actual value and try to find out
what is the accuracy okay so in this
case let's say this is an example of a
confusion Matrix and it is a binary
Matrix so you have the actual values
which is the labeled data right and
which is so you have how many yes and
how many know so you have that
information and you have the predicted
values how many yes and how many you
know right so the total actual values
the total yes is 12 plus 130 and they
are shown here and the actual value
knows are 9 plus 3 12 okay so that is
what this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nodes okay so this is
the way to look at this confusion Matrix
okay and uh out of this what is the
meaning convey so there are two or three
things that needs to be explained out
right the first thing is for a model to
be accurate the values across the
diagonal should be high like in this
case right that is one number two the
total sum of these values is equal to
the total observations in the test data
set so in this case for example you have
12 plus 3 15 plus 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix the numbers is equal to
the size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 12 21 and divide
it by the total so in this case what
will it be let me take a pen so your
your diagonal values is equal to if I
say t is equal to 12 plus 9 so that is
21 right and the total data set is equal
to right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85 percent right
so this is 85 percent so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you you don't have
to calculate those numbers on the fly
right so a couple of hints are that you
take some numbers which are with which
add up to 100 that is always a good idea
so you don't have to really do this
complex calculations so the total value
will be 100 and then diagonal values you
divide once you find the diagonal values
that is equal to your percentage okay
all right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and then
otherwise it will be pretty difficult to
so if we use the same example of the
confusion Matrix
and we can explain that so A confusion
Matrix looks somewhat like this and when
we just
look somewhere like this and we continue
with the previous example where this is
the actual value this is the predicted
value and in the actual value we have 12
plus 1 13 yeses and 3 plus 9 12 nodes
and the predicted values there are 12
plus 3 15 yeses and 1 plus 9 10 loss
okay now this particular case which is
the false positive what is a false
positive first of all the second word
which is positive okay is referring to
the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay you consider this row so this
is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay
and then the first term is for the
actual so true positive so true in case
of actual is yes right so true positive
is this one okay and then in case of
actual the negative now we are talking
about let's say true negative true
negative negative is this one and the
true comes from here so this is true
negative right 9 is true negative the
actual value is also negative and the
predicted value is also negative okay so
that is the way you need to explain this
the terms false positive false negative
and true positive true negative then uh
you might have a question like what are
the steps involved in the machine
learning process or what are the three
steps in the process of developing a
machine learning model right so it is
around the methodology that is applied
so basically the way you can probably
answer in your own words but the way the
model development of the machine
learning model
so first of all you try to understand
the problem and try to figure out
whether it is a classification problem
or a regression problem based on that
you select a few algorithms and then you
start the process of training these
models
so you can either do that or you can
after due diligence you can probably
decide that there is one particular
algorithm which is more suitable usually
it happens through trial and error
process but at some point you will
decide that okay this is the model we
are going to use
so in that case we have the model
algorithm and the model decided and then
you need to do the process of training
the model and testing the model and this
is where if it is supervised learning
you split your data the label data into
training data set and test data set and
you use the training data set to train
your model and then you use the test
data set to check the accuracy whether
it is working fine or not so you test
the model before you actually put it
into production right so once you test
the model you're satisfied it's working
fine then you go to the next level which
is putting it for production and then in
production obviously new data will come
and
inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values either it is
regression or
you know so this can be an iterative
process so it is not a straightforward
process where you do the training
through the testing and then you move it
to production now so during the training
and test process there may be a
situation where because of either
overfitting or not things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be a an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so let net this is a continuous
process of tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so it can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then obviously the
question would be then what is the
difference right so deep learning you
need to mention there are two key parts
that interviewer will be looking for
when you are defining deep learning so
first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
scope and deep learning is one one part
of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning uh the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and this data has several let's say if
it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have census data and we
have all this so those are the features
now that may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many teachers and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manual
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
from probably will add value to your
response all right so the next question
is what is the difference between or
what are the differences between machine
learning and deep learning so here this
is a quick comparison table between
machine learning and deep learning and
in machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on low
end system so you don't need a large
machine and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural networks
so here in deep learning we use neural
line so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines because it needs a lot
of computing power and the Machine
learning features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of then you
might get a question around the uses of
machine learning or some real life
applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
or actually supervised machine learning
it could be a very specific question
around supervised decision learning so
this is like give examples of supervised
machine learning use of supervised
machine learning in modern business so
that could be the next so there are
quite a few examples or quite a few use
cases if you will for supervised
definition learning the very common one
is email spam detection so you want to
train your application or your system to
detect between spam and non-spam so this
is a very common business application of
a supervised machine learning so how
does this work the way it works is that
you obviously have historical data above
of your emails and they are categorized
as spam and not spam so that is what is
the labeled information and then you
feed this information or the all these
emails as an input to your model right
and the model will then get trained to
detect which of the emails are to detect
which is Spam and which is not so that
is the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
a healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it will be MRI or
any of these images are available and
they are saying that okay this x-ray
image is deflective of the person has an
illness or it could be cancer whichever
illness right so it is stacked as
defective or clear or good image and
effective image something like that so
we come up with a binary or it could be
multi-class as well saying that this is
deflective to 10 percent this is 25
percent and so on but let's keep it
simple you can give an example of just a
binary classification that would be good
enough so you can say that in healthcare
Diagnostics using which we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
uh unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the names suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
art of supervised learning
this has come into existence is that in
supervised learning you need labeled
data so all your data for training your
model has to be labeled now this is a
big problem in many Industries or in
many under many situations getting the
label data is not that easy because
there's a lot of effort in labeling this
data let's take an example of diagnostic
images we can just
extra images now there are actually
millions of x-ray images available all
over the world but the problem is they
are not labeled so their images are
there but whether it is effective or
whether it is good and information is
not available along with it right in a
form that it can be used by a machine
which means that somebody has to take a
look at these images and usually it
should be like a doctor and then say
that okay yes this image is clean and
this image is cancerous and so on and so
forth now that is a huge effort by
itself so this is where semi-supervised
learning comes into play so what happens
is there is a large amount of data maybe
a part of it is labeled then we try some
techniques to label the remaining part
of the data so that we get completely
labeled data and then we train our model
so I know this is a little long-winding
explanation but unfortunately there is
no quick and easy definition for
semi-supervised machine learning this is
the only way probably to explain this
concept
we may have another question as what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in a
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together
then Association you can I think the
best way to explain Association is with
an example in case of Association you
try to find out how the items are linked
to each
example if somebody bought maybe a
laptop or the person has also purchased
a mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying and laptops are also buying the
mouse so that means there is an
association between laptops and
people who are buying red are also
buying so that is the association that
can be created so this is unsupervised
learning one of the techniques okay all
right then we have very fundamental
question what is the difference between
supervised and unsupervised machine
learning so machine learning these are
the two main types of machine learning
supervised and unsized and in case of
supervised and again here probably the
key word that the person may be wanting
to hear is labeled data now very often
people say we have historical data and
if we run it it is supervised and if we
don't have historical data yes but you
may have historical data but if it is
not labeled then you cannot use it for
so it is it's very key to understand
that we put in that keyword label okay
so when we have labeled data for
training our model then we can use
supervised learning when if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these
so there can be another question a
little bit more theoretical and
conceptual in nature this is about
inductive machine learning and
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
ready they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it what can be an
example so we can probably tell the
person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw a
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or you throw
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
probably that explanation
sufficient the next question is are knnn
and K means clustering similar to one
another or are they same right because
the letter K is kind of common between
them okay so let us take a little while
to understand what these two are one is
KNN another is K means a KNN stands for
K nearest neighbors and K means of
course is the clustering mechanism now
these two are completely different
except for the letter K being common
between them KN is completely different
k-means clustering is complete
KNN is a classification process and
therefore it comes under supervised
learning whereas k-means clustering is
actually a unsupervised okay when you
have K and N when you want to implement
k n n which is basically K nearest
neighbors the value of K is a number so
you can say k is equal to 3 you want to
implement k n n with K is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular
but it is possible that they may be from
two or three different classes okay so
let's say they are from two classes and
then they are from two classes now
usually you take a odd number you assign
odd number two so if there are three of
them and two of them belong to one class
and then one belongs to another class so
this new object is assigned to the class
to which the two of them belong now the
value of K is sometimes tricky whether
should you use three should you use five
should you use seven it can be tricky
because the ultimate classification can
also vary so it's possible that if
you're taking K as3 the object is
probably in one particular class but if
you take K is equal to 5 maybe the
object will belong to a different class
because when you're taking three of them
probably two of them belong to a class
one and one belong to class two whereas
when you take five of them it is
possible that only two of them belong to
class one and the three of them belong
to Class 2 so which means that this
object will belong to class 2 right so
you see that so it is the class
allocation can vary depending on the
value of K now K means on the other hand
is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to some of features okay
and but the similarity of course is the
the letter K and in case of K means also
we specify its value and it could be
three or five or seven there is no
technical limit as such but it can be
any number of clusters that you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects there
is a similarity to that extent that K is
a number in both the cases but actually
these two are completely different
processes
we have what is known as naive based
classifier and people often get confused
thinking that naive base is the name of
the person who found this classifier or
who developed this classifier which is
not 100 True base is the name of the
person bais is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of this particular
classifier an ibase classifier is a
probability based classifier and it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but even if we go with this assumption
this whole algorithm works very well
even with this assumption and that is
the good side of it but the term comes
from that so that is the explanation
that you can then there can be question
around reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that on that to explain the
process so first of all reinforcement
learning has an environment and an agent
and the agent is basically performing
some actions in order to achieve a
certain goal and this goals can be
anything either it is related to game
then the goal could be that you have to
score very high score value High number
or it could be that your number of lives
should be as high as possible don't lose
life so this could be some of them a
more advanced examples could be for
driving the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of
it is rewarded and every time it takes a
step which goes against that core right
contrary or in the reverse Direction it
is penalized okay so it is like a
keratin as
now how do you use this to create a game
of chess so to create a system to play a
game objects now the way this works is
and this could probably go back to this
alphago example where alphaco defeated a
human Champion so the way it works is in
reinforcement learning the system is
allowed for example in this case we are
talking about Chess so we allow the
system to first of all watch playing a
game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being play the game at because this is
reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent it works on
that so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you are playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to play chess and and learn
on its own over a period of repetition
so I think you can probably explain it
to this much to this extent and I
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SPM you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is
okay so we have to try out a bunch of
algorithms see which one gives us the
best performance and best accuracy
accuracy and then decide to go with that
particular algorithm so in machine
learning a lot of it happens through
trial and error there is no real
possibility that anybody can just by
looking at the problem or understanding
the problem tell you that okay in this
particular situation this is exactly the
algorithm that you should use then the
questions may be around application of
machine learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously something like uh you know
Amazon a website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of a
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
agenda and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine work all right
then the question can be something very
basic like when will you go for
classification versus regression right
when do you do classification instead of
instead of regression or when will you
use classification instead of regression
now yes so so this is basically going
back to the understanding of the basics
of classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values and other is the
continuous values so for regression if
we are trying to find some continuous
values you use regression whereas if you
are trying to find some discrete values
you use classification and then you need
to illustrate what are some of the
examples so classification is like let's
say there are images and you need to put
them into classes like cat dog elephant
title something like that so that is the
classification problem or it can be that
is a multi-class classification problem
it could be binary classification
problem like for example whether a
customer will buy or he will not buy
that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not it's a classification
problem that's a binary classification
right whether it's going to rain or not
rain however you also have to predict
what is going to be the temperature
tomorrow right now temperature is a
continuous value you can't answer the
temperature in a yes or no kind of a
response right so what will be the
temperature tomorrow so you need to give
a number which can be like 20 degrees 30
degrees or whatever right so that is
where you use regression one more
example is stock price prediction so
that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continue so
the next question is more about a little
bit of a design related question to
understand your Concepts and things like
that so it is how will you design a spam
filter so how do you basically design or
Developers
so I think the main thing here is he is
looking at probably understanding your
Concept in terms of what is the
algorithm you will use or what is your
understanding about difference between
classification and regret
and things like that right and the
process of course the methodology and
the process so the best way to go about
responding to this is we say that okay
this is a classification problem because
we want to find out whether an email is
a spam or not spam so that we can apply
the filter accordingly so first thing is
to identify what type of a problem it is
so we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector machines for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you use your training data set
to train your model that or your
algorithm that you have used or rather
the model actually
so and you actually will have three
models let's say you are trying to test
out three algorithms so you will
obviously have three models so you need
to try all three models and test them
out as well see which one gives the best
accuracy and then you decide that you
will go with that model okay so training
and test will be done and then you zero
in on one particular model and then you
say okay this is the model will you use
we will use and then go ahead and
Implement that or put that in production
so that is the way you design a Spanish
the next question is about random Forest
what is random form so this is a very
straightforward question however the
response you need to be again a little
careful while we all know what is random
Forest explaining this can sometimes be
tricky so one thing is random Forest is
kind of in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and trees will basically
for doing if it is classification mostly
it is classification you will use the
the trees for classification and then
you use voting for finding that the
final class so that is the underlyings
but how will you explain this how will
you respond to this so first thing
obviously we will say that random Forest
is one of the algorithms and the more
important thing that you need to
probably the interviewer is is waiting
to here is Ensemble learner right so
this is one type of Ensemble learner
what is Ensemble learner Ensemble
learner is like a combination of
algorithms so it is a learner which
consists of more than one algorithm or
more than one or maybe models okay so in
case of random Forest the algorithm is
the same but instead of using one
instance of it we use multiple instances
of it and we use so in a way that is a
random Forest is an ensemble learner
there are other types of Ensemble
Learners where we have like reuse
different algorithms itself so you have
one maybe logistic regression and
combined together and so on and so forth
or there are other ways like for example
splitting the data in a certain way and
so on so that's all about Ensemble we
will not go into that but random Forest
itself I think the interviewer will be
happy to hear this word Ensemble
Learners and so then you go and explain
how the random Forest works so if the
random Forest is used for classification
then we use what is known as a voting
mechanism so basically how does it work
let's say your random Forest consists of
100 trees okay and each observation you
pass through this forest and each
observation let's say it is a
classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and ten of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way a random Forest works
for classification it is a regression
problem it's somewhat similar but only
thing is instead of what what we will do
is sorry in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
hundred trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the 100 trees right so
this is also a way of reducing the error
so maybe if you have only one tree and
if that one tree makes a header it is
basically hundred percent wrong or 100
right right but if you have on the other
hand if you have a bunch of trees you
are basically medicating that reducing
so that is the way random Forest works
so the next question is considering the
long list of machine learning algorithms
how will you decide on which one do you
want to use so once again here there is
no way to outright say that this is the
algorithm that we will use for a given
data set this is a very good question
but then the response has to be like
again there will not be a one-size-fits
all so we need to first of all you can
probably shorten the list in terms of by
saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithm
right so for example if it's a class
station you cannot use linear regression
algorithm or if it is a regression
problem you cannot use svm or maybe now
you can use svm but maybe a logistic
regression right so to that extent you
can probably shorten the list but still
you will not be able to 100 decide on
saying that this is the exact algorithm
that I am going to use so the way to go
about is you choose a few algorithms
based on what the problem is you try out
your data you train some models of these
algorithms check which one gives you the
lowest error or the highest accuracy and
based on that you choose that particular
algorithm
all right then they can be questions
around bias and variants so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual value so that
is the bias okay and whereas they are
all all the values are probably they are
far off but they are very near to each
other though the predicted values are
close to each other right while they are
far off from the actual value but they
are close to each other you see the
difference so that is bias and then the
other part is your variance now variance
is when the predicted values are all
over the place right so the variance is
high that means it may be close to the
Target but it is kind of very scattered
so the point the predicted values are
not close to each other right in case of
bias the predicted values are close to
each other but they are not close to the
Target but here they may be close to the
Target but they may not be close to each
other so they are a little bit more
scattered so that is what in case of a
variance okay then the next question is
about again related to bias and variance
what is the trade-off between bias and
variance yes I think this is a
interesting question because these two
are heading in different directions so
for example if you try to minimize the
bias variance will keep going high and
if you try to minimize the variance bias
will keep going high and there is no way
you can minimize both of them so you
need to have a trade-off saying that
okay this is the level at which I I will
have my bias and this is the level at
which I will have variance so the
trade-off is that pretty much attack you
you decide what is the level you will
all rate for your bias and what is the
level you will tolerate for variance and
a combination of these two in such a way
that your final results are not way off
and having a trade-off will ensure that
the results are consistent right so that
is basically the output is consistent
and which means that they are close to
each other and they are also accurate
which that means they are as close to
the Target as possible right so if
either of these is high then one of them
will go off the track define precision
and Recall now again here I think it
would be best to draw a diagram and take
up in the confusion Matrix and it is
very simple the definition is like a
formula your Precision is true positive
by true positive plus false positive and
your recall is true positive by true
positive plus false negative okay so
that's you can just show it in a
mathematical way that's pretty much you
know
that's the easiest way to define so the
next question can be about decision tree
what is decision tree pruning and why is
it so basically decision trees are
really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and conditions can become very
complicated and this can also lead to
overfitting which is basically that
during training you will get 100
accuracy but when you're doing testing
you'll get a lot of Errors so that is
the reason pruning needs to be done so
the purpose or the reason for doing
decision tree pruning is to reduce
overfitting or to cut down on
overfitting and what is decision trip
rolling it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing those internal
nodes all right so the next question can
be what is logistic regression uh so
basically logistic regression is one of
the techniques used for performing
classification specially binary
classification now there is something
special about logistic regression and
there are a couple of things you need to
be careful about first of all the name
is a little confusing it is called
logistic regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if it's
really in
Vite and they can also ask this like a
trick question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from log so that the mathematical
derivation was log and therefore the
name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between 0
and 1 and we can set a threshold that
can vary typically it is 0.5 so any
value above 0.5 is considered as 1 and
if the probability is below 0.5 it is
considered as zero so that is the way we
calculate the probability of the system
calculates the probability and based on
the threshold it sets a value of 0 or 1
which is like a binary classification
zero or one okay then we have a question
around K nearest neighbor algorithm so
explain K nearest neighbor algorithm so
first of all what is the K nearest
neighbor algorithm this is a
classification algorithm so that is the
first thing we need to mention and we
also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
want to find out which class this
belongs right so you go about as the
name suggests it you go about finding
the nearest neighbors right the points
which are closest to this and how many
of them you will find that is what is
defined by K now let's say our initial
value of K was Phi okay so you will find
the K the five nearest data points so in
this case as it is Illustrated these are
the five nearest data points but then
all five do not belong to the same class
or cluster so there are one belonging to
this cluster one the second one
belonging to this cluster two three of
them belonging to this third cluster
okay so how do you decide that's exactly
the reason we should as much as possible
try to assign an odd number so that it
becomes easier to assign this so in this
case you see that the majority actually
if there are multiple classes then you
go with the majority so since three of
these items belong to this class we
assign which is basically the in in this
case the green or the tennis or the
third cluster as I was talking
right so we assign it to this third
class so in this case it is uh that's
how it is decided okay so K nearest
neighbor so first thing is to identify
the number of neighbors that are
mentioned as K so in this case it is K
is equal to 5 so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that and and then the new data
point is assigned to that class okay so
that's pretty much how K nearest
neighbors work with this we wrap up AIML
and deep learning interview questions if
you have any questions do let us know in
the comment sections and do not forget
to subscribe to Simply learn thanks for
watching this video
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here