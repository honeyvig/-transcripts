hi everyone and welcome to this
fantastic machine learning python full
course but before we begin if you enjoy
watching these kind of videos and find
them interesting then subscribe to our
YouTube channel because we bring the
best videos for you daily also hit the
Bell icon to never miss any updates from
Simply loone so without any further
delay let's go through the agenda for
today's video we'll brief with a machine
learning introduction and after that
we'll see the types of machine learning
moving forward we'll cover the machine
learning road map skills required for
machine learning engineers and the top
applications and projects for machine
learning we'll walk you through some
fantastic Concepts like AI versus
machine learning versus deep learning
proceeding with the course we'll teach
you different machine learning
algorithms with Hands-On demo
explanations in Python we'll walk you
through some unique projects like fake
news Det protection and time series
these projects will serve you as the
finest portfolios for your future
interviews speaking of interviews we
have covered you along with the most
frequently Asked machine learning
interview questions to help you crack
the most challenging interviews but
before we begin if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no simply learns postgraduate
program in Ai and machine learning from
Peru University in collaboration with
the ibbm should be your right choice for
admission to this postgraduate program
ni and machine learning candidates with
two plus years of work experience are
preferred and a candidate should have a
bachelor's degree with an average of 50%
or higher marks and basic understanding
of programming Concepts and Mathematics
for more details check out the link
mentioned in the description box below
with that in mind let's hand over this
session to a training export here we
have our um looks a little bit like
Frankenstein our Frankenstein looking
robot today let me tell you what is
machine learning machine learning works
on the development of computer programs
that can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in two days this was
back in July 29th
2016 so that's pretty impressive this
amount of time to continue to grow in
his development and it's smart enough to
leave spaces in the brick work for
wiring and pling and can even cut and
shape bricks to size Amazon Echo relies
on machine learning and with more data
it becomes more accurate play your
favorite music order pizza from dominoes
voice control your home request rides
from Uber have you ever wondered the
difference between AI machine learning
and deep learning artificial
intelligence a technique which enables
machines to mimic human behavior this is
really important because this is how we
are able to gauge how well our
computations or what we're working on
works is the fact that we're mimicking
human behavior we're using this to
replace human work and make it more
efficient and make it more streamlined
and more accurate and so the center of
artificial intelligence is the big
picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks Alpha go natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we'd
have a very hard time tracking and it's
able to come up with an answer from that
now let's see how machine learning works
first we start with training the data
once we've trained the data the train we
go into the machine learning algorithm
which then puts the data into a
processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training datea and we find out it didn't
do a good job predicing it and it gets a
thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
you try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can
now let's look into the types of machine
learning machine learning is primarily
of three types first one is supervised
machine learning as the name suggests
you have to supervise your machine
learning while you train it to work on
its own it requires labeled training
data next up is unsupervised learning
wherein there will be training data but
it won't be labeled finally there's
reinforcement learning wherein the
system learns on its own let's talk
about all these types in detail let's
try to understand how supervised
Learning Works look at the pictures very
very carefully the monitor depicts the
model or the system that we are going to
train this is how the training is done
we provide a data set that contains
pictures of a kind of a fruit say an
apple then we provide another data set
which lets the model know that these
pictures wear that of a fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used in F
filtering spam mails from your email
account as well yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby you put a
burning candle in front of the baby the
baby does not know that if it touches
the flame its fingers might get burned
so it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works we provide
the machine with a data set wherein we
ask it to identify a particular kind of
a fruit in this case an Apple so what it
does as a response it tells us that it's
a mango but as we all know it's a
completely wrong answer so as a feedback
we tell the system that it's wrong it's
not a mango it's an apple what it does
it learns from the feedback and keeps
that in mind when the next time when we
ask a same question it gives us the
right answer it is able to tell us that
it's actually an apple that is a
reinforced response so that's how
reinforcement learning works it learns
from his mistakes and experiences this
model is used in games like Prince of
Persia or Assassin's Creed or FIFA where
in the level of difficulty increases as
you get better with the games just to
make it more clear for you let's look at
a comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this is actually an
apple that is called label data so the
system learns from the label data and
makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervisor and unsupervised learning
artificial intelligence and machine
learning have permeated every aspect of
Our Lives if you are a programmer
chances are you have begun utilizing
github's co-pilot an AI tool that
transforms natural language PRS into
coding suggestions streamlining the
programming process as a writer you may
have encountered open AIS gp3 or similar
Auto regressive language models that
leverage deep learning to generate text
resembling human language manyu of has
dedicated a few hours to experimenting
with d 2 the AIML power text to image
generator capable of producing intricate
visuals based on the most unusual
request not only is d 2 exponentially
more powerful but it also has the
potential to revolutionize the field of
digital art consider its impact on a
digital artist illustrator or graphic
designer career imagine creating a
highly realistic image within seconds
using an app these technologies have
significant real world applications with
far-reaching implications let me tell
you some fascinating facts about machine
learning according to recent studies
machine learning related job postings
have increased bi a staggering
344 in the past 5 years companies across
the globe are actively seeking
professionals who can harness the power
of data and build intelligent systems
the average salary is
$190,000 in us and 26 laks perom in
India accelerate your career in Ai and
ml with our comprehensive postgraduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and hackathons and ask
me anything sessions by IBM with three
Capstone and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Purdue
faculty and IBM experts ensure top-notch
education simply learns job assist helps
you get noticed by Leading companies
live sessions on AI trends like chat GPT
generative Ai and explainable AI this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision Gans
caras tensor flow and many more skills
enroll now and unlock exciting AI ml
opportunities the link is mentioned in
the description box below with that
having said now here is a comprehensive
stepbystep guide to learning machine
learning number one is skills required
number two job rolls in ml number third
companies hiring ml engineers and the
number four is future scope of ml now
starting with the number one that is
fundamentals of machine learning
familiarize yourself with the
fundamentals of machine learning in the
initial stages of learning to drive we
are familiarized with the different
elements varieties and regulations
pertaining to operating a car it is
essential to delve into the fundamentals
of machine learning to understand what
lies ahead and the required knowledge
now moving to number two that is
acquiring Proficiency in python or R
programming language to excel in the
field of artificial intelligence and
machine learning it is crucial to
develop a strong command of python or
our programming language both Python and
R are widely used in the data science
Community due to their versatility and
extensive libraries for scientific
Computing so now moving on to the third
step that is gain knowledge of essential
python libraries for machine learning
once you have acquired Proficiency in
python as part of our machine Learning
Journey the subsequent step involves
familiarizing yourself with essential
python libraries crucial for working
with data and implementing machine
learning Solutions the key python
Library is that you should learn from
machine learning are numpy pandas
matplot Li and psyit learn now moving to
step four that is learn and Implement
various machine learning algorithms
after gaining Proficiency in python as
part of our machine Learning Journey the
subsequent Milestone involves learning
various machine learning algorithms and
their implementation using python listed
below are some of the key machine
learning algorithms that are essential
to learning number one linear regression
number two logistic regression number
three passive aggression number four
Nave Base number five support Vector
machines now moving to step five Master
the concepts and implementation of
neural networks once you have acquired
knowledge of python and machine learning
algorithms the next significant step in
the machine learning road map is to
learn neural network architecture and
their implementations using python
outlined below are several crucial
neural network architectures that are
essential to learning number number one
artificial neural networks number two
CNN number three
RNN number four long short term memory
now moving to step six that is engage
Hands-On projects to apply your
knowledge and reinforce your
understanding after acquiring knowledge
of python machine learning algorithms
and neural network architectures the
next crucial step is the machine
learning road map and is to gain
practical experience by working on
projects that allow you to apply what
you have learned the first project is
Iris flower classification number two
California house price prediction number
three stock price prediction number four
customer segmentation now we'll move to
job roles in machine learning machine
learning as highlighted earlier has
gained immense popularity for its
ability to elevate human efforts and
enhance machine performance through
autonomous learning this popularity has
resulted in lucrative and sought after
career options with the in the field
including roles like machine learning
engineer so the duty of a machine
learning engineer encompasses the
creation construction and deployment of
machine learning models collaborating
closely with data scientist and software
Engineers they participate in the
development and execution of machine
Learning Systems according to glasto ml
Engineers can earn up to
$150,000 in us and 11 lakhs perom in
India now moving to data scientist the
role of a data scientist involves
Gathering scrutinizing and interpreting
extensive data sets leveraging machine
learning algorithms they uncover
patterns and glean insights from the
data utilizing this knowledge to inform
decisions and address challenges
according to glast data scientist earn
$145,000 in us and 13 lakhs perom in
India now moving to NLP Engineers the
specific duties may vary based on the
role and sector but is outlined by
freelancer map an NLP engineer typically
engaged in tasks such as designing
natural language processing systems and
addressing speech patterns and AI speech
recognition according to glassor NLP
engineer can earn
$120,000 in us and 10 lakhs perom in
India now moving to computer vision
engineer Engineers specializing in
computer vision operate within the realm
of computer vision employing machine
learning to empower computers to
comprehend and interpret visual data
from their surroundings the
responsibilities include tasks such as
image and video analysis and object
detection according to glasto CV
Engineers earn
$156,000 in us and 8 lakhs perom in
India now moving to business
intelligence developer the main role of
a bi developer is to develop deploy and
maintain bi tools and interfaces they're
also responsible for simp simplifying
highly technical language and complex
information into layman's terms for
everyone else in the company to
understand according to classro business
intelligence developers earn
$15,000 in us and 7 lakhs perom in India
now we'll see the top companies hiring
for machine learning engineer number one
is Amazon then we have asenta Google
Apple Intel so these are the top hiring
companies that h machine learning
Engineers now we'll talk about future of
machine learning machine learning has a
bright future but faces several
difficulties ml is predicted to grow
increasingly pervasive as technology
develops revolutionizing sectors
including healthc care Banking and
transportation the work Market will
change due to AI driven automation
necess setting new position and skills
Welcome to our video on skills required
for an ml engineer machine learning has
been the talk of the lately every
organization has realized the potential
of machine learning in improving their
business objectives and attaining the
Enterprise goals this expanding demand
has led to a lot of people applying for
machine learning jobs and upskilling
them themselves in the field of machine
learning you can take up this growing
opportunity in the field of machine
learning and utilize it to land yourself
a very challenging fulfilling and high
ping job in this video we will be
breaking down in complete detail each
and every skill that you would need in
order to crack the machine learning
engineer job interview well ml is not
just a passing Trend it's a sismic shift
that is reshaping our world and creating
new avenues for Innovation and Discovery
so by embracing a career in ml you
become part of dynamic field that
thrives on solving complex problem
pushing boundaries and making a profound
impact on society so the demand for ML
professional is skyrocketing across
industries from Healthcare and finance
to entertainment and transportation
organization actively seeking talented
individuals who can harness the power of
AI to drive their business forward but
what skill does it takes to become an ml
engineer how can you embark on this
thrilling Journey we have the answers to
all your questions also accelerate your
career in Ai and ml with our
comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alum I membership
and ask me anything sessions by IBM with
three Capstone projects and 25 plus
industry projects using real data set
from Twitter Uber and more you will gain
practical experience master classes by
keltech faculty and IBM experts ensure
top-notch education simply learns job
assist help you get notice by Leading
companies this program covers Python
supervis and unsupervised learning NLP
neural networks computer Visions g k us
tlow and many more other skills so
enroll now and unlock exciting Ai and ml
opportunities the link is in the
description box below so without any
further delay let's get started to
become a machine learning engineer you
need a combination of technical skills
non-technical skills and some bonus
skills so here are some essential skills
required to pursue a career as an ml
engineer so first we will talk about
some technical skills to become a ml
engineer so first one in the list is
programming languages strong programming
skills are essential you should be
proficient in at least one programming
languages such as python or R python is
widely used in the ml Community due to
its Rich libraries that is numai pandas
tensorflow and pytorch that supports ml
task and the second on the list is
machine learning algorithms and
techniques you should have a solid
understanding of various ml algorithm
such as linear regression logistic
regression decision trees random Forest
neural network and deep learning
familiarize yourself with the principles
Behind These algorithms their pros and
cons and when to use them so third one
on the list is data pre-processing ml
models require clean and well prepared
data you should know how to handle
missing data deal with normalized and
standardized data and the final is
perform feature engineering
understanding data pre-processing
technique is crucial for Effective ml
model training and the fourth one is
data manipulation and Analysis data is
the foundation of ml model you should be
skilled in the data manipulation and
Analysis using libraries like napai and
pandas this includes cleaning and
transforming data exploratory data
analysis which is Eda and understanding
the statical properties of the data and
the fifth one is machine learning
libraries and Frameworks familiarity
with popular ml libraries and framework
is essential some some commonly used one
include numai pandas tensorflow and pyot
these Library provide pre-implemented ml
algorithms neural network architectures
and tools for model training and
evaluation now that we have seen the
Technical Machine learning engineering
skills let us have a look at the
non-technical machine learning skills so
the first one is industry knowledge
machine learning projects that
effectively tackle genuine challenges
are likely to achieve great success
regardless of the industry you are
involved in it is crucial to have a
comprehensive understanding of its
operation and identify ways to optimize
business outcomes and the second one on
the list is effective communication
effective communication plays a crucial
role in facilitating these interactions
companies seeking skilled ml engineer
value candidates who can effectively
Cove technical discoveries to
non-technical terms like marketing or
sales demonstrating Clarity and fluency
in their explanation moving forward
let's see some bonus skills to become ml
engineer the first one on the list is
reinforcement learning in 2023
reinforcement learning emerg as a
catalyst for numerous captivating
advancement in deep learning and
artificial intelligence to pursue a
career in robotics self-driving cars or
any other related feed it is crucial to
comprehend this concept and the second
on the list is computer vision computer
vision and machine learning are
fundamental branches of computer science
that can independently fuel highly
Advanced system relying on CV and ml
algorithm however the combination has a
potential to unlock greater even
possibilities and achievements so
remember that the field of ml is
constantly evolving through continuous
learning and staying updated with the
latest development and the research
papers machine learning has improved our
lives in a number of wonderful ways
today let's talk about some of these I'm
Rahul from Simply learn and these are
the top 10 applications of machine
learning first let's talk about virtual
personal assistants Google Assistant
Alexa Cortana and Siri now we've all
used one of these at least at some point
in our lives now these help improve our
lives in a great number of ways for
example you could tell them to call
someone you could tell them to play some
music you could tell them to even
schedule an appointment so how do these
things actually work first they record
whatever you saying send it over to a
server which is usually in a cloud
decode it with the help of machine
learning and neural networks and then
provide you with an output so if you
ever noticed that these systems don't
work very well without the internet
that's because the server couldn't be
contacted next let's talk about traffic
predictions now say I wanted to travel
from Buckingham Palace to LS cricket
ground the first thing I would probably
do is to get on Google Maps so search
it
and let's put it
here so here we have the path you should
take to get to Lodge cricet ground now
here the map is a combination of red
yellow and blue now the blue regions
signify a clear road that is you won't
encounter traffic there the yellow
indicate that they're slight ly
congested and red means they're heavily
congested so let's look at the map a
different version of the same map and
here as I told you before red means
heavily congested yellow means slow
moving and blue means clear so how
exactly is Google able to tell you that
the traffic is clear slow moving or
heavily congested so this is with the
help of machine learning and with the
help of two important measures first is
the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and and with the help of sensors
some of the other popular map services
are Bing Maps maps.me and here we go
next up we have social media
personalization so say I want to buy a
drone and I'm on Amazon and I want to
buy a DJI mavic Pro the thing is it's
close to one laap so I don't want to buy
it right now but the next time I'm on
Facebook I'll see an advertisement for
the product next time I'm on YouTube
I'll see an advertisement even on
Instagram I'll see an advertisement so
here with the help of machine learning
Google has understood that I'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which have already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity fake
accounts where these accounts only last
for how long the transaction takes place
and stop existing after that and man in
the middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward in un networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and ischemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment trug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other finding a suitable
job in the field of machine learning is
becoming increasingly difficult the
ideal way to display your machine
learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
reccommendation system fake news
detection and many more are the best way
to improve your early programming skills
you may have the knowledge but putting
it to the use what is keep you
competitive here are 10 machine learning
projects that can increase your
portfolio and enable you to acquire a
job as a machine learning engineer at
number 10 we have loan approval
prediction system in this machine
learning project we will analyze and
make prediction about the loan approval
process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting fake news this ml project for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with escalan the model is then fitted
using a passive aggressive classifier
that has been initialized finally the
accuracy score and the confusion Matrix
indicate how well our model performs the
link for the project is in the
description box below at number eight we
have personality prediction system the
idea is based on determining an
individual personality using machine
learning techniques a person personality
influences both his personal and
professional life nowadays many company
are short listing applicant based on
their personality which increases job
efficiency because the person is working
on what he is good at rather than what
is compelled to do in our study we
attempted to combine personality
prediction system using machine learning
techniques such as SVD na Bas and
logistic regation to predict a personal
personality and talent prediction using
phrase frequency method this model or
method allows users to recognize their
personality and Technical abilities
easily to learn about mode this project
check the link in the description box
below at number seven we have Parkinson
disease system Parkinson disease is a
progressive central nervous system
element that affects movement and cause
tremors and stiffness it comprises five
stages and affects more than 1 million
worldwide each other in this machine
learning project we will develop an svm
model using python modules psyched learn
numpy and pandas and svm we will import
the data extract the features and label
and scale the features split the data
set design an an model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available to today are the commercial
the community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre-trained models
were accessible in Python but changing
the voice of volume was often difficult
it also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text MLS speech
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the phenomes and the
phonetics on the speech for this project
we will utilize pyttsx3 pyttsx3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and Python 3 to learn more about this
project check the link in the
description box below at number four we
have sentiment analysis sentiment
analysis also known as opinion mining is
a straightforward process of determining
the author's feeling about a text what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement use
deep learning is an any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and F form of the
biometric verification over time this
project will use open CV and pH
recognition libraries to create a phas
detection system open CV provides a
realtime computer vision tool library
and Hardware we can create amazing
realtime projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens data set
generated by the more than 6,000 users
to learn how to create movie
recommendation system for yourself or
for your loved on check the project in
the description box below human versus
artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every Nick and Nook we've gone
to the Moon uh we've gone into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you
human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
tasks that require human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer where it's
identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give him a
bad Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by here's a few
hours and suddenly you wonder how many
cars buses trucks and so on passed by in
the six hours now chances are you're not
going to sit by the road for six hours
and count buses cars and trucks unless
you're working for the city and you're
trying to do City Planning and you want
to know hey do we need to add a new
truck route maybe we need a Bicycle Link
they have a lot of bicyclists here that
kind of thing so maybe City Planning
would be great for this machine machine
learning well the way machine Learning
Works is we have labeled data with
features okay so you have a truck or a
car a motorcycle a bus or a bicycle and
each one of those are labeled it comes
in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle this look a little bit more
in depth on this in the model here it
actually the features we're looking at
would be like the tires someone sits
there and figures out what a tire looks
like takes a lot of work if you try to
try to figure the difference between a
car tire a bicycle tire a motorcycle
tire uh so in the M machine learning
field this could take a long time if
you're going to do each
individual aspect of a car and try to
get a result on there and that's what
they did do that was a a very this is
still used on smaller amounts of data
where you figure out what those features
are and then you label them deep
learning so with deep learning one of
our Solutions is to take a very large
unlabeled data set and we put that into
a training model using artificial neural
networks and then that goes into the
neural network itself when we create a
neural network and you'll see um the
arrows are actually kind of backward but
uh which actually is a nice point
because when we train the neural network
we put the bicycle in and then it comes
back and says if it said truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns once
you've trained the neural network you
then put the new data in and they call
this testing the model so you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28 by 28 pixels
that's a lot of information there um
could you imagine trying to guess that
this is a bicycle image by looking at
each one of those pixels and trying to
figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in as all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and outc comes and says okay this
is a
bicycle a lot of times they call this
the magic Black Box why because as we
watch it go across here all these
weights and all the math behind this and
it's not it's a little complicated on
the math side you really don't need to
know that when you're programming or
doing working with the Deep learning but
it's like magic you you don't know you
really can't figure out what's going to
come out by looking what's in each one
of those dots and each one of those
lines lines are firing and what's going
in between them so we like to call it
the magic box uh so that's where deep
learning comes in and in the end it
comes up and you have this whole neural
notwork comes up and it says okay we
fire all these different pixels and we
connects all these different dots and
gives them different weights and it says
okay this is a bicycle and that's how we
determine what the object is present in
the data with deep learning machine
learning we're going to take a step into
machine learning here and you'll see how
these fit together in a minute the
system is able to make predictions or
take decisions based on past data that's
very important for machine learning is
that we're looking at stuff and based on
what's been there before we're creating
a decision on there we're creating
something out of there we're coloring a
beach ball we're telling you what the
weather is in Chicago what's nice about
machine learning is a very powerful
processing capability it's quick and
accurate outcomes so you get results
right away once you program the system
the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do problem solved in an end
to end method so instead of having to
break it apart and you have the first
piece coming in and you identify tires
and the second piece is identifying uh
labeling handlebars and then you bring
that together that if it has handlebars
and tires it's a bicycle and if it has
something that looks like a large Square
it's probably a truck the neural
networks does this all in one Network
you don't really know what's going on in
all those weights and all those little
bubbles uh but it does it pretty much in
one package that's why the neural
network systems are so big nowadays and
coming into their own best features are
selected by the system and it this is
important they kind of put it as it's on
a bullet on the side here it's a subset
of machine learning this is important
when we talk about deep learning it is a
form of machine learning there's lots of
other forms of machine learning data
analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test um you put in your
information you then have your group of
uh test and then you held some aside you
see how does it do it's very quick to
test it and see what's going on with
your deep learning and your neural
network are they really all that
different AI versus machine learning
versus deep learning concepts of AI
so we have concepts of II you'll see
natural language processing uh machine
learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books it goes through some deep learning
the Digital Data is analyzed text mining
comes through the particular book you're
looking for maybe it's a genre books is
identified and in this case uh we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens uh so
we look at some of the pieces here this
information goes into uh the as far as
this example the translation of the
handwritten printed data to digital form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
uh what you're looking for and say oh
you're looking for auto repair and
computer so you're looking for automated
cars once it's identified then of course
it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as a big picture
how does this compare to The Human
Experience in either uh doing the same
thing as a human we do or it does it
better than us and machine learning
which has a lot of tools uh is something
that learns from data past experiences
it's programmed it's uh comes in there
and it says hey we already had these
five things happen the sixth one should
be about the same and then uh then
there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the uh backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the processing
machine learning uh spam detection very
common machine learning doesn't really
have the human interaction part so this
is the part where it goes and says okay
that's a Spam that's not a Spam and it
puts it in your spam
folder search engine result refining uh
another example of machine learning
whereas it looks at your different
results and it Go and it uh is able to
categorize them as far as this had the
most hits this is the least viewed this
has five stars um you know however they
want to weight it uh all exam good
examples of machine learning and then
the Deep learning uh deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so it's able to
look at the exit in the middle
and it goes okay we want to know what
that is in French and it's able to push
that out in French French and learn how
to do
that and then we have chatbots um I
remember when Microsoft first had their
little paperclip um boy that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being were you just doing a search on
certain words it's now able to start
linking those words together and form a
sentence in that chat box types of AI
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to reciter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at like a neural network
that's been programed to identify cars
it doesn't remember all those pictures
it has no memory as far as the hundreds
of pictures you process through it all
it has is this is the pattern I use to
identify cars as the final output for
that neural network we looked at so when
they talk about limited memory this is
what they're talking about they're
talking about I've created this based on
all these things but I'm not going to
remember any one
specifically theory of Mind systems
being able to understand human emotions
and how they affect decision-making to
just their behaviors according to their
human understanding this is important
because this is our page mark this is
how we know whether it is an artificial
intelligence or not is it interacting
with humans in a way that we can
understand uh without that interaction
is just an object uh so we talk about
theory of mind we really understand how
it interfaces that whole if you're in
web development user experience would be
the term I would put in there so the
theory of mind would be user experience
how's the UI connected together and one
of the final things as we get into
artificial intelligence is systems being
aware of themselves understanding their
internal States and predicting other
people's feelings and act appropriately
so as artificial intelligence continues
to progress uh we see ones are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based based
on all that information predicting which
action would work the best what would
help people uh if I know that you're
having a cup of coffee first thing in
the morning is what makes you happy as a
robot I might make you a cup of coffee
every morning at the same time uh to
help your life and help you grow that'd
be the self-awareness is being able to
know all those different things types of
machine learning and like I said on the
last slide this is very important this
is very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first one is supervised learning systems
are able to predict future outcome based
on past data requires both an input and
an output to be given to the model for
it to be trained so in this case we're
looking at anything where you have 100
images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
ident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them CU it's
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees all we told it was hey sep
separate this picture by things that
don't match and things that do match and
come together and finally there's
reinforcement learning systems are given
no training it learns on the basis of
the reward punishment it received for
performing its Last Action it helps
increase the efficiency of a tool
function or a program reinforced
learning or reinforcement learning is
kind of you give it a yes or no yes you
gave me the right response no you didn't
and then it looks at that and says oh
okay so based on this data coming in uh
what I gave you was a wrong response so
next time I'll give you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
we're grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe have like the case of
one of the things 500 points of data and
30 different fields it starts getting
really confusing there in artificial
intelligence or machine learning and the
Deep learning aspect really shines when
you get to that larger data that's
really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop
with no problem and do the calculations
there where with the machine learning
usually needs a higher-end system to
work it takes a lot more processing
power to build those neural networks and
to train them it goes through a lot of
data when we're talking about the
general machine learning tools most
features need to be identified in
advanced and manually coded so there's a
lot of human work on here the machine
learns the features from the data it is
provided so again it's like a magic box
you don't have to know what a tire is it
figures it out for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts and the
problem is solved in an endtoend manner
so you only have one neural network or
two neural networks that is bringing the
data in and putting it out it's not
going through a lot of different
processes to get there and remember you
can put machine learning and deep
learning together so you don't always
have just the Deep learning solving the
problem you might have solving one piece
of the puzzle
with regular machine learning and most
machine learning tools out there they
take longer to test and understand how
they work and with the Deep learning
it's pretty quick once you build that
neural network you test it and you know
so we're dealing with very crisp rules
limited
resources you have to really explain how
the decision was made when you use most
machine learning tools but when you use
the Deep learning tool inside the
machine learning tools the system takes
care of it based on its own logic and
reasoning and again it's like a magic
black box you really don't know how it
came up with the answer you just know it
came up with the right answer a glimpse
into the future so a quick glimpse into
the future artificial intelligence using
it to detecting crimes before they
happen humanoid AI helpers which we
already have a lot of there'll be more
and more maybe it'll actually be
Androids that'd be cool to have an
Android that comes and gets stuff out of
my fridge for me machine learning
increasing efficiency in health care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant simply
learns postgraduate program nii and
machine learning from puru University in
collaboration with the IB M should be
your right choice for more details check
out the link mentioned in the
description box below with that in mind
let's hand over this session to a
training expert knows the algorithm is a
step by-step process to approach a
particular problem there are numerous
examples of algorithm from figuring out
sets of number to finding Roots through
maps to showing data on a screen let's
understand this by using an example
every algorithm is built on inputs and
outputs Google search algorithm is no
different the input is the search field
and the output is the page of result
that appears when you enter a particular
phrase or keyword also known as sep or
search engine result page Google has a
search algorithm so it can sort results
from various website and provides the
users the best result when you start a
search you will see the search box we'll
attempt to guess what you are looking
for in order to better understand what
the user is looking for the algorithm is
trying to gather as many as suggestions
from them as possible the result from
the search field that best matches the
query will be ranked the choose F
website will Rank and in what position
using more than 200 ranking variables
now let's take an example of coding
program and see how the algorithm
works here we will use a case of
computer program wherein we want to
print the multiplication table of any
number let's step two the algorithm
start here and then it assign a value to
a variable the variable I is having an
initial value of one the system will
read the number the number in case is
two now the system has a condition a
condition can now either be true or
false if the value of I reaches 11 then
the loop will end otherwise value of I
will multiply by the number the initial
value of I is 1 so for the first time
the system output will be two now value
of I will be increased by 1 according to
the loop condition the system will then
move back and check for the condition
again the new value of I is two which is
still less than 11 the system will again
print 2 into I which is 2 into 2 on the
screen the new output result will be
four the system will keep following the
same procedure repeatedly until the
value of I becomes 11
once the value of I becomes 11 then only
the algorithm will terminate after
discussing how an algorithm work let's
move forward and see some popular
machine learning
algorithms some popular machine learning
algorithms are first one is linear
regression algorithm second one is
logistic regression algorithm and the
third one is decision tree and the
fourth one is support Vector machine
algorithm svm and the fifth one is K&N K
nearest neighbor algorithms and the
sixth one is K means clustering
algorithms and the seventh one is random
Forest algorithms and the last but not
the least algorithm is a PRI algorithms
let's go through them in detail one by
one linear regression is one of the most
famous and straightforward machine
learning algorithms utilized for
predictive analysis linear regression
show the linear connection between the
dependent and the independent factors
the equation of line is y = mx + b here
y stand for the response variable or a
dependent variable whereas X is for the
Predator variable or an independent
variable it attempts best to fit line
between the dependent and independent
variables and this best fit line is
known as line of regression or
regression
line let's take a real application
example in predicting consumer Behavior
businesses use linear regression to
forecast things like how much a client
is likely to spend things like targeted
marketing and product development May
benefit from this Walmart for instant
use linear regression to forecast which
good would be in high demand Across the
Nation moving forward let's see types of
linear
regression there are two types of linear
regression algorithm the first one is is
simple linear regression and the second
one is multiple linear
regression in simple linear regression
if an independent variable is utilized
to forecast the worth of a mathematical
dependent variable then at that point
such a linear regression algorithm is
called Simple linear regression the
equation of line will be y = a 0 + A1 X
and the second one is multiple linear
regression if the dependent variables
declines on the Y and the independent
variable on the X then such a
relationship is known as negative linear
relationship the line of equation will
be minus of a0 + A1
X moving forward let's see logistic
linear
regression logistic regression is the
supervised machine learning algorithm
utilized to anticipate all the
categorical factors or discrete values
it could be very well used for the
grouping issues in machine learning and
the result of the logistic regression
can be either yes or no zero or one men
or wom and so on it gives the values
which lies between zero and one for
example a credit card business is
interested in knowing whether the
transaction amount and the credit scope
have an impact on the probability that a
particular transaction would be
fraudlent the business can use logistic
regression to determine how these two
Predator values can relate the
probability that a transaction is
fraudin the response variable in the
model has two possible outcomes first
one is the transaction is fraud and the
second one is the transaction is not
fraud in logistic regression rather than
fitting a regression line we fit an S
form logistic capability which predicts
two greatest value zero or one the
logistic regression equation can be
calculated from linear regression
equation the steps to get logistic
regression equations are the equation of
a straight line can be return as y = b 0
+ B1 X1 + B2 X2 till BN xn in logistic
regression y can be between 0 and 1 only
so for that let's divide the above
equation by 1 - y then the equation will
be y upon 1 - y that is 0 for y0 and
Infinity for y = 1 but range between
minus infinity to plus infinity then we
have to take the logarithm of equation
and now it will become log of Y upon 1 -
y = b 0 + B1 X1 + B2 X2 so on till B and
xn let's move forward and see types of
logistic
regression there are three types of
logistic regression that can be
classified first one is binomial in
binomial logistic regression there can
be only two possible types of dependent
variables like yes or no pass or fail
man woman and many more and the second
one is multinomial in multinomial
logistic regression there can be three
or more possible unordered ways of
dependent variable such as horse cow and
sheep and the last one is ordinal in
ordinal logistic regression there can be
three or more possible ordered ways of
dependent variable such as small medium
or large moving forward let's see
decision trees in
detail a decision tree is a tree
structured classifier that could be used
for classification and regression a
decision tree is a tree in which each
non-leaf node is assigned to an
attribute additionally each are contain
one of the available values for its
parent node which is associated with
each Leaf node that is the node from
where the arc is directed
let's see some decision tree
terminology first one is root that
contains the entire data set the next
one is node attach for the data of a
certain attribute and the third one is
branch which connect the node to
internal node or the internal node to
Leaf node and the fourth one is leaf
node the terminal node that predicts the
outcome let's move forward and see
decision Tre algorithms the first one is
select the best attribute to use use the
current node in the tree the second one
is for each possible values select the
attributes the third one is partition
the examples using the possible values
of this attribute and assign these
disjoint subset of the examples to the
appropriate child node recursively
generate each child node until ideally
all examples for a node have the same
label like class moving forward let's
understand the decision tree for
building a decision tree step one is
Select an attribute then split the data
into its children in a tree continue
splitting with available
attributes and keep splitting until Leaf
node are pure like only one classs a
maximum depth is these a performance
metric is achieved let's move forward
and see svm algorithm support Vector
machine
algorithms a support Vector machine is a
well-known supervised machine learning
model it is utilized for both
information classification and
regression it is regularly utilized for
the grouping issues we can involve it in
different life care system and we can
involve it in typically happy or sad
look Arrangements we can involve it in
filters if we make specific looks it
would add the particular filter
according to the expression the scope of
articulation lies between happy and sad
support Vector machine helps him to
recognize hand return return characters
use yd like checks continue to be the
significant part of the majority of
non-cash transaction and are frequently
WR by the pupil the current check
processing system in many developing
nations involves a bank employe to read
and manually enter the information in a
check while also verifying the data like
signature and date a handwritten text
recognition system can reduce expenses
and labor hours because a bank must
handle several checks each day moving
forward let's see the algorithm of
svm the objective of support Vector
machine is to make the best line or
Choice limit that can isolate n
dimensional space into classes so we can
undoubtly put the new data of interest
in the right category later on this best
decision boundary is known as a hyper
plan let's move forward and see types of
support Vector machine support vector
machine can be of two types first one is
linear svm second one is nonlinear svm
let's move forward and see linear spvm
linear svm is utilized for linearly
detachable information which implies if
a data set can be ordered into two
classes by utilizing a straight line
then such information are named linearly
separable information and a classifier
is utilized called linear SPM classifier
moving forward let's see nonlinear SP M
nonlinear svm is utilized for
non-directly isolated information and
that implies in the event that are data
set can't be categorized by utilizing a
straight line such information is named
non-directed information and the
classifier utiliz is called a nonlinear
svm classifier moving forward let's see
KNN algorithm in
detail Cann is a supervised learning
technique Cann classifies new data into
our targeted classes depending on the
features of its neighboring points and
also be used for the regression problems
it is an instance-based learning
algorithm and a bit lazy learning
algorithm Cann calculation stores every
one of its accessible information and
orders another information Point based
on the likeliness this means that when
new data information appears it usually
tends to be successfully categorized
into a good suit classes using the can
an algorithm let's imagine we have an
image of animal that resembles a cow or
Ox however here we are not sure if it is
a cow or o SN method is based on a Lous
Matrix it will identify the properties
of new data that are related to the
image of cow or oxs and based on those
quality it will classify the data is
belonging to either cow or ugs group
moving forward let's see how does sknn
work
the steps to implement KL algorithms are
step one decide on the neighbors SK
numbers step two calculate the ukan
distance between K Neighbors in step two
third one is based on the determined
ukan distance select the K closest
neighbors step four is count the numbers
of data points in each category between
these K neighbors step five assign the
fresh data points in the category where
the highest neighbors count and then
Cann model is ready let's say we need to
add a new data point to the vital
category at first we will decide on the
numbers of neighbors therefore we will
pick K = to
5 then the ukan distance between the
data points and then can be determined
the distance between two points known as
the ukan distance can be determined by
under root of X2 - X1 whole sare + Y2 -
y1 whole s then we determine the closest
neighbors by calculating the ukan
distance there are three closest
Neighbors in category a and two closest
neighbor in category B this new data
point Must Fall with category a because
as we can see its three closest
neighbors are also from group a after
understanding KL algorithm let's move
forward and see K means algorithms in
detail the K means is a cluster falls
under that is an unsupervised learning
algorithm it is used to address machine
learning clustering problems and
utilized to tackle the grouping issues
in machine learning it permits us to
Bunch the information into various
Gatherings it is a helpful method for
finding the classification of groups in
the unlabeled data set without the
requirement of any training this cin
algorithm groups the data into similar
classes let's see some application of K
means clustering let's see some
applications of K means clustering
diagnostic system the medical profession
uses K means crusting in creating a
smarter medical decision support system
especially in the treatment of lever
alignments the second one is search
engines clustering forms a backbone of
search engine when a search engine is
performed the search result need to be
grouped and the search engines very
often use clustering to do this moving
forward let's see how K means algorithm
works the steps to implement K means
algorithm are step one select the number
K to set the number of clusters step two
select a random K points or centroid
step three assign each data point the
closest centroid data forms to
predefined K cluster step four determine
the variance and set a new Gravity
points for each cluster step five repeat
the third step this means reallocating
each data point to the new closest
centroid cluster step six if a
reassignment occurs go to step four
otherwise go to exit step seven the
model is ready to use so now we have an
clear understanding of how K means
algorithm work let's move forward to see
the graphical representation of K means
algorithm consider that there are two
variables M1 and M2 this is a scatter
plot of these two variables along the X
and Y AIS we should accept the number of
K of bunches that is K = to 2 to
recognize the data set and to place them
into various groups it implies here we
will attempt to Bunch these data set
into two unique groups we will really
want to pick an irregular keyo or
centroid to frame this group these
centroids can be either the focus of the
data set or some of other points thus
here we are choosing under two points as
SK points which are not the piece of our
data set we will assign every data of
interest in the scatter plot to the
nearest ke point or or centroid we will
register it by applying some math that
we consider to find the distance between
two points that is ukan distance thus we
will draw a median between both the
centroids from the graph the left half
of the line are Clos points to the right
and the K1 centroid green is near the
orange centroid we should veriy them as
green and orange for Clear
representation as the need might arise
to track down the nearest group we will
repeat the cycle by picking another
centroid to pick the new centroid we
will figure out the center point of
gravity of these centroid and we'll
track down new centroids then we will
reassign every piece of information to
highlight new centroid for this we will
repeat a similar course of tracking down
a middle line the middle will be like as
seen in the picture one orange point is
on the left half of the line and two
green points are on the right thus these
three points will be appointed to the
new centroids as reassignment has
occurred we will again go to step four
tracking down new centroids or k points
we will repeat the cycle by tracking
down the center point of gravity of
centroids so the new centroids will be
displayed as like this we now have new
centroid so once more Define the middle
boundary and reassign the data of
Interest by this graph there are no
unique pieces of information on one of
the other side of the line implying our
model is shaped by the previous graph
there are no unique pieces of
information on one or the other line
implying our model is shaped as our K
means model is ready and the two last
groups will be displayed as like these
now we have an clear understanding of
how K means clustering algorithm works
now let's move forward to understand
random Forest
algorithm random Forest is an adaptable
simple to utilize machine learning
algorithm that produces even without the
hyper boundary tuning an extraordinary
outcome more often than not it is
likewise quite possibly the most
utilized algorithm because of its effort
and variety like it tend to be utilized
for both grouping and classification
task random Forest is a classifier that
contains various Choice trees on
different subsets of the given data set
and takes the normal to work on the
present exactness of the data set
instead of depending on the choice tree
the r Forest takes the forecast from
each tree and in light of the larger
part of WS of expectation it predicts
the final result now let's move forward
and see how does random Forest
work we should see the random forest in
order since the arrangement is now and
again thought to be the structured block
of machine learning this is what a
random Forest would look like with two
trees the random Forest has a similar
hypermeter to the our decision tree or a
badging classifier luckily there is a
compelling reason need to consolidate a
decision tree with a badging classifier
since you can undoubtly utilize the
classifier classes of random forest with
random Forest you can likewise manage
task using the algorithm regression
random Forest add extra aridness to the
model while deing the trees rather than
looking for the M element while parting
a node it looks to the best component
among an irregular subset of highlights
these outcomes in a wide variety often
result in a superior model subsequently
in a random Forest just a random subset
of the element is thought about the
algorithms you might make trees more
random by involving random edges for
each component instead of looking for
the most ideal limits like a typical
Choice tree does let's move forward and
see some application of random Forest
algorithms random Forest is involved at
work by researchers in numerous Ventures
including banking stock exchanging
medication and many more it is utilized
to predict things which assist these
businesses with running productively
like client activity patient history and
safety in banking random Forest is used
to identify clients who are more likely
to pay back their deps on schedule
additionally it is utilized to forecast
who will make more frequent use of Bank
Services even fraud detection uses it
the Robin Hood of algorithms indeed
random Forest is a tool used by stock
traders to forecast future stock
Behavior retail businesses utilize it to
make product recommendation and forecast
client satisfaction random forage can be
used in healthcare industry to examine a
patient medical history and detect
disorders random Forest is a tool used
by pharmaceutical expert to determine
the ideal mix of of ingredients in
treatment or to forecast drug
sensitivity by seeing application of
random Forest algorithm let's move
forward and see some difference between
decision trees and random
Forest let's see the difference between
random forest and decision tree the
first one is while building a random
Forest the number of rows is selected
randomly in decision trees it builds
several decision trees and find out the
output the second one is it combines two
or more decision trees together in
decision trees whereas the decision is a
collection of variables or data sets or
attributes the third one is it gives
accurate results whereas it gives less
accurate results the fourth one is by
using random Forest it reduces the
chance of overl whereas decision trees
it has the possibility of overl the
fifth one is random Forest is more
complicated to interpreters whereas the
decision tree is simp simple it is easy
to read and understand after seeing what
is zom Forest how it works let's move
forward to see ariary algorithms in
detail the AI algorithm utilize standard
item sets to create affiliation rules
and is intended to chip away the
information basis containing exchanges
with the assistance of these affiliation
rules it decide how firmly or feebly two
objects are associated this algorithm
utilize a breath first search and
history to work out the item set
Association effectively it is the
iterative interaction for finding out
the successive item set from the huge
data set let's move forward and see
steps for AI algorithms the steps for AI
algorithms are Step One is establish
minimal support and confidence for item
set in the transactional database the
second one is take all transaction
supports with a greater support support
value the minimum or choosen support
value in step two the third one is track
down all the rules in these subsets with
confidence value greater than the
threshold value the fourth one is
arrange the rules to lower the lift at
last we will see some advantages and
disadvantages of AI algorithm the
advantages of AI algorithms are easy to
understand an algorithm and the second
one is the join and prun steps of the
algorithms can be easily imp implemented
on the large data set the disadvantages
of AI algorithms are the AI algorithms
work slowly as compared to the other
algorithm and the second one is the PRI
algorithm's times and space complexity
are o of 2D which is very low compared
to the other ones let's move forward and
see some Hands-On lab
demo so we will see a Hands-On lab demo
on linear regression as we know linear
regression is a way to find or to
predict DCT the relationship between two
variables generally we use X and Y so
first we will open command promt to
write command to open jupyter notebook
so we will here write
Jupiter notebook and then press
enter so this is the landing page of
jupyter notebook and select open new
python
file so this is how jupyter notebook UI
looks like so at first we will import
some major libraries of python which
will help us in mathematical functioning
so the first one is
numai napai is a python Library used for
working with arrays it also has
functions for working in domains like
linear algebra and matrices it is an
open source project and you can use it
freely numai stand for numerical python
so we will write like
import numpy as NP
here NP is used for denoting num so we
will import the next libraries pandas
pandas is a software Library written for
Python programming for like data
manipulation and Analysis in particular
it offers data stres and operation for
manipulating numerical labels and time
series so we will write import
pandas as PD here PD is used for
denoting panda
so our next library is mat plot lip mat
plot lip is a graph plotting library in
Python that serves as a visualization
utility M BL Li is a open source so we
can use it freely and it is most return
in Python a few segments in C and
JavaScript for platform
compatibility for importing M plot lip
you have to write import Mt plot
Li do lip Dot pelot
s PL so after importing the libraries we
will move ahead and import data set so
for importing data set we have to write
data
set equals to PD do read underscore CSV
and here we have to give the path of the
file data
set do
CSV here PD is for Panda's library and
read is is used for reading the data set
from the machine and CSV CSV is used for
the type of file which you want to read
so after reading let's see our data so
we will write data
set dot
head and press enter here head is used
for retrieving the first five lines from
the data so our data is coming properly
so moving ahead now let's first Define X
and Y axis for X we have to write xal to
data set
do
iock bracket colon comma colon again
minus
one dot values for xaxis and for y axis
we have to write y = to data
set do
iock bracket column comma
1 dot
values so if we will use data set. iog
for minus1 values for x suis it will
select till the second last column of
the data frame instead of the last
column and I know this is as the second
last column value and the last column
value for the row is different whereas
if you will use for the y axis values
return a series Vector a vector does not
have a column size so moving ahead let's
see the value for x and y axis first we
will see the value for x like just you
have to write X and press enter so
these are the arrays value for x and for
y axis you have to type Y and then enter
so these are arrays value for y axis so
after this now let's split the data set
into training and testing separating
data into training and testing set is an
important part for evaluating data
mining models typically when you
separate a data set into a training set
and a testing set most of the data set
is used for training and a similar
portion of data set is used for testing
so we will explain it into 70 and 30
ratio so for splitting we need to import
some more libraries for this process so
we will write from
skarn do model underscore
selection
selection
import
train underscore
testore
split psyched learn like SK learn is
most useful and robust library for
machine learning in Python it provides a
selection of efficient tools for machine
learning and statical modeling including
classification regression and clustering
so after importing let's write code for
the splitting data so we will write
here
xcore
train comma xcore test comma Yore
train comma
Yore
test equals
to P underscore test underscore split in
Brackets we have to write X comma
y comma
test
underscore
size equals to
0.3 comma
random _
State equals to
zero
random with random State equals to zero
we get the same train and test sets
across different execution so after this
let's see the values together so we will
write xcore
train comma
xcore
test comma y _
train comma
Yore
test so these are the values of array X
and array y together moving ahead now
let's work with regression first we need
to import the library for regression so
we will use
from
eson do
linear
underscore model
model
import
linear
regression we already discussed skar is
used for machine learning and linear
regression is a major part of machine
learning so after this let's make a
function for regression as RG for easy
use so we will write
R equals
to
linear regression
and brackets first we try to train and
then test and compare so we will write
here
r
dot
bit braet xcore
train comma
Yore
train now let's predict values but
prediction values are always different
so we will predict values for y first so
we will write y underscore
predict = to
R do
predict sorry
predict or xcore
test so like
Yore
credit then enter this is when the
predict functions come into the picture
python predict function enables us to
pred predict the labels of the data
values on the basis of training model
the predict function accept only a
single argument which is xcore test here
usually the data to be tested so when
you will check the value for y test you
will see the different values like
Yore
test so you can see there are totally
different values from x-axis now let's
display on graph for training data set
so we have to write PLP do
scatter xcore
Trin comma Yore
train comma
color equals to you can choose by
yourself I will choose
red and then
PLT do
plot xcore
train
comma regression point for predicting
values for
xcore
train comma color equals to
Blue so this color is called the
regression line and the PLT dot
title
linear
regression
regression
salary versus
experience and I will prefer the size
would be uh
0 so let's set the labels for x and y
axis so X label year of
employee of employee comma size will be
I will prefer size will be
1550 then for y axis we will
write
here like
salaries
or I will preper this same size
15 let's show the plot by writing PLT do
show
then hopefully we are fortunate
everything is going to be fine perfect
so you can see we have linear regression
line fitting through our data set so
this is how the linear regression work
for training data set let's see how it
will work for testing data
set now let's predict what T data set
you can copy for the same and like you
can change it so we can copy
here so we can paste here so we have to
just change here train to test data
set test here then
again test
here then
again test here
everything everything we will reduce
some
size to 16 let's see then it will
12
12
so perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
works for testing data set if if you're
an expiring AIML engineer then there is
no better time to train yourself in
exciting field of machine learning if
you are looking for a course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than our
celtech in partnership with IBM so why
wait join now seats are filling fast
find the course link from the
description box below so what is the
site kit learn it's simple and efficient
tool for data mining and data analysis
it's built on on numpy scipi and matplot
Library so it interfaces very well with
these other modules and it's an
open-source commercially usable BSD
license BSD originally stood for
Berkeley software distribution license
but it means it's open sourc with very
few restrictions as far as what you can
do with it another reason to really like
the S kit learn setup so you don't have
to pay for it as a commercial license
versus many other copyrighted platforms
out there what we can achieve using the
site kit learn we use class the two main
things classification and regression
models classification identifying which
category an object belongs to for one
application very commonly used a Spam
detection so is it a Spam or is it not a
Spam yes no in banking you might be is
this a good loan bad loan today we'll be
looking at wine is it going to be a good
wine or a bad wine and regression is
predicting an attribute associated with
an object one example is stock prices
prediction what is going to be the next
value if the stock today sold for $23 $5
a share what do you think it's going to
sell for tomorrow and the next day and
the next day so that'd be a regression
model same thing with weather weather
forecasting any of these are regression
models where we're looking at one
specific prediction on one attribute
today we will be doing classification
like I said we're going to be looking at
whether a Wine's good or bad but
certainly the regression model which is
in many cases more useful because you're
looking for an actual value is also a
little harder to follow sometimes so
classification is a really good place to
start we can also do clustering and
model selection clustering is taking an
automatic grouping of similar objects
into sets customer segmentation as an
example so we have these customers like
this they'll probably also like this or
if you like this particular kind of uh
features on your objects maybe you like
these other objects so it's a referral
is a good one especially in amazon.com
or any of your shopping networks model
selection comparing validating and
choosing parameters and models now this
is actually a little bit deeper as far
as far as a site kit learn we're looking
at different models for predicting the
right course or the best course or
what's the best solution today like I
said we're looking at wine so it's going
to be well how do you get the best wine
out of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and find tuning
now this is only part one so we're not
going to do too much tuning on the
models we're looking at but I'll Point
them out as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and I'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
I like to use the Jupiter notebook and I
use it out of the Anaconda Navigator so
if you install the Anaconda Navigator
by default it will come with the jupyter
notebook or you can install the jupyter
notebook by itself this code will work
in any of your python setups I believe
I'm running an environment of 3.7 setup
on there i' have to go in here under
environments and look it up for the
python setup but it's one of the 3xs and
we go a and launch this and this will
open it up in a web browser so it's kind
of nice it keeps everything separate and
in this Anaconda you can actually have
different environments different
versions of python different modules
installed in each environment so it's a
very powerful tool if you're doing a lot
of development and jupyter notebook is
just a wonderful visual display
certainly you can use I know spiders
another one which is installed with the
Anaconda I actually use a simple
notepad++ when I'm doing some of my
python script any of your IDE will work
fine jupyter notebook is iron python
because it's designed for the interface
but it's good to be aware of these
different tools and when I launch the
Jupiter notebook it'll open up like I
set a web page in here and we'll go over
here to new and create a new python
setup like I said I believe this is
python 37 but any of the three this the
py kit learn works with any of the 3xs
there's even 27 versions so it's been
around a long time so it's very big on
the development side and then the uh
guys in the back guys and gals developed
they went ahead and put this together
for me and let's go ahead and import our
different packages now if you've been
reading some of our other tutorials uh
you'll recognize pandas as PD pandas
library is pretty white widely used it's
a data frame setup so it's just like
columns and rows in a spreadsheet with a
lot of different features for looking
stuff up Seaborn sits on top of matap
plot Library this is for a graphing
we'll see that how quick it is to throw
a graph out there to view in the Jupiter
notebook for demos and showing people
what's going on and then we're going to
use the random Forest the SVC or support
vector classifier and also the neural
network so we're going to look at this
we're actually going to go through and
look at three different classifiers that
are most common some of the most common
classifiers and we'll show how those
work in the pyit learn setup and how
they're different and then if you're
going to do your um setup on here you'll
want to go ahead and import some metrics
so the SK learn. metrics on here and
we're use the confusion metrics and the
classification report out of that and
then we're going to use from the sklearn
pre-processing the standard scaler and
label encoder standard scaler is
probably the most commonly used
pre-processing there's a a lot of
different pre-processing packages in the
sklearn and then model selection for
splitting our data up it's one of the
many ways we can split data into
different sections and the last line
here is our percentage matap plot
library in line some of the caborn and
matplot Library will go ahead and
display perfectly in line without this
and some won't it's good to always
include this when you're in the Jupiter
notebook this is Jupiter notebook so if
you're in an IDE when you run this it
will actually open up a new window and
display the graphics that way so you
only need this if you're running it in a
editor like this one with the
specifically jupyter notebook I'm not
even familiar with other editors that
are like this but I'm sure they're out
there I'm sure there's a Firefox version
or something Jupiter notebook just
happens to be the most widely used out
there and we can go ahead and hit the
Run button and this now has saved all
this underneath the packages so my
packages are now all loaded I've run
them whether you run it on top we run it
to left and all the packages are up
there so we now have them all available
to us for our project we're working on
and I'm just going to make a little side
note on that when you're playing with
these and you delete something out and
add something in even if I went back and
deleted this cell and just hit the
scissors up here these are still loaded
in this kernel so until I go under
kernel and restart or restart and clear
or restart and run all I'll still have
access to pandas uh important to know
because I've done that before I've
loaded up maybe not a module here but
I've loaded up my own code and then
changed my mind and wondering why does
it keep putting out the wrong output and
then I realize it's still loaded in the
kernel and you have to restart the
kernel just a quick side note for
working with a Jupiter notebook and one
of the troubleshooting things that comes
up and we're going to go and load up our
data set we're using the pandas so if
you haven't yet go look at our pandas
tutorial a simple read the CSV with the
separation on here and so let me go
ahead and run that and that's now loaded
into the variable wine and let's take a
quick look at the actual file I always
like to look at the actual data I'm
working with in this case we have wine
quality d r I'll just open that up I
have it in my open Office setup
separated by semicolons that's important
to
notice and we open that up you'll see we
have go all the way down here looks like
1,600 lines of data minus the first one
so 15 1,599 lines and we have a number
of features going across the last one is
quality and right off the bat we see
that quality is uh has different numbers
in it 5 six 7 it's not really I'm not
sure how how high of a level it goes but
I don't see anything over a seven so
it's kind of five through seven is what
I see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with I didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates pH density total
suf sulfur dioxide and so on those are
all the features we're going to be
looking at and since this is a pandas
we'll just do wine head and that prints
the first five rows rows of data that's
of course Panda's command and we can see
that looks uh very similar to what we're
looking at before we have everything
across here it's automatically assigned
an index on the left that's what pandas
does if you don't give it an index and
for the column names it has assigned the
first row so we have our first row of
data pulled off the our comma separated
variable file in this case semicolon
separated and it shows the different
features going across and we have what 1
2 3 4 five 6 7 8 9 10 11 features 12
including quality but that's the one we
want to work on and understand and then
because we're in uh Panda's data frame
we can also do wi. info and let's go
ahead and run that this tells us a lot
about our variables we're working with
you'll see here that there is, 1599
that's what I said from the spreadsheet
so that looks correct non null float 64
this is very important information
especially the non-null so there's no
null values in here that can really trip
us up in pre-processing and there's a
number of ways to process non-null
values one is just to delete that data
out of there so if you have enough data
in there you might just delete your
non-null values another one is to fill
that information in with like the
average or the most common values or
other such means but we're not going to
have to worry about that but we'll look
at another way because we can also do
wine is null and sum it up and this will
give us a similar it won't tell us that
these are float values but it will give
us a summation oops there we go let me
run that it'll give us a summation on
here how many null values in each one so
if you wanted to you know from here you
would be able to say okay this is a null
value but she doesn't tell you how many
are null values this one would clearly
tell you that you have maybe five null
values here two null values here and you
might just if you had only seven null
values and all that different data you'd
probably just delete them out where if
uh 90% of the data was null values you
might rethink either a different data
collection setup or find a different way
to deal with the null values we'll talk
about that just a little bit in the
models too because the models themselves
have some built-in features uh
especially the forest model which we're
going to look at this point we need to
make a choice and to keep it simple
we're going to do a little
pre-processing of the data and we're
going to create some bins and bins we're
going to do is 2 comma 6.5 comma 8 what
this means is that we're going to take
those values if you remember up here let
me just scroll back up here we had our
quality the quality comes out between
between two and eight basically or one
and eight we have 5556 you can see just
in the just in the first five lines of
variation in quality we're going to
separate that into just two bins of
quality and so we've decided to create
two bins and we have bad and good it's
going to be the labels on those two bins
we have a spread of 6.5 and an exact
index of eight the exact index is
because we're doing 0 to8 on there the
6.5 we can change we could actually make
this smaller or greater but we're only
looking for the really good wind we're
not looking for the 0 1 2 3 4 five six
we're looking for wies with seven or
eight on them so high quality you know
this is what I want to put on my dinner
table at
night I want to taste the good wine not
the semi good wine or mediocre wine and
then this is a pandas so PD remember
stands for pandas pandas cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bins 2 comma 6.58 so two different bins
and our labels bad and good and we can
also do uh let me just do it this way
one quality since that's what we're
working on and let's look at unique
another pandas command and we'll run
this and I get this lovely error why did
I get an error well because I replaced
wine quality and I did this cut here
which changes things on here so I
literally altered one of the variables
is saved in the memory so we'll go up
here to the kernel restart and run all
it starts it from the very beginning and
we can see here that that fixes the
error because I'm not cutting something
that's already been cut we have our wine
quality unique and the wine quality
unique is a bad or good so we have two
qualities objects bad is less than good
meaning bad's going to be zero and
Good's going to be one and to make that
happen we need to actually encode it so
we'll use the label quality equals label
encoder and the label encoder let me
just go back there since this is part of
sklearn that was one of the things we
imported was a label encoder you can see
that right here from the SK learn.
processing import standard scaler which
we're going to use in a minute and label
encoder and that's what tells it to use
bad equals z and good equals 1 and we'll
go ahe and run that and then we need to
apply it to the data and when we do that
we take our wine quality that we had
before we're going to set that equal to
label quality which is our encoder and
let's look at this line right here we
have do fit transform and you'll see
this in the pre-processing these are the
most common used is fit transform and
fit transform because there's so often
that you're also transforming the data
when you fit it they just combined them
into one command and we're just going to
take the wine quality feed it back into
there and put that back in our wine
quality setup and run that and now when
we do uh the wine and the head of the
first five values and we go ahead and
run this you can see right here
underneath quality 0000 0 have to go
down a little further to look at the
better wines let's see if we have some
that are ones yeah there we go there's
some ones down here so when we look at
10 of them you can see all the way down
to zero or one that's our quality and
again we're looking at high quality
we're looking at the seven and the
eights or 6.5 and up and let's go ahead
and grab our where was it here we go
wine quality and let's take another look
at what else more information about the
wine quality itself and we can do a
simple pandas thing value counts I type
that in there correctly and we can see
that we only have
2117 of our wines which are going to be
the higher quality so so 217 and the
rest of them fall into the bad bucket
the zero which is uh 1382 so again we're
just looking for the top percentage of
these the top what is that it's probably
about a little little under 20% on there
so we're looking for our top wines our
seven and8 and let's use our uh let's
plot this on a graph so we take a look
at this and the SNS if you remember
correctly that is let me just go back to
the top that's our Seaborn Seaborn sits
on top of map plot Library it has a lot
of added features plus all the features
of the map plot library and it also
makes it quick and easy to put out a
graph and we'll do a simple bar graph
and they actually call it count plot and
then we want to just do count plot the
wine quality so let's put our wine
quality in there and let's go ahead and
run this and see what that looks like
and nice inline remember this is why we
did the in line so it make sure it
appears in here and you can see the blue
space or the first space represents
lowquality wine and our second bar is a
high quality line and you can see that
we're just looking at the top quality
wine here most of the wine we want to
just give it away to the neighbors no
maybe if you don't like your neighbors
maybe give them the good quality wine
and I don't know what you do with the
bad quality wine I guess use it for
cooking there we go but you can see here
it forms a nice little graph for us with
the Seaborn on there and you can see our
setup on that so now we've looked at
we've done some pre-processing we've
described our data a little bit we have
a picture of how much of the wine what
we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment if you're in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what uh in this
case quality what we're looking for and
we can take this we can go wine it's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality and make sure
we have our axis equals one if you left
it out it would still come out correctly
just because of the way it processes um
on the defaults and then our y if we're
going to remove quality for our X that's
just going to be one and it is just the
quality that we're looking at for y so
we put that in there and we'll go ahead
and run this so now we've separated the
features that we want to use to predict
the quality of the line and the quality
itself the next step is if you're going
to um create a data set in a model we
got to know how good our model is so
we're going to split the data train and
test splitting data and this is one of
the packages we imported from sklearn
and the actual package was train test
split and we're going to do XY test
size2 random State 42 and this returns
four variables and most common you'll
see is capital x Train That's so we're
going to train our set with capital X
test that's a data we're going to keep
on the side to test it with Y train y
remember stands for the quality or the
answer we're looking for so when we
train it we're going to use x train and
Y train and then y test to see how good
our X test does and the train test split
let me just go back up to the top that
was part of the sklearn model selection
import train test split there is a lot
of ways to split data up this is when
you're first starting you do your first
model you probably start with the the
Basics on here you have one test for
training one for test our test size is2
or 20% and random State just means we
just start with a it's like a random
seed number so that's not too important
back there we're randomly selecting
which ones we're going to use since this
is the most common way this is what
we're going to use today there is and
it's not even an sklearn package yet so
someone's still putting it in there one
of the new things they do is they split
the data into thirds and then they'll
run the model on each of they combine
each of those those thirds into two
thirds for training and one for testing
and so you actually go through all the
data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you know have a test side
one test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out it
works great so we have our XT train our
X test our y train and our y test and
then we need to go ahead and do the
scaler and let's talk about this because
this is really important some models do
not need to have scaling going on most
models do and so we create our scalar
variable we'll call it SC standard
scaler and if you remember correctly we
imported that here wrong with the label
encoder the standard scaler setup so
there's our scaler and this is going to
convert the values instead of having
some values that go from zero remember
up here we had some values are 54 60 40
59 102 so our total sulfur dioxide would
have these huge values coming into our
model and some models would look at that
and they'd become very biased to sulfur
dioxide it'd have the hugest impact and
then a value that had 0076 098 or
chlorides would have very little impact
because it's such a small number so when
we take the scaler we kind of level the
playing field and depending on our
scaler it uh sets it up between zero and
one a lot of times is what it does let's
go ahead and take a look at that and
we'll go ahead and start with our X
train and our X train equals SC fit
transform we talked about that earlier
that's an sklearn setup it's going to
both fit and transform our X train into
our X U train variable and if we have an
X train we also need to do that to our
test and this is important because you
need to note that you don't want to
refit the data we want to use the same
fit we used on the TR training is on the
testing otherwise you get different
results and so we'll do just oops not
fit transform we're only going to
transform the test side of the data so
here's our X test that we want to
transform and let's go ahead and run
that and just so we have an idea let's
go ahead and take and just print out our
X train oh let's do uh first 10
variables very similar to the way you do
the head on a a data frame you can see
here our variables are now much more
uniform and they've scale them to the
same scale so they're between certain
numbers and with the basic scaler you
can fine-tune it I just let it do its
defaults on this and that's fine for
what we're doing in most cases you don't
really need to mess with it too much it
does look like it goes between like
minus probably minus 2 to two or
something like that that's just looking
at the train variable we'll go and cut
that one out of there so before we
actually build the models and start
discussing the sklearn models we're
going to use we covered a lot of ground
here most of when you're working with
these models you put a lot of work into
pre prepping the data so we looked at
the data noticed that it's uh separated
loaded it up we went in there we found
out there's no null values that's hard
to say no no null values we have uh
there's none there's none null Val I
can't say it and of course we sum it up
if you had a lot of null values this
would be really important coming in here
so is there a null summary we looked at
pre-processing the data as far as the
quality and we're looking at the bins so
this would be something you might start
playing with maybe you don't want super
fine wine you don't want the seven and
eights maybe you want to split this
differently so certainly you can play
with the bins and get different values
and make the bins smaller or lean more
towards the lower quality so you then
have like medium to high quality and we
went ahead and gave it uh labels again
this is all pandas we're doing in here
setting it up with unique labels and
group names bad good bad is less than
good that can be so important you don't
know how many times people go through
these models and they have them reversed
or something and then they go back and
they're like why is this data not
looking correct so it's important to
remember what you're doing up here and
double check it and we used our label
encoder so that was um to set that up as
equality 01 good in this case we have uh
bad good 01 and we just double check
that to make sure that's what came up in
the quality there and then we threw it
into a graph because people like to see
graphs I don't know about you but you
start looking at all these numbers and
all this text and you get down here and
you say oh yes you know here this is how
much of the wine we're going to label as
subpar not good and this is how much
we're going to label as good and then we
got down here to finally separating out
our data so it's ready to go into the
models and the models take X and A Y in
this case X is all of our features minus
the one we're looking for and then Y is
the features we're looking for so in
this case we dropped quality and in the
Y case we added quality and then because
we need to have a training set and a
test set so so we can see how good our
models do we went ahead and split the
models up XT train X test y train y test
and that's using the train test split
which is part of the sklearn package and
we did um as far as our testing size 02
or 20% the default is 25% so if you
leave that out it'll do default setup
and we did a random State equals 42 if
you leave that out it'll use a random
State I believe it's default one I'd
have to look that back up and then
finally we scaled the data this is so
important to scale the data going back
up to here if you have something that's
coming out as 100 is going to really
outweigh something that's
0071 that's not in all the models
different models handle it differently
and as we look at the different models
I'll talk a little bit about that we're
going to only look at three models today
three of the top models used for this
and see how they compare and how the
numbers come out between them so we're
going to look at three different setups
oh let me change my cell here to
markdown there we go and we're going to
start with the random Forest class
classifier so the three sets we're
looking at is the random Forest
classifier support vector classifier and
then a neural network now we start with
the random Forest classifier because it
has the least amount of uh Parts moving
parts to fine-tune and let's go ahead
and put this in here so we're going to
call it RFC for random force classifier
and if you remember we imported that so
let me go back up here to the top real
quick and we did an import of the random
forth classifier from SK learn Ensemble
and then uh we'll all we also let me
just point this out here's our svm where
we inputed our support Vector classifier
so svm is support Vector model support
vector classifier and then we also have
our neural network and we're going to
from there the
multi-layered patron classifier kind of
a mouthful for the P Patron don't worry
too much about that name it's just it's
a neural network there's a lot of
different options on there in setups
which is where they came up with the
percept Seaton but so we have our three
different models we're going to go
through on here and then we're going to
weigh them here's our metrics we're
going to use a confusion metrics also
from the SK learn package to see how
good our model does um with our split so
let's go back down there and take a look
at that and we have our RFS equals
random forest classifier and we have n
estimators equals 200 this is the only
value you play with with a random Forest
classifier how many Forest do you need
or how many trees in the forest so how
many models are in here that makes it
pretty good as a startup model because
you're only playing with one number and
it's pretty clear what it is and you can
lower this number or raise it usually
start up with a higher number and then
bring it down to see if it keeps the
same value so you have less you know the
smaller the model the better the fit and
it's easier to send out to somebody else
if you're going to distribute it now the
random Forest classifier um everything I
read says it's used for kind of a medium
siiz data set so you can run it in on
Big Data you can run it on smaller data
obviously but tends to work best in the
mid-range and we'll go ahead and take
our RFC and I just copied this from the
other side fit XT train comma y train so
we're sending it our features and then
the quality in the Y train what we want
to predict in there and we just do a
simple fit now remember this is sklearn
so everything is fit or transform
another one is predict which we'll do in
just a second here let's do that now
predict RFC equals and it's our RFC
model predict and what are we predicting
on well we trained it with our train
value so now we need our test our X test
so this has done it this is going to do
this is the three lines of code we need
to create our random Forest variable fit
our training data to it so we're
programming it to fit in this case it's
got 200 different trees it's going to
build and then we're going to predict on
here let me go ahead and just run that
and we can actually do something like oh
let's do predict
RFC just real quick we'll look at the
first 20 variables of it uh let's go and
run that and uh in our first 20
variables we have three wins that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you can remember correctly
um we'll go Ahad and take this out of
here this is based on our test so these
are the first 20 values our test and
this has as you can see all the
different features listed in there and
they've been scaled so when you look at
these they're a little bit confusing to
look at and hard to read but we have
there's a minus 01 so this is 36-01 so1
164 minus
.09 or no it's still minus one so minus
.9 all between zero and one on here I
think I was confused earlier and I said
zero between two NE -2 but between minus
one and one which is what it should be
in the scale and we'll go ahead and just
cut that out of there run this this we
have our setup on here so now that we've
run the prediction and we have predicted
values well one you could uh publish
them but what do we do with them well we
want to do with them is we want to see
how well our model model performed
that's the whole reason for splitting it
between a training and testing model and
for that if you remember we imported the
classification report that was again
from the sklearn there's our confusion
Matrix and classification report and the
classification report actually sits on
the confusion Matrix so it uses that
information and our classification
report we want to know how good our y
test that's the actual values versus our
predicted RFC so we'll go ahead and
print this report out and let's take a
look and we can see here we have a
Precision out of the zero we had about
092 that were labeled as uh bad that
were actually bad and out of precision
for the um Quality wines we're running
about 78% so you kind of give us an
overall 90% And you can see our F1 score
or support setup on there our recall you
could also do the confusion Matrix on
here which gives you a little bit more
information but for this this is going
to be good enough for right now we're
just going to look at how good this
model was because we want to compare the
random force classifier with the other
two models and you know what let's go
ahead and put in the um confusion Matrix
just so you can see that on there with Y
test and prediction RFC so in the
confusion Matrix we can see here that we
had
266 correct and seven wrong these are
the Miss labels for bad wine and we had
a lot of Miss labels for good wine so
our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead oops cell
change cell type to markdown and run
that so we have a nice label let's look
at our svm classifier our support Vector
model and this should look familiar we
have our clf we're going to create
what's we'll call it just like we call
this an RFC and then we'll have our cf.
fit and this should be identical to up
above X train comma y train and uh just
like we did before let's go ahead and do
the prediction and here is our CF
predict and it's going to equal the
cf. predict and we want to go ahead and
use
xcore test and right about now you can
realize that you can create these
different models and actually just
create a loop to go through your
different model model and put the data
in and that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and I'm just going to copy this right
off of
here they say you shouldn't copy and
paste your code and the reason is is
when you go in here and edit it you
unbearably will miss something we only
have two lines so I think I'm safe to do
it today and let's go ahead and run this
and let's take a look how the svm
classifier came out so up here we had a
9 90% and down here we're running about
an
86% so it's not doing as good now
remember we randomly split the data so
if I run this a bunch of times you'll
see some changes down here so these
numbers this size of data if I ran it
100 times it would probably be within
plus or minus three or four on here in
fact if I ran this 100 times you'd
probably see these come out almost the
same as far as how well they do in
classification and then on the confusion
Matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12
so it's it's doing not quite as good
that shows up here 71% versus 78% and
then if we're going to do a uh svm
classifier we also want to show you one
more and before I do that I kind of
tease you a little bit here before we
jump into neural networks the um big
save all deep learning because
everything else must be shallow learning
that's a joke let's just take a little
bit about the svm versus the random
Forest classifier the svm tends to work
better on smaller numbers it also works
really good on um a lot of times you
convert things into numbers and bends
and things like that the random Forest
tends to do better with those at least
that's my brief experience with it where
if you have just a lot of raw data
coming in the svm is usually the fastest
and easiest to apply model on there so
they they each have their own benefits
you'll find though again that when you
run these like a 100 times difference
between these two on a data set like
this is going to just go away there's
Randomness involved depending on which
data we took and how they classify them
the big one is the neural networks and
this is what makes the neural networks
nice is they can do they can look into
huge amounts of data so for a project
like this you probably don't need a
neural network on this but it's
important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and theyve just come come
out with different ways of feeding that
data in where the series and the Order
of the words is really important same
thing with uh starting to predict in the
stock market if you have tons of data
coming in from different sources the
neural network can really process that
in a powerful way to pull up things that
aren't seen before when I say lots of
data coming in I'm not talking about
just the high lows that you can run an
svm on real easily I'm talking about the
data that comes in where you have maybe
you pulled off the Twitter Feats and
have word counts going on and you've
pulled off the uh the different news
feeds the business are looking at and
the different releases when they release
the different reports so you have all
this different data coming in and the
neural network does really good with
that pictures picture processing Now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by Google it has a neural
network for doing it's kind of goofy but
you can put Little Star Wars Androids
dancing around your pictures and things
like that that's all done with the
neural network so it has a lot of
different uses but it's also requires a
a lot of data and is a little
heavy-handed for something like this and
this should now look familiar because
we've done it twice before we have our
multi-layered Perron classifier we'll
call it an mlpc and it's this is what we
imported mlpc classifier there's a lot
of settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then I went
ahead and just did three layers probably
get by with a lot less on this but you I
didn't want to sit and play with it all
afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with overfitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500 the defaults 200
because I use three layers of 11 each
which is by the way kind of a default I
use I realized that usually you have
about three layers going down and the
number of features going across you'll
see that's pretty common for the first
first classifier when you're working in
neural networks but it also means you
have to do higher iterations so we up
the iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have uh you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
XT train y train and then we going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict X test let's just put
that down here do predict X test and if
I run that we've now programmed it we
now have our prediction here same as
before and we'll go ahead and do the
copy print again always be careful with
the copy paste because you always run
the the chance of missing one of these
variables so if you're doing a lot of
coding you might want to skip that copy
and paste and just type it in and let's
go ahead and run this and see what that
looks like and we came up with an 88%
we're going to compare that with the 86
from our tree our svm classifier and our
90 from the random forest classifier and
keep in mind random Forest classifiers
they do good on midsize data the svm on
smaller amounts of data although to be
honest I don't think that's necessarily
the split between the two and these
things will actually come together if
you random a number of times and we can
see down here the N of good wines
mislabeled with setup on there it's on
par with our random Forest so it had 22
25 shouldn't be a surprise it's
identical it just didn't do as good with
the bad wines labeling what's a bad wine
and what's not see yeah cuz they had 266
and seven we had down here 260 and 13 so
mislabeled a couple of the bad wines as
good wines so we've explored three of
these basic classifiers these are
probably the three most widely used
right now I might even throw in the
random tree if we open up their website
and we go under supervised learning
there's a linear model we didn't do that
almost most of the data usually just
start with a linear model because it's
going to process the quickest and use
the least amount of resources but you
can see they have linear quadratic they
have kernel Ridge there's our support
Vector stochastic gradient nearest
neighbors nearest neighbors is another
common one that's used a lot very
similar to the svm gausian process cross
decomposition naive B days this is more
of an intellectual one that I don't see
used a lot but it's like the basis of a
lot of other things decision tree
there's another one that's used a lot
Ensemble methods not as much multiclass
and multi-label algorithms feature
selection neural networks that's the
other one we use down here and of course
the forest so you can see there's a in
sklearn there are so many different
options and theyve just developed them
over the years we covered three of the
most commonly used ones in here and went
over a little bit over why they're
different neural network just because
it's fun to work in deep learning and
not in Shallow learning as I told you
that doesn't mean that the S SPM is
actually shallow it's U does a lot of it
covers a lot of things and same thing
with the decision the random forest
classifier and we notice that there's a
number of other different classifier
options in there these are just the
three most common ones and I'd probably
throw the nearest neighbor in there and
the decision tree which is usually part
of the decision for us depending on what
the back end you're using and since as
human beings um if I was in the share
holder's office I wouldn't want to leave
them with a confusion Matrix they need
that information for making decisions
but we want to give them just one
particular score and so I would go ahead
and we have our sklearn metrics we're
going to import the accuracy score and
I'm just going to do this on the um
random Forest since that was our best
model and we have our CM accuracy score
and I forgot to print it remember in
jupyter Notebook we can just do the last
variable we leave out there will print
and so our CM accurate score we get is
90%
and that matches up here we should
already see that up here in Precision so
you could either quote that but a lot of
times people like to see it highlighted
at the very end this is our Precision on
this model and then the final stage is
we would like to use this for future so
let's go ahead and take our wine if you
remember correctly we'll do wine head of
10 we'll run that remember our original
data set we've gone through so many
steps now we're going to go back to the
original data and we can see here we
have our top 10 our top 10 on the list
only two of them make it as having high
enough quality wine for us to be
interested in them and then let's go
ahead and create some data here we'll
call it X new equals and this is
important this data has to be we just
kind of randomly selected some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our X
new equals
7.3.5 and so on and then it is so
important this is where people forget
this step X new equals SC remember SC
that was our standard scaler variable we
created if we we go right back up here
before we did anything else we created
an sc we fit it and we transformed it
and then we need to do what transform
the data we're going to feed in so we're
going to go back down here and we're
going to transform our X new and then we
were going to go ahead and use the where
are we at here we go our random forest
and if you remember all it is is our RFC
predict model right there let's go ahead
and just grab that down here and so our
y new equals here's our RFC predict
we're going to do our X new in and then
it's kind of nice to know what it
actually puts out so according to this
it should print out what our prediction
is for this wine and oh it's a bad wine
okay so we didn't pick out a good wine
for our ex new and that should be
expected most of wine if you remember
correctly only a small percentage of the
wine met our quality requirements so we
can look at this and say oh we'll have
to try another wine out which is fine by
me CU I like to try out new wines and I
certainly have a collection of old wine
bottles and very few of them match but
you can see here we've gone through the
whole process just a quick re rehash we
had our Imports we touched a lot on the
sklearn our random Forest our svm and
our MLP classifier so we had our um
support Vector classifier we had our
random forest and we have our neural
network three of the top used
classifiers in the sklearn system and we
also have our confusion matri Matrix and
our classification report which we used
our standard scaler for scaling it and
our label encoder and of course we need
to go ahead and split our data up in our
imp plot line train and we explored the
data in here for null values we set up
our quality into bins we took a look at
the data and what we actually have and
put a nice little plot to show our
quality what we're looking at and then
we went through our three different
models and it's always interesting
because you spend so much time getting
to these models and then you kind of go
through the models and play with them
until you get the best training on there
without becoming biased that's always a
challenge is to not overtrain your data
to the point where you're training it to
fit the test value and finally we went
ahead and actually used it and applied
it to a new wine which unfortunately
didn't make the cut it's going to be the
one that we drink a glass out of and
save the rest from
cooking of course that's according to
the random Forest on there because we
used the best model that it came up with
if you're an expiring AIML engineer then
there is no better time to train
yourself in exciting field of machine
learning if you are looking for a course
that covers everything from fundamentals
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our celtech in partnership with IBM
so why wait join now seats are filling
fast find the course link from the
description box below Eve Bas classifier
have you ever wondered how your mail
provider implements spam filtering or
how online news channels perform news
text classification or how companies
perform sentimental analysis of their
audience on social media all of this and
more is done through a machine learning
algorithm called naive Bay classifier
welcome to naive Bay tutorial my name is
Richard kersner I'm with the simplylearn
team that's www.s simplylearn tocom get
certified get ahead what's in it for you
we'll start with what is naive Bay a
basic overview of how it works we'll get
into naive bays and machine learning
where it fits in with our other machine
learning tools why do we need naive Bay
and understanding naive Bay classifier a
much more in-depth of how the math works
in the background finally we'll get into
the advantages of the Nave Bay
classifier in the machine learning setup
and then we'll roll up our sleeves and
do my favorite part we'll actually do
some python coding and do some text
classification using the naive Bay what
is naive Bay let's start with a basic
introduction to the Bay theorem named
after Thomas baze from the 1700s who
first coined this in the western
literature naive baz classifier works on
the principle of conditional probability
as given by the Bas theorem before we
move ahead let us go through some of the
simple Concepts in the probability that
we will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they could come up as head heads they
come up as head tail tell head and tell
tail when doing the math on probability
we usually denote probability as a p a
capital P so the probability of getting
two head heads equals 1/4 you can see in
our data set we have two heads and this
URS once out of the four possibilities
and then the probability of at least one
tail occurs 3/4 of the time you'll see
on three of the coin tosses we have
tails in them and out of four that's
3/4s and then the probability of the
second coin being head given the first
coin is tail is 1/2 and the probability
of getting two heads given the first
coin is ahead is 1/2 we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins is easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the base
theorem gives us the conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Baye theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals a probability of
B given a times probability of a you can
easily move that around and multiply it
and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with tells in
them giving us three quarters or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins cuz you can easily understand the
math the probability of throwing a tail
doesn't matter what comes before it and
the same with the head so still going to
be 50% or 1/2 but when that come when
that probability gets more complicated
let's say you have a D6 dice or some
other instance then this formula really
comes in handy but let's stick to the
simple example for now in this sample
space let a be the event that the second
coin is head and B be the event that the
first coin is tail else again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
Time the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being Tails given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1/2
* 1/2 over 1/2 or 1 12 = .5 or 1/4 so
the base theorem basically calculates
the conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of of online shopping further in
this tutorial understanding naive bays
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
Bas fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also regression
but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is naive
Bay used let's look at some of the used
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
risk for cancer or heart disease or
other ailments and news classification
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with
the naive Bay understanding naive Bay
classifier now we already went through a
basic understanding with the coins and
the two heads and two tails and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Bas classifier is
based on the B the
which gives a conditional probability of
in event a given event B and that's
where the probability of a given b
equals the probability of B given a *
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we can multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easily come up with a new formula for
the probability of B to me staring at
these algebraic functions
kind of gives me a slight headache it's
a lot better to see if we can actually
understand how this data fits together
in a table and let's go ahead and start
applying it to some actual data so you
can see what that looks like so we're
going to start with the shopping demo
problem statement and remember we're
going to solve this first in uh table
form so you can see what the math looks
like and then we're going to solve it in
Python and in here we want to predict
whether the person will purchase a
product are they going to buy or don't
buy very important if you're running a
business you want to know how to
maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's uh on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visuals great but we do need to dig
into the data so let's go ahead and take
a look at the data set we have a small
sample data set of 30 rows we're showing
you the first 15 of those RADS for this
demo now the actual data file you can
request just type in below under the
comments on the YouTube video and we'll
send you some more information and send
you that file as you can see here the
file is very simple columns rows we have
the day the discount the free delivery
and did the person purchase or not and
then we have under the day whether it
was a week day a holiday was it the
weekend this is a pretty simple set of
data and long before computers people
used to look at this data and calculate
this all by hand so let's go ahead and
walk through this and see what that
looks like when we put that into tables
also note in today's world we're not
usually looking at three different
variables in 30 rows nowadays cuz we're
able to collect data so much we're
usually looking at at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those dates
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to this table
for our Baye theorem let the event buy
ba now remember when we looked at the
coins I said we really want to know what
the outcome is did the person buy or not
and that's usually event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so it's 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right you can see that nine
out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and 8 out of 24 purchases on a
holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dates two out of six didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out 11 out
of 30 so out of the 30 people who came
into the store throughout the weekend
weekday and holiday 11 of those
purchases were made on the week day and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or .2 so there's
a 20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B if a in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making a
purchase at all doesn't matter what day
of the week is roughly 0.2 or 20% and
the the probability of a weekday no
purchase is roughly two out of six so
two out of six of our no purchases were
made on the weekday and then finally we
take our P of a if you looked we've kept
the symbols up there we got P of
probability of B probability of a
probability of B if a we should remember
that the probability of a if B is equal
to the first one times the probability
of no per buys over the probability of
the weekday so we could calculate it
both off the uh table we created we can
also calculate this by the formula and
we get the 367 which equals or 33 * 2
over 367 which equals. 179 or roughly uh
17 to 18% and that'd be the probability
of no purchase done on the weekday and
this is important because we can look at
this and say as the probability of
buying on the weekday is more than the
probability of not buying on the weekday
we can conclude include that customers
will most likely buy the product on a
weekday now we've kept our chart simple
and we're only looking at one aspect so
you should be able to look at the table
and come up with the same information or
the same conclusion that should be kind
of intuitive at this point next we can
take the same setup we have the
frequency tables of all three
independent variables now we can
construct the likelihood tables for all
three of the variables we're working
with we can take our day like we did
before we have weekday weekend and
holiday we filled in this table and then
we can come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full table so now
we have our probabilities for a discount
and whether the discount leads to a
purchase or not and the probability for
free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes let's dig deeper
into the math and actually see what this
looks like and we're going to start with
looking for the probability of them not
purchasing on the following combinations
of days we are actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
going to set equal to is it a holiday
did they get a discount yes and was it a
free delivery yes before we go further
let's look at the original equation the
probability of a if B equals the
probability of B given the condition a
and the probability times probability of
a over the probability of B occurring
now this is basic algebra so we can
multiply this information together so
when you see the probability of a given
B in this case the condition is b c and
d or the three different variables we're
looking at and when you see the
probability of B that would be the
conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in
just a second in the formula times the
full probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is a day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a if B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the pro probability of a no buy
is across all the data so that's where
we get the 6 out of 30 we divide that
out by the probability of each
category over the total number so we get
the 20 out of 30 had a discount 23 out
of 30 had a yes for free delivery and 11
out of 30 were on a holiday we plug all
those numbers in we get
178 so in our probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 * 21 over 24 * 8
over 24 * the P of a 24 over 30 divided
by the probability of the discount the
free delivery times the day or 20 over
30 20 3 over3 * 11 over 30 and that
gives us our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals
986 we have a probability of no purchase
equals 178 so finally we have a
conditional probabilities of purchase on
this day let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals
98686 plus. 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.7 1% and
the likelihood of no purchase is
15.29% given these three different
variables so it's if it's on a holiday
if it's uh with a discount and has free
delivery then there's an 84.7 1% chance
that the customer is going to come in
and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand why you want to why you want
to use the naive Bays what are the
advantages of naive bays and then once
we understand those advantages we just
look at that briefly then we're going to
dive in and do some python coding
advantages of naive Bay classifier so
let's take a look at the six advantages
of the naive Baye classifier and we're
going to walk around this lovely wheel
looks like an origami folded paper the
first one is very simple and easy to
implement certainly you could walk
through the tables and do this by hand
you got to be a little careful because
the notations can get confusing you have
all these different probabilities and I
certainly mess those up as I put them on
you know is it on the top or the bottom
got to really pay close attention to
that when you put it into python it's
really nice CU you don't have to worry
about any of that you let the python
handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amounts of data this
is great powerful tool for that handles
both continuous and discrete data it's
highly scalable with number of
predictors and data points so as you can
see you just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast it
can be used in real time predictions
this is so important this is why it's
used in a lot of our predictions on
online shopping carts uh referrals spam
filters is because there's no time delay
as it has to go through and figure out a
neural network or one of the other mini
setups where you're doing classification
and certainly there's a lot of other
tools out there in the machine learning
that can handle these but most of them
are not as fast as the naive Bay and
then finally it's not sensitive to
irrelevant features so it picks up on
your different probabilities and if
you're short on date on one probability
you can kind of it automatically adjust
for that those formulas are very
automatic and so you can still get a
very solid predictability even if you're
missing data or you have overlapping
data for two completely different areas
we see that a lot in doing census and
studying of people and habits where they
might have one study that covers one
aspect another another one that overlaps
and because the two overlap they can
then predict the unknowns for the group
that they haven't done the second study
on or vice versa so it's very powerful
in that it is not sensitive to the
irrelevant features and in fact you can
use it to help predict features that
aren't even in there so now we're down
to my favorite part we're going to roll
up our sleeves and do some actual
programming we're going to do the use
case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bays so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right sub subgroups
I'm not sure where they actually pulled
the actual data we're going to use from
it's one of the standard sets but
certainly this can be used on any of our
news headlines and classification so
let's see how it can be done using the
naive Bay classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you a
graph of what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever uh interface IDE
you want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
have have a nice view of what's going on
uh and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage mat plot library in
line now three of these lines of code
are all about plotting the graph this
one let's The Notebook notes and is the
inline setup that we want the graphs to
show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
IDs don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop up and
the graph pops up on there so you have a
that setup also but for this we want the
matap plot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our SK learn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP nump as NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library. pyplot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import caborn as SNS and we're
going to do the SNS doet now caborn sits
on top of pip plot and it just makes a
really nice heat map it's really good
for heat maps and if you're not familiar
with heat maps that just means we give
it a color scale the term comes from the
brighter red it is the hotter it is in
some form of data and you can set it to
whatever you want and we'll see that
later on so those you'll see that those
three lines of code here are just
importing the graph function so we can
graph it and as a data scientist you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything and then from the s SK learn.
dat sets we're going to import the fetch
20 news groups very common one for
analyzing tokenizing words and setting
them up and exploring how the words work
and how do you categorize different
things when you're dealing with
documents and then we set our data equal
to fetch 20 news groups so our data
variable will have the data in it and
we're going to go ahead and just print
the target names data. Target names and
let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp o Ms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually here going to go ahead and get
it get the data all set up and take a
look at our data and let's move this
over to our Jupiter notebook and let's
see what this code does first we're
going to set our categories now if you
noticed up here I could have just as
easily set this equal to data. Target
names because it's the same thing but we
want to kind of spell it out for you so
you can see the different categories it
kind of makes it more visual so you can
see what your data is looking like in
the background once we've created the
categories we're going to open up a
train set so this training set of data
is going to go into fetch 20 news groups
and it's a subset in there called train
and categories equals categories so
we're pulling out those categories that
match and then if you have a train set
you should also have the testing set we
have test equals fetch 20 news groups
subset equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates train.
dat and we're just going to look at data
piece number five and let's go ahead and
run that and see what that looks like
and you can see when I print train. dat
number five under train it prints out
one of the Articles this is article
number five you can go through and read
it on there and we can also go in here
and change this to test which should
look identical because it's splitting
the data up into different groups train
and test and we'll see test number five
is a a different article but another
article in here and maybe you're curious
and you want to see just how many
articles are in here we could do length
of train. dat and if we run that you'll
see that the training data has
11,314 articles so we're not going to go
through all those articles that's a lot
of articles but um we can look at one of
them just so you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment ID VT
line 58 lines 58 8 in article uh Etc and
you can scroll all the way down and see
all the different parts to there now
we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you wait this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category fits in based
on these words is where the challenge
comes in now that we've viewed our data
we're going to dive in and do the actual
predictions this is the actual naive
base and we're going to throw another
model at you or another module at you
here in just a second we can't go into
too much detail but it deals
specifically working with words and text
and what they call tokenizing those
words so let's take this code and let's
uh skip on over to our Jupiter notebook
and walk through it and here we are in
our jupyter notebook let's paste that in
there and I can run this code right off
the bat it's not actually going to
display anything yet but it has a lot
going on in here so the top we had the
print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction. text we're going to
import tfidf vectorizer I told you we're
going to throw a module at you we can't
go too much into the math behind this or
how it works you can look it up the
notation for the math is usually tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times they're used in a
document how many times or how many
documents they're used in and it's a
well-used formula it's been around for a
while it's a little confusing to put
this in here uh but let's let it know
that it just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you put a weit on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value in telling you what
this document's about same with words
like in an article in articles in cost
of un maybe cost might or where words
like criminal weapons destruction these
might have a heavier weight because they
describe a little bit more what the
article's doing well how do you figure
out all those weights in the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our SK learn. naive Bas and
that's our multinomial NB multinomial
naive Bas pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline import
make pipeline now the make pipeline is
just a cool piece of code because we're
going to take the information we get
from the tfidf vectorizer and we're
going to pump that into the multinomial
INB so a pipeline is just a way of
organizing how things flow it's used
commonly you probably already guess what
it is if you've done any businesses they
talk about the sales pipeline if you're
on a work crew or project manager you
have your pipeline of information that's
going through where your projects and
what has to be done on what order that's
all this pipeline is we're going to take
the tfid vectorizer and then we're going
to push that into the multinomial INB
now we've designated that as the
variable model we have our pipeline
model and we're going to take that model
and this is just so elegant this is done
in just a couple lines of code model.fit
and we're going to fit get the data and
first the train data and then the train
Target now the train data has the
different articles in it you can see the
one we were just looking at and the
train. target is what category they
already categorized that that particular
article as and what's Happening Here is
the train data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it weights all
the words in there so there's thousands
of words with different weights on them
I me remember once running a model on
this and I literally had 2.4 million
tokens go into this so when you're
dealing like large document bases you
can have a huge number of different
words it then takes those words gives
them a weight and then based on that
weight based on the words and the
weights and then puts that into the
multinomial NB and once we go into our
naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um Talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
used the term. predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name it's
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there there and
let's go ahead and run that code take it
just a moment and remember we had the
inline that way my graph shows up on the
inline here and let's walk through the
code and then we'll look at this and see
what that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual true truth was
either agreed with it or is something
different and it's going to add up those
numbers so we can take a look and just
see how well it worked and we're going
to set a variable mat equal to confusion
Matrix we have our test Target our test
data that was not part of the training
very important in data science we always
keep our test data separate otherwise
it's not a valid model if we can't
properly test it with new data and this
is the labels we created from that test
data these are the ones that we predict
it's going to be so we go in and we
create our SN heat map the SNS is our
caborn which sits on top of the P plot
so when we create a SNS do heat map we
take our confusion Matrix and it's going
to be uh matt. T and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 00001 format d and c
bar equal have to do with the uh format
if you take those out you'll see that
some things disappear and then the X
tick labels and the Y tick labels those
are our Target names and you can see
right here that's the alt atheism comp
Graphics comp osms windows.
miscellaneous and then finally we have
our pl. xlabel remember the SNS or the
caborn sits on top of our map plot
Library our PLT and so we want to just
tell it xlabel equals a true is is true
the labels are true and then then the Y
label is prediction label so when we say
a true this is what it actually is and
the prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way we
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides because they have a
black background they put in there that
helps it shine a little bit better so
you can see the graph a little bit
easier so in reading this graph what we
want to look at is how the color scheme
has come out and you'll see a line right
down the middle diagonally from upper
left to bottom right what that is is if
you look at the labels we have our
predicted label on the left and our true
label on the right those are the numbers
where the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk poit politics
miscellaneous social religion Christian
versus Alt atheism it mislabeled some of
those and those are very similar topics
you could understand why it might
mislabel them but overall it did a
pretty good job if we're going to create
these models we want to go ahead and be
able to use them so let's see what that
looks like to do this let's go ahead and
create a definition a function to run
and we're going to call this function
let me just expand that just a notch
here there we go I like mining big
letters predict categories we want to
predict the CATE category we're going to
send it s a string and then we're
sending it train equals train we have
our training model and then we had our
pipeline model equals model this way we
don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model. predicts so it's going to send
whatever string we send to it it's going
to push that string through the pipeline
the model pipeline it's going to go
through and and uh tokenize it and put
it through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
it'll put that through our naive Bay and
from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train. Target names predict of zero and
remember that the train. target names
that's just categories I could have just
as easily put uh categories in there.
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from um I don't know
what the actual numbers are but let's
say zero equals alt atheism so we're
going to convert that zero to the word
or one maybe it equals comp Graphics so
we're going to convert number one into
comp Graphics that's all that is and
then we got to go ahead and and then we
need to go ahead and run this so I load
that up and then once I run that we can
start doing some predictions I'm going
go ahead and type in predict C category
and let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out is returning the
variable train. Target names it'll
automatically print that for you in your
own ID you might have to put in print
let's see where else we can take this
and maybe you're a space sence buff so
how about sending load to
International Space
Station and if we run that we get
science space or maybe you're a uh
automobile buff and let's do um oh they
were going to tell me Audi is better
than BMW but I'm going to do BMW is
better than an Audi so maybe your car
buff and we run that and you'll see it
says recreational I'm assuming that's
what r C stands for Autos so I did a
pretty good job labeling that one how
about uh if we have something like a
caption running through there president
of India and if we run that it comes up
and says talk politics miscellaneous if
you're an expiring a IML engineer then
there is no better time to train
yourself in exciting field of machine
learning if you are looking for a course
that covers everything from fundamentals
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our celtech in partnership with IBM
so why wait join now seats are filling
fast find the course link from the
description box
look in this lesson you are going to
understand the concept of text
mining by the end of this lesson you
will be able to explain text mining
execute text processing
task so let's go ahead and understand
text mining in detail let's first
understand what text mining mining is
text mining is the technique of
exploring large amounts of unstructured
Text data and analyzing it in order to
extract patterns from the text Data it
is aided by software that can identify
Concepts patterns topics keywords and
other attributes in the data it utilizes
computational techniques to extract and
summarize the high quality information
from unstructured textual resources
let's understand the flow of text mining
there are five techniques used in text
mining system information extraction or
text
pre-processing this is used to examine
the unstructured text by searching out
the important words and finding the
relationships between them
categorization or text transformation
attribute generation categorization
technique labels the text document under
one or more categories classification of
Text data is done based on input output
examples with
categorization clustering or attribute
selection clustering method is used to
group text documents that have similar
content clusters are the partitions and
each cluster will have a number of
documents with similar content
clustering makes sure that no document
will be omitted from the search and it
deres all the documents that have
similar content visualization technique
the process of finding relevant
information is simplified by
visualization technique this technique
uses text Flags to represent a group of
documents or a single document and
compactness is indicated using colors
visualization technique helps to display
textual information in a more attractive
way summarization or interpretation or
evaluation summarization technique will
help to reduce the length of the
document and summarize the details of
the documents it makes the document EAS
easy to read for users and understand
the content at the
moment let's understand the significance
of text
mining document clustering document
clustering is an important part of text
mining it has many applications in
Knowledge Management and information
retrieval clustering makes it easy to
group similar documents into meaningful
groups such as in newspapers where
sections are often grouped as business
Sports politics and so on pattern
identification text mining is the
process of automatically searching large
amount of text for text patterns and
recognition of features features such as
telephone numbers and email addresses
can be extracted using pattern matches
product insights text mining helps to
extract large amounts of text for
example customer reviews about the
products mining consumer reviews can
reveal insights like most loved feature
most hated feature improvements require
required and reviews of competitor's
products security monitoring text mining
helps in monitoring and extracting
information from news articles and
reports for national security purposes
text mining makes sure to use all of
your available information it is a more
effective and productive knowledge
discovery that allows you to make better
informed decisions automate information
intensive processes gather business
critical insights and mitigate
operational risk
let's look at the applications of text
mining speech recognition speech
recognition is the recognition and
translation of spoken language into text
and vice versa speech often provides
valuable information about the topics
subjects and concepts of multimedia
content information extraction from
speech is less complicated yet more
accurate and precise than multimedia
content this fact motivates
content-based speech analysis for
multimedia Data Mining and retrieval
where audio and speech processing is a
key enabling
technology spam filtering spam detection
is an important method in which textual
information contained in an email is
extracted and used for
discrimination text mining is useful in
automatic detection of spam emails based
on the filtering content using text
mining an email service provider such as
Gmail or Y
mail checks the content of an email and
if some malicious text is found in the
mail then that email is marked a Spam
and sent to the spam
folder sentiment analysis it is done in
order to determine if a given sentence
expresses positive neutral or negative
sentiments sentiment analysis is one of
the most popular applications of text
analytics the primary aspect of
sentiment analysis includes data
analysis of the body of the the text for
understanding the opinion expressed by
it and other key factors comprising
modality and mood usually the process of
sentiment analysis works best on text
that has a subjective context than on
that with only an objective context
e-commerce
personalization text mining is used to
suggest products that fit into a user's
profile text mining is increasingly
being used by e-commerce retailers to
learn more about the consumers as it is
the process of analyzing textual
information in order to identify
patterns and gain insights e-commerce
retailers can Target specific
individuals or segments with
personalized offers and discounts to
boost sales and increase Customer
Loyalty by identifying customer purchase
patterns and opinions on particular
products let's look at natural language
toolkit library in
detail natural language toolkit is a set
of open-source python models that are
used to apply statistical natural
language processing on human language
data let's see how you can do
environment setup of
nltk go to Windows start and launch
python interpreter from Anaconda prompt
and enter the following commands enter
command python to check the version of
python installed on your system enter
import nltk to link you to the nltk
library available to download then enter
nltk doownload function that will open
the nltk download window check the
download directory select all packages
and click on download this will download
nltk onto your python once you have
downloaded the nltk you must check the
working and functionality of it in order
to test the setup enter the following
command in Python idle from nltk Doc
Corpus import Brown brown. word
parenthesis
parenthesis the brown is an nltk Corpus
that shows the systematic difference
between different genres available words
function will give you the list
available words in the genre the given
output shows that we have successfully
tested the nltk installed on
python let's Now understand how you can
read a specific module from nltk corpora
if you want to import an entire module
from nltk corpora use asterisk symbol
with that module named import command
enter the command from nltk book import
asterisk it will load all the items
available in NLT K's book module now in
order to explore Brown Corpus enter the
command nltk doc Corpus import Brown
this this will import Brown Corpus on
the python enter brown. categories
function to load the different genres
available select a genre and assign that
genre to a variable using the following
syntax variable name is equal to brown.
wordss categories is equal to genre name
now in order to see the available words
inside the selected genre just enter the
defined variable name as a
command let's understand text extraction
and pre-processing in
detail so let's first understand the
concept of
tokenization tokenization is the process
of removing sensitive data and placing
unique symbols of identification in that
place in order to retain all the
essential information concerned with the
data by its security it is a process of
breaking running streams of text into
words and sentences it works by
segregating words using punctuation and
spaces text extraction and
pre-processing
engrams now let's look at what engram is
and how it is helpful in text mining
engram is the simplest model that
assigns these probabilities to sequences
of words or sentences engrams are
combinations of adjacent words or
letters of length and in the source text
so engram is very helpful in text mining
when it is re ire to extract patterns
from the text as in the given example
this is a sentence all of these words
are considered individual words and thus
represent unigrams a2r or Byram is a
two-word sequence of words like this is
is a or a sentence and a thre or trigram
is a three-word sequence of words like
this is a or is a
sentence let's now understand what stop
words are and how you can remove
them stop words are natural language
words that have negligible meaning such
as a n and or the and other similar
words these words also will take up
space in the database or increase the
processing time so it is better to
remove such words by storing a list of
stop words you can find the list of stop
words in the nltk data directory that is
stored in six different languages use
the following command to list the stop
words of English language defined in
nltk Corpus importing nltk will import
the nltk Corpus for that instance enter
from nltk Corpus import Stop wordss will
import Stop wordss from nltk Corpus Now
set the language as English so use set
function as set under braces stop
wordswords set genre as English stop
words are filtered out before processing
of natural language data as they don't
reveal much information so as you can
see in the given example before
filtering the sentence the tokenization
of stop word is processed in order to
remove these stop words and the
filtering is applied in order to filter
the sentence based on some
criteria text extraction and
pre-processing
stemming stemming is used to reduce a
word to stem or base word by removing
suffixes such as helps helping helped
and helper to the root word help the
stemming process or algorithm is
generally called a stemmer there are
various stemming algorithms such as pter
stemmer Lancaster stemmer snowball
stemmer Etc use any of the stemmers
defined under nltk stem Corpus in order
to perform stemming as shown in the
example here we have used Porter stemmer
When You observe the output you will see
that all of the words given have been
reduced to their root word or
stem text extraction and pre-processing
lemmatization lemmatization is the
method of grouping the various inflected
types of a word in order that they can
be analyzed as one item it uses
vocabulary list or a morphological
analysis to get the root word it uses
word net database that has English words
linked together by their semantic
relationship as you can observe the
given example the different words have
been extracted to their relevant
morphological word using
lemmatization text extraction and
pre-processing POS
tagging let's now look at different part
of speech tags available in the national
language toolkit Library a POS tag is a
special label assigned to each token or
word in a text Cor purpose to indicate
the part of speech and often also other
grammatical categories such as tense
number either plural or singular case
Etc POS tags are used in text analysis
tools and algorithms and also in Corpus
searches so look at the given example
here Alice wrote a program is the source
text given the POS tags given are Alice
is a noun wrote is a verb a is an
article and program is an adjective look
at the given example to understand how
POS tags are defined so the given
sentence or paragraph contains different
words that represent different parts of
speech we will first use tokenization
and removal of stop words and then
allocate the different POS tags these
are shown with different words in the
given sentence POS tags are useful for
lemmatization in building named entity
recognition and extracting relationships
between words
text extraction and pre-processing named
entity recognition now let's understand
what named entity recognition is all
about Neer seeks to extract a real world
entity from the text and sorts it into
predefined categories such as names of
people organizations locations Etc many
realworld questions can be answered with
the help of name entity recognition were
specified products mentioned in
complaints or reviews does the Tweet
contain the name of a person does the
Tweet contain the person's address as
you can see in the given example Google
America Larry Page Etc are the names of
a person place or an organization so
these are considered named entities and
have different tags such as person
organization gpe or geopolitical entity
Etc NLP process
workflow now you have an understanding
of all nltk tools so now let's
understand the natural language
processing workflow step one
tokenization it splits text into pieces
tokens or words and removes punctuation
step two stop word removal it removes
commonly used words such as the is R Etc
which are not relevant to the analysis
step three stemming and litiz it reduces
words to base form in order to be
analyzed as a single item step four POS
tagging it tags words to be part of
speech such as noun verb adjective Etc
based on the definition and context step
five information retrieval it extracts
relevant information from the
source M1 Brown Corpus problem statement
the Brown University standard Corpus of
present day American English also known
popularly as brown Corpus was compiled
in the 1960s as a general Corpus in the
field of Corpus Linguistics it contains
500 samples of English language text
totaling roughly 1 million words
compiled from Works published in the
United States in
1961 we will be working on one of the
subset data set and perform text
processing tasks let us import the nltk
library and read the
ca10 corpus import
nltk we will have to make sure that
there are no slashes in between hence we
will use the replace function within
pandas for the
same let's have a look at the data
once
tokenization after performing sentence
tokenization on the data we
obtain similarly after applying sentence
tokenizer the resulting output shows all
individual words
tokens
stop word removal let's import the stop
word library from nltk Doc Corpus import
stopwords we also need to ensure that
the text is in the same case nltk has
its own list of stop wordss we can check
the list of stop wordss using stopwords
do wordss and English inside the
parenthesis map the lowercase string
with our list of word
tokens let's remove the stop wordss
using the English stop wordss list in
nltk we will be using set checking as it
is faster in Python than a
list by removing all stop words from the
text we
obtain often we want to remove the
punctuations from the documents too
since python comes with batteries
included we have string. punctuation
from string import
punctuation combining the punctuation
with the stop wordss from
nltk
removing stop wordss with
punctuation stemming and limit ization
we will be using stemming and
lemmatization to reduce words to their
root form for example walks walking
walked will be reduced to their root
word walk importing Porter stemmer as
the stemming library from nltk do stem
import Porter
stemmer printing the stem
words
import the wordnet litier from nltk do
stem printing the root
words
we also need to evaluate the POS tags
for each
token create a new word list and store
the list of word tokens against each of
the sentence tokens in data 2 for OD and
tokenized also we will check if there
were any stop words in the recently
created word
list we will now tag the word tokens
accordingly using the POS tags and print
the tag T
output for our final text processing
task we will be applying named entity
recognition to classify named entities
in text into predefined categories such
as the names of persons organizations
locations expressions of times
quantities monetary values percentages
Etc
back
now press the tagged sentences under the
chunk parser if we set the parameter
binary equals true then named entities
are just tagged as NE otherwise the
classifier adds category labels such as
person organization and
gpe create a function named Nam as
extract entity names along with an empty
list named as entity
names we will now extract named entities
from a nltk chunked expression and store
them in the empty created
above
again we will set the entity names list
as an empty list and we extract The
Entity names by iterating over each tree
in chunked
sentences great we have seen how to
explore and examine the Corpus using
text processing techniques let's quickly
recap the steps we've covered so far One
Import the nltk library two perform
tokenization three perform stemming and
lemmatization four remove stop words
five perform named entity
recognition structuring sentences
syntax let's first understand what
syntax is syntax is the grammatical
structure of sentences in the given
example this can be interpreted as
syntax and it is similar to the ones you
use while writing codes knowing a
language includes the power to construct
phrases and sentences out of morphemes
and words the part of the grammar that
represents a speaker's knowledge of
these structures and their formation is
called syntax phrase structure rules are
rules that determine what goes into a
phrase that is constituents of a phrase
and how the constituents are ordered
constituent is a word or group of words
that operate as a unit and can be used
to frame larger grammatical units the
given diagram represents that a noun
phrase is determined when a noun is
combined with a determiner and the
determiner can be optional a sentence is
determined when a noun phrase is
combined with a verb phrase a verb
phrase is determined when a verb is
combined optionally with the noun phrase
and prepositional phrase and a
prepositional phrase is determined when
a preposition is combined with a noun
phrase a tree is a representation of
syntactics structure of formulation of
sentences or strings consider the given
sentence the factory employs 12.8% of
Bradford County what can be the Syntax
for pairing this statement let's
understand this a tree is produced that
might help you understand that the
subject of the sentence is the factory
the predicate is employe and the target
is
12.8% which in turn is modified by
Bradford County syntax parses are often
a first step toward deep information
extraction or semantic understanding of
text rendering syntax
trees download the corresponding exe
file to install the ghost script
rendering engine based on your system
configuration a in order to render
syntax trees in your notebook let's
understand how you can set up the
environment variable once you have
downloaded and installed the file go to
the folder where it is installed and
copy the path of the file now go to
system properties and under Advanced
properties you will find the environment
variable button click on that to open
the popup box tab of the environment now
open the bin folder and add the path to
the bin folder in your environment
variables now you will have to modify
the path of the environment variable use
the given code to test the working of
syntax tree after the setup is
successfully
installed structuring sentences chunking
and chunk
parsing the process of extraction of
phrases from unstructured text is called
chunking instead of using just simple
tokens which may not represent the
actual meaning of the text it is
advisable to use phrases such as Indian
team as a single word instead of Indian
and team as separate words the chunking
segmentation refers to identifying
tokens and labeling refers to
identifying the correct tag these chunks
correspond to mixed patterns in some way
to extract patterns from chunks we need
chunk parsing the chunk parsing segment
refers to identifying strings of tokens
and labeling refers to identifying the
correct chunk type let's look at the
given example you can see here that
yellow is an adjective dog is a noun and
the' is the determiner which are chunked
together into a noun phrase similarly
chunk parsing is used to extract
patterns and to process such patterns
from multiple chunks while using
different
parsers let's take an example and try to
understand how chunking is performed in
Python let's consider the sentence the
little Mouse at the fresh cheese
assigned to a variable named scent using
the word tokenize function under nltk
corpora you can find out the different
tags associated with the sentence
provided so as you can see in the output
different tags have been allocated
against each of the words from the given
sentence using
chunking NP chunk and
parser you will now create grammar from
a noun phrase and will mention the tags
you want in your chunk phrase within the
function here you have created a regular
expression matching the string the given
regular expression indicates optional
determiner followed by optional number
of adjective followed by a noun you will
now have to parse the chunk therefore
you will create a chunk parser and pass
your noun phrase string to it the parser
is now ready you will use the parse
parenthesis parenthesis within your
chunk parser to parse your sentence the
sentence provided is the little mouse at
the fresh cheese this sentence has been
parsed and the tokens that match the
regular expressions are chunked together
into noun phrases
NP create a verb phrase chunk using
regular Expressions the regular
expression has been defined as optional
personal pronoun followed by zero or
more verbs with any of its type followed
by any type of adverb you'll now create
another chunk parser and pass the verb
phrase string to it create another
sentence and tokenize it add POS tags to
it so the new sentence is she is walking
quickly to the mall and the POS tag has
been allocated from nltk corpora now use
the new verb phrase parser to parse the
tokens and run the results you can look
at the given tree diagram which shows a
verb parser where a pronoun followed by
two verbs and an adverb are chunked
together into a verb
parse structuring sentences
chinking chinking is the process of
removing a sequence of tokens from a
chunk how does chunking work the whole
chunk is removed when the sequence of
tokens spans an entire chunk if the
sequence is at the start or the end of
the chunk the tokens are removed from
the start and end and a smaller chunk is
retained if the sequence of tokens
appears in the middle of the chunk these
tokens are removed leaving two chunks
where there was only one before consider
you create a chinking grammar string
containing three things chunk name the
regular expression sequence of a chunk
the regular expression sequence of your
 here in the given code we have the
chunk regular expression as optional
personal pronoun followed by zero or
more occurrences of any type of the verb
type followed by zero or more
occurrences of any of the adverb types
the regular expression says that
it needs to check for the adverb in the
extracted chunk and remove it from the
chunk inside the chinking block with
open curly braces and closing curly
braces you have created one or more
adverbs you will now create a parser
from nltk do
regex parser and pass the grammar
to it now use the new parser to
par the tokens sent three and run the
results as you can see the parse tree is
generated while comparing the syntax
tree of the parser with that of
the original chunk you can see that the
token is quickly adverb chined out of
the chunk let's understand how to use
context free
grammar a context free grammar is a four
Tuple sum ntrs where sum is an alphabet
and each character in sum is called a
ter terminal NT is a set and each
element in NT is called a non-terminal r
the set of rules is a subset of NT times
the set of sum U NT s the start symbol
is one of the symbols in NT a context
free grammar generates a language L
capturing constituency and ordering in
CFG the start symbol is used to derive
the string you can derive the string by
repeatedly replacing a non terminal on
the right hand side of the production
until all non-terminals have been
replaced by terminal symbols let's
understand the representation of context
free grammar through an example in
context free grammar a sentence can be
represented as a noun phrase followed by
a verb phrase noun phrase can be a
determiner nominal a nominal can be a
noun VP represents the verb phrase a can
be called a determiner flight can be
called a noun
consider the string below where you have
certain rules when you look at the given
context free grammar a sentence should
have a noun phrase followed by a verb
phrase a verb phrase is a verb followed
by a noun a verb can either be S or met
noun phrases can either be John or Jim
and a noun can either be a dog or a cat
check the possible list of sentences
that can be generated using the rules
use the join function to create the
possible list of sentences you can check
the different rules of grammar for
sentence formation using the production
function it will show you the different
tags used and the defined context free
grammar for the given
sentence demo two structuring sentences
problem statement a company wants to
perform text analysis for one of its
data sets you are provided with this
data set named tweets. CSV which has
tweets of six US Airlines along with
their sentiments positive positive
negative and neutral the tweets are
present in the text column and
sentiments in Airline sentiment column
we will be retrieving all tags starting
with at the rate in the data set and
save the output in a file called
references. txt let us first import the
panda library and read the tweets data
set
extract the features text and Airline
sentiment we will iterate through the
data set using reg X find the relevant
tweets
now we will import the iter tools module
it returns efficient
iterators
the result is stored in a file named
references.
txt
let's extract all noun phrases and save
them in a file named noun phrases for
left Carro Airline sentiment rightcar
review.
txt
here left carat Airline uncore sentiment
right carrat has three different values
positive negative and neutral so three
files will be
created
now we will iterate all the leaf nodes
and assign them to noun phrases
variable
this means that the functions in iter
tools operate on iterators to produce
more complex
iterators using the map function we will
get all the noun phrases from the
text putting it into
list
creating a file name in the name of
review.
txt
great we have now seen how to explore
and examine the Corpus using text
processing techniques if you're an
expiring AIML engineer then there is no
better time to train yourself in
exciting field of machine learning if
you are looking for a course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than our Cel
Tech in partnership with IBM so why wait
join now seats are filling fast find the
course link from the description box
below so first we will import some major
libraries of python so here I will write
import pandas as
PD
import
numpy as
NP then
import
cbor as
SNS okay then
import
skarn do model
selection ort
train
underscore testore
split before that I will
import M plot
Li P
plot as
PLT okay
then I will write here from skon
dot
matrix
import
accuracy
four then
from
Escalon do
Matrix
import
classification to report
and import R then import
string
okay then press enter so it is
saying okay here I have to write
from everything seems
good loading let's
see okay till then numai is a python
Library used for working with arrays it
also has function for working with the
domain of linear algebra and
matrices it is an open source project
and you can use it
freely number stand for numerical
python pandas so panda is a software
Library written for Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operation for manipulating
numerical tables and time
series then cbor an open source python
Library based on matplot lib is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbone functions
with ease then M PL lip for Python and
its numerical extension numpy met plot
Li is a cross platform for the data
visualization and graphical charting
package
as a result it presents a strong open
source suitable for metlab the apis for
met plot Li allow programmers to
incorporate graphs into gii applications
then this train test split we may build
our training data and the test data with
the aid of Escalon train test split
function this is so because the original
data set often serves as both the
training data and the test data starting
with the single data set we divide it
into two data sets to obtain the
information needed to create a model
like hor and test accuracy score the
accuracy score is used to gge the
model's Effectiveness by calculating the
ratio of total true positive to Total
true negative across all the model
prediction this R regular expression the
functions in the model allow you to
determine whether a given text fits a
given regular expression or not which is
known as
re okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating
data the Str Str class is a built-in
string class in Python because python
strings are immutable they cannot be
modified after they have been
formed okay so now let's import the data
set we will be going to import two data
set one for the fake news and one for
the True News or you can say not fake
news
okay so I will write here
efcore pi
to PD
do read undor
CSV or what can I say DF fake
okay _
fake okay
then fake do CSV you can download this
data set from the description box below
then data dot true equals to pd. read
underscore
CSV sorry
CSV then take news sorry true true.
CSV
okay then press
enter so these are the two data set you
can download these data set from the
description box below so let's see the
board data set okay then I will write
here data underscore
fake do
head so this is the fake data okay
then
data underscore
true
Dot and this is the two
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here data let's go
fake
cl equals to
Z then data underscore
true and cl
= to
1
okay
then I will write here data underscore
fake dot shape and data underscore
true do
shap okay then press
enter so the shape method return the
shape of an array the shape is a tle of
integers these number represent the
length of the corresponding area
dimension in other words a tle
containing the quantities of entries on
each axis is an array shape Dimension so
what's the meaning of
shape in the fake word in this data set
we have 2 3 4 8 1 rows and five columns
and in this data set true we have 2 1 41
7even rows and five column okay so these
are the rows column rows column for the
particular data set
so now let's move and let's remove the
last 10 rows for the manual testing okay
then I will write here data underscore
fake let's go
manual
testing equals to dataor
fake do
tail what the last 10 rows I have to
write here
10 okay so for
I in
range 2 3
4 8 1 sorry
0o comma 2
3
470 comma
-1 okay
then DF underscore not DF
data underscore
fake dot
drop
one instead of one I can write here
I
comma
equals
to0
Place equals to
true then
data not
here data
underscore same I will write for I will
copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true where I can write
true
okay then I have to change your
[Music]
number 2
1
416
write 2 1 40
6 -
1
same so press
enter x =
0 Let's in X maybe you mean d0 or of
this okay we will put here double
course I'm putting
this
f. drop I is z in place
okay also write equals to
equals yeah
so okay XIs is not
defined now it's working
so let me see
now
data
underscore pi.
shape
okay and data dot
true and data underscore
true.
shape as you can
see 10 rows are deleted from each data
set
yeah so I will write here data uncore
fake underscore
manual
testing
class = to
Z and data
underscore true
_ manual _
testing plus equals
to
1
okay just ignore this
warning then let's
see data
underscore
bore manual
testing.
head as you can see we have this and
then data dot sorry underscore
truecore
manual
testing dot
at
10
okay
this is this is the uh true data
set so here I will merge data let us go
merge
to PD
dot
concat concat is used for the
concatination
dataor
fake
data
underscore comma
XIs = to
Z then data uncore
merge do
head the top 10
rows
yeah as you can see the data is merged
here okay first it will come for the
fake news and then be the for the True
News then let's merge true and fake data
frames
okay we did this
and let's Mery column then data do
merge Dot
columns or let's see the
columns it has not defined what data
underscore
much these are the columns name title
Tex subject date class
okay
now let's remove those columns which are
not required for the further process so
here I will write write data
underscore or equals to data underscore
merge
prop title we don't
need
then subject we don't need
then
one so let's check some null
values it's giving
error of
this that's good then
data dot
isnull
sum
Center so no null values okay then let's
do the random shuffling of the data
frames okay for that we have to write
here data equals to data do
sample
one
then data okay
data do
head okay now you can see here the
random shuffling is
done and one for the true data set and
zero for the fake news one
okay then let me write here data Dot
reset underscore
index
Place equals to
True data dot
drop comma X is = to 1
then comma in
place equals to
True
okay then let me see columns now data
dot
columns so here we have two columns only
rest we have deleted
okay
so let me see data Dot
right yeah everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write
[Music]
here
what okay you can use any name
text and text equals
to text do
lower okay and
textt to r dot for the
substring remove these
things uh from the
datas okay so for that I'm writing
herea okay then text equals to are dot
substring
comma comma
text okay then I have to write text
equals
to I do
substring
ww
dot
s+
comma comma
text
okay then text equals
to I
substring
comma
okay then text equals to R do
substring
then
percentage
s again percentage or for r. SK
function right here
string do
punctuation
comma then comma then
text
right then text equals to R do
substring and
N
comma text equals to r
dot
substring write
here
and again
D then
again
okay then
comma then again
texture okay then at the end after WR
here return text so everything like uh
this this type of special character will
be removed from the data set okay let's
run this let's
see
yeah so here I will addite DF sorry not
DF
data
data
then
texts to
data dot
apply to the function name wordp what
opt okay
press enter yeah so now let's uh Define
the dependent and independent variables
okay x equals to
data
text and Y =
to
data
class okay then splitting training and
testing
data okay sorry so here I will write X
underscore
train comma xcore
test uh then Yore
train comma Yore test equals to train
underscore testore
split then X comma
y comma
test let's go size is equals to
0.25 okay press
enter so now let's convert X to vectors
for that I have to write
here Z it's
X so here I will write from
Escalon dot
feature extraction
dot text
import T
vectorizer
okay then
vectorization equals to T
fid
vectorizer okay
then
three
underscore
train equals
to
vectorization vectorization do
fit then
transform
xcore
train okay then X Vore test equals
to
factorization dot
transform xort test okay then press
enter uh
so now let's see our first model
logistic
regression so here I will write
from Escalon
dot linear uncore
model OKAY
import
logistic okay then lot equals
to
logistic
regression and have to write here LR
dot
fit then XV
dot not DOT train comma X Vore
test okay let
enter
xv. okay here I have to write y
train okay and press
enter work so here I will write
prediction
underscore linear regression
equal
to l r.
predict Vore
test okay let's see the accuracy
score for that I have to write LR DOT
score then XV underscore
test comma
Yore test
okay let's see the accuracy so here as
you can see accuracy is quite good
98% now let's
print the
classification
Port Yore test
comma prediction of linear regression
okay so this is you can see Precision
score then F1 score then support value
accuracy okay so now we will uh do this
same for the decision free gradient
boosting classifier random Forest
classifier okay then we will do model
testing then we will predict this
school okay so now for the decision tree
classification so for that I have to
import from
skon dot
Tre
import
decision
three
classifier okay then at the short form I
will write here I will copy it from
here
then
okay then I have to write the same as
this so I will copy it from
here
and
yeah let's change linear
regression
to season three
classific
okay then I will write here
same go
dtal to tt.
predict X
Vore
test still loading it's it will take
time
okay till then let me write here for the
accuracy it do
score
Vore test comma
y
okay let's wait
okay run
the accuracy so as you can see accuracy
is good than this linear regression okay
logistic
regression okay so let
me show you the let me
predict
print so this is the accuracy score this
is the all the
report
yeah now let's move for the uh gradient
boosting classifier okay for that I
write from
Escalon dot
assemble
port
radiant
boosting
classifier
pacifier I will write here
GB equals to let me copy it from
here
I will give here random let go
State equals to
Z wait wait wait wait so I will write
here GB dot
fit XV underscore train comma Yore train
okay then press
enter here I will write predict
underscore
GBS to GB do fit sorry
redit
three DOT
test doore
test till then it's loading so I will
write here uh for the score then I will
add GB do
score then Vore test
comma Yore test okay so let's wait it is
running this
part till then let me write for the
printing
this
okay it's taking
time taking time still taking
time what if I will run
this it's not coming because of
this yeah it's done now so you can see
the accuracy
is uh not good than
decision tree but yeah it is also good
99.4 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from Escalon
dot
symble
import
random
Forest
classifier
okay and here I will write
RF
to right I will copy it from
here
then then random
State equals
to Z
then RF dot
fit Vore
train comma Yore
train okay then press
enter and predict
underscore
RC
RF equals
to RF do
predict Vore test
okay till then I will write here it's
still loading it will take time so till
then I will write for this score score
accuracy
score XV test comma Yore
test okay then I will write here till
then
print
classification
port and Yore
test
comma
will take time little
bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you but after writing the code
so so I have made two functions one for
the output label and one for the manual
testing Okay so it will predict the all
the from the all models from
the repeat so it will
predict the it the news is fake or not
from all the models okay so for that let
me write here
newss to
string
okay then I will write here manual
underscore
testing
so here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the internet okay from
the Google the news which is not fake
okay I'm adding which is not fake
because I already know I searched on
Google so I'm entering this so just run
it let's see what is
showing okay string input object is not
callable okay let me check this
first okay I have to give here s Str
only yeah let's
check okay I have to add here again the
script yeah manual testing is not
defined let me see manual
testing okay I have to edit some
something it is just GB and it is just
RF GBC is not defined okay okay so what
I have to do have to remote
this
this okay everything seems
sorted
now
as I said to you I just copied this news
from the internet I already know the
news is not fake so it is showing not a
fake news okay so now what I will do I
will
copy one fake news from the
internet and let's see it is detecting
it or not okay
so let me run
this and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand till
here so I hope you guys must have
understand how to detect a fake news
using machine learning you can you can
copy any news from the internet and you
can
check what is time series
forecasting making scientific projection
based on the data with historical time
stamps is known as time series
forecasting it entails creating model
through historical study using them to
draw conclusion and guide strategic
decision making in the
future the fact that the future result
is wholly unknown at the time of of the
task and can only be anticipated through
analysis and evidence-based priers is an
essential distinction in forecasting
give yourself a chance to Simply lar
professional certificate program in Ai
and machine learning which comes with
completion certificate and in-depth
knowledge of AI and machine learning
check this course detail from the
description box
below so here is one question for you
guys I will give you exactly one minute
for this you can comment or you can give
your answer in the chat section so I can
see if the answers given by you are
right or wrong okay so the question is
which type of programming does python
support I'm repeating again which type
of programming does python support
option A objectoriented programming
option b structured programming option C
functional programming and option D all
of the
above oh let us know in your answer in
the chat section or in the comment
section so I'm starting a timer of of 1
minute just type your answer in the
comment section or in the chat section
do let me know your answers please so
I'm starting the timer of 1
minute which type of programming does
python support object oriented structure
programming functional or all of the
above do let me know your answers
please you can comment or you can give
your answer in the chat section so I can
see if the ansers given by you are right
or wrong
which type of programming does python
support objectoriented structured
functional or all of the
above 30 seconds
meaning which type of programming does
python support object oriented
structured function or all of the above
let us know your answers in the chat
section or in the comment section
below and 10 seconds more which type of
programming does python support 5
Seconds
more so the alerted time is over we will
give a reply to those who gave the
correct answer and for those who didn't
give the correct answer we will give you
reply with the correct answer okay now
let's move to our programming
part so we will open command prom to
write a command to open jupyter notebook
so here I will write
Jupiter
notebook let okay
Jupiter notebook press
enter we take
time
open so this is the landing page of
jupyter notebook and here I will select
new python kernel file
so this is how Jupiter Kel look lies so
here what we will do we will import some
major libraries of python which will
help us in analyzing the data okay
import
numai s
NP okay then
import
pandas
ASD
then
import C
born as
SNS fourth one is
from mat plot
Li
Port P
plot s
PLT okay then we will import some model
libraries so here I will addite
from
stats
models
TSA do
API
import
exponential
muting and comma
then
simple XP
smoothing then one
more
hold we will write here import sorry
from let kear do
linear underscore
model
import
linear
regression okay then
import
warnings
warnings then we will write here
warnings
dot filter
warnings it should be
ignore yes there will be no error it's
still loading yeah so numai numai is a
python Library used for working with
arrays it also has a function for
working with the domain of linear
algebra and matrices it it is an open
source project and you can use it freely
numai stand for numerical python second
is Panda pandas is a software Library
written for the Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operations for
manipulating numerical data and time
cies then cbone an open- Source python
Library based on met plot lab is called
cbon it is utilized for data exploration
and data visualization
with data frames and the Panda's labr
cbone function with
ease M plot lip for Python and its
numerical extension numai M PL lib is a
crossplatform data visualization and
graphical charting package as a result
it presents a strong open source
substitute for
MPL the apis for MPL lib allow
programmers to incorporate graphs into
GUI
applications linear regression the
machine learning method regression built
on linear supervised learning analysis
regression is done regression creates a
value for the aim prediction using
independent variables as inputs it main
goal is to investigate the relationship
between factors and
forecasting exponential
smoothing the exponential Windows
function is a general method for
smoothing time series data known as
exponential smoothing it contrast to the
ordinary moving average which weights
previous data
quality exponential function use weights
that decrease exponentially with time
and there is one more simple exponential
smoothing the simple exponential
smoothing classes use Simple exponential
smoothing models to give very simple
time Ser
analysis a weight average of the most
recent value and the preceding smooth
value constitute the predicted value the
contribution of old value degrade
exponentially as a result of this
smoothing
parameter okay so after importing
libraries let's import data set for this
I will write here
DF or you can write data frame PD
dot
seore
CSV here I will write
monthly uncore
CSV Dot
CSV okay this is my file name you can
download this file from the description
box
below and for seeing the data I will
write here DF dot
head and then press
enter yeah so here PD is for Panda's
Library read is used for reading the
data set from the machine and CSV is
used for the type of file which you are
using okay or which you want to read and
if if you want to see the top five rows
of your data set you can use head and if
you want to see the last five rows of
your data set you can use tail instead
of head okay this one you can write here
tail so moving forward let's see how
many rows and columns are present in our
data set for that I have to write DF dot
shape okay then press enter okay it will
give error why
because this yeah
so here you can see 847 rows and two
columns date and price
only so moving forward let's do some Eda
explorat data analysis okay for that I
have to write here
print
data date
range of
of gold
prices available
from then here I have to give qu
brackets then DF dot location
Lo then
colon
comma date
okay D is capital here so I have to
write
date then from location
zero
2 again same thing DF
Dot
looc
date to the length
of
EF minus
one oh everything seems good yeah then
press
enter
okay we have dat range of gold prices
available from this 1950 to 2020 okay in
our data
set then here I will write
date equals to PD
do
date
range start
from slash one SL
1950
comma and equals
to 8 SL
1/2020 comma
frequency equals to
median okay
then then here I will write
date then press enter
yeah so here you can see T time index
okay from starting till end these dot
dots and here I will
write
DF
month equals
to
date have dot
drop
State Comm XIs is
one comma in
place to
True T should be
capital okay then DF equals to
DF dot
set IND
X and
month and
DF dot
head let
enter so instead of this
date we have adjusted this
month okay for particular
value so moving forward let's see the
graph different different graphs okay DF
do
plot then figure set should
be
to 20 comma
8 then
plot dot
title
is like
gold
prices
monthly
since
1950 and
onwards okay title should be this then X
level should be PLT do X
label will be months
then PLT do y
labels should be
price then I will write here PLT dot
grid okay press
enter okay title spelling my bad tle
okay then press enter
oh why label it is
sorry
yeah here you can see gold prices
monthly since 1950 and
onwards okay till from 1950 and from
till 2020 okay this is the
price then moving forward let's see
another graph so I will write here for
that round
DF dot
describe comma
3 and here you can see the count
variable and the average is
46.57 standard deviation that minimum
price value is this 20% this and the
maximum value is this okay
so
so the average gold price in last 70
years is this 41 16.55 7
okay only 25% of the time the gold price
is above
447 the highest gold price ever touch
this this one 1840
807 so we will do visual anal analysis
so here I will
write ax equals to PLT do
subplots
subplots okay then figure
size to
25 comma
art okay then SNS dot box
plot is
xals
to DF do
index dot ear comma y equals to DF do
values column comma
0a ax = to
X
okay then same PLT do
title gold
price
monthly
1950 same graph will come but in the
different format
okay let me remove
this then PLT dot X
label label must be
here LT dot y
lbel
ice and PLT
dot x
s will be
rotation to okay I will give rotation
90 PLT dot
grid can write here instead of grid I
can write here directly show
so grid is this format this box format
okay so press
enter it's loading yeah so here you can
see from 1950 every year is here till
2020 so how the gold prices are
decreasing and increasing
okay let's see the another
graph
I will write here
PR that'ss
models
models dot
Graphics
dot TSA
plots
for month
plot
okay then I will write here figure comma
ax equals to then PLT
dot
subplots figer
size equals to 22 comma
8 okay then M
plot DF comma y
label to
gold
price okay comma then ax equals to a
x i will give the title PLT
dot
title I will copy from here
and PLT do X
label
month and PLT dot y
label the
price
PLT do
git so here you can see gold prices
monthly since
1950 like for every month like January
February March April so on till December
okay we will cover one more
graph okay and many many more
graphs
so so we
will go with the next
graph for that I will write
here let's
go comma
ax equals to PLT Dot
subplots and figure size
=
to 22 comma
8 then same
SNS dot
workot x equals
to DF do
index do month
name
okay then y =
to DF do
values comma
0 comma ax = to
ax okay then
pt.
title we will put put same title so I
can copy from it
here let's copy from here and paste it
here
okay and PLT
dot X
label and PLT do y lbel
price and PLT do
GD okay pl.
show this time let's not use
grid
Okay okay something PS month
name
yes you can
see for every month this is another type
of
graph okay box plot graph so why we are
creating so much graphs because we are
doing Eda exploratory data analysis in
this we have to see the multiple
different and different different types
of
graph okay so moving forward let's see
average gold price per year like Trend
since
1950 so for that I have to write here
DF let's go
yearly let's go
sums to DF do
resample
sample then
a dot
mean DF
underscore
yearly underscore
sum do
plot PLT do
title and here I will write
average gold
price
yearly
since
1950 or you can write onwards from
onwards
1950 so here I will write PLT do X
label
here and PLT do y
label
ice okay then PLT do grid this time
we'll use
grid so here you can see the average
gold price yearly since 1950 okay this
is the chart till 2020 like sometimes up
sometimes down
okay and we will see now like average
gold price per quarter like trends like
since
1950 so here I will write DF
dot not DOT DF dot
quarterly okay underscore sum equals
to DF do
resample
water
[Music]
Q dot
mean
okay
okay move
this then DF
underscore
quarterly go
sum do
plot
and the same PLT do
title from
here here I will write average good
price
quarterly
quarterly okay since
1950 then PLT do X
label label is now here
quarter PLT do y
label is
price okay now PLT do
should
this
time
okay efcore
quarterly okay
quarterly yeah here you can see the
price prediction okay let me set to the
grid
only it's not
this visuals are not good so
crit yeah so here you can see the
quarterly prices
prediction okay average gold price
quarterly
prediction so like moving
forward we will see now average gold
price per decade like per 10 years okay
so from 1950 only so here I will write
DF underscore
DEET underscore sum
= to DF do
resample every 10 year okay 10
year do
mean we are writing mean because we are
like putting out
average okay so dfdore deet
underscore
sum do
plot PLT do
title
average
goal
price per
decade
since
1950
okay since 1950 yeah
perfect so here I will get PLT dot X
label
then decade decade is of like every 10
year PLT do y
label and PLT do
grid
yeah so here you can see the average
gold price per decade like from 1950 to
1960 like this is straight then again
straight then up then sometimes down
then up and down okay every 10 years you
can see 19902 2010 and
2020 so moving forward let's do like
analysis in coefficient of variation the
coefficient of variation CV is a
statical measure of the relative
dispersion of data points in a data
series around the
mean and in finance the coefficient of
variation allows investor to determine
how much verality or risk is assumed in
comparison of the amount like amount to
the return expected from
investors okay the lower the ratio of
the standard deviation to mean return
the better risk return
tradeoff okay let's like let us look now
the CV values for each year in gold
prices
so CV means coefficient of variation in
prices okay so here I will write DF _ 1
equals to DF dot Group
by do
index
do do
mean
name dot
columns equals
to I WR
price and
mean okay then again
df1 equals to
df1 dot range
or what we can do instead of range we
can write
merch tf. Group
by go DF do
index do
EO standard
deviation
Dame
columns equals
to
price standard
deviation comma
left
index to
true then comma right
index Al so
true
okay and here DF uncore
1
SC plus
to
efcore
1 standard
deviation SL
df1 to
100 like Dot
round figure should be like two after
decimal how much like numbers you want
to
see then
df1 do
head enter yeah so here you can see for
every year I have mean standard
deviation and this coefficient of
variation okay for every year
so like moving forward let's see the
average gold price per year
again so for that
figure
underscore figure dot comma
ax equals to
PLT dot
subplot bigger
size 15
comma
10
okay
then I will write here
df1
is
coore Dot
Plot PLT do
title average
gold price
early
since
1950 okay then PLT do X
label is
here uh
PLT do y
label y
label
coefficient of variation I'm writing
CV in
percent okay then PLT do
show get DF uncore invalid
syntax what C
Capital do
plot
okay so df1
df1
here
syntax
okay so here you can see average gold
price since
1950
okay this is like percentage CV in
percent
okay like
good you can say the
chart so the CV value deed its highest
in 9 1978 like somewhere here like 198
1978 okay like near to 25% which could
have made the asset as a highly risky
but in 2020 the CV value is closer to 5%
which makes the asset variable by good
investment okay so now what we will do
we will do time series
forcasting okay we will train model we
will build model different different
model we will take train and test split
to build time series forecasting model
so for that let me do like this
first
yeah so here I will write
train equals to
DF DF do
index dot per
year okay equals to
2015 and for the testing
we will write
DF DF do
index.
2015 okay for training we are taking
till 2015 for and for the testing we are
taking till 2020 from 2015
okay then I will write here how many
columns present in train test so for
that I will write print
train do
shape and
print test do shift press enter 792 rows
and one column in train training for
training the model and 55 to test the
model
okay so now let's see the training data
and testing data so
train like square brackets so
price do
plot then figure
size equals
to 13 comma 5
and font size should
be to
15
test
I Dot
Plot bigger
size bigger
size
let me
give same 13 comma
5 and font size should
be okay then PLT dot let me add
grid PLT do
gr then
PLT Dot
Legend
Training
data
comma test
data okay then
PLT show I will tell you what the legend
is
okay so here you you can see the
training data in blue and the testing
data okay this is known as the legend
this this
portion and yeah so here you can see
month-wise
okay till
2020 and from
1950 right these are the
prices and this is the chart so moving
forward let's do model formation now
okay we will do two model s
linear linear regression and the na base
one okay so first we are first we will
go
from linear regression for that I will
write train underscore
time to I +
1
1
for I in range
test underscore time equals
to I
+ length like
train one for I
in change then length should be
test okay so for
length training
time comma length should be
tested
prenter okay this is the training and
this for the testing 792 rows and here
55 rows in
testing so
LR uncore LR means linear regression let
me make it
capital
train equals to make a
copy L
rore test equals to test
copy dot
copy
then
LR
okay so
LR
R
train
time
to train
time okay and
LR underscore
test
time equals to test
time okay underscore is there
yeah
so here I will write LR equals to
linear
regression do fit to the
model LR
train and for the
time l
underscore
Train
Price
values linear regression is not defined
okay my bad L should be Capital yeah
so now see the graph so test
underscore
prediction let go model
1 to l
dot
predict
lore
test
time L lore
test
first
to test underscore
predictions score
model
one okay let's
create the graph
figure figure size should
be to 14 comma
6 PLT do
plot
Rain price
comma label should
be
train okay then
plot PLT do
plot
test price
label equals to
test PLT do
plot then L train or
test
forast
label equals to regression on time okay
regression on
time
then again PLT do
Legend l equals
to
best PLT do
get that's
enter
okay yeah have
to
loading one more
error
price
price
okay is the
price so here you can see regression on
time test data is this green one and and
this training data and the testing
data okay let's find the map now so for
this we'll write here
DF
map comma
prediction then
return round
we do
mean
ABS
actual
section
je
I
100 comma
two okay
forgot to give the
yeah so for for getting the map you have
to write a map
underscore
model let's go test equals to map
test
price dot
values
comma test
underscore
section model
one
okay then print
is perc
3 Dot 3
F then here I
percent AP
model 1 underscore
test
comma
percentage press
enter
next
okay
is not defined map
test model one
test May
test
values the
test
FS model
one
okay here a is
small
so map here you can see
29.760426
start and
now results equals to PD
dot data
frame test
map in
percent
score model
one comma
index equals
to
regression on
time okay then I will print the
results okay D should be
capitalle pandas has no attribute data
frame
OKAY model one is not
F that's cool
T well it's perhaps you got to forget a
comma
colon
model one
okay here you can see the test map
regression on time so let's do with the
name now we have to perform the same
pattern so what I will do what I I will
write the code and get back to you okay
so I'm done with the code uh so here you
can see I have same pattern train and
test copy and this is the name forecast
on the test data so this is the line and
this is the training and this orange one
is
testing the same we have got M like
19.38
okay so and regression is
this the name model so what we will do
we will create now final model of ours
okay
and we will forecast final forecasting
we'll
do for that I have to write
final model equals to
exponential
smothing okay DF
comma and equals
to
netive comma
seasonal purum additive
comma
fit
smoothing level = to
0.4
comma
smoothing
Trend to
03 comma
smoothing
seasonal = to
0.6 okay press
enter exponential
exponential so
map final underscore model is equals
to map like DF
price dot values comma
final let's go
model
dot fitted
values then
print
map comma
map
score final underscore
model okay so map is 17.24 which is
quite good for the final
model
okay so getting the prediction for the
same number of time stamps at the
present time in the test data so I will
write here
predictions equals to
final underscore model dot
forecast
steps equals to length.
test
center now we will compute 95 5% of
confidence interval for the predicted
value
so f equals to PD dot data
frames
then
low
CI
prediction
1.96
into NP do standard deviation of
final model
dot1
okay and comma
right here
prediction okay then upper
CI
setion
plus
1.96
into dot standard
deviation the same final
model to
one okay
then
prediction undor DF dot
head so this is lower CI prediction and
the upper CI okay how much it will
forecast
or now at the end at the final State
what we will do we will plot a graph
okay forecast graph along with the
confidence band So for that X is equals
to DF
dotplot
label equals
to
actual comma figure
size
16 comma
9 addtion
DF do
doot ax equals
to
this froma
label underscore
DF it's lower
CI what I will do instead of
this label equals
to
forast comma
Alpha = to
0.5 okay
this do
fill
between
Shore
DF do
index comma prediction underscore
DF
_
CI comma prediction underscore
DF this
CI
then
coloral
to
M
comma
Alpha =
to5
okay then
AIS do
set underscore X
Lael year
month
AIS do
set y
label PLT do
Legend should be
location to
best then PLT do
grid PLT do
show okay this is PLT
only then press
enter okay 16 comma 9 positional
argument follows keyword
argument
15 okay okay I have to give
you
the upper
CI upper
C so here you can see our final model
forecasting so till 2020 it is showing
like normal and after that till
2030 okay this will be the forecast as
per the
data okay
so here you can
see you can see we have the map is
17.24 okay then here I did the
prediction for the testing data and here
I've created the data
frames these are the data frames lower
CI prediction and upper CI and then I
have
created the and then I have created the
final graph of the
forecasting
okay if you're an expiring AIML engineer
then there is no better time to train
yourself in exciting field of machine
learning if you are looking for a course
that covers everything from fundamentals
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our celtech in partnership with IBM
so why wait join now seats are filling
fast find the course link from the
description box below open CV open
source computer vision library is an
open source computer vision and machine
learning software Library it is written
in C++ but as a binding for various
programming languages such as python
Java MLB open CV was designed with the
goal of providing a common
infrastructure for computer vision
applications and to accelerate the use
of machine learning perception in
commercial product open CV is widely
used in a variety of indes including
robotics automative and Healthcare it's
supported by large community of
developers researchers and users who
contribute to its development and
provide support to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide support to its users so now
let's see what is object detection
object detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name so here I will write object
detection
demo okay
so first we will import some major
Library like open CV so for that we will
write import CV2 and the next one is
import M plot
lib do p
plot as PLT so why we are writing PLT
because we can't write again and again M
plot li. P plot okay it's a long one so
we can write a short from
PLT so yeah so let's run this so what is
open CV opencv is an open source
software library for computer vision and
machine learning the opencv full form is
open source computer vision Library it
was created to provide a shared
infrastructure for application for
computer vision and to speed up the use
of machine learning perception in
consumer products open CV as a BSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and open CV is one of them
second one is mat blood Li mat BL Li is
a easy to use and an amazing visualize
library in Python it is built on numpy
array and designed to work with broader
CPI stack and consist of several prods
like line barath scatter histogram and
many others okay so moving forward we
will import our file okay so here I will
write config
file equals to this is our file name SSD
underscore
mobilet
V3
large
Coco
202 14 do
PB okay so you can find this file on the
description box
below Frozen
model
to I explain you every single thing
inference
graph. PB okay so let me run it first
mobile net as a name applied the mobile
net model is designed to use in Mobile
applic app ation and its tensorflow
first mobile computer vision model
mobile net use depthwise separable
convolutions it significantly deduces
the numbers of parameter when compared
to the network with regular convolutions
with the same depth in the NS this
result in the light weight of the deep
neural network so mobile net is a class
of CNN that was open sourced by Google
and therefore this gives us an excellent
starting point for training our
classifiers that are insanely small and
insanely fast okay so what is this large
Coco this is a data set Coco data set
like with applications such as object
detection segmentation and captioning
the Coco data set is widely understood
by the state-of-the-art of neural
network its versatility and the
multi-purpose scene variation serve best
to train a computer vision model and
Benchmark its performance okay so what
is coko the common object in context is
one of the most popular large scale
label images data set available for
public use it represent a handful of
object we encounter on a daily basis and
contains image Inn notations in 80
categories I will show you the
categories I have with over 1.5 million
object instances okay so modern day AI
driven solution are still not capable of
producing absolute accuracy and result
which comes down to the fact that Coco
data set is a major Benchmark for CV to
train test and polish refine models for
faster scaling of The annotation
Pipeline on the top of that the Coco
data set is a supplement to transfer
learning where the data used for one
model serves as a starting point for the
another so what is frozen insurence
graph like freezing is the process to
identify and save all the required
graphs like weights and many others in a
single file that you can usually use a
typical tensorflow model contains four
files and this contains a complete graph
okay so forward let's create one model
here I will write model to CV2 do
DNN
model and then config
file so here I'm giving the parameters
two parameters like frozen model and
config file's score here yeah run it
first okay there is error
return okay so error is CV2 DNN detction
model return is
result an exception set the question
comes what is the meaning of deduction
model or DNN deduction model so this
class represent the high level API for
object detection networks detection
models allows to set parameters for
pre-processing input image detection
model creates net from file and with
train weights and config sets it
processing input runs forward pass and
return the result
deduction okay moving forward let's set
the class labels
okay
class
labels file
name to labels.
dxt I will put this file on the
description box below you can download
from there
open file
name
pass
labels
District
so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into this class labels okay so here
if I will
print class labels
so these are the 80 categories in the
Coco data
set okay this person bicycle car
motorbike aerplane bus train these all
are the 0
categories I will put this file label.
txt in the description box below you can
download from there okay
fine so let's print the length of the
Coco data set or you can see class
labels
this 0 as you can see I have already
told
you the length will be
80 so here let's set this some model
input size scaling mean and all so I
will write here model dot set
input
size 320 comma
320 do
set
input
scale
1.0 SL
127.5 okay I will explain you don't
worry model do
set
set input
[Music]
mean
127.5 comma
127.5 comma
127.5 okay and then model dot
set
put B
will
be what is set input
size
okay so set input size is a size of new
frame a shape of the new blob less than
zero okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame or you can say
the parameter will be the multiply of
the frame values or you can say
multiplier for the frame values okay so
input mean so it set the mean value for
the frame the frame in which the photo
will come the video will come or my
webcam will come so it set the mean
value for the frame or the four
parameters mean scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swep RB so it's set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swap I'll be flag
which indicates the swap first and the
last channels so moving forward we will
Port one
image I am
read o.
jpg
dot I am sure
so this is the size of 320 by 320 okay
so first thing is you can download this
the random picture from the Google I
took from Google itself so now what we
will do we will set the class
index the confidence
value
value
the B box B box is the boundary box
which I will create for the particular
person cycle motorbike and the car okay
bals to
model the confidence
threshold threshold is used
for if my model will confirm it's the
particular image which this is the
texting is correct it will print the
name
okay so let me
print
print
class class index is coming 1 2 3
4 okay so one means
person two means bicycle three means car
and four means motorbike this is the
class index index for particular label
what I will do I will print the
boxes
font
scale = to three and the font equals to
CV2
dot font
her
L
for
class index and the confidence and the
boxes
and do
pattern confid
is
that box the boundary
box
okay then I will write here CV2 do
rectangle make the
rectangle set the
image and
boxes 5 comma 0 comma 0
this is the color of the box and this
will be the
thickness okay then I will write CV2 do
put
text image
then class
labels I will write class index minus
one because always index start with zero
that's fine and the box
is
z
10 comma
boxes
one
4 okay
F
comma
scale
font
scale
color
to this will be the text color 0 comma
255 comma
zero and the
thickness
three let me run
it h no error okay now PLT do I am
show then CV2 do
CVT
color
then CV
T2 dot
color color than
BGR to
brg that is why we wrote swap RB equals
to true because every time we will
convert BGR to
brg
sorry GB RGB so we don't have to write
again and again it will convert all the
files into RGB okay run
it
okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will car is coming okay
so it's detecting the
right for the image now we will do this
for the video and for the
webcam we are done with this image one
and then now I will write
here okay so this is we do for the
video for the video I will write here
cap equals to capture you can write any
name so CV2
dot
video
capture so you can take any random video
I took this
pixels
George can comment down
share the
link app dot is
open so here I will write
cap equals
to
CV2 sorry
CV2 dot
video
capture
zero and if
not cap do is
open
then then
is
output
error
can't open the
video can't open the
video here everything will be the same
font
scale equals to three
okay font equals to CV2
dot
font okay so here I will write while
true comma frame
equals to cap do
sheet this is for the reading of the
file the same I will write class
index comma
confidence comma bound
boxum model do
detect name and the
confidence
threshold = to
0.55 okay everything is the same we did
before so here I will
print
class
index okay so here I will write
if
and of the class
index
does not equals to
zero then what to perform is here I have
to write
for class
index comma
confidence comma
boxes
inip
plus Tex do flatten
flatten is a layers
okay
confides
flatten e
box and
if
class index is greater than equals to
80
then
what to do then I will copy from
here okay the same thing I have to write
here
so here I will write CV2 dot I'm
show this will be the return in the
frame object
detection by simply
Lear and
frame so if CV2 dot weit
key to
and
zero
FFX
to o
d q okay
then here I will write
break will be break
when get into two the weight key will be
two okay I will tell you what is the
weight key
here I will write cap.
release and CV2
do
destroy
all
windows
okay so now let me
run let's see error there will be error
okay
Python Programming langage
modules let me run it
again the
keys
okay video is
here the video is here as you can see
see bicycle the person the person the
bus car track traffic light the person
person so our object detection for the
video is coming right okay person okay
person traffic light
[Music]
bus this is how you can do for the video
okay so now let's we will do for the uh
webcam
live so this is for the video
so if we want to do for the webcam we
okay so we need to just
change one one thing only we have to
change instead of giving the file we
have to write one here okay the rest
will be the
same got it so I have to just shut down
my webcam so let me shut down the webcam
and get back to you
as you can
see this is a 320 by 320 box
so so this is coming right okay so I if
I will show this the mobile phone is
coming right down okay so this is how
you can do the correct object detection
okay users of Microsoft Excel May format
arrange and compute data in a
spreadsheet data analyst and other
analysts can make information easier to
examine as data is added or altered by
organizing data using tools like Excel
the boxes in excels are referred to as
cell and they are arranged in a row and
column the key features of excels are
the spreadsheet document in msxl can
have headers and Footers and user can
protect their data by giving it password
protection and the second one is
filtering is supported allowing you to
locate the necessary data in your
spreadsheet and replace it with the
appropriate
value after discussing what is Excel
let's go through and see what is python
so python is the one of the most popular
programming language available today it
is widely used in various sector of
business such as programming web
development machine learning and data
science given its SP use it is not
surprising that python has surpassed
Java as a top programming language
python frequently used to create
software website and to perform data
analysis and many more because python is
a joural purpose language it may use to
develop a wide range of programs and
isn't Ted for any particular issues so
the key features of python are it is
open source and free python can be
downloaded from the company official
website it is simple to download and
install
python is open source because allowing
user with solid technical background to
modify the code to suit certain business
use case and product requirements since
python is a language for beginners most
anyone with the understanding of
programming can quick pick it up and
begin
coding so after seeing what is python
let's move forward and see what is
automating Excel with python we all know
python is dominant everywhere and we
also know that compare comp to other
language python is beginner friendly and
simpler to use automation is the one of
the coolest thing you can do with python
so how to automate an Excel seed in
Python imagine that you are asked to
create accounts for 30,000 employees on
a website what would you think you will
undoubtly become frustrated carrying out
this task manually and repeatedly
additionally it will take too much time
which is not the vice choice so just try
to picture that that what it is like for
employees who work in data entry the
responsibility is to extract the data
from tables like Excel or Google seet
and insert it in the another location
they read various magazines and website
get the data there and they entered it
into the database additionally they must
perform the calculation for the entries
in general this job performance
determines how much money is made
greater entry volume more pay of course
everyone wants a higher s in their job
so however don't you find doing the same
things over and over
boring the question is now how can I
accomplish it
quickly and how to automate my work
spend an hour in coding and automating
these kind of codes to make your life
simpler rather than performing these
kinds of things by hand by just writing
few lines of python code you can
automate your levious activity in
simpler so overall python Excel
automation is a creative method that
allow you to build visual reports on
python in a smoother manner similar to
how you would on Excel businesses can
use Python Excel automation to
streamline their operation in accordance
with their requirements so here is one
question for you guys I will give you
one minute for this you can comment or
you can give answer in chat section so I
can see the answers are given by you are
correct or not I'm repeating again here
is one question for you guys I will give
you 1 minute for this you can comment or
you can give your answer in chat section
so I can see if the answers given by you
are right or wrong so the question is
which type of programming does python
support objectoriented programming
structured programming functional
programming and all of the mention so
I'm repeating again which type of
programming does python support option A
objectoriented programming option b
structured programming functional
programming and all of the mention so I
starting the timer of 1 minute just type
your answers in comment section or in
chat section do let me know your answers
guys so like please I want that everyone
should participate in this so I'm
starting the timer
so so your time starts
now so I want that everyone should
participate in this guys please to let
me know your answer in chat section or
in comment
section so I can see if the answers
given by you are right or wrong
so like 36 seconds or
more please guys I want that everyone
should participate in this
so let me know your answers in chat
section or comment section so you can
just type the answers or a b c d your
wish totally your wish
so 10 seconds are left guys hurry up
please I want that everyone should
participate in this 5 Second
more okay so time is over we will give
reply those who gave correct answers and
those who didn't give correct answer we
will give you a reply with the correct
answer okay so no
worries so now let's move to our
programming part to perform Excel
automation using python so first we will
open command prom to write command to
open jupyter notebook Twitter
notebook
yeah so this is the landing page of
jupyter notebook and select open select
here new open new
file so this is how the jupyter notebook
UI looks like so at first we will import
some major libraries of python which
will help us in importing our workbook
or worksheet so the first one is uh let
me first rename this we will write Auto
meeting sell using pyth
yeah sounds
good okay so the first one is open pixl
open py XL okay I have to write import
yeah perfect so a python package called
open pyxl can read and write Xcel 20110
xlx format xlsm format xltx and XL TM
files it was created since there was no
library that could read and write office
open XML files natively from python
since open pyxl was initially based on
PHP XL all pred to the PHP XEL team so
open pyxl does not by default provide
protection from your quadratic blowup or
billion laughter XML assault install
diffused XML to protect yourself from
these assaults
so we will write open pyxl and then like
yeah
so then we will write uh for importing
our workbook so we can write here from
open
pyxl
import
workbook
C underscore
workbook
this Capital
here so this is for code for the
importing of workbook so let me write in
command like you can import workbook
like this so I can write you can import
workbook
this okay perfect
yeah so
so
here so let me import workbook
then so I have to write WB for Workbook
equals to
load underscore
workbook the destination of the workbook
so uh the destination is in like desktop
so I have to write C
users
lp9
375
stop FL
[Music]
automation LAN
dot cell SX
format okay cool users SLP this one
desktop automation simply learn. xlx
fine so our workbook is loaded to
jupyter notebook we can see yeah perfect
no error so let's uh active this
workbook for the use so we have to write
w as for worksheet so worksheet equals
to wb.
active active like import from open py
Excel a workbook there must be always
been one worksheet in a workbook it is
accable through the workbook do property
active so that's why we have write wb.
active so after making workbook active
so we will print our workbook like we
can
print so
print
WS so
here so here you can see the sheet one
so it is showing worksheet sheet one why
sheet one because if you open your
workbook and see here is your worksheet
name
like I can so this is my
automation
f
this is simply
learn so here you can see the sheet one
that is why it is showing sheet one
there so uh okay so let me just finish
it
so moving forward let's change the
worksheet name
so I will write here
import open
pyxl so like it's your wish that you
want to write this import file or like
this thing this is not necessary at all
so now I will import the workbook so I
will write here WB equals to open py XL
do load
underscore
workbook 1 pyxl it
is my bad sorry
guys
so you have to give the path so I can
copy path from
here yeah
perfect okay sounds good
yeah here
wbore
sheet equals to WB and square brackets
then you have to write the sheet name so
there we have sheet one sheet
one so sheet one okay
correct so I can change with w bore
sheet do
title
sheet do
title equals
to code
with simply
learn
perfect yeah seems good let me save
this WD
Dove and the same
copy here
paste yeah so when a sheet is generated
it immediately receives a name like they
have sequential numbers of names like
sheet sheet one sheet two and so on with
the worksheet you can modify this name
whenever you want to like using the
title property
so here let me save this
and yeah no error so again go to Simply
learn yeah you can see code with simply
learn first it was sheet one and now it
is changed to code B simply learn so let
me cut this
again
okay so if you want to change the color
of your sheet name tab where is return
code with simply learn so you can write
uh like
here you can write
WD or okay let me make this new
one okay right I will write here so
underscore
sheet dot
sheet underscore
properties tab
color
color
alss
to here you have to give
hex values so four zeros
okay so let me save this okay it is
saying sheet one why sheet one because
we already changed sheet one to code
with simply learn so what we have to do
is here we have to write code
with simply learn
simply
Lear okay seem same simply Lear simply
Lear same code code yeah so what I will
do here I will just make W Capital so
yeah now it will work fine yeah
so if you save or run your code uh like
like this let
me let me show you one thing uh uh
like color is already changed wait for 1
minute guys I will show you something
crazy so it will see like this
W okay so here I will small
so if you save or run your code not
actually save but run your code while
your Excel seed is open so my Excel seed
is open code will simply
learn
okay so my Excel seat is open so it will
give you error like which error I repe
like what
error this okay let me do one thing this
is one okay it will give
you simply learn one does not exist let
me check this is simply learn code with
simply
learn one
code with okay WS Capital
there okay see this is a permission
denied then the location it is showing
permission
denied so this is something important to
remember that Excel file should be
closed while running the code I
repeating again so like this is
something important to remember that
Excel file should be closed while
running the
code so first what I have to do is close
the Excel file then I have to run it
again so no error now so here you can
see what we simply learn that red color
came yeah
perfect so moving forward let's create
new workbook till we were working on the
same worksheet which was present in my
like desktop so let's create new
workbook so for that I have to
write
import cells X
writer so I will write here in command
to
create
newbook WB equals to
xlsx
writer
Dot
bugbook and where you want to save the
workbook so I will give the
same
a but with different name so this time I
will give only like coders
simply okay simply
coders then I will write WD CL
close okay I will run it
yeah so the primary class export by the
XLS writer module in the workbook class
and that is only class you will need to
insensate directly so the workbook class
represent both the whole spreadsheet as
it appears in the Excel and the Excel
file as it is stored on the disc
internally so you can see now yeah you
can see now the workbook which is
created in the desktop with the same
name is like simply
coders so this is done let's move
forward and let's retrieve cell values
so right now we are not having any data
inside the workbook so let's put some
random values like names and all so uh I
will put some random
values we'll put
here names
okay here
myang
goal okay
W like
jelly
it
so let's save it let's save
it and let's move to our coding part so
here what I have to write to retrieve
cells here I just write a equals to A is
variable here so I'm assigning is
Ws
that A1 value the value which is
presented at
A1 so here I will write in commands like
to retrieve
cells
okay so here I will
print print
a okay
okay what I have to do is like we have
created this one file so it is not
giving the uh this is giving sheet 1. A1
why because it is taking from the simply
coders one so what I can do is I can
import this file because we have save
data in this
file so let paste this here and make it
active get active yeah so yeah you now I
haven't write here
value so that is why it is giving the
cell name and that A1 name cell name A1
only so now it will
print names why is why names because at
the
A1 at the A1 position we have names so
if you want to like print a
so what you have to do is okay let me
check this
B3 okay if you want to print Angeli you
have to just write here just copy paste
this
code okay just write here the
location
B3 you can see Angel is printed so this
is how you can retrieve
cells and we can do the same for the
other cells too so there is one thing
like
retrieving we can change the cells
values too so just we have to write here
so let me give first enter enter enter
for the better
visual
yeah so here what we can write is
Ws like I want to change the name of uh
name of name
of uh
I will take this A4
ismith let me close
this okay I will give here
A4 dot
value equals to here you can give the
value so here you can write is Smith you
can write John here as a replace value
so I will write here in as a
command change cell
value
okay so what you have to do is just save
it
wb.
save you have to give the part for the
same
yeah fine
so let me save this and let me save this
first and run this first okay no error
and let's open our workbooks to see the
changes so here you can see the changes
has been implemented in our workbook
first there it was like Smith and now
John so like this you can change the
cell values and let's move forward and
see see how we can merge the cells in
the
worksheet so like just close this and
yeah so we are doing for the merging
cells so here I will write from open
pyxl Dot
styles
styles
import
alignment so here I will
write mer
cells for your better
understanding
okay so here I have to write WB equals
to
workbook okay let me active this
workbook
wb.
active
merge the
cell which one uh let's take for a while
A1 and
B2 okay A and B should be in capital
letters so after that we have to save
this we will save in this
only so I can copy I can paste it here
okay
fine like using merge undor cells and
after giving cell position you can
easily merge the cell so let's see the
output so let me open this simply learn
file you can see the they are merged
together and like but what if you want
the values at the same times so you can
write here I have to cut
this so you can write here like from
open
pyxl Dot
Styles
port
alignments alignment a should be
Capital
alignment okay so the same thing you
have to write wdb equals to
workbook WS equals to WB do
active don't have to give
parenthesis
ws.
merge underscore
cells
cells
A1
B2 so here I have to give the cell value
equals to ws.
cell which row which column so I can
write row equals to
1
column = to
1 so
cell dot
value equals
to here you can give the value which you
want to print at the cell so here I will
write code with simply learn
okay fine so what we can do this we can
give the
alignment alignment equals
to
alignment horizontal
horizontal equals to
Center and from from
vertical and from vertical the
same
Center okay let me save this file wb.
save at the same location so I can copy
the path from
here copy here and paste here so I
will yeah seems good let me save this
why it is giving error uh cell object
has no attribute alignment okay
alignment spelling is wrong sorry my bad
GN M and
T it seems good
now oh good to go
so so here you can see
like by giving row equals to 1 columnals
to 1 basically position and after giving
the value to what to print on that cell
and one more thing is alignment like it
is not necessary to give alignments like
I just wrote it to cover it uh like in
video so you can give alignments like uh
right left this time I gave the center
to print in the center so let's uh move
and see the
results so you can see the value has
been printed exactly at the center of
the
cell I will make it like this you can
see the value is printed exactly at the
center of the cell because I gave the
alignment center so let's do one more
for like to see what is the default
place if you don't give the alignments
okay if you don't give the center
alignments what is the default value so
let me first remove this okay save no
issues okay
yeah
so
align
alignments or here what we can do is
like from open
pxl do
Styles
import
alignment is it's not necessary to write
import
alignments okay this is
without okay I can write this
default seems
good okay everything we have to cover
from here so I can just copy from here
and paste it here so what we will do
here we will change the cell value we
will use like
C six and
B
8 sorry guys
so uh here I have to give
row equals to
6 and column must
be 8 so let make it B C6
B6 okay let's column should be
six
so code with simply learn coders of
Simply
learn coders of Simply learn like we are
giving the value so at the same time we
have to just save it wb.
saave
save
copy paste
here okay so this time we are not giving
the alignments you can see here we have
gave alignments not now so let's run
this and after that we will open our
workbook so this is saying mer cells
okay two must be greater than three okay
so here what is
happening okay I can write here
D6 right so now now it will run properly
I guess yeah it's
working so let me open
this so you can
see this is the default value of like
uh so here you can see the value is
printed at its default location not in
centered or not in right not in left it
is default location
so till now we know how to much sell
so I repeat I hope you guys understand
till here if you have any questions or
any query regarding any code or question
just put as in comments our team will
shortly provide you the correct
solution
so moving
forward till now we know how to merell
so let's learn how to UNM sell Okay so
let me open the
code yeah let me open the code and write
for
you so let me quickly write the code how
we can merel so we can write
open okay
from from
open
Dy
XL
Styles so just I can copy paste from
here
I have to write here import
alignments okay alignment
only so what we are doing here we will
not give any value or any this so here I
will remove like
B6 six actually we can give like
a
6 B6 as a
value so let's see the cells are merge
or Not by running it okay see permission
denied because our Excel was open so let
me save this and close it and run it
again no error so
so so here you can see let me Zoom if I
can yeah so as you can see A6 and B6 are
merged so what should I do to unmerge I
have to
write as you can write here
like here you can write WS do
unmerge
underscore
cells okay cells got it and just give
this same
value A6 uncore
B6 okay colon B6 let me save this again
no error let me open the
file so you can see here A6 and B6 now
unmerged so by just using unmerged cells
you can easily unmerge the cells in your
workbook so let's move forward and see
how can you set dimensions for a
particular cell or column so let me
close this and go to the workbook again
this is something frustrating by like by
regularly doing the same thing going
next next
next
o
so like you need a particular column
height should be something and WID
should should be something so that is
what I like meant to be Dimensions so
let's write code for it you will see and
understand by seeing so
import open
pyxl hashtag to
set
dimensions of a
particular
cell
okay WB equals to open
pyxl Dot
workbook worksheet equals to workbook do
active
ws. row uncore
dimensions
okay one I will explain you don't
worry
height equals to uh let me give 70 for a
while okay ws.
column
underscore
dimension
with I will explain you don't worry
don't worry
it so let me save the file wb.
save and let me copy the path from
here
okay everything
seems good
okay y column okay column name is like
function nor column Dimension
okay
Dimensions no error so like one height
70 and B width is 8 column one let me
open the
workbook so here you can
see by giving row Dimension and column
Dimension and by adding height and width
then we can set the dimension for a
particular cell y 1 and B for this you
can see one for the column like two for
the column three for the column let me
Zoom it you can see now one for the all
the one column Row Two for the two Row
three for the three row like this for
you can say I use one for the the
particular row or you can say one for
denoting row particular Row 1 2 3 4 5 6
like this okay I repeat and particular
alphabet to denote the column see a for
this column B for this column C for this
column D for this column like this
okay so here you can see every cell of
Row one has the same height this this
this this this this this this and every
cell of
p and every cell of column B has same
width B has same
width so like we can do one more thing
we can print value though in a
particular cells like okay let
me close this and open the this
one so what can I do is I can
write
here
like we can write at WD so
cell row = to 1 comma column = to
1
okay dot
value equals
to
hello okay and WS do
cell here I will write row = to
2 comma
column = to
[Music]
2 dot
value dot
value equals
to
coders
so I have to write it here not below
this yeah okay cool let me give this
space
line okay so let me save
this and me
check so here you can see Hello is for
the row one and coder at the column B
okay fine so let me close this and okay
next I repeat next let's see how we can
move data to a particular row or column
or you can say a particular cell here we
need data to make it move so let's fill
some random data on it so what I can do
is I can put some random
data of like
fruit
vegetables and some names like
mayang
jelli
above
forell oh
no and we can give like anything is
Smith
John and
Chloe
Sara
and
S
okay I I think this much is enough so
let me save
this and yeah so data part is done and
let's move to the this one and write
import open
pyxl
okay so let me write here move
cell you can say move or jump whatever
you want to say so from open
pyxl
import
workbook comma
load underscore
workbook
be equals
to
load can give the path like this yes C
and
V okay let's make it active WS equals to
WB do
active perfect seems good
okay so I have to move move
WS do
move let score
range this is I can
give okay let me see the data
first okay I will make anav here
and mayank here or here okay so let's
see again I forgot it is
B 5 and A4 B5
A4
okay so what we can do it uh
like
Oh
e
5 comma uh like
row to
two and
call rows equals to two and calls equals
to
two so let me save this w.
save
and so here what I'm doing is changing
the position of the value which is
present in the
location uh A4 and B5 so and moving them
by you can see the value will jump by
two rows and two columns because I gave
here two rows rows equals to 12 columns
equals to
two so let me save this run this
okay
and so here you can
see so let me run this and what you have
to do is open this
file yeah so you can see A2 means 1 2 it
comes fruit comes here and coders come
here so this is how you can move your
coding part I repeat so this is how you
can move the cell values so let's move
forward
and see moving forward let's see how we
can insert our row in a workbook okay so
first what I will do is
I will add some data
like name here like
fruits and something like
car and here I can write
bike okay something here
coding okay all the data is fill and all
rows are filled by some values so here
we have no like no empty row present
here till now so what we will do
is we will insert a row okay so what we
have to write is import open
pyxl
hashtag
insert
po okay from open
p
import
book comma
load
uncore to
load let me copy the
part
see so let me make it
active so I want to insert
row
the position
two and one more
row at the position
five so let me save this I will copy
from
here okay I will make it like this so
let me save this no error let's move to
the
workbook so you can see here row two is
created and row five is created with no
values
so here you can see rows are added to
the particular position or you can say
the indexes so we have seen how we can
insert the rows let's see the deletion
of the rows so what we can write is let
me open the
code so what we can write is let me copy
to save our
time okay just delete
this so we can write
ws.
delete
score rows and which Row 2 five uh I
will delete row number two for a while
so let me save this
and run
this so you now you can see empty Row 2
is
deleted so by using insert rows and
giving them index you can insert the row
at the index position you give and
meanwhile you can use delete uncore rows
function to delete the particular row so
moving forward let's quickly write code
for the insert
column so for inserting column
I will copy from
here yeah insert
row
column and this one is
for
elting
R
okay so I have to write here
WS
Dot active not active sorry
insert uh calls at the position
second okay at the position one
and okay I will make it
insert calls at the position
three okay let me save this file by
copying this
path
okay saving let me do one thing yeah
perfect so let me open the
workbook so here what I done is insert
call at second position here we can't
give alphabet we have to give a number
for denoting a position
so here you can see the column is added
at the position one and
three so by using insert uncore calls
and by giving index you can insert
column in your workbook so let's do one
thing
guys okay first let me close this and
yeah so what I was saying that let do
one thing guys you will tell me how can
I delete a particular
column okay let me write here the
question commments like
how
can
you delete
a
particular
particular
column
okay uh I'm giving you one minute for
this just comment the code for this
column deletion or else you can reply on
chat I will suggest you to write a
comment because after this session our
team can easily check who gave the
correct answer and who didn't give the
correct answer don't worry I will
provide you the answers at the end of
the session or our team will reply at
your comment at the same time so uh like
for those who are watching this video
like we watch normal video means you are
not watching the live session so what
you can do is just pause the video right
now and put the comment how you can
delete the column
I'm repeating again I will suggest you
to write a comment because after this
session our team can easily check who
gave the correct answer and who didn't
give the correct answer don't worry I
will provide you the answer at the same
time or our team will reply at your
comment at the same time and those who
are watching the video like we watch
normal video so that you can do is just
you can pause this video and put the
comment how you can delete the
column so I guess time is up guys I hope
most of you gave the correct answers by
putting comments we will verify your
comments and we will back to you soon so
okay moving forward let's see how we can
style the font in Excel okay this is
something crazy and interesting so you
will not get bored and writing from open
pyxl Dot
Styles
import
font so here I will write for your
styling okay so what I can do is like WB
equals to
open
pyxl do
workbook WS
dot equals to WP do
active I don't worry I will explain
you row = to
1 comma
column = to 1 =
to
Lear
okay and I will set the size of the cell
to WS equals to
to
cell row equals
to
1 comma column = to
1
dott
value okay I have to write
font equals to
font equals to
font and I have to write
size equals to
24 okay here I have to
give dot
value
okay now we can do it for the many
others
what we will do it we
will change
hello not world again we will write
coder hello coders
and here we will write code with simply
learn and here we will
write hello YouTube
okay
so what I will do is like it
will two column two row two two
here here what I will do is font size
equals to 24 and and I will make it a t
lals to
True okay fine and I will make
it
old hold equals to
true and here I will
give
three
three
okay and at the end what I can do
is okay
bye
new so let me save it quickly WB
dove
with the
F so let's run this and see the
results so let's run this again one more
time so here what I did is
just gave the particular row and column
value where we want to print the value
like simply learn hello coders hello
YouTube code with simply
learn okay and by using font I am giving
the font size and font like italic bold
and so on like this is italic this is
bold okay and here you can see at the
End by using name here you can see at
the End by using name
be assigning the styles of the
particular text like Times New Roman and
you can give the name of your choice so
why it is not
because I have gave the it is
collapsing so here I have right 444 okay
now it will it is collap collapsing now
hello YouTube and simply learn so what I
will do is now I will
run
now you can
see yeah perfect so this is simple one
this is in italic this is in bold and
this hello YouTube is in Times New Roman
style
okay so okay let's move forward and do
something with
Sals um like what we can do let's do
color them okay let's write code for
this how we can like color a particular
cell we know how to color the particular
tab of
sheet and now let's color it so we can
write is
import open py
XL here I will write
coloring
partic uh cell
okay so I will first copy this thing
here from
here okay I don't think I want this I
have to write from open E
XL
FES
port
Pon
fill I want something like
this okay so wbd doop pyxl
workbook so here I can write open
pyxl do load
workbook okay
workbook and here I will give the
path
yeah it seems
good so what I will do
is make it active and I can write here
WS equals to
WB square brackets then I can write uh
name of working sheet I don't
remember only sheet
sheat so we can write here fill
underscore cell equals to
pattern
fill a
pattern type should be I will prefer
solid
okay and
comma here
comma FG
color color foreground color equals
to I will write here
f c 2 C
0 3 okay fine we can make four
cells four
cells four cells are enough I guess so
we can write here cell
one paste copy paste copy paste so we'll
Four 3 2 solid solid solid
yeah so let's change color we can give
03 FC
F4 here I will give uh 35
FC
03
perfect here I can
give oh
FC
a03 so now let me
assign me assign the cells to them
WS like
uh I can assign
a A
1 sign A2
dot fill equals
to
fillore
cell
okay
fill underscore cell
one that seems
good this I can make it for 4 v v
v
M here A2
A4
A5
A3 everything seems
good let me save this so I will copy
from
here why error permission denied because
it was open so let me cut
this so let me
see so here I'm using the pattern field
to fill the color in solid style and
giving the FG color like foreground
color and at the below you can see I'm
like assigning the cells to a particular
pattern field cells so
so let's see the output yeah it's
working here you can see the results we
colored these for the particular cell
like A2 A3 A4
A5 so this is something you can style
the color so let's move forward and see
how we can import the image in our
workbook like something here something
here something here let's see so let me
first cut this
okay let me do one more thing yeah
perfect
coloring particular
cell so how we can import our file or
image you can say so I will
write
from open
pyxl
port load uncore
workbook okay load uncore workbook loore
workbook
and I have to write here from open
pyxl Dot
drawing do
image import
image okay
fine buug
book
write WD equals
to load
underscore
workbook give let me copy this and this
whole so logo is
for variable use for the image image
okay so I don't have any image for right
now let me do one
thing uh let me import one image any
kind of image for a while so I can do
like so this is my
image what can I do
is what can I do is I can give the path
like
uh path would be
same PA would be
same we will change here something don't
worry don't
worry
desktop don't remember name of the file
this is picture
two okay p is is small or capital P is
capital so I have to write
picture
2.png okay
so I will give
logo dot height equal to
150
logo dot
width equals
to 150 a bit of resizing to not fill the
whole spreadsheet with the logo so
that's why just for that so ws.
add
image to logo comma cell in which cell
you want to print I will
M2 fine so here we will save
it by using this
one
okay
perfect cannot image import name image
from
op so might be there is
problem open pyxl
Dot
drawing do
image okay import
image I think I is capital here
yes
seems logo image okay ice Capital
here yeah it seems
good yeah here is the picture so what I
do is what can I do is
so here we need one more Library drawing
image uh variable logo this one is for
the location of the image and giving and
height and width because a bit of
resizing to not fill the whole
spreadsheet with a logo like with the
whole spreadsheet with the logo so just
a bit of
resizing and add image function is used
to append image on it and M2 is for like
this M2 M2 is for position
M2 like that it's for in which cell you
want to append it
so image is coming to the particular
index
M2
okay so this is how you can import image
in Excel using python okay let's move
forward and see how we can import data
and time so let's write code for that
quickly
okay oh what I will do is I will write
here
import date
time I will
write date
time
from open
pyxl
import
workbook
port
so what can I do
is make it WBAL to
workbook okay BL should be Capital
here and WS = to WB do
active what can I write is Ws do
cell Row
1 comma
column = to
1 dot
value equals to
date and
time
something
so I can try copy it paste it what I can
do is let me first change the
three
o equals to
3
okay column equals to two I guess
value so here I can assign date and time
date
time. dat time do
now dot
string format St
strf strf
time what you have to write it percent
y percent M for
month percent D for
days percent
H then
percent M for minutes and
percent
seconds so this should be in between
this
okay
yeah so let me save
this close
it import data time not date time it's
date time
time okay row equals to
1 data time not date
time
and date time dot
now dat time dot date
time dat time dat time objects has no
attribut M St strf time okay time it is
yeah okay yeah finally it's working no
error so air like
okay so here I have imported some
libraries like date time and so on Str
strf time used for string formatter and
date time now used for the current date
time this date time now is used for for
the current date
time and so let's run this and
see here you can see the correct time
and date is coming it can be different
from your from which country you are
watching this live session so what if
you want to print the time after like 2
seconds late so for that we can write
here
quickly so I will copy this
whole so like we have to copy this and
just what you have to do is uh let me
make for to see the
difference v v okay three are
enough so what I will do is
C
four
five okay fine what I will add here is
time dot
sleep how much second you want like two
for this one
and something for this one like 5
seconds for this
one
okay so what we can do is let's save
it and let's see the
results so here I used the time do slam
function given parameter as two or four
I guess five for printing the seconds
difference so here you can see the
difference in seconds okay the time is
not coming wait wait let me
check okay why permission denied sorry
guys
now I have to run it open
it and it is not coming why
so should
night
okay let me run it
again
okay okay because I'm giving seconds
difference that that is why is giving
permission denied so let us wait for 2
five 5
Seconds okay I think 5 seconds are over
now we can open
it yeah you can see 47 seconds and 49 2
second difference and 49 54 5 Second
difference so here I used the time sleep
function given parameter as 25 for
printing this the second difference so
let's move to the workbook uh I repeat
here you can see the difference in
seconds so I hope you understand till
here if you have any questions or any
query regarding any code or question
just put as in comment our team will
shortly provide you the correct solution
I'm repeating again I hope you
understand till here if you have any
questions or any doubt or any query
regarding any code or whatever it is so
just put as in comments or our team will
shortly provide you the correct solution
so moving
forward moving forward let's see how we
can add values as an
array okay so what I will write
import
xlsx
writer WB equals to
xlsx writer dot
workbook okay I will give the
path from
here okay WS equals
to wb.
at
worksheet W should not be Capital
here okay so what I will do is row
equals to
0 and column equals to
0 okay so I will write here
uh data equals
to and take is the names just
names
just random names they
are above let's
take one more
K
so for
item and
data okay
WS do
write I will explain you line by line
don't
worry row
comma
column comma
item perfect so here what I will do is
row + = to 1 or you can write row equal
to row + 1
WB
Dot
close okay let's save
this let's see our data is appended or
not okay first I will explain this code
to you so here I have created one new
workbook uh like uh name no I haven't
created so let me create
two okay
example
okay so run this yeah perfect so I
repeat so here I have created one new
workbook name example so at row 0 column
0 I'm printing my content values like
for data values using for loop I moving
to the next itation of the values like
from anju to kosu and then mang then so
on
on so let's see the
results
okay yeah so here you can see all the
values are printed so using these quotes
of line you can do things like this
easily so as we know how to write
multiple values at the same time what if
if we want to retrieve values of a
particular row or column so for that we
need
so we need data for that so let me add
the full
data of let let
me copy some data from somewhere so just
give me a
minute
okay
is the blank
worksheet let me give me one minute
guys okay what I will take
is this
one I can take this some values we can
copy like uh 50 values okay days I
know
yeah we have data now simply
Lear have data in the simply learn
okay
now we can write code for that like how
to R multiple
column so the
same I
want
see
to WB do
active
Okay
WS equals
to
WB
spreadsheet
column in
worksheet
a
okay so what I have to do is just print
column
values column.
value I hope it will work uh
wb.
active workbook object has no attribute
active Okay let me see
okay
sorry import open
pyxl XLS wrer is used for to create new
workbook okay open
pyxl do
load underscore
workbook now it will work I
guess still it is not
working
it should work I
guess open
pyxl open pyxl Dot load okay okay sorry
my bad here dot now it will work okay
sheet it's too much this is sheet
one you never
know when the sheet will create sheet
one
yeah you can see all the list of column
A is printed here all the 50 values and
you can match from here
to all the 50s values okay Trail Travis
Scot so let me match
thisis Scot
okay
so I hope you you understand till here
if you have any questions or any query
regarding any code so just put as in
comment or our team will shortly provide
you the correct solution after doing
importing exporting the values and all
let's perform some basic operation like
addition average and all okay so first
we will start with the
addition so I will write here so we need
data in workbook for the
addition so let's take some data from
from which
set the same insta data
set so I will take
this
this 50 values are
enough okay copy
it and let me paste
here
so data we have let's code for it we
have to write here
import open
pyxl we are doing some of
columns not column column values values
yeah like from
open py Excel
import
workbook comma
loore
workbook so WB equals to
load
underscore not
workbook
workbook copy the
pot
okay so I will make it
active
so here what I'm doing is I will tell
you shortly so it is in C I guess C or
B which
column
b b
5 1 I guess yeah
b51 B should be
Capital equal ALS
to see carefully what I'm doing like
sum from B
one
one till B I guess 50 values we have 50
okay B should be Capital my bad
sorry okay this is what I'm doing and WD
do save where is the save
one
okay so here I have added all the values
which are between in the range of B1 to
b50 and what I'm doing is the result
will be displayed on the b51 cell okay
so let's run this
code okay
let's go to the
b51 yeah result is coming for verifying
the result is correct or not I won't use
calculator for this uh so we can use
this uh code for the same for average
product and count so let's do it
quickly okay so let me just copy paste
this stuff c v this is will be
for
average a spelling mistake
average okay what I will do is just
write
capital Bridge here okay and I will
print it to
52 okay code is is almost same just one
change like you have to write instead of
sum you have to write average there and
just okay let's see
then so now you can see this is the 137
something something something is the
average of these all values everything's
look good let's see the count
quickly
Let's uh
we C is almost same just we have
to okay
Capital
53
visible okay this count will give you
how many values are there between the
range till 1 to 50 so let's check let's
run it again and check if it's working
or not yeah 49 values
okay
so what I'm planning is let's do one
thing now guys uh you will tell tell me
how I can find the product of a
particular range I'm writing question as
a command uh as I did
before
so how
to okay okay how to uh
find
product
product of
numbers
like
where
where where the range is
between 8 A2 to a
98
okay so
I'm giving you one minute for this just
comment the code for this uh like
multiplication uh or else you can reply
on chat you can write just syntax only
for the product I will suggest you to
write a comment because after this
session our team can easily check who
gave the correct answer and who didn't
give the correct answer don't worry I
will provide the answers at the end of
the session or our team will reply at
your comment at the same time and those
who are watching the video like we watch
normal video or you are not attending
the live session so you can just pause
this video and comment on how we can
find the product in Excel using
python I'm repeating again I will
suggest you to write a comment because
after this session our team can easily
check who gave the correct answer and
who didn't give the correct answer don't
worry I will provide you the answers at
the end of the session or our team will
reply at your comment at the same time
those who are watching this video like
we used to watch normal videos uh so
what you can do is
just pause this video and comment on how
we can find the product in Excel using
python so I guess time is up guys I hope
most of you gave the correct answer by
putting comments we will verify your
comments and we will back to you
soon so moving at the last of the
session let's do something with charts
so what I will do is first I will move
it bit up yeah
so let me write code for that for the
chart so I will write from
open
pyxel
import
workbook open
pyxl do
chart
import reference
comma line
[Music]
chart okay we will create line chart WB
equals to
workbook WS equals
to workbook do
active so after this I will set the
chart
title t it equals
chart
okay let me create the rows so rows
equals
[Music]
to uh I will do
is I will create something
like
PS okay
comma
uh
sale EG
sell
this not
here
comma and I have to give the values
weeks are
Monday okay let me keep this
same forood sale should be like
100 comma
200
Tuesday comma
200 comma 300
W
day
300 comma
400 for the
Thursday
400 20
30
20
comma 60 comma
30
okay it seems
good let me
[Music]
do like
for
R okay Row in
rows okay WS do
append
oh very
values equals
to
reference WS comma
Min call = to 2 comma
Min row = to 1 comma
Max column = to 3 comma Max underscore
row
equals to
7 okay it seems good okay X do
values equals
to
reference underscore
string equals
to
chart
[Music]
A2
A7 so chart equals to line chart we are
making so let add the data to line chart
object
chart do addore
data
values comma
titles di T TL yes
titles uncore from underscore
data to
True okay should be
capital so let's set
xaxis chart do
set
categories xcore
values okay let's set
chart
dot to
sales chart
dot
XIs equals to
weeks and
chart
doy
access do title equals to
fruit
and
veggies
sales chart.
Legend do
position to
B
okay so ws.
addore
chart
chart
comma H1
cell so WB okay let me copy this code
from
here
yeah
so here I'm importing reference and
charts Library like this reference and
charts
library and just giving title as charts
you can see here
chart uh so rows are the data from which
we will make chart and for Loop is to
append the data on the
workbook chart add data chart add data
values titles from data equals to tr
this is for adding data to the line
chart object just giving the title xaxis
and y axis and all so let's save this
and see whether chart is visible or not
let's save
this okay what is going cannot import
name reference from open file reference
okay let me
see error in line two error in line
two okay sorry my bad okay now it will
workbook fourth number work okay W
should be
capital W should be Capital now it will
work I
guess again that's refy
R okay
X do
values dot my
b h a
t c h r
t do save okay WB do
save okay finally no
error so let's save this and see whether
chart is visible or not let me go to the
simply
learn yeah you can see chart is coming
and it is it looks so good if you're an
expiring AIML engineer then there is no
better time to train yourself in
exciting field of machine learning if
you are looking for a course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than our
celtech in partnership with IBM so why
wait join now seats are filling fast
find the course link from the
description box
below hello everyone welcome to the
session I'm Mohan from Simply learn and
today we'll talk about interview
questions for machine learning now this
video will probably help you when you're
attending interviews for machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked uh questions
and to help you in answering these
questions we tried our best to give you
the best possible answers but of course
what is more important here is rather
than the theoretical knowledge you need
to kind of add to the answers or
supplement your answers with your own
experience so the responses that we put
here are a bit more generic in nature so
that if there are some Concepts that you
are not clear this video will help you
in kind of getting those Concepts
cleared up as well but what is more
important is that you need to supplement
these responses with your own practical
experience okay so with that let's get
started so one of the first questions
that you may face is what are the
different types of machine learning now
what is the best way to respond to this
there are three types of machine
learning if you read any material you
will always always be told there are
three types of machine learning but what
is important is you would probably be
better off emphasizing that there are
actually two main types of a machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is l labed
data right so if you just say past data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like like what
what are the ways of Performing
supervised learning and unsupervised
learning or what are some of the
techniques so supervised learning we we
perform or we do uh regression and
classification and unsupervised learning
uh we do clustering okay and clustering
can be of different types similarly
regression can be of different types but
you don't have to probably elaborate so
much if they are asking uh for uh just
the different types you can just mention
these and just at a very high level if
it's but if they want you to elaborate
give examples then of course I think
there is a different question for that
we will see that later then the third so
we have supervised then we have
unsupervised and then reinforcement you
need to provide a little bit of
information around that as well because
it is sometimes a little difficult to
come up with a good definition for
reinforcement learning so you may have
to little bit elaborate on how
reinforcement learning works right so
reinforcement learning works in in such
a way that it basically has two parts to
it one is the agent and the environment
and the agent basically is working
inside of this environment and it is
given a Target that it has to achieve
and uh every time it is moving in the
direction of the target so the agent
basically has to take as some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal uh a Target is
nothing but a goal okay then it is
rewarded and every time it is going in a
Direction where it is away from the goal
then it is punished so that is the way
you can a little bit explain and uh this
is used primarily or very very impactful
for teaching the system to learn games
and so on examples of this are basically
used in alphago you can throw that as an
example where alphago used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world Champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting may be a
little difficult to understand
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes easy to explain this now
let's say you're teaching a child about
some recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples okay so this is a a small
child and for the first time you're
teaching the child to recognize fruits
then so what will happen so this is very
much like that is your training data set
so what you will do is you will take a
basket of fruits which consists of
apples oranges and pineapples okay and
you take this basket to this child and
uh there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so for
and then again you keep repeating that
right so till that basket is over this
is basically how training work in
machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then the process
what has happened the child has pretty
much memorized these so even before you
finish that basket right by the time you
are halfway through the child has
learned about recognizing the Apple
orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100% accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple right so that means
it has kind of memorized this data now
let's say you bring another basket of
fruits and it will have a mix of maybe
apples which were already there in the
previous set but it will also have in
addition to Apple it will probably have
a banana or maybe another fruit like a
jack fruit right so this is an
equivalent of your test data set which
the child has not seen before some parts
of it it probably has seen like the
apples it has seen but this banana and
jack fruit it has not seen so then what
will happen in the first round which is
an equivalent of your training data set
towards the end it has 100% it was
telling you what the fruits are right
Apple was accurately recognized orange
were was accurately recognized and
pineapples were accurately recognized
right so that is like 100% accuracy but
now when you get another a fresh set
which were not a part of the original
one what will happen all the apples
maybe it will be able to recognize
correctly but all the others like the
jack fruit or the banana will not be
recognized by the child right so this is
an analogy this is an equivalent of
overfitting so what has happened during
the training process it is able to
recognize or reach 100% accuracy maybe
very high accuracy okay and we call that
as very low loss right so that is the
technical term so the loss is pretty
much zero and accuracy is pretty much
100% whereas when you use testing there
will be a huge error which means the
loss will be pretty high and therefore
the accuracy will be also low okay this
is known as overfitting this is
basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of the question there are
multiple ways of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like Dropout in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have thousand records with
labeling information now one way of
doing it is you use all the Thousand
records for training and then maybe
right which means that you have exposed
all this thousand records during the
training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results all right
but there is a flaw there what is the
flaw this is very similar to human
beings it is like you are showing this
model the entire data as a part of
training okay so obviously it has become
familiar with the entire data so when
you're taking a part of that again and
you're saying that I want to test it
obviously you will get good results so
that is not a very accurate way of
testing so that is the reason what we do
is we have the label data of this
thousand records or whatever we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model now
the training process remember is not
just about passing one round of this
data set so let's say now your training
set has 800 records it is not just one
time you pass this 800 records what you
normally do is you actually as a part of
the training you may pass this data
through the model multiple times so this
thousand records may go through the
model maybe 10 15 20 times till the
train
is perfect till the accuracy is high
till the errors are minimized okay now
so which is fine which means that your
that is what is known as the model has
seen your data and gets familiar with
your data and now when you bring your
test data what will happen is this is
like some new data because that is where
the real test is now you have trained
the model and now you are testing the
model with some data which is kind of
new that is like a situation like like a
realistic situation because when the
model is deployed that is what will
happen it will receive some new data not
the data that it has already seen right
so this is a realistic test so you put
some new data so this data which you
have set aside is for the model it is
new and if it is able to accurately
predict the values that means your
training has worked okay the model got
drained properly but let's say while
you're testing this with this test data
you're getting lot of errors that means
you need to probably either change your
model or retrain with more data and
things like that now coming back to the
question of how do you split this what
should be the ratio there is no fixed uh
number again this is like individual
preferences some people split it into
50/50 50% test and 50% training Some
people prefer to have a larger amount
for training and a smaller amount for
test so they can go by either 6040 or
7030 or some people even go with some
odd numbers like
6535 or uh 63.3 3 and 33 which is like
1/3 and 2/3 so there is no fixed rule
that it has to be something the ratio
has to be this you can go by your
individual preferences all right then
you may have questions around uh data
handling data manipulation or what do
you call data management or Preparation
so these are all some questions around
that area there is again no one answer
one single good answer to this it really
varies from situation to situation and
depending on what EX exactly is the
problem what kind of data it is how
critical it is what kind of data is
missing and what is the type of
corruption so there a whole lot of
things this is a very generic question
and therefore you need to be little
careful about responding to this as well
so probably have to illustrate this
again if you have experience in doing
this kind of work in handling data you
can illustrate with example saying that
I was on one project where I received
this kind of data these were the columns
where data was not filled or these were
the this many rows where the data was
missing that would be in fact a perfect
way to respond to this question but if
you don't have that obviously you have
to provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or corrupt
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much much
half of your data having these missing
values in terms of number of rows okay
that could be one situation another
situation could be that you have records
or data missing but uh when you do some
initial calculation how many records are
corrupt or how many rows or observations
as we call it has this missing data
let's assume it is very minimal like 10%
okay now between these two cases how do
we so let's assume that this is not a
mission critical situation and in order
to fix this 10% of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50 % of your data is like that
because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10% of the
records column 1 is missing and another
10% column two is missing another 10%
column 3 is missing and so on and so
forth so it adds up to maybe half of
your data set so you cannot completely
remove half of your data set then the
whole purpose is lost okay so then how
do you hand then you need to come up
with ways of filling up this data with
some meaningful value right that is one
way of handling so when we say
meaningful value what is that meaningful
value let's say for a particular colum
you might want to take a mean value for
that column and fill wherever the data
is missing fill up with that mean value
so that when you're doing the
calculations your analysis is not
completely we off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
it to right there may be situations
where if the missing values instead of
putting mean may be a good idea to uh
fill it up with the minimum value or
with a zero so or with a maximum value
again as I said there are so many
possibilities so there is no like one
correct answer for this you need to
basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose choose a
classifier based on a training set data
size so again this is one of those
questions uh where you probably do not
have like a one siiz fit all on first of
all you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that that yes this is the
classifier that will work here no that's
not the right way so you need to still
uh you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion Matrix so the question
can be explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by taking an example and
drawing like a small diagram otherwise
it can really become tricky so my
suggestion is to take a piece of pen and
paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and it
is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many no so you have that
information and you have the predicted
values how many yes and how many no
right so the total actual values the
total yes is 12 + 11 13 and they are
shown yeah and the actual value no are 9
+ 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and uh out
of this what is the meaning convey so
there are two or three things that needs
to be explained outright the first thing
is for a model to be accurate the values
across the diagonal should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 + 3 15 + 10 25 so
that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let's me uh
take a pen so your your diagonal values
is equal to if I say d is equal to 12 +
9 so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 20
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw a diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and Pen
otherwise it will be pretty difficult to
to explain this so we use the same
example of the confusion Matrix and and
uh we can explain that so A confusion
Matrix looks somewhat like this and um
when we just take yeah it look somewhat
like this and we continue with the
previous example where this is the
actual value this is the predicted value
and uh in the actual value we have 12 +
1 13 yeses and 3 + 9 12 Nos and the
predicted values there are 12 + 3 15
yeses and uh 1 + 9 10 NOS okay now this
particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predicted value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system has predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
all positive values this entire row is
positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive positive and
the actual value is also positive okay I
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predicted value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row corresponds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and and
the actual value is this one right this
is predicted negative and the actual
value is also negative therefore this is
a true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right N9 is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing uh machine
learning model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model model and then
you use the test data set to check the
accuracy whether it is working fine or
not so you test the model before you
actually put it into production right so
once you test the model you're satisfied
it's working fine then you go to the
next level which is putting it for
production and then in production
obviously new data will come and uh the
inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values whether it is
regression or classification now so this
can be an iterative process so it is not
a straightforward process where you do
the training do the testing and then you
move it to production no so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you're are defining deep
learning so first is of course deep
learning is a subset of machine learning
so machine learning is still the bigger
let's say uh scope and deep learning is
one one part of it so then what exactly
is the difference deep learning is
primarily when we are implementing these
our algorithms or when we are using
neural networks for doing our training
and classification and regression and
all that right so when we use neural
network then it is considered as deep
learning and the term deep comes from
the fact that you can have several
layers of neural networks and these are
called Deep neural networks and
therefore the term deep you know deep
learning uh the other difference between
machine learning and deep learning which
the interviewer may be wanting to hear
is that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and uh this data has several let's say
if it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have sensus data and we
have all the so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many features and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking prority of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and uh the problem
is divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural
length so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
highend machines uh because it needs a
lot of computing power and the Machine
learning features are or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep planning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the the
meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
will for supervised machine learning the
very common one is email spam detection
so you want to train your application or
your system to detect between spam and
non span so this is a very common
business application of supervised
machine learning so how does this work
the way it works is that you obviously
have historical data of of your emails
and they are categorized as spam and not
spam so that is what is the labeled
information and then you feed this
information or the all these emails as
an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use use that to train
your model right so this is one example
now there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it be MRI or any
of these images are available and they
are tacked saying that okay this x-ray
image is defective of the person has an
illness or it could be cancer whichever
illness right so it is tacked as
defective or clear or good image and
defective image something like that so
we come up with the binary or it could
be multiclass as well saying that this
is defective to 10% this is 25% and so
on but let's keep it simple you can give
an example of just a binary
classification that would be good enough
so you can say that in healthcare
Diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether whether this person is having
that illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised and so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the labeled data is
not that easy because there's a lot of
effort in labeling this data the let's
take an example of the diagnostic images
we can just let's say take X-ray images
now there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
effective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
and uh then say that okay yes this image
is clean and this image is cancerous and
so on and so forth now that is a huge
effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no uh quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying laptops are also buying the mouse
so that means there is an association
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and unised
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labeled data now
very often people say yeah we have
historical data and if you we run it it
is supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we put in that keyword
labeled okay so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and dedu inductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person a video that
fire can burn the fing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN and another is K
means KNN stands for K nearest neighbors
and K means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them K andn is completely
different K means clustering is
completely different KNN is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a unsup
provise okay when you have KN andn when
you want to implement KN andn which is
basically K nearest neighbors the value
of K is a number so you can say k is
equal to 3 you want to implement KN andn
with K is equal to 3 so which means that
it performs the classification in such a
way that how does it perform the
classification so it will take three
nearest objects and that's why it's
called nearest neighbor so basically
based on the distance it will try to
find out its nearest objects that are
let's say three of the nearest nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular class but it is possible that
they may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign a odd number to so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K as
three the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to class one and one belong
to class two whereas when you take five
of them it is possible that only two of
them belong to class one and three of
them belong to class two so which means
that this object will belong to class
two right so you see that so this the
class allocation can vary depending on
the value of K now K means on the other
hand is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to some of their features
okay and but the similarity of course is
the the letter K and in case of K means
also we specify its value and it could
be three or five or seven there is no
technical limit as such but it can be
any number of clusters that uh you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects so
there is a similarity to that extent
that K is is a number in both the cases
but actually these two are completely
different processes we have what is
known as n based classifier and people
often get confused thinking that naive
base is the name of the person who found
this uh classifier or who develop this
classifier which is not 100% true base
is the name of the person b y is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classifier n based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of one feature
of a class is not related to the
presence of any other feature of maybe
other classes right so which is not a
very strong or not a very what do you
say accurate assumption because this
features can be related and so on but
even if we go with this assumption this
whole algorithm works very very well
even with this assumption and uh that is
the good side of it but the term comes
from there so that is the explanation
that you can give then there can be
question around reinforcement learning
it can be paraphrased in multiple ways
one could be can you explain how a
system can play a game of chess using
reinforcement learning or it can be any
game so the best way to explain this is
again to talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain in the
process so first of all reinforcement
learning has an environment and an agent
and the agent is basically performing
some actions in order to achieve a
certain goal and these goals can be
anything either if it is related to game
then the goal could be that you have to
score very high score high value High
number or it could be that your number
of lives should be as high as possible
don't lose lives so these could be some
of them more Advanced examples could be
for driving in the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the deaths for example it is rewarded
and every time it takes a step which
goes against that go right contrary or
in the reverse Direction it is penalized
okay so it is like a carrot and stick
system now how do you use this to create
a game of chess or to create a system to
play a game of chess now the way this
works is and this could probably go back
to this alphao example where alphao
defeated a human Champion so the way it
works is in reinforcement learning the
system is allowed for example if in this
case we're talking about Chess so we
allow the system to first of all watch
playing a game of chess so it could be
with a human being or it could be the
system itself there are computer games
of Chess right so either this new
learning system has to watch that game
or watch uh human being play the game
because this is reinforcement uh
learning is pretty much all visual so
when you're teaching the system to play
a game the system will not actually go
behind the scenes to understand the
logic of your software of this game or
anything like that it is just visually
watching the screen and then it learns
okay so reinforcement learning to a
large extent works on that so you need
to create a mechanism whereby your model
will be able to watch somebody playing
the game and then you allow the system
also to start playing the game so it
pretty much starts from scratch okay and
as it moves forward it it it's at right
at the beginning the system really knows
nothing about the game of chess okay so
initially it is a clean slate it just
starts by observing how you're playing
so it will make some random moves and
keep losing badly but then what happens
is over a period of time so you need to
now allow the system or you need to play
with the system not just 1 2 3 four or
five times but hundreds of times
thousands of times maybe even hundreds
of thousands of times and that's exactly
how alpha go has done it played millions
of games between itself and the system
right so for the game of chess also you
need to do something like that you need
to allow the system to play chess and
then learn on its own over a period of
repetition so I think you can probably
explain it to this much to this extent
and it should be uh sufficient now this
is another question which is again
somewhat similar but here the size is
not coming into picture so the question
is how will you know which machine
learning algorithm to choose for your
classification problem now this is not
only classification problem it could be
a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision Tri you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine works so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously something like uh you know
Amazon website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
Association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
gender and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine work all right
then the question can be uh something
very basic like when will you go for
classification versus regression right
when do you do classification instead of
regression or when do you use class
classification instead of regression now
yes so so this is basically going back
to the understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values and other is the
continuous values so for regression if
you are trying to find some continuous
values you use regression whereas if
you're trying to find some discrete
values you use classification and then
you need to illustrate what are some of
the examples so classification is like
let's say there are images and you need
to put them into classes like cat dog
elephant tiger something like that so
that is a classific ification problem or
it can be that is a multiclass
classification problem it could be
binary classification problem like for
example whether a customer will buy or
he will not buy that is a classification
binary classification it can be in the
weather forecast area now weather
forecast is again combination of
regression and classification because on
the one hand you want to predict whether
it's going to rain or not that's a
classification problem that's a binary
classification right whether it's it's
going to rain or not rain however you
also have to predict what is going to be
the temperature tomorrow right now
temperature is a continuous value you
can't answer the temperature in a yes or
no kind of a response right so what will
be the temperature tomorrow so you need
to give a number which can be like 20
30 or whatever right so that is where
you use regression one more example is
stock price prediction so that is where
again you will use regression so these
are the various examples so you need to
illustrate with examples and make sure
you include those keywords like discrete
and continuous so the next question is
more about a little bit of a design
related question to understand your
Concepts and things like that so it is
how will you design a spam filter so how
do you basically design or develop a
spam filter so I think the main thing
here is he's looking at probably
understanding your Concept in terms of
uh what is the algorithm you will use or
what is your understanding about
different difference between
classification and regression uh and
things like that right and the process
of course the methodology and the
process so the best way to go about
responding to this is we say that okay
this is a classification problem because
we want to find out whether an email is
a spam or not spam so that we can apply
the filter accordingly so first thing is
to identify what type of a problem it is
so we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector missions for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and uh
then you split that into training and
test data sets you use your training
data set to train your model that or
your algorithm that you have used or
rather the model actually so and you
actually will have three models let's
say you are trying to test out three
algorithms so you will obviously have
three models so you need to try all
three models and test them out as well
see which one gives the best accuracy
and then you decide that you will go
with that model okay so training and
test will be done and then you zero in
on one particular model and then you say
okay this is the model will we use we
will use and then go ahead and Implement
that or put that in production so that
is the way you design a Spam F the next
question is about random Forest so what
is random Forest so this is a very
straightforward question however the
response you need to be again a little
careful while we all know what is random
Forest explaining this can sometimes be
tricky so one thing is random Forest is
kind of in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and uh trees will basically we
will use for doing if it is
classification mostly it is
classification you will use the the
trees for classification and then you
use voting for finding the the final
class so that is the underly but how
will you explain this how will you
respond to this so first thing obviously
we will say that random Forest is one of
the algorithms and the more important
thing that you need to probably the
interviewer is is waiting to hear is
Ensemble learner right so this is one
type of Ensemble learner what is
Ensemble learner Ensemble learner is
like a combination of algorithms so it
is a learner which consists of more than
one algorithm or more than one maybe
models okay so in case of random Forest
algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a random Forest is an ensemble
learner there are other types of
Ensemble Learners where we have like we
use different algorithms itself so you
have one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
inter you will be happy to hear this
word Ensemble Learners and so then you
go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees okay and
each observation you pass through this
forest and each observation let's say it
is a classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and 10 of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way random Forest works for
classification if it is a regression
problem it's somewhat similar but only
thing is instead of vot what we will do
is so in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and uh you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
100 trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the the 100 trees right
so this is also a way of reducing the
error so maybe if you have only one tree
and if that one tree makes error it is
basically 100% wrong or 100% right right
but if you have on the other hand if you
have a bunch of trees you are basically
mitigating that error or reducing that
error okay so that is the way random
Forest works so the next question is
considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to outright say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a one size
fits all so we need to first of all you
can probably shorten the list in terms
of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably uh shorten the list because
you don't have to use all of them if it
is a classif ification problem you only
can pick from the classification
algorithms right so for example if it's
a classification you cannot use linear
regression algorithm there or if it is a
regression problem you cannot use svm or
maybe no you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100%
decide on saying that this is the exact
algorithm that I'm going to use so the
way to go about is you choose a few
algorithms based on the problem is you
try out your data you train some models
of these algorithms check which one
gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then there can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example bias in machine learning it
occurs when the the predicted values are
far away from the actual value so that
is a bias okay and whereas they are all
all the values are probably they are far
off but they are very near to each other
though the predicted values are close to
each other right while they are far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance now variance is when the
predicted values are all over the place
right so the variance is high that means
it makes be close to the Target but it
is kind of very scattered so the points
the predicted values are not close to
each other right in case of buyers the
predicted values are close to each other
but they are not close to the Target but
here they may be close to the Target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
tradeoff between bias and variance yes I
think this is a interesting question
because these two are heading in
different directions so for example if
you try to minimize the bias variance
will keep going high and if you try to
minimize the variance buas will keep
going high and there is no way you can
minimize both of them so you need to
have a tradeoff saying that okay this is
the level at which I will have my bias
and this is the level at which I will
have variance so the trade-off is that
pretty much uh that you you decide what
is the level you will tolerate for for
your buyers and what is the level you
will tolerate for variance and a
combination of these two in such a way
that your final results are not we off
and having a tradeoff will ensure that
the results are consistent right so that
is basically the output is consistent
and which means that they are close to
each other and they are also accurate
that means they are as close to the
Target as possible right so if either of
these is high then one of them will go
off the track Define precision and
Recall now again here I think uh it
would be best to draw a diagram and take
uh the confusion Matrix and it is very
simple the definition is like a formula
your Precision is true positive by true
positive plus false positive and your
recall is true positive by true positive
plus false negative okay so that's you
can just show it in a mathematical way
that's pretty much uh you know that can
be shown that's the easiest way to
define so the next question can be about
decision tree what is decision tree
pruning and why is it so basically
decision trees are really simple to
implement and understand but one of the
drawbacks of decision trees is that it
can become highly complicated as it
grows right and the rules and the
conditions can become very complicated
and this can also lead to overfitting
which is basically that during training
you will get 100% accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose or the
reason for doing decision tree pruning
is to reduce overfitting or to cut down
on overfitting and what is decision tree
pruning it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are are several
internal nodes and then you have the
leaf nodes now if there are too many of
these internal nodes that is when you
face the problem of overfitting and
pruning is the process of reducing those
internal nodes all right so the next
question can be what is logistic
regression uh so basically logistic
regression is um one of the techniques
used for performing classification
especially binary classification now
there is something special about
logistic regression and there are a
couple of things you need to be careful
about first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if if
it's really you know if it is required
and they can also ask this like a trick
question right so that is one part
second thing is the term logistic has
nothing to do with the use ual Logistics
that we talk about but it is derived
from log so that the mathematical
derivation involves log and therefore
the name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between 0er
and one and we can set a threshold that
can vary typically it is 0.5 so any
value above 0.5 is considered as one and
if the probability is below5 it is
considered as zero so that is the way we
calculate the probability or the system
calculates the probability and based on
the threshold it sets a value of zero or
one which is like a binary
classification Z or one okay then we
have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is a k nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that'll be the best way so draw
some diagram like this and let's say we
have three clusters or three classes
existing and now you want to find for a
new item that has come you want to find
out which class this belongs to right so
you go about as the name suggests you go
about finding the nearest name neighbors
right the points which are closest to
this and how many of them you will find
that is what is defined by K now let's
say our initial value of K was five okay
so you will find the K the five nearest
data points so in this case as it is
Illustrated these are the five nearest
data points but then all five do not
belong to the same class or cluster so
there are one belonging to this cluster
one the second one belonging to this
cluster two three of them belonging to
this third cluster okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
a odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as I
was talking about right so we assign it
to this third class so in this case it
is uh that's how it is decided okay so K
nearest neighbor so first thing is to
identify the number of neighbors that
are mentioned as K so in this case it is
K is equal to five so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that okay and and then the uh
new data point is assigned to that class
okay so that's pretty much how K nearest
neighbors work and with that we have
come to the end of this machine learning
python full course I hope you found it
valuable and entertaining please ask any
questions about the topics covered in
this video in the comment box below and
a team of experts will assist you in
addressing your problems as soon as
possible so thank you so much for being
here today we'll see you next time until
then keep learning and stay tuned to
Simply learn staying ahead in your
career requires continuous learning and
upscaling whether you're a student
aiming to learn today's top skills or a
work working professional looking to
advance your career we've got you
covered explore our impressive catalog
of certification programs in Cutting
Edge domains including data science
cloud computing cyber security AI
machine learning or digital marketing
designed in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to career your success click the
link in the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to n up and get certified click
here