welcome to this AI cost for beginners AI
has been rapidly evolving from the half
of this decade growing field bring the
plenty of opportunities AI Engineers AI
Specialists ml Engineers are in high
demand and tend to earn handsome salary
packages on an average AI engineer earns
up to
$157,000 per year so if you are looking
to step into this amazing field this
course will be perfect to get the basic
knowledge of AI and its algorithms with
Hands-On demo so without any further
delay over to our tutors and before that
if you one of the aspiring Ai and ml
Enthusiast looking for online training
and graduating from the best
universities or a professional who
elicits to switch careers in Ai and ml
by learning from the experts and try
giving a short to Simply learns
postgraduate program in Ai and machine
learning in collaboration with bu
University and IBM the course link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered and we also
have a boot camp for you guys and if you
are one of the aspiring Ai and ml
Enthusiast looking for online training
and graduating from the best
universities or a professional who
elicits to switch careers in Ai and ml
by learning from the experts then try
giving a short to Simply learns celtech
Ai and machine learning boot camp the
course link or the boot camp link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered now let's
take a minute to hear from our Learners
who have experienced massive success by
opting out to these courses and
bootes being in the IT industry for 20
plus years has taught me one thing you
need to keep updating your skills on a
regular basis no matter what level you
at I recently completed the professional
certificate program in Ai and machine
learning from Simply learn in
partnership with P University and I must
say it was very insightful I wanted to
have a better understanding of AI and ml
to effectively manage a design and
development of the products of the
company and this course has helped me do
exactly that hi hi I'm Hussein Ari mza I
live in San Jose California I moved to
the US around 20 years back throughout
my career I have worked at large
organizations as well as startups right
now I work as vice president of product
and customer operations at window
Digital Services and AI based startup in
Silicon Valley I spent my time in
between two products one is AI based
chatbot which is focused at auto
dealerships the second is a video
compression Technology based on AI and
ml as a senior professional this course
has helped me to more effectively manage
AI and ml projects in my current
organization I have also gained a better
Hands-On knowledge with a better
understanding of the Practical
possibilities this helps me work more
efficiently with my team while we are
planning and implementing our products
and services overall I had a wonderful
experience with simply learn their
course material was comprehensive and
the faculty was extremely experienced uh
The Faculty was able to adjust their
teaching style in order to cater to the
over skill set of the class industry
relevant projects are based on real life
scenarios this helps at reinforcing a
practical aspects of AI ML and helps to
correlate it with real life projects
which I work with on an everyday basis
when I am not working I love to travel
and experience Outdoors especially
hiking I also enjoy music movies hanging
out with friends I feel downtime when
you are not working is very important in
order to make your work life life a lot
more effective in the rapidly evolving
world of technology it's important to
keep upskilling for every working
professional stay relevant continue
learning picture this a machine that
could organize your cupboard just as you
like it or serve every member of the
House a customized cup of coffee makes
your day easier doesn't it these are the
products of artificial intelligence but
why use the term artificial intelligence
well these machines are artificial
Incorporated with humanlike intelligence
to perform tasks as we do this
intelligence is built using complex
algorithms and mathematical functions
but AI may not be as obvious as in the
previous examples in fact AI is used in
smartphones cars social media feeds
video games banking surveillance and
many other aspects of our daily life the
real question is what does an AI do at
its core here is a robot we built in our
lab which is now dropped onto a field in
spite of a variation in lighting
landscape and dimensions of the field
the AI robot must perform as expected
this ability to react appropriately to a
new situation is called generalized
learning the robot is now at a crossroad
one that is paved and the other Rocky
the robot must determine which path to
take based on the circumstances this
portrays the robot ot's reasoning
ability after a short stroll the robot
now encounters a stream that it cannot
swim across using the plank provided as
an input the robot is able to cross this
stream so our robot uses the given input
and finds the solution for a problem
this is problem solving these three
capabilities make the robot artificially
intelligent in short AI provides
machines with the capability to adapt
reason and provide
Solutions well now that we know what AI
is let's have a look at the two broad
categories and AI classified into weak
AI also called narrow AI focuses solely
on one task for example alphago is a
maestro of the game go but you can't
expect it to be even remotely good at
chess this makes alphago a weak AI you
might say Alexa is defin Ely not a weak
AI since it can perform multiple tasks
well that's not really true when you ask
Alexa to play despacito it picks up the
key words play and despacito and runs a
program it is trained to Alexa cannot
respond to a question it isn't trained
to answer for instance try asking Alexa
the status of traffic from work to home
Alexa cannot provide you this
information as she is not trained to
and that brings us to our second
category of AI strong AI now this is
much like the robots that only exist in
fiction as of now Ultron from Avengers
is an ideal example of a strong AI
That's because it's self-aware and
eventually even develops emotions this
makes the ai's response
unpredictable You Must Be Wondering well
how is artificial intelligence different
from machine learning and deep learning
we saw what AI is machine learning is a
technique to achieve Ai and deep
learning in turn is a subset of machine
learning machine learning provides a
machine with the capability to learn
from data and experience through
algorithms deep learning does this
learning through ways inspired by the
human brain this means through deep
learning data and patterns can be better
perceived Ray kheel a well-known
futurist predict predicts that by the
year 2045 we would have robots as smart
as humans this is called the point of
Singularity well that's not all in fact
Elon Musk predicts that the human mind
and body will be enhanced by AI implants
which would make us partly cyborgs
artificial intelligence is reshaping
Industries and transforming the wav
leavea and work it encompasses a wide
range of Technologies including machine
learning deep learning natural language
processing computer vision and many more
with its ability to analyze vast amount
of data and make Intelligent Decisions
AI has become a game changer across
various domains hi everyone welcome to
Simply learns YouTube channel this AI
road map guides you in navigating the
part towards building a successful
career in artificial intelligence the
ultimate goal of artificial intelligence
is to create intelligent missions that
can perform complex task exhibit humanik
intelligence and contribute positivity
to society Yi presents exciting career
opportunity in various Industries and
sectors roles like AI engineer data
scientist NLP Engineers computer vision
Engineers AI research scientists robotic
engineers and many more offer exciting
prospects for working with cutting ITS
Technologies and making an impact
through artificial intelligence
Innovation according to glass door the
average reported salary of an AI
engineer in the United States is around
$115,000 per year however in India it is
10 lakh per anom leading companies
worldwide are fully aware of the immense
value of AI and are actively pursuing
skilled AI Engineers to contribute to
their research development and
implementations of AI technology among
this top companies are Google Microsoft
Amazon Goldman apple and JP Morgan Chase
therefore top companies hiring AI
engineer provide excellent opportunity
for aspiring professionals on that note
elevate your career with our Ai and ml
course developed in partnership with P
University and IBM gain expertise in
high demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning chat GPT
generative Ai explainable Ai and more
Unleash Your Potential and unlock
exciting opportunity in the world of AI
and ml this course cover tools like
python tensor flow Kos CH GPT open AI gy
M plot LA and many more so hurry up and
find the course Link in the description
box for more details our Learners have
experienced huge success in their
careers listen to their experience find
the simply learn Course Review Link in
the description box without any further
delay let's dive into the topic now if
you desire to become an AI engineer here
are the steps you can follow to achieve
your goal firstly obtain a strong
foundation in mathematics and
programming gain a strong foundation in
creating mathematic Concepts like linear
algebra calculus and probability theory
in addition to that it is crucial to ur
Mastery of a programming languages like
python commonly used in Ai and become
proficient in coding then earn a degree
in a relevant field that means earn a
bachelor's or master degree in computer
science data science AI or a related
field to gain a comprehensive
understanding of AI principles and
techniques next gain knowledge in
machine learning and deep learning to
become a AI engineer develop familiarity
with ML algorithms neural networks and
deep learning Frameworks example tensor
Flow by torch to train and optimize
models using real world data set
followed by that work on practical
projects gain hands-on experience and
showcase your skills by working on AI
projects moreover build a portfolio of
projects demonstrating your ability to
solve eii problems impressing potential
employers then stay updated with the
latest advancements stay updated with
the latest trends and research in AI a
rapidly involing field and expanding
your knowledge by reading research
papers participating in online courses
and workshops and joining AI communities
next collaborate and network engage with
AI communities attend conference
participate in online forums to connect
with Professionals in this field
collaborating with others can enhance
your learning experience and open new
opportunities later seek internships or
entry-level positions gain practical
experience through AI internships or
entry-level roles in industry or
research institutions this provides
valuable exposures and help develop your
skills finally continuously learn and
adapt in the rapidly involving field of
AI stay updated on new developments
explore specialized areas and embrace
emerging Technologies and tools to
pursue a career as an AI engineer now
that we have covered the essential steps
to become an AI engineer let's explore
the necessary programming languages and
algorithms for aspiring EI Engineers so
mastering programming languages like
python R Java and C++ is vital for
acquiring Proficiency in AI this
languages enables you to construct a
deploy models effectively
additionally it would help if you gain a
thorough understanding of machine
learning algorithms such as linear
regressions K nearest neighbors na base
and support Vector missions this
languages and algorithms are fundamental
tools in the field of AI they will
enable you to develop and Implement
effective artificial intelligence models
hello everyone and Welcome to our video
on the skills required for an AI
engineer with the Advent of artificial
intelligence and its Superior abilities
to improve human life the need and
demand for expert a a professional is at
an all-time high this expanding demand
has led to a lot of people applying for
AI jobs and upskilling themsel in the
field of AI but doing this is just like
playing a game but not knowing its rules
for someone to become an AI professional
and learn a job like an AI engineer they
first have to understand the demand from
this role and what skills are expected
of an AI engineer so if you're someone
who wishes to become an AI engineer or
if you wish to upgrade your skills in Ai
and get promoted as an AI engineer then
make sure you was watch this amazing
video till the very end in this video we
will be breaking down in complete detail
each and every skill that you would need
in order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
Trend it's a sesmic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing
boundaries and making a profound impact
or Society the demand for AI
professional is scating across
industries from healthare and finance to
entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what is skills does it take
to become an AI engineer how can you
embark on the thrilling Journey we have
answered to all of your questions also
accelerate your career in AI ml with a
comprehensive postgraduate program in a
machine learning gain expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything session by AB with
three Capstone project and 25 plus
industry projects using real data set
from Twitter Uber and many more you will
gain practical experience master classes
by ctech faculty and IBM expert ensure
topnotch education simply learn job
assist help you to get notice by Leading
companies this program covers statistic
python
supervised and unsupervised learning NLP
neural network computer vision G caras
tensorflow and many more skills so
enroll now and unlock exciting AI ml
opportunities the link is in the
description box below so without any
further delay let's get started some
steps are crucial to master in the field
of AI and becoming an AI engineer let's
go through them real quick first
establish a strong foundation in
mathematics and programming start by
gaining a solid understanding of
critical Concepts like linear algebra
calculus and probability Theory
additionally it is crucial to become
proficient in programming language like
python which is commonly used in Ai and
developed coding skills and the next one
is pursue a degree in relevant field
earn a bachelor or master degree in
computer science data science or AI or
related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensor flow
and Pie torch to train and optimize
models using real world data sets
afterward engage in Practical projects
gain hands-on experience and demonstrate
your skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry-level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fastp world of
AI it is crucial to stay updated on new
developments explore specialized areas
and embrace emerging Technologies and
tools continuous learning and
adaptability are essential for purching
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an AI
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is
$115,000 per year however in India it is
10 lakh perom so these figures are way
better than the average selling figures
of any job road so let's discuss the
skills you need to become an AI engineer
you should have a strong background in
data science and machine learning here
is a breakdown of your skills number one
strong programming abilities the
typically refers to expertise in one or
more programming languages commonly used
in data science and machine learning
such as python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm
implementation and the second one is
knowledge of machine learning algorithms
this involves understanding and
familiarity with the wide range of
machine learning algorithms including
both supervised and unsupervised
learning you should be able to select
and apply appropriate algorithm for
specific problem as well as evaluate and
optimize their performance and the third
one is Proficiency in stat statistic and
Mathematics sound knowledge of
statistics and Mathematics is
fundamental for data analysis and
machine learning you should be
comfortable with statical Concept
hypothesis testing regression analysis
probability Theory lar algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensor flow P torch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as Apachi Hadoop or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed and envirment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be Adept
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the field and continuing learning to
stay at the Forefront of data science
and machine learning hello everyone and
Welcome to our video on the skills
required for an AI engineer with the
Advent of artificial intelligence and
its Superior abilities to improve human
life the need and demand for expert AI
professional is at 10 all-time high this
expanding demand has led to a lot of
people applying for AI jobs and
upskilling themsel in the field of AI
but doing this is just like playing a
game but not knowing its rules for
someone to become an AI professional and
learn a job like an AI engineer they
first have to understand the demand from
this role and what skills are expected
of an AI engineer so if you are someone
who wishes to become an AI engineer or
if you wish to upgrade your skills in Ai
and get promoted as an AI engineer then
make sure you watch this amazing video
till the very end in this video we will
be breaking down in complete detail each
and every skill that you would need in
order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
Trend it's a sesmic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing
boundaries and making a profound impact
on society the demand for AI
professional is scating across
industries from healthare and finance to
entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skills does it take to
become an AI engineer how can you embark
on the thrilling Journey we have
answered to all of your questions also
accelerate your career in AI ml with
Comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything session by IIA with
three Capstone project and 25 plus
industry projects using real data set
from Twitter Uber and many more you will
gain practical experience master classes
by ctech faculty and IBM expert ensure
topnotch education simply learn job
assist help you to get noticed by
Leading companies this program covers
statistic Python supervis and and
supervised learning NLP neural network
computer vision G caras tensorflow and
many more skills so enroll now and
unlock exciting AI anml opportunities
the link is in the description box below
so without any further delay let's get
started some steps are crucial to master
in the field of AI and becoming an AI
engineer let's go go through them real
quick first establish a strong
foundation in mathematics and
programming start by gaining a solid
understanding of critical Concepts like
linear algebra calculus and probability
Theory additionally it is crucial to
become proficient in programming
language like python which is commonly
used in Ai and developed coding skills
and the next one is pursue a degree in
relevant field earn a bachelor or master
degree in computer science data science
or AI or Rel discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensor flow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fastp world of
AI it is crucial to stay updated on new
developments explore specialized areas
and embrace emerging Technologies and
tools continuous learning and
adaptability are essential for pursuing
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an AI
engineer
so according to glass door the average
reported salary of an AI engineer in the
United States is $15,000 per year
however in India it is 10 lakh perom so
these figures are way better than the
average selling figures of any job road
so let's discuss the skills you need to
become an AI engineer you should have a
strong background in data science and
machine learning here is a breakdown of
your skills number one strong
programming abilities the typically
refers to expertise in one or more
programming languages commonly used in
data science and machine learning such
as Python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm implen
and the second one is knowledge of
machine learning algorithms this
involves understanding and familiarity
with a wide range of machine learning
algorithms including both supervised and
unsupervised learning you should be able
to select and apply appropriate
algorithm for specific problem as well
as evaluate and optimize their
performance and the third one is
Proficiency in statistic and Mathematics
sound knowledge of statistics and
Mathematics is fundamental for data
analysis and machine learning you should
be comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popular in recent years and
familiarity with deep learning
Frameworks like tensor flow P torch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as Apache Hadoop or
Apache spark or or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be Adept
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data sense and machine
learning remember to stay updated with
the latest advancement in the field and
continuing learning to State at the
Forefront of data science and machine
learning machine learning is
transforming Industries and driving
Innovation by enabling systems to learn
from data and make Intelligent Decisions
this has resulted in a skyrocketing
demand for machine learning Engineers
these professionals possess the skills
to develop Advanced algorithms well
productive models and extract valuable
insights from vast amounts of data the
ARA of Big Data necessitates
professionals who can derive insights
from vast information and machine
learning Engineers possess the skills to
analyze complex data sets extract
valuable knowledge and build predictive
models for accurate forecast process
Automation and optimized decision making
Industries such as Healthcare Finance
e-commerce and autonomous vehicles
heavily rely on machine learning to
enhance processes and drive growth as
businesses increasingly recognize the
power of data driven insights the demand
for skilled machine learning Engineers
continues to search they are
instrumental in Revelation the potential
of machine learning to optimize
operations improve efficiency and gain a
Competitive Edge with the market value
of machine learning expected to grow
exponentially now is the time to enter
into the field of machine learning and
become an ml engineer who are at the
Forefront of technological advancement
and Bing lucrative career opportunities
in this Dynamic field that opens doors
to a world of endless possibilities and
Promises of fulfilling and impactful
career on that note hello everyone
welcome to Simply learn today we will
explore a profession that has witnessed
a huge surge in demand in recent years
which is going to be the next big thing
in the future can you guess which
profession we talking about well you got
it right that's machine learning
engineer having said that if you want to
Embark your career as an ml engineer
then our postgraduate programing ml can
be the right option for you this anml
postgraduate program by simply learn in
collaboration with PUO University covers
a wide range of topics and provides
hands-on experience in developing a and
machine learning solutions for
professionals who want to upgrade their
skills and prer to a career in the field
of AI and machine learning
this course features master classes by
Purdue faculty and IBM experts exclusive
hackathons to help you master various
skills like statistics python supervised
learning neural networks NLP and much
more by covering tools and techniques
like numpy pandas python scipi along
with amazing industry projects so what
is stopping you from building your
future in& ml enroll now course link is
added in the description box below make
sure to check that out without any
further Ado let's get started me meet
John who has been working as a software
developer in the IT industry for over 5
years now John has been hearing a lot
about machine learning lately and how it
has been revolutionizing the tech
industry he wonders whether learning
machine learning would be worth it and
that's when his colleague Harry helps
him out hey harry I've been reading a
lot about machine learning engineering
lately what are your thoughts absolutely
machine learning is booming right now
really quite intriguing but how can I
become an ml engineer well with the
right skill set you can my friend
possibilities are endless wow that's
impressive looks like machine learning
can be a rewarding career with great
opportunities and lucrative salaries
absolutely John so let me tell you more
about the working aspects of machine
learning engineer in detail we'll start
this tutorial with a quick introduction
to who ml engineer is and next we'll
understand what exactly does a machine
learning engineer do next we'll dive
into the roles and responsibilities of a
machine learning engineer after that
we'll discuss some of the most important
skills that an ml engineer should
possess and finally we'll talk about the
salary and job toob prospects of a
machine learning engineer so firstly who
is a machine learning engineer a machine
learning engineer is a skilled
professional who combines experties in
programming mathematics and machine
learning to develop and deploy
intelligent systems that can learn from
data and make accurate predictions or
decisions the role of a machine learning
engineer involves working with large
data sets pre-processing and cleaning
the data selecting appropriate machine
learning algorithms and training models
using this data they often collaborate
with data scientist and domain experts
like software Engineers to integrate
machine learning Solutions into
production environments for optimizing
and fine-tuning models for accuracy and
efficiency as well as evaluating their
performance and finally they perform a
thorough effective data exploration and
visualization techniques in order to
contribute for better understanding of
the data and identifying patterns making
informed decisions throughout the
machine learning pipelines so what are
the roles and responsibilities of a
machine learning engineer firstly
problem understanding and solution
design machine learning Engineers work
closely with stakeholders domain experts
to understand the business problem or
objective that machine learning can
address they analyze requirements
identify suitable data sources and
propose machine learning based Solutions
this involves translating business
problems into technical specifications
defining measurable goals and designing
the overall architecture of the machine
Learning System data preparation and
Analysis machine learning Engineers need
to have a deep understanding of the data
they're working with this involves
cleaning pre-processing and transforming
raw data into a format suitable for
training and evaluating ml models
exploratory data analysis techniques are
often used to gain insights and
understand the underlying patterns and
relationships within the data model
development and evaluation ml Engineers
are responsible for developing training
and fine-tuning machine learning models
this involves selecting the appropriate
algorithms architectures and techniques
based on the problems at hand you will
need to implement an experiment with
different models such as decision trees
neural networks support Vector machines
and much more and finally monitoring
maintenance and iterative Improvement
after deployment ml Engineers monitor
the performance of deployed models and
address any sort of issues that arise
they design and Implement monitoring
systems to track model performance
detect any kind of anomalies and
identify data drift or concept drift
regular model maintenance is necessary
to retrain and refine models using new
data to maintain accurate and
adaptability so these are some of the
day-to-day roles and responsibilities of
an ml engineer well let us now talk
about the skill set that a ml engineer
should possess firstly they should have
a strong knowledge on programming tools
now strong programming skills are
essential for machine learning Engineers
programming languages like R which is a
free software environment for
statistical Computing and Graphics can
surely help but mainly they use python
python is widely used in the machine
Learning Community due to its simplicity
extensive libraries and strong ecosystem
so you need to familiarize yourself with
python fundamentals data manipulation
libraries like numai and pandas and
machine learning Frameworks such as
tensorflow pyo and psychic learn also
familiarize yourself with programming
Concepts data structures and algorithms
and additionally knowledge of other
languages like R and Java can be
beneficial secondly machine learning and
deep learning Concepts ml Engineers
should have a deep understanding of
various Ma machine learning algorithms
including supervised learning
unsupervised learning and reinforcement
learning familiarize yourself with
Concepts like KNN linear regression
decision trees support Vector missions
neural networks Ensemble methods and
much more not just that furthermore ml
Engineers need a solid foundation in
mathematics and statistics this includes
knowledge of linear algebra calculus
probability Theory and statistical
analysis understanding these Concepts is
crucial for Designing and implementing
machine learning algorithms now in
addition to these technical skills
machine learning Engineers should also
possess strong analytical and soft
skills to excel in their roles they need
to think critically and out of the box
to understand problems break them down
into manageable components and develop
appropriate machine learning solutions
they should be able to analyze data
identify patterns and make informed
decisions based on the results and
finally communication skills now
effective communication is crucial for
machine learning Engineers they need to
expand complex technical Concepts to
non-technical stakeholders as well while
collaborating with cross functional
teams and present their findings and
recommendations strong written and
verbal communication skills will surely
help convey information clearly and
concisely which not only enhance their
professional growth but also contribute
to their overall effectiveness as a
machine learning engineer with all that
having said let us now discuss some of
the salary and job prospects of a
machine learning engineer now the salary
of machine learning engineer can vary
significantly based on V various factors
in United States of America machine
learning Engineers are in high demand
particularly in technology hubs like
Silicon Valley in California and Tech
hubs like New York according to a report
by glass door the average salary of a
professional ml engineer is around
$19,000 per year now salaries in the US
tend to be higher due to the cost of
living and F competition for talent and
entry-level machine learning Engineers
can expect salaries ranging from $80,000
to $120,000 per year while mid-level
professionals with a few years of
experience can around $10,000 to
$180,000 per year and finally a senior
level or an experienced machine learning
engineer can command salaries exceeding
$200,000 per year as well especially in
Top tire companies or leadership
positions on the other hand in India the
salary range for machine learning
Engineers is relatively lower compared
to the US but it's important to consider
the lower cost of living as well now the
average salary of a machine Lear
learning engineer in India is around 8.5
lakhs while entry-level machine learning
engineers in India can expect salaries
ranging from 4 lakhs to 8 lakhs per year
and mid-level professionals can ear on
on somewhere between 8 lakhs to 15 lakhs
per year and finally senior level
machine learning Engineers with
extensive experience and expertise can
earn salaries ranging from 15 lakhs to
30 lakhs per year now additionally other
benefits and compensation components
such as bonuses stock options Healthcare
benefits and retirement plan should also
be considered but let's talk about
reality it's crucial to note that these
figures are approximate and can vary
based on various such factors so here
are some of the salary deciding factors
of an machine learning engineer firstly
company and location now the location of
the employing company can impact a
machine learning engineer salary large
technology companies or organizations
with a strong Focus on& ml tend to offer
competitive compensation packages to
attract top talent additionally the cost
of living in different Geographic areas
can affect salary ranges with salaries
often being higher intake hubs or cities
with a higher cost of living work
experience experience plays a
significant role in determining the
salary of an ml engineer generally
Engineers with more years of experience
tend to command higher salaries this is
because experience indicates a deeper
understanding of machine learning
Concepts techniques and real world
applications which makes experienced
Engineers more valuable to the employers
out there skill set and expertise now
specific skills and expertise possessed
by a machine learning engineer can
impact their salaries as well ml
Engineers proficient in popular
programming languages like python R Java
and Frameworks like tensorflow and py to
and librar such as psyit learn are often
in high demand and can negotiate higher
salaries additionally specialized
knowledge in areas like deep learning
natural language processing computer
vision can also be valuable and
potentially result in higher
compensation and finally the industry
that you're working on and the current
demand prends now the industry which an
machine learning engineer Works can
influence their salary as well
industries that heavily rely on machine
learning and AI applications such as
Finance Healthcare e-commerce and
autonomous vehicles offer higher salary
packages to machine learning Engineers
due to the high demand for their
expertise also it's important to note
that these relative importance of these
factors may vary depending on the
specific circumstances and job market
conditions salaries can also be
influenced by economic factors market
demand and individual negotiation skills
overall machine learning engineers in
both us India and the rest of the world
can earn competitive salaries and it's
essential to assess the overall
compensation package growth
opportunities and Company culture when
evaluating job
opportunities artificial intelligence is
a vast topic and it is something which
will have to master bit by bit however
there are many basic projects that you
can take up as a beginner in AI there
are plenty of resources on online but
sometimes as a beginner in AI doesn't
know where to start this video will help
you to get interesting ideas on projects
in AI hey everyone welcome to Simply
learn before moving on please make sure
you subscribe to Simply learn's YouTube
channel and press the Bell icon to never
miss any updates let us Define
artificial intelligence First artificial
intelligence is a technique of turning a
computer based robot to work and act
like humans now let me list out
important AI projects for you face
detection system it is a form of
biometric recognition a method for
identifying or confirming someone's
identity by glancing at their face is
called facial recognition people can be
identified by securing a match on facial
ID using this technique realtime visuals
videos and photos can be the sources to
run phase detection this technology is
mostly employed in SE security and law
enforcement open CV is the best
technology to create it a python package
called open CV is made specifically to
address computer vision jobs computer
vision is a process used in the
processing of images by computers it is
concerned with the in-depth
comprehension of digital photos or films
it is necessary to automate operations
that can be performed by human visual
systems therefore a computer should be
able to identify items like a statue or
a lamp poost or even the face of a human
being second one is chatbot it is the
best idea if you have chosen chatbot as
your project topic this will make your
resume more attractive in case you're
looking for a job an oracle survey
suggests that 80% of the businesses uses
chatbot you can use Python Java Ruby C++
or PHP p as a programming language to
develop a chatbot designing and building
NLP chatbots that accept speech and Text
data is made easier by dialog flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chatbot
regardless of how excellent your chatbot
is there is always room for development
the finest chatboard developers
constantly enhance their boards over the
time using Ai and machine learning
social media recommendation system the
rise of web services like Netflix Amazon
and YouTube has increased the use of
recommended systems in our daily lives
they are algorithms that insist users in
finding information that is pertinent to
them recommender systems are important
in some firms since they can generate a
lot of income or allow you to set
yourself apart from rivals in order to
provide recommendations it evaluates the
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirants one will love to develop stock
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can and provide them a
better understanding of various aspects
of the field coming to medical diagnosis
this project is advantageous from a
medical standpoint AI projects can be
created to detect heart-based diseases
and also detect cancer it is intended to
offer patients with heart illness online
medical advice and guidance after
processing the data the system will
search the database for any illnesses
that might be connected to the given
details using data mining techniques
this intelligent system determines the
disease that the patient's information
most closely
resembles based on the system diagnosis
users can then speak with qualified
medical professionals users of the
system can also view information about
various doctors coming to an important
project that is search engine search
engines are utilized by all we look for
information on the greatest product to
buy a nice area to hang out or Sol
solutions to any questions we have MLP
is quite significant in modern search
engines because a lot of language
processing takes place there python is
the widely used language to develop any
search engine search engine is mainly
confined with lots and lots of data it
is helpful for any AI career aspirant
next is virtual assistance here the
challenge is to build a virtual
assistant to assist user why do you need
virtual assistant assistant in your
devices when you are building your own
it is also interesting for a ml
developer to build a virtual assistant
it involves NLP and data mining
voice-based virtual assistants are
popular today because they make life
easier for users NLP is utilized to
comprehend human language in order to
construct this system when a voice
command is received the system will
translate it into machine language and
store the commands in its database hate
speech detection in social media
automated hate speech detection is a
crucial weapon in the fight against hate
speech propagation especially on social
media for the job many techniques have
been developed including a recent
explosion of deep learning based system
for the objective of detecting hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning learning and combination of
both of them on the other hand a number
of data set benchmarks including Twitter
sentiment analysis have been introduced
and made available for the evolution of
the performance of these algorithms and
the last one is predicting house price
you will need to estimate the sale price
of a brand new home in any place for
this assignment the data set for this
project includes information on the cost
of homes in various City neighborhoods
the UCI machine learning repository is
the place where you may find the data
set needed for This research you will
also receive other data set with
information on the age of the population
the city's crime rate and the location
of non- retail Enterprises in addition
to the pricing of various residences
it's an excellent project to test your
knowledge if you are an new finding a
suitable job in the field of machine
learning is becoming increasingly
difficult the ideal way to to display
your machine learning skill is in the
form of portfolio of data science and
machine learning projects a solid
portfolio of projects will illustrate
that you can utilize those machine
learning skills in your profile as well
projects like movie reccommendation
system fake news detection and many more
are the best way to improve your early
programming skills you may have the
knowledge but putting it to the use what
is keep you competitive here are 10
machine learning projects that can
increase your portfolio and enable you
to acquire a job as a machine learning
engineer at number 10 we have loan
approval prediction system in this
machine learning project we will analyze
and make prediction about the loan
approval process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting fake news this ml project for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with escalan the model is then fitted
using a passive aggressive classifier
that has been initialized finally the
accuracy score and the confusion Matrix
indicate how well our model performs the
link for the project is in the
description box below at number eight we
have personality prediction system the
idea is based on determining an
individual personality using machine
learning techniques a person personality
influences both his personal and
professional life nowadays many company
are shortlisting applicants based on
their personality which increases job
efficiency because the person is working
on what he's good at rather than what is
is compelled to do in our study we
attempted to combine personality
prediction system using machine learning
techniques such as s SVD na base and
logistic regression to predict a person
personality and talent prediction using
phrase frequency method this model or
method allow users to recognize their
personality and Technical abilities
easily to learn about more this project
check the link in the description box
below at number seven we have Parkinson
disease system Parkinson disease is a
progressive central nervous system
element that affects movement and cause
tremors and stiffness it comprises five
stages and affects more than 1 million
worldwide each other in this machine
learning project we will develop an s VM
model using python modules psyched learn
numpy and pandas and svm we will import
the data extract the features and label
and scale the features split the data
set design an an model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre-trained models
were accessible in Python but changing
the voice of volume was often difficult
it also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text ml speech
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the Phenom and the phonetics
on the speech for this project we will
to pyttsx3 pyttsx3 is a python text to
speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3 to
learn more about this project check the
link in the description box below at
number four we have sentiment analysis
sentiment analysis also known as opinion
mining is a straightforward process of
determining the author's feeling about a
text what was the user intention when he
or she wrote something to determine what
could be personal information we employ
a variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement use
deep learning is an any sort of work
many of you like myself would choose a
convention neural network as a deep
learning technique for answering any
computer vision problem statement and in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use open CV and pH
recognition libraries to create a phase
detection system open CV provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens lit set
generated by more than 6,000 users to
learn how to create movie recommendation
system for yourself or for your loved on
check the project in the description box
below hello everyone welcome to this
video on what is mlops by simply learn
and let me tell you guys that we have
regular updates on multiple Technologies
if your a tech geek in a continuous ear
for the latest technological Trends then
consider getting subscribed to a YouTube
channel and press that Bell icon to
never miss any update from Simply learn
in this video first we will see what
mlops is after that we will cover the
use of mlops after that we will see some
real world application of mlops after
that we will see components of mlops at
the end we will see difference between
mlops and devops by the end of this
video I can assure you that all your
questions and doubts related to what is
mlops will have been clear on that note
if you're an aspiring AIML engineer then
there is no better time to train
yourself in the exciting field of
machine learning if you are looking for
course that covers everything from the
fundamental to Advanced Techniques like
machine learning algorithm development
and unsupervised learning look no
further than our keltech program in
partnership with IBM this AIML program
boot camp is collaboration with celtech
will help you advance your career as an
AIML specialist the AIML boot camp
includes live classes delivered by
industry expert handson legs industry
relevant projects and master classes by
ctech professors key feature of this
amazing course includes earn up to 22
CEOs from Kelch ctme online convocation
by CCH ctm program director simply learn
Career Services help you get noticed by
top hiring companies earn a boot camp
celtech certificate industry relevant
Capstone project in three domains CCH
ctm Circle membership 25 plus Hands-On
project across industry verticals with
integrated Labs so why wait join now
seats are feeling fast find the course
link from the description box below so
moving forward let's see what is mlops
mlops short for machine learning
operation is a set of practices tools
and principles that aim to streamline
and enhance the deployment management
and monitoring of machine learning
models in production of environments it
draws inspiration from devops
development and operation practices and
applies them specifically to the
challenges of depl and maintaining ml
model effectively moving forward let's
see what is the use of mlops mlop serves
crucial purposes in the context of
deploying and managing ml models in
production environment the first one is
efficient development mlops helps
automate the deployment process of
machine learning models making it faster
repeatable and less error prone this
enables the organization to quickly
transition from development to
production reducing the time it takes to
make model available to end users the
second one is collaboration mlops
encourages collaboration between
different teams such as data scientist
machine learning engineer operation and
business stakeholders by fostering
better communication and Alignment mlops
helps deliver solution that meets
Technical and business requirement the
third one is monitoring and maintenance
mlops provides tools for monitoring the
performance of deployed models in real
world condition this allows teams to
detect anomalies track key performance
metrics and address issue prompty
ensuring that the models continue to
deliver accurate result over time the
fourth one is cost efficiency by
optimizing resource utilization and
automatic process melops can lead to
cost saving efficiently manage
deployment reduce operational overhead
and resource wastage making machine
learning project more economically
viable the fifth one is Security in many
Industries compliance and security are
Paramount concern mlop provides
practices for managing access controls
data privacy and regular environment
ensuring deployed model adhere to
necessary standard so in summary mlops
plays a critical role in maximizing the
value of machine learning by ensuring
that models are efficiently deployed
monitored maintained and improved in
production environments it enables
organization to drive consistent
reliable and scalable benefits from
their machine learning investment moving
forward let's see some real world
application of mlops so mlops find
application in various areas where
machine learning models are deployed in
real world production environment some
typical application of mlops include the
first one is Predictive Analytics mlops
helps deploy Predictive Analytics that
analyze historical data to predict
future events these models are used in
finance market sales and other
Industries to forecast Trends and
optimize decision making the second one
is image video analysis mlops is used to
application like image classification
object detection and facial recognition
these Technologies are employed in
security system a autonomous vacus and
Medical Imaging the third one is
Healthcare and medical diagnostic mlops
AIDS in development and deploying models
for medical imaging analysis disease
diagnosis drug Discovery and patient
risk assessment the fourth one is
autonomous system mlops is crucial for
autonomous systems like wle drones and
Robotics ensuring that AI model function
reliable and adapt to changing condition
so these are some just a few example of
mlop and the explication of mlops are
continuously expanding as organization
explore new ways to leverage machine
learning in their operation so
accelerate your career in AIM with
Comprehensive postgraduate program in
AIML machine learning boost your career
with this AIML course delivered in
collaboration with part University and
IBM learn in demand skills such as
machine learning deep learning NLP
computer vision reinforcement learning
generative prompt engineering chat G and
many mo you will receive a prestigious
certificate and ask maning session by
IBM with five Capstone in different
domains using real data set you will
gain practical experience master classes
by PD University and puru faculty and
IBM experts andure Top Notch education
simply R job assist help you get notice
by Leading companies so why wait join
now seats are filling fast course link
is in the description box below and in
the pin comment moving forward let's see
component of mlops the extent of mlops
in machine learning project can vary
based on the project needs sometimes
mlops covers everything from preparing
the data to making the final model model
other times it might just involve
putting the model into action most
companies use mlops for first one is
exploited data analysis looking into the
data to understand it the second one is
getting the data ready and making
valuable features from it data
preparation and feature engineering the
third one is model training and the F
tuning the fourth one is model review
and governance fifth one is model
inference and serving model sixth one is
model deployment and monitoring the last
one one is teaching the model again
automatically when needed moving forward
let's see some difference between mlops
and devops the first one is mlops
specifically focus on the deployment
monitoring and management of machine
learning model while devops devops Prim
focus on the collaboration and
integration between development and
operation the second one is it is
centered around machine learning AI
application addressing the unique
challenges of deploying and maintaining
models while in devops it applies to
software development in in general
covering all aspects of software
engineering and deployment the third one
is mlops involves automating the end to
endend machine learning life cycle from
data pre-processing to data model
monitoring and retraining ha vo devops
aims to steam length the deployment life
cycle automate deployment pipelines
improve collaboration and ensure faster
and more Reliable Software releases and
the fourth one is mlops employs tools
for model versioning model packaging
model monitoring automated retraining
and model explanation while develops
employees tools like Version Control
Systems like G continuous integration
continuous deployment cicd pipelines
configuration management and
infrastructure as scod to achieve its
goal the last one is efficient model
deployment continuous model performance
monitoring reproductivity collaboration
between data scientists and engineers
and adapting to changing data and
requirements here the faster development
Cycles continuous integration quick and
automated deployment improve team
collaboration and increase software
reliability in summary
while devops and mlops share concept of
automation collaboration and continuous
Improvement devops has a broader focus
on software development and deployment
while mlops is tailors to change the
challenges specific to deployment and
managing machine learning models in
production settings both practices aims
to enhance efficiency collaboration and
reliability in their respective domains
and we also have a boot camp for you
guys and if you are one of the aspiring
a ml Enthusiast looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers in Ai and ml
by learning from the experts then try
giving a short to Simply learns keltech
Ai and machine learning boot camp the
course link or the boot camp link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered today we
are going to discuss the top 10 AI tools
for business in 2023 but before we begin
if you haven't already then consider
getting subscribed to our YouTube
channel and hit the the Bell to get
notified and also if you are an aspiring
artificial intelligence engineer looking
for online training and certifications
from the prestigious universities and
collaborating with leading experts to
have the most robust Foundation skills
knowledge and what it takes to be the
best then search no more simply launch
postgraduate program in artificial
intelligence from herd University in
collaboration with IBM and simply learns
artificial intelligence and machine
learning boot camp from CTIC University
should be your right choice for more
details use the links in the description
box below with that in mind let's get
started with the first one that is
clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages moving to answering
questions clickup AI can answer your
questions about a variety of topics such
as project management productivity or
business and next we have brainstorming
ideas clickup AI can help you brainstorm
ideas for projects products and
marketing campaigns and lastly
summarizing documentations clickup AI
can summarize documents into a concise
and easy to understand format clickup AI
is still under development but it is
already a powerful tool that can help
you save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus role
based AI tools with ready to made
prompts SLR command quickly to
brainstorm your ideas access the AI
command from the text tool barar to edit
text summarize long form content with
click of a button it can integrate with
click a productivity platform and can
help you with a wide variety of tasks
such as generating text translating
languages answering questions
brainstorming ideas and summarizing
documentations now moving ahead into the
second tool for today's discussion which
is about chat GPD chat gbt is an AI
powered language model developed by open
AI it is capable of generating humanlike
text based on context and past
conversations it is still under
development but it has already been used
for a variety of purposes such as
chatting with humans generating text
translating languages answering
questions let's discuss each one of
these chatting with humans chat GPT can
hold conversations with humans Alit in a
limited capacity it can answer questions
follow instructions and even tell
stories when it comes to generating text
chat gbt can generate text for a wide
variety of purposes such as writing blog
posts creating marketing copy or
summarizing documents it can also
translate languages and answer all of
your questions some of the key features
of chat GPT are as follows it has a
clean and minimalistic UI which makes it
extremely easy to use it has natural
language processing it has previous
conversation details about an
interactive which is two-way dialogues
and and then it supports 50 plus
languages that is spoken languages and
it has a vast knowledge of Base spanning
multiple Industries and also has vast
knowledge based subject areas now we
will move into the next tool for today's
discussion that is doll e or doll E2 is
a text to image diffusion model created
by open AI it can generate realistic
images from text descriptions for
example you can give it a prompt like a
cat riding a unicycle on a rainbow which
will generate an image of exact that
dolly is still considered to be under
development but it has already been used
to create some amazing images it has the
potential to be used for a variety of
purposes such as creating art designing
products generating Market materials
educating people when it comes to
creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly to
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
flyers and advertisements and lastly
when it comes to educating people doly
to can be used to create educational
material such as illustrations and
diagrams some of the most important key
features of dolly2 are it is very
intuitive interface that makes it easy
to use it can generate unque and high
quality images in seconds the native
editing tool lets you generate text
promps for replacing different elements
in a picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is brecka
bra brecka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat GPT four natural
language processing technology to
convert text descriptions into fully
functional web apps with a responsive
user interface breakup braack offers a
free plan that allows users to create up
to six apps per month and paid plans
that start at $68 per month here are
some key features of Bricker Brack it is
a time-saving and coste effective AI
tool for hiring professional and
experienced developers the built
functional applications from scratch in
minutes just using text an intuitive
drag and drop editor can let you create
customized AI tool to suit your needs
and requirements you can export your app
in file formats such as HTML CSS and
JavaScript it has unlimited free web app
hosting capabilities now we will be
proceeding into the next tool for
today's discussion which is to Tom is an
AI powered storytelling tool that helps
you create and share immersive
narratives quickly and easily it uses a
combination of machine learning and
natural language processing to generate
presentations outlines and stories with
text images and other media to AI is
easy to use just type a description of
your desired presentation or story into
a text box and Tom AI will generated for
you you can then customize the content
and style for your liking Tom AI is a
powerful tool that can be used for a
variety of purposes including creating
presentations for your work or school
writing posts or articles developing
marketing materials pitching ideas to
investors sharing personal stories and
much more to AI is still under
development but it is already a valuable
tool for anyone who wants to create and
share their ideas in a more engaging way
some of the key features of to AI are as
follows it can create a complete
presentation with text prompt only adds
specific slides pages to a presentation
with natural language prompts converts
and upload a document into a
presentation with a click of a button it
can integrate with the dolly to to
produce unique AI images for your
presentation embedded live Pages for any
website or external apps and lastly it
has sharing and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain a I is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the DAV
00003 text model from open AI to
generate text and rephrase existing text
it also offers text generation based on
a prompt and creating custom AI tasks
second AI is still under development but
it has a wide potential to be a powerful
tool for anyone who wants to improve
their writing skills skills here are
some key features of second brain it can
be helpful to training your new
chatboard and embedding it on your side
page the chatbot can be trained with
data from 90 different languages it
quickly and accurately responds to your
customer queries you can also create
multiple Bots depending on your paid
plan and lastly it is great for
entrepreneurs if you looking for a way
to improve your writing skills second
brain AI is a great option to consider
it is easy to use use and affordable and
it has potential to help you write
better articles email and other content
now let us proceed into the next tool
that is Jasper AI Jasper AI is an AI
writing assistant that helps you create
high quality content quickly and easily
it uses a variety of AI models to
generate text translate languages write
different kinds of creative content and
answer your queries in an informative
way Jasper AI is a powerful tool that
can be used for a variety of purposes
including writing blog post and articles
creating marketing copies translating
languages answering customer questions
generating creative content such as
poems code scripts musical pieces emails
letters Etc some of the key features of
Jasper AI are it has 50 plus templates
to speed up the content creation process
it can generate content in 30 plus
languages it lets you repurpose content
for multiple platforms it has buil
collaboration tools for working with
your team it can find human content
output to match your Brand's tone and
style and lastly it can integrate with
grammarly copy space and Surfer SEO
Jasper AI is still under development but
it has already been used for businesses
of all sizes to improve their content
marketing customer service and much more
if you are looking for a way to improve
your content creation Jasper AI is a
great option to consider it is easy to
use and affordable and it has has the
potential to help you create better
content that will engage your audience
and help you achieve your business goals
next is plus AI plus AI is an AI tool
that helps businesses automate their
reporting and analytics it uses a wide
variety of AI models to extract insights
from data generate reports and create
dashboards plus AI is a powerful tool
that can be used for a variety of
purposes including automating reporting
and analytics tasks extracting insights
from data generating reports and
dashboards improving decision making
reducing costs the key features of plus
AI are as follows it is part by the
latest in generative AI it has
integration between Google slice and
PowerPoint which is seamless it can
create a presentation that needs only
minor editing it has the ability to Rite
content on slides which is a biggest
game changer plus AI is still under
development but it has already been used
by businesses of old sizes to improve
their reporting and analytics
capabilities now let's proceed into the
ninth one which is about firefli
fireflies is an AI powered note maker
that can transcribe summarize and
analyze meetings it uses a combination
of machine learning and natural language
processing to automatically capture and
transcribe your meetings and then
generate summaries and insights that you
can use to stay on top of your work here
are some things that you can do with
fireflies AI stay on top of your work by
automatically transcribing and
summarizing your meetings get insights
into your team's discussions by
analyzing meeting transcripts share
meeting transcripts and insights with
your team members use meeting
transcripts to create presentations or
records and some of the key features of
fireflies are as follows it can record
and transcribes calls instantly it has
Chrome extension to capture all your
meetings it has simple to use search
that allows you you to review your calls
an easy to use meeting P to invite
Fireflight pods to a meeting transcribe
existing audio files instantly inside
the dashboard offer native integration
to dialers zapier and other apis and
lastly it eliminates the task of note
taken if you're looking for AI tool that
can help you improve your productivity
firefli AI is a great option to consider
it is easy to use and affordable and it
has the potential to save time and stay
on top of your work now the last tool
for today's discussion which is
speechify speechify is an AI powered
text to speech DTS tool that helps you
read and listen to text more efficiently
it can read allow any text you provide
including PDFs articles websites and
social media posts speech fi also offers
a variety of features to customize your
listening experience such as adjusting
the reading speed voice and background
noise some of the key features of
speechify are it has web based with
chrome and Safari extensions more than
15 languages are supported by speech fi
it has over 30 voices to select from and
it can scan and convert printed text to
speech speechify is an excellent tool
for anyone who wants to improve their
reading comprehension Focus or
productivity it can also be helpful for
people with dyslexia ADHD or other
learning disabilities AI tools for
business in 2023 but before we begin if
you haven't already then consider
getting subscribed to our YouTube
channel and hit the Bell to get notified
and also if you are an aspiring
artificial intelligence engineer looking
for online training and certifications
from the prestigious universities and
collaborating with leading experts to
have the most robust Foundation skills
knowledge and what it takes to be the
best then search no more simply launch
postgraduate program in artificial
intelligence from p University in
collaboration with IBM and simply learns
artificial intelligence and machine
learning boot camp from calch University
should be your right choice for more
details use the links in the description
box below with that in mind let's get
started with the first one that is
clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages moving to answering
questions clickup AI can answer your
questions about a variety of topics such
as project management productivity or
business and next we have brainstorming
ideas click up AI can help you
brainstorm ideas for projects products
and marketing campaigns and lastly
summarizing documentations clickup AI
can summarize documents into a concise
and easy to understand format clickup AI
is still under development but it is
already a powerful tool that can help
you save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus
role-based AI tools with ready toade
prompts slri command quickly to
brainstorm your ideas access the AI
command from the text tool bar to edit
text summarize long form content with
click of a button it can integrate with
click up productivity platform and can
help you with a wide variety of tasks
such as generating text translating
languages answering questions
brainstorming ideas and summarizing
documentations now moving ahead into the
second tool for today's discussion which
is about Chad GPT Chad GPT is an AI
powered language model developed by open
AI it is capable of generating humanlike
text based on context and past
conversations it is still under
development but it has already been used
for a variety of purposes such as
chatting with humans generating text
translating languages answering
questions let's discuss each one of
these chatting with humans chat GPT can
hold conversations with humans Alit in a
limited capacity it can answer questions
follow instructions and even tell
stories when it comes to generating text
chat gbt can generate text for a wide
variety of purposes such as writing blog
posts creating marketing copy or
summarizing documents it can also
translate languages and answer all of
your questions some of the key features
of chat GPT are as follows it has a
clean and minimalistic UI which makes it
extremely easy to use it has natural
language processing it has previous
conversation details about an
interactive which is two-way dialogues
and then it supports 50 plus languages
that is spoken languages and it has a
vast knowledge of Base spanning multiple
Industries and also has vast knowledge
based subject areas now we will move
into the next tool for today's
discussion that is doll e or doll E2 is
a text to image diffusion model created
by open AI it can generate realistic
images from text descriptions for
example you can give it a prompt like a
cat riding a unicycle on a rainbow which
will generate an image of exact that
dolly is still considered to be under
development but it has already been used
to create some amazing images it has the
potential to be used for a variety of
purposes such as creating art designing
products generating Market materials
educating people when it comes to
creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly 2
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
flyers and advertisements and lastly
when it comes to educating people Dolly
to can be used to create educational
material such as illustrations and
diagrams some of the most important key
features of dolly2 are it is very
intuitive interface that makes it easy
to use it can generate unique and high
quality images in seconds the native
editing tool lets you generate text
prompts for replacing different elements
in a picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is brecka
bra bricka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat GPT 4 natural
language processing technology to
convert text descriptions into fully
functional web apps with a responsive
user interface breakup bra offers a free
plan that allows users to create up to
six apps per month and paid plans that
start at $68 per month here are some key
features of Bricker bra it is a
time-saving and costeffective AI tool
for hiring professional and experienced
developers the built-in functional
applications from scratch in minutes
just using text an intuitive drag and
drop editor can let you create
customized AI tool to suit your needs
and requirements you can export your app
in file formats such as HTML CSS and
JavaScript it has unlimited free web app
hosting capabilities
now we will be proceeding into the next
tool for today's discussion which is Tom
Tom is an AI powered storytelling tool
that helps you create and share
immersive narratives quickly and easily
it uses a combination of machine
learning and natural language processing
to generate presentations outlines and
stories with text images and other media
to AI is easy to use just type a
description of your desired presentation
or story into a text box and to AI will
generate it for you you can then
customize the content and style for your
liking to AI is a powerful tool that can
be used for a variety of purposes
including creating presentations for
your work or school writing posts or
articles developing marketing materials
pitching ideas to investors sharing
personal stories and much more to AI is
still under development but it is
already a valuable tool for anyone who
wants to create and share their ideas in
a more engaging way some of the key
features of Tom AI are as follows it can
create a complete presentation with text
prompt only adds specific slides pages
to a presentation with natural language
prompts converts and upload a document
into a presentation with a click of a
button it can integrate with the dolly
to to produce unique AI images for your
presentation embedded live Pages for any
website or external apps and lastly it
has shared and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain AI is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the d003
text model from open AI to generate text
and rephrase existing text it also
offers text generation based on a prompt
and creating custom AI tasks second AI
is still under development but it has a
wide potential to be a powerful tool for
anyone who wants to improve their
writing skills here are some key
features of second brain it can be
helpful to training your new chatboard
and embedding it on your site page the
chatbot can be trained with data from 90
different languages it quickly and
accurately responds to your customer
queries you can also create multiple
Bots depending on your PID plan and
lastly it is great for entrepreneurs
hello and welcome to this video on edge
AI for artificial intelligence if you're
interested in learning more about how AI
is revolutionizing the world around us
this video is for you AI is a New
Concept in the world of AI that refers
to devices and sensors at the edge of
the network that can collect and process
data in real time this allows for faster
more accurate decision making and
greater efficiency in a variety of
applications such as self-driving
vehicles Smart Homes and health
monitoring so how is Aji different from
cloud computing and cloud computing AI
the main difference is the location
where the data is processed AI enables
devices at the edge of the network to
process data locally without having to
send it to the cloud this can lead to
faster more responsive systems and can
help you preserve bandwidth and reduce
latency in contrast cloud computing AI
processes data in the cloud where it can
assess vast amount of data and compute
resources but also requires a reliable
internet connection and can be slower as
Aji becomes more advanced it has the
potential to transform many Industries
such as Healthcare Transportation
manufacturing and a lot it can enable
new application such as autonomous
vehicles remote surgery and real-time
monitoring of machines and equipments
while cloud computing AI has many
benefits AI offers the advantages of
faster decision- making reduced latency
and improved efficiency so if you're
interested in learning more about the
future of ai ai is definitely a topic
worth exploring on that note hello
everyone and welcome to Simply learn in
today's video we'll understand what Edge
Computing is and how it is different
from traditional cloud computing but
before we begin make sure to subscribe
to our channel to stay updated with all
the latest Technologies and hit that
Bell icon to never miss any updates from
us so without any further delay let's
get started wait if you want to become
an expert in Edge Computing then look no
further than Caltech postgraduate
program in a machine learning offered by
simply learn which is an Onin program
designed to provide students with a
comprehensive understanding of AI and
machine learning Concepts tools and
techniques this program is created in
collaboration with CeX ctme and IBM and
it is designed to provide Learners with
a strong foundation in Ai and and
machine learning Concepts this program
covers various topics such as data
science statistics deep learning
computer vision and NLP and help you
gain the right skill set on various
tools such as caras mat part Leb tens
oflow Django and many more furthermore
the curriculum is structured around
interactive online classes live sessions
with industry experts and Hands-On
projects so why wait hurry up enroll now
in Caltech postgraduate program in a
machine learning and create your own
successful career link is are in the
description box below so make sure you
check that out so without any further
Ado let's jump directly into our today's
topic so firstly let us understand what
is Edge Computing the term Edge
Computing for AI refers to the process
of running Ai computations and
processing close to the data source or
Edge devices at the edge of a network as
opposed to relying on centralized
cloud-based servers in order to enable
realtime data analysis pick our response
times and a decreased Reliance on cloud
infrastructure it deploys AI models and
algorithms directly on edge devices or
local
servers a edge Computing typically
entails the deployment of light
lightweight AI models designed to use on
edge devices with constrained
computational power then these models
are frequently deployed to the edge
after being trained using cloud-based
resources The Edge devices gather data
from their sensors or from outside
sources process the data locally using
the AI models that have been deployed
and give prompt responses or start
processes based on the data but what to
the rise of edge AI well with rapid
advancement in technology in every
sector and businesses are looking to
increase a inform to increase workflow
productivity and security and for that
computer programs must be able to
identify patterns and Carry Out tasks
repeatedly and safely in order to assist
people now the range of tasks that human
perform however covers in different
situations that are impossible to fully
describe in codes and rules because the
world is very unstructured well with the
help of developments in Hai machines and
gadgets can Now function with the
intelligence of human cognition whenever
they may be smart applications with AI
capabilities can learn to carry out the
same tasks under various conditions just
like in real life and here are the three
recent innovations that explain why
using AI models at the edge is on the
rise well first on the list we have
advancement of neural networks now over
the years as you know neural networks
and related AI infrastructure have
advanced and matured allowing for more
generalized machine learning
capabilities this means organizations
have to gain a deeper understanding on
How to Train AI models effectively and
deploy them in production at the
edge development in computational
infrastructure now running AI at the
edge requires substantial computational
power the field has witnessed notable
advancements in computational
infrastructure to support AI at the edge
particularly recent strides in highly
parallel graphic processing units or
gpus and CPUs have enabled efficient
execution of neural networks gpus are
specifically designed to handle complex
parallel computation making them a
suitable choice for processing Ai
workloads and finally widespread usage
of iot devices now the widespread
adoption of Internet of think devices
has generated vast amounts of data these
devices such as industrial sensors smart
cameras and Robotics provide a rich
source of data that can be utilized for
AI analysis at the edge the availability
of diverse iot devices and their ability
to capture real-time data has
facilitated the deployment of AI models
at the edge but why deploy AI at the
edge what are the benefits of edge
Computing now Edge Computing for AI
addresses the limitation and challenges
associated with conventional Cloud
Centric AI methods so here are some of
the advantages which it offers firstly
decreased latency Now by performing AI
processing at the edge there is a
significant reduction in network latency
since data no longer needs to be
transmitted to the cloud and back this
is particularly beneficial for
applications that demand real-time
responses or decision making let's say
like autonomous vehicles or Industrial
Automation secondly realtime data
analysis Edge Computing for AI enables
imate data analysis and inference
directly on the edge devices themselves
this is especially valuable for
time-sensitive applications that require
realtime insights or predictions
eliminating the need for Reliance on
cloud
connectivity enhanced privacy and
security well Edge Computing enhances
privacy and security by keeping data and
AI processing local sensitive
information can be processed on the edge
devices without being transmitted to the
cloud minimizing the risk of data
breaches and ensuring compliance with
data privacy regulations next we have
bandwidth optimization Edge Computing
reduces the volume of data uh that needs
to be transmitted to the cloud sending
only relevant or let's say summarized
data now this optimization minimizes
bandwidth requirements reduces Network
cost and even lightens the load on the
cloud infrastructure in total and
finally it offers offline functionality
Edge Computing enables AI models to
function even in situation which limited
or no internet connectivity this
capability is crucial for scenarios that
require continuous operation in remote
or disconnected environments such as
certain iot applications or field
deployments so these are some of the
reasons why we need to deploy AI at the
edge and the benefit it's offering now
comes the main question how exactly does
Edge Computing works well Edge Computing
is primarily focused on the proximity
and location of data processing and
storage the fundamental principle is
simple and it's not feasible to bring
data to a centralized Data Center the
alternative is to bring the data center
close to the data now in order to enable
machines to perform tasks like let's say
visual perception object detection
driving cars understanding speech
speaking walking and emulating other
human skills they need to replicate
human intelligence in a functional
manner so for that AI uses a deep neural
network as a data structure to emulate
human cognition these dnns undergo
training in data centers and are exposed
to numerous examples of specific task or
question type along with their correct
answer and once the training is complete
the model advances to become a inference
engine capable of addressing real world
questions now from there on here's how
it typically Works firstly deployment of
inference engine now in a deployments
the inference engine which is the
component responsible for making
predictions or performing AI tasks is
deployed on local computers or devices
located in diverse environments like
factories hospitals cars satellites
homes and much more problem
identification and data upload now while
the inference engine operates at the
edge it may encounter situation where it
struggles to provide accurate
predictions or encounter challenging
scenarios in such cases the problematic
data associated with these instances is
commonly identified and upload to a
centralized Cloud structure now from
there on there is a continuous
cloud-based model training now the cloud
infrastructure typically in a data
center receives the problematic data
from Edge devices data scientists and AI
experts can then utilize this data to
refine and train the original AI models
further this involves using the Cloud's
computation resources larger data sets
and collaboration tools for model
training and Improvement now and finally
with the updated model deployed at the
edge after being retrained the AI system
continues to operate gaining new data
and insights from its environment the a
model can collect further additional
data which can be periodically uploaded
to the cloud and further training and
continuous Improvement this feedback
loops ensures that the edji models
become progressively smarter over time
so this is in a nutshell how Edge
Computing for AI Works hope you have
understood well let us talk some of the
use cases of edge Computing for AI in
real world well the combination of AI
iot and Edge Computing has opened up a
multitude of opportunities for Edge AI
revolutionizing the various aspects of
our daily lives so let's Del you into
some key areas where Edge AI is making a
significant impact first on the list we
have healthare Hai is enhancing medical
Diagnostics and patient care for
instance by deploying AI algorithms on
medical imaging devices Radiologists can
accurately and efficiently identify and
analyze pathologies variable devices
with local Edge analytics enable
realtime monitoring of patient data
facilitating any early detection of
anomalies And Timely
interventions next autonomous vehicles
Edge air plays a pivotal role in
enabling realtime navigation and
decision making for autonomous vehicles
AI algorithms running on edge devices
within Vehicles process sensor data
analyze the environment and make
split-second decisions to ensure safe
and efficient driving agriculture hji is
being used to address agriculture
challenges as well such as pollination
autonomous drones equipped with aai
capabilities can identify and pollinate
plants aiding in crop cultivation and
maximizing yields this application
demonstrates the diverse potential of
edge AI Beyond traditional domain
next we have smart home and cities now
edai enhances the functionality and
efficiency of Smart Homes and cities
local processing and Analysis of data
from iot devices enable realtime
automation Energy Management security
surveillance and personalized user
experience Aji power system adapt to
individual preferences optimize
resources usage and respond to events in
real time next we have industrial
applications Ed AI is transforming
Industries thoroughly through real-time
monitoring predictive maintenance and
process optimization Edge devices
equipped with AI algorithms analyze
sensor data detect anomalies and predict
equipment failures reducing downtime and
improving efficiency next we have
security and surveillance well a plays a
crucial role in security and
surveillance which empar real-time video
analytics for security and surveillance
applications Edge devices with a
capabilities can analyze video feeds
locally detect suspicious activities
like theft identify objects or
individuals of interest and Trigger
alerts or responses enhancing security
measures and reducing response times and
finally we have personal devices H AI is
increasingly being integrated into
personal devices such as your
smartphones laptops variables and smart
home assistant this allows for on device
AI processing enabling faster and more
personalized experience without raying
heavily on cloud connectivity which can
support voice recognition facial
recognition among other features as well
well as more businesses and industries
recognize the advantage of AI its
adoption is rapidly growing the ability
to process data locally enhance privacy
reduce latency and make real-time
decision empow organizations in various
sectors to enhance operations deliver
better services and improve user
experience so these were some of the
realtime applications of Ed and with
that we've come to the end of today's
session on what H Computing for AI is
all of you must be already aware of
blockchain but let me give you a quick
refresher a blockchain is a digital
Ledger that store information like a
digital database without a central
Authority controlling it instead a
blockchain is maintained by a network of
computers each one is storing a copy of
The Ledger when a new transaction is
added to the Ledger the network has to
approve it before it can be recorded
other side Lang chain is like a
blockchain but with a few differences
unlike a blockchain which stores
information about financial trans action
Lang chain can store any kind of data or
information like medical records supply
chain data or even social media profiles
Lang chain design also allows for
private transaction where only a certain
people can access and view the store
data which differs from a regular
blockchain sounds fascinating right well
it is and there is a lot more to learn
and discuss about blockchain technology
in this video so stay tuned till the
very end as we embark on this amazing
journey of exploring Lang chain
according to recent studies Ai and
machine learning related job posting
have increased by 3.44% in the past 5
years companies across the globe are
actively seeking professional who can
harness the power of data and build
intelligence system the average salary
is $150,000 in the US and 15 lakhs in
India as the field evolves new job
titles and responsibilities are
anticipated to C to Industry
requirements prominent companies like
Microsoft Google Amazon IBM Goldman s
and apple are actively recruiting
machine learning Engineers hey everyone
welcome to Simply L YouTube channel but
before we dive into that don't forget to
subscribe like and share there is no
better time to train yourself in the
exciting field of machine learning if
you are looking for a course that covers
everything from the fundamental to
Advanced Techniques for that accelerate
your career in Ai and ml with our
comprehensive postgraduate program in Ai
and machine learning gain expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything sessioned by IBM
with three Capstone and 25 plus industry
project using real data sets from
Twitter Uber and many more you will gain
practical experience master classes by
Caltech faculty and IBM expert and no
top notch education simply run job asist
help you to get notice by Leading
companies this programs covers
statistics python supervised and
unsupervised learning NLP neural network
computer Visions G scar us T of flow and
many more skills so enroll now and
unlock exciting a ml opportunities the
link is in the description box below so
without any further a delay let's get
started so what is Lang
chain Lang chain is a versatile open
source framework that empowers AI
developers to integrate large language
model llms such as gbt 4 with the
external data available in the python or
JavaScript packages Lang chain addresses
the limitation of GPT models trained
only up until 2021 while gp4 models
exhibit impressive general knowledge
their potential is greatly enhanced by
connecting them to customize data and
customized computations and this is
precisely what Lang Chain facilit by
enabling lm's access comprehensive
database Lang chain empowers them to
generate responses based on the most
upto-date information found in the
reports documents and website content
since the launch of GTP 4 in March
langin has garnered substantially
popularity due to its f flexibility and
the my rate of possibilities it unlocks
when combined with the high powered llm
so moving forward let's see why we need
Lang chain so Lang chain represents a
practical solution to overcome this
limitation through a practical approach
it involves pre-processing the Text
corpus by dividing it into manageable
chunks or summaries and adding them in a
vector space and subsequently searching
for similar chunks when a question is
posed so this methodology of
pre-processing real time and data
collection and interaction with the llm
is not only applicable in the context
but also extend to other scenarios such
as code semantic search so Lang
simplifies the composition of these
components by offering an abstraction
the simplification of the process often
called prompt Plumbing plays a crucial
role and will grow in significance as
llms becomes more powerful and
necessitate the provision of additional
data during the prompt execution so
moving forward let's see how does Lang
chain work so Lang chain simplifies the
process of organizing large volume of
data that can be easily accessed by the
llm requiring minimal computational
power it operates by taking a
subsequential data source like 50 page
PDF and breaking it down into manageable
chunks and that are embedded into a
vector store so with the vectorized
representation of the extensive document
at hand we can combine them with the llm
to selectively retrieve the necessary
information for creating prompt
completion pairs so when we input a
prompt into our chatbot equipped with
langin it queries the vector store to
find parent information a kind of
miniature Google tailored to your
document once the relevant details are
obtained they are combined with the llm
prompt and fed into the llm to generate
our desired response after this
integration of Lang chain with the open
AIS llms also enables the creation of
application capable of taking action
such as web serving email sending and
other task related to AP and at the end
you will get a reply so this is how Lang
chain works with open AI llms moving
forward let's see some use cases of Lang
chain so there is a wide range of
potential application for Lang chain
here are few examples so first one is
personal AI email assistant Lang chain
can be used to develop an intelligent
email assistant that understood and
generates responses based on the user
preferences and writing style the second
one is AI study body so langin can
assist a student in their Studies by
providing real-time access to relevant
research papers textbooks and
educational resources offering
personalized study recommendation and
answering theoretical questions and the
third one is AI data analytics Lang
chain can be employed to enhance data
analytics by allowing large language
model to access and analyze V amount of
structured and unstructured data
enabling more comprehensive insight and
prediction and the fourth one is custom
company customer service chatbot langin
can power chatbot that provide AED
customer service experience by
integrating up-to-date company
information product knowledge and the
customer data to offer personalized and
accurate responses and the fifth one is
social media content creation so Lang
chain can help content creator generate
engaging and relevant social media
content by leveraging its ability to
access the latest information Trends and
insight from various sources so these
example illustrate just the fraction of
the potential application for lanin and
its versatility extend to a wide array
of other scenarios as well so after
seeing use cases of Lang chain let's
move forward and see using open AI how
Lang chain works or not so welcome to
the demo part so here first we will set
up the what the problem is using the
chat
gbt so this is the Chad GB this is
something that we all know and I'm going
to first ask Chad GB what is Lang
chain and we will see what it will say
okay so here I will write
what okay what is
Lang
chain so as you can see it is saying I'm
sorry but as of my knowledge cut off in
September 2021 I'm not familiar with the
term langang chin it is possible that it
could be a term or concept that emersed
or became popular at that
time and could you provide more context
or details about what you are referring
to that way I can try to assist you
better so as you can see why this is
happened it is because langin is a more
recent
Library so this is the new library and
Lang chain is I think published on uh
somewhere around January and it is new
and Chad GPT has not been trained on the
data more recent than I think like
around 2021 as you can see it is written
here September 2021 okay so this is is
the problem number one so let's see the
problem number two so here I will write
hey
chat
GPT tell me how
many
meetings the meetings are blogged
today us
it us it
my
Google
calendar again it is saying I'm sorry
but as an AI language model I don't have
access to personal data about
individuals unless it has been shared
with me during our conversation I can
browse or retrieve information from your
Google calendar or any other personal
documents or accounts however if you
have access to your Google calendar you
can easily check the number of meetings
blocked today by opening your calendar
and looking at the schedule events for
the day the calendar interfaces should
display the number of meetings or events
you have scheduled and why this is
happening because we are using chat GPT
open a okay but if we will use API of
open AI okay currently I'm using the
chat GPT
but if we will use the open AI API so we
can connect to our Google Drive Google
Calendar and any
anything after that we can easily
retrieve our
information okay so this is where so as
you can see using this image what Lang
chain
does okay is it is going to connect your
AI models okay these are the AI models
open AI hugging phases so what the
langet does okay using
llms it is going to connect to outside
sources like Google drive or Wikipedia
wallframe or you know Google cender
anything to the outside
sources and you can do some really cool
things without those outside sources
okay you can ask to the chat GPD and or
you can make your own model llm model
but without llm model and langin you you
can't connect with the outside
resources okay so all of a sudden you
can start to chain together commands
Okay that is why the length chain is
used chain okay these are the llm models
open air hugging faces and you can chain
both of them using Lang chain okay you
can chain the outside sources and this
llms using Lang
chain okay uh after chaining llm and
outside is using Lang chain you can ask
any question to them okay what is my how
many documents I wrote in the last year
and this month or you can say hey chity
what's the weather today to open AI or
anything you can ask these uh kind of
questions and the application for the
langin are the endless and we have
already seen some of them okay so now
let's see the documentation part of the
Lin official documentation part of the
Lang chain so as you can see this is the
official documentation of the Lang chain
okay so here as you can see what is the
Lang Lang is a framework for developing
application powered language models LM
models it enables applications that are
data Weare or agent so as you can see
there are the certain modules and the
example use cases as you can see the
chatbots and you can create the M uh you
know chat boards using llms like open
and there are the many things like
interaction with apis
autonomas okay you can check
anything these are some the uh modules
of the Lang chain Lang CH provides
standard extendable interfaces okay so
these are the modules and these are the
use
cases okay ecosystems and the support
API references these are the API
differences how you can use the API and
as you can see this is the installation
part using pip you can use install the
Lin and the cond part is here the cond
installed okay so this is this you know
official document you can go through it
and you will learn you will definitely
learn something if you will go through
the official document and we also have a
boot camp for you guys and if you are
one of the aspiring a ml Enthusiast
looking for online training and
graduating from the best universities or
a professional who El assits to switch
in Ai and ml by learning from the
experts then try giving a short to
Simply learns celtech Ai and machine
learning boot camp the course link or
the boot camp link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete overview of the program being
offered first let's understand what
really is artificial
intelligence artificial intelligence is
the science of building intelligent
machines from vast volumes of
data this data can be structured
semistructured or unstructured in nature
AI systems learn from past experiences
and perform humanlike tasks artificial
intelligence enhances the speed
precision and effectiveness of human
efforts AI uses sophisticated algorithms
and methods to build machines that can
make decisions on their own deep
learning and machine learning are the
two subsets of artificial intelligence
so you need both machine learning
algorithms and deep learning networks to
build intelligent systems AI is now
being widely used in almost every sector
of business such as Transportation
Healthcare banking retail entertainment
and
e-commerce now let's look at the
different types of artificial
intelligence so AI can be classified
based on capabilities and
functionalities under capabilities there
are three types of artificial
intelligence they are narrow AI General
Ai and super AI under functionalities we
have four types of artificial
intelligence reactive machine limited
memory theory of mind and self-awareness
let's look at them one by
one first we will look at the different
types of artificial intelligence based
on
capabilities so what is narrow AI narrow
AI also known as weak AI focuses on one
narrow task and cannot perform Beyond
its limitations it aims at a single
subset of cognitive abilities and
advances in that Spectrum applications
of narrow AI are becoming increasingly
common in our day-to-day lives as
machine learning and deep learning
methods continue to
evolve Apple Siri is a simple example of
a narrow AI that operates with a limited
predefined range of functions CI often
has challenges with tasks outside its
range of abilities IBM Watson
supercomputer is another example of
narrow AI which applies cognitive
Computing machine learning and natural
language processing to process
information and answer your
questions IBM Watson once outperformed
human contestant Ken Jenkins to become
the champion on the popular game so
Jeopardy other examples of narrow AI
include Google Translate image
recognition software recommendation
systems spam filtering and Google's page
ranking
algorithm next we have General
artificial intelligence or general AI
General AI also known as strong AI has
the ability to understand and learn any
intellectual task that a human can
General artificial intelligence has
received a $1 billion investment from
Microsoft through open AI it allows the
machine to apply Knowledge and Skills in
different contexts AI researchers and
scientists have not achieved strong AI
so far to succeed they would need to
find a way to make machines conscious
programming a full set of cognitive
abilities Fujitsu built the K computer
which is one of the first fastest
computerss in the world it is one of the
most notable attempts at achieving
strong AI it took 40 minutes to simulate
a single second of neural activity so it
is difficult to determine whether or not
strong AI will be achieved in the near
future tan 2 is a superc computer
created by China's national university
of Defense technology it currently holds
the record for CPS at 33.86 petaflops
although it sounds exciting the human
brain is estimated to be capable of one
exaflop now CPS means characters per
second that A system can
process third in the list of AI that is
based on capabilities we have super AI
super AI exceeds human intelligence and
can perform any task better than a
human the concept of artificial super
intelligence sees AI evolved to be so
akin to human emotions and experiences
that it doesn't just understand them it
evokes emotions needs beliefs and
desires of its own its existence is
still hypothetical some of the key
characteristics of super AI include the
ability to think solve puzzles make
judgments and decisions on its
own now if you have enjoyed watching
this video so far please make sure to
subscribe to our YouTube channel and hit
the Bell icon to stay updated with all
the latest Technologies also if you have
any questions related to this video
please put it in the chat section our
team of experts will help you address
your
questions moving ahead now we will see
the different types of artificial
intelligence based on
functionalities in this category first
we have reactive machine a reactive
machine is the basic form of AI that
does not store memories or use past
experiences to determine future actions
it works only with present data they
simply perceive the world and react to
it reactive machines are given certain
tasks and don't have capabilities Beyond
those
duties IBM's deep blue which defeated
chess Grandmaster Gary casprov is a
reactive machine that sees the pieces on
a chess board and reacts to them it
cannot refer to any of its prior
experiences and cannot improve with
practice deep blue can identify the
pieces on a chessboard and know how each
moves it can make predictions about what
moves might be next for it and its
opponent
it can choose the most optimal moves
from among the possibilities deep blue
ignores everything before the present
moment all it does is look at the pieces
on the chest board as it stands right
now and choose from possible next
moves up next we have limited memory
limited memory AI learns from past data
to make decisions the memory of such
systems is
shortlived while they can use this data
for a specific per period of time they
cannot add it to a library of their
experiences this kind of technology is
used for self-driving Vehicles they
observe how other vehicles are moving
around them in the present and as time
passes that ongoing collected data gets
added to the static data within the AI
machine such as Lane markers and traffic
lights they're included when the vehicle
decides to change lanes to avoid cutting
of another driver or being hit by a
nearby vehicle
Mitsubishi Electric is a company that
has been figuring out how to improve
such technology for applications like
self-driving
cars then we have theory of Mind theory
of Mind represents a very advanced class
of technology and exists as a concept
this kind of AI requires a thorough
understanding that the people and the
things within an environment can alter
feelings and
behaviors it should be able to
understand people emotions sentiment and
thoughts even though a lot of
improvements are there in this field
this kind of AI is not complete yet one
real world example of theory of Mind AI
is Kismet a robot head made in the late
'90s by a Massachusetts Institute of
Technology researcher Kismet can mimic
human emotions and recognize them both
abilities are key advancements in theory
of Mind AI but Kismet can't follow gazes
or convey attention to humans sopia from
Hansen robotics is another example where
the theory of Mind AI was implemented
cameras within Sophia's eyes combined
with computer algorithms allow her to
see seek and follow faces sustain eye
contact and recognize individuals she is
able to process speech and have
conversations using natural language
subsystem finally we have
self-awareness self-awareness AI only
exists
hypothetically such systems understand
that internal traits States and
conditions and perceive human
emotions these machines will be smarter
than the human
mind this type of AI will not only be
able to understand and evoke emotions in
those it interacts with but also have
emotions needs and beliefs of its own
while we are probably far away from
creating machines that are self-aware we
should focus our efforts towards
understanding memory learning and the
ability to base decisions on past
experiences let's look at the definition
of each of these learning
techniques supervised learning uses
labeled data to train machine learning
models labeled data means that the
output is already known to you the model
just needs to map the inputs to the
outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal below
you can see we have a trained model that
identifies the picture of a cat
unsupervised learning uses unlabeled
data to train machines unlabelled data
means there is no fixed output variable
the model learns from the data discovers
patterns and features in the data and
Returns the output here is an example of
an unsupervised learning technique that
uses the images of vehicles to classify
if it's a bus or a truck so the model
learns by identifying the paths of a
vehicle such as the length and width of
the vehicle the front and rear end
covers roof hoods the types of Wheels
used at Etc based on these features the
model classifies if the vehicle is a bus
or a
truck reinforcement learning trains a
machine to take suitable actions and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different parts
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algorithms that come
under these learning techniques some of
the commonly used supervised learning
algorithms are linear regression
logistic regression support Vector
machines K nearest neighbors decision
tree random forest and knife
base examples of unsupervised learning
algorithms are K means clustering
hierarchical clustering DB scan
principal component analysis and others
choosing the right algorithm depends on
the type of problem you're trying to
solve some of the important
reinforcement learning algorithms are Q
learning Monte Carlo sarsa and deep Q
Network now let's look at the approach
in which these machine learning
techniques work
so supervised learning takes labeled
inputs and Maps it to known outputs
which means you already know the target
variable unsupervised learning finds
patterns and understands the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning
methods so supervised learning methods
need external supervision to train
machine learning models and hence the
name
supervised they need guidance and
additional information to return the
result unsupervised learning techniques
do not need any supervision to train
models they learn on their own and
predict the output similarly
reinforcement learning methods do not
need any supervision to train machine
learning models and with that let's
focus on the types of problems that can
be solved using these three types of
machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next
slide an unsupervised learning is used
for clustering and Association problems
while reinforcement learning is
reward-based
so for every task or for every step
completed there will be a reward
received by the agent and if the task is
not achieved correctly there will be
some penalty
used now let's look at a few
applications of supervised unsupervised
and reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or
benign now talking about the
applications of unsupervised learning we
have customer segmentation So based on
customer Behavior likes dislikes and
interests you can segment and cluster
similar customers into a group another
example where unsupervised learning
algorithms are used as customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tusks profit
estimation of a company if I was going
to invest in a company I would like to
know how much money I could expect to
make so we'll take a look at a venture
capitalist firm and try to understand
which companies they should invest in so
we'll take the idea that we need to
decide the companies to invest in we
need to predict the profit the company
makes and we're going to do it based on
the company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing uh we might have the
location we might have what kind of
administration is going through based on
all this different information we would
like to calculate the profit now in
actual it there's usually about 23 to 27
different markers that they look at if
they're a heavy duty investor we're only
going to take a look at one basic one
we're going to come in and for
Simplicity let's consider a single
variable R&D and find out which
companies to invest in based on that so
when we take a R&D and we're plotting
The Profit based on the R&D expenditure
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we can draw a line right
through the data and when you look at
that you can see how much they invest in
the R&D is a good marker as to how much
profit they're going to have we can also
note that companies spending more on R&D
make good profit so let's invest in the
ones that spend a higher rate in their
R&D what's in it for you first we'll
have an introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with learning
your regression let's look at another
example of machine learning based on the
amount of rainfall how much would be the
crop yield so here we have our crops we
have our rainfall and we want to know
how much we're going to get from our
crops this year so we're going to
introduce two variables independent and
dependent the independent variable is a
variable whose value does not change by
the effect of other variables and is
used to manipulate the dependent
variable it is often denoted as X in our
example rainfall is the independent
variable this is a wonderful example
because you can easily see that we can't
control the rain but the rain does
control the crop so we talk about the
independent Dev variable controlling the
dependent variable let's Define
dependent variable as a variable whose
value change when there is any
manipulation the values of the
independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values let's
take our data coming in and this is kind
of random data from any kind of project
we want to divide it up into numerical
and categorical so numerical is numbers
age salary height where categorical
would be a description the color a dog's
breed gender categorical is limited to
very specific items where numerical is a
range of information now that you've
seen the difference between numerical
and categorical data let's take a look
at some different machine learning
definitions when we look at our
different machine learning algorithms we
can divide them in into three areas
supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 - y1 1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the Y AIS is the amount of crop
yield you can expect for the amount of
rainfall represented by the Green Dot so
if we have an idea what the rainfall is
for this year and what's going on then
we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some python script
that this math that you're going to see
here is already done automatically for
you you don't have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was 1 we
got Y = 2 when X was uh two y was four
and so on and so on if we go ahead and
plot this data on a graph we can see how
it forms a nice line through the middle
you can see where it's kind of grouped
going upwards to the right the next
thing we want to know is what the means
is of each of the data coming in the X
and the Y the means doesn't mean
anything other than the average so we
add up all the numbers and divide by the
total so 1 + 2 + 3 + 4 + 5 over 5 = 3
and the same for y we get four if we go
ahead and plot the means on the graph
we'll see we get three comma four which
draws a nice line down the middle a good
estimate here we're going to dig deeper
into the math behind the regression line
now remember before I said you don't
have to have all these formulas
memorized or fully understand them even
though we're going to go into a little
more detail of how it works and if
you're not a math whz and you don't know
if you've never seen the sigma character
before which looks a little bit like an
e that's opened up that just means
summation that's all that is so when you
see the sigma character it just means
we're adding everything in that row and
for computers this is great because as a
programmer you can easily iterate
through each of the XY points and create
all the information you need so in the
top half you can see where we've broken
that down into pieces and as it goes
through the first two points it computes
the squared value of x the squared value
of y and x * Y and then it takes all of
X and adds them up all of Y adds them up
all of X squ adds them up and so on and
so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x * Y where the sum equals
66 this all comes from our formula for
calculating a straight line where y
equals the slope * X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is in just a
minute and where that information comes
from it's called the square means error
but we'll go into that in detail in a
few minutes all you need to do is look
at the formula and see how we've gone
about computing it line by line instead
of trying to have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and on the top
part if you read through the brackets
you have the number of data points times
the sum of x * Y which we computed one
line at a time there and that's just the
66 and take all that and you subract it
from the sum of x times the sum of Y and
those have both been computed so you
have 15 * 20 and on the bottom we have
the number of lines times the sum of X2
easily computed as 86 for the sum minus
I'll take all that and subtract the sum
of X2 and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2. two
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where M = 6 and C = 2.2 we're going to
take these values and we're going to go
ahead and plot them we're going to
predict them so y = 6 * where x = 1 +
2.2 = 2.8 so on and so on and here the
Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e square if
we put these into a nice chart where you
can see X and you can see Y where we
actual values were and you can see y
predicted you can easily see where we
take Yus y predicted and we get an
answer what is the difference between
those two and if we square that
y- y prediction squared we can then sum
those squared values that's where we get
the 64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2.4 we check this
error for each line and conclude the
best fit line having the least e Square
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square Square distance between the
data points and the regression line now
we only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc what you want to take away from this
is whatever formula is being used you
can easily using a computer programming
and iterating through the data calculate
the different parts of it then that way
these complicated formulas you see with
the different summations and absolute
values are easily computed one piece at
a time up until this point we've only
been looking at two values X and Y well
in the real world it's very rare that
you only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look at
what happens when you have multiple
inputs so in multiple linear regression
we have uh well we'll start with the
simple linear regression where we had y
= m + x + C and we're trying to find the
value of y now with multipolar
regression we have multiple variables
coming in so instead of having just X we
have X1 X2 X3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M1 M2 M3 and we still just have
the single coefficient so when you're
dealing with multiple linear regression
you basically take your single linear
regression regression and you spread it
out so you have y = M1 * X1 + M2 * X2 so
on all the way to m to the nth x to the
n and then you add your coefficient on
there implementation of linear
regression now we get into my favorite
part let's understand how multiple
linear regression works by implementing
it in Python if you remember before we
were looking at a company and just based
on its R&D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict his profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it CU it's such a
visual to look at and so easy to use um
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as py
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your sklearn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import matplot
library. pyplot as PLT and then caborn
as SNS caborn works with the matplot
library so you have to always import
matplot library and then Seaborn sits on
top of it and we'll take a look at what
that looks like you could use any of
your own plot libraries you want there's
all kinds of ways to look at the data
these are just very common ones and the
caborn is so easy to use it just looks
beautiful it's a nice representation
that you can actually take and show
somebody and the final line is the Amber
sign map plot library in line that is
only because I'm doing an inline IDE my
interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting
because we're just setting up variables
in fact it's not going to do anything
that we can see but it is importing
these different libraries and setup the
next step is load the data set and
extract independent and dependent
variables now here in the slide you'll
see companies equals pd. read CSV and it
has a long line there with the file at
the end 1,000 companies. CSV you're
going to have to change this to fit
whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals pd.
reader CSV and I've changed this path to
match my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the iocation and because
this is companies as a PD data set I can
use this nice notation that says says
take every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a panda data set
but we can easily extract the data from
our Panda's data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the pandas sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spin ad
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
65,7
4920 compared to the administration cost
of 100 3
6,897 180 so on so on helps to create
the profit of
19226 and 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
uh notebook and see we got going and so
we have the visualization and again
we're using SNS which is pandas as you
can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file in
other means and that's a map plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
cuz it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns and
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration Coster
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is r& D spending
column one which is Administration
column two which is marketing spending
and column three is State and you'll see
under State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have xal X
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital X train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referenes when
we're doing different uh models usually
see that a capital x and you see the
train and the test and the lowercase Y
what this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to to shoot for in this tutorial
the next part train test split we take X
and we take y we've already created
those X has the columns with the data in
it and Y has a column with profit in it
and then we're going to set the test
size equals 0. 2 that basically means
20% So 20% of the rows are going to be
tested we're going to put them off to
the side so since we're using a th000
lines of data that means that 200 of
those lines we're going to hold off to
the side to test for later and then the
random State equals zero we're going to
randomize which ones it picks to hold
off to the side we'll go ahead and run
this it's not overly exciting CU it
setting up our variables but the next
step is the next step we actually create
our linear regression model now that we
got to the linear regression model we
get that next piece of the puzzle let's
go ahe and put that code in there and
walk through it so here we go we're
going to paste it in there and let's go
ahead and since this is a shorter line
of code let's zoom up there so we can
get a good luck and we have from the SK
learn. linear model we're going to
import linear regression now I don't
know if you recall from earlier when we
were doing all the math let's go ahead
and flip back there and take a look at
that do you remember this or we had this
long formula on the bottom and we were
doing all this suiz and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
computed individually and then we do the
regressor do fet in this case we do X
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy
xal True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept Ste off
to the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficient for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do the
regressor intercept and let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 * X1 plus M2 * X2
and so on and so on plus C the
coefficient so these variables fit right
into this formula y equal slope 1 *
column 1 variable plus slope 2 * column
2 variable all the way to the m into the
n and x to the N plus C the coefficient
or in this case you have - 8.89 to the
power of 2 etc etc times the First
Column and the second column and the
third column and then our intercept is
the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the R squar value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from sklearn metrics we're going to
import R2 score that's the R squar value
we're looking at the error so in the R2
score we take our y test versus our y
predict y test is the actual values
we're testing that was the one that was
given to us so we know are true the Y
predict of those 200 values is what we
think it was true and when we go ahead
and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squ value of. 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression the why linear
regression ression now that we have come
this far I'm planning to take a closer
look at our cells so that we can
estimate cells in the future how about
we hire a data scientist you can see our
two uh corporate individuals looks more
like they should be agents or secret
agents someplace discussing how to
better forward their company and so
they're going to come in and ask the
data scientist to come in good idea this
will help us to keep a constant track of
our sales so why do we need linear
regression let's assume that we need to
predict the number of skiers based on
snow so this happens to be uh we've kind
of jumped one business to the next we're
looking at the skiing business very
popular in a lot of areas and it's based
on snowfall a lot of times you figure
you don't have snow you don't have
skiers but can we actually use something
more specific instead of look it's
snowing and instead of saying Hey look
it's snowing we can actually start
drawing graphs and the graph shows that
with an increase in snowfall the
frequency of skars also increases so and
there's a pretty direct correlation if
you've ever been up to ski areas when
there's a lot of of snowfalls the skiers
all show up because they know it's going
to be better skiing right afterward so
it's kind of easy to see why skiers and
snowfall would go together and usually
draws a nice straight line and you can
easily predict how many skiers and that
way you can also predict how many people
you need to service them how many lifts
to have up and running and all the stuff
that goes with running a ski area thus
we see that the number of skiers are
directly proportional to the amount of
snowfall regression models a relation
between a dependent Y and an independent
X variable this is real important to
understand regression because when we
talk about linear regression it's the
basis of almost all our machine learning
algorithms out there and it's usually an
underlying part of the math in our deep
learning so it all starts here with
linear regression and we talk about a
dependent Y and an independent X
variable these are numbers we're usually
talking about floats so we want to have
an actual number value coming out and
that's different than something that's
categorical where we want to know yes or
no true false so regression means we're
looking for a number the the independent
variable is known as predictor variable
inde dependent variable is known as the
response variable so we have one
predictor variable the amount of
snowfall and one response variable how
many skiers are going to show up and
then we can take this relationship and
it can be expressed as y equals beta
we're going to use the Greek um letter
beta and we have beta not plus beta 1 X1
and if you continue out it'd be plus
beta 2 X2 and so on till the nth degree
in this case snowfall is an independent
variable and skiers is a dependent
variable so we kind of had a little
quick overview let's go ahead and talk a
little bit more about what is linear
regression so we're going to go from why
predicting number of skiers for snowfall
and uh we looked at the formula a little
bit but let's look a bit closer at what
exactly is going on with linear
regression and the question is going to
come up in an interview what is linear
regression linear regression is a type
of statistical analysis that attempts to
show the relationship between two
variables linear regression creates a
predictive model on any data showing
Trends in data the model is found by
using the least Square method there are
other methods the least Square method
happens to be the most commonly used out
there and we usually start with the
least Square method since it's the most
common and usually works the best on
most models and we're already looking at
how linear regression works but let's
dig deeper into it and let's take a
closer look how linear regression works
I will provide you with a data set which
has rent area other information in it
looks like our secret agents have uh put
on their casual wear for this one you
need to predict rent accordingly we are
given area and rent here so you can do
linear regression with more variables
but we're just going to look at the area
and then from that try to figure out
what the rent should be we plot the
graph and if you look at it here you can
see that the graph kind of a linear
pattern with a little dip in it so the
area seems the rent seems to be based on
the area in most of our work as data
scientists we usually try to start with
a physical graph if we can so we can
actually look at it and say hey does
what I'm fitting to this graph look
right you can solve a lot of problems
that arise by looking at the graph and
saying no that doesn't look right at all
or oh I should be looking over here for
this then we find the mean of area and
rent so the mean just means average and
if you take one two 3 four five and add
them all together and divide by in this
case there's five areas you get three
and the rent is 2 465 and 8 you had all
together and divide by five you get five
and we plot the mean on the graph so you
can see right here here's the mean of
the rent and the area it's kind of a
very Central Point which is what we're
looking for the average of everything in
there the best fit line passes through
the mean this is real important when
you're um eyeballing it as humans we can
do this fairly easy if it's a straight
line through a bunch of data it gets
more complicated mathematically when you
start adding multiple variables and
instead of a line you have a curve but
for basic linear regression we're going
to draw a line line through the data and
the line should go through the means
that's going to be part of the best fit
for that line but we see there are
multiple lines that can pass through the
means so depending on how you look at it
you can kind of Wiggle the line around
back and forth so we keep moving the
line to ensure that the best fit line
has the least Square distance from the
data points and this is you can see
right here residual we have our data
point and then we look at this distance
between them and we square that distance
and that's what they mean by the least
squared distance we want all those
distances to add up to the smallest
amount we can when we talk about that
distance we call it the residual it
equals the Y actual minus the Y
predicted very straightforward just
looking at the distance you can
accidentally switch these and it'll come
out the same because we're going to
square it but when you're working with
other sets of linear regression you want
to make sure you do the Y actual minus
the Y predicted the value of M and C for
the best fit line y equal mx + C can be
calculated using the mentioned formula
and you can see here we have m equal
equals the number of points that's what
n stands for then we have the sigma the
Greek symbol is there which is a
summation of x * y minus the summation
of x * the summation of Y over the
number of the summation of X2 minus the
summation of X2 of X all of it squared
that can be a little confusing trying to
say that and then of course your C value
is going to be the summation of y * the
summation of x^2 minus the summation of
x * the summation of x * y over the
total count of the summation x^2 minus
the summation of x^ squar that's a
mouthful normally we don't worry too
much about these formulas other than to
know they're there now if you're a
mathematician you might go back in there
and work on those and those people who
originally started to put together the
different models in R of course had to
know all this math to build them and had
to test it out that's one of the nice
things about R what is important is that
you know about these formulas so that
you know where it's coming from so if
you're using one linear regression model
this is how this one works there's other
linear regression models based on
different math and so you'd have to be
aware when that math changes and how
that changes the model we find the
corresponding values so in this case if
you're going to break this up and you're
building your own model instead of using
the one in R you would have your rent
you would have your area you would find
your rent squared your area squared and
your rent times area and if we go back
one we can see that that's all the
different little pieces in this model we
have x * y we have x we have the sum of
X which is going to be squared we have
the sum of X the sum of Y this is the
same these are all the little pieces in
here that we see for m equals and C
equals and so once we compute all these
we have 15 25 55 145 88 very easy to do
on a computer thankfully you could have
millions of these points and it would do
it in a very short period we can then
plug in those values very easily into
this formula and get a final answer and
we'll see that m equal 1.3 and C = 1.1
and then when we take this for the Y
predicted equals m of i x of I + C we
can take these and actually run the data
we have so we're going to see how it
predicts so now we find the Y the value
of y predicted so we have our xn and we
want to know what Y what we think y will
be based on our formula we generated the
linear regression formula and so we come
in here we have our X we have our actual
y then we have our y predict what we
think it's going to be and then we have
uh we take y y - y predict and we get
those values minus point4 it's off by. 3
this one's off by one this one's on off
by minus 1.3 by4 and we go ahead and
square that and then we can sum that
square and average it out and we'll get
a value this is just a summation down
here of Yus y predicted square is 3.1
and we call that the least Square value
for this line is 3.1 and we go ahead and
divide that by five so when we plot the
Y predict and this is the best fit line
and you can see that does a pretty good
job going right through the middle of
lines and in something like rent versus
area if you're trying to figure out how
much rent to charge or how many people
are allowed to be in the area that's
going to work but if you're looking for
the rent value compared to the area this
gives you a good idea based on the area
of what you should rent the place for
it's close enough that in the business
world this would work you're not
Computing something down to the
millimeters or micrometers or Nuclear
Physics we're just charging people and
if you're off by a couple dollars it's
not a big deal you'll be pretty close to
what you think it's worth and get the
right value for your property use case
predicting the revenue using linear
regression now that you have a good idea
of how linear regression works and we're
kicking back on our lounging sofa today
let's work on a real life scenario where
you have to predict the revenue so we're
going to take a data set and we'll pull
this up in just a minute we'll have a
paid organic social and revenue there
are three attributes we'll be working on
the first one is our paid traffic so all
the traffic through the the
advertisement that comes in the non-paid
traffic from search engines so these are
people coming to the website and are
doing so because they did a search for
something specific so it's pulling it
from your website all the traffic coming
from social networking sites so we want
to know what's coming from Twitter and
Facebook and somehow they've linked into
your website and coming into your
marketing area to buy something and
we'll use this to predict the revenue so
we want to know how all this traffic
comes together and how it's going to
affect our CES if the traffic's coming
in we're going to make use of the
multiple linear regression model so
you'll see that this is very similar to
we had before where we had y = m * x + C
but instead we have y = M1 x1+ M2 X2
plus M3 X3 plus c m is the slope so you
have M1 M2 M3 the three different slopes
of three different lines Y is the
revenue so we're only looking for one
variable and remember this is regression
so we're looking for a number X1 is
going to be the paid traffic and of
course M1 the corresponding lope X2 the
organic traffic and X3 the social
traffic we will follow these steps to
create the regression model we'll start
by generating inputs using the CSV files
we'll then import the libraries
splitting the data into train and test
very important whenever you're doing any
kind of data science applying regression
on paid traffic organic traffic social
traffic so we're going to apply our
regression model on them and then we're
going to validate the model that's the
whole reason we split it to a training
and testing is so we can now validate to
see just how good our model is now
before we open up our I always like to
just take a quick look at the data
itself and we're using the rev. CSV file
and this is a CS file or comma separated
variable file we'll just open that with
word pad any kind of text editor would
be nice to show you what's going on and
you can see we have paid organic social
and revenue across the top so those are
your titles for your columns and the
values going down and this is the same
as what we saw on the slide just a
minute ago paid organic social in the
revenue from them they probably have it
in more of a spreadsheet than just a
word pad and I'm using the newer version
of our studio which already has a nice
layout automatically opens up with your
console and terminal window down and
left you have your script and we'll run
it in script and then when I run it line
by line in script you'll see it show up
on the console bottom left I could type
it straight into the console and it
would do the same thing and then on the
right we have our environment and under
the environment we'll see uh plots if
you're in an older version the plots
will pop up and you usually are running
just in the console and then you have to
open up a different window to run the
script and you can run the script from
there this opens up everything at the
same time when you use the new R Studio
setup and the first thing we want to do
is we want to go ahead and generate a
variable we'll call it cells and we want
to load this with our data set and so uh
in R we use the less than minus sign
that's the same as aign to it's like an
arrow pointing at cells and it says
whatever it goes past here is going to
be assigned to cells and we want to read
in our CSV file and then I went ahead
and uh fixed the CSV file up a little
bit when I say the CSV file I mean the
path to it um and we'll go ahead and
just paste that in there and this is my
edited version and you'll see I took
wherever we had the um backward slash I
switched it for a forward slash because
a backward slash is an escape character
and whenever you're programming in just
about any scripting language you want to
do the forward slash instead so uh like
a good Chef I prepped this in copied it
into my clipboard and we can see the
full path here and the full path ends in
our rev. CSV file so that's the path on
my machine and I'm going to run that and
now we have down here you see it appear
right here uh oh must have an error in
my path it says object rev not found let
me fix that real quick and run it again
and this time it does assign the cells
the data set and then we'll just type in
cells and this is the same as printing
it so if you work in other languages
you'll do like print and then sell or
maybe print cells. head if you're in
pandas and python or something like that
but in R you just type in the variable
we'll run this and you can see it
printed out all those columns so that's
the same thing we just looked at and it
says uh Max print so it emitted 750 rows
so it has a total of a th000 rows and we
can scroll up here on our console where
the answer came in and if I scroll up to
the top who way up to the top there we
go you'll see paid organic social and
revenue so everything looks good on this
and then there's a lot of cool things we
can do with it one of the things we can
do is I can summary it so let's do a
summary of celles and we'll run that and
the summary of cells comes in it tells
you your Min first quarter median mean
third quarter Max for paid organic
social breaks it up and you can actually
get kind of a quick view as to what the
data is doing you can take more time to
study this a lot of people look at this
and you can see from the median the mean
that tells you a lot and it's divided
into quarters so you have your first
quarter value and your third quarter
setup on there and Max it just gives you
a quick idea of what's going on on in
the data summary is a really wonderful
tool one of the reasons I like to jump
into R before I might do a major project
in something else or you can run
everything in our and let's take a look
at the head head of cells and let me run
this and you can see it just shows the
head of our data the first six rows in
this case and it starts with one
important to know other different
systems start with zero and go up R is
one that starts with the first as one
and then uh if we want to do a plot I'll
go ahead and do a plot cells and another
really powerful tool that you can just
jump right into with r once you have
your data in here and you'll see that
the plot comes over on the right hand
side and what it does is it Compares
them so if I look at my paid and my
organic and you cross it two you'll see
there's a nice line down the middle big
black line where it plots the two
together let me just widen this a little
bit so we can see more of it there we go
the same thing with paid and social paid
and revenue so each one of these has a
clear connection to each other so that's
a good sign we have have a nice
correlation between the data and you can
just eyeball that that's a good way to
jump in and explore your data rather
quickly and see what's going on so we'll
go ahead and I'll put a um pound sign
there hashtag and uh splitting the data
into training and test data and we want
to split it so that we have something to
work with to train our data set and when
we do our data set you do this with any
data set because you want to make sure
you have a good model and then once
we've trained it we're going to take the
other set of dat we pulled off to the
side and we'll test it to see how good
it is if you test it with the training
data it's already got that memorized and
should give you a really good answer in
fact when you get into more advanced
machine learning tools you watch those
very closely you watch your training set
and how it does versus your testing set
and if your training set starts doing
better than your testing set then you're
starting to overtrain it now that's more
with deep learning and uh some other
packages that are beyond what we're
working with today but you know this is
important it's very important to
understand that that test in that
training correlate with each other and
gives you an idea what's going on and
make sure that you're set up correctly
and we'll go ahead and set a seed of two
and that's for our random generators so
that when we run them you could see them
with just a r there's ways to randomize
a number you could do like date and
stuff like that but that way we always
use the same random numbers so we'll
seat it with two and then we're going to
need a library in this case we're going
to import the library and we want to
import CA tools and CA tools has our
split function so we're going to be able
to split our data along with many the
other tools we're going to need for this
example so they're all built into the ca
tools that's why we need to import that
a lot of times they call them library or
sometimes you he them referred to as
packages so let's go ahead and run that
line and that brings the library in so
now we can use all those different tools
that are in there and then I'm going to
do a split and we're going to assign to
the split that's going to be our
variable sample split okay so that's one
of the tools we just brought in is to
sample is the actual keyword or the
function word sample dosit and we're
going to take our cells and we're going
to go ahead and a split ratio equals to7
and let's do 0.7 so we make sure it
knows it's a decimal point or float and
what this is saying is that we're going
to take our um cells data the variable
we created called cells that has the
data in there and I want to split it and
I'm going to split it so that 70% of it
goes in one side we'll use that for
training and 30% will go into the other
side which we'll use then for testing
out our model let me go ahead and run
that and oops hold on one second got an
error there spit is definitely very
different than split I don't think we
want to spit our data out we which is
actually kind of what we're doing is
spitting it out but we want to split we
want to split the data some me get that
spelled correctly and then when I type
in Split we can just run that because
that's now a variable you'll see that it
has true false false true so it
generates a number of interesting
different statistics going across there
as far as the way it splits the data and
right here here if you have not used R
in a while or um if you're new to R
completely that line one that's what the
little one means down there and then
true false false true that means that's
how we're looking at the data we're
splitting all those different pieces of
data in line one different directions
and so we now want to create our train
set and we're going to assign that and
when we take it and we assign to the
train set a subset of cells and then the
subset of cells is going to be based on
split equal and put this in Brackets
true so you can see the values down here
is capital T capital r capital u capital
E so I just want to reflect that on the
train and this is going to take
everything where our split variable is
equal to true and it's going to set
cells equal to that and we'll go ahead
and run that and then we're going to set
our test variable as a subset of cells
and if we assign the true to the train
set then we want to assign the false to
our test set so now we'll have uh this
will have 30% of the variables in it and
train will have 70% we'll go ahead and
run that so we now created both our
training set and our test set and we can
just real quickly type in train hit the
run on there and you can see our train
set if we scroll way up to the top I'll
have the column names on there which
should match what we had before paid
organic social and revenue and then
we'll type in test when I hit run on
there that's going to do the same thing
it'll sped out all the test variables
going out so now that we have the test
and train variables we want to go ahead
and create the model and you'll see this
in any of the machine learning tutorials
they always refer to them as models
we're modeling a function of some kind
on the data to fit the data and then
we're going to put the the test data
through there to see how well it does
and so we're going to create the
variable called models and I'm going to
assign that LM that is our linear
regression model I love how simplified R
makes it just LM linear regression and
then model M um I guess it dates back
cuz the linear regression model is like
one of the first major models used so
they kept it easy on there and uh
Revenue happens to be the main variable
that we're want to track so if you
remember correctly from our formula Y is
the revenue and then we have X is our
paid traffic X2 organic traffic and X3
our social traffic so Y is what we want
to predict in here and then you'll see
this notation where we have our squiggle
and the period which which means we're
going to match up the revenue lines with
the lines of the data we're putting in
here and then I'll put comma and then
our data equals train and of course
that's the training data that we created
so when I hit the Run remember we did
all that discussion about all those
different functions and formulas to
compute our model and how that's set up
when it comes down to it we spend all
our time setting up our data and then I
hit the Run button on the single line
and our model's been trained so we now
have a trained model here and we can do
a summary of the model some sum is such
a wonderful command because you can do
that on r that works on all kinds of
different things on there oops and of
course it does help if I remember that
my model has a capital N when I run it
and you'll see right here tells you a
little bit about the model use the
summary on there comes down here has our
residuals this is all the information on
there as far as like the minimum the
median the Max has all our coefficients
in there if you remember correctly from
our formula let's just go ahead and
switch back over to that we have M1 X1
M2 X2 M3 X3 plus c well that's right
here here's our intercept and then we
have our different values for each of
these we have our intercept our paid
organic and social and then it also
shows us error and information on the
error and uh one of the really nice
things about when you're working with
are you can come down here and you see
um where we have our Stars down here and
it says uh three stars really good two
stars maybe one star probably no
correlation and we can see with all of
these it has three stars on them so out
of three stars we get three three stars
in all these there's no four stars four
stars would mean that you have the exact
same data going in as coming out and
then if we're going to do this we want
to actually run a prediction on here and
that's that we saved our test uh data
for so let's come down here we'll do our
prediction we'll take this variable and
we'll sign it predict model and remember
the predict comes from when we imported
that package the ca tools that's all
part of the ca Tools in there so we're
going to predict we're going to use a
model and then we're going to use our
test data pretty straightforward quick
and easy we'll run this and then if we
go ahead and type in the predict it'll
print out what's in that variable and
you'll see down here the predicted
values it expects to come out it's going
to see our revenue is and it goes
through and it gives it both the line
number and the actual Revenue value so
that's quick and easy boy we got a
prediction really fast and this is also
how you would do it if you had new data
coming in after this you could predict
the revenue based on these other factors
so now that we have a training model
which or we train the data with our our
training data or we trained the model
with our training data and we've done a
prediction on our model with our test
data we want to look this up and we took
a quick glance from our training thing
our training said it trained really well
but that's not the final word on it the
final word is going to be comparing it
so we want to go ahead and we going to
do comparing predicted versus actual
values what I'm going to do is I'm going
to do a plot and I'm going to do our
test and we're going to do test revenue.
type equals and we're going to put this
in as a type uh type one and or L and
then we have lty y = 1.8 and we're going
to set it for column blue that's a lot
of stuff in there let's go ahead and
just run that so we can see what that
looks like and what's that doing it's
going to generate this nice graph we see
on the right there's our graph you can
see the data comes up and down um we're
plotting the prediction on there so this
is the values we predicted and then
let's go ahead and do uh lines and I
actually did this backwards so let me
try that again again and we'll start
with the red one we're going to do the
first one in red and I want to start
with the actual test Revenue so here's
our test revenue and we'll go ahead and
run this and so we have our red plot
over here in red go and take a look at
that pull that over you can see how that
looks goes up and down hard to track
there and then we're going to do a
couple things here we'll plot our test
revenue and then will make it a little
pretty although it's kind of hard to say
see the way I have it scrunched up here
on the right as far as size and Ing and
everything but we'll go ahead and have
our um columns in blue and run our
prediction on there too you can see they
overlap the two graphs so we have our
test revenue and our prediction and then
finally what I started with is we'll go
ahead and plot the prediction fully on
the bottom and run that and if you look
over here on the graph we put the blue
lines over the red lines and so you'll
see a couple spots where there's red
underneath there but for the most part
our predictions pretty right on so it
looks really tight looks like a good set
of predictions for what we're working on
and this is what we were looking at the
slide right before we started diving
into the r Studio we can see in the
slide here here's a red that's a little
bit better spread out than uh what I
have on my screen for my R studio and
the graph shows a predicted Revenue we
see that the two lines are very close so
again they're tight it's right on this
is what we're looking for in our
prediction model but it's not good
enough just having a graph you always do
both and the reason we do both is you
want to have a visual of it cuz
sometimes you look at these you get to
the graph you're like oh my gosh what is
that and then sometimes you look at the
graph you go that's right on how come my
um accuracy doesn't look right and you
realize that your accuracy might be off
or you're plotting it incorrectly so
let's go ahead and look up the accuracy
and we'll use rmsse that's going to be
the variable name we're going to give it
and sqrt the square root of the mean and
mean just means average and then we want
prediction minus cells revenue and then
we're going to take this whole thing and
we're going to square it so we we've got
here is we're going to go through let's
just explain this formula we're using is
I'm going to look at the what I predict
it for to be and what the actual sales
what's our prediction versus our sales
comparison to the revenue and when we
compare those two we're going to find
the average and then I'm going or we're
going to square each one of those values
and then we're going to average the
square and then find the square root of
that and that's quite a mouthful the
reason we do it this way the reason we
Square the value and then we find the
means is to generate an answer based on
doesn't matter whether it's plus or
minus there's a lot of other formulas
you can use there to check your accuracy
and all kinds of other things you can do
but a quick straightforward way of doing
it is just like this and then let's go
ahead and run this and then we type in
the rmse and that'll give us an actual
printed value out let's just see what
that looks like and so we have
86663387945
that to say uh 50,000 that's pretty
close that's that's actually a pretty
good accuracy given this model takes a
little bit more when you're playing with
accuracies and start dividing out other
different aspects of it to get something
you can communicate better with but you
always start with the square root means
that's always a good place to start
there's no better time to train yourself
in the exciting field of machine
learning if you're looking for a course
that covers everything from the
fundamentals to Advanced Techniques like
machine learning algorithm development
and unsupervised learning look no
further than our CCH post program in Ai
and machine learning in partnership with
IBM this a ml course coveres the latest
tools and Technologies from the AI
ecosystem and features master classes by
celtech faculty and IBM experts
hackathon and ask me anything sessions
this program showcases Kelch ctm is
excellence and IBM's industry progress
the artificial intelligence course
covers key Concepts like statistics data
science with python machine learning
deep learning NLP and reinforcement
learning through an Interactive Learning
model with live sessions and R now and
unlock exciting a ml opportunities the
course link is mentioned in the
description box below what is logistic
regression let's say we have to build a
predictive model or a machine learning
model to predict whether the passengers
of the Titanic ship have survived or not
the ship rck so how do we do that so we
use logistic regression to build a model
for this how do we use logistic
regression so we have the information
about the passengers their ID whether
they have survived or not their class
and name and so on and so forth and we
use this information where we already
know whether the person has survived or
not that is the labeled information and
we help the system to train based on
this information with based on this
labeled data this is known as labeled
data and during the process of building
the model we probably will remove some
of the non-essential parameters or
attributes here we only take those
attributes which are really required to
make these predictions and once we train
the model we run new data through it
whereby the model will predict whether
the passenger has survived or not let's
start with what is supervised learning
supervised learning is one of the two
main types of machine learning methods
here we use what is known as labeled
data to help the system learn this is
very similar to how we human beings
learn so let's say you want to teach a
child to recognize an apple how do we do
that we never tell the child okay this
is an apple has a certain diameter on
the top certain diameter at the bottom
and this has a certain RGB color no we
just show an apple to the child and tell
the child this is Apple and then next
time when we show an apple child
immediately recognizes yes this is an
apple supervised learning works very
similar on the similar lines so where
does logistic regression fit into the
over overall machine learning process
machine learning is divided into two
types mainly two types there is a third
one called enforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
uses techniques like classification and
regression now supervised learning is
used when you have labeled data you have
historical data that then you use
supervised learning when you don't have
labeled data then you us unsupervised
learning it's in supervised learning
there are two types of techniques that
are used classification and regression
based on what is the kind of problem we
are solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict the share price or you want to
predict the temperature that will be
there what will be the temperature
tomorrow that is where you use
regression whereas classification are
discrete values is will the customer buy
the product or will not buy the product
will you get a promotion or you will not
get a prom motion I hope you're getting
the idea or it could be multiclass
classification as well let's say you
want to build an image classification
model so the image classification model
would take an image as an input and
classify into multiple classes whether
this image is of a cat or a dog or an
elephant or a tiger so there are
multiple classes so not necessarily
binary classification so that is known
as multiclass classification so we are
going to focus on classification because
logistic regression is one of the
algorithms used for classification now
the name may be a little confusing in
fact whenever people come across
logistic regression it always causes
confusion because the name has
regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you're wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we going to focus on
classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few decision trees are
quite popular and very easy to
understand and therefore they used for
classification then we have K nearest
neighbors this is another algorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into a little bit
of details about logistic regression and
if you're in SPID python Enthusiast or
an A ml Enthusiast looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers in AI by
learning from the experts then try
giving a short to Simply learns
postgraduate program in Ai and machine
learning in collaboration with P
University and IBM and the course link
is mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered the term
generative AI has emersed seemingly out
of nowhere in recent months with a
notable search in interest according to
Google Trends even within the past year
the spike in curiosity can be attributed
to the introduction of generative models
such as d 2 B and
chgb however what does generative AI
entail as a part of our introductory
series on generative AI this video will
provide a comprehensive overview of a
subject starting from the basics the
explanation Will C to all levels of
familiarity ensuring that viewer gain a
better understanding of how this
technology operates and its growing
integration to our daily lives
generative AI is after all a tool that
is based on artificial intelligence a
professional who Els to switch careers
with AI by learning from the experts
then try giving a short simply L
postgraduate program in Ai and machine
learning from P University in
collaboration with IBM the link in the
description box should navigate to the
homepage where you can find a complete
overview of the program being offered
take action up skill and get ahead what
is is generative AI generative AI is a
form of artificial intelligence
possesses the capability of to generate
a wide range of contact including text
visual audio and synthetic data the
recent excitement surrounding generative
AI stems from the userfriendly
interfaces that allow users to
effortlessly create high quality text
graphics and video within a seconds now
moving forward let's see how does
generative AI Works generative AI begin
a prompt which can take form of text
image video design audio musical notes
or any input that AI system can process
various AI algorithm that generate new
content in response to the given prompt
this content can range from essay and
problem solution to realistic created
using images or audio of a person in the
early stages of generative AI utilizing
the technology involved submitting data
through an API or a complex process
developers need to acquaints with a
specialized tool and writing application
using programming language like like
python some of the recent and fully
operational generative AIS are Google
Bart Dal open AI chat gbt Microsoft Bing
and many more so now let's discuss chat
GPT Dal and B which are the most popular
generative AI interfaces so first is DAL
2 which was developed using open as GPT
implementation in 2021 exemplify a
multimodel AI application it has been
trained on a v data set of images and
corresponding textual description Dal is
capable of establishing connection
between various media forms such as
Vision text audio it specifically links
the meaning of words to visual elements
open a introduced an enhanced version
called d 2 in 2022 which empowers user
to generate imagery in multiple Styles
based on their prompts and the next one
is CH gbt in November 2022 chat GP and
AI power chatboard built on open AI GPD
3.5
implementation gained immense popularity
worldwide open AI enable user to
interact with and fine tune the chat Bo
text responds through a chat interface
with interactive feedback unlike earlier
version of GPT that was solely
accessible via an API chat GPT brought a
more interactive experience on March 14
2023 open a released GPT 4 chat GPT
integrate the conversational history
with a user making a genuine dialogue
Microsoft impressed by the of new chat
GPT interface announced a substantial
investment in open Ai and integrated a
version of GPT into its B search engine
and the next one is Bard Google bard
Google was also an earlier Fortuner in
advancing Transformer AI techniques for
language processing protein analysis and
other content types it made some of
these model open source for researchers
but were not made available through a
public interface in response to
Microsoft integration of GPT into being
Google hardly launched a public facing
chat Bo named Google Bart bar debut was
met by an error when the language model
incly claimed that the web telescope was
the first to discover a planet in a
foreign solar system as a consequences
Google stock price suffered a
significant decline meanwhile Microsoft
implementation of chat GPT and GPT power
system also face criticism for producing
inaccurate result and displaying eratic
behavior in their early irritation so
moving for forward let's see what are
the use cases of generative AI
generative AI has broad applicability
and can be employed across a wide range
of use cases to generate diverse form of
content recent advancement like GPT have
made this technology more accessible and
customizable for various application
some notable use cases for generative AI
are as follows chatbot implementation
generative AI can be utilized to develop
chatbots for customer service and
Technical Support enhancing interaction
with users and providing efficient
assistance
the second one is language dubbing
announcement in the real in the realm of
movies and educational content
generative AI can contribute to
improving dubbing in different languages
ensuring accurate and high quality
translation and the third one is content
writing generative AI can assist in
writing email responses dating profiles
resumes and term papers offering
valuable support and generating
customized content tailor to specific
requirement and the fourth one is Art
generation leveraging generative a
artists can create photo realistic
artwork in various Styles enabling the
exploration of new artistic expression
and enhancing creativity the fifth one
is product demonstration videos
generative AI can hun to enhance product
demonstration video making them more
engaging visually appealing and
effective in showcasing product features
and benefits so generative AI
versatility allow it to employed in many
other application making it a valuable
tool for Content creation enhancing user
experience across diverse domains so
after seeing use cases of generative AI
let's see what are the benefits of
generative AI so generative AI offers
extensive application across various
business domains simplifying the
interpretation and comprehension of
existing content while also enabling the
automated creation of a new content
developers are actively exploring ways
to leverage generative AI in order to
enhance the optimize existing workflows
and even to reshave workflows entirely
to harness the potential of Technology
fully implementing generative a can
bring numerous benefits including
automated content creation generative AI
can automate the manual process of
writing content saving time and effort
by generating text or other form of
content the next one is efficient email
response responding to emails can be
made more efficient with generative a
reducing the effort required and
improving response time and the third
one is enhanced technical support
generative AI can improve responses to
specific technical queries providing
accurate and helpful information to
users or customers and the fourth one is
realistic person Generation by
leveraging generative AI it becomes
possible to create realistic
representation of people enabling
applications like virtual characters or
avatars and the fifth one is coherent
information summarization generative AI
can summarize complex information into a
coherent narrative distilling key points
and making it easier to understand and
communicate complex concept the
implementation of generative AI offers a
range of potential benefits steamingly
processed and enhancing content Creation
in various areas of business operation
so after seeing advantages of generative
AI let's move forward and see what are
the limitations of generative AI early
implementation of generative AI serve as
Vivid examples highlighting the numerous
limitation associated with this
technology several challenges arise from
the specific approaches employed to
implement various use case for instance
s while a summary of a complex topic May
more reader friendly than explanation
incorporating multiple supporting
sources the ease of readability comes at
the expense of transparent identifying
the information sources so the first one
is when implementing or utilizing a
generative AI application it is
important to consider the following
limitation I repeat the first one is
lack of source identification generative
AI does not always provide clear
identification of content Source making
it difficult to trace and verify origin
of the information the second one is
assessment of bias assessing the bias of
original sources used generative AI can
be challenging as it may be difficult to
determine the underlying perspective or
agenda of the data utilized in the
training process the third one is
difficulty in identifying inaccurate
information generative AI can generate
realistic content making identifying
inaccuracy or falsehoods within the
generated output harder and the fourth
one is
adaptability to a new
circumstances understanding how to
fine-tune generative AI for a new
circumstances or specific context can be
complex requiring careful consideration
and expertise to achieve desired result
and the fifth one is glossing over bias
Prejudice and hatred generative AI
results May amplify or prep petate
biases prejudices or hateful content
present in the training data requiring
Vigilant scrutiny to prevent such issues
so awareness of these limitation is
crucial when the implementing of
utilizing generative AI as it helps
users and developers critically evaluate
and mitigate potential risk and
challenges associated with the
technology so future of generative a
furthermore advances in AI development
platforms will contribute to the
accelerated progress of research and
development in the realm of generative
AI the development will en Compass
various domains such as text images
videos 3D contact drugs Supply chains
logistic and business processor while
the current stand loan tools are
impressive the true transformative
impact generative AI will realize while
these capabilities are seemingly
integrated into I rep into the existing
tools with regular use and we also have
a boot camp for you guys and if you are
one of the aspiring Ai and ml Enthusiast
looking for online training and
graduating from the best universities or
a professional who elicits to switch
cers in Ai and ml by learning from the
experts then try giving a short to
Simply learns keltech Ai and machine
learning boot camp the course link or
the boot camp link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete overw of the program being
offered hello and welcome to this video
on edge AI for artificial intelligence
if you're interested in learning more
about how AI is revolutionizing the
world around us this video is for you
Aji is a New Concept in the world of AI
that refers to devices and sensors at
the edge of the network that can collect
and process data in real time this
allows for faster more accurate decision
making and greater efficiency in a
variety of applications such as
self-driving Vehicles Smart Homes and
health monitoring so how is Aji
different from cloud computing and cloud
computing AI the main difference is the
location where the data is processed AI
enables devices at the edge of the
network to process data locally without
having to send it to the cloud this can
lead to faster more responsive systems
and can help you Reserve bandwidth and
reduce latency in contrast cloud
computing AI processes data in the cloud
where it can assess vast amount of data
and compute resources but also requires
a reliable internet connection and can
be slower as Aji becomes more advanced
it has the potential to transform many
Industries such as Healthcare
Transportation manufacturing and a lot
more it can enable new applications such
as autonomous vehicles remote surgery
and real-time monitoring of machines and
equipment ments while cloud computing AI
has many benefits AI offers the
advantages of faster decision- making
reduced latency and improv efficiency so
if you're interested in learning more
about the future of AI Aji is definitely
a topic worth exploring on that note
hello everyone and welcome to Simply
learn in today's video we'll understand
what Edge Computing is and how it is
different from traditional cloud
computing but before we begin make sure
to subscribe to our channel to stay
updated with all the latest Technologies
and hit that Bell icon to never miss any
updates from us so without any further
delay let's get started but wait if you
want to become an expert in Edge
Computing then look no further than
Caltech postgraduate program in a
machine learning offered by simply learn
which is an Onin program designed to
provide students with a comprehensive
understanding of AI and machine learning
Concepts tools and techniques this
program is created in collaboration with
CTIC ctme and IBM and it is designed to
provide Learners with a strong strong
foundation in Ai and and machine
learning Concepts this program covers
various topics such as data science
statistics deep learning computer vision
and NLP and help you gain the right
skill set on various tools such as caras
matb Leb tensorflow Jango and many more
furthermore the curriculum is structured
around interactive online classes live
sessions with industry experts and
Hands-On projects so why wait hurry up
enroll now in Caltech postgraduate
program in a machine learning and create
your own successful career link is add
in the description box below so make
sure you check that out so without any
further Ado let's jump directly into our
today's topic so firstly let us
understand what is Edge Computing the
term Edge Computing for AI refers to the
process of running Ai computations and
processing close to the data source or
Edge devices at the edge of a network as
opposed to relying on centralized
cloud-based servers in order to enable
real-time data analysis pick our
response times and a decreased Reliance
on cloud infrastructure it deploys AI
models and algorithms directly on edge
devices or local
servers AI Edge Computing typically
entails the deployment of lightweight AI
models designed to use on edge devices
with constrained computational power
then these models are frequently
deployed to the edge after being trained
using cloud-based resources The Edge
devices gather data from their sensors
or from outside sources process the data
locally using the AI models that have
been deployed and give prompt responses
or start processes based on on the data
but what L to the rise of H AI well with
rapid advancement in technology in every
sector and businesses are looking to
increase automation to increase workflow
productivity and security and for that
computer programs must be able to
identify patterns and Carry Out tasks
repeatedly and safely in order to assist
people now the range of tasks that human
perform however covers in different
situations that are impossible to fully
describe in codes and rules because the
world is very unstructured
well with the help of developments in
haai machines and gadgets can Now
function with the intelligence of human
cognition wherever they may be smart
applications with AI capabilities can
learn to carry out the same task under
various conditions just like in real
life and here are the three recent
innovations that explain why using AI
models at the edge is on the rise well
first on the list we have advancement of
neural networks now over the years as
you know neural networks and related AI
infrastructure have advanced and matured
allowing for more generalized machine
learning capabilities this means
organizations have to gain a deeper
understanding on How to Train AI models
effectively and deploy them in
production at the edge development in
computational infrastructure now running
AI at the edge requires substantial
computational power the field has
witnessed notable advancements in
computational infrastructure to support
AI at the edge particularly recent
strikes in highly parallel graphic
processing units or gpus and CPUs have
enabled efficient execution of new
neural networks gpos are specifically
designed to handle complex parallel
computation making them a suitable
choice for processing Ai workloads and
finally widespread usage of iot devices
now the widespread adoption of Internet
of think devices has generated vast
amounts of data these devices such as
industrial sensors smart cameras and
Robotics provide a rich source of data
that can be utilized for AI analysis at
the edge the availability of diverse iot
devices and their ability to capture
real-time data has facilitated the
deployment of AI models at the edge but
why deploy AI at the edge what are the
benefits of edge Computing now Edge
Computing for AI addresses the
limitations and challenges associated
with conventional Cloud Centric AI
methods so here are some of the
advantages which it offers firstly
decreased latency Now by performing AI
processing at the edge there is a
significant reduction in network latency
since data no longer needs to be
transmitted to the cloud and back this
is particularly beneficial for
applications that demand real time
responses or decision making let's say
like autonomous vehicles or Industrial
Automation secondly realtime data
analysis Ed Computing for AI enables IM
data analysis and inference directly on
the edge devices themselves this is
especially valuable for time senstive
applications that require realtime
insights or predictions eliminating the
need for Reliance on cloud
connectivity enhanced privacy and
security well Edge Computing enhances
privacy and security by keeping data and
AI process local sensitive information
can be processed on the edge devices
without being transmitted to the cloud
minimizing the risk of data baches and
ensuring compliance with data privacy
regulations next we have bandwidth
optimization Edge Computing reduces the
volume of data uh that needs to be
transmitted to the cloud sending only
relevant or let's say summarized data
now this optimization minimizes
bandwidth requirements reduces Network
cost and even lightens the load on the
cloud infrastructure in total and
finally it offers offline functionality
Edge Computing enables AI models to
function even in situation which limited
or no internet connectivity this
capability is crucial for scenarios that
require continuous operation in remote
or disconnected environments such as
certain iot applications or field
deployments so these were some of the
reasons why we need to deploy AI at the
edge and the benefit it's offering now
comes the main question how exactly does
Edge Computing works well Edge computing
is primarily focused on the proximity
and location of data processing and
storage the fundamental principle is
simple when it's not feasible to bring
data to a centralized data center the
alternative is to bring the data center
close to the data now in order to enable
machines to perform tasks like let's say
visual perception object detection
driving cars understanding speech
speaking walking and emulating other
human skills they need to replicate
human intelligence in a functional
manner so for that AI users
a deep neural network as a data
structure to emulate human cognition
these dnns undergo training in data
centers and are exposed to numerous
examples of specific task or question
type along with their correct answer and
once the training is complete the model
advances to become a inference engine
capable of addressing real world
questions now from there on here's how
it typically Works firstly deployment of
inference engine now in AJ deployments
the inference engine which is the
component responsible for making
prediction
or performing AI tasks is deployed on
local computers or devices located in
diverse environments like factories
hospitals cars satellites homes and much
more problem identification and data
upload now while the inference engine
operates at the edge it may encounter
situation where it struggles to provide
accurate predictions or encounter
challenging scenarios in such cases the
problematic data associated with these
instances is commonly identified and
upload to a centralized Cloud structure
now from there on there is a continuous
cloud-based model training now the cloud
infrastructure typically in a data
center receiv receives the problematic
data from Edge devices data scientists
and AI experts can then utilize this
data to refine and train the original AI
models further this involves using the
Cloud's computation resources larger
data sets and collaboration tools for
model training and Improvement now and
finally with the updated model deployed
at the edge after being retrained the AI
system continues to operate gaining new
data and insights from its environment
the AJ model can collect further
additional data which can be
periodically uploaded to the cloud and
further training and continuous
Improvement this feedback loops ensures
that the edji models become
progressively smarter over time so this
is in a nutshell how Edge Computing for
AI Works hope you have understood well
let us talk some of the use cases of
edge Computing for AI in real world well
the combination of AI iot and Edge
Computing has opened up a multitude of
of opportunities for Edge AI
revolutionizing the various aspects of
our daily lives so let's Del into some
key areas where Edge AI is making a
significant impact first on the list we
have healthare hii is enhancing medical
Diagnostics and patient care for
instance by deploying AI algorithms on
medical imaging devices Radiologists can
accurately and efficiently identify and
analyze pathologies variable devices
with local Edge analytics enable
realtime monitoring of patient data
facilitating any early detection of
anomalies And Timely
interventions next autonomous vehicles
Edge air plays a pivotal role in
enabling realtime navigation and
decision making for autonomous vehicles
AI algorithms running on edge devices
within Vehicles process sensor data
analyze the environment and make
split-second decisions to ensure safe
and efficient driving agriculture hji is
being used to address agriculture
challenges as well such as pollination
autonomous drones equipped with AI
capabilities can identif ify and
pollinate plants aiding in crop
cultivation and maximizing yelds this
application demonstrates the diverse
potential of edge AI Beyond traditional
domains next we have smart home and
cities now H AI enhances the
functionality and efficiency of Smart
Homes and cities local processing and
Analysis of data from iot devices enable
real-time automation Energy Management
security surveillance and personalized
user experience Aji power system adapt
to individual preferences optimize
resources use Ag and respond to events
in real time next we have industrial
applications Edge AI is transforming
Industries thoroughly through realtime
monitoring predi of Maintenance and
process optimization Edge devices
equipped with AI algorithms analyze
sensor data detect anomalies and predict
equipment failures reducing downtime and
improving efficiency next we have
security and surveillance while a plays
a crucial role in security and
surveillance which empow realtime video
analytics for secur security and
surveillance applications Edge devices
with AI capabilities can analyze video
feeds locally detect suspicious
activities like theft identify objects
or individuals of interest and Trigger
alerts or responses enhancing security
measures and reducing response times and
finally we have personal devices edai is
increasingly being integrated into
personal devices such as your
smartphones laptops variables and smart
home assistants this allows for on
device AI processing enabling faster and
more personalized experience without
relying heavily on cloud connectivity
which can support voice recognition
facial recognition among other features
as well well as more businesses and
industries recognize the advantage of AI
its adoption is rapidly growing the
ability to process data locally enhance
privacy reduce latency and make
real-time decision empow organizations
in various sectors to enhance operations
deliver better services and improve user
experience so the these are some of the
realtime applications of Ed and with
that we've come to the end of today's
session on what hedge Computing for AI
is hello everyone and welcome to this
fantastic video on what is Lang chain on
the simply Lear YouTube channel all of
you must be already aware of blockchain
but let me give you a quick refresher a
blockchain is a digital Ledger that
store information like a digital
database without a central Authority
controlling it instead a blockchain is
maintained by a network of computers
each one is storing a cop of The Ledger
when a new transaction is added to the
Ledger the network has to approve it
before it can be recorded other side
Lang chain is like a blockchain but with
a few differences unlike a blockchain
which stores information about financial
transaction L chain can store any kind
of data or information like medical
records supply chain data or even social
media profiles Lang chains design also
allows for private transaction where
only a certain people can access and
view the store data which defers from a
regular blockchain sounds fascinating
right well it is and there is a lot more
to learn and discuss about blockchain
technology in this video so stay tuned
till the very end as we embark on this
amazing journey of exploring Lang chain
according to recent studies Ai and
machine learning related job posting
have increased by 3 44% in the past 5
years companies across the globe are
actively seeking professional who can
harness the power of data and build
intelligent system the average salary is
$150,000 in the US and 15 lakhs in India
as a field evolves new job titles and
responsibilities are anticipated to C to
Industry requirements prominent
companies like Microsoft Google Amazon
IBM Goldman s and apple are actively
recruiting machine learning Engineers
hey everyone welcome to Simply Lars
YouTube channel but before we dive into
that don't forget to subscribe like and
share there is no better time to train
yourself in the exciting field of
machine learning if you are looking for
a course that covers every everything
from the fundamental to Advanced
Techniques for that accelerate your
career in Ai and ml with our
comprehensive postgraduate program in Ai
and machine learning gain expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything session by IBM with
three Capstone and 25 plus industry
project using real data sets from
Twitter Uber and many more you will gain
practical experience master classes by
ctech faculty and IBM and no top notch
education simply run job asist help you
to get noticed by Leading companies this
program covers statistics python
supervised and unsupervised learning NLP
neural network computer Visions g scaras
t oflow and many more skills so enroll
now and unlock exciting a ml
opportunities the link is in the
description box below so without any
further delay let's get started so what
is Lang
chain Lang chain is a versatile open
source framework that empowers AI
developers to integrate large language
model llms such as gbt 4 with the
external data available in the python or
JavaScript packages Lang chain addresses
the limitation of GPT models trained
only up until 2021 while GPT for models
exibit impressive general knowledge
their potential is greatly enhanced by
connecting them to customize data and
customized computations and this is
precisely what Lang Chain facilate by
enabling lm's access comprehensive
database Lang chain empowers them to
generate responses based on the most
upto-date information found in the
reports documents and website content
since the launch of GTP 4 in March lch
has garnered substantially popularity
due to its flexibility and the my rate
of possibilities it unlocks when
combined with the high powered llm so
moving forward let's see why we need
Lang chin so langin represents a
practical solution to overcome this
limitation through a practical approach
it involves pre-processing the tax C
purpose by dividing it into manageable
chunks or summaries and adding them in a
vector space and subsequently searching
for similar chunks when a question is
posed so this methodology of
pre-processing real time and data
collection and interaction with the llm
is not only applicable in the context
but also extend to other scenarios such
as code semantic search so Lang
simplifies the composition of these
components by offering an abstraction
the simplification of the process often
called prompt Plumbing plays a crucial
role and will grow in significance as
llms becomes more powerful and
necessitate the provision of additional
data during the prompt execution so
moving forward let's see how does Lang
chain work so Lang chain simplifies the
process of organizing large volume of
data that can be easily accessed by the
llm requiring minimal computational
power it operates by taking a
subsequential data source like 50 page
PDF and breaking it down into manageable
chunks and then that are embedded into a
vector store so with the vectorized
representation of the extensive document
at hand we can combine them with the llm
to selectively retrieve the necessary
information for creating prompt
completion pairs so when we input a
prompt into our chatbot equipped with
langin it queries the vector stored to
find perent information a kind of
miniature Google tailored to your
document once the relevant details are
obtained they are combined with the llm
prompt and fed into the llm to generate
our desired response after this
integration of L chain with the open AIS
llms also enables the creation of
application capable of taking action
such as web serving email sending and
other task related to AP and at the end
you will get a reply so this is how Lang
chain works with open AIS llms moving
forward let's see some use cases of Lang
chain so there is a wide range of
potential application for Lang chain
here are few examples so first one is
personal AI email assistant Lang chain
can be used to develop an intelligent
email assistant that understood and
generates responses based on the user
preferences and writing style the second
one is AI study body so langin can
assist a student in their Studies by
providing realtime access to relevant
research papers textbooks and
educational resources offering
personalized study recommendation and
answering theoretical questions and the
third one is AI data analytics Lang
chain can be employed to enhance data
analytics by allowing large language
model to access and analyze V amount of
structured and unstructured data
enabling more comprehensive insight and
prediction and the fourth one is custom
company customer service chatbot langen
can power chatbot that provide tailored
customer service experience by
integrating upto-date company
information product knowledge and the
customer data to offer personalized and
accurate responses and the fifth one is
social media content creation so Lang
chain can help content creator generate
engaging and relevant social media
content by leveraging its ability to
access the latest information Trends and
insight from various sources so these
example illustrate just the fraction of
the potential application for lanin and
its versatility extend to a wide area of
other scenarios as well so after seeing
use cases of Lang chain let's move
forward and see using open AI how Lang
chain works or not so welcome to the
demo part so here first we will set up
the what the problem is using the chat
gbt
so this is the chat GPT this is
something that we all know and I'm going
to first ask chat GPT what is Lang
chain and we will see what it will say
okay so here I will write
what okay what is
Lang
chain so as you can see it is saying I'm
sorry but as of my knowledge College cut
off in September 2021 I'm not familiar
with the term langang chin it is
possible that it could be a term or
concept that emersed or became popular
at that
time and could you provide more context
or details about what you are referring
to that way I can try to assist you
better so as you can see why this is
happened it is because langin is a more
recent
Library so this is the new library and
Lang chain is I think published on uh
some around January and it is new and
Chad GPT has not been drained on the
data more recent than I think like
around 2021 as you can see it is written
here September 2021 okay so this is the
problem number one so let's see the
problem number two so here I will write
hey
chat
GPT tell me how many
meetings the meetings are blocked
today
using using
my
Google
calendar again is saying I'm sorry but
as an AI language model I don't have
access to personal data about
individuals unless it has been shared
with me during our conversation I can
browse or retrieve information from your
Google calendar or any other personal
documents or accounts however if you
have access to your Google calendar you
can easily check the number of meetings
blocked today by opening your calendar
and looking at the schedule events for
the day the calendar interfaces should
display the number of meetings or events
you have scheduled and why this is
happening because we are using chat GPD
open AI okay but if we will use API of
open AI okay currently I'm using the
chat GPT but if we will use the open AI
API so we can connect to our Google
Drive Google Calendar and any
anything after that we can easily
retrieve our
information Okay so so this is where so
as you can see using this image what
Lang chain
does okay is it is going to connect your
AI models okay these are the AI models
open AI hugging
phases so what the langet does okay
using
llms it is going to connect to outside
sources like Google drive or Wikipedia
wallframe or you know Google cender
anything to the outside sources
and you can do some really cool things
without those outset sources okay you
can ask to the chat GPT and or you can
make your own model llm model but
without llm model and Lang chain you
can't connect with the outside
resources okay so all of a sudden you
can start to chain together commands
Okay that is why the Lang chain is use
chain okay these are the llm models open
air hugging faces and you can chain both
of them using L chain okay you can
change the outside sources and this llms
using Lang
chain okay uh after chaining llm and
outside sources using L chain you can
ask any question to them okay what is my
how many documents I wrote in the last
year and this month or you can say hey
chity what's the weather today to open
AI or anything you can can ask these uh
kind of
questions and the application for the
Lang chain are the endless and we have
already seen some of them okay so now
let's see the documentation part of the
langin official documentation part of
the langin so as you can see this is the
official documentation of the Lang chain
okay so here as you can see what is the
Lang chain Lang is a framework for
developing application powered language
models LM models it enables applications
that are data or agent so as you can see
there are the certain modules and the
example use cases as you can see the
chatbots and you can create the M uh you
know chat BS using llms like open Ai and
there are the many things like
interaction with apis
autonomas okay you can check
anything these are some the uh modules
of the Lang change Lang provides
standard extendable interfaces okay so
these are the modules and these are the
use
cases okay ecosystems and the support
API references these are the API
references how you can use the API and
as you can see this is the installation
part using pip you can use install the
langin and the K part is here the cond
install Okay so this is this you know
official document you can go through it
and you will learn you will definitely
learn something if you will go through
through the official document what is
transfer learning by simply learn do you
know training a classifier to
distinguish beverages can help predict
the cuine of an image to know more about
transfer learning and how it works stay
tuned till the end of this video in this
video we will cover topics like what
transfer learning is how transfer
Learning Works moving forward we will
dive into why you should use transfer
learning after that we will cover the
steps to use transfer learning and at
the end we will see popular model
trained using transfer learning and let
me tell you guys that we have regular
updates on multiple Technologies if you
are a tech geek in a continuous H for
the latest technological Trends then
consider getting subscribed to our
YouTube channel and press that Bell icon
to never miss any update from Simply by
the end of this video I can ensure that
all your questions and doubts related to
transfer learning will have been cleared
also accelerate your career in Ai and ml
with our comprehensive postgraduate
program in Ai and machine learning boost
your career with this Ai and ml course
delivered in collaboration with bu
University and IPM learning in demand
skills such as machine learning deep
learning NLP computer vision
reinforcement learning generative AI
prompt engineering chat GPT and many
more you will receive a prestigious
certificate and ask me anything session
by IBM with five Capstone in different
domains using real data set you will
gain practical experience master classes
by bu University and IBM experts ensure
top-notch education simply learn job
assist helps you get noticed by Leading
companies the this programs cover
statistics python supervised and
unsupervised learning NLP neural network
computer vision G scas tensor flow and
many more skills so why wait enroll now
and unlock exciting AI nml opportunity
the course link is in the description
box below so without any further Ado
let's get started so what is transfer
learning transfer learning is machine
learning refers to reusing a pre-end
model to improve prediction on a new
task it involves using knowledge gained
from a previous assignment to tackle a
related problem for instance a model
train to recognize backpacks can also be
used to identify other objects like
sunglasses due to the substantial CPU
power required this approach is widely
utilized in computer vision and natural
language processing task including
sentiment analyses so moving forward
let's see how transfer learning works so
how does transfer learning work in
computer vision neural network have
distinct objectives for each layer
detecting edges in the first layer and
identifying forms in the middle layer
and and capturing task specific features
in the later layer transfer learning
utilize the early and Center layers for
a pre-end model and only retrains the
later layer it leverages the label data
from its original task for instance if
you have a model trained to identify
backpacks in images and now want to use
it to detect sunglasses we will retrain
the later layers to understand the
distinguished features of sunglasses
from the other objects so moving forward
let's see why should you use transfer
learning transfer learning offers
several advantages including reduce
training time improve neural network
performance in most cases and the
ability to work with limited data
training a neural model from scratch
typically requires a substantial amount
of data which may not always be readily
available transfer learning becomes
valuable in such scenarios here is why
you should consider using transfer
learning first one is efficient use of
data with pre-trained models you can
perform well even with limited training
data that is especially Ben beneficial
in task like NLP where obtaining large
label data set can be challenging and
time confusing the second one is faster
training building a deep neural network
from a scratch of a complex task can be
timec consuming taking days or even
weeks by leveraging transfer learning
the training time is significantly
reduced as you start with a model that
has already learned General features
from a related problem now moving
forward let's see steps to use transfer
learning the first one is training a
model to reuse it in machine learning
training a model involves providing it
with the data to learn patterns and make
prediction once a model is trained on a
specific task it can be reused and three
purpose for related task saving time and
computational resources the second one
is using a pre-train Model A pre-train
model is a model that has already been
trained on a large data set for a
specific task instead of training a
model from scrp using a pre-train model
as a starting point allow us to benefit
from the knowledge it has gained during
its previous training
the third one is extraction of features
features extraction is a process in
which meaningful patterns and
characteristics are identified and
separated from a raw data in the context
of machine learning it involves
identifying relevant information from
input data to feed into a model for a
better prediction the fourth one is
extraction of features in neural
networks in neural networks feature
extraction involves identifying
important patterns or features in the
data at different network layers the
early layers typically capture simple
features like edges while deeper layers
capture more complex feature relevant to
the task at hand this hierarchical
representation enables neural network to
learn and generalize from the data
effectively so moving forward let's see
some popular models trained using
transfer learning so numerous machine
learning models have been trained using
transfer learning some popular ones
include the first one is vgg16 and
vgg19 these model was trained on the
image net data set for image
classification test the second one one
is inception V3 these models were
pre-trained on image net and are known
for their effectiveness in I repeat in
object DET I repeat in object detection
and object recognition the third one is
bird B directional encoder
representation from Transformer this
language model is preten on the
extensive text collection and find
extensive application in NLP tasks like
sentimental analysis and named entity
recognition the fourth one is GPT
generated pre-trained Transformer series
these models are pre-trained language
models for various NLP tasks these are
just a few example of pre-train models
that have been used in transfer learning
to accelerate training and improve
performance across different task in
this video we are going to take you
through an interesting AI tool which can
help you in different ways the flare AI
we are going to discuss and this AI can
help you create your own advertisement I
mean if you own a product or if you want
to advertise something then this tool is
the goto option for you and we are going
to see how to use this tool in this
particular video but before we begin if
you enjoy watching these videos and if
you find them interesting then subscribe
to Simply learns YouTube channel because
we bring the best videos for you daily
also hit the Bell icon to never miss any
updates from Simply learn we all know
that AI is expanding at a very rapid
Pace chat GPT is a great example so in
this time begging a job in this field is
super beneficial because of of course it
offers great salary packages and AI is
interesting as we all know so on that
note we present to you the simply learns
postgraduate program in Ai and machine
learning in collaboration with perdu
University boost your career with this
Ai and ml course delivered in
collaboration with Peru University and
IBM you can learn in demand skills such
as machine learning deep learning NLP
computer vision reinforcement learning
generative AI prompt engineering chat
GPT and many more you'll receive a
prestigious certificate and ask me
anything session by IBM with five
capstones in different domain using real
data sets you'll gain practical
experience as well master classes by P
faculty and IBM experts ensure top-notch
education simply learns job assist helps
you get noticed by Leading companies so
this program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision G caras
tensorflow and many more skills so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below so let's begin with Flare AI what
we have to do first is we have to go to
Google and just write here
flare AI fine so this is the first
option you'll get click on it flare so
this is the interface of flare AI the AI
design tool for Branded content so it
can help you with your brand definitely
you'll get better reach because
what usually happens is we use different
tools like Adobe Photoshop and other
tools to just show our brand item in
different ways but with this AI tool we
can use this and it's not time consuming
as well so let's start what we'll do is
we'll right here create with there so
here you can see we have a demo a demo
video basically so here you can see we
have a product a perfume bottle and here
we have different items or we can say
different options we can generate we can
customize and that's it just right here
click generate it will generate a
beautiful image for you so let's start
or if you want to watch the video here's
the video so here you can see this image
turned into that so we don't have to do
much of editing everything was done by
AI so let's see how we can do it so
we'll click on create with fler now
continue with
Google so you will have to log in
although it's free to use this AI tool
is completely free so here we are we are
going to click on create new project so
here you can see we have a box for
placing our product here so let's say we
are placing something out of here fine
so here you can see we have a Starbucks
coffee let's place it in the middle so
generated image images will appear here
see here you can see generated images
will appear here it's written now click
on generate what you want to do so here
you can see we have different templates
fine we have this template we can use
this we can use any of them so let's
use this one
fine and click on this click on generate
so paper cup on a rock surrounded by
trees in front of Canyon and River fine
so here you can see it's still
progressing and now here you can see we
have a coffee mug present over here
Starbucks coffee if I zoom in for you a
little bit so here you can see how nice
it's looking it's not even looking like
that this cup is edited it's looking
like that this cup is actually placed
there so in another way if we want to do
the same thing what we have to do is we
have to use ad do Photoshop we have to
use different tools right but here you
can see how detailed it is we have
shadows for the cup as well not just
that but if
you click here on generate and if you'll
find the mirror one you'll get to see
the
reflection completely so if I click here
paper cup on a wet surface surrounded by
rain in front of Spotlight and strong
Shadows so it's pretty good we'll copy
it as well we'll need it later on I'll
tell you why we need why we'll need it
so here you can see the it's good right
so you here you can see
and here you can see we have a image the
paper cup what it said paper up on a wet
surface the surface is wet surrounded by
rain so here you can see rain in front
of a spotlight and strong shadows in
front of Spotlight and strong Shadows so
here you can see the shadow of a paper
cup fine similarly we can use different
tools we have different elements as well
like if you want to
use for example if you go on humans if
you want to
use we have a hand thing so here you can
see if you want to put it here and if
you want to bring it to the front so
here you can see it's on a hand so here
you can see how we can use this tool
fine so we have a number of options
available here if I if we go to a sets
so here you can see we have a perfume
bottle as well we'll delete this one
we'll use a perfume bottle it's from
Chanel and now what we are going to do
is we are going to delete them as well
now what we are going to do is we are
going to generate something else fine so
now we are going to use the editor so
here you can see we have a product which
is a perfume bottle so we have to edit
the prompt in the form below fine the
product is a perfume bottle then we have
a placement where we want to place it on
a
mirror or here you can see we have
different options so let's say we are
using wooden surface surrounded
by surrounded by let's say fls fine now
create this image in your mind we have a
perfume bottle with which is present on
a wooden surface its placement is on a
wooden surface and it is surrounded by
flowers fine now number of results how
many number of results you want I'll
make it extra strong extra strong color
strength and outline strength just to
show you guys how it looks now click on
generate and wait for few seconds not
even minutes but it will be done in few
seconds here you can see how fast this
AI is so this AI tool is very useful if
you have your own brand and if you have
your own products not services but
product so here you can see it's present
on a wooden surface and we have flowers
on it fine so what we'll do is we'll
write here next
to Orange fine generate it's to it
totally depends on your creativity how
you want to create your image how you
want to present your brand service or
your BR brand product so here you can
see we are done with almost 85% we have
to wait for a few more seconds not even
a minute as I already mentioned and it
will be done so here you can see it's
present on
a wooden
surface and the Orange is not there
basically what we have to do is we have
to go to generate we have to use
something else
then we'll
use element fine let's see if it works
or not perfume bottle on a wooden
surface next to alements in front of
Spotlight and strong shadow
okay we are not going to use
this what we are going to do
is nature in the background let's
see how it looks wait for few more
seconds and again it will give you an AI
generated image which will be present on
a wooden surface next to alements in
front of nature in the background now
here you can see we have a wooden
surface we have almonds present over
here and and we have nature behind it
wow it looks it looks fine not that good
so what we'll do next is we'll write
here
circular glass platform and we'll write
here next to a mug we are not going to
use jungle we are going to use orange
sunset in the background maybe it will
look better because sunsets are always
beautiful so let's see how it comes out
so here we have
the output so it's present on a circular
glass platform okay it's fine we have a
mug okay the mug is not
visible or maybe this is the mug
it's not that good and then we have the
sunset so Sunset is here here you can
see now the image looks real that's the
point it's not something that's that
looks like that it's edited or it's just
cropped from somewhere and put it over
there no it looks real fine right it has
the shadows and all the edges
and saturation it's changed as per the
background so right now the background
is sunset so here you can see the orange
lights here present over here fine so
that's how you can use this tool to
create an advertisement for your brand
product now the next thing is not just
that but it allows you to upload your
own pictures so it's quite interesting
let me just delete all of
them so what you have to do is you have
to download the picture of your product
upload it from here and we have a
product of an iPhone so here I have this
picture okay so I'll skip this for now
because I'm going to show you if I write
here let's say it's an
iPhone done now here you can see we have
a product placed here so here you can
see we have a background right we have
blue and and write this base as
well so the first thing we are going to
do is we are going to click on that we
are going to click on edit now the best
part is we can remove this background
and it turns out to be so good let's
check it out so click on remove
background it will take few seconds to
remove all this from your background and
you'll notice something that the
background the edges of the
phone how nicely it has done it so here
you can see there's nothing wrong and
every Edge is is perfectly fine right
the background is completely removed now
now what we have to do is we have to
generate a image so we have an iPhone we
want it to be present at a circular
glass platform we
will a golden sparkles in front
of let's remove it and see what else we
have desert in the background let's try
this generate it and it will again take
few
seconds so are you guys ready to check
out an iPhone present in the desert so
basically the idea of this AI tool is it
saves a lot of time for anyone right so
here you can see the iPhone is present
in a circular glass platform
right and we have what else we had okay
we had golden sparkles golden sparkles
are here
somewhere and we have desert desert in
the background so desert here you can
see we have a
desert now what we'll do is we'll write
here
mirror and we want it to be present
in
water and
droplets fine so let's try it I haven't
tried it EI either till now so it will
take sometime the iPhone will be placed
on water with some droplets surrounding
it and the background will be okay a
mirror so it looks good right so here
you can see the shadow actually the
shadow of the phone how realistic it
looks so this is how you can use this AI
tool and not just that but you can
download your images so here you can see
click on download image and done the
image is downloaded and just click you
have from this to this and it totally
depends on your creativity how you want
to present your brand product and if
you're an inspiring python Enthusiast or
an A ml Enthusiast looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers in AI by
learning from the experts then try
giving a short to Simply learns
postgraduate program in Ai and machine
learning in collaboration with P
University and IBM and the course link
is mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered all right
what is logistic regression as I
mentioned earlier istic regression is an
algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if the it's going to break down in the
near future so this is like a
classification problem find out whether
your car will break down or not so how
are we going to perform this
classification so here's how it looks if
we plot the information along the the X
and Y AIS X is the number of years since
the last service was performed and Y is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and and that
has been plotted along X and Y AIS so
this provides an idea or from this graph
we can find out whether your car will
break down or not we'll see how so first
of all the probability can go from 0 to
one as you all aware probability can be
between 0er and one and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year 2 years or 3 years still
after the service the chances of your
car breaking down are very limited right
so for example chances of your car
breaking down or the probability of your
car breaking down within 2 years of your
last service are 0.1 probability
similarly 3 years is maybe3 and so on
but as the number of years increases
let's say if it was six or seven years
there is a almost a certainty that your
car is going to break down that is what
this graph shows so this is an example
of of application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class at the
outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be zero and any value above 0.5 is
considered to be one so an output of
let's say 8 will mean that the car will
break down so that is considered as an
output of one and let's say an output
of. 29 is considered as zero which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so that can
cause confusion so let's try to remove
that confusion so what is linear
regression linear regression is a
process is once again an algorithm for
or supervised learning however here
you're going to find a continuous value
you're going to determine a continuous
value it could be the price of a real
estate property it could be your hike
how much hike you're going to get or it
could be a stock price these are all
continuous values these are not discrete
compared to a yes or a no kind of a
response that we are looking for in
logistic regression so this is one
example of a linear regression let's say
the HR team of a company tries to find
out what what should be the salary hike
of an employee so they collect all the
details of their existing employees
their ratings and their salary hikes
what has been given and that is the
labeled information that is available
and the system learns from this it is
trained and it learns from this labeled
information so that when a new employees
information is fed based on the rating
it will determine what should be the
height so this is a linear regression
problem and a linear regression example
now salary is a continuous value you can
get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on that
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the label data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion please just plotted the data
about whether a person has got an
employee has got promotion or not yes no
right so there is nothing in between and
what is the employees rating okay and
ratings can be continuous that is not an
issue but the output is discrete in this
case whether employee got promotion yes
no okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors th root mean mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0er and one but still the straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odds so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
spe so this is how you measure the odds
now the values of the odd range from 0
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to Z and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1 now this is how an
equation of a straight line Looks So Y
is equal to Beta 0 plus beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 minus PX
this is an extension of the calculation
of odds that we have seen right and that
is equal to Beta 0 + beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of PX you will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this PX is equal
to 1 by 1 + e^ of minus beta 0 + beta 1
x and recall this is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logistic regression all
right and if this is plotted this is how
the sigmoid curve is obtained so let's
compare linear and logistic regression
how they are different from each other
let's go back so linear regression is
solved or used to solve regression
problems and logistic regression is used
to solve classification problems so both
are called regression but linear
regression is used for solving
regression problems where we predict
continuous value whereas logistic
regression is used for solving
classification problems where we have
had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and um linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s-shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to raise rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or a elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and um show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks so first
thing we do is import all the libraries
and then we basically take a look at the
images and see what is the total number
of images we can display using mat plot
liet some of the images or a sample of
these images and um then we split the
data into training and test as I
mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
first of all this as you can see this is
a matrix and the size of the Matrix
depends on how many outputs we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonal should have the
maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
just of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual lab that's all it is okay
yeah so we are showing a little bit more
information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the the
predicted value and the actual value is
is exactly the same where as in this
case right it has there are I think 37 +
5 yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
the this shape now from here onwards we
are basically once again importing some
of the libraries that are required like
numpy and M plot and we will take a look
at uh some of the sample images that we
have loaded so this one for example
creates a figure uh and then we go ahead
and take a few sample images to see how
they look so let me run this code and so
that it becomes easy to understand so
these are about five images sample
images that we are looking at 0 1 2 3 4
so this is how the images this is how
the data is okay and uh based on this we
will actually train our logistic
regression model and then we will test
it and see how well it is able to
recognize so the way it works is the
pixel information so as you can see here
this is an 8 by 8 pixel kind of a image
and uh the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and nonactivation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn uh for
which number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and test so that the training
data set is used to train the system so
we pass this probably multiple times uh
and then we test it with the test data
set and the split is usually in the form
of and there are various ways in which
you can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as 20 23
that means 23% of the entire data is
used for testing and the remaining 77%
is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is labeled data for each
of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train Yore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is 13 83 by 64
and Yore train has 1383 because there is
nothing like the second part is not
required here and then xcore test shape
we see is 414 so actually there are 414
observations in test and 1383
observations in train so that's
basically what these four lines of code
are are saying okay then we import the
logistic regression library and uh which
is a part of psychic learn so we we
don't have to implement the logistic
regression process itself we just call
these the function and uh let me go
ahead and execute that so that uh we
have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basic basically creates
an instance of logistic regression model
and then the second line is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as a
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion underscore Matrix method takes
two parameters one is the Yore test and
the other is the prediction so what is
the ycore test these are the labeled
values which we already know for the
test data set and predictions are what
the system has predicted for the test
data set okay so this is known to us and
this is what the system has uh the model
has generated so we kind of create the
confusion Matrix and we will print it
and this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is predic good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers uh which means the accuracy is
very low the diagonal indicates correct
prediction that this means that the
actual value is same as the predicted
value here again actual value is same as
the predicted value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44 Plus 43 and so on
and divide that by the total number of
test observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map uh looks uh and
as you can see here the diagonals again
are are all the values are here most of
the values so which means reasonably
this seems to be reasonably accurate and
yeah basically the accuracy score is 94%
this is calculated as I mentioned by
adding all these numbers divided by the
total test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four
this is an image of zero so the
predicted value is also zero actual
value is of course zero then this is the
image of nine so this is also been
predicted correctly n and actual value
is nine and this is a image of one and
again this has been predicted correctly
as like the actual value okay so this
was a quick demo of logistic regression
how to use logistic regression to
identify images Okay so so let's
summarize what we have learned in this
video we discussed about what is
logistic regression and how it fits into
the overall machine learning we
discussed about the different types of
machine learning and how logistic
regression is one of the algorithms for
classification and then we compared
logistic regression with linear
regression because the names may be
sounding similar but these are
completely different algorithms linear
regression is used for predicting Contin
values whereas logistic regression is
used for finding discrete values and
then we talked about the math behind
logistic regression and we derived that
function as well the sigmoid function we
did a quick table comparison between
linear and logistic regression we saw
some examples of day-to-day life how
logistic regression is used for some of
the class for solving some of the
classification problems and then in the
end we looked at the code to see how we
can Implement logistic regression to
train the module to identify images
there's all kinds of regression models
that come out of this so we put them
side to side we have our linear
regression which is a predictive number
used to predict a dependent output
variable based on Independent input
variable accuracy is a measured uh using
least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you a an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we talk about linear regression and
we're talking about regression it means
the numberers coming out linear usually
means we're looking for that line versus
a different model and it's used in
business domain forecasting stocks uh
it's used as a basis of all of most um
uh predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at uh a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value a low
value volume uh that kind of thing so
when you're looking at something that's
straight numbers um and are connected in
that way usually you're talking about a
linear regression model and that's where
you want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mention is is what
people usually think of um although they
use it for classification
of um like a window of things so you
could take a window of stock history and
you could CL generate classifications
based on that and separate the data that
way if it's going to be that this
particular pattern occurs it's going to
be upward trending or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care or do I even want to look
at it so let's do a demo so you can get
a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and nonpoisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number
number um so let's predict the price
which the insurance should be sold to
and the second one is we're looking at
either it's poisonous or it's not
poisonous so first off before we begin
the demo I'm in the Anaconda Navigator
in this one I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
lgit um is one of the one of the names
that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models
and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we pre-load all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet you rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoded is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um
um clear all that there we go uh these
two right here are what this is all
about logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh in Insurance costs uh how
close to the insurance cost does it
measure that we expect it to be you know
if you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
a eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this has
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know to give you a quote for what we're
going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 two 3 four five six
seven so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn
um what to expect them to charge on
there uh so is this going to be you know
is this person going to cost me uh
$16,814.23
one other thing you really need to
notice on this data um and I mentioned
it before but I'm going to mention it
again because pre-processing data is so
much of the work in data science um sex
well how do you how do you deal with
female versus male um are you a smoker
yes or no what does that mean region how
do you look at Region it's not a number
how do you draw a line between Southwest
and
Northwest um you know they're they're
objects it's either your Southwest or
you're Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label
fit right here where we're going to go
ahead and do the data set uh. six. drop
duplicates and then for data set sex
we're going to do the label transform
the data s sex and so we're looking
right here at um male or female and so
it usually just converts it to a zer1
because there's only two choices on
here same thing with the smoker it's
zero or one so we're going to transfer
the trans change the smoker uh 01 on
this and then finally we did region down
here region does it a little bit
different we'll take a look at that and
um it's I think in this case it's
probably going to do it because we did
it on this label
transform um with this particular setup
it gives each region a number like 0 1
two three so let's goe and take a look
and see what that looks like go and run
this and you can see that our new data
set um has age that's still a number
uh Sex Is Zero or one uh so zero is
female one is male number of children we
left that alone uh smoker one or zero it
says no or yes on there we actually just
do one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
uh 01 to three so there three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 one two 3 A lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set drop the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the data and
then finally the big ones uh this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with the new models good
to see where it comes from and this
comes from the S kit uh learn and this
is the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I looking at this is
I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so when we go in this and we
go ahead and run this you'll see it
loads the linear regression model and
just has a nice output that says hey I
loaded the linear regression model and
then the second part part is we did the
fit and so this model is now trained our
linear model is now trained on the
training
data and so one of the things we can
look at is the um um for idx and call a
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so when
you're looking at the back end of the
data you remember we had that formula uh
BX X1 Plus bxx2 plus the plus the uh
intercept uh and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region gets a
minus value so when it heads it up I
guess a region you can read a lot into
these numbers uh gets very complicated
there's ways to mess with them if you're
doing a basic linear regression model
you usually don't look at them too
closely uh but you might start looking
in these and saying saying hey you know
what uh smoker look how smoker impacts
the cost um it's just massive uh so this
is a flag that hey the value of the
smoker really affects this model and
then you can see here where the body
mass index uh so somebody who is
overweight is probably less healthy and
more likely to have cost money and then
of course age is a factor um and then
you can see down here we have uh sex is
than a factor also just it changes as
you go in there negative number it
probably has its own meaning on there
again it gets really complicated when
you dig into the um workings and how the
linear model works on that and so um we
can also look at the intercept this is
just kind of fun um so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that and so you
can see right here we can go back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out and if
you're going to create a model to
predict something uh we'll go ahead and
predict it here's our y prediction value
linear model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction
and so when we pull this up uh you can
see here that we have uh the actual cost
and what we predicted the cost is going
to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this take a look at
the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns and the
data and we're going to transform into a
um label encoder and so when we run this
you can see how this gets shifted from
uh xbxx K to 01 2 3 4 5 or whatever it
is class is 01 one being poisonous zero
looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring at 0 1 2 3 4 5 um in
this particular example it's not going
to make that big of a difference how we
encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our um train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because then that that's
what it does is it goes through the math
and it goes hey can we minimize the
error and it keeps finding a lower and
lower error and it still is changing
that number so that means it hasn't
conversed yet it hasn't find the lowest
amount of error it can and the default
is 100 uh there's a lot of settings in
here so when we go in here to let me
pull that up from the sklearn uh so we
pull that up from the sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and I'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
down here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we actually predicting
the data so we we've done our first two
lines of code as we create the model we
fit the model to our training data and
then we go go ahead and predict for our
test data now in the previous model we
didn't dive into the test score um I
think I just showed you a graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy W that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here um
let's see I believe its poisonous was
one uh zero is edible so let's go ahead
and run that Z 1 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh start eat
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as uh the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science talking about your machine
learning
code and hopefully you got a
classification is probably one of the
most widely used tools in machine
learning in today's world it is also one
of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important term
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithms and there are
a lot out there so we're only going to
touch briefly on a variety of them so
you can see how they the different
flavors work and we'll have some
Hands-On Demos in Python embedded
throughout the tutorial classification
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and where you
end when you start running these um
machine learning algorithms and
classification and the classification
algorithms is a little black box in a
lot of respects and we'll look into that
you can see what I'm talking about when
we start swapping in and out different
models let's say we are given the task
of classifying a given bunch of fruits
and vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be
grouped together and so we have a data
set we'll call it the bunch is divided
into clusters one of which consists of
the fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out in images
to see what is malignant what is benign
very popular one can you classify
flowers the
uh data set uh certainly in Wildlife can
you classify different animals and track
where they're going classification is
really the bottom starting point or the
Baseline for a lot of machine learning
and setting it up and trying to figure
out how we're going to break the data up
so we can use it in a way that is
beneficial so here the fruits and the
vegetables are grouped into clusters and
each clusters has a specific
characteristic IE whether they are a
fruit or a vegetable and you can see we
have a pile of fruits and vegetables we
feed it into the algorithm and the
algorithm separates them out and you
have fruits and vegetables so some
important terminologies before we dig
into how it sorts them out and what that
all means uh when we look at the
terminologies you have a classifier
that's the algorithm that is used to map
the input data to a specific category
the classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times the
classifier is part of the classification
model um and then you choose which
classifier use after you choose which
model you're using where features are
what goes in labels are what comes out
so your classifier model is right in the
middle of that that's that little black
box we were just talking about clusters
they are a group of data points which
have some common characteristics binary
classification it is a classification
condition with two outcomes which are
either true or false multi-label
classification this is a classification
condition where each sample is assigned
to a set of labels or targets multiclass
classification the classification with
more than two classes here each sample
is assigned to one and only one label
when we look at this group of
terminology IES a few important things
to notice uh going from the top clusters
when we cluster data together we don't
necessarily have to have an end goal we
just want to know what features cluster
together these features then are mapped
to the outcome we want in many cases the
first step might not even care about the
outcome only about what data connects
with other data and there's a lot of
clustering algorithms out there that do
just the clustering part binary class
classification it is a classification
condition with two outcomes which are
either true or false we're talking
usually um it's a uh it's either a cat
or it's not a cat it's either a dog or
it's not a dog um that's the kind of
thing we talk about binary
classification and then that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multiclass classification where
each one's a binary uh you can either be
a cat or a dog you can't be both a cat
and a dog real world applications so to
make sense of this uh of course the
challenge is always in the details is to
understand how we apply this in the real
world so in real world applications we
use this all the time we have email spam
classifier so you have your uh email
inbox coming in uh it goes through the
email filter that we usually don't see
in the background and it goes this is
either valid email or it's a Spam and it
puts it in the spam filter if that's
what it thinks it is uh Alexa's voice
classifier Google Voice any of the voice
classifiers they're looking for points
so they try to group words together and
then they try to find those groups of
words trigger a CL a classifier so it
might be that the classifier is to open
your task program or open your text
program so you can start sending a text
sentimental sentiment analysis is really
big uh when we're tracking products we
tracking marketing trying to understand
uh whether something is liked or
disliked is huge uh that's like one of
the biggest driving forces in sales
nowadays and you almost have to have
these different filters going on if
you're running a large business of any
kind fraud detection uh you can think of
banks uh they find different things on
your bank statement and they detect that
there's something going on there they
have algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned the cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with the
classification when the wild animals in
our area are out like foxes we actually
have a mountain lion that lives in the
area so it's nice to know when he's here
handwriting prediction uh classifying A
B C D and then classifying words to go
with that so let's go ahead and uh roll
our sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms uh let's go
back and take a look at our definition
we have a classifier and a
classification model so we're looking at
the classifier an algorithm that is used
to map the input data to a specific
category one of those algorithms is a
logistic regression the logistic
regression is a classification algorithm
used to model the probability of a
certain class or event existing such as
past fail or when lose Etc it provides
its output using the logistic function
or sigmoid function to return the probab
ability value that can then be mapped to
two or more discret classes a sigmoid
function is an activation function that
fits the variable and limit the output
to a range between 0er and one a
standard sigmoid function or logistic
function is represented by the formula
FX = 1 / 1 + e- x where X is the
equation of the line and E is the
exponential just taking a quick look at
this um you can think of this as being
being a point of uncertainty and so as
we get closer and closer to the middle
of the line it's either um activated or
not and we want to make that just shoot
way up uh so you'll see a lot of the
activation formulas kind of have this
nice s curb where it approaches one and
approaches zero and based on that
there's only a small region of error and
so you can see in the sigmoid logistic
function uh the 1 over 1 + e- x to the-
X you can see it FES that that nice s
curb uh we also can use a tangent
variation there's a lot of other
different uh models here as far as the
actual algorithm this is the most
commonly used one let's go ahead and
roll up our sleeves and take a look at a
demo that is going to use a logistic
regression so we're going to have the
activation formula and the model because
you have to have you have to have both
for this we will go into our Jupiter
notebook now I personally use the
Anaconda navigator to open up the
Jupiter notebook
to set up my IDE as a webbased it's got
some advantages that it's very easy to
display in uh but it also has some
disadvantages in that if you're trying
to do multi-threads and multi-processing
you start running into a single git
issues with python and then I jump to uh
py charm really depends on whatever ID
you want just make sure you've installed
uh numpy and the SK learn modules into
your Python and whatever environment
you're working in so that you'll have
access to that for this demo now the
team in the back has prepared my code
for me which I'll start bringing in one
section at a time so we can go through
it before we do that it's always nice to
actually see where this information is
coming from and what we're working with
uh so the first part is we're going to
import our packages which you need to
install into your Python and that's
going to be your numpy um we usually use
numpy as NP and then from sklearn the
learn model we're going to use a
logistic regression and from sklearn
metrics we're going to import the
classification report confusion Matrix
and if we go ahead and open up S uh the
scikit-learn dorg and go under their API
you can see all the different um
features and models they have and we're
looking at the linear model uh logistic
regression one of the more common
classifiers out there and if we go ahead
and go into that and dig a Little Deeper
you'll see here where they have the
different settings and it even says
right here note the regularization is
applied by default so by default that is
the activation formula being used now
we're not going to spend we might come
back to this look at some of the other
models cuz it's always good to see what
you're working with but let's go ahead
and jump back in here and we have our
Imports we're going to go ahead and run
those uh so these are now available to
us as we go through our Jupiter notebook
script and they put together a little
piece of data for us this is simply um
going through uh 0 to1 actually let's go
ahead and print this out over here we'll
go ahead and print X just so you can see
what we're actually looking for four and
when we run this you can see that we
have our X is 01 2 through 9 we reshaped
it the reason for this is just looking
for a row of data usually we have
multiple features we just have the one
feature which happens to be 0o through
nine and then we have our 10 answers
right down here uh 0 1 0 00111111 you
can bring in a lot of different data
depending on what you're working with uh
you can make your own you can instead of
having this as just a single you could
actually have like multiple features in
here but we just have the one feature
for uh this particular demo and this is
really where all the magic happens right
here uh and I told you it's like a black
box that's the part that is is is kind
of hard to follow and so if you look
right here we have our model we talked
about the model right there and then we
went ahead and set it for Library linear
as I showed you earlier that's actually
default so it's not that important uh
random State equals zero this stuff you
don't worry too much about and then with
the S kit learn you'll see the model fit
this is very common to S kit they use
similar stuff in a lot of other
different packages but you'll you'll see
that that's very common you have to fit
your data and that means we're just
taking the data and we're fitting our X
right here which is our features that's
our X and here's y these are the labels
we're looking for so before we were
looking at is it fraud is it not is it
cat is it not um that kind of thing and
this is looking at 01 so we want to have
a binary setup on this and we'll go
ahead and run this uh you can see right
here it just tells us what we loaded it
with as our defaults and that this model
has now been created and we've now fit
our data to it and then comes the fun
part you work really hard to clean your
data to um bake it and cook it there's
all kinds of I don't know why they go
with the cooking terms as far as how we
get this data formatted then you go
through and you pick your model you pick
your solver and you have to test it to
see hey which one's going to be best and
so we want to go ahead and evaluate the
model and you do this is that once
you've figured out which one is going to
work the best for you you want to
evaluate it so you can compare it to
your last Model and you can either
update it to create a new one or maybe
change the um solver to something else I
mentioned tangent that's one of the
other common ones that's commonly used
with language for some reason the
tangent even though it looks almost to
me identical to the one we're using with
the sigmoid function uh it for some
reason it activates better with language
even though it's a very small shift in
the actual math behind it we already
looked at the um data early but we'll go
and look at it again just you can see we
look at we have our rows of Zer one row
only has one entity and we have our
output that matches these rows and these
do have to match you'll get an error if
you put in something with a different
shape so if you have um 10 rows of data
and nine answers it's going to give you
an error because you need to have 10
answers for it a lot of times you
separate this too when you're doing
larger models uh but for this we're just
going to take a quick look at that the
first thing we want to start looking at
is uh The Intercept one of the features
inside our linear regression model we'll
go ahead and run that and print it uh
you'll see here we have an intercept of
minus 1.51 6 and if we're going to look
at the uh intercept we should also look
at our um coefficients and if you run
that you'll see that we get a we get the
um our coefficient is the 7035 you can
just think of this as your eidan
geometry for very B basic model like
this where it intercepts the Y at some
point and we have a coefficient
multiplying by it a little more
complicated in the back end but that is
the just a this simple model with just
the one feature in there and we'll go
ahead and uh we'll reprint the Y because
I want to put them on top of each other
with the Y predict and so these were the
Y values we put in and this is the Y
predict we had coming out and you can
see um yeah here we go uh there's the Y
actual and there's what the prediction
comes in uh now keep in mind that we
used the actual complete data as part of
our training uh that is if you're doing
a real model a big stopper right there
because you can't really see how good it
did unless you split some data off to
test it on this is the first step is you
want to see how your model actually test
on the data you trained it with and you
can see here there is this point right
here where it has it wrong and this
point right here where it also has it
wrong and it makes sense cuz we're going
our input is 0 1 0 through 9 and it has
to break it somewhere and this is where
the break is uh so it says this half the
data is going to be zero because that's
what it looked like to me if I was
looking at it without an algorithm and
this data is probably going to be one
and I I didn't I forgot to point this
out so let's go back up here I just kind
of glanced over this window here where
we did a lot of stuff let's go back and
and just take a look at that what was
done here is we ran a prediction uh so
this is where our predict comes in is
our model. predict so we had a model fit
we created the model we programmed it to
give us the right answer uh now we go
ahead and predict what we think it's
going to be there's our model. predict
probability of X and then we have our y
predict which is very similar but this
is has to do more with the probability
numbers so if you remember down below we
had the setup where we're looking at uh
that sigmoid function that's what this
is returning and the Y predict is
returning a zero or a one and then we
have our um confusion Matrix we'll look
at that and we have our report which
just basically Compares our y to our y
predict which we just did it's kind of
nice as simple data so it's really easy
to see what we're doing that's why we do
use the simple data this can get really
complicated when you have a lot of
different features and things going on
and splits uh so here we go we've had
our I printed out our actual and our
prediction so this is the actual data
this is what the predict ran um and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that uh this is great
if you have a lot of data to look at but
you can see right here our confusion
Matrix says uh if you remember from the
confusion Matrix we have the two this is
two correct one two and uh it's been a
while since I looked at a a confusion
Matrix there's the two and then we have
this one which is our six that's where
the six comes from and then we have have
this one which is the um one false this
is the two ones so we have this one here
and this one here which is
misclassified this really depends on
what data you're working with as to what
you're is important um you might be
looking at this model and if this model
this confusion Matrix comes up and says
that uh you've
misclassified even one person as being
nonmalignant cancer that's a bad model
uh I wouldn't want that classification
I'd want this number to be zero I
wouldn't care if this false positive was
off by a little bit more long as I knew
that I was correct on the important
factor that I don't have cancer so you
can see that this confusion Matrix
really aims you in the right direction
of what you need to change in your model
how you need to adjust it uh and then
there's of course a report reports are
always nice um if you notice we
generated a report earlier we'll go and
just print the report up and you can
remember this is our report it's a
classification report y comma y predict
so we're just putting in our two values
basically what we did here visually with
our actual and our predicted value and
we'll go ahead and run the report and
you can see it has the Precision uh the
recall your F1 score your support uh
translated into accuracy macro average
and weighted average so it has all the
numbers a lot of times when working with
um clients or with the shareholders in
the company this is really where you
start because it has a lot of data and
they can just kind of stare at it and
try to figure it out and then you start
bringing in like the confusion Matrix I
almost do this in Reverse as to what
they show I would never show your
shareholders The Intercept of the
coefficient that's for your internal
team only working on machine language uh
but the confusion Matrix and the report
are very important those are the two
things you really want to be able to
show on these uh and you can see here we
did a decent job of um classifying the
data managed to get a significant
portion of it correct uh we had our was
it accuracy here is a 080 F1 score uh
that kind of thing so you know it's a
pretty accurate model of course this is
pretty goofy it's very simple model and
it's just splitting the model between uh
ones and zeros so that was our demo of
the U on logistic regression on there
let's go take a look at K nearest
neighbors uh this one is another very
highly used and important algorithm to
understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based on the similarity measure the K
nearest neighbor finds out the class of
the new data point by finding its
nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the new data
point then the KNN classifies the new
data point as Class A the K and K
nearest neighbors is the number of
nearest neighbors we are looking for I
IE if we say k equal 3 this means that
we are looking for nearest three
neighbors of unclassified data point
usually we take the K value between 3 to
10 as it leads to a better result a
smaller value of K means that noise will
have a bigger influence on the result
and a larger value of K makes it
computationally expensive hence the data
scientists prefer the range of K between
3 and 10 when we talk about noise you
remember the data we just looked at was
01 1 0 0 it had some some values where
cut it and said everything to the right
is a one everything to the left is a
zero but it had some ones and zeros
mixed in there that's called noise
that's what they're talking about is
there's some things that are right in
the middle in the classification which
makes it very hard to classify so
suppose we're trying to find the class
for a new Point indicated by the red
color and you can see it's kind of right
between the cat right between the dogs
let k equal three so we are finding the
three in in for the red data point by
looking at the plot on the right we can
see that the red data point belongs to
the class dogs as it has two votes for
class dog and one vote for class cat and
if you ask the question well what are
you measuring the distance what is that
distance um it could be the measurements
of the ears whether they're pointed or
floppy that might be one of the features
you're looking at is how floppy the ears
are um another one might be the whiskers
versus the nose um and then you take
those measurements and using uh one of
the most common things in K means
measurement is the ukian geometry you
can figure out the distance between
those points there's a lot of different
algorithms for that but you can think
about it that you do have to have some
kind of solid data to measure and so we
can conclude that the new data point
belongs to the class dog so let's go
ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our Jupiter notebook and open
up a new um Python Programming script
page of course once we're in here we'll
want to look at the uh ssy kit learn um
I did just a quick search for SK
neighbors Ken neighbors classifier um
this actually is the older version 0 no
023 is a one we want and you'll see here
that we have all their defaults in
Neighbors equals 5 at defaults we were
talking about that between three and 10
there's different ways to weigh it
there's an algorithm based on it I
mentioned you cian geometry finding the
distance there's other algorithms for
figuring out what that distance is and
how to weight those uh and there's a lot
of other parameters you can adjust for
the most part the K means uh basic setup
is a good place to start and just let
the defaults go uh we might play with
some of those we'll see what the guys in
the back did and from here we're going
to import numpy we're going to use
pandas if you haven't been running
pandas pandas is our data frame which
sits on top of numpy uh data frames are
you know numpy is our number array
pandas is our data frame map plot
Library cuz we're going to plot some
graphs everybody likes some pretty
pictures it makes it a lot easier to see
what's going on when you have a nice
display and that's also what the caborn
is in here in the uh setup that sits on
top of the map plot Library the ones we
really want to look at right here are
the what we're bringing in from sklearn
these ones right here uh so from sklearn
we're going to load I mentioned the
breast cancer that's a very popular one
because it has I believe it's 36
measurements so there's 36 features and
unless you're a expert you're not going
to know what any of those features
really mean you can sort of guess what
their special measurements they take of
when they take a image uh and of course
our confusion Matrix so that we can take
a look and see what the data looks like
and how good we did uh and then we have
our knen at Neighbors classifier on here
uh and then I mentioned that uh whenever
you do train training and testing you
want to split the data up you don't want
to train the data and then test it on
the same data that just tells you how
good your training model is it doesn't
tell you whether it actually works on
unknown data and so this just splits it
off so that we can train it and then we
can take a look at data we don't have in
there and see how good it did and we'll
go ahead and load our uh data up so
here's our our setup on that oops there
we go so we're going to go ahead and
load the data up uh we have our x value
and that's going to come from our breast
cancer. data and column breast cancer
feature names so there's our actual um
all our different features we'll print
that out here in a second and then we
have our um mean area mean compactness
so I guess we're going to take the data
and we're only going to use a couple of
the columns this just makes it easier to
read um of course when you actually were
going to do this you'd want to use all
your columns and then we have our y and
this is simply um whether it's either
malignant or B9 and then um we want to
go ahead and drop the first line because
that's how it came in on there and we'll
go ahead let's just take a look at this
a little closer here let's go and run
this real quick and just because I like
to see my data before I run it we can
look at this and we can look at the
original features remember we're only
going to use two features off of here
just to make it a little easier to
follow and here's the actual data and
you can see that this is just this
massive stream of data coming in here
it's going to just skip around cuz
there's so much in there to set up I
think there's like 500 if I remember
correctly and you can see here's all the
different measurements they take but we
don't we don't really need to see that
on here we're just going to take a look
at just the two columns and then also
our solution we'll go and just do a
quick uh print y on here so you can see
what the Y looks like and it is simply
just 0000 0 you know B9 00001 so a one
means it's B9 zero means it's malignant
uh is what we're looking at on that and
cut that out of there the next stage is
to go ahead and split our data I
mentioned that earlier uh we'll just go
ahead and let them do the splitting
forest for us uh we have XT train X test
y train y test and so we go ahead and
train test split XY random State equals
1 makes it nice and easy for us we'll go
and run that and so now we have our
training and our testing train means
we're going to use that to train the
model and then we're use the test to
test to see how good our model does and
then we'll go ahead and create our model
here's our knnn model the K neighbors
classifier n neighbors equals 5 the
metrics is ukian remember I talked about
ukian uh this is simply your c = A2 +
B2 + u u A2 = B2 + C2 + C2 + d^2 and
then you take the square root of all
that that's what they're talking about
here it's just the length of the
hypotenuse of a triangle but you can
actually do that in multiple dimensions
just like you do in two Dimensions with
a regular triangle and here we have our
fit this should start to look familiar
since we already did that in our last
example that's very standard for s s kit
and any other one although sometimes the
fit algorithms look a little bit more
complicated because they're doing more
things on there especially when you get
into neural networks uh and then you
have your K neighbors it just tells you
we created a um a k neighbor setup they
kind of wanted us to reformat the Y but
it's not that big of a deal for this uh
and it comes out and shows that we're
using the ukian uh metric for our
measurement so now we've created a model
here's our live model we fitted the data
to it we say hey here's our training
data uh let's go ahead and predict it so
we're going to take our y predict equals
KNN predict y test so this is data we
have not this model has not seen this
data and so we're going to create a
whole new set of data off of there now
before we look at our prediction in fact
let's um I'm going to bring this down
and and put it back in here later let's
take a look add our X test data versus
the Y test what does it look like and so
we have our mean area we're going to
compare it to our mean compactness we're
going to go ahead and run that and we
can see here the data if you look at
this just eyeballing it we put it in
here we have a lot of blue here and we
have a lot of orange here and so these
dots in the middle especially like this
one here and these here these are the
ones that are going to give us false
negative and so we should expect this is
your noise this is where we're not sure
what it is and then B9 is in this case
is done in blue and maligant is done in
one uh so if you look at it there's two
points based on these features which
makes it really hard to have a 100%
where the 100% is down here um or up
here that's kind of the thing I'd be
looking for when we're talking about
cancer and stuff like that where you
really don't want any uh false negatives
you want everything you false positive
great you're going to go in there and
have another setup in there where you
might get a get a op Toops or something
like that done on it again that's very
data specific on here uh so now let's go
ahead and pull in and and get our um our
prediction in here and we'll create our
y prediction we'll go and run that so
now this is loaded with what we think
the unknown data is going to be and we
can go ahead and take that and go ahead
and plot it because it's always nice to
have some pretty pictures and when we
plot it we're going to do the mean area
versus mean compactness again you look
at this map and you can see that there's
some clear division here we can clearly
say on some of the stuff that are why
prediction if we look at this map up
here and this map down here we probably
got some pretty good deal it looks
pretty good like they match a lot this
is of course just eyeballing it really
you don't want to eyeball these things
you want to show people the pictures so
that they can see it and you can say hey
this is what it looks like uh but we
really want the confusion Matrix and we
do the Y test and the Y predict we can
see in the confusion Matrix here it did
pretty good um and we'll just go ahead
and point that out real quick um here's
our 42 which is positive and our
79 and if I remember correctly I'd have
to look at the data which one of these
is a false negative I believe it's the
nine that's scary I would not want to be
one of those nine people told that I
don't have cancer and then suddenly find
out I do uh so we would need to find a
way to sort this out and there is
different ways to do that uh a little
bit past this but you can start messing
with the actual ukian geometry and the
activation uh measurements and start
changing those and how they interact but
that's very Advanced there's also other
ways to classify them or to create a
whole another class right here of we
don't knows those are just a couple of
the solutions you might use for that but
for a lot of things this works out great
uh you can see here you know maybe
you're trying to sell something well if
this was not uh life dependent and this
was if I display this ad 42 of these
people are going to buy it and if I
display this other ad if I don't display
it 79 people are going to go a different
direction or whatever it is so maybe
you're trying to display whether they're
going to buy something if you add it on
to the website in which case that's
really a good numbers you've just added
a huge number of sales to your company
so that was our K nearest neighbors uh
let's go ahead and take a look at
support Vector machines so support
vector machines uh is the main objective
of a support Vector machine algorithm is
to find a hyperplane in an n-dimensional
space in is a number of features that
distinctly classifies the data points
and if you remember we were just looking
at those nice graphs we had earlier in
fact Let Me Go a and flip back on over
there if we were looking at this data
here we might want to try to find a nice
line through the data and that's what
we're talking about with uh this next
setup so the main objective support
Vector machine algorithm is to find a
hyperplane in an N dimensional space N
is a number of features that distinctly
classifies the data points to separate
the two classes of data there are many
hyperplanes that can be chosen our
objective is to find the plane that has
the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyper
plane depends on the number of features
if there are two input features then the
hyper plane is just a line if there are
three features then the hyperplane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support vector classifier
or hard margin and that's why I just
showed you on the other data you can see
here we look for a line that splits the
data evenly the problem with hard margin
is that it doesn't allow outliers and
doesn't work with
non-linearly separable data and we just
were looking at that let me flip back on
over here and when we look at this setup
in here and we look at this data here we
go look how many outliers are in here
these are all with all these blue dots
on the wrong side of the line would be
considered an outlier and the same with
the red line and so it becomes very
difficult to divide this data unless
there's a clear space there we go
therefore we introduce soft margins
which accept the new data point and
optimize a model for nonlinear data
points soft margins pass through the
data points at the border of the classes
the support vector machine can be used
to separate the two classes of shapes
here we can see that although triangles
and diamond shapes have pointy edges but
we are able to classify them in two
categories using a support Vector
machine let's go ahead and see what that
looks like in a demo we flip back on
over to our Jupiter notebook we always
want to start with taking a look at the
sklearn uh API in this case the svm SVC
there is a significant number of
parameters because SK the um svm has
been a lot of development in the recent
years and it's become very popular if we
scroll all the way down to methods
you'll see right here as our fit and our
predict that's what you should see in
most of the um sidekit learn packages so
this should look very familiar to what
we've already been working on and we'll
go ahead and bring in our import and run
that uh we should already have pandas
numpy our map plot Library which we're
going to be using to graph some of the
things a little bit different right here
you'll see that we're going to go ahead
and bring in and um one of the fun
things you can do with test data is uh
make circles circles are really hard to
classify you have a ring on the inside
and a ring on the outside and you can
see where that can cause some issues and
we'll take a look at that a little
closer in a minute here's our svm uh
setup on there and then of course our
metrics because we're going to use that
to take a closer look at things so we go
and run this whoops already did once
we've gone ahead and done that we're
going to go ahead and start making
ourselves some data and this part
probably a little bit more complicated
uh than we need I'm not going to go too
much in detail on it uh we're going to
make a mesh grid and we'll see what that
looks like in a minute we're defining
the minimums you can see in here create
a mesh grid of points here's our
parameters of x y and H it's going to
return XX y y you can also send a note
to us make sure you get a copy of this
if you want to dig deeper into this
particular code especially and we have a
y men y Max uh y men y Max + one here's
our mesh grid we're actually going to
make XX and y y plot the Contours all
the way through and this is just way of
creating data it's kind of a fun way to
create some data we go plot Contours ax
LF XX y y and uh return and out add some
perimeters here so that when we're doing
it we have our setup on there train
property and then we'll go ahead and
make our data just throw that right in
there too in the same setup and run that
uh so now we have X and Y we're going to
make our circles we have n samples equal
samples in this case we're going to have
500 samples we went ahead and gave it a
noise of 0.005 random State 123 these
are all going into oops go back up here
we go make our mesh grid make circles
there it is uh so this is going into our
make circles up here and this is part of
that setup on this and then once we've
gone ahead and make the circle let's go
ahead and plot it uh that way you can
see what we're talking about right now
what I'm saying is really confusing
because without a visual it doesn't
really mean very much what we're
actually doing so I'm going go ahe and
run this with the plot and let's go back
up here and just take a look we've made
our Circle we have our end samples
equals sample so we're going to have 500
we're going to have training property
point8 here's our data frame we go and
load it into the data frame so we can
plot it groups DF Group by label uh this
is kind of a fun way if you have
multiple columns you can really quickly
pull whatever setup is in there and then
we go ahead and plot it and you can see
here we have two rings that are formed
and that's all this is doing is just
making this data for us this is really
hard data to figure out um a lot of
programs get confused in this because
there's no straight line or anything
like that but we can add planes and
different setups on here and so you can
see we have some very interesting data
we have our zero is in blue and our
one's in the yellow in the middle and
the data points are an XY coordinate
plot on this one of the things we might
want to do on here is go ahead and find
the Min to Max ra ratio uh set up in
there and we can even do let's just do a
print X so you can see what the data
looks like that we're producing on this
there we go so uh xal x - Xmen over x
max - Xmen all we're doing is putting
this between zero and one whatever this
data is we want um a zero to one setup
on here if you look at this all our data
is
8.5 that's what this particular line is
doing that's a pretty common thing to do
in processing data especially in neural
networks uh neural networks don't like
big numbers they create huge biases and
cause all kinds of problems and that's
true in a lot of our models some of the
models it doesn't really matter but when
you're doing enough of these you just
start doing them uh you just start
putting everything between zero and one
there's even some algorithms to do that
in the SK learn although it's pretty as
you can see it's pretty easy to do it
here so uh let's go ahead and jump into
the next thing which is a linear kind of
setup Cal 1.0 this is the svm
regularization parameter we're going to
use models here's our svm and let's go
and scroll up just a notch there we go
now we so here we are with our model
we're going to create the the setup with
the SVC kernel is linear and we'll come
back to that CU that's an important uh
setup in there as far as what our kernel
is and you'll you'll see why that's so
important here in a minute cuz we're
going to look at a couple of these and
so this one is we're actually going to
be changing how that processes it uh and
then our C here's our one our 0.0 and
then the rest of this is uh plotting
we're just adding titles making the
Contours so it's a pretty graph you can
actually spend this would be a whole
class just to do all the cool things you
can do with Scatter Plots and regular
plots and colors and things like that in
this case we're going to create a graph
with a nice white line down the middle
so that you can see what's going on on
here and when we do this you can see
that as it as it split the data uh the
linear did just that it drew a line
through the data and it says this is
supposed to be blue and this is supposed
to be red and it doesn't really fit very
well that's because we used a linear
division of the data and it's not very
linear data on it's anything but linear
so when we when we look at that it's
like well okay that didn't work what's
the next option well there's a lot of
choices in here one of them is just
simply we can change this from the
kernel being linear to poly and we'll
just go back up here and use the same
chart uh oops here we go uh so here we
go uh linear kernel we'll change this to
poly and then when we come in here and
create our model here's our model up
here linear we can actually just go
right in and change this to the poly
model and if you remember when we go
back over here to the SBC and let's
scroll back up here there's a lot of
different options oops even further okay
so when we come up here we start talk
about the kernel here's the kernel
there's linear there's poly RBF sigmoid
precomputed there's a lot of different
ways to do this setup um and their
actual default is RBF very important to
note that uh so when you're running
these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the uh spvm the Kel is so important
you really need to know that and we
switched our kernel to poly and when we
run this this uh you can see it's
changed a little bit we now have quite
an interesting looking diagram and you
can see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is uh we do a a poly fit uh you
can see if you have a split in data
where there's a group in the middle this
one kind of data the groups on the
outside are different the polyfit or the
poly kernel is what's going to be fit
for that so uh if that doesn't work then
what are we going to use well uh they
have the RBF kernel and let's go ahead
and take a look and see what the RBF
looks like and uh let me go and turn
there we go turn my drawing off and the
RBF kernel oops there we go RBF and then
of course for our title it's always nice
to have it match with the RBF kernel and
we go ahead and run this
and you can see that the RBF kernel does
a really good job uh it actually has
divided the data up on here and this is
the kind of what you expect here was
that ring here's our inner ring and an
outer ring of data and so the RBF fits
this data package quite nicely um and
that when we talk about svm it really is
powerful in that it has this kind of
sorting feature to it in its algorithms
this is something that is really hard to
get the SK uh means to do or the K means
uh setup and so when you start looking
at these different machine learning
algorithms understanding your data and
how it's grouped is really important it
makes a huge difference as far as what
you're Computing and what you're doing
with it so that was the demo on the
support Vector uh certainly you could
have done continued on that and done
like a confusion Matrix and all that
kind of fun stuff to see how good it was
and split the data up to see how it uh
uh vectorizes the visual on that so
important it makes a big difference just
to see what it looks like and that giant
donut and why it it does Circle so well
or your poly version or your linear
version so we've looked at some very
numerical kind of uh setups where
there's a lot of math involved ukian
geometry um that kind of thing a totally
different machine learning algorithm for
approaching this is the decision trees
and there's also Forest that go with the
decision trees they're based on multiple
trees combined the decision tree is a
supervised learning algorithm used for
classification it creates a model that
predicts the value of a Target variable
by learning simple decision rules
inferred from the data features a
decision tree is a hierarchal tree
structure where an internal node
represents features or tribute the
branch represents a decision Rule and
each Leaf node represents the outcome
and you can see here where they have the
first one uh yes or no and then you go
either left or right and so forth one of
the coolest things about decision trees
um is and I'll see people actually run a
decision tree even though their final
model is different because a decision
tree allows you to see what's going on
you can actually look at it and say why
did you go right or left what was the
choice where's that break uh and that is
really nice if you're trying to share
that information with somebody else as
to why when you start getting into the
why this is happening decision trees are
very powerful so the topmost note of a
decision tree is known as the root note
it learns to Partition on the basis of
the attribute value it partitions the
tree in a recursive manner so you have
your decision node if you get yes you go
down to the next note that's a decision
Noe and either yes you go to if it ends
on a leaf node then you know your answer
uh which is yes or no so there's your
there's your in classification set up on
there here's an example of a decision
tree that tells whether I'll sleep or
not at a particular evening mine would
be depending on whether I have the news
on or not do I need to sleep no okay
I'll work uh yes is it raining outside
yes I'll sleep no I'll work so I guess
if it's uh not raining outside it's
harder to fall asleep where they have
that nice uh rain coming in and again
this is really cool about a decision
tree is I can actually look at it and go
oh I like to sleep when it rains outside
so when you're looking at all the data
you can say oh this is where the switch
comes in when it rains outside I'll
sleep really good if it's not raining or
if I don't need sleep then I'm not going
to sleep I'm going to go work so let's
go ahead and take a look at that that
looks like in the code just like we did
before we go ahead and open up the site
kit setup just to tell you what the
decision tree classifier has you have
your parameters which we'll look a
little bit more in depth at as we write
the code but it has uh different ways of
splitting it the strategy used to choose
a split at each node uh Criterion max
depth remember the tree how far down do
you want it do you want it to take up
the space of your whole computer with
and and map every piece of data or you
know the small small that number is the
smaller the level the tree is and the
less processing it takes but it's also
more General so you're less likely to
get as in-depth and answer um and then
of course minimal samples you need for
to split samples for the leaf there's a
lot of things in here as far as what how
big the tree is and how to define it and
when do you define it and how to weight
it and they have their different
attributes which you can dig deeper into
uh that can be very important if you
want to know the why of things uh and
then we go down here to our methods and
you'll see just like everything else we
have our fit method very important uh
and our predict uh the two main things
that we use what is what we're going to
predict our X to be equal to and we'll
go ahead and go up here and start
putting together the code uh we're going
to import our numpy our pandas there's
our confusion Matrix our train test
Split Decision tree classifier that's
the big one that we're actually working
with uh that's the line right here where
we're going to be oops decision tree
there it is decision tree class
classifier that's the one I was looking
for and of course we want to know the
accuracy and the classification report
on here and we're going to do a little
different than we did the other examples
and there's a reason for this let me go
and run this and load this up here uh
we're going to go ahead and build things
on functions and this is when you start
splitting up into a team this is the
kind of thing you start seeing a lot
more both in teams and for yourself
because you might want to swap one data
to test it on a different data depending
on what's going on uh so we're going to
have our import data here um the data
set length the balance and so forth um
this just returns balance data let me
just go ahead and print CU I'm curious
as to what this looks like import data
and it's going to return the balance
data so if I run that uh if we go ahead
and print this out here and run that you
can see that we have uh a whole bunch of
data that comes in there and some
interesting setup on here has a let's
see BR RR I'm not sure exactly what that
represents on here uh 111 112 and so
forth so we have a different set of data
here the shape is uh five columns 1 2 3
4 five uh seems to have a number at the
beginning which I'm going to guess uh b
r l a letter I mean and then a bunch of
numbers in there one one one1 let's see
down here we got 555 uh set up on this
and let's see balance data and since it
said balance data I'm going to guess
that uh b means balanced R means you
need to move it right and L means it's
um needs to be moved left or skewed to
the left I'm not sure which one uh let's
go and close that out and we'll go ahead
and create a function to split the data
set uh X balance data equals data values
y balance equals data values of zero
there's that letter remember left right
and balance then we're looking for the
values of 1 through five and we go ahead
and split it just like you would X train
y train set random State 100 test size
is.3 so we're taking 30% of the data and
it's going to return your X your y your
y train your uh your X train your X test
your y train your y test again we do
this because if you're running a lot of
these you might want to switch how you
split the data and how you train it I
tend to use a bfold method I'll take a
third of the data and I'll train it on
the other two thirds and test it on that
third and then I'll switch it I'll
switch which third is a test data and
then I can actually take that
information and correlate it and it
gives me a really uh robust package for
figuring out what the complete accuracy
is uh but in this case we're just going
to go ahead this is our function for
splitting data and this is where it kind
of gets interesting because remember we
were talking a little bit about uh the
different settings in our model and so
uh in here we're going to create a
decision tree but we're going to use the
Gen Genie setup and where did that come
from uh what's the genie on here uh so
if we go back to the top of their page
and we have what uh criterian are we
going to use we're going to use Genie
they have Genie and entropy those are
the two main ones that they use for the
decision tree uh so this one's going to
be Genie and if we're going to have a
function that creates the Genie model
and it even goes down here and here's
our fit train of the Genie model uh
we'll probably also want to create one
for entropy sometimes I even just
um I might even make this just one
function with the different setups and I
know one of my one of the things I
worked on recently I had to create a one
that tested across multiple models and
so I would send the parameters to the
models or I would send this part right
here where it says decision tree
classifier that whole thing might be
what I send to create the model and I
know it's going to fit we're going to
have our XT train and we're going to
have a predict and all that stuff is the
same so you can just send that model to
your function uh for testing different
models uh again this just gives you one
of the ways to do it and you can see
here we're going to chain train with the
genie and we're also going to chain
train with the entropy to see how that
works and if you're going to have your
models going two separate models you're
sending there we'll go ahead and create
a prediction this simply is our y
predict equals our uh whatever object we
sent whatever model we sent here the CL
LF object and predict against our X test
and you can see here print y y predict
and return y predict set up on here
we'll load that definition up and then
if you're going to have a function that
runs a predict and print some things out
uh we should also have our accuracy
function so here's our calculate the
accuracy what are we sending we're
sending our y test data this could also
be y actual and Y predict and then we'll
print out a confusion Matrix uh then
we'll print out the accuracy of the um
score on here and and print a report
classification report bundle it all
together there so if we bring this all
together we have um all the steps we've
been working towards which is importing
our data by the ways you'll spend 80% of
your time importing data in most machine
learning setups and cooking it and
burning it and getting it formatted so
that it it uh works with whatever models
you're working with the decision tree
has some cool features in that if you're
missing data it can actually pick that
up and just skip that and says I don't
know how to split this there's no way of
knowing whether it rained or didn't rain
last night so I'll look at something
else like whether you watched uh TV
after 8:00 you know that blue screen
thing uh so we have our function
importing our data set we bring in the
data we split the data so we have our X
test test and Y train and then we have
our different models our clf Genie so
it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy uh and
then once we have that we have our
function for making the prediction
and we have our function for calculating
the accuracy uh and then if we're going
to have that we should probably have our
main code involved here this probably
looks more familiar if you're depending
on what you're working on if you're
working on like a pi charm then you
would see this in throwing something up
real quick in Jupiter notebook uh so
here's our our main data import which
we've already defined uh we get our
split data we create our Genie we create
our entropy so there's our two models
going on here there's our two models so
these are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie and we want to go ahead
with the genie and print our um our
predictions YX test to the genie and
calculate the accuracy on here and then
we want to print the results using
entropy so this is just the same thing
coming down like we did with the genie
we're going to put out our y predict
entropy and our calculations so let's go
ahead and run that just see what this uh
piece of code does uh we do have like
one of our data needs to be is getting a
warning on there that's nothing major
because it's just a simple warning
probably an update of a new version's
coming out uh and so here we are we have
our data set it's got 625 you can
actually see an example of the data set
B meaning balanced I guess and here's
our five data points 1111 means it's
balanced it's skewed to the right with
1112 uh and so forth on here and then
we're going to go ahead and predict
from our prediction whether it's to the
right or to the left you can think of a
washing machine that's SK that's banging
on one side of the thing or maybe it's
an automated car where we down the
middle of the road that's IM balance and
it starts going veering to the right so
we need to correct for it uh and when we
print out the confusion Matrix we have
three different variables r l and B so
we should three the three different
variables on here and you have as far as
whether it predicts in this case the
balance there's not a lot of balance
loads on here and and didn't do a good
job guessing whether it's balanced or
not that's what I took from this first
one uh the second one I'm guessing is
the right so it did pretty good job
guessing the right balance you can see
that a bunch of them came up left
unbalanced um probably not good for a
automated car as it tells you 18 out of
the uh 18 missed things and tells you to
go the wrong direction and here we are
going the other way uh 19 to 71 and of
course we can back that up with an
accuracy report on here you can see the
Precision how well the left and right
balance is 79% 79% precision and so
forth and then we went and used the
entropy and let me just see if we can
get so we can get them both next to each
other here's our entropy of our um the
first setup our first model which is the
Genie model 6718
1971
6322
2070 pretty close the two models you
know that's not a huge difference in
numbers this second one of entropy did
slightly it looks like slightly worse
because it did one better as far as the
right balance and did what is this four
worse on the left balance or whatever uh
so slightly worse if I was guessing
between these I'd probably use the first
one they're so close though that
wouldn't be it wouldn't be a Clear
Choice as to which one worked better and
there's a lot of numbers you can play
with here which might give better
results depending on what the data is
going in now uh one of the takeaways you
should have from the different category
routines we ran is that they run very
similar you you certainly change the
perimeters in them as to whether you're
using what model you're using and how
you're using it and what data they get
applied to but when you're talking about
the scikit learn package it does such an
awesome job of making it easy uh you
split your data up you train your data
and you run the prediction and then you
see what kind of accuracy what kind
confusion confusion Matrix it generates
so um we talk about algorithm selection
logistic regression K nearest neighbors
uh logistic regression is used when we
have a binomial outcome for example to
predict whether an email is Spam or not
whether the tumor is malignant or not
the logistic regression works really
good on that you can do it in a k
nearest neighbors also the question is
which one will it work better in um I
find the logistic regression models work
really good in a lot of Rod number so if
you're working with say the stock market
is this a good investment or a bad
investment um so that's one of the
things it handles the numbers better K
nearest neighbors are used in scenarios
where
nonparametric no fixed number of
perimeters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection uh so K means
really good in finding the patterns um
I've seen that as a pre-processor to a
lot of other processors where you use
the K nearest neighbors to figure out
what data groups together very powerful
package support Vector machines uh
support Vector machines are used
whenever the data has higher Dimensions
the human genome microarray svms are
extensively used in the hard handwriting
recognition models and you can see that
we were able to switch between the
parabolic and the circular setup on
there where you can now have that donnut
kind of data and be able to filter that
out with the support Vector machine and
then decision trees are mostly used in
operational researches specifically in
decision analysis to help identify a
strategy most likely to reach any goal
they are pre preferred where the model
is easy to understand I like that last
one it's a good description is it easy
to understand so you have data coming in
when am I going to go to bed you know is
it raining outside you can go back and
actually look at the pieces and see
those different decision modes takes a
little bit more to dig in there and
figure out what they're doing uh but you
can do that and you can actually help
you figure out why um people love it for
the why Factor so uh strings and
limitations big one on all of these the
strengths and limitations we talk about
logistic regressions uh the strings are
it is easy to implement and efficient to
train it is relatively easy to
regularize the data points remember how
we put everything between zero and one
when you look at logistic regression
models uh you don't have to worry about
that as much limitations has a high
Reliance on proper representation of
data it can only predict a categorical
outcome with the K nearest neighbors it
doesn't need a separate training period
new data can be added seamlessly without
affecting the accuracy of the model uh
kind of an interesting thing because you
can do partial training uh that can
become huge if you're running across
really large data sets or the datas
coming in you can continually uh do a
partial fit on the data with the K
nearest neighbors and continue to adjust
that data uh it doesn't doesn't work on
high dimensional and large data sets we
were looking at the breast cancer uh 36
different features what happens when you
have 127 features or a million features
and you say well what do you have a
million features in well if I was
analyzing uh log um the legal documents
I might have a tokenizer that splits a
words up to be analyzed and that
tokenizer might create 1 million
different words available that might be
in the document for doing weights uh
sensitive to noisy data outliers and
missing values that's a huge one with K
nearest neighbors they really don't know
what to do with a missing value how do
you compute the the distance if you
don't know what the value is uh the svm
uh Works more efficiently on
high-dimensional data it is relatively
memory efficient so it's able to create
those planes with only a few different
variables in there as opposed to having
to store a lot of data for different
different uh features and things like
that it's not suitable for a large data
sets uh the svm you start running this
over gigabytes of data causes some huge
issues underperforms if the data has
noise or overlapping that's a big one we
were looking at that where the spvm
splits it and it creates a soft buffer
but what happens when you have a lot of
stuff in the middle uh that's hard to
sort out it doesn't know what to do with
that causes SPM to start crashing or not
perform as well decision trees handles
nonlinear perimeters and missing values
efficiently the missing values is huge
I've seen this in uh was it the wine
tasting data sets where they have three
different data sets and they share
certain features uh but then each one
has some features that aren't in the
other ones and it has to figure out how
to handle those well the decision tree
does that automatically instead of
having to figure a way to fill that data
in before processing like you would with
the other models uh it's easy to
understand and has less training period
so it trains pretty quickly uh comes up
there and just keeps forking the tree
down and moving the parts around and so
it it doesn't have to go through the
data multiple times guessing and
adjusting it just creates the tree as it
goes overfitting and high variants are
the most annoying part of it that's
that's an understatement uh that has to
do with how many leavs and how many
decisions you have it du the more you
have the more overfit it is to the data
and also uh just in making the choices
and how the choices come in it might
overfit to a specific feature because
that's where it started at and that's
what it knows and it really um is
challenged with large data sets they've
been working on that with the data
Forest but it's not suitable for large
data sets it's really something you'd
probably run on a single machine and not
across um not across a uh data pool or
anything decision tree one of the many
powerful tools in the machine learning
library begins with a problem I think I
have to buy a car so in making this
question you want to know how do I
decide which one to buy and you're going
to start asking questions is a mileage
greater than 20 is a price less than 15
will it be sufficient for six people
does it have enough airbag anti-lock
brakes all these questions come up then
as we feed all this data in we make a
decision and that decision comes up oh
hey this seems like a good idea here's a
car so as we go through this decision
process using a decision tree we're
going to explore this maybe not in
buying a car but in how to process data
what's in it for you let's start by
finding out what is machine learning and
why we even want to know about it for
processing our data and we'll go into
the three basic types of machine
learning and the problems that are used
by Machine learning to solve finally
we'll get into what is a decision tree
what are the problems a decision tree
solves what are the advantages and
disadvantages of using a decision tree
and then we want to dig in a little deep
into the mechanics how does the decision
tree work and then we'll go in and do a
case loan repayment prediction where we
actually going to put together some
python code and show you the basic
python code for generating a decision
tree what is is machine learning there
are so many different ways to describe
what is machine learning in today's
world and illustrate it we're going to
take a graphic here and uh making
decisions or trying to understand what's
going on and really underlying machine
learning is people want to wish they
were smarter wish we could understand
the world better so you can see a guy
here who's uh saying hey how can I
understand the world better and someone
comes up and says let's use artificial
intelligence machine learning is a part
of artificial intelligence and now he's
get a big smile on his face cuz now he
has artificial intelligence to help him
make his decisions uh and they can think
in new ways so this brings in new ideas
so what is machine learning this is a
wonderful graph here you can see where
we have a learn predict decide these are
the most basic three premises of machine
learning in learning we can describe the
data new ways and able to learn new
aspects about what we're looking at and
then we can use that to predict things
and we can use that to make decisions so
maybe it's something that's never
happened before but we can make a good
guess whether it's going to be a good
investment or not it also helps us
categorize stuff so we can remember it
better so it's easier to pull it out of
the catalog we can analyze data in new
ways we never thought possible and then
of course there's the very large growing
industry of recognize we can do facial
recognition driver recognition automated
car recognition all these are part of
machine learning going back to our guy
here who's in his ordinary system and
would like to be smarter make better
choices what happens with machine
learning is an application of artificial
intelligence where in the system gets
the ability to automatically learn and
improved based on experience so this is
exciting because you have your ordinary
guy who now has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you'd already have a
list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learning tool and that lets you
predict on the next person whether
they're going to be able to make their
payments or not on their loan if you
have one category where you already know
the answer answers the next one would be
you don't know the answers you just have
a lot of information coming in
unsupervised learning allows you to
group liked information together so if
you're analyzing photos it might group
all the images of trees together and all
the images of houses together without
ever knowing what a house or a tree is
which leads us to the third type of
machine learning the third type of
machine learning is reinforcement
learning unlike supervised or
unsupervised learning you don't have the
data prior to starting so you get the
data one line at a time time and then
whether you make a good choice or a bad
choice the machine learning tool has to
then adjust accordingly so you get a
plus or minus feedback you can liken
this to the way a human learns we
experience life one minute at a time and
we learn from that and either our
memories as good or we learn to avoid
some problems in machine learning to
understand where the decision tree fits
into our machine learning tools we have
to understand the basics of some of the
machine learning problems and three of
the primary ones fall underneath classif
ification problems with categorical
Solutions like yes or no true or false
one or zero this might be does it belong
to a particular group yes or no then we
have regression problems where there's a
continuous value need to be predicted
like product prices profit and you can
see here this is a very simple linear
graph uh you can guess what the next
value is based on the first four it kind
of follows a straight line going up and
clustering this is problems where in the
data needs to be organized to find
specific patterns like in the case of
product recommendation they group all
the different products that people just
like you viewed on a shopping site and
say people who bought this also bought
this the most commonly used for the
decision tree is for classification for
figuring out is it red or is it not is
it a fruit or is it a vegetable yes or
no true false left or right 01 and so
when we talk about classification we're
going to look at the basic machine
learning these are the four main tools
used in classification there's the Nave
Bay logistic regression decision tree
and random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or a curve
through the data they work Wonderful in
a lot of problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest
Forest but today we're just going to
focus on the decision
tree what is a decision tree let's go
through a very simple example before we
dig in deep decision tree is a
tree-shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction let's start with
a simple question how do identify a
random vegetable from a shopping bag so
we have this group of vegetables in here
and we can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
it's going to be a what looks to be a
red chili and if it's true it's going to
be a bell pepper from the capsicum
family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error before before we dig
deeper into the mechanics of the
decision tree let's take a look at the
advantages of using a decision tree and
we'll also take a glimpse at the
disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and nonlinear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction if
we're going to look at the advantages of
a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a General
solution for all the data High variance
the model can get unstable due to small
variation in data low bias Tre a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after after the data
set is split so in this case based on
the color yellow we've split one group
of animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
uh deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math at how
they compute entropy finally we went to
the different parts of our tree and they
call the leaf node Leaf node carries the
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node how does a
decision tree work wonder what kind of
animals I'll get the jungle today maybe
you're the hunter with the gun or if
you're more into photography you're a
photographer with a camera so let's look
at this group of animals and let's try
to classify different types of animals
based on their features using a decision
tree so the problem statement is to
classify the different types of animals
based on their features using a decision
tree the data set is looking quite messy
and the entropy is high in this case so
let's look at a training set or a
training data set and we're looking at
color we're looking at height and then
we have our different animals we have
our elephants our giraffes our monkeys
and our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a uh e funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math you don't really have to memorize
this math just be aware that it's there
and this is what's going on in the
background and so we have three dra s
two tigers one monkey two elephants a
total of eight animals gathered and if
we plug that into the formula we get an
entropy that equals 3 over 8 so we have
three drafts a total of eight times the
log usually they use base two on the log
so log base 2 of 3 over 8 plus in this
case let's say it's the elephants 2 over
8 2 elephants over total of 8 * log base
2 2 over8 plus one monkey over total of
8 log base 2 1 over8 and plus 2 over 8
of The Tiger log base 2 over 8 and if we
plug that into our computer our
calculator I obviously can't do logs in
my head we get an entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent inter P values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as a condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes
the Tigers the monkey and the elephants
all separated into their own own groups
this tree can now predict all the
classes of animals present in the data
set with 100% accuracy that was easy use
case loan repayment prediction let's get
into my favorite part and open up some
Python and see what the programming code
in the scripting looks like in here
we're going to want to do a prediction
and we start with this individual here
who's requesting to find out how good
his customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and run it in so let's flip on
over and here I'm using the Anaconda
Jupiter notebook now you can use any
python IDE like to run it in but I find
the jupyter notebook's really nice for
doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numers it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more we explore the data in just a
minute then we're going to need to split
the data so I'm going to bring in our
train test and split and this is coming
from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this this very lengthy setup
on here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want to
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have SK
learn. cross validation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap pop it right in there with
model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 four
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet so you can see what
that looks like and here I've opened it
up in the open Office Cal which is
pretty much the same as um Excel and
zoomed in and you can see we've got our
columns and our rows of data little
easier to read in here we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lind function from Python and
when I run this you'll see that it's a
thousand long and that's what we
expected there's a th lines of data in
there if you subtract the column head
and this is one of the nice things when
we did the uh balance data from the
panda read CSV you'll see that the
header is row zero so it automatically
removes a
row and then show shows the data
separate it does a good job sorting that
data out for us and then we can use a
different function and let's take a look
at that and again we're going to utilize
the tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about this shape is not
only does it give me the length of the
data we have a thousand lines it also
tells me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the uh pandas module for head
another beautiful thing in the data set
that we can utilize so let's put that on
our sheet here and we have print data
set and balance data. head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
our print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as the head it
prints the first five lines of the data
and we always start with zero so we have
five Lin we have 0 1 2 3 4 instead of 1
2 3 4 5 that's a standard scripting and
programming set is you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X X and Y to denote the data
n and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember we always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to3
so it's roughly 30% will be used in the
test and then we use a random state so
it's completely random which rows it
takes out of there and then finally we
get to actually build our decision tree
and they've called it here clf entropy
that's the actual decision tree or
decision tree classifier and in here
they've added a couple variables which
we'll explore in just a minute and and
then finally we need to fit the data to
that so we take our clf entropy that we
created and we fit the X train and since
we know the answers for X train or the Y
train we go aad and put those in and
let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf entropy equal
decision tree classifier it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that is
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves is five
so it's going to have at least five
leavs at the end so I'll have at least
three splits I'll have no more than
three layers and at least five end leavs
with the final result at the bottom now
that we've created our decision tree
classifier not only created it but
trained it let's go ahead and apply it
and see what that looks like so let's go
ahead and make a prediction and see what
that looks like we're going to paste our
predict code in here and before we run
it let's just take a quick look at
what's this doing here we have a
variable y predict that we're going to
do and we're going to use our variable
clf entropy that we
created and then you'll see do predict
and that's very common in the sklearn
modules that there different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
would be the new loans you're putting in
here to guess whether the person's going
to be uh pay them back or not in this
case so we need to test out the data and
just see how good our sample is how good
of our tree does at predicting the loan
payments and finally since Anaconda
Jupiter notebook is works as a command
line for python we can simply put the Y
predict e in to print it I could just as
easily have put the
print and put brackets around y predict
e in to print it out we'll go ahead and
do that it doesn't matter which way you
do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here here remember it's 30% of a th000
so you should have about 300 answers in
here and this tells you which each one
of those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from sklearn docs import accuracy
score and you could probably run a
script make your own script to do this
very easily how accurate is it how many
out of 300 do we get right and so we put
in our y test that's the one we ran the
predict on and then we put in our y
predict in that's the answers we got and
we're just going to multiply that by a
100 because this is just going to give
us an answer as a decimal and we want to
see it as a percentage and let's run
that and see what it looks like and if
you see here we got an accuracy of 93.
66667 so when we look at the number of
loow loans and we look at how good our
model fit we can tell people it has
about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses a decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not there's no
better time to train yourself in the
exciting field of machine learning if
you're looking for a course that covers
everything from the fundamentals to
Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our CCH postgraduate program in Ai
and machine learning in partnership with
IBM this a ml course covers the latest
tools and Technologies from the AI
ecosystem and features master classes by
celtech faculty and IBM experts
hackathon and ask me anything sessions
this program showcases celtech ctm is
excellence and IBM's industry progress
the artificial intelligence course
coveres key Concepts like statistics
data science with python machine
learning deep learning NLP and
reinforcement learning through an
Interactive Learning model with live
sessions androll now and unlock exciting
a ml opportunities the course link is
mentioned in the description box below
Forest currently today is used in remote
sensing uh for example they're used in
the etm devices if you're a space buff
that's the enhanced thermatic mapper
they use on satellites which see uh far
outside the human Spectrum for looking
at land masses and they acquire images
of the Earth's surface the accuracy is
higher and training time is less than
many other machine learning tools out
out there also object detection
multiclass object detection is done
using random Forest algorithms a good
example is a traffic where you're trying
to sort out the different cars buses and
things and it provides a better
detection in complicated environments
they're very complicated up there and
then we have uh another example connect
and let's take a little closer look at
connect connect they use a random Forest
as part of the game console and what it
does is it tracks the body movements and
it recreates it in the game and let's
see what that looks like like uh we have
a user who performs a step in this case
it looks like Elvis Presley going there
that is then recorded So the connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have uh prints going on this one
from Elvis presle to Prince it's great
uh so it marks user base on the accuracy
if we look at that a little closer we
have a training set to identify body
parts where are the hands where are the
feet uh what's going on with the body
that then goes into a random Forest
classifier that learns from it once we
train the
classifier then identifies the body
parts while the person's dancing it's
able to represent that in a computer
format and then based on that it scores
the game and how accurate you are as
being Elvis Presley or prince in your
dancing so here we wrap up AI course for
beginners if you have any queries feel
free to post them in the comment section
below if you like this video C
subscribing to Simply learn and hit the
Bell icon to never miss any updates from
us thanks for watching staying ahead in
your career requires continuous learning
and upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing Cy cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and and click here to watch
similar videos to ner up and get
certified click
here