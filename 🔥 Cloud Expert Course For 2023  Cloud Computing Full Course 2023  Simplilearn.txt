hello everyone I am mayank and welcome
to this amazing video on cloud experts
course by simply learn in this video we
will clear your doubt regarding the best
roles salaries and certification you can
choose to enter Advanced your career in
cloud computing but before we begin if
you enjoyed these type of videos and
find them interesting then subscribe to
our YouTube channel as we bring you the
best video daily also hit the Bell icon
to never miss any update from Simply
love so let's get started
in this video first you will learn about
what cloud computing is and the types of
cloud computing following that you will
come across the fundamentals of cloud
computing like cloud computing roadmap
cloud computing certification how to
become Cloud company expert and cloud
computing job roles then you will be
introduced to various Cloud Computing
Services providing platform like AWS
Azure and gcp first you will learn about
AWS and its various interesting Services
following that you will learn about
Azure and how it differs from AWS then
you will learn about gcp and its
provided services and how it completes
with other two leading cloud computing
platform AWS and Azure in the end you
will see top 30 cloud computing
interview question that will help you
build your career in cloud computing and
see you you can proceed to become a
cloud computing engineer which will
cover all the relatable data like
eligibility political idea
qualifications and skills required for
you to proceed in this path by this end
of this video I can assure you that all
your cloud computing related questions
and outs will be have been cleared and
whether you are a fresher or an Art
Expert with some experience you can
improve your skills in cloud computing
by getting the proper training and
certification on that note we present
postgraduate program in cloud computing
by simply love the postgraduate program
in cloud computing design collaboration
with Caltech ctme helps you become an
Azure AWS and gcp expert this in-depth
cloud computing certification course
lets you master key architecture
principle and develop the skills needed
to become a cloud committing expert
benefits of this postgraduate program
include Caltech ctma postgraduate
certificate enrollment and simple and
services receive up to 30 CEUs from
caltex Atma simply learn job assist
helps you to get noticed by top hiring
companies attend master classes from
caltex ctme live virtual classes led by
industry expert Hands-On project and
integrated Labs online convocation by
Celtic ctme program director 40 plus
hands-on experience projects and
integrated Labs caption projections four
domains caltex ctm Circle membership you
can also
certification and its in-depth course
curriculum which covers the key concept
of azure AWS and gcp platforms and
services such as AWS Lambda Amazon S3
Azure app services and many more this
programs also cover the essential skills
to become an expert in cloud computing
and you can consider these programs
because it provides you with the
certification from caltex atme a
well-reputated university in United
States so check out the link mentioned
in the description box below so without
any further Ado let's get started what
is cloud computing as we learn what is
cloud computing we will also be learning
about how things were before cloud
computing and benefits of cloud
computing different types of cloud
computing available and some of the
famous companies that are using cloud
computing and the getting benefited out
of it we're going to learn all that
before cloud computing existed if we
need any it servers or application let's
say a basic web server it does not come
easy now here is an owner of a business
and I know you would have guessed it
already that he's running a successful
business by looking at the hot and fresh
brewed coffee in his desk and lots and
lots of paperwork to review and approve
now he had a smart not only smart
looking but a really smart worker in his
office called Mark and on one fine day
he called Mark and said that he would
like to do business online in other
words he would like to take his business
online and for that he needed his own
website as the first thing and Mark puts
all his knowledge together and comes up
with this requirement that his boss
would need lots of servers database and
softwares to get his business online
which means a lot of investment and Mark
also adds that his boss will need to
invest on acquiring technical expertise
to manage the hardware and software that
they will be purchasing and also to
monitor the infrastructure and I've
after hearing all this his boss was
close to dropping his plan to go online
but before he made a decision he chose
to check if there are any alternatives
where he don't have to spend a lot of
money and don't have to spend acquiring
technical expertise now that's when Mark
opened this discussion with his boss and
he explained his boss about cloud
computing and he explained his boss the
same thing that I'm going to explain to
you in some time now about what is cloud
computing what is cloud computing cloud
computing is the use of a network of
remote servers hosted on the internet to
store manage and process data rather
than having all that locally and using
local server for that cloud computing is
also storing our data in the internet
from anywhere and accessing our data
from anywhere throughout the internet
and the companies that offer those
services are called Cloud providers
cloud computing is also being able to
deploy and manage our applications
services and network throughout the
globe and manage them through the web
management or configuration portal in
other words cloud computing service
providers give us the ability to manage
our applications and services through a
Global Network or Internet example of
such providers are Amazon web servers
and Microsoft Azure now that we have
known what cloud computing is let's talk
about the benefits of cloud computing
now I need to tell you the cloud
benefits is what is driving Cloud
adoption like anything in the recent
days if you want an IT resource or a
service now with Cloud it's available
for me almost instantaneously and it's
ready for production almost the same
time now this reduces the go live date
and the product and the service hit the
market almost instantaneously compared
to the Legacy environment and because of
this the companies have started to
generate Revenue almost the next day if
not the same day planning and buying the
right size Hardware has always been a
challenge in Legacy environment and if
you're not careful when doing this we
might need to live with the hardware
that's undecised for the rest of our
lives with Cloud we do not buy any
hardware but we use the hardware and pay
for the time we use it if that Hardware
does not fit our requirement release it
and start using a better configuration
and pay only for the time you use that
new and better configuration in Legacy
environments forecasting demand is an
full-time job but with Cloud you can let
the monitoring and automation tool to
work for you and to rapidly scale up and
down the resources based on the need of
that R not only that the resources
Services data can be accessed from
anywhere as long as we are connected to
the internet and even there are tools
and techniques now available which will
let you to work offline and will sync
whenever the internet is available
making sure the data is stored in
durable storage and in a secure fashion
is the talk of the business and Cloud
answers that million dollar question
with Cloud the data can be stored in a
highly durable storage and replicated to
multiple regions if you want and the
data that we store is encrypted and
secured in a fashion that's beyond what
we can imagine in local data centers now
let's bleed into the discussion about
the types of cloud computing very lately
there are multiple ways to categorize
cloud computing because it's ever
growing now we have more categories out
of all these six sort of stand out you
know categorizing cloud based on
deployments and categorizing cloud based
on services and again under deployments
categorizing them based on how they have
been implemented you know is it private
is it public or is it hybrid and again
categorizing them based on the service
it provides is it infrastructure as a
service or is it platform as a service
or is it software as a service let's
look at them one by one let's talk about
the different types of cloud based on
the deployment models first in public
Cloud everything is stored and accessed
in and through the internet and any
internet users with proper permissions
can be given access to some of the
applications and resources and in public
Cloud we literally own nothing beat the
hardware or software everything is
managed by the provider AWS Azure and
Google are some examples of public Cloud
private Cloud on the other hand with
private Cloud the infrastructure is
exclusively for and single organization
the organizations can choose to run
their own cloud locally or choose to
Outsource it to a public cloud provider
as managed services and when this is
done in the service the infrastructure
will be maintained on a private Network
some examples are VMware cloud and some
of the AWS products are very good
example for private Cloud hybrid cloud
has taken thanks to the whole new level
with hybrid Cloud we get the benefit of
both public and private Cloud
organizations will choose to keep some
of their applications locally and some
of the application will be present in
the cloud one good example is NASA it
uses hybrid Cloud it uses private Cloud
to store sensitive data and uses public
Cloud to store and share data which are
not sensitive or confidential let's now
discuss about cloud based on service
model
the first and the broader category is
infrastructure as a service here we
would rent the service network storage
and will pay for them in an hourly basis
but we will have access to the resources
we provision and for some we will have
root level access as well ec2 in AWS is
a very good example it's a VM for which
we have root level access to the OS and
admin access to the hardware the next
type of service model would be platform
as a service now in this model the
providers will give me a pre-built
platform where we can deploy our codes
and our applications and they will be up
and running we only need to manage the
codes and not the infrastructure here in
software as a service the cloud
providers sell the end product which is
a software or an application and we
directly buy the software on a
subscription basis it's not the infra or
the platform but the end product or the
software or a functioning application
and we pay for the hours we use the
software and in here the client
maintains full control of the software
and does not maintain any equipment
Amazon and Azure also sell products that
are software as service this chart sort
of explains the difference between the
four models starting from on-premises to
infrastructure as a service to platform
as a service to software as a service
this is self-explanatory that the
resource managed by us are huge in
on-premises that towards your left as
you watch and it's little less in
infrastructure as a service as we move
further towards the right and further
reduced in platform as a service and
there's really nothing to manage when it
comes to software as a service because
we buy this software not any
infrastructure component attached to it
there are lots of famous well-known
organizations around the world they have
moved or have migrated to the cloud
environment 20 rest is one company which
is one of the world's largest visual
book marketing tool with more than 70
million monthly active users now
Pinterest was able to scale its business
because it was all built on Amazon web
services with a company with a less
number of employees Pinterest decided
that they do not want a dedicated staff
time to manage a data center instead
Pinterest uses AWS or they used AWS to
manage their high performance social
application and store more than 8
billion objects and 400 terabytes of
data in AWS Cloud using a service called
Amazon simple storage service or in
short Amazon S3 and not only that they
are running 2 lakh 25 000 instance hours
a month and they run that on Amazon
compute service called Amazon ec2 or
Amazon elastic compute Cloud Spotify is
an another company that got benefited
using AWS it's an online music service
offering and it offers instant access to
over 16 million licensed songs Spotify
needed a peculiar requirement it needed
a storage solution that could scale very
quickly without incurring long lead
times for upgrades long story short
Spotify used Amazon S3 which is Amazon
simple storage service and Amazon S3
gave them the confidence in their
ability and to expand their storage
quickly while also providing High data
durability the third biggest company I
want to mention is Netflix it's the
world's largest internet television
network with more than 100 million
members in more than 190 countries
Netflix uses Amazon web services for
nearly all its Computing and storage
needs now that includes database
analytic recommendation Engines video
transcoding and a lot more not only that
this company is planning to use AWS
Lambda to build an automated or an
rule-based self-managing infrastructure
and replace some of their inefficient
process to reduce the rate of errors and
replace inefficient processes to reduce
the rate of errors and save their
valuable time Expedia is another famous
company that got benefited by using
Cloud Expedia provides travel booking
service through its Flagship site called
expedia.com and not only that they also
run 200 plus travel booking sites around
the world Xperia is all on AWS and by
using AWS Expedia has become more
resilient with an high scalable
infrastructure and better cloud services
because it had offloaded the it
management to the cloud provider itself
in this case AWS and because of this
Xperia's developers have been able to
innovate faster while saving the company
millions and millions of dollars we can
categorize the different types of cloud
computing based on two wide categories
one being a deployment model and the
other one being service model let's talk
about the deployment model first
deployment model is categorized into
three types first one is public and then
private and then hybrid cloud in other
words public Cloud private cloud and
hybrid Cloud it'll be easy for me to
explain and also it will be easy for you
to understand if I walk you through this
example consider the different types of
vehicles we use to commute from one
place to another for example if I want
to travel I can pick a bus which is
accessible to anyone I get in and I pay
for the seat that I occupy and I pay for
the time that I will be traveling in it
and I'm done cost is very less here a
similar kind of thing happens in the
public Cloud I pay only for the resource
that I use and I pay for how long I use
it if I use less I pay less if I use
more I pay more for that month simple on
the other hand private Cloud was like
buying your own car and using it for
commuting purpose here I pay a huge
amount on up front and it is all owned
only by me I do not pay for it in an
hourly fashion but completely and all
upfront the cost here is very huge and
thirdly if I want the best of both types
like the comfort of the own car and
still don't want to pay all upfront
otherwise one only to pay for the time
that I use the service I can rent a car
similarly I can have it in a hybrid
environment meaning if I already have a
DC I can integrate it with the cloud and
use both the DCS and that would become
an hybrid environment all right so that
was good off in learning let's summarize
the types of cloud based on deployment
models and as we know now about the
public Cloud public cloud is in Cloud
infrastructure that's made available to
the general public over the internet and
it is owned by the cloud provider some
of the major players as Cloud providers
are AWS Microsoft Azure IBM's blue cloud
and Sun Cloud and Prime private Cloud
now this Cloud infrastructure is
exclusively operated by a single
organization it can be managed by
organizations or third party and may
exist on-premises or off-premises
doesn't matter but the point here is
this is exclusively operated for a
single organization and some companies
that provide private Cloud are AWS and
VMware and hybrid Cloud gives the best
of both public and the private Cloud for
example the federal agencies they opt
for private clouds for storing and
developing personal data and they use
public Cloud to share the non-sensitive
data with the general public or with
other government departments now let's
talk about different clouds based on the
service model if we need to categorize
them broadly we can categorize them as
infrastructure as a service IAS or
platform as a service paas or software
as a service SAS they sometimes are
referred to as is
as now at this moment you could be like
this guy thinking Sam I thought you're
done categorizing the cloud now you're
going to talk about three more
categories which one should I pick well
let me explain if all that you want is
just in VM and you have all the
expertise to install the software on top
of it and make it work then go for is if
you only want a platform or an
interfaced program or an interface to
upload a program and make it run then
pick pass or if all that you want is a
finished product hosted in the cloud and
be able to access it through the
internet then go for SAS here you get a
username and password for an application
and you can begin to customize the
application based on your needs all
right let's talk about is in a bit more
detail I as it gives basic Computing
infra it's based on a pay for what you
use model and some of the cloud
providers who are big players are AWS
Azure and Google and here the users
generally will be it admins in pass the
provider gives you a platform or a
runtime environment for developing
testing and managing application it's
platform ready you buy the platform you
upload a code and you start working on
it and it allows the software developers
to deploy applications without running
the underlying infrastructure and as you
might have guessed by now the
interesting candidates who would use Paz
is software developers and in SAS
everything is managed by the vendor be
it the hardware or the software it's
managed by the vendor and we pay for the
service and we pay for it through a pay
as you go subscription model and as you
might have guessed the end users here
would be End customer itself all right
let's put together everything in the
same page and compare and contrast the
different types of service models in
this chart it explains the difference
between the four models starting from
on-premises to is and then pass and says
it is self-explanatory that the
resources managed by us are huge in
on-premises and little less in is and
further less or reduced in Paz and
nothing to manage when it comes to SAS
let me also explain the different types
of cloud services through an example
like this let's say that you have a
crush on cake and you're planning to
bake one yourself now let's look at the
options you can have you can make all
the ingredients yourself be the floor
butter and you know put together and
bake the whole thing yourself using your
own oven pan you know the needed water
and the rest you get an idea right
everything is yours and that's on
premises all that you use is owned by
you and nothing is managed by the vendor
the other options you can have is buy
the ingredients and mix and bake the
cake yourself now this would be like is
here the infra is managed by the
provider and we get to use it and
customize it the way we want it here the
cloud service is in shared
responsibility the other options you
still have on hand is simply pick a
phone and order a cake now this is a lot
simpler than the rest we discussed so
far you know it's simply picking the
phone and ordering the cake and pay for
it when it arrives simple and when it
reaches home you will have to arrange
the table garnish the cake if that's
needed and then enjoy the cake it's the
same way with pass just get the platform
in which you would run your code and
upload your code and start running your
application here you and the vendors
still share the responsibility you still
have one more option left that is simply
go out and dine this is a lot lot
simpler that it requires no effort from
us at all you buy the fully finished and
garnished cake and pay for it and walk
out no responsibility on making the cake
it's the same way with SAS we buy the
finished product and pay for the
finished application SnapChat reports
that more than 5 billion Snapchat
generated daily and more than 100
billion messages are sent on WhatsApp
deal do you ever wonder if storing this
data in local storage is possible the
answer is clearly no according to the
projections by 2025 over 100 zettabytes
of data will be stored in the cloud
storage one zettabyte is equal to
trillion gigabytes now imagine the
amount of data the world generates and
how it is being managed so Cloud experts
are responsible for this smooth
transition Google workspace has already
surpassed the mark of 2 billion users in
2020 and 60 of the total corporate data
is already present in the cloud so these
stats are enough to let anyone know
where this technology is headed and what
wonders are awaiting in cloud computing
Tech giants like Google Amazon and
Microsoft are trying to conquer this
exponentially growing Market with their
products like Amazon web services Google
Cloud platform and Microsoft Azure for
them in this fast-paced world a job in
this field is more than worth it the
salary is offered in this field are very
lucrative and fascinating among all the
job roles currently available in cloud
computing a cloud architect in India
earns an average salary of 20 lakhs per
annum and in the United States it goes
around 125 000 so these figures are
stated by glassdoor.com and show how
beneficial it is to back a job in this
sector today but the fact is nothing
comes easy so we at simply learn will
save some of your time and let you know
about the process of how to become a
cloud expert in 2023 we'll take you
through the complete cloud computing
roadmap but before we begin if you enjoy
watching these videos and find them
interesting please subscribe to our
Channel because we bring the best
also hit the Bell icon to get notified
whenever we drop a new video so without
any further delay let's go through the
agenda for today's video
this video will provide all the Vital
Information about cloud computing
we'll start this video with an
introduction to cloud computing
following that we'll take you through
the skills to become a successful Cloud
expert we'll then take you through the
steps you can take to become a cloud
engineer and finally we'll explore some
of the potential roles and growth
opportunities in the field of cloud
computing so let's start this video with
what is cloud computing
cloud computing in simple terms refers
to the practice of using remote servers
on the internet to store manage and
process data instead of relying on your
local computer it allows you to access
your files applications and services
from anywhere within active internet
connection providing convenience
scalability and flexibility for
individuals and businesses alike
a number of skills are required to
become a master in the field of cloud
computing the first one is technical
skills
Cloud Engineers should have a strong
foundation in areas such as networking
operating systems and programming
languages they should be familiar with
Cloud platforms like Amazon web services
Microsoft Azure or Google Cloud platform
problem solving abilities Cloud
Engineers often encounter complex
technical challenges developing strong
problem solving and troubleshooting
skills is crucial to identify and
resolve issues efficiently
security and compliance knowledge Cloud
Engineers must understand security
practices data privacy regulations and
compliance standards they need to
protect sensitive data and maintain
compliance with industry regulations
continuous learning cloud technology
evolves rapidly so it's essential for
cloud Engineers to have a thirst for
learning keeping up with the latest
trends attending training programs and
obtaining relevant certifications will
help you stay ahead
now that we have a grasp of the skills
required let's discuss the steps you can
take to become a cloud engineer get
familiar with Cloud Concepts
start by gaining a basic understanding
of cloud computing Concepts familiarize
yourself with the advantages different
service models such as infrastructure as
a service platform as a service and
software as a service and deployment
models like public private and hybrid
second is learn Cloud platforms and
Technologies choose a cloud platform to
specialize in such as AWS Azure or gcp
explore their documentation online
tutorials and resources to understand
their offerings services and tools gain
hands-on experience by experimenting
with the platforms and building simple
projects acquiring relevant
certifications Cloud certifications
validate QR skills and demonstrate your
expertise to potential employers
consider pursuing certifications such as
AWS certified Cloud practitioner Azure
fundamentals or Google Cloud certified
associate Cloud engineer these
certifications provide a solid
foundation and enhance your credibility
and simply learn has got your back as
always the postgraduate program in cloud
computing design in collaboration with
Caltech ctme helps you become an Azure
AWS and gcp expert at once this in-depth
cloud computing certification course
lets you master key architectural
principles and develop the skills needed
to become a complete Cloud expert
benefits of this postgraduate program
include Caltech ctme postgraduate
certificate enrollment in simple learns
job assist you will receive up to 30
CEUs from Caltech ctme
simply learns job assist help you get
noticed by top hiring companies you can
also attend master classes from Caltech
ctme instructors and live virtual
classes led by industry experts you will
also get Hands-On projects and
integrated Labs there are many more
other features of this postgraduate
program so you can also fast track your
career with this cloud computing
certification and its in-depth course
curriculum which covers the key concept
of azure AWS and gcp platforms and
services such as AWS Lambda Amazon S3
Azure app services and many more so this
program also covers the essential skills
to become an expert in cloud computing
and you can consider this program
because it provides you with a
certification from Caltech C team a
well-reputed university in the United
States so what are you thinking you can
find the link mentioned in the
description box below the next step is
gain practical experience practical XP
is crucial in the field of cloud
engineering look for opportunities to
work on real world projects through
internships freelance work or personal
projects this hands-on experience will
help you apply your knowledge and
develop problem solving skills the last
step to become a cloud expert is
networking and collaboration connect
with Professionals in the cloud
computing industry through networking
events online forums and social media
platforms engage in discussions ask
questions and learn from their
experiences building a solid
Professional Network can open doors to
new opportunities as a cloud engineer
you'll have diverse career opportunity
let's explore some of the potential
rules and growth paths in cloud
computing Cloud administrator Cloud
experts or administrators oversee and
manage a company's Cloud infrastructure
including policies security protocols
uptime monitoring and technology updates
Cloud infrastructure engineer this role
focuses on designing and managing the
underlying infrastructure of cloud
platforms ensuring scalability security
and performance Cloud Solutions
architect Cloud Solutions architect
design and Implement cloud-based
solutions for organizations they analyze
business requirements and develop
architectural designs to meet specific
needs Cloud security engineer Cloud
security engineer specializes in
securing cloud infrastructures and
services they Implement robbers security
measures monitor for vulnerabilities and
respond to security incidents the
salaries in any of these fields are
lucrative and fascinating
congratulations you have now learned the
steps to become a cloud expert remember
it's a dynamic field that requires
continuous learning and adaptation so
start your journey by building a solid
foundation acquiring relevant
certifications gaining practical
experience and networking with
professionals the cloud is much more
than just a reliable storage solution in
the year 2021 like the year before
proved to be a watershed even for cloud
computing
it gave businesses more freedom to
operate in the face of covert 19. cloud
computing according to it experts will
be at the Forefront of all Technologies
used to address key businesses concerns
in the coming years
cloud computing has generated more
anticipation excitement and investment
than any other I.T area in the last
decade according to IDC the cloud will
increase at a rate of 22 percent next
year with the market value of 277
billion dollars
as a result there will surely be a
significant demand for cloud
professionals therefore a right
certification in cloud computing can
help you earn a handsome salary of 145
000 to 227 000 in the United States and
close to 16 lakh to 24 lakh rupees in
India
so let's have a look at the top 10
certification in cloud computing
the first one is AWS certified Solutions
architect associate
AWS certified Solutions architect
associate is a set of technical
certifications offered by Amazon web
services for beginners and professionals
working in Enterprise architecture and
solutions architecture
this certificate AIDS in the
identification and development of
personnel with crucial capabilities for
cloud implementation
AWS certified solution architect
associate certification verifies your
competence to build and deploy
distributed systems on AWS
so this certification is for persons who
can do solution architecture such as
deploying and securing web applications
and who have worked with AWS services
for at least a year
and its difficulty level is easy
skills required for this certification
is understanding of specific programming
or scripting languages like Java python
C hash and many other programming
language available data storage
fundamentals networking
AWS server selection Cloud specific
patterns and Technologies
moving on here are a few certification
details you will need to proceed with
its examination
cost of examination is 150 USD
and the average annual salary is 130
883 dollars
the duration of this examination is 130
minutes with number of questions 65.
you have multiple choice and multiple
response questions and the passing score
is 70 percent
and the granule time of the
certification is two years
the company is hiring our Amazon IBM cab
Gemini mindtree and equia
these are the top companies that hire
second is AWS certified Cloud
practitioner
the AWS certified Cloud practitioner
certification is a foundational level
test designed for people who can
successfully show an overall
understanding of the AWS Cloud according
to Amazon
this exam verifies a candidate's
understanding of essential Cloud
infrastructure and Architectural
Concepts as well as key AWS services
the certification provides an overview
of AWS key Services as well as security
and Network
so this is the idle search Cloud
certification for beginners or anyone
interested in learning more about cloud
computing and the AWS Cloud platform
so the difficulty level is easy
the skills required for this
certification is
oversight on creating architecture for
AWS platform ability to deploy AWS
Solutions and applications building
Cloud Solutions
developing plans for the adoption of
cloud Solutions management and
monitoring of cloud platforms
moving on here are a few certification
details you will need to proceed with
this examination
the cost of examination is 100 USD
average annual salary is 113 932 dollars
examination duration is 90 minutes
number of questions is 65
types of questions are multiple choice
and multiple response question
passing score 70 and again the renewable
time for the certification is two years
and the famous company hiring our Amazon
net app essential Deloitte and solial
LLC
moving on third is AWS certified
developer associate
AWS certified developer associate is an
Amazon web service examination that
measures a person's ability to
successfully demonstrate deploying and
maintaining applications on the AWS
platform
this is the ideal Cloud certification
for programmers and software developers
interested in building Cloud native
applications and the difficulty level is
intermediate
skills required for this certifications
are
use the AWS Services API CLI and
software development kits to write
applications
identify key features of AWS Services
understand the AWS shared responsibility
model user continuous integration of
continuous delivery pipeline to deploy
application on AWS and use and interact
with AWS services
so here are the few certification
details
cost of examination is 150 USD
average annual salary is 130 272 dollars
exam duration is 130 minutes number of
questions is 65
type of questions multiple choice and
multiple response questions
the passing score is 70 renewable time
is two years
and the company hiring at Amazon IBM
capgemini TCS and Oracle
fourth is AWS certified
system operation administrator associate
the AWS certified
sysops administrator associate
certification is designed to demonstrate
technical skills for system
administrators in Cloud operations jobs
this certificate assists organizations
in identifying and developing personal
with crucial abilities for cloud
implementation
the ability to develop manage and
operate workloads on AWS is demonstrated
by earning the certification
this associate certification is for
professionals who have at least one year
of experience with AWS deployment
management and operations
it teaches you how to identify the right
service for your needs and the
difficulty level is intermediate
so these are the skills required for the
certification understanding of AWS
tenants hands-on experience with the AWS
CLI and sdks tools understanding of
Network Technologies understanding of
security concept with hands-on
experience in implementing security
controls and compliance requirements
understanding of virtualization
technology
moving on here are a few important
details related to this certification
so cost of examination is 150 USD
average annual salary is 111
966 dollars
the duration of examination is 130
minutes and the number of questions is
65
which are multiple choice and multiple
response questions
with passing score of 70 and renewable
time of two years
and here are the companies hiring
Amazon IBM capgemini TCS and Oracle
fifth is Microsoft certified Azure
fundamentals
Azure fundamental certifications allows
you to demonstrate your understanding of
cloud Concepts Azure Services Azure
workloads Azure security and privacy and
Azure pricing and support
the Azure Foundation certification
intended for people who already have a
basic understanding of cloud services
it will explain you Cloud Concepts as
well as how to use them and the
difficulty level is easy
skills required for this certification
is the different cloud computing
Concepts
mean Azure Services describing Azure
security private privacy compliance and
Trust
Azure pricing and support
and here are a few certification details
required for the certification
cost of examination is 99 USD which can
vary
average annual salary is one hundred ten
thousand dollars
the duration of this examination is 85
minutes
and the number of questions varies
between 40 to 60. and the type of
questions are multiple choice and
multiple response questions
the passing score for this examination
is 70 and the renewable time is 6 months
and the top companies hiring are
Microsoft Dell Essentia and cognizant
sixth is Microsoft certified Azure
administrator associate
and as your administrator is in charge
of deploying monitoring and managing
Microsoft Azure Solutions which includes
significant compute
storage Network and Security Services
we're earning these this certification
you will be able to demonstrate that you
can deploy and manage Azure compute
resources
this certificate will prepare you to
develop administer and monitor cloud
services such as Storage security and
virtual environments among other things
and the difficulty level is intermediate
so the skills needed for this
certification is
main Azure services
describing Azure security private and
compliance and Trust
knowledge of azure pricing and support
a knowledge of different cloud computing
Concepts
and here are the further important
details needed for the certification
cost of examination is 165 USD average
annual salary is 107 6083 dollars
exam duration is 120 Minutes with number
of questions
varying between 40 to 60.
and the type of questions are multiple
choice and multiple response questions
with the passing score of 70 percent
renewal time is 6 months and the top
companies hiring are Microsoft Accenture
TCS and Yahoo
seventh is Google associate Cloud
engineer
an associate Cloud engineer delivers and
protects applications
and infrastructure oversees various
projects operations and maintains
corporate solution to ensure that the
fulfill performance goals
this person has worked with both public
clouds and on-premises systems
for programmers developers and software
Engineers this is the greatest Google
Cloud certification its Holder will be
responsible for deploying web
applications in the cloud as well as
monitoring and managing operations and
the difficulty level is intermediate
here are the some important skills
required for you to excel this
certification basic understanding of
Google Cloud platforms products and
services and basic understanding of
cloud Concepts such as virtual machines
containers and networking
and these are the few important
certification details that you need to
take care of cost of examination is 125
USD with average annual salary of 109
415 dollars
examination is 120 Minutes with number
of questions 50. and the type of
questions are multiple choice and
multiple response questions with a
passing score of 70 percent
and the renewable time is two years and
the top company is hiring a Google
Goldman Sachs
here comes the eight one that is
Microsoft certified Azure solution
architecture expert
to implement Solutions and Azure
solution architect work
with Cloud administrators
cloud dbas and clients these this
credential displays the ability to
provide advice to stakeholders and
translate business requirements into
safe scalable and dependable Solutions
this certificate is for experienced
programmers developers and devops
Engineers who wish to become Azure
professionals
this is the ideal Cloud certification
and the difficulty level is hard
and these are the skills required for
this Advanced certification
deploying and configuring infrastructure
implementing workloads and security
creating and deploying apps implementing
authentication and securing data
developing for the cloud and for this
Azure storage
and these are the details you need to
remember cost of examination 165 USD
average annual salary
135
000 with exam duration of 150 minutes
and number of questions varies between
40 to 60.
type of questions are multiple choice
and multiple response questions with the
passing score of 70 percent
and the renewable time is six months
and the company is hiring are IBM
Microsoft Infosys and Medline
coming to the ninth one is Google
professional Cloud architect
a Google Certified professional Cloud
architect means you demonstrate the
ability to design and plan A Cloud
solution architecture
manage and provision the Cloud solution
infrastructure
designed for security and compliance
the professional Cloud architect exam
measures your abilities to design and
plan A Cloud solution architecture
manage and provision Cloud solution
infrastructure and design for security
and compliance
this is the idle Google Cloud
certification for experienced it
professionals interested in becoming
solution architects for Google Cloud
technology such as bigtable bigquery and
other GCB platform services and the
difficulty level is hard
and these are the skills you need to
focus on
proficiency with command command line
Linux operating system and system
operations
and three plus years of Industry
experience
and here are the further details you
need to take care of
cost of examination is 200 USD with
average annual salary of 140 000 dollars
the examination duration is 120 Minutes
with number of questions 40. the type of
questions are multiple choice and
multiple response questions with a
passing score of 70 percent
and the renewable time is two years
and the company is hiring are Google and
golden sets
and here comes the last but not the
least 10th certification that is AWS
architect professional
the AWS certified Solutions architect
professional exam verifies Advanced
technical knowledge and experience in
designing distributed applications and
systems on the Amazon web services
platform
this certification AIDS in the
identification and development of
personnel with crucial capabilities for
cloud implementation
the AWS certified Solutions architect
Prudential verifies your ability to
develop Implement and evaluate
applications on AWS under a variety of
conditions
this Advanced certification teaches you
how to create and deploy scalable web
applications on Amazon AWS servers as
well as how to choose the right service
and power for your application and the
difficulty level is hard
and the skills required for this
certification is
familiarity with AWS CLI AWS apis AWS
cloud formation templates
and windows and Linux environments
ability to provide best practice
guidance on the architectural design
across multiple applications and
projects of the Enterprise
ability to evaluate Cloud application
requirements and ability to design a
hybrid architecture using key AWS
Technologies
and here are the details you need to
know before applying for the exam
cost of examination is 300 USD with
average annual salary of 118 266 dollars
the examination duration is 180 minutes
with number of questions 75.
types of questions are multiple choice
and multiple response questions
with the passing score of 70 percent
and the renewal time is 2 years
and the top companies hiring are Amazon
Central IBM and mindtree
so this was all about top 10 Cloud
certification in 2022 and whether you
are a fresher or an ID expert with some
experience you can improve your skills
in cloud computing by getting the proper
training and certification on that note
we present postgraduate program in cloud
computing by simply love the
postgraduate program in cloud computing
design collaboration with Caltech ctme
helps you become an Azure AWS and gcp
expert this in-depth cloud computing
certification course lets you master key
architecture principle and develop the
skills needed to become a cloud
commanding expert benefits of this
postgraduate program include caltex CDMA
post graduate certificate enrollment and
simply and services receive up to 30
CEUs from caltex Atma simply learn job
assist help you to get noticed by top
hiring companies attend master classes
from caltex atme live virtual classes
led by industry expert Hands-On project
and integrated Labs online conversation
by Celtic ctme program director 40 plus
hands-on experience projects and
integrated labs caption projections four
domains caltex ctme Circle membership
you can also fast check your career with
this class Computing certification and
its in-depth course curriculum which
covers the key concept of azure AWS and
gcp platform and services such as AWS
Lambda Amazon S3 Azure app services and
many more these programs also cover the
essential skills to become an expert in
cloud computing and you can consider
these programs because it provides you
with the certification from caltex atme
a well-reputated university in United
States so check out the link mentioned
in the description box below so without
any further Ado let's get started be
that of a cloud developer security
engineer a full stack developer sysops
administrator Solutions architect Cloud
architect and so much more now let's
have a look at some of the major cloud
computing roles first off we have
Solutions architect now these are
individuals who are responsible for
analyzing the technical environment in
which they are going to produce the
solutions the requirements and the
specifications secondly they are
required to select an appropriate
technology that satisfies set
requirements they need to estimate and
manage the usage and the operational
costs of the solutions they provide and
they need to support project management
as well as solution development next we
have sysops administrators they are
involved in deploying managing and
operating highly scalable and fault
tolerant systems they need to select an
appropriate service based on compute
security or data requirements they need
to estimate and manage usage and
operational costs and they need to be
able to migrate on-premises workloads
onto an appropriate cloud computing
platform so among both of these roles
there are certain requirements that are
remaining constant now let's have a look
at the steps you need to take to become
a cloud computing engineer your first
step is to gain Proficiency in a cloud
computing platform now the first step is
to become proficient in at least one of
the three major cloud computing
platforms be it AWS Azure or the Google
Cloud platform now there are a huge
number of resources that you can find on
the internet it could be YouTube videos
articles virtual or physical classrooms
and so much more now after you're done
learning you can get certified by
Microsoft Azure AWS or the Google Cloud
platform now for AWS you have a number
of different certifications which can be
divided into three categories which are
the foundational which is just the
basics the associate level
certifications the professional level
certifications and the specialty
certifications similarly with Microsoft
Azure you have certifications that
enable you to become an Azure developer
associate and as your administrator
associate and Azure architect
professional and a devops engineer now
most cloud computing platforms have a
free tier that you can take advantage of
these provide a number of free services
for a period of time some of which are
free forever so you can use these
platforms to your advantage and do as
much practice as you can on them now if
you want to learn more about cloud
computing you can also check out Simply
learns YouTube channel model then you
can go on to the playlist section right
here and you can find comprehensive
videos on a number of different cloud
computing platforms AWS and Microsoft
Azure our AWS tutorial videos talk about
what exactly is AWS how you can become
an AWS Solutions architect Amazon ec2 S3
some of the other services and so much
more we also have detailed tutorials on
Azure which talks about what exactly is
azure the certifications provided by
Azure some of the services like machine
learning as your active directory and so
much more and now we're at Step 2 being
experienced in at least one programming
language unlike general purpose
programming languages like C C plus plus
C sharp and so on cloud computing
requires ones that are a lot more data
oriented now some of the major
programming languages that are used in
cloud computing are go python closure
and Java now as I said before there is a
wealth of resources that you can learn
from there are free websites that you
can practice your hold on like quick
code code academy and several others
there's also resources like YouTube
videos as well as there's the option of
online or offline classes now we're at
step 3 specialization you'll also need
to be well versed with a number of key
Concepts these are storage and
networking now with storage you need to
know how data can be stored and where it
can be accessed from you need to know
how it can be accessed from multiple
different resources you'll also need to
have some experience with the services
provided by Azure and AWS like the
Amazon S3 in AWS and the appropriately
named Azure storage from Microsoft Azure
with networking you need to have a
strong understanding of the networking
fundamentals as well as virtual networks
next up we have virtualization and
operating systems with virtualization
you need to know how virtual networks
which is just a combination of different
virtual machines can be used to emulate
different components in a particular
system with operating systems you need
to have a very strong standing operating
systems like Windows and Linux next up
we have security and Disaster Recovery
now you need to understand how data
application as well as infrastructure
can be protected from malicious attacks
With Disaster Recovery you need to be
prepared for any unexpected circumstance
by making sure your systems are always
safe and are regularly backed up to
prevent any sort of loss of data then we
have web services and devops now you
need to have a strong understanding of
apis or application program interfaces
and web services some amount of
experience with web design also can be
of great help with devops you need to
have a strong understanding of how cloud
computing is able to provide a
centralized platform on which you can
perform testing deployment and
production for devops automation
moreover with devops you understand the
Synergy that the operations as well as
the development teams have with each
other and for the success of any project
and finally you're a cloud computing
engineer now let's have a a look at the
salaries of cloud computing engineers in
the United States cloud computing
Engineers earn around 116 000 dollars
per annum in India a cloud computing
engineer is paid approximately 6 lakh 66
000 rupees per annum now how can simply
learn help you become a cloud computing
engineer so let's head on to simplion's
website here we have the cloud architect
Masters program now this deals with a
number of different courses all of which
that can help you get started in your
journey to becoming a cloud computing
engineer this master's program covers a
number of different courses like AWS
technical Essentials Microsoft Azure
fundamentals AWS developer associate and
so much more it provides you 40 plus in
demand skills and 25 plus Services
provides you a master certification it
has 16 plus real life projects and helps
you get a salary that ranges between 15
to 25 lakh rupees per annum it also
covers a variety of tools like Amazon
ec2 Azure data Factory virtual machines
and so much more so why don't you head
on to simplylearn.com and get started on
your journey to getting certified and
getting ahead let's have a look at some
of the skills required to become a cloud
administrator first off you need
experience with programming languages
like C sharpen.net you'll also need to
be experienced with devops tools like
Jenkins Docker ansible and Chef you'll
need to have an understanding of
database configuration so that you can
take advantage of all the relevant
information about the hardware and
software components used by the
organization you'll also need to have
experience with Cloud infrastructure
systems like servers storage Network and
visualization software finally you'll
also need to configure virtual machines
vpns and Cloud servers now let's have a
look at the salary offer to Cloud
administrators now in the United States
the average salary of a cloud
administrator is approximately 65 000
per annum similarly in India it's
approximately 7 lakh rupees per annum
now let's have a look at some of the
companies hiring for cloud
administrators we have companies like
Wipro Infosys Accenture IBM and so on so
here I see a comment which says I'm a
fresher and want to learn about AWS how
can I start so actually any fresher can
make a career in AWS as long as you have
interest towards the subject now if you
are from a non-technical background you
need to put in a lot more hard work and
have a lot of patience so I would
recommend that before you take up the
AWS course you have a look at the course
curriculum types of AWS courses
opportunities in the market for AWS
certified candidates and so on so now
let's go to number six Cloud application
developer now a cloud application
developer mainly focuses on implementing
and maintaining an organization's Cloud
infrastructure now they're involved with
designing building creating analyzing
and maintaining Cloud systems now these
individuals are also involved with
ensuring that there's an effective
design of business processes in the
cloud they're also involved with a
number of different tasks like
optimizing efficiency and performance
scaling application components and
security issues now let's have a look at
some of the skills required to become a
cloud application developer first off
you need to have experience with
database languages like MySQL SQL and
mongodb then you need to have experience
with programming languages like python
Ruby and Perl so that you can code and
create cloud computing applications
you'll also need to have experience with
Linux now this is because most Cloud
infrastructures are created with Linux
servers you'll also need to have
experience with cloud service providers
like Amazon web services Google Cloud
platform and Microsoft zero you'll need
to have experience with information
security with the help of certifications
you'll also need to know how to create
microservices and to create Cloud
applications now let's have a look at
the salary offered to a cloud
application developer in the United
States it's approximately seventy
thousand dollars per annum in India the
average salary of a cloud application
developer is approximately 8 lakh rupees
per annum now let's have a look at some
of the companies hiring Cloud
application developers you have
companies like Bosch McAfee sap and so
on so now let's go to number five Cloud
network engineer a cloud network
engineer is responsible for implementing
supporting maintaining and optimizing
the network Hardware software and
communication Links of the
organization's Cloud infrastructure he
or she basically focuses on Automation
and security while building the cloud
infrastructure enhancing Network tooling
visibility and improving productivity
now let's have a look at some of the
skills required to become a cloud
network engineer first off obviously you
need to have experience with networking
which means the familiarity with the
internet and Van communication
Technologies protocols and best
practices you need to have an
understanding of cloud security and how
you can design a public cloud with
multiple options you'll need to have an
experience with data center
Administration which basically means an
understanding on infrastructure design
operations and life cycle management and
finally you need to have experience with
cloud computing platforms like Amazon
web services Microsoft Azure and Google
Cloud platform next let's have a look at
the salaries offered to a cloud network
engineer firstly the average salary for
cloud network engineer in the United
States is approximately seventy two
thousand dollars per annum in India the
average salary of a cloud network
engineer is approximately 4 lakh rupees
per annum now that we're done with this
let's have a look at the company's
hiring Cloud Network Engineers we have
companies like Amazon web services
Rakuten Cisco and so much more now for
number four we have Cloud automation
engineer a cloud automation engineer
focuses on cloud Automation
orchestration and integration he or she
implements optimizes and supports the
cloud infrastructure and ensures it has
high availability they also have to
increase the cost Effectiveness and
availability of the cloud now let's have
a look at some of the skills required to
become a cloud automation engineer first
off since the role is cloud-based
experience with the cloud service
platform like Microsoft Azure AWS or
Google Cloud platform is an absolute
must you'll also need to have experience
with programming languages like Python
and go that will help you make the
process of automation easier next
experience with devops tools like chef
ansible and puppet are seen as a huge
plus you'll also need to have an
understanding of Docker and containers
experience with databases like postgres
SQL and MySQL that can handle multiple
workloads would be very useful and
finally you need to have experience with
virtualization which means you need to
know how you can work with virtual
servers applications storage and
networks with this you can reduce the
amount of resources required by the
organization now let's have a look at
the salary offered to Cloud automation
engineers in the United States the
average salary for cloud automation
engineer is approximately 79 000 per
annum the average salary for cloud
automation engineer in India is
approximately 5 lakh rupees per annum
now that we're done let's have a look at
some of the companies hiring Cloud
automation Engineers we have companies
like Oracle Google Kappa Gemini Disney
and so on and now to number three
Cloud security manager a cloud security
manager is someone who's responsible for
providing security for cloud-based
digital platforms and protecting the
organization's data he or she may be
involved with creating new security
methods or to analyze existing ones they
may also have to create cloud-based
applications performing threat
simulations and providing security
recommendations now let's have a look at
some of the skills required to become a
cloud security manager first off you
need to have experience with at least
one of the popular cloud service
providers like Amazon web services
Microsoft Azure and gcp you'll also need
to have experience in programming
languages like python experience with
commonly used devops tools like Jenkin
so that you can Implement continuous
integration or continuous deployment
models would be very helpful you'll also
need to be well versed with TCP
protocols and other networking Concepts
you need to be well versed with pki SSL
SSH https and so on experience with
web-based Analytics and services would
also be very helpful now let's have a
look at the salary offered to a cloud
security manager in the United States
the average salary of a cloud security
manager is approximately 99 000 per
annum in India the average salary for
cloud security manager is approximately
11 lakh rupees per annum now let's have
a look at some of the companies hiring
Cloud security managers we have
companies like Deloitte VMware Cisco
Dell and so on now let's move on to
number two Cloud engineer a cloud
engineer is an individual who is
responsible for any technological duties
that are associated with cloud computing
including design planning management
maintenance and support they're also
involved with orchestrating and
automating cloud-based platforms
throughout the organization let's have a
look at the skills required to become a
cloud engineer first off you'll need to
have experience with at least one of the
popular cloud service providers like AWS
Microsoft Azure or the Google Cloud
platform you you'll need to have an
understanding of networking Concepts
like building and accessing servers
virtual networks and so on this enables
Cloud Engineers to make sure that the
network is responsive to the users next
you need to have experience with
virtualization with virtualization you
can reduce how many Hardware units you
need with the help of virtual machines
it also makes resources scalable and
fault tolerant in an organization
experience with the Linux operating
system would also be very helpful
considering how thirty percent of the
servers that powers your are Linux based
you'll need to know about apis to design
restful Services experience with devops
is also a major skill to have since
you'll be able to help handle work
dependencies between the development and
operation teams and finally you need to
have programming experience with
languages like Java python C plus plus
and Ruby next let's have a look at the
salary offered to a cloud engineer in
the United States the average salary of
a cloud engineer is approximately 100
five thousand dollars per annum the
average salary in India is approximately
five lakh rupees per annum now let's
have a look at some of the companies
hiring Cloud Engineers we have companies
like McAfee IBM JPMorgan visa and so on
now I see a pretty good question in the
chat how do you become a cloud engineer
so to become a cloud engineer you need
to have experience with the programming
language experience with at least one
cloud service platform which is Amazon
web services or Microsoft Azure or
Google Cloud platform the choice is
yours and you need to specialize in a
particular service be it storage
networking disaster recovery and so on
and now for number one Cloud architect a
cloud architect is responsible for
managing cloud computing architecture in
an organization he or she handles
everything to do with front-end
platforms servers storage delivery and
networks now let's have a look at some
of the skills required to become a cloud
architect first off you need to have
knowledge in programming knowledge about
Perl python Ruby PHP Java and net can
provide users with the ability to build
deploy and manage applications quickly
you'll need to know about the basics of
networking since most of the work what
you'll be doing would be web related
knowledge about networking can be really
helpful knowledge about data storage
fundamentals are also really important
this provides you with the knowledge to
determine which data storage option to
use and when being experienced with
cloud service platforms is also really
important platforms like AWS Google
Cloud platform and Microsoft Azure are
what you are going to be basing most of
your work on experience with them is
pretty important and finally Cloud
security this will help you make sure
that your data is secure and only
authorized code is run and the right
people are allowed to run it now let's
talk about the salary of a cloud
architect the average salary for cloud
architect is approximately 107 thousand
dollars per and in India the average
salary of a cloud architect is
approximately 16 lakh rupees per annum
and there you go those are the top job
roles in the field of cloud computing
now let's have a look at some of the
companies hiring Cloud Architects we
have companies like eyepro Huawei
Hewlett Packard Enterprise and so on
hello everyone let me introduce myself
as Sam a multi-platform cloud architect
and trainer and I'm so glad and I'm
equally excited to talk and walk you
through the session about what AWS is
and talk to you about some services and
offerings and about how companies get
benefited by migrating their
applications and infra into AWS so
what's AWS let's talk about that now
before that let's talk about how life
was without any cloud provider and in
this case how life was without AWS so
let's walk back and picture how things
were back in 2000 which is not so long
ago but a lot of changes a lot of
changes for better had happened since
that time now back in 2000 a request for
a new server is not an happy thing at
all because a lot of money a lot of
validations a lot of planning are
involved in getting a server online or
up and running and even after we finally
got the server it's not all said and
done a lot of optimization that needs to
be done on that server to make it worth
it and get a good return on investment
from that server and even after we have
optimized for a good return on
investment the work is still not done
there will often be a frequent increase
and decrease in the capacity and you
know even news about our website getting
popular and getting more hits It's still
an Bittersweet experience because now I
need to add more servers to the
environment which means that it's going
to cost me even more but thanks to the
present day Cloud technology if the same
situation were to happen today okay my
new server it's almost ready and it's
ready instantaneously and with the Swift
tools and technologies that Amazon is
providing uh in provisioning my server
instantaneously and adding any type of
workload on top of it and making my
storage and server secure you know
creating a durable storage where data
that I stored in the cloud never gets
lost with all that features Amazon has
got our back so let's talk about what is
AWS there are a lot of definitions for
it but I'm going to put together a
simple and a precise definition as much
as possible now let me iron that out
Cloud still runs on an Hardware all
right and there are certain features in
that infrastructure in that cloud
infrastructure that makes cloud cloud or
that makes AWS a cloud provider now we
get all the services all the
Technologies all the features and all
the benefits that we get in our local
data center like you know security and
compute capacity and databases and in
fact you know we get even more cool
features like content caching in various
global locations around the planet but
again out of all the features the best
part is that I get or we get everything
on a pay as we go model the less I use
the less I pay and the more I use the
less I pay per unit very attractive
isn't it right and that's not all the
applications that we provision in AWS
are very reliable because they run on a
reliable infrastructure and it's very
scalable because it runs on an on-demand
infrastructure and it's very flexible
because of the designs and because of
the design options available for me in
the cloud let's talk about how all this
happened AWS was launched in 2002 after
the Amazon we know as the online retail
store wanted to sell their remaining or
unused infrastructure as a service or as
an offering for customers to buy and use
it from them you know sell
infrastructure as a service the idea
sort of clicked and AWS launched their
first product first product in 2006
that's like four years after the idea
launch and in 2012 they held a big sized
customer even together inputs and
concerns from customers and they were
very dedicated in making those requests
happen and that habit is still being
followed it's still being followed as
reinvent by AWS and at 2015 Amazon
announced its Revenue to be 4.6 billion
and in 2015 through 2016 AWS launched
products and services that helped
migrate customer services into AWS well
their web products even before back this
is when a lot of focus was given on a
developing migrate trading services and
in the same year that's in 2016 Amazon's
revenue was 10 billion and not but not
the least as we speak Amazon has more
than 100 products and services available
for customers and get benefited from all
right let's talk about the services that
are available in Amazon let's start with
this product called S3 now S3 is a great
tool for internet backup and it's it's
the cheapest storage option in the
object storage category and not only
that the data that we put in S3 is
retrievable from the internet S3 is
really cool and we have other products
like migration and data collection and
data transfer products and here we can
not only collect data seamlessly but
also in a real-time way monitor the data
or analyze the data that's being
received that they're cool products like
AWS data transfers available that helps
achieve that and then we have products
like ec2 elastic compute Cloud that's an
resizable computer where we can anytime
anytime after the size of the computer
based on the need or based on the
forecast then we have SIMPLE
notification Services systems and tools
available in Amazon to update us with
notifications through email or through
SMS now anything anything can be sent
through email or through SMS if you use
that service it could be alarms or it
could be service notifications if you
want stuff like that and then we have
some security tools like KMS key
management system which uses AES 256-bit
encryption to encrypt our data at rest
then we have Lambda as service for which
we pay only for the time in seconds
seconds it takes to execute our code and
we're not paying for the infrastructure
here it's just the seconds that the
program is going to take to execute the
code for the short program will be
paying in a milliseconds if it's a bit
bigger program program will be probably
paying in 60 seconds or 120 seconds but
that's a lot cheap lot simple and lots
cost effective as against paying for
service on an hourly basis which a lot
of other services are well that's cheap
but using Lambda is a lot cheaper than
that and then we have services like
Route 53 at DNS service in the cloud and
now I do not have to maintain and DNS
account somewhere else and my cloud
environment with AWS I can get both in
the same place all right let me talk to
you about how AWS makes life easier or
how companies got benefited by using AWS
as their ID provider for their
applications or for the infrastructure
now Unilever is a company and they had a
problem right and they had a problem and
they picked AWS as a solution to their
problem all right now this company was
sort of spread across 190 countries and
they were relying on a lot of digital
marketing for promoting their products
and their existing environment their
legacy local environment proved not to
support their changing I.T demands and
uh they could not standardize their old
environment now they chose to move part
of their applications to awoliers
because they were not getting what they
wanted in their local environment and
since then you know rollouts were easy
provisioning your applications became
easy and even provisioning
infrastructure became easy and they were
able to do all that in push button
scaling and needless to talk about uh
backups that are safe and backups that
can be securely accessed from the cloud
as needed now that company is growing
along with AWS because of their Swift
speed in rolling out deployments and
being able to access secure backups from
various places and generate reports and
in fact useful reports out of it that
helps their business now on the same
lines let me also talk to you about
Kellogg's and how they got benefited by
using Amazon now Kellogg's had a
different problem it's one of its kind
now their business model was very
dependent on an infra that will help to
analyze data really fast right because
they were running promotions based on
the analyzed data that they get so
they're being able to respond to the
analyzed data as soon as possible was
critical or vital in their environment
and luckily sap running on Hana
environment is what they needed and you
know they picked that service in the
cloud and that sort of solved the
problem now the company does not have to
deal with maintaining their legacy infra
and maintaining their heavy compute
capacity and maintaining their a
database locally all that is now moved
to the cloud or they are using Cloud as
their it service provider and and now
they have a greater and Powerful it
environment that very much complements
their business coming to the AWS
services so essentially AWS services are
compute storage database migration
networking and content delivery
developer tools management tools Media
Services machine learning analytics
security identity and compliance mobile
services
application integration AR and VR which
is augmented reality and virtual reality
customer engagement business
productivity desktop and app streaming
internet of things which is iot
now let's look into
the compute service which is one of the
widely used service on the AWS and what
does the compute service do these
Services help developers build deploy
and scale an application in the cloud
platform that is ec2
Lambda elastic container service elastic
load balancer light sail and elastic
bean stock these are the services which
lie in the compute service section only
one of the most widely used service
within the compute section is ec2 which
stands for elastic Cloud compute it is a
web service that allows developer to
rent virtual machines and helped to
resize the compute capacity
so here what you can do is you can run
the virtual machines and you have the
Privileges to select the type of
operating system that you want uh should
be running on your instances or on your
virtual machines and likewise later you
can customize as per your requirement
Lambda is a serverless compute service
it is also responsible to execute code
for a specific application so those who
are from the development background they
can focus more on creation of a code
they don't have to create a server
manage it instead they can use a Lambda
where you they can deploy their code
directly onto the Lambda server then
comes this storage service now couple of
storage services are S3 Glacier EBS
which is elastic block Storage storage
Gateway
now AWS provides web data storage
service for archiving data also its main
advantage is disaster data recovery with
high durability let's look into some of
the essential storage services and one
of the most widely used storage which is
S3 which stands for simple storage
service in the simple storage service
what you need to do is you have to
create a bucket and in that bucket you
have to put the files in it so S3 gives
open cloud-based storage service which
is utilized for online data backup then
comes the ABS which is an elastic block
storage now you can understand EBS as a
virtual hard drive also which attaches
with the ec2 and it provides High
availability storage volume for
persistent data it is mainly used by
Amazon ec2 instances then you have the
database Services AWS database domain
service offers cost efficient highly
secure and scalable database instances
in the cloud and some of the database
services are RDS which is a relational
database service
dynamodb the non-relational or nosql or
nosql database service elastic cache and
Amazon redshift now one of the essential
database service is the dynamodb it is a
flexible no SQL database service which
offers fast and reliable performance
with no scalability issues
it is fast reliable highly scalable and
suitable for small scale applications
like mobile applications gaming
applications or anything with respect to
the Internet of Things devices there the
dynamodb is most widely used or suitable
then comes the relational database
service that is the RDS which is a
structured database service
it is a managed distributed relational
database cloud service that helps
developers to operate and scale database
in a simple manner so RDS has different
vendors platform with respect to the
database usage and that includes the
postgresql MySQL then you have Oracle
Microsoft or Ms SQL and they have their
own
customize
database as well that is called as the
Amazon Aurora
along with that they have a Maria
database or mariadb as well so these are
couple of vendors that give their
database engines that you can use on the
RDS now coming to the networking
Services it offers a highly secure Cloud
platform and helps in connecting your
physical Network to your private virtual
network with high transfer speed
now some of the services in the
networking and content delivery are VPC
which is a virtual private Cloud a very
important service in order to make your
applications or Services more secure
Route 53 which is a DNS mapping Service
Direct Connect which directly connects
with the AWS services and with your data
centers
and the cloud front that is basically a
Content delivery service now coming to
the VPC or a virtual private Cloud it
helps a developer to deploy AWS
resources such as Amazon ec2 instances
in a private virtual Cloud so that you
can actually make your ec2 isolated or
make it more secure and even you can
make it for a public access also depends
on the administrator that how they want
to customize it so the complete control
of a VPC and its networking is with the
admins then comes the Route 53 service
it is a web service with highly
available domain name system or the DNS
that helps user to Route software by
translating text into IP address and
that is why it is called as a DNS
mapping service and that helps you to
use your domains or the external domains
pointed to the AWS services in case if
you use AWS for hosting your websites or
the applications note DNS translates
text into the IP address now coming to
the developer tool services it helps a
user build deploy and run an application
source code automatically it also
updates the server and instance on the
workload so first is the code star code
build code deploy code pipeline so code
star it is a service designed to manage
application development at a single
place
here developers can quickly develop
build and deploy applications on AWS
so all the manage app development can be
done with the code star code build
removes the hassle of managing physical
servers and helps developer build and
test code with continuous scaling so
security identity and compliance
services
helps in monitoring a safe environment
for your AWS Resources by providing
limited access to specific users so in
case let's assume that you have to give
an access to someone but with limited
privileges you can primarily use IIM in
that case and if you want to make your
applications or
deployments more secure then you can use
these services like KMS IM Cognito
Waf which acts as a firewall now the IM
service which is the identity access
management is a framework that helps in
maintaining access to AWS services in a
secure way so what happens is that the
admin who has the complete access of the
AWS console provide access to users and
there can be different users and they
would have the privileged accesses
defined by the Admin so what type of
permissions the admin gives them they
would have those limited access on the
AWS console KMS enables users to create
and manage the encryption keys that are
used for encrypting data coming to
management tool services with the help
of management tools using the service an
individual can optimize cost minimize
risk and automate all the resources
running on the AWS infrastructure
efficiently so with the management tools
you can
monitor the resources application it's
its tools and the utilizations and along
with that you can scale up scale down
the resources likewise with the help of
the management tools you can also do the
auditing task so one of the essential
services in the management tool services
the cloud watch
it is a monitoring tool for AWS
resources and customer applications
running on AWS platform so let's assume
that you have used ec2 dynamodb S3 RDS
and you want to monitor those resources
you can use the cloud watch that can
give you the results coming to the cloud
formation
this service helps you in monitoring all
your AWS resources at one place so that
you can spend minimum time in managing
those resources and maximum time on
developing the application so with the
help of the cloud formation you can
deploy the entire solution with the help
of creation of a template you just need
to create one template and you have to
deploy it the rest of the things will be
done by the AWS and hence it is a kind
of an automation task only now let's
look into the demo of some of the
essential services
so we'll start with the ec2 and I have
already logged in into my AWS account so
where exactly you can find the ec2 just
click on the services
under the compute section you can find
the ec2 ec2 stands for elastic Cloud
compute service
primarily used for creating the virtual
machines
so I'll click on the service and quickly
I'll show you how the virtual machine is
created and how we can basically access
it
so I would be creating one virtual
machine or an instance with the Windows
operating system
so here you can see there are three
instances which are already running let
me create another one
we have to click on launch instances
and here you have to select some
configuration details
so most of the configuration details
I'll be taking as default
and wherever it is necessary I'll be
making the changes
so first of all you have to select the
operating system in the form of Ami and
I would be looking for Windows
2016 and then you have to select the
type of instance with respect to
the CPUs number of CPUs and the memory
capacity so I'll go with T2 dot micro
which is a free tier eligible instance
in the configure instance details the
rest of the things we'll keep it as
default as of now
click on add storage
this is a basically
the virtual hard drive or the EBS the
elastic block storage that is attached
with the ec2 instance
so primarily it is giving us 30 GB of
space without any additional cost
we can leave the tags as blank
click on configure security groups and
here you can see that the RDP Port is by
default open
which will actually allow us to RDP or
to have a remote connectivity of our
instance
click on review and launch
now to give an authentication or to
provide an authentication we should have
a key pair with us so that the AWS can
understand or can tally the key pair and
give us the access
so I already have a key pair created
what I'll do is I'll create a new key
pair for this instance and put a random
name let's say I put it something like
demo
download the key pair
and make sure that you keep your key
pair in a safe and secure place
click on launch instances
now you will see that there is an
instance ID that has been created which
is a number alphanumeric number that is
randomly given by the AWS
you can name your instances
so let's assume let's say we name our
instance as
windows 2016.
click on Save
and
the AWS deploys our instance on a
respective infrastructure
it gives us the IP addresses the public
IP and the private IP which is an
internal IP and we have to access the
instance from the public IP only so now
to access that instance we have to open
up the RDP
put the public IP
click on connect
and when it asks you for the
Authentication
you put a username as administrator
and the password you have to generate by
providing the keep so click on connect
and then you have to click on RDP client
here you have to get a password so click
on get password
browse and when you click on browse you
have to provide demo dot pem file so it
should be in the download section here
itself
just provide that key pair and it is
going to give you the password in the
encrypted format decrypt your password
copy that
and then provide those details
to the RDP client click on OK
and now it should allow you to login to
the instance let's wait for the windows
to appear so here you can see that we
have
logged in into the windows instance the
windows 2016 screen is available in
front of us if getting your learning
started is half the battle what if you
could do that for free visit scale up by
simply learn click on the link in the
description to know more
demonstration I would be explaining you
about
how to use a storage service
specifically the S3 service which is
most widely used service under the
storage section
now the S3 stands for simple storage
service primarily used for storing the
objects and the files
and what we need to do is we just have
to click on the S3 service under the
storage section
so when you click on S3 service you have
to create the buckets inside the S3
and the buckets are the places where you
keep your folders or you upload the
files that are available on your systems
so
here if you'll notice that the S3
service is a global Service that means
it is irrespective of the region
and the buckets when you create they are
created in a specific region now what is
the benefit
the benefit is that if you create a
bucket in different regions all those
buckets in different regions can be
viewed from a single dashboard
and you don't have to change the region
again and again to view the S3 buckets
so what you need to do is you have to
click on create bucket
and here you have to specify the name of
the bucket so let's say I put something
like demo AWS
and uh
that's the bucket name I'm going to
select now make sure that the bucket
name starts with the lower case and it
should be always unique now why it
should be unique because since S3 is a
global Service so it may be that
somebody else could be using your
the name that you have provided so it
should be always unique otherwise it
will not be allocated
now you have to select a region and that
would be
let's say I go with
the Ohio region
and in the bucket settings
you can basically change and configure
these settings according to your
requirement by default when you create a
bucket
it blocks all the public access from the
bucket so when you have all public
access blocked
no object can be viewed from the S3
Bucket from a public network or from or
by anyone else
and hence in order to
view the objects or the other files in
the bucket you have to unblock or
uncheck this option so that first of all
you make the bucket accessible from the
public network likewise you can
customize as per the requirement and
then click on acknowledgment the rest of
the things will keep it as default and
click on create bucket now it says that
the bucket with the same name already
exists so that means it is not a unique
name somebody else might have been using
this name so I'll try to keep it more
unique and I'll try to assign some
number so let's say I put something like
987
and it says it is already existed let's
say 9876
and then click on create bucket now if
it creates a bucket that means we have
been allocated with that bucket name so
let's wait for
wait for a minute to get that bucket
created
so that happens quickly and you can see
there are a couple of buckets already
created here and these buckets can be
viewed in a single dashboard
so some of the buckets are in Mumbai
region some of the buckets are in Ohio
region but they are available in the
same or a single dashboard so I'll open
up the bucket that I have recently
created and I'll try to upload some
documents now what you can do is you can
create a folder also inside the S3 and
when you create a folder you can upload
the objects accordingly otherwise you
can directly also upload the objects so
click on upload and we will upload any
random file from a system onto the sket
so I'll basically file
and then click on upload
now it is uploading my object from my
system onto the S3 bucket
and now it has successfully done so it
says the messages successful now in
order to validate I will click on the
bucket from the S3 service and view this
object
now you can see here is my bucket so
I'll just open up this bucket and these
dnsrecords.csv file is available with us
now the S3 is not only limited till
storing the objects or the files it has
many other functionalities and the
features also like you can enable
versioning you can host a static website
on the S3
along with that you can have a cross
region replication enabled so that you
can have a high availability of your
objects or you can have a redundancy of
your critical objects in different
regions so likewise there are more
features that would be covered up in the
details section of an S3 service now
coming to another
section of the essential services that
is the database
now here you can see I have a database
section in the AWS and it has multiple
service within
the RDS is there dynamodb is their
elastication the other database services
are there
so I would be showing a demo on the
dynamodbeam which is a no SQL database
now
when we say it is a nosql database that
means it is a non-relational database
service where we can create a database
table directly from the web console we
don't require a separate database engine
like in the case of RDS
and in the tables you can insert the
values and view those values directly
from the AWS dashboard itself
so it says Amazon dynomial DB is a fast
and flexible nosql database service
primarily suitable for iot and web
gaming and other mobile applications so
what you need to do is it is a
straightforward uh database service
which is which can be accessed
while creating a table itself and it's a
compute based database service that is
the reason that it is more fast
so click on create table
and what you need to do is you just have
to put a table name so I'll just put
something like test
and uh in the partition key so these
partition keys are unique entities so
what you need to do is you have to
specify a partition key so I'll put
something like ID
and the string instead of string I'll
use a number
likewise I can add sort Keys Also let's
say I put a name
and the name should be in the string
format
now these are the unique entries these
are the fixed entries in the table and
after that we are going to put the
attributes and in the attributes the
data will be inserted so what you need
to do is rest of the things will keep it
as default as of now click on create
and
here you can see the table has been
created the test table has been created
now if I click on the items so you would
see that it has the sort Keys available
the ID and the sort key associated with
that that is the name but it does not
have any entry because we have not added
any value or the attributes so how we
can add or insert the values in this
table that can be done many ways
you can enter manually you can use the
help of CLI to enter the large chunk of
data directly upload it onto the
dynamodb table and also you can use the
apis also in order to insert the data
inside the table so what you will do is
we will click on create item
and in the ID we will put some value
let's say number one string let's say we
put something like
we will go with a random name so instead
of putting any name we'll put a value
like ABCD that should be fine and then
we will insert some attributes
so let's say I put
string as Rank and I'll put something
like rank 2.
and
then click on Save
so here you will see
just refresh the database table
close it and open it up again
click on the items
now here you can see the id1 name is
ABCD got the rank two likewise you can
click on create items let's say the
serial number or the id2 name let's say
we put something like we can go with XYZ
any random thing put it up as
number rank
and let's say this particular value got
a rank 3
likewise you can add some more items
string let's say you put something like
hgk
and
you put a rank let's say it's a rank
number one holder rank one holder
right so likewise uh this is just an
example likewise you can add the
attributes as per your need and the
table can be filtered out based on the
attributes also so if you have to search
some values in the dynamodb table so you
can always use filters uh to basically
search the values inside the table apart
from that the dynamodb table has lot of
other functionalities
it can be basically you can have a
backup
of a table created in the dynamodb table
in the Dynamo database and then you can
retrieve or recover the data by
restoring the backups from the dynamodb
this database can be created in the
cluster format also
so these are couple of features that you
can use with the dynamodb
now let's move into the next section
now coming to the networking services so
there are
some of the networking Services which
are very useful and that includes a VPC
which is a virtual private cloud cloud
front root 53 API Gateway Etc
now I will basically demonstrate about
the Route 53 service in this demo so in
the root 53 is basically a DNS mapping
service so what you can do is let's
assume that you are hosting any web
application on the server and you want
to Route the domain traffic onto those
servers you need to have the help of
Route 53 to do that
so what you need to do is you just have
to click on Route 53 service
and from this service you can register
your domains also otherwise if you have
domains purchased from any external site
you can point them to the Route 53 name
servers also so first of all
in the Route 53 you have to create a
hosted zone so there is already one
hosted Zone all created
now when you create a hosted Zone you
have to specify the domain name
so let me show you I have one dummy
domain
what's the domain that has been defined
so what you have to do is when you click
on create hosted Zone you have to put a
domain here you can see example.com
likewise you have to put your own domain
and uh
click on the public hosted Zone
click on create hosted Zone and it is
going to give you four name server now
those four name servers have to be
updated
on
the platform from where you have
purchased the domain
so that is mandatory in order to Route
the traffic to the Route 53 service
right so here you can see I already have
a hosted Zone created for a domain
and it has given me
four name servers
these are the name servers and these
name servers have been updated
in a record set
from where the domain has been purchased
right
once it is done then you have to
route the domain traffic to a server so
what you have to do is you have to click
on create record
and in the create record you have to
specify
the IP address or you can use the Alias
also where the traffic should be routed
to where your application is hosted at
so ideally it is a server details
and it exists with certain
policies also
so you can see some view existing
records so here you can see
this particular domain
is routed to a DNS value which is hosted
in the elastic Beanstalk instead of that
you can put an IP address of the ec2
instance also you can use the S3 URL
also you can use the cloudfront URL also
so likewise what will happen is that the
domain traffic will be routed to the
server where actually the application or
a web application is hosted ad
now I am using a routing policy as
simple routing policy that means all the
traffic should be routed to that
particular domain
whereas there are other routing policies
also in the Route 53 that includes a
weighted routing policy which acts as a
kind of a load balancing geolocation
routing policy multi-value answer
routing policy and then you have a
redundancy based routing policy which is
a failover one so likewise you can
select as per your requirement
so what you need to do is you just have
to click on create records uh you have
to specify you have the domain so you
have to specify any uh particular info
you want to put before the domain
otherwise you can leave it as blank and
in the record type let's assume that you
are using an IP address of a server
where your web application is hosted at
so you can basically use a routes
traffic to an ipv for address and put an
IP address make sure that you put a
public IP address or the elastic IP
address attached to your instance in
case if you are not using any particular
IP address or the URL you can use the
Alias also so likewise the records can
be created along with that the Route 53
Services used for domain verifications
also like if you want the email services
on onto your web application then you
can basically verify your domain
directly from the root 53 service you
can get the verification done for the
SSL certificate creation for that also
the records will be directly created
from the root 53 service because it is
actually managing or hosting your domain
now with respect to the security
services
the most widely used service is the IM
which is identity access management
that lies under security identity and
compliance
so in from the IM you can create the
user's uh whom you want to give an
access to your AWS console you can
create groups and you can add multiple
users in that group give them the
permissions you can create the rules
also so that multiple Services can
interact or integrate it together
so how the IM is used we click on the IM
service
and I'll show you how a particular user
can be created and how the user is
basically uses the credentials to access
the AWS console now the IM dashboard is
open and when you open up the IM
dashboard it gives you the URL so this
is the URL through which the user has to
actually
access the AWS console by providing the
user credentials now how the users are
created just click on the users
and
here you just have to click on add user
so I'll create one sample user let's say
I put something like sample user
and what type of an access you want to
give that particular user do you want to
give a programmatic access which is
primarily as a CLI access or you want to
give the AWS Management console so right
now we'll go with the AWS Management
console access
now do you want an auto generation of
the password or do you want to customize
the password so let's assume that we
customize the password so put any value
make sure the password meets all the
criterias
and uh
then click on next permissions now what
kind of permissions you want to give to
that user let's assume that you want to
give an access of a particular service
only to that user so you have to
actually search you have to actually
search a policy for that particular
service that can be given an access to a
user otherwise if you want to give an
admin policy or the admin access to that
user you can search an admin policy
there so the permissions are important
otherwise the user would not have
privileges to access
the AWS console
so
we'll attach the existing policies
directly so click on it
and here let's assume that I want to
give an admin access to that user so
I'll just click on the admin access now
to a particular user you can give
multiple policies also it is not
necessary that you have to give only a
single policy
so you can provide a multiple policies
to the users also now click on next tags
let let's make the tags blank click on
review and create a user
now you'll see that our sample user
would be created and to access the
console we'll just copy this URL which
has the account information as well that
is the account number
so copy the link or you can copy the
complete URL log out from the root
account
and then
paste the URL that was copied from the
console
and then we have to give the user
credentials
so we'll put sample user as the username
and the password that we provided while
creating the user
click on sign in and if it is correct
then it should allow us to login into
the AWS console while asking the
password change
so we'll change the password
confirm password change
and now it should allow us to login into
the root accounts AWS console with the
admin privileges
now the next service is the monitoring
services and under the monitoring
Services primarily the cloud watch the
cloud trail the cloud formation these
are some of the services that are most
widely used
now the cloud watch is a service which
is primarily used for monitoring the
metrics primarily of the servers like
for example if you create an ec2
instance and you want to basically
watch out for the
metrics associated with the CPU
utilization or the storage utilization
the network and network out all those
information you can get it from the
cloud watch
now cloudwatch is not only
related to monitoring the metrics it can
generate the alarms also and here you
can get the events also generated which
or the events that can be created which
can trigger the Lambda function as well
so what you have to do is you just have
to
click on a dashboard so it's entering
metrics are viewed
with the help of the cloud watch so
first of all you have to create a
dashboard
now you have to put a dashboard name
let's say I put something like
monitoring
ec2
click on create dashboard now how do you
want the reports to be published you
have to actually select a widget I want
that okay the report should be visible
in the numeric form so I'll select a
number
now click on the dashboard that was
created and uh
so again add the widget
and here uh we'll select the metrics
primarily for the ec2
for instance metric so we have one
single instance running now there are 14
metrics available for that particular
instance now I would be looking for the
CPU utilization for this particular
instance so I'll just select for the CPU
utilization and it is going to give me
some information about what is the
current CPU utilization of that
particular instance so that is somewhere
around 29.3 percent
uh the CPU utilization has been done for
the single instance that is running in
my ec2 dashboard so likewise you can add
some more metrics and view them in this
particular dashboard and the cloud watch
will keep on publishing the data at a
refresh interval of five minutes that is
uh the default value and whether you are
a fresher or an IIT expert with some
experience you can improve your skills
in cloud computing by getting the proper
training and certification on that note
we present postgraduate program in cloud
computing by simply love the
postgraduate program in cloud computing
design collaboration with caldexctme
helps you become an Azure AWS and gcp
expert this in-depth cloud computing
certification course lets you master key
architecture principle and develop the
skills needed to become a cloud
commanding expert benefits of this
postgraduate program include caltex CDMA
postgraduate certificate enrollment and
simple and services receive up to 30
CEUs from caltex atme Simply learn job
assist helps you to get noticed by top
hiring companies attend master classes
from Caltech ctme live virtual classes
led by industry expert Hands-On project
and integrated Labs online convocation
by Celtic ctme program director 40 plus
hands-on experience projects and
integrated Labs caption projections four
domains caltex ctm Circle membership you
can also fast check your career with
this class Computing certification and
its in-depth course curriculum which
covers the key concept of azure AWS and
gcp platform and services such as AWS
Lambda Amazon S3 Azure app services and
many more these programs also cover the
essential skills to become an expert in
cloud computing and you can consider
these programs because it provides you
with the certification from caltex atme
a well-reputated university in United
States so check out the link mentioned
in the description box below so without
any further Ado let's get started hi
there I'm Samuel a multi-platform cloud
architect and I'm very excited and
honored to walk you through it's
learning series about AWS let me start
this session with this scenario let's
imagine how life would have been without
Spotify for those who are hearing about
Spotify for the first time a Spotify is
an online music service offering and it
offers instant access to over 16 million
licensed songs Spotify now uses AWS
Cloud to store the data and share it
with their customers but prior to AWS
they had some issues imagine using
spotify before AWS let's talk about that
back then users were often getting
errors because Spotify could not keep up
with the increased demand for storage
every new day and that led to users
getting upset and users canceling the
subscription the problem Spotify was
facing at that time was their users were
present globally and were accessing it
from everywhere and they had different
latency in their applications and
Spotify had a demanding situation where
they need to frequently catalog the
songs released yesterday today and in
the future and this was changing every
new day and the songs coming in rate was
about 20 000 a day and back then they
could not keep up with this requirement
and needless to say they were badly
looking for a way to solve this problem
and that's when they got introduced to
AWS and it was a perfect fit and match
for their problem AWS offered a
dynamically increasing storage and
that's what they needed AWS also offered
tools and techniques like storage life
cycle management and trusted advisor to
properly utilize the resource so we
always get the best out of the resource
used AWS addressed their concerns about
easily being able to scale yes you can
scale the AWS environment very easily
how easily one might ask it's just a few
button clicks and AWS sold spotify's
problem let's talk about how it can help
you with your organization's problem
let's talk about what is AWS first and
then let's bleed into how AWS became so
successful and the different types of
services that AWS provides and what's
the future of cloud and AWS in specific
let's talk about that and finally we'll
talk about a use case where you will see
how easy it is to create a web
application with AWS all right let's
talk about what is AWS AWS or Amazon web
services is a secure cloud service
platform it is also pay as you go type
billing model where there is no upfront
or Capital costs we'll talk about how
soon the service will be available well
the service will be available in a
matter of seconds with AWS you can also
do identity and access management that
is authenticating and authorizing a user
or a program on the Fly and almost all
the services are available on demand and
most of them are available
instantaneously and as we speak Amazon
offers 100 plus services and this list
is growing every new week now that would
make you wonder how AWS became so
successful of course it's their
customers let's talk about the list of
well-known companies that has their ID
environment in AWS Adobe Adobe uses AWS
to provide multi-terabyte operating
environments for its customers by
integrating its system with AWS Cloud
Adobe can focus on deploying and
operating its own software instead of
trying to you know deploy and manage the
infrastructure Airbnb is another company
it's an Community Marketplace that
allows property owners and travelers to
connect each other for the purpose of
renting unique vacation spaces around
the world and the Airbnb Community users
activities are conducted on the website
and through iPhones and Android
applications Airbnb has a huge
infrastructure in AWS and they are
almost using all the services in AWS and
are getting benefited from it another
example would be Autodesk Autodesk
develops software for engineering
designing and entertainment Industries
using services like Amazon ideas or
rational database service and Amazon S3
or Amazon simple storage service
Autodesk can focus on deploying or
developing its machine learning tools
instead of spending that time on
managing the infrastructure AOL or
American online uses AWS and using AWS
they have been able to close data
centers and decommission about 14 000
in-house and co-located servers and move
Mission critical workload to the cloud
and extend its Global reach and save
millions of dollars on energy resources
a bitdefender is an internet security
software firm and their portfolio of
softwares include antivirus and
anti-spyware products bitdefender uses
ec2 and they're currently running few
hundred instances that handle about 5
terabytes of data and they also use
elastic load balancer to load balance
the connection coming in to those
instances across availability zones and
they provide seamless Global delivery of
service because of that the BMW group it
uses aw use for its new connected Car
application that collects sensor data
from BMW 7 Series cars to give drivers
dynamically updated map information
Canon's offers Imaging products division
benefits from faster deployment times
lower cost and Global reach by using AWS
to deliver cloud-based services such as
mobile print the office Imaging products
division uses AWS such as Amazon S3 and
Amazon Route 53 Amazon cloudfront and
Amazon IM for their testing development
and Production Services Comcast it's the
world's largest cable company and the
leading provider of internet service in
the United States Comcast uses AWS in a
hybrid environment out of all the other
Cloud providers Comcast chores AWS for
its flexibility and scalable hybrid
infrastructure Docker is a company
that's helping redefine the way
developers build ship and run
applications this company focuses on
making use of containers for this
purpose and in AWS the service called
the Amazon ec2 container service is
helping them achieve it the esa or
European Space Agency although much of
esa's work is done by satellites some of
the programs data storage and Computing
infrastructure is built on Amazon web
services Esa shows AWS because of its
economical pay as you go system as well
as its quick startup time the Guardian
newspaper uses AWS and it uses a wide
range of AWS services including Amazon
Kinesis Amazon redshift that power and
analytic dashboard which editors used to
see how stories are trending in real
time Financial Times FD is one of the
world's largest leading business news
organization and they used Amazon
redshift to perform their analysis A
Funny Thing Happened Amazon Redshirt
performs so quickly that some analysis
thought it was malfunctioning they were
used to running queries overnight and
they found that the results were indeed
correct just as much faster by using
Amazon redshift FD is supporting the
same business functions with costs that
are 80 percentage lower than what was
before general electric GE is at the
moment as we speak migrating more than
9000 workloads including 300 desperate
Erp systems to AWS while reducing its
data center footprint from 34 to 4 over
the next three years similarly Howard
Medical School HTC IMDb McDonald's NASA
Kellogg's and lot more are using the
services Amazon provides and are getting
benefited from it and this huge success
and customer portfolio is just the tip
of the iceberg and if we think why so
many adapt AWS and if we let AWS answer
that question this is what AWS would say
people are adapting AWS because of the
security and durability of the data an
end-to-end privacy and encryption of the
data and storage experience we can also
rely on AWS way of doing things by using
the AWS tools and techniques and
suggested best practices built upon the
years of experience it has gained
flexibility there is a greater
flexibility in AWS that allows us to
select the OS language and database easy
to use swiftness in deploying we can
host our applications quickly in AWS be
it a new application or migrating an
existing application into AWS
scalability the application can be
easily scaled up or scaled down
depending on the user requirement cost
saving we only pay for the compute power
storage and other resources you use and
that too without any long-term
commitments now let's talk about the
different types of services that AWS
provides the services that we talk about
fall in any of the following categories
you see like you know compute storage
database Security customer engagement
desktop and streaming machine learning
developers tools stuff like that and if
you do not see the service that you're
looking for it's probably is because AWS
is creating it as we speak now let's
look at some of them that are very
commonly used within compute Services we
have Amazon ec2 Amazon elastic Beanstalk
Amazon light sale and Amazon Lambda
Amazon ec2 provides compute capacity in
the cloud now this capacity is secure
and it is resizable based on the user's
requirement now look at this the
requirement for the web traffic keeps
changing and behind the scenes in the
cloud ec2 can expand its environment to
three instances and during no load it
can shrink its environment to just one
resource elastic Beanstalk it helps us
to scale and deploy web applications and
it's made with a number of programming
languages elastic Beanstalk is also an
easy to use service for deploying and
scaling web applications and services
deployed a bit in java.net PHP node.js
python Ruby Docker and lot other
familiar services such as Apache
passenger and IIs we can simply upload
our code and allows Beanstalk
automatically handles the deployment
from capacity provisioning to load
balancing to Auto scaling to application
Health monitoring and Amazon light sale
is a virtual private server which is
easy to launch and easy to manage Amazon
light cell is the easiest way to get
started with AWS for developers who just
need a virtual private server light cell
includes everything you need to launch
your project quickly on a virtual
machine like SSD based storage a virtual
machine tools for data transfer DNS
management and a static IP and that too
for a very low and predictable price AWS
Lambda has taken Cloud Computing
Services to a whole new level it allows
us to pay only for the compute time no
need for provisioning and managing
servers an AWS Lambda is a compute
service that lets us run code without
provisioning or managing service Lambda
executes your code only when needed and
scales automatically from few requests
per day to thousands per second you pay
only for the compute time you consume
there is no charge when you core is not
running let's look at some storage
services that Amazon provides like
Amazon S3 Amazon Glacier Amazon abs and
Amazon elastic file system Amazon S3 is
an object storage that can store and
retrieve data from anywhere websites
mobile apps iot sensors and so on can
easily use Amazon S3 to store and
retrieve data it's an object storage
built to store and to try any amount of
data from anywhere with its features
like flexibility and managing data and
the durability it provides and the
security that it provides Amazon simple
storage service or S3 is a storage for
the internet and Glacier Glacier is a
cloud storage service that's used for
archiving data and long-term backups and
this Glacier is an secure durable and
extremely low cost cloud storage service
for data archiving and long-term backups
Amazon EVS Amazon elastic blog store
provides Block store volumes for the
instances of ec2 and this elastic Block
store is highly available and a reliable
storage volume that can be attached to
any running instance that is in the same
availability Zone ABS volumes that are
attached to the ec2 instances are
exposed as storage volumes that persist
and independently from the lifetime of
the instance and Amazon elastic file
system or EFS provides an elastic file
storage which can be used with AWS cloud
service and resources that are on
premises and Amazon elastic file system
it's an simple it's scalable it's an
elastic file storage for use with Amazon
cloud services and for on-premises
resources it's easy to use and offers a
simple interface that allows you to
create and configure file systems
quickly and easily Amazon file system is
built to elastically scale on demand
without disturbing the application
growing and shrinking automatically as
you add and remove files to your
application have the storage they need
and when they need it now let's talk
about databases the two major database
flavors are Amazon RDS and Amazon
redshift Amazon RDS it really eases the
process involved in setting up operating
and scaling a rational database in the
cloud Amazon RDS provides cost efficient
and resizable capacity while automating
time consuming administrative tasks such
as Hardware provisioning database setup
patching and backups it sort of frees us
from managing the hardware and sort of
helps us to focus on the application
it's also cost effective and resizable
and it's also optimized for memory
performance and input and output
operations not only that it also
automates most of the services like
taking backups you know monitoring stuff
like that it automates most of those
Services Amazon redshift Amazon redshift
is a data warehousing service that
enables users to analyze the data using
SQL and other business intelligent tools
Amazon redshift is an fast and fully
managed data warehouse that makes it
simple and cost effective and lays all
your data using standard SQL and your
existing business intelligent tools it
also allows you to run complex analysis
queries against petabyte of structured
data using sophisticated query
optimizations and most of the results
they generally come back in seconds all
right let's quickly talk about some more
services that AWS offers there are a lot
more services that AWS provides but are
we going to look at some more services
that are widely used AWS application
Discovery Services help Enterprise
customers plan migration projects by
gathering information about their
on-premises data centers in a planning a
data center migration can involve
thousands of workloads they are often
deeply interdependent server utilization
data and dependency mapping are
important early first step in migration
process and this AWS application
Discovery service collects and presents
configuration usage and behavior data
from your servers to help you better
understand your workloads Route 53 it's
a network and content delivery service
it's an highly available and scalable
Cloud domain name system or DNS service
and Amazon Route 53 is fully compliant
with IPv6 as well elastic load balancing
it's also a network and content delivery
service elastic load balancing
automatically distributes incoming
application traffic across multiple
targets such as Amazon ec2 instance
containers and IP addresses it can
handle the varying load of your
application traffic in a single
availability zones and also across
availability zones AWS Auto scaling it
monitors your application and
automatically adjusts the capacity to
maintain steady and predictable
performance at a lowest possible cost
using AWS Auto scaling it's easy to set
up application scaling for multiple
resources across multiple services in
minutes Auto scaling can be applied to
web services and also for DB Services
AWS identity and access management it
enables you to manage access to AWS
services and resources securely using IM
you can create and manage AWS users and
groups and use permissions to allow and
deny their access to AWS resources and
moreover it's a free service now let's
talk about the future of AWS well let me
tell you something cloud is here to stay
here's what in store for AWS in the
future as years pass by we're gonna have
a variety of cloud applications born
like iot artificial intelligence
business intelligence serverless
Computing and so on cloud will also
expand into other markets like
healthcare banking space automated cars
and so on as I was mentioning some time
back lot or greater Focus will be given
to artificial intelligence and
eventually because of the flexibility
and advantage that cloud provides we're
going to see a lot of companies moving
into the cloud all right let's now talk
about how easy it is to deploy an web
application in the cloud so the scenario
here is that our users like a product
and we need to have a mechanism to
receive input from them about their
likes and dislikes and you know give
them the appropriate product as per
their need alright though the setup and
the environment it sort of looks
complicated we don't have to worry
because AWS has tools and Technologies
which can help us to achieve it now
we're going to use services like Route
53 services like Cloud watch ec2 S3 and
lot more and all these put together are
going to give an application that's
fully functionable and an application
that's going to receive the information
like using the services like Route 53
Cloud watch ec2 and S3 we're going to
create an application and that's going
to meet our need so back to our original
requirement all I want is to deploy a
web application for a product that keeps
our users updated about the happenings
and the newcomings in the market and to
fulfill this requirement here is all the
services we would need ec2 here is used
for provisioning the computational power
needed for this application and ec2 has
a vast variety of family and types that
we can pick from for the types of
workloads and also for the intents of
the workloads we're also going to use S3
for storage and S3 provides any
additional storage requirement for the
resources or any additional storage
requirement for the web applications and
we are also going to use cloudwatch for
monitoring the environment and
cloudwatch monitors the application and
the environment and it provides trigger
for scaling in and scaling out the
infrastructure and we're also going to
use Route 53 for DNS and Route 53 helps
us to register the domain name for our
web application and with all the tools
and Technologies together all of them
put together we're going to make an
application a perfect application that
caters our need all right so I'm going
to use elastic Beanstalk for this
project and the name of the application
is going to be as you see GSG sign up
and the environment name is GSG signup
environment one let me also pick a name
let me see if this name is available yes
that's available that's the domain name
so let me pick that and the application
that I have is going to run on node.js
so let me pick that platform and launch
now as you see elastic Beanstalk this is
going to launch an instance it's going
to launch the monitor ring setup or the
monitoring environment it's going to
create a load balancer as well and it's
going to take care of all the security
features needed for this application
all right look at that I was able to go
to that URL which is what we gave and
it's now having an default page shown up
meaning all the dependencies for the
software is installed and it's just
waiting for me to upload the code or in
specific the page required so let's do
that
let me upload the code I already have
the code saved here
so that's my code
and that's going to take some time all
right it has done its thing and now if I
go to the same URL look at that I'm
being thrown an advertisement page all
right so if I sign up with my name email
and stuff like that you know it's going
to receive the information and it's
going to send an email to the owner
saying that somebody had subscribed to
your service that's the default feature
of this app look at that email to the
owner saying that somebody had
subscribed to your app and this is their
email address stuff like that not only
that it's also going to create an entry
in the database and dynamodb is the
service that this application uses to
store data there's my dynamodb and if I
go to tables right and go to items I'm
going to see that a user with name
Samuel and email address so and so has
said okay or has shown interest in the
preview of my site or product so this is
where this is how I collect those
information right and some more things
about the infrastructure itself is it is
running behind and load balancer look at
that it had created a load balancer it
had also created an auto scaling group
now that's the feature of elastic load
balancer that we have chosen it has
created an auto scaling group and now
let's put this URL you see this it's
it's not a fancy URL all right it's an
Amazon given URL a dynamic URL so let's
put this URL behind our DNS let's do
that
so go to Services go to Route 53
go to hosted Zone and there we can find
the DNS name right so that's a DNS name
all right
all right let's create an entry
and map that URL to our load balancer
right
and create now technically if I go to
this URL it should take me to that
application all right look at that I
went to my custom URL and now that's
pointed to my application previously my
application was having a random URL and
now it's having a custom URL so now what
is azure what's the big cloud service
provider all about so Azure is a cloud
computing platform provided by Microsoft
now it's basically an online portal
through which you can access and manage
resources and services now resources and
services are nothing but you know you
can store your data and you can
transform the data using services that
Microsoft provides again all you need is
the internet and being able to connect
to the Azure portal then you get access
to all of the resources and their
services in case you want to know more
about how it's different from its rival
which is AWS I suggest you click on the
top right corner and watch the AWS
versus Azure video so that you can
clearly tell how both these cloud
service providers are different from
each other now here are some things that
you need to know about Azure it was
launched in February 1st 2010 which is
significantly later than when AWS was
launched it's free to start and has a
pay-per-use model which means like I
said before you need to pay for the
services you use through Azure and one
of the most important selling points is
that 80 percent of Fortune 500 companies
use Azure Services which means that most
of the bigger companies of the world
actually recommend using Azure and then
Azure supports a wide variety of
programming languages the C sharp
node.js Java and so much more another
very important selling point of azure is
the amount of data centers it has across
the world now it's important for a cloud
service provider to have many data
centers around the world because it
means that they can provide their
services to a wider audience now Azure
has 42 which is more than any cloud
service provider has at the moment it
expects to have 12 or in a period of
time which brings its total number of
regions it covers to 54. now let's talk
about Azure Services now Azure Services
have 18 categories and more than 200
services so we clearly can't go through
all of them it has services that cover
compute air and machine learning
integration management tools identity
devops web and so much more you're going
to have a hard time trying to find a
domain that Azure doesn't cover and if
it doesn't cover it now you can be
certain they're working on it as we
speak so first let's start with the
compute Services first virtual machine
with this service what you're getting to
do is to create a virtual machine of
Linux or Windows operating system it's
easily configurable you can add RAM you
can decrease RAM you can add storage
remove it all of it is possible in a
matter of seconds now let's talk about
the second service cloud service now
with this you can create a application
within the cloud and all of the work
after you deploy it deploying the
application that is is taken care of by
Azure which includes you know
provisioning the application load
balancing ensuring that the application
is in good health and all of the other
things are handled by Azure next up
let's talk about service fabric now with
service fabric the process of developing
a micro service is greatly simplified so
you might be wondering what exactly is a
micro service now a micro service is
basically an application that consists
of smaller applications coupled together
next up functions now with functions you
can create applications in any
programming language that you want
another very important part is that you
don't have to worry about any hardware
components you don't have to worry what
Ram you require or how much storage you
require all of that is taken care of by
Azure all you need is to provide the
code to Azure and it will execute it and
you don't have to worry about anything
else now let's talk about some
networking Services first up we have
Azure CDN or the content delivery
Network now the Azure CDN service is
basically for delivering web content to
users now this content is of high
bandwidth and can be transferred or can
be delivered to any person across the
world now these are actually a network
of servers that are placed in strategic
positions across the world so that the
customers can obtain this data as fast
as possible next up we have expressr now
with this you can actually connect your
on-premise network onto the Microsoft
cloud or any of the services that you
want through a private connection so the
only communication that happens is
between your on-premise network and the
service that you want then you have
virtual Network now with virtual Network
you can have any of the Azure Services
communicate with each other in a secure
manner in a private manner next we have
Azure DNS so Azure DNS is a hosting
service which allows you to host their
DNS or domain name system domains in
Azure so you can host your application
using Azure DNS now for the storage
cells first up we have test storage with
this storage you are given a cost
effective option of choosing HDD or
solid state drives to go along with your
virtual machines based on your
requirements then you have blob storage
now this is actually optimized to ensure
that they can store massive amounts of
unstructured data which can include Text
data or even binary data next you have
file storage which is a manage file
storage and can be accessible via the
SMB protocol or the server message block
protocol and finally you have queue
storage now with Q storage you can
provide durable message queuing for an
extremely large workload and the most
important part is that this can be
accessed from anywhere in the world now
let's talk about how Azure can be used
firstly for application development it
could be any application mostly web
applications then you can test the
application see how well it works you
can host the application on the internet
you can create virtual machines like I
mentioned before with the service you
can create these virtual Machines of any
size or Ram that you want you can
integrate In Sync features you can
collect and store matrices for example
how the data Works how the current data
is how you can improve upon it all of
that is possible with these services and
your virtual hard drives which is an
extension of the virtual machines where
these services are able to provide you a
large amount of storage where data can
be stored let's talk about Azure
Services as I told you Azure provides
services for a wide range of domains now
let's have a look at some of these
domains there's Ai and machine learning
compute containers database identity
management tools networking Security
Storage and so much more now let's have
a look at some of the individual
services within these domains firstly
you have Azure virtual machines with
Azure virtual machines what you get is
the opportunity to create Windows or
Linux virtual machines now all of this
is possible in a matter of seconds with
a large amount of customization now
let's have a look at some of its
features firstly you can choose from a
wide variety of virtual machine options
then you have a large amount of
optimization available to you for
example what size of operating system do
you want how much size do you want
allocated to it what version of the
system is it and so much more then it
provides low cost and per minute billing
now Azure provides you per minute
billing which means that you're only
charged for how much time you use the
service and finally you have enhanced
security and protection for your virtual
machines next we have service fabric now
with service fabric you have platform
which enables you to create micro
Services now this also makes the process
of application lifecycle management a
whole lot easier as a direct result you
can create applications with a faster
time to Market it supports Windows Linux
on-premises or other clouds and it
enables you to do a tremendous amount of
scaling up depending on your requirement
and finally we have functions now with
functions you can build applications
with the help of serverless computing
here the users only pay for the amount
of resources that they've used you can
create applications in any language that
you want and the only thing you need to
worry about is the code of the
application everything other than that
that is the hardware requirements are
taken care of by Azure now let's have a
look at the networking Services firstly
we have the Azure CDN or the content
delivery network with Azure CDN what you
get is the ability to deliver your
content with reduced load times fast
responsiveness and less bandwidth now
CDN can be integrated with several other
Azure services so that the process can
move at a faster rate it can handle
heavy loads and traffic spikes with ease
it also provides a robust security
system now with the content that's
delivered you can get Advanced analytic
data with which you can understand how
customers are using your content next we
have express route with express route
you can connect your on-premises network
to Azure through a private Network Now
by default this lowest latency it
increases the emphasis on reliability
and speed and it can be of great use
when you have to transfer large amounts
of data between networks now another way
this can be useful is if it's used to
add compute or storage capacity to Data
Centers next we have Azure DNS domain
name service or Azure DNS can be used to
host your domains on Azure this provides
High availability and great performance
it provides fast responses to DNS
queries by taking advantage of
Microsoft's Global Network it also
provides High availability next we have
virtual Network Azure virtual Network
allows the Azure resources to
communicate with each other or other
on-premise networks via the Internet and
all of this is kept extremely secure now
with this users can create their own
private Network for communication it
provides users with an isolated and
extremely secure environment for their
applications to run now all of the
traffic stays entirely within the Azure
Network and it also allows users to
design their own networks next we have
traffic manager now with traffic manager
you can route income traffic to improve
your performance and availability now
one thing it provides is multiple
failover options so if a particular
situation goes wrong there's always an
option to consider to salvage the
situation it helps reduce application
runtime and enables the distribution of
user traffic across multiple locations
it also helps the people who are using
it to know where the customers
connecting from across the world next we
have load balancer with this you have
provided the ability to instantly scale
applications at the same time providing
High availability and improved Network
performance for users applications it
can be integrated into virtual machines
and cloud services it provides highly
reliable applications it also allows
users to secure and integrate security
groups finally we have Azure VPN Gateway
now this allows users to connect their
on-premise networks to Azure using a
side-to-site VPN now this allows users
to connect their virtual machine to
anywhere in the world through a point to
site VPN and also it's very easy to
manage and is highly available now let's
talk about the storage Services first we
have data Lake storage now with this
what you get is a scalable data storage
with an emphasis on cost Effectiveness
and scalability now it comes with
maximum use when you integrate it with
other services so that you can get
analytics on how the data is being used
it is also integrated with other
services like the Azure blob storage now
it is also optimized for Big Data
analysis tools like Apache spark and
Hadoop next up we have blob storage The
Blob storage provides a storage capacity
for data now depending on how often a
particular data is used it is classified
into different tiers now all the data
that is within the blob storage is
unstructured data now it has a way of
ensuring the the data Integrity is
maintained every time a particular
object is being changed or the data has
been accessed and it also helps improve
app performance and reduces bandwidth
consumption next we have q storage now
with this you have a message queuing
system for large workloads this allows
users to build flexible applications and
separate functions not to mention with
this you can be sure that your
individual components will not fail it
also makes sure that your application is
scalable
Q storage provides queue monitoring
which helps ensure that the customer's
demands are met then we have files
stored now with file storage you can
perform file sharing with the help of
the SMB protocol or the server message
block protocol now this data is
protected by SMB 3.0 and the https
protocol in this case like we mentioned
in functions Azure takes care of all the
hardware needs and the operating system
deployments on its own it also improves
on-premises performance and other
capabilities lastly we have table
storage with table storage you can
deploy semi-structured data sets and
nosql key value store now this is used
for creating applications which have a
flexible data schema and also
considering how it has a very strong
consistency model it's mainly aimed for
end prices next let's have a look at
some web and mobile services first we
have the Azure search now with Azure
search you get a cloud search service
which is powered by artificial
intelligence with this you can develop
web application as well as mobile
applications now one big Advantage is
that you don't have to set up or manage
your search indices Azure takes care of
that and by extension it increases your
development speed the artificial
intelligence also will provide insights
and structured information that you can
use to improve the search and structured
information next we have logic apps now
with this you can create integration
Solutions which can connect applications
that are important to your business now
with this you can visually create
business processes and workflows you can
integrate SAS or software as a service
applications and Enterprise applications
and more importantly it allows you to
unlock data within a firewall and
securely connect to services next we
have web applications now with web apps
you can create deploy and scale web
applications according to business
requirements now it supports both
windows and Linux platforms and it helps
with continuous integration or
deployment abilities another very
important aspect of this is that the
data can be deployed and hosted across
multiple locations in the world and
finally we have mobile apps with mobile
apps you can create applications for iOS
Android and Windows platforms one
advantage is that it automatically
scales Up and Down based on your
requirements now in situations where you
have network issues offline data syncing
ensures that your applications work
anyway and you can create cross-platform
applications or native applications for
iOS Android and Windows next let's have
a look at some container services first
let's talk about ACS or Azure container
services it is also known as the Azure
kubernetes Services as it's a fully
managed kubernetes container
orchestration service now what this
means is that it eases the process of
container integration and deployment it
also can be used with other resources
from security like virtual networks
cryptographic keys and so much more to
ensure that your container is kept
secure next we have container instances
now this is similar to functions in a
way just that in this we're using
containers without having to manage
servers now applications can be
developed here without managing virtual
machines or learning new tools all that
is azure's problem to take care of and
it enables building applications without
having to manage the infrastructure that
is all you need to worry about is
running the container next let's have a
look at some database Services first we
have the SQL database now with SQL
database what you get is a relational
cloud data based service now this means
that it helps accelerate your app
development and makes it easier for you
to maintain your application now SQL
database is also used extensively in
migrating workloads to the cloud and
hence saves time and cost it also helps
improve your performance by integrating
machine learning and Adaptive
Technologies into your database next we
have Azure Cosmos DB now this is a
globally distributed multi-model
database service now what this means is
that with this you can create
application with support nosql it
provides a high grade security system
has high availability and low latency
now this is usually used in situations
where you have a diverse and highly
unpredictable workload now let's have a
look at some security and identity
Services firstly we have the Azure
active directory now if you want to know
more about Azure active directory I
suggest you click on the top right
corner and watch our video on the Azure
active directory this is just an intro
production so with this you can manage
user identities and you can make sure
the resources are kept safe with the
help of access policies most of these
are intelligence driven now one of the
main features is that you can have
access to your applications from any
location or device it helps increase
your efficiency and helps down cutting
costs when it comes to having a help
desk it can also help improve security
and can respond to Advanced threats in
real time next you have Azure active
directory b2c it helps provide customer
identity and access management in the
cloud now protecting customer identity
is extremely important for an
organization and that's what Azure adb2c
does now it also enables the application
to be scaled to great amounts even
billions of customers next we have the
Azure security Center this is basically
like a command post with which you get a
complete view of the security across
users on your on-premises and Cloud
workloads so with this you are given
threat protection method that adapts to
situations and helps reduce exposing you
to threats it also has rapid threat
response and makes the process of
finding and fixing vulnerabilities a
whole lot easier next up let's talk
about monitoring and Management Services
so first let's have a look at as your
advisor now Azure advisor is basically a
guide for the best practices when it
comes to Azure now when you follow these
it improves performance security cost
and increases availability now it also
learns from how you use the services on
your configuration and usage pattern and
the adjustments that it suggests can be
implemented very quickly and easily next
we have Network Watcher now with this
you can monitor diagnose and understand
the working of your network now you can
monitor your network without actually
having to login to your virtual machine
now you can also use something known as
network security flow logs to understand
the traffic pattern how much traffic is
coming toward you how much you're giving
and so much more it also helps diagnose
VPN problems that you might have with
detailed logs and finally you have the
Azure resource manager now with this you
can ensure that the resources that you
have are managed and deployed at a
consistent rate now this makes it
extremely easy for you to manage and
visualize your resources that are used
in your applications or some other
requirements and you can control who can
access your resources as well as perform
actions on it and now let's have a look
at how we can use one of these services
to satisfy a particular requirement so
the friend asks that she needs one last
bit of help she wants a friend's help to
host the website on Azure and here's how
we can do it so now we're going to put
one of these Services into action so
this is the Azure dashboard so from here
we are going to create a virtual machine
within which we are going to create a
website a very simple HTML website
nonetheless so let's get started so
first what we're going to do is create a
resource a resource Group so what a
resource Group is is actually a
repository of all the resources like
virtual machines or storage that you are
going to use in your project so in this
we need a virtual machine so let's get
that created so first we'll name it
demo
is us and will created now this will
take a few seconds
and that's it so now that that's done we
go on ADD
so we're going to be using a Windows
operating system so we'll just search
for virtual machine
we select this
and create
okay so here we are going to set up
we're going to select the resource Group
the one just that we just created a
virtual machine name
okay
Let's uh storage name
[Music]
passwords
and then
just Breeze through the rest of it we
can introduce the size we don't need
this big
and create
now that the validation is done the next
step is to
create
now this will take a few minutes so
we'll wait and as you can see here the
virtual machine has been created we'll
go to resource
so here all we need to do is connect and
download the RDP file
let's see if we run this file and see
what happens
now this error shows up in a whole lot
of systems that I've used so this is a
very easy way to troubleshoot this
problem so here's what you need to do
now so these are the problems there's
remote access to the server is not
enabled the remote computer is turned
off the remote computer is not available
on the network
so what we're going to do is go to
networking
and add an inbound Port Rule now we're
trying to access a computer remotely
right so to remotely access the computer
we would need to add an inbound Port
rule so that's what we're going to do
right now
so we can keep the sources any port
range as any and our destination Port as
3389 if you've noticed there we'd
actually have used the 3389 as the
inbound port and then we change the name
as 3389 and this rule is added
now after this is done all we need to do
you can see here that we refresh the
page this would be here
a new rule has been added now so we'll
go to overview
and try to connect again
do the same thing again
and this time it worked so here we'll
input the name
and our password
and that's it now we just press yes
and there you go the system has started
now your virtual machine is started now
in a while the server manager will pop
up
so there's some things that we need to
do so that we can host the website so
the first thing is to create an inbound
Port so we'll turn off the virtual
machine for now
come back to that later first we'll go
to a resource Group
check the name of our resource
and then we'll look at all its resources
here
so here we need to find the network
security group
and add an inbound security rule press
add we take any Source any Source Port
range
and then we add Port 80 which is the one
that we're going to use and we'll put
the name as port
80.
we'll add the security rule
which will take a few seconds
and that's that now it's a lot now let's
go back to our virtual machine
here you go
and let's get it started
login again
and there you have it it's on again now
we'll go to server manager so we need to
add roles and features
so most of this we can just run through
we'll leave them at the default values
same and here we need to add a web
server which is the IIs so we'll select
this
we add the features next so we can add
the dotnet 3.5 features as well next
and
will install now
we'll wait for the installation to
finish
and now the installation is done so now
what we're going to see is if the
website can be reached I will go to the
public address which is given here so
we've come back to our original system
click
and you can see that the is Page has
been created
now we need to work more to create the
HTML page here's what we'll need to do
to run our website so right now what
we're getting is this the IIs page now
let's make it our own HTML page a very
simple HTML page from there on we can
create the website so go back to the
virtual machine and then we go to C
drive
so the IIs is located in a location
which is in C drive in it Pub and www
root so if you click on this actually
what you'll get is the page I showed you
earlier which is this one so now we'll
go to the server manager
click on tools
and find the IIs manager
so we click on this
and sites now there's a default website
we can now add a new one
a site name
ammo
we need a physical path we'll put it
inside the folder we just saw which is
right here C drive
unit Pub WWE root within which we'll
create a new folder it's for simply on
demo
I'll save it and put 80 and okay
now this is assigned to another site so
we'll change that but we'll wait for
that okay
now for this we'll just change it we
click on this
so since both of them have the same port
we'll change that go here edit binding
we change this
to something else
okay and close
so now both are different and now we can
start our website
it's already started now
we'll start the website right click this
is a manage website
and start
and that's it
now we will go to the
to the inner Pub ww root and we have
simply learned demo so here we will
create the
HTML website or the HTML
the document
so
index Dot HTML
okay
open this
it's a very simple website now so
we keep it save it
so here what we're going to do is save
as
an HTML file but for all
all files
we close this now to make sure this is
recognized as a DOT HTML file it go to
view
options
here we'll hide extensions from known
file types
I'll just rename this
[Music]
and there you have it it's a DOT HTML
file
now let's go back
to our original computer
and on this page
and there you have it now this is just
the initial version more work has to be
done on it but your first step is
already done hi guys I'm Raul from
Simply learn and today I'd like to
welcome you all to the greatest debate
of the century today I am joined by two
giants of the cloud computing industry
they'll be going head to head with each
other to decide who amongst them is
better it's going to be one hell of
fight now let's meet our candidates on
my left we have AWS whose voiced by a
picture hi guys and on my right we have
Microsoft Azure whose voice by Anjali
hey there so today we'll be deciding
who's better on the basis of their
origin and the features they provide
their performance in the present day and
comparing them on the basis of pricing
market share and options free tier and
instance configuration now let's listen
to their opening statements let's start
with AWS launched in 2006 AWS is one of
the most commonly used cloud computing
platforms across the world companies
like Adobe Netflix Airbnb HTC Pinterest
and Spotify have put their faith in AWS
for their proper functioning it also
dominates the cloud computing domain
with almost 40 percent of the entire
market share so far nobody's even gotten
close to beating that number AWS also
provides a wide range of services that
covers a great number of domains domains
like compute networking storage
migration and so much more now let's see
what Azure has to say about that Azure
was launched in 2010 and is trusted by
almost 80 percent of all Fortune 500
companies the best of the best companies
in the world choose to work only with
Azure Azure also provides its services
to more regions than any other cloud
service provider in the world Azure
covers 42 regions already and 12 more
are being planned to be made Azure also
provides more than 100 Services spanning
a variety of domains now that the
opening statements are done let's have a
look at the current market status of
each of our competitors this is the
performance route here we have the stats
for the market share of AWS Azure and
other cloud service providers this is
for the early 2018 period Amazon web
services takes up a whopping 40 percent
of the market share closely followed by
sgr at 30 and other cloud services
adding 30 this 40 indicates most
organizations clear interest in using
AWS we are number one because of our
years of experience and Trust we've
created among our users sure you're the
market leader but we are not very far
behind let me remind you more than 80
percent of the Fortune 500 companies
trust Azure with their cloud computing
needs so it's only a match of time
before Azure takes the lead the rest of
the 30 person that is in AWS or Azure
accounts to the other cloud service
providers like Google Cloud platform
Rackspace IBM software and so on now for
our next round the comparison round
first we'll be comparing pricing we'll
be looking at the cost of a very basic
instance which is a virtual machine of
two virtual CPUs and 8 GBS of RAM for
AWS this will cost you approximately
0.0928 US dollars per hour and for the
same instance in Azure it will cost you
approximately 0.096 US dollars per hour
next up let's compare market share and
options as I mentioned before AWS is the
Undisputed market leader when it comes
to the cloud computing domain taking up
40 of the market share by 2020 AWS is
also expected to produce twice its
current Revenue which comes close to 44
billion dollars not to mention AWS is
constantly expanding its already strong
roaster of more than 100 services to
fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gartner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a matter
of time before Azure can increase from
its 30 market share and surpass AWS this
becomes more likely considering how all
companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirements now
let's compare AWS and azure's free
offerings AWS provides a significant
number of services for free helping
users get hands-on experience with the
platform products and services the free
tier Services fall under two categories
services that will remain free forever
and the others that are valid only for
one year the always free category offers
more than 20 services for example Amazon
SNS sqs cloudwatch Etc and the valid for
your category offers approximately 20
services for example Amazon S3 ec2
elastic cache Etc both types of services
have limits on the usage for example
storage number of requests compute time
Etc but users are only charged for using
services that fall under the valid for a
year category after a year of their
usage a shop provides a free tier as
well it also provides services that
belong to the categories of free for a
year and always free there are about 25
plus always free services provided by
Azure these include app service
functions container service active
directory and lots more and as of the
valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of 200
US dollars to access all their services
for 30 days now this is a unique feature
that Azure provides where users can use
their credits to utilize any service of
a choice for the entire month now let's
compare instance configuration the
largest instance that AWS offers is that
over whopping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of ram s16 virtual CPUs and now for
the final round each of our contestants
will be shown facts and they have to
give explanations for these facts we
call it the rapid fire round first we
have features in which AWS is good and
Azure is better AWS does not cut down on
the features it offers its users however
it requires slightly more Management on
the user's part Azure goes slightly
deeper with the services that fall under
certain categories like platform as a
service and infrastructure as a service
next we have hybrid Cloud where AWS is
good and Azure is better OK although AWS
did not emphasize on hybrid Cloud
earlier they are focusing more on
technology now Azure has always
emphasized on hybrid cloud and has
features supporting it since the days of
its inception for developers AWS is
better and Azure is good of course it's
better because AWS supports integration
with third-party applications well Azure
provides access to data centers that
provide a scalable architecture for
pricing both AWS and Azure are at the
same level it's good for AWS because it
provides a competitive and constantly
decreasing pricing model and in the case
of azure it provides offers that are
constantly experimented upon to provide
its users with the best experience and
that's it our contestants have finished
giving their statements now let's see
who won surprisingly nobody each cloud
computing platform has its own pros and
cons choosing the right one is based
entirely on your organization's
requirements and whether you are a
fresher or an Art Expert with some
experience you can improve your skills
in cloud computing by getting the proper
training and certification on that note
we present postgraduate program in cloud
computing by simply love the
postgraduate program in cloud computing
design collaboration with Caltech ctme
helps you become an Azure AWS and gcp
expert this in-depth cloud computing
certification course lets you master key
architecture principle and develop the
skills needed to become a cloud
committing expert benefits of this
postgraduate program include Caltech
ctma postgraduate certificate enrollment
and simple and services receive up to 30
CEUs from caltex Atma simply learn job
assist helps you to get noticed by top
hiring companies attend master classes
from caltex ctme live virtual classes
led by industry expert Hands-On project
and integrated Labs online convocation
by Celtic ctme program director 40 plus
hands-on experience projects and
integrated Labs caption projections four
domains caltex ctm Circle membership you
can also
certification and its in-depth course
curriculum which covers the key concept
of azure AWS and gcp platform and
services such as AWS Lambda Amazon S3
Azure app services and many more these
programs also cover the essential skills
to become an expert in cloud computing
and you can consider these programs
because it provides you with the
certification from caltex atme a
well-reputated university in United
States so check out the link mentioned
in the description box below so without
any further Ado let's get started let's
understand why Google Cloud platform so
Google Cloud platform is popular for
many reasons now let us see few of most
important reasons why it stands out
when you talk about pricing pricing is
one of the significant factors that make
Google Cloud Stand Out Among the other
Cloud providers
it offers a monthly pricing plan which
is billed according to monthly usage and
when we talk about billing here the
billing can be in hours it can be in
minutes and it can also be in seconds
there are different options when you
talk about your pricing which can be
found from your Google Cloud web page
now pricing could be based on preemptive
machines pricing could be based on
reserved instances or reserved resources
I'll show you the link where you can
find more details on pricing part of it
so Google Cloud really has various
pricing options which help customers in
their different requirements whether
they would go for any of the service
models such as infrastructure as a
service platform as a service or even
software as a service one more
attractive thing about Google Cloud
pricing is that it provides committed
use discounts
for example under this scheme you can
purchase a specific amount of virtual
CPU course and memory for up to 57
discount off regular prices if you
commit usage for either one or three
years now this is just one option there
are various such options which you can
learn about from the Google clouds page
and that really suits different
customers for their different
requirements
now when we talk about speed we don't
need to really challenge this aspect
when it comes to Google services so
Google provides its Google cloud and
Google app customer speed up to 10
terabytes because of its faster cable
system there are different kind of
machines which can be used if you are
talking about computation if you are
talking about memory hungry applications
or even storage intensive workloads okay
all in all speed is one of the defining
characteristics of Google cloud services
the cable has connections over us West's
course main cities in Japan and even
major hubs in Asia this speed enhances
performances and leads to customer
satisfaction now when we talk about
customers any or every customer would
prefer to have low latency High
throughput Based Services they would
want to use higher speeds to process
their data in as less time as possible
Google provides a low latency Network
infrastructure in fact you can say that
when you are using Google cloud services
you are using the services from the same
infrastructure which Google uses for its
popular services such as Google search
or even YouTube which is one of the
second largest repository which can be
accessed for videos now when we talk
about Big Data big data is data which is
very complex has lot of other
characteristics such as your volume you
have velocity you have variety you have
velocity validity volatility virility
and so on so if an organization has is
working on Big Data Google Cloud can be
a better choice because Google has many
Innovative tools for cloud warehousing
for example such as bigquery and even
real-time data processing tools such as
data flow bigquery is a data warehouse
that allows massive processing of data
at high speeds basically working on your
structured data Google also has launched
some new machine learning from
artificial intelligence tools now there
are various other services which we can
which we can use from Google Cloud
platform but let's understand what is
Google Cloud platform and what are some
of the services even the services which
are not less tested here can be found in
your console from Google Cloud so what
is Google Cloud platform gcp it is a set
of Cloud Computing Services provided by
Google that runs on the same
infrastructure as I mentioned that
Google uses for its end user products
like YouTube Gmail and even Google
search the various set of services
offered by Google Cloud platform are so
you have Services which are specific to
Computing requirements and again in
Computing you have various different
options available you have machines
which are compute optimized machines
which are memory optimized machines
which are storage optimized and also we
have certain machines such as preemptif
which basically means that you could get
a machine to work on at a far lesser
price than any other machine but when we
talk about preemptive these are the
machines which can be requested on
demand and they can be taken back by
Google at any time you have networking
related Services which can be very
useful when you are setting up your
applications or your services across
Globe you also have different Services
which are specific to machine learning
and organizations which would be
interested in working on machine
learning or artificial intelligence
would be really interested in using
these Services there are also innovative
solutions to work on big data and Big
Data related Technologies now when we
talk about Google Cloud platform domains
so we can break down these Services into
specifics such as you have compute now
the compute service allows for computing
and hosting the cloud
now when you talk about Computing here
there are different services such as app
engine you have compute engine you have
kubernetes you have Cloud functions and
Cloud run when you talk about storage
and database so the storage and database
service allows application to store
media files backups or other file like
objects now various Services Under This
are as follows you have cloud storage
Cloud SQL Cloud bigtable for
unstructured data you have Cloud spanner
cloud data store persistent disks and
Cloud memory store when we talk about
networking the networking service allows
us to load balanced traffic across
resources now as I mentioned earlier
resources could be your different
resources which you would be using from
a cloud platform such as your devices
your instances memory optimize or CPU
optimized instances us or other
resources creating DNS records and much
more so various Services Under This are
VPC that stands for virtual private
Cloud you have Cloud load balancing
Cloud armor Cloud CDN you have Cloud
interconnect DNS and network service
tiers when we talk about the big data
service this allows us to process and
query big data in Cloud now various
Services under these are as follows you
have bigquery cloud cloud data proc
Cloud composer cloud data lab cloud data
prep Cloud Pub sub which is publishing
subscribing system you have cloud data
studio now you also have the developer
tools and developer Tools service
includes tools related to development of
an application now various Services
under these are as follows that is you
have Cloud SDK software development kit
you have deployment manager Cloud Source
Repository and Cloud test lab when we
talk about identity and security which
is one of the primary concerns for any
organization or any user who would be
interested in using a cloud platform
Google Cloud really has taken care of
this so when you talk about identity and
security domain this deals with Security
Services now various Services here are
Cloud identity you have identity and
access management that is cloud IAM you
have identity aware proxies you have
cloud data loss prevention API security
key enforcement Key Management Service
and many more you also have cloud
services which are related to internet
of things so very much would be used by
organizations who would be working on
iot devices or the data generated by
these devices so various Services here
are Cloud iot Core you have Edge TPU and
Cloud iot also when we talk about Cloud
AI that is artificial intelligence this
comprises of services related to machine
learning and much more so you have Cloud
Auto ml Cloud TPU Cloud machine learning
engine job Discovery dialogue flow
Enterprise natural language Cloud text
to speech and much more
if you would be interested in Services
related to API platform then there are
different Services Under This category
or this domain so you have Maps platform
you have APG API platform monetization
developer portal analytics that is API
analytics APG sends Cloud endpoints and
service infrastructure so these are some
of the listing of services under each
Cloud platform domain now let's look at
Ferrero use case and let's also
understand what was done here so Ferrero
is one of the famous chocolate and ranks
third among worldwide chocolate and
confectionary producers it was found in
1946 in Italy I'm sure you would have
seen the Ferrero chocolates when you
would have gone out to buy some
chocolates so the challenges here was
that Ferrero as we know is sold in every
supermarket and is known for its quality
once the prisoners grew some issues
arose right and that's what happens when
the business grows you have issues
popping up which could be related to the
volume of data the speed with which the
data is getting generated the variety of
data and also looking at your platforms
with support
different Dynamic applications or your
scalability requirements performance
requirements and so on so it needed data
storage processing and Analysis system
for a wide customer database
there was a huge gap between the company
and the people who bought its goods
because the company relied on data given
sales outlets now that's one of the
challenge Ferrero wanted to create a
digital ecosystem where there was a
point of contact with its customers and
also a foundation for an Innovative
data-driven marketing strategy what was
the solution here so one of the service
of cloud platforms or Google Cloud
platform is bigquery and this was an
answer to ferrero's challenges since it
was capable of hyper fast and efficient
data analysis as a solution now as I
mentioned bigquery is a data warehouse
which allows you to store structured
data now this could directly be used as
a service where you could store in any
amount of data and you would not be
paying for storage so there are
different pricing models and for data up
to one terabytes you would not be
charged anything and if you would be
accessing the data that is reading the
data or processing the data from
bigquery that's where the pricing model
kicks in
Now using Google Cloud's bigquery
business analysts of Ferrero were able
to store and analyze massive data sets
in a very reliable fast and affordable
manner
consumer behavior and sales pattern data
reports were easy to build and automate
and the analysis also followed Ferrero
to adopt
advertising across various marketing
channels to serve the customer needs in
a better way what was the result
they could divide their database into
real-time actionable consumer clusters
to generate more accurate user profiles
Ferrero was also able to personalize its
marketing strategies to match the user
needs
now Google Cloud platform completely
tailored the website mobile content and
advertising and created a very cost
effective media by strategy now these
were some Basics on Google Cloud now as
I said you can always find a lot of
details about pricing about services and
also all the services are accessible in
your free trial account now let me just
walk you through here so one is you
could always find details on
documentation on each of these services
so if I would click on getting started
you have quick start which basically
shows you short tutorials you have
trainings and you have also
certifications now if we click on quick
start now that takes me to this page
which shows me
quick starts or if you would want to
understand about different projects if
you would want to look into the
documentation of creating a Linux
virtual machine or storing a file and
sharing it deploying a container Docker
container image and so on so you have a
lot of quick starts here you can also
look at Cloud minute
on the same page on the right side you
have docs and once you click on that it
takes you to these links now this shows
you build Solutions which shows your top
use cases best practices all the
solutions it's always good to learn from
these use cases which are available and
here you have different feature products
and all the different Services which
Google Cloud offers now you can click on
featured products and that basically
shows you a list of these products now
here we have all the solutions which you
can look at architecture database
Enterprise level big data and Analysis
gaming related internet of things and so
on now if you go down here it shows you
featured products and that shows you
some of the important products such as
compute engine which is belonging to the
compute domain you have Cloud run you
have anthos which is for migration and
basically Cloud adoption when
organizations would want to move from
on-premise to cloud-based Solutions you
have Vision AI you have cloud storage
now that basically allows you to store
any kind of data whether that's an
object so this is acting as an object
storage you have Cloud SQL which is
basically a ready to use service where
you would be using MySQL postgres or any
other SQL Server database Services you
have bigquery which is a data warehouse
which basically allows you to store your
structured data and then you have your
AI and machine learning related products
so you have automl Vision AI video AI
text to speech speech to text and so on
now you also have different platform
accelerators which can be used and in
any of these cases for example if I
would click on compute engine which is a
featured product from Google Cloud now
that shows you basically your quick
starts using your Linux machines how-to
guides which tells you completely
working on a VM instance
or working on storage working on
persistent disks and so on and it shows
you the documentation here now you also
have products and pricing option which
you can see for gcd pricing and you
could straight away go to the pricing
you could be looking at Solutions which
talks about infrastructure modernization
now this is something which
organizations are interested in when
they would want to move from their
on-premise solution to your Google Cloud
now here I can say for example I click
on infrastructure modernization you can
always find some case studies what are
the different solutions we have here
right when you talk about Google Cloud
so you can always click on see Solutions
and you can look at VM migration or sap
Cloud right why Google Cloud what it can
be used for VMware as a service or HPC
that is high performance Computing and
about these services we will learn in
detail later now I can go back on the
same page where I was clicking on the
services so you have quick starts you
have how-to guides you have deep
understanding of different concepts
right and here if I click on all how to
guides so that shows me what are the
different ways in which you can work
with compute engine and working with
different instances although it's quite
exhaustive content but then if you
follow steadily then you can learn a lot
about Google Cloud now here I can just
go back so this is just giving you some
idea on the different products which are
available from Google Cloud looking into
different sections finding right
documentation here right and you can
always look into each one of these in
detail for each product which Google
Cloud offers
now if we scroll down we could see all
the options or all the domains which we
see here right let me just go back
because we got into Solutions now we
have quick starts right and then you can
basically scroll all the way down to
look at Cloud SDK which can be set up on
your Windows machine like I have set it
up on my Windows machine plus when you
use Google console you have a GUI which
I'll show you in couple of minutes and
also a cloud shell where you can use
your command line options to work with
Google Cloud platform you have Cloud
console which is nothing but your GUI to
access your resources now you can also
look at in-depth tutorials pricing and
you have different other options so
let's just click on pricing here and
then we have your price list which
basically gives you details of different
services what are available and what are
the services or what are the prices so
for example if I click on compute engine
right and this shows me the pricing
aspect of compute engine which belongs
to compute domain and here you can see
you have VM instance pricing you have
networking pricing sold tenant nodes
which are specific to particular
organizations or if organizations would
want to have dedicated nodes you have
GPU based pricing right so General
processing units you have disk and image
pricing and you can click on any of the
links and you can see pricing which also
shows you your different kind of machine
types so here you have different kind of
machine types which says N1 N2 n2d E2
you have memory optimized machine types
you have compute optimized you have
premium images and you can basically
look at all the categories you can look
at disk pricing which involves your
persistent disk pricing for ssds or sdds
what are the kind of images what are the
different network Services right and you
can always look at your machine types
you can choose a particular region right
there is the concept of region and
availability zones here and as per
region you could look at the prices
you can also look at standard prices you
could be looking for what are the free
tier machines if I'm specifically
looking for VM instance pricing I can
click on this one and that takes me to
the VM instance pricing what is the
billing module what is the instance
uptime what is the resource base pricing
and then you can also look at different
kind of discounts such as sustained use
discounts committed use discounts
discounts for preemptable VM instances
and so on and this is how you can be
looking at pricing for all different
resources and you can choose the
resource which you are interested in and
then look for the pricing benchmarks
reservations if you would want to use
and you would want to benefit if you
would want to look at what are the
quotas and limits and so on so please
explore this link and you can find lot
of information when it comes to
technical options looking at different
Google Cloud products which we briefly
discussed when it comes to domains and
what each product can be used for you
can always come back to the main page
where I was showing you different
Services we went into pricing straight
away right now you can always come back
to this page on your cloud.google.com
and then you have your Solutions
which talks about different products
right and you can click on these or you
can look into technical documentation so
this talks about your different featured
Solutions infrastructure Solutions you
have data center migration related and
so on so I could be talking more and
more about Google Cloud platform it's a
ocean it's a huge chunk of different
services for different organizational
requirements so look into this link and
also what you can do is create a account
on Gmail I mean you could create a free
account or you could go to
cloud.google.com and then you could
create a free account like I have
created here and I can just click on
Console now that's my GUI or Google
Cloud console which basically allows me
to work with Google Cloud now there are
a lot of options here by default when
you create an account now in my case on
the top it says it's a free trial
account I have three hundred dollars
credit and out of that 251 dollars is
left and 237 days out of a year are left
for my account now here I can basically
see the project right and by default we
can create an project or by default
there is a project existing for any user
so when you log in it creates a
particular project and you could create
a new project and project would be to
dedicate your different services or
different resources per different
project so this is your dashboard which
basically shows your project which shows
you a project number and a project ID
which is always unique now you can click
on this and this if you would want you
can hide this information you can also
look into the documentation part of it
then it shows you the different services
or a graphical information of services
which you might have used in past so you
have compute engine which shows you how
how much percentage of CPU was used you
can always go to compute engine as a
service you can look at your Google
route platform status and look at the
Google Cloud status now this is billing
now since this shows me the billing
period for April month and I can always
look at detailed charges I can look at
reporting I can look at different apis
which Google offers and for your
different kind of work you can always go
to the API overview or the API link and
enable or disable any API now once you
enable or disable any API then you can
use that so you have new section you
have documentation and you have getting
started guides which tells you how to
work or enable different apis if you
would want to deploy a pre-built
solution at Dynamic logging monitoring
errors deploying a Hello World app and
so on now this is your dashboard I can
click on activity and that that would
show me what kind of activity I would
have done in my cloud platform I can
always choose the kind of activities by
saying activity type I can choose the
resources now it shows me that I created
some VM instances I deleted them I
updated some metadata I basically worked
on instances I changed some firewall
rules here I have then also worked on
some other services or apis so I created
some buckets which is for object storage
and again I have been working with some
instances and creating some firewall
rules here I have granted some
permissions I am setting up a policy
right so this activity gives me a
history of what things I have done over
past couple of months while working on
Google Cloud platform now
on the top left corner you have the
hamburger menu which you can click on
this so that's your navigation menu and
when you click on this this shows you
home it takes you to the marketplace it
specifically takes you to billing you
can always look at apis and services so
here in API Services I can pretty much
go to a dashboard and I can see a report
on the traffic or errors or latency and
the kind of apis which are available so
you have compute engine API you have
bigquery API bigquery data transfer API
bigquery storage cloud data proc so
these are some of the apis or Services
I've used in past might be I was
evaluating a product might be I was
using a particular product so you have
whenever you would want to use a
particular service from Google Cloud
platform you would be enabling these
apis so here we see some apis for data
proc you have logging monitoring you
have have resource manager API you have
Cloud SQL which allows you to directly
use MySQL or postgres you have cloud
storage right and so many apis so if I
would want to enable a particular API
which is not right now required but then
it's good to know you can always click
on this one you can search for an API so
for example if I would say data proc
right and that shows me the cloud data
proc API which manages Hadoop based
clusters and jobs on Google Cloud
platform so I can spin up a Hadoop
cluster and I can start running some
jobs on a Hadoop cluster on demand and
as I'm done with it I can just get rid
of it so this is an API which I would
have to enable and then for every
particular service you also have an
admin API which you would enable
now going back so coming out from this
API Library I got into API and services
now there is you can look into the
library you can look at credentials and
you can also look at different time
intervals for which you can see the
usage of your different apis now going
back to the menu you have support
wherein you can always reach out to the
Google support team if you are using a
paid version even with free trial you
can try to reach out and you will find
someone to help but it is always good
when you have a billing cycle reaching
out the customer care you have identity
access management and admin and this is
required when you're working with
different apis or Services when you
would want to have relevant access you
have getting started you have security
related options you have anthos which is
mainly for migration now here you can
look at your different domains such as
which we discussed so you have of
compute and in compute you have
different services so you have app
engine you have compute engine
kubernetes you have Cloud functions and
Cloud run you can look into the storage
aspect which shows you bit table which
is usually and mainly for your
unstructured data big table which is
something which gave rise to popular
nosql databases like edgebase and
Cassandra you have data store which can
be used you have firestore file store
storage which can be used to put in data
of any kinds you have SQL for structured
data you have spanner as a service you
have memory store and you have data
transfer now when it comes to the
networking domain you have all these
services such as VPC virtual private
network network related services for
your load balancing for using a
cloud-based DNS or Cloud Define Network
you have hybrid connectivity network
service tiers security and intelligence
then you have other options for your
operations which can be used and here
you have different other tools which can
be used such as Cloud build you have
Cloud tasks containers registry
deployment manager and many more big
data specific you have the services here
so you have data proc which can be used
to spin up your clusters you have
published subscribe messaging systems
such as now Kafka which you might have
heard of originates its idea from here
so Pub sub you have data flow you have
iot core bigquery which is a data
warehouse package or data warehousing
solution from Google Cloud now then you
have your artificial intelligence
related services and other Google
Solutions so you can always find a huge
list of services or you can say at high
level Solutions offered by Google cloud
and we can use these to basically test
some of the solutions use them and work
on them so I'll give you a quick demo on
different Services which can be used
here from your Google Cloud platform now
this is your Google Cloud platform and
this is your console now this also gives
you a cloud shell which can be activated
so that you can work from command line
and you can always find a lot of
documentation on that so you can also
set up your Cloud shell that is SDK on
your Windows machine and if you have set
it up then basically you could be doing
something like gcloud right if the cloud
has been set up so in my case I had set
up the cloud SDK and basically I can get
into that by looking at what path I have
set up so Cloud SDK and then I can be
using it from my Windows machine as per
my comfort I can also activate the cloud
shell here which will basically open up
a terminal and at any point of time you
can open up this one Cloud cell in a
different window it is preparing the
cloud shell where it will by default set
up your project it will set up the
metadata and now I am logged into my
Google Cloud account from command line
and here I can basically use gcloud
right and that basically shows you the
different options which are available
which you can be using so if you would
want to use a particular service you can
also try giving a help and that shows
you how do you manage your Google Cloud
platform right so you have different
options for billing to work with your
different services and basically you
could be working on Google Cloud using
this Cloud shell from your command line
usually for people who are learning old
cloud in the beginning it is always good
to go for console and use your different
services from here in an easier way but
as I said you can always use the command
line for example if I would just go
ahead and type gcloud create instances
and that will directly take me to the
cloud console documentation which shows
you different options which you can use
to work with instances so I could do a
gcloud compute instances create and then
I can give my instance name and so on
and I'll show you some examples on that
so you can be using your Cloud console
right that is your Cloud shell and you
can straight away start working from
command line however I would suggest
using console in the beginning and when
you are well experienced then you can
start using Cloud shell to do things
from command line and when you are fully
experienced you can always switch and
certain things are usually useful or
easier when done from command line and
some are easier when you do it from the
console and you can use any one of these
options now we can go to Google Cloud
console now as of now I can close this
one I still have my cloud cell open if I
would want to look into it I can click
on this one and I can straight away go
to compute engine region where I would
want to work on creating some instances
on Google Cloud platform using the
compute engine service and then
basically connecting to those instances
and basically trying out some basic
things so you can click on VM instances
and then once this comes up I can always
create some instances now if you see
here I have some instances already
created right and I can continue working
on those I can create new instances I
can use different options while creating
instances and I'll show you that in a
demo in quick seconds so let's have a
quick demo on setting up gcp instances
now before that let's have a quick recap
so when you talk about instance or a
virtual machine it is hosted on Google's
infrastructure right and you can create
your Google Cloud instance using this
Google Cloud console that is by clicking
on this and then going to compute engine
and clicking on VM instances now you can
also do that from Google Cloud command
line tool that is cloud shell and you
can be doing that using compute engine
API so compute engine instances can run
the public images for Linux or Windows
servers that Google provides you also
have option of creating or using custom
images that you can create and import
from your existing systems you can
deploy Docker containers which are
automatically launched on instance
running containers optimized OS now when
you talk about instances and projects
always remember that in each instance
belongs to a Google Cloud console
project and a project can have one or
more instances
when you talk about instances and
storage options each instance has a
small boot persistent disk which I will
show you in for this queens that
contains the OS you can add more storage
space if needed and when you talk about
instances and network a project can have
up to five VPC networks so VPC network
is virtual private networks wherein you
can virtual private Cloud networks where
you can have your resources within your
own subnet and each instance belongs to
one VPC Network
now instances in same network can
communicate with each other through
local area network protocol an instance
uses internet to communicate within any
machine so that could be virtual
physical or outside its own network when
you talk about instances and containers
you should remember that compute engine
instances support a declarative method
for launching your application using
containers now you can create a VM
instance or a VM instance template you
can provide a Docker image and launch
the configuration
so there are different ways in which you
can create these instances and once you
create these instances you can say for
example you're creating a Linux instance
you can associate SSH keys with your
Google account or your G Suite account
and then manage your admin non-admin
access to the instance using IM roles if
you connect to your instance using
gcloud or SSH from console which we will
see later compute engine can generate
SSH keys for you and apply that to your
Google cloud or G Suite account now what
we can do is we can see how we can
create your Google Cloud instances from
your console or from the command line
let's have a look in creating instances
using GCB console now that we have
learned on some basics of Google Cloud
platform what are the different Services
what are different domains basically
looking at your Google Cloud console or
even Cloud Shell let's go ahead and
create some BM instances now that's from
your compute engine service and let's
try connecting to these instances and
see how this works let's also see what
are the different options which are
available when you would want to create
the virtual machine instances here so
when you click on your drop down from
the top left and choose compute engine
so it brings you to this page so I can
show that again so you can click on
compute engine click on VM instances and
that basically brings you to this page
now here it tells you compute engine
lets you use Virtual machines that run
on Google's infrastructure so we can
create micro VMS or large instances
running different distributions of Linux
or Windows using standard images that is
public images and you can also have your
own image so let's create a VM instance
by clicking on create
now that's basically helping me to
create an instance here so it shows me
the instance name I can give it
something so let me say C1 now I can add
labels so what is label it basically
allows you to organize your project add
arbitrary labels as key value pairs to
your resources this so this is basically
categorizing your labels and projects if
you have multiple projects now remember
if you have your Cloud console and if
you have created your free trial account
it will allow you to do these things if
not you may have to go back to the
billing section and see if the billing
is enabled which also means that when
you are creating your Google Cloud
account it will ask you to key in your
credit card details but they do not
charge anything or they might charge
might be one dollar or one rupee
depending on your location and that's
also refunded but that's just to verify
your card now once I've given the name I
can choose a region so I would basically
choose Europe since I'm in Europe I
would be clicking on Europe West 3 and
here I can choose an availability Zone
availability zone is basically to make
your services or instances or any other
resources highly available so you can
choose one of the availability zones now
I would choose save S3 now this one we
can scroll down and here it says machine
configurations you have general purpose
machines you have memory optimized
machines which is M1 series and you can
always go back to the Google Cloud page
and see what a particular kind of
machine specializes in so I would click
on General's purpose and in general
purpose you have different categories so
you have N1 which is powered by Intel
Skylake CPU platform or you have E2
which is CPU platform selected selection
based on availability so let's have N1
selected now this one shows me the
machine type and if you are using your
free tier account then you can start
with correcting a micro machine which is
one virtual CPU core you can go for a G1
small which is one virtual CPU core and
1.7 GB Ram you can even go up to a
high-end machine and then you can
basically see if you are using a free
account how many of these machines you
can use if I would be selecting eight
virtual CPU cores and 32 gigabyte of
memory then it would allow me to create
at least two instances by this
configuration we will use N1 standard
which is one virtual CPU core 3.75 GB
memory now then we can also deploy a
container image to this VM instance if
you would be interested in deploying a
container image let's not get into that
right now here it shows you the port
disk and it shows you dbn gnu 9 Linux 9.
what I would do is I can go for this
distribution or I can choose a Linux
distribution of my choice so here you
have public images you have custom
images you also have snapshots if you
have created backup of your previous
images here I can choose for example
Ubuntu and then I can choose a
particular version so let's go for
Ubuntu 18.04 you can go for the latest
one also 18.04 would be good enough and
here it tells me what is a boot disk so
you have ssds or you have standard
persistent disks now ssds are little
expensive in comparison to your standard
persistent disks or your sdds but then
ssds are faster so as of now we can
choose standard persistent disk as it is
and we can let the gigabyte be 10 now
depending on your requirement you can
increase this you can even add disks
later that's not a problem click on
select now here I have access Scopes so
here I will say allow default access you
can also set access for each API or you
can give full access to all Cloud apis
so that based on your requirement you
can any time change later also we will
also say allow HTTP traffic and we can
also choose allow https traffic that
basically allows me to access this
machine or Services which are HTTP based
accessible from this machine now I can
just click on create however it would be
good to basically enable connectivity to
this machine now we can do that in
different ways one is when you bring up
your machine it will have an SSH access
which you can log in from the cloud
platform here itself or what you can do
is you can create a private and a public
key using some softwares like putty or
puttygen so for example if you do not
have that on your machine you can
download you can just type in download
putty that takes you to the putty.org
page and here you can click on download
putty and scroll down which shows you
for your 64-bit machine which I have in
my case you can download putty.exe which
is basically your SSH internet client to
connect to your machines you can also
use putty gen which will be allowing you
to create a private and a public key
which I have done in my case let me show
you how so what you can do is you can go
to puttygen to begin with and here you
can click on generate now then to have
this key generated just move your cursor
on the top here in the empty space and
that creates your key you can give it a
name so for example let's give a
username I would give hdu now I can give
a password for this one so let me give a
simple password
and what I can also do is I can copy
this public key from here and for my
later usage I can just keep it in my
notepad file which I can use later and
I'll show you when so now we can save
this private key and this will basically
allow me to save my private key I can
choose desktop and I can give it a name
so let's say new key new key and that
will be getting saved in a DOT PPK file
so let's save it and that's done so we
also have our public key and we have
saved our private key now we know that
when you would want to connect using SSH
you need your private keys to the client
and public key also has to be existing
so let's take this public key so let's
do a control a I'm going to copy this
and I'm going to come back to my Google
Cloud console and here you can click on
security so you can click on the
security tab here here scroll down and
just give your public key once you give
that it resolves and shows you the name
and this is good enough so that I can
use my SSH client to connect to this
machine I can click on Create and this
will basically create my VM instance it
will take some time and then your
instance will have an internal IP which
will show up here external IP and it
will also show you options to connect to
these machines so this is my internal IP
this is my external IB which I can use
to connect from a client I can easily
connect from the option here which says
SSH and I can say open in a browser
window I can even open this in a custom
Port I can look at the gcloud command
which you can give from cloud shell or
you can use another SSH so let's first
do a open in browser window and let's
see if this connects so we can easily
connect to our instance the Ubuntu 18
instance which I just set up here easily
in couple of seconds now this is trying
to establish a connection using your SSH
keys and when you do that it also
basically brings up this web browser so
I'm already connected it shows my
username which is my cloud account
username and I have been connected to
this machine here using SSH it did not
ask me for any password and basically
now I can just check what do I have in
my Linux file system right and I can
anytime login as root by doing a sudo Su
for example let's try installing a
package and I can say appkit install
whim or app get installed wget or aptkit
install open SSH and all these packages
are already existing so not an issue now
I can start using this instance I can
just look at the disk what is available
okay now we gave an 10 gigabyte out of
which we see 8.3 gigabyte here for the
dev sda1 and then 1.8 gigabyte available
and you can continue using this machine
this was the easiest way of connecting
to a VM instance using SSH
now what we can also do is I can just
leave this and I will now try to connect
using an external SSH client right and
here you can copy the public IP so when
you want to connect to an instance you
will have to get the public IP also
remember that if you select and stop
this machine which will stop your
billing counter and if you start it
again the internal IP will remain same
but the external IP is the one which
will change
I can obviously select this machine
anytime and I can do a cleanup and I can
delete it I can do a start of the
machine if it is stopped and I can even
import the virtual machine to be used
later so there are different options
which you can always use so this is my
instance and if you would want to look
at the details click on this one C1 and
that should basically allow you to look
at the details so it shows you what is
the instance ID what is the machine type
is it reservation specific what is the
CPU platform what's the zone and all the
other details
at any point of time if you would want
to edit you can always click on edit and
you can change the details as you need
now you can also look at the equivalent
rest command to basically use the rest
API to connect to this instance for now
I have copied the public IP and I would
want to connect it using putty so let's
go in here and let's give the hostname
so I can give Ubuntu I can give my IP
address so that's in my session now I
will click on SSH I would go into
authentication I'll click on browse and
this is where I need to choose the PPK
file so this is the one which we created
new key let's select this
and then I can come back to session I
can even save this and I can call it as
my instance one let's save it and you
can create any number of instances so
you see I have created different
instances here for my Google cloud or
Amazon related instances and I can click
on open it says the service host key is
not cached in the registry that's fine
just click on yes
and basically it says no authentication
method supported now this could be
because we have not enabled your SSH
access so let's look at that so now
let's see if we were trying to connect
using putty what was the issue here so
if I go back to my putty select my
instance load it it says Port 22 I'm
giving the username which is Ubuntu or
sorry that's the wrong username we gave
and that might be the reason we had set
up the user as sdu so let's save this
again and now let's try connecting to
this one and it asks for my password and
you are able to connect this right now
if there was any other issue related to
network connectivity then we could look
at the rules the inbound and outbound
rules which allow us to look into the
machine now we are connected to our
machine here using hdu user I can log in
as root and I can continue working so
not only from the SSH within the cloud
console but you can use an external SSH
client and connect to your machine so
this is your Ubuntu machine and we can
basically look at the space and that
basically confirms we are connecting to
the same machine which shows 8.3
gigabytes here and 1.8 gigabytes here
which we were seeing from the ssh in the
browser now let's close this and let's
go back to our instance page now here
you can always look at the network
details and this will show you your
different kind of rules that is ingress
or egress rules which basically allows
to connect to this machine from an
external network or for this machine to
connect to an external network so here
we have different firewall rules which
shows default allow HTTP that's your
Ingress Rule and it tells you apply to
all it shows me what are the IP ranges
where I can specifically give the ipf my
machine it shows the protocols it shows
what are the different ports which you
have used for these services for example
RDP or SSH which shows 22 you have icmp
HTTP and https now anytime if you would
want to make a change to these rules
that can be done by going into your
network details and say for example you
would want to work on firewall rules so
we have these firewall rules here you
can click on this one so right now we
are looking into the network domain and
we are looking into VPC Network right
and this shows me what are the different
rules we have now if I would want to
create a different firewall rule for a
different protocol I can always click on
create firewall rule I can give it a
name okay I can say what if you would
want to turn on the firewall logs you
can basically say what is the kind of
traffic so Ingress applies to incoming
traffic and egress applies to outgoing
traffic and you can then basically
choose what are the IP ranges from where
you want that connection to be coming in
or to going out you can choose a
particular protocol you can give a
protocol here with comma separated
values and you can create a firewall
rule so this might be required depending
on the services which you are running
wherein you might want to enable your
access to your machine from an external
service or to an external service so you
can always go to network details from
here you can then go into firewall rules
create a firewall rule apply that to
your instance and restart your instance
so as of now we don't need to create any
network details here because my Ingress
or egress rules are already available
right now I can basically then have my
instance stopped and removed so I can
just do a stop or since this is running
the ideal wave would be to do a stop I
could also do a reset now what does
reset do reset does not basically delete
the machine what it does is it does a
cleanup of the machine and it brings it
to the initial state so sometimes we
might have installed certain things on
our machine we would want to clean them
up and at that time reset can be useful
I can basically click on delete right
and I can select this and I can underway
cleanup and this is good always when you
are using a free trial account try to
use different Services play around with
them and then you can clean up so that
you do not waste your free billing
credit right and you can use it for
Meaningful stuff now I have clicked on
delete and within few seconds my
instance which I had created will be
deleted also to remember is if you are
creating multiple instances then you can
connect from one machine to other
machine using SSH by using the private
file so for that we can learn in detail
later so this is just a simple example
of using your compute engine creating
your instances connecting to it from an
internal SSH or from an external SSH
client such as putty where you have
already created your private and public
Keys now I can click on the browser here
and then I can basically come out and I
can basically looking into any
particular service so we just looked
into compute engine right you have
different other options you have
instance groups you have instance
templates for example let's click on
instance templates here and that
basically shows you that you don't have
any instance template and this basically
facilitates say for example you are
working as a admin and you would want to
create an instance template so that
using that you can describe a VM
instance and then you can basically use
this template to create different
instances you can go for soul tenant
nodes you can look for machine images
you can also look at your disks you can
create a snapshots and you can look at
different options here now let's come
back here and let's click on home and
that should take you back to your home
page which basically shows you if a
particular API which you have used
recently which shows me in the graph
here so I can go to the api's overview
right and that basically shows me if
there were any errors if I was using the
compute engine API to basically create a
VM instance and that's why we were
seeing some spike in the graph so this
is a quick demo of using your compute
engine service provided by Google Cloud
wherein you can create VM instances and
use those VM instances for your
application installation for any other
purpose now that we have seen how you
use your Cloud console to create an
instance and also clean it up let's also
understand how you can do using your
command line options and let's see what
it takes or what are the different
commands which you can use to create
your instances
now you can always when when you're
creating an instance you can use compute
engine which Provisions resources to
start the instance so instance basically
has different states which we can see
when we are creating the instance so you
basically start the instance instance
moves into staging that is prepared for
your first boot finally it boots up and
then it moves into running so when you
look at instant states which we will
create instance in C it will basically
have different states such as
provisioning where resources are being
allocated for instance but instances not
yet running then it goes into staging
where resources have been acquired and
instance is being prepared for your
first boot then instance is booting up
and running and if you are stopping an
instance it goes into being stop status
and it would be moved to the terminated
option you can also do a repairing of
instance and finally you can terminate
your instance or clean it up by stopping
and then deleting it now when you say
stopping and resetting an instance you
can stop the instance as I showed
earlier if you no longer need it but if
you need for future use you can just use
the reset option which will basically
wipe the content tense of instance or
any application State and then finally
you can stop it and have it terminated
now when you would want to do that using
your Cloud console or we have seen the
options now let's also see from the
command line tool how you can do it so
here I have the cloud cell which I
brought up from here and I basically
opened it in a new window so command
line tool enables you to easily manage
your compute engine resources in a
friendlier format than using your
compute engine API now gcloud which is
part of cloud SDK is the main command
here and then you can always auto
complete the different options here so
when you would want to create or work on
gcloud you can just type in gcloud here
and then for example I could just say
help to see different options of gcloud
which will show me the different options
which you can use here now we would be
interested in compute instances so I
could also do a gcloud compute
compute instances and then I can
basically say create and then I can do a
help now that should show me different
options which work with gcloud compute
instances create command which is
expecting an instance name so your
Google Cloud SDK which we can set up on
our Windows machine or even on your
Linux machine a set of tools that helps
you to manage resources and applications
hosted on your gcp that is your Google
Cloud platform now here you have options
such as gcloud which I'm showing you
right now you have Gs util and then you
have BQ so that also can be used so you
can set up a Google Cloud compute if
using gcloud now what we can do is if we
are setting up our SDK on our Windows
machine then we would have to do a
gcloud init which basically initializes
your configurations for GE cloud
now here we are using a cloud shell
which was started from the console and
we don't have to basically give your
gcloud init command because it's already
initialized now you can always look at
your default Zone you can look at your
region what is being used all those
things are coming from the metadata
which is being used so for example I
could be looking at the metadata for my
particular project by just doing a g
cloud and then I can say compute and I
would be interested in Project info so I
can just do a project minus info and
then I can do a describe and then I need
my project ID so I can say minus minus
project and I can get my project ID from
here so you can click on this one and
that's my project ID so I can click
click on this one and here I can just do
a right click and if it does not paste
then you can do a control V and then I
can try to look at my project info so
this basically will give me the metadata
which is by default set and we can
always look at what are the regions or
what is the Zone which has been set so
if it has been set so I'm looking at my
details it is showing me my SSH keys and
then it can be using your default region
and your default available zones it also
shows my username and other details so
this is basically to look at the
metadata which is available now at any
point of time I can basically say add
metadata I can basically choose metadata
option and I can say my default region
should be Europe which I was choosing
earlier so I can just bring up this
command again and what I can do is I can
say here where I have gcloud compute
project info where I did a describe
earlier now what I could also do is I
can just say add metadata
and then I can specify what is the
metadata I would be interested in adding
I can then say Google compute
default
region and then I can basically give a
region for example Europe and then I can
say West 3
and if you are not well experienced you
can always do it from console or you can
be doing it from here so I'm giving
Google Cloud compute Google compute
default region then I can also give a
Google compute default Zone
and then I can pass in a value for
default zone so I can say Europe West 3
and then basically I can give an
availability zone so if it is basically
giving me some error where I'm trying to
pass in these values so I can just give
it this way and then it basically says
that if you can look into particular
help to see what is the command which
you would have to do now I can basically
do a g Cloud init here initial
configuration and this basically says
that it is initializing my default
configuration it says re-initialize this
configuration from cloud shell you want
to create a new configuration so if I
had updated my metadata then basically I
could just do a cloud in it and I could
be re-initializing my default
configuration or properties which I have
passed constant so as of now I will not
activate or change the default region
let's go for a simple way in which you
can create your compute engine so for
example I'll just say one and it is
reinitializing it asks me what is the
username and let's select that it says
what is the project and let's select
that and do you want to configure a
default compute region and zone right
and I will just say Yes And basically
then it shows me different options which
we have here so there are too many
options here and then here we were
interested in Europe West 3 so let's
choose 21 right and then that basically
allows me to choose my region and my
zone so it also gives you some
specification so it says your project
default compute Zone has been set to
Europe West 3A you can change it by
running gcloud config set and you can
give a compute zone right so I can
always give these commands I can get
help information here so I can just say
gcloud config set and I can be
specifying the compute zone so I can say
compute slash Zone and I can give a Zone
I can say compute slash region and I can
give a region name if I would want to do
it or the easier ways like what I did
right now so you can basically do a
command gcloud in it and then basically
it allows you to to change your
configuration or set default things here
you can all the times you can say gcloud
config unset to basically remove a
compute zone or a compute region now
there are different ways so if you were
working on a Linux machine you could
always use an export command something
like this so you could do export compute
sorry Cloud
SDK and then you can say compute
underscore Zone and then give your Zone
name or compute underscore region and
give your region or you could add it in
your bash RC file right now that is when
you have your Cloud SDK set up on your
Linux machine or on Windows machine and
you would want to specifically set a
Zone and a region for all your compute
related resources so we don't need to do
that the default settings are already
given here right and what we can do is
we can start by quickly looking into
gcloud compute instances options so I
can say gcloud compute instances and
then I can just do a list at any point
of time if you would need help you can
just do a gcloud compute and then say
minus minus help or I could just do a
gcloud compute and that shows me
different options which I have here from
which I use instances I can always type
instances and again hit enter and it
shows me different options what you
would want to do so I can initially just
type list which should show me what are
the list or what are the instances
available and as of now we don't have
any instances I can always do a list and
then I can specify minus minus format
and then try to get the information in a
Json format or EML or minus minus format
text
so you can always do a list you can do a
filter so there are different options
with your list and you can you can try
doing a help here and that basically
shows you what are the options so for
example if I do a help and it shows me
with list what are the things you can do
so you can give a name you can give a
regular expression you can say in
particular Zone you can use a minus
filter so there are different commands
which are available and you can always
find all those options here as I showed
you earlier so now what we would be
interested in is basically going for
your compute instances I can always do a
SSH and I can create an instance I can
add and remove metadata right for my
instances by giving a particular Zone by
giving a particular region and if you
would want to do that now here what we
can do is we can can basically create an
instance by just giving a name to that
particular instance and we can then go
back to our console and see what has it
done so I can say here create so that's
an option let's see what does the create
do so it says okay you are giving a
create option but you would have to give
a name so let's say let's call it E1 and
that will be the name of my instance and
this one is an easy command from your
Cloud shell which basically creates an
instance you see the Zone which has been
set it is the standard machine type it
is not preemptable it has an internal IP
and it has an external IP now I have
created an instance and here I can just
go back and then I can just do a Refresh
on this page and that already shows me
the instance which I have created and
then you can use the same method to
connect to it using an SSH so I have an
instance created from my cloud shell and
what I can do is I can basically look
into instances and see what are the
different options with instance so if we
did a create right you have an option
delete we did a create you have an
option delete you can be in deleting the
instance from the command line or you
can use other options so you can do a
list you can do a stop you can start so
I can basically do a stop and let's say
even let's go ahead and do a stop
commands are pretty easy here to
remember or you can always use the help
option and now I'm trying to stop the
instance and then basically I can go
ahead and delete it so this is a simple
way where I created a instance from the
command line or from the console which I
showed earlier and similarly you can be
working with other options so now I have
created stop the instance let's do a
listing to see if my instance shows up
it shows up right and it says the status
is terminated so you have stopped it
it's in the terminated status and now
what I can do is I can go ahead and
delete it
so it says this will be lost are you
sure you would want to continue just say
yes and that should take care of
deleting your instance
so this is a quick demo on creating your
instance we have already seen how you
can connect to these instances using SSH
now that we have created instances let's
also see how to use Google Cloud's
storage service which can be used to
load data or upload data so for this we
will have to look into your Google cloud
storage options and here you can look in
cloud storage so let's go back here on
the top which shows me different
Services which we have here and let's
look into storage so this one shows me
your storage option now you can click on
browser and that basically shows me your
storage browser which shows me on the
top you have options for creating a
bucket now Google Cloud Storage allows
you to store any kind of data here the
easiest and the simplest way would be by
using Cloud console although you can
again use cloud shell and in that you
can use your GS util command and GS util
basically has different options where
you can be using say for example I would
want to work on buckets so I can be
using MB and then I can use my command
line option to create buckets I can put
in data there I can browse it and I can
access the data from command line so
let's create a bucket here let's click
on this let's give it a name so let's
say my data important that's the name
now I can click on continue straight
away I can look into all of these
options if I'm interested in so you can
basically be looking at your monthly
cost estimate in the beginning now I can
click on continue or you can say choose
where to store your data and this one
shows you different options so you have
region specific click
so let's also give the bucket name with
all lower case that's what is required
yeah so coming back to the location type
you can choose multi-region which
basically allows High availability so
your bucket or your storage option will
be accessible across regions you can
also give dual region and high
availability and low latency across two
regions I can also say region specifics
or for our use case we can just keep
region specific which can keep our cost
low but in business use cases you would
be going for multi-region now here you
have location and I would again go for
say Europe and let's choose Europe
Wesley Frankfurt it is always a good
practice that when you create your
instances when you create your storage
or you use different Services try to
have a geographical region Chosen and
then you would try to put things on your
services within the particular region
within a particular Zone unless you
would want to make it accessible and
available across regions
now I can then choose a default storage
class so when you say default storage
class there are different storage class
and each one is for a different use case
so you have standard which is best for
short-term storage and frequently
accessed data you have near line so this
is basically best for backups and data
access less than once a month you can
also have code lines so these are
basically like your cold storage or
freezing storage which you might have
heard generically as terms so you can
choose one of these storage classes
depending on what will be the use case
for this particular storage bucket so
let it be standard now how to control
access to objects so you can basically
say specify access to individual objects
by using object level permissions now
you can give permissions to the bucket
you can just say uniform access to all
objects in the bucket by using only
Bucket Level permissions so you can
choose that you can also go into
advanced settings and here you can see
different ways in which you can have
configuration set up now you can also
have a retention policy to specify the
minimum duration where that this
bucket's object must be protected from
deletion and you can set a retention
policy will not get into all that we
will just first try creating a bucket so
just click on Create and that should
create your bucket wherein I have my
bucket now I can click on overview to
see the details what is the region which
region it belongs what is the storage
class anytime you can always click on
edit bucket and make some changes here
you can look at the permissions so
bucket uses fine grained access allowing
you to specify access to individual
objects and then you can basically look
at who has access to this so in my case
editors of the project they basically
are the bucket owners owners of project
viewers of the project right and you can
basically choose what kind of access you
need so for this you can always go into
cloud storage and then you can decide
what kind of access you would want to
give whether that's a storage admin it's
an object admin object Creator object
viewer and so on you can always look at
storage Legacy
and for any other services depending on
what apis you have enabled right you can
always control your permissions so right
now it is storage Legacy bucket reader
and that's fine and this one is bucket
owner and then might be I was using some
other services like data proc which uses
cloud storage and that's why I have
given data's proc service agent also so
these are some of the members you can
remove you can view by specific members
you can view by roles so what are the
different roles which have access to
this so it says storage Legacy bucket
owner there are two owners based on this
particular project so these are Auto
controlled but then you can add or
remove machines you can save storage
cost by adding a life cycle rule to
delete objects after the duration of
current retention policy so you can add
different policies and you can basically
control your bucket now my bucket is
already created right so I can go back
and then I see my bucket is already here
I can click on this option and you can
anytime edit the bucket permissions you
can edit the labels the default storage
class you can just go ahead and delete
the bucket you can export it to Cloud
Pub sub so basically if you would want
to have the content from this bucket
being accessible in a message queuing
system you can go for Pub sub you can
process with Cloud functions and you can
scan with cloud data loss prevention so
there are different options which are
available we can always select this
bucket and delete it now I can click on
the bucket and that basically shows me
different ways in which I can upload
some data here so I can just click on
upload files and here then I can choose
some files so for example I'll go in
here I'll go into data sets and I have
different data sets so for example let's
choose this one which is a CSV file and
then I'm just uploading this to my
storage right it's as simple as this so
you can drag and drop your files or you
can just upload your files and my file
is uploaded here now I can basically
edit permissions for this one right so
this is in the bucket so anyone who has
access to this bucket can basically
download this file it can they can copy
move or rename this you can export it
and you can look at the permissions of
this file so let's look at edit
permissions and it says
for this project whoever is the owner it
has access I have also given a specific
user my Gmail ID and I have given access
so at any point of time you can just say
add item and then you can start giving
different kind of accesses so let's
click on cancel here now my data is
already uploaded into this particular
bucket and that's a simple usage of your
cloud storage now what we can also do is
I can basically select this and I can
delete it I can also create specific
folders and then basically load data
into it I can click on this one I can
just do a download and then I can
download it anywhere on my machine so
let's go to desktop and let's download
this file so we not only uploaded some
content in the bucket but we also
downloaded that in a different location
which will be accessible now what I
could have also done is I could have
created a folder here and I could have
specified say for example immediate
immediate data
okay and I'm clicking on this one so
that's my immediate data I can click on
this one and now I can upload my data
specifically to this particular folder
by using the same mechanism you can just
do upload file let's choose air
passengers let's say open and my file
will be uploaded so you can always
choose what is the retention expiry date
for this one so as of now there is
nothing right but you can be deleted
objects can be deleted or modified until
there is a minimum duration you can
control all of that you can basically
download it right you can copy it and
you can move it and rename it so this is
a simple example where I created a
particular bucket right now if for
example you click on this transfer so it
says cloud storage data transfer
products have moved you can now find
storage transfer service on-premise data
and transfer Appliance in new data
transfer section right so you can always
go back
to cloud storage you can also look at
transfer options which are mainly when
you are using your on-premise service
and you would want to upload data in a
Google Cloud right so this is my bucket
here now I can go to Cloud console and
here what I can do is if I would be
interested in working so I can just say
GS util right and then we have for
example let's try a help okay and that
basically shows me my GS util now you
can always go to Quick Start gsutil tool
and this is one more way wherein
basically you can do it from the command
line so working with buckets so you can
do a GS util MB minus B and then on a
particular Zone if you would be
interested in and then you can say GS
colon and give a name so my awesome
bucket this is how you will create a
bucket which will show you it is
creating a bucket and then basically you
can upload an image or some data from
internet you can just do a w get
download the file and then you can just
do a GS util copy command and that
basically will pick up the file from
your Cloud cell location put it in your
bucket once you have done that you can
always do a copy right so here you are
doing a copy from your local machine
that is your Cloud cell machine to your
bucket and you can do the reverse of
that that is you can do a copy and you
can point your bucket and the file and
download it to your desktops so you can
just download that using the command
line and you can copy the object to a
folder in the bucket so that is by
creating a particular folder and you are
pushing the file into a particular
bucket you can list the contents of a
bucket using LS and then give your
bucket name so GS colon so we can just
try this one to be simple and rest you
guys can try so so here you can just do
a GS util you can do a LS GS colon slash
slash and then I need to give my bucket
name so in my case the bucket name was
my data important so let's try that so
let's save my data important and I'm
trying to list the bucket and the
content it has right and then you have a
folder and then you can look into the
folder and look into the files so this
is a simple quick option wherein you can
use your Cloud cell you could be using
your Google Cloud SDK from your local
machine or you can be using Cloud
console to create a bucket upload some
data on it download the data see if the
data is accessible and that basically
shows you the power of cloud storage
where you can easily upload any kind of
data now what we can also do is as
you're using a free account or even a
paid account unless and until you would
want to keep the bucket you can select
it and you can just go ahead and delete
it so this will basically ask you to key
in the bucket name and let's say my data
important you need to confirm your
bucket name click on confirm and you
could have done that using gsutil and a
delete command from the command line
that is from cloud cell so we have now
created a bucket we have uploaded some
data into it we saw how you can download
it or create a folder and upload some
specific data into it and also you can
do the same thing using GS util tools
wherein you can list your bucket create
your buckets delete it create some
folders into it and do everything from
the command line so this is in simple
way you can use your gcp where either
you can spin up your machines or you can
use cloud storage to basically use a
storage or a easy to use instance on
Google Cloud platform in this video we
will compare and contrast AWS Azure and
gcp based on a few related Concepts
around these cloud computing platforms
it will help us understand the
functioning of these top Cloud platforms
and will also let us figure out the
individuality of each one of them but
before starting with the comparison
let's have a quick introduction of AWS
versus Azure versus gcp so let's get
started
Amazon web services or AWS is a cloud
computing platform that manages and
maintains hardware and infrastructure
reducing the expense and complexity of
purchasing and running resources on-site
for businesses and individuals
these resources are available for free
or for a fee per usage
Microsoft Azure is a cloud computing
service that offers a collection of
Cloud Computing Services for building
testing deploying and managing
applications in the cloud including
remotely hosted and managed versions of
Microsoft Technology
Google Cloud platform offers a variety
of Cloud Computing Services for building
deploying scaling monitoring and
operating a cloud
the services are identical to those that
power Google products such as Google
search Gmail YouTube and Google Drive
now let's move on to the comparison
between AWS Azure and gcp
we will be comparing them based on a few
major parameters like origin
service integration
availability Zone
Cloud tools like compute
storage
networking
market share
pricing
and at last who uses them
now let's move ahead and start with the
first comparison origin
in the year 2006 Amazon web services or
AWS was introduced to the market
and in the year 2010 Azure launched its
services whereas on the other hand gcp
was established in the year 2008.
from the start AWS has been supportive
of the open source concept but the open
source Community has a tense
relationship with azure
on the other hand gcp similar to AWS
provides Google cloud with managed open
source services that are tightly linked
AWS offers services on a large and
complex scale that could be manipulated
but Azure support is comparatively low
quality whereas gcp's monthly support
price is almost a hundred and fifty
dollars for the silver class which is
the most basic of services and is quite
expensive
now let's move on to the service
integration of these Cloud platforms
service integration is a set of tools
and technology that connects different
applications systems repositories and
data and process interchange in real
time
AWS makes it simple for users to combine
services such as Amazon ec2 Amazon S3
Beanstalk and others and on the other
hand Azure allows customers to
effortlessly combine Azure VMS as your
app service SQL databases and other
services whereas users can utilize gcp
to combine services such as compute
engine cloud storage and Cloud SQL now
that we know briefly about all these
Cloud platforms let's have a look at the
availability zones of these platforms
because AWS was the first in the cloud
domain they have had more time to build
and extend their network but Azure and
gcp both have various locations around
the world
but the distinction is in the amount of
availability zones they have AWS now
offers 66 availability zones with an
additional 12 on the pipeline close to
it Azure is available in 140 countries
and is available in 54 regions
throughout the world but Google Cloud
platform is now available in 20 Global
areas with three more on the way
now let's move on to the next important
factor which is tools
now let's move ahead and have a look at
the first feature which is compute
elastic compute cloud or ec2 is aws's
compute service which offers a wide
range of features including a large
number of instances support for both
windows and Linux high performance
Computing and more
Azure on the other hand as virtual
machines is Microsoft azure's core
cloud-based compute solution
it includes Linux Windows server and
other operating systems as well as
better security and Microsoft program
integration
in comparison to its competitors
Google's Computing Services catalog is
somewhat smaller compute engine that
companies principal service offers
custom and pre-defined machine types per
second invoicing Linux and Windows
support and carbon neutral
infrastructure that uses half the energy
of traditional data centers
within the compute category Amazon's
different container services are gaining
prominence it has Docker kubernetes and
it's also fargate service which
automates server and cluster management
when using containers as well as other
alternatives
Azure unlike AWS uses virtual machine
scale sets of two container services
Azure container services is based on
kubernetes and container service uses
Docker Hub and Azure container registry
for management
for Enterprises interested in deploying
containers Google offers the kubernetes
engine and it's also worth noting that
Google was significantly involved in the
kubernetes project providing an
extensive knowledge in this field
now let's move on to the next parameter
of comparison which is storage
simple storage service for object
storage elastic block storage for
persistent block storage and elastic
file system for file storage are among
aws's storage offerings
block storage for rest based object
storage of unstructured data Q storage
for a large volume workload file storage
and disk storage are among Microsoft
Azure core storage services
gcp offers an increasing number of
storage options its unified object
storage service cloud storage also has a
persistent disk option relational
database service or RDS dynamodb no SQL
database elastic and memory data store
redshift data warehouse Neptune graph
database and database migration service
are all SQL compatible databases offered
by Amazon
the database choices in Azure are
specifically wide SQL database mySQL
database and postgre SQL database are
the three SQL based choices
now when it comes to databases gcp
offers the SQL based Cloud SQL and Cloud
spanner a relational database built for
Mission critical workloads now the next
parameter is networking
AWS uses Amazon virtual private cloud or
VPC
on the other hand Azure uses Azure
virtual Network or v-net
and gcp uses Cloud virtual Network now
let's move on to another factor which is
market share and pricing
all these cloud services are based on
comparative pricing strategies which
means you need to pay on the basis of
its usage
according to Canalis the worldwide Cloud
Market Rose 35 percent to 41.8 billion
in the first quarter of 2021. AWS
accounts for 32 percent of the market
with Azure accounting for 19 and Google
accounting for seven percent
on one hand Amazon charges on a yearly
basis and on the other hand Microsoft
Azure and Google services charge on a
minute basis and also all of them
provide you a standard price for you to
access these services
AWS charges roughly 69 per month for a
very basic instance with two virtual
CPUs and eight gigabytes of RAM
and AWS largest instance with 3.84 TB of
RAM and 128 vcpus will set you back
roughly
3.97 per hour but in Azure the same type
of instance one with two vcpus and eight
gigabytes of RAM cost roughly 70 U.S per
month and azure's largest instance has
3.89 TB of RAM and 128 virtual CPUs and
it cost about
6.79 per hour
compared to AWS gcp will supply you with
the most basic instance which includes
two virtual CPUs and eight gigabytes of
RAM for 25 percent less and as a result
it will set you back roughly 52 dollars
every month and the largest instance
that includes is 3.75 TB of RAM and 160
vcpus and it will cost you about 5.32
cents per hour
Amazon other than this also provides
spot instances reserved instances and
dedicated hosts where you can look for
multiple offers and discounts
but for Azure it provides special prices
to developers based on situations or
even Azure hybrid benefit which benefits
your organization up to 40 percent if it
uses Microsoft software in their data
centers
whereas Google offers quite assorted
pricing to its customer compared to the
other two
it gives you sustained use discounts
which activate if you use the same
instance for a month
the printable instance which is very
similar to Amazon's spot instances but
one thing is common in all three cloud
services which is that they all offer
long-term discounts
now let's have a look at the last
comparison which is the companies that
are using them because AWS is the oldest
player in the cloud business it has the
largest user base and Community Support
as a result AWS has the largest number
of high-profile and well-known clients
including Netflix Airbnb Unilever BMW
Samsung missinga and others
with time Azure is getting a large
number of high-profile customers Azure
currently boasts around 80 percent of
Fortune 500 firms as customers Johnson
Controls polycom Fujifilm HP Honeywell
apple and others are among its key
clients
Google Cloud on the other hand uses the
same infrastructure as Google search and
YouTube and as a result many high-end
Enterprises trust Google Cloud HSBC
PayPal 20th Century Fox Bloomberg
Domino's and other are among Google
Cloud's many clients and whether you are
a fresher or an ID expert with some
experience you can improve your skills
in cloud computing by getting the proper
training and certification on that note
we present postgraduate program in cloud
computing by simply love the
postgraduate program in cloud computing
design collaboration with Caltech ctme
helps you become an Azure AWS and gcp
expert this in-depth cloud computing
certification course lets you master key
architecture principle and develop the
skills needed to become a cloud
commanding expert benefits or this
postgraduate program include Caltech
CDMA postgraduate certificate enrollment
and simple and services receive up to 30
CEUs from caltex atme Simply learn job
assist help you to get noticed by top
hiring companies attend master classes
from
is led by industry expert Hands-On
project and integrated Labs online
convocation by Celtic ctme program
director 40 plus hands-on experience
projects and integrated Labs caption
projections four domains caltex ctme
Circle membership you can also fast
check your career with this class
Computing certification and its in-depth
course curriculum which covers the key
concept of azure AWS and gcp platform
and services such as AWS Lambda Amazon
S3 Azure app services and many more
these programs also cover the essential
skills to become an expert in cloud
computing and you can consider these
programs because it provides you with
the certification from caltex atme a
well-reputated university in United
States so check out the link mentioned
in the description box below so without
any further Ado let's get started hey
everyone welcome to Simply learns
YouTube channel today we will be
discussing the cloud computing interview
questions but before that let me share a
few facts with you according to O'Reilly
more than 90 percent of the
organizations are expected to increase
their Cloud infrastructure in 2021 the
upshot is that the majority of cloud
users plan to scale their business in
the cloud rather than reducing their
Cloud usage share suppose you are very
much interested in Cloud industry then
you have come to the right place welcome
to this video on cloud computing
interview questions in this video we
will learn everything about AWS and
Azure in detail our interview questions
are divided into three segments beginner
level intermediate level and advanced
level by the end of this video I can
assure you that all your AWS and Azure
related queries would have been answered
for this training with me I have our
experienced AWS and Azure expert Sam who
will take you through the various topics
of azure so let's get started with an
exciting video on cloud computing
interview questions before we begin
please make sure to get subscribed to
our YouTube channel and hit that Bell
icon to never miss an update from Simply
learn hi there I'm Samuel and I'm here
to walk you through some of the AWS
interview questions which we find are
important and our hope is that you would
use this material in your interview
preparation and be able to crack that
cloud interview and step into your dream
Cloud job by the way I'm and Cloud
technical architect trainer and an
interview panelist for cloud Network and
devops so as you progress in watching
you're going to see that these questions
are practical scenario based questions
that tests the depth of the knowledge of
a person in a particular AWS product or
in a particular AWS architecture so why
wait let's move on all right so in an
interview you would find yourself with a
question that might ask you define and
explain the three basic types of cloud
services and the AWS products that are
built based on them see here it's a very
straightforward question just explain
three basic types of cloud service and
when we talk about basic type of cloud
service it's compute obviously that's a
very basic service storage obviously
because you need to store your data
somewhere and it's working that actually
connects a couple of other services to
your application this basic will not
include monitoring these basic will not
include analytics because they are
considered as optional they are
considered as Advanced Services you
could choose a non-cloud service or a
product for monitoring of and for
analytics so they're not considered as
basic so when we talk about Basics they
are compute storage and networking and
the second part of the question says
explain some of the AWS products that
are built based on them of course course
compute ec2 is a major one that's that's
the major share of the compute resource
and then we have platform as a service
which is elastic bean stock and then
function as a service which is Lambda
Auto scaling and light cell are also
part of compute services so the compute
domain it really helps us to run any
application and the compute service
helps us in managing the scaling and
deployment of an application again
Lambda is a compute service so the
compute service also helps in running
event initiated stateless applications
the next one was storage a lot of
emphasis is on storage these days
because if there's one thing that grows
in a network on a daily basis that
storage every new day we have new data
to store process manage so storage is
again a basic and an important cloud
service and the products that are built
based on the storage service is our S3
object storage Glacier for archiving EBS
elastic block storage as a drive
attachment for the ec2 instances and the
EFS file share for the ec2 instances so
the storage domain helps in the
following aspects it holds all the
information that the application uses so
it's the application data and we can
also archive old data using storage
which would be Glacier and any object
files and any requirement for Block
storage can be met through elastic Block
store and S3 which is again an object
storage talking about networks it's just
not important to answer the question
with the name of the services and the
name of the product it'll also be good
if you could go in depth and explain how
they can be used right so that actually
proves you to be a person knowledge
tangible enough in that particular
service or product so talking about
networking domain VPC networking can't
imagine networking without VPC in in the
cloud environment especially in AWS
Cloud environment and then we have Route
53 for domain resolution or for DNS and
then we have cloudfront which is an edge
caching service that helps customers get
our customers to read their application
with low latency so networking domain
helps with some of the following use
cases it controls and manages the
connectivity of the AWS services within
our account and we can also pick an IP
address range if you're a network
engineer or if you are somebody who
works in networks or are planning to
work a network you will soon realize the
importance of choosing your own IP
address for easy remembering so having
an option to have your own IP address in
the cloud own range of IP address in the
cloud it really helps really really
helps in Cloud networking the other
question that get asked would be the
difference between the availability Zone
and the region actually the question
generally gets asked so to test how well
you can actually differentiate and also
correlate the availability Zone and the
region relationship right so a region is
a separate geographic area like the
us2s-1 I mean which represents North
California or the AP South which
represents Mumbai so regions are a
separate geographic area on the contrary
availability Zone resides inside the
region you shouldn't stop there you
should go further and explain about
availability zones and availability
zones are isolated from each other and
some of the services will replicate
themselves within the availability zone
so availability Zone does replication
within them but regions they don't
gender early do replication between them
the other question you could be asked is
what is auto scaling what do we achieved
by Auto scaling so in short Auto scaling
it helps us to automatically provision
and launch new instances Whenever there
is an demand it not only helps us
meeting the increasing demand it also
helps in reducing the resource usage
when there is low demand so Auto scaling
also allows us to decrease the resources
or resource capacity as per the need of
that particular R now this helps
business in not worrying about putting
more effort in managing or continuously
monitoring the server to see if they
have the needed resource or not because
Auto scaling is going to handle it for
us so business does not need to worry
about it and auto scaling is one big
reason why people would want to go and
pick a cloud service especially in AWS
the ability to increase and Shrink based
on the need of that art that's how
powerful is auto scaling the other
question you could get asked is what's
geo-targeting in Cloud front now we know
that cloudfront is caching and it caches
content globally in the Amazon caching
service Global wide the whole point is
to provide users worldwide access to the
data from a very near us server possible
that's the whole point in using or going
for cloudfront then what do you mean by
Geo targeting geotargeting is showing
customer and specific content based on
language we can customize the content
based on what's popular in that place we
can actually customize the content the
URL is the same but we could actually
change the content a little bit not the
whole content otherwise it would be
dynamic but we can change the content a
little bit specific a file or a picture
or a particular Link in a website and
show customized content to users who
will be in different parts of the globe
so how does it happen cloudfront will
detect the country where the viewers are
located and it will forward the country
code to the origin server and once the
origin server gets the specialized or a
specific country code it will change the
content and it will send to the caching
server and it get cached there forever
and the user gets to view a Content
which is personalized for them for the
country they are in the other question
you could get asked is the steps
involved in using cloud formation or
creating a cloud formation or a backing
up an environment within cloud formation
template we all know that if there is a
template we can simply run it and it
Provisions the environment but there is
a lot more going into it so the first
step in moving towards infrastructure as
a code is to create it the cloud
formation template which as of now
supports Json and yaml file format so
first create the cloud formation
template and then save the code in an X3
bucket S3 bucket serves as the
repository for our code and then from
the cloud formation call the file in the
S3 bucket and create a stack and now
cloud formation uses the file reads the
file understands services that are being
called understands the order understands
how they are connected with each other
cloud formation is actually an
intelligent service it understands the
relation based on the code it would
understand the relationship between the
different services and it would set an
order for itself and then would
provision the services one after the
other let's say a service has a
dependency and the dependent service the
other service which this service let's
say Service A and B service B is
dependent on service a let's say cloud
formation is an intelligent server
service it would provision the resource
a first and then would provision
resource B so what happens if we inverse
the order if we inverse the order
resource B first gets provision and
because it does not have dependency
chances that the cloud formation's
default behavior is that if something is
not provisioned properly if something is
not healthy it would roll back chances
that the environment provisioning will
roll back so to avoid that cloud
formation first Provisions all the
services that has or that's dependent on
that's depended by another service so it
Provisions those service first and then
Provisions the services that has
dependencies and if you are being hired
for a devops or you know if the
interviewer wanted to test your skill on
a systems aside this definitely would be
a question in his list how do you
upgrade or downgrade a system with near
zero downtime now everybody is moving
towards zero downtime or near zero
downtime all of them want their
application to be highly available so
the question would be how do you
actually upgrade or downgrade a system
with near zero downtime now we all know
that I can upgrade an ec2 instance to a
better ec2 instance by changing the
instance type stopping and starting but
stopping and starting is gonna cause a
downtime right so that's you should be
answering or you shouldn't be thinking
in those terms because that's the wrong
answer specifically the interviewer
wants to know how do you upgrade a
system with zero downtime so upgrading
system with zero downtime it includes
launching another system parallelly with
the bigger ec2 instance type over the
bigger capacity and install all that's
needed if you are going to use an Ami of
the old machine well and good you don't
have to go through installing all the
updates and installing all the
application from the Ami once you have
launched it in a bigger instance locally
test the application education to see if
it is working don't put it on production
yet test the application to see if it is
working and if the application works we
can actually swap if your server is
behind and behind a Route 53 let's say
all that you could do is go to Route 53
update the information with the new IP
address new IP address of the new server
and that's going to send traffic to the
new server now so the cutover is handled
or if you're using static IP you can
actually remove the static IP from the
old machine and assign it to the new
machine that's one way of doing it or if
you are using elastic Nik card you can
actually remove the new card from the
old machine and attach the new cards to
the new machine so that way we would get
near zero downtime if you are hired for
an architect level you should be
worrying about cost as well along with
the technology and this question would
test how well you manage costs cost so
what are the tools and techniques we can
use in AWS to identify and correct
identify and know that we are paying the
correct amount for the resources that we
are using or how do you get a visibility
of your AWS resources running one way is
to check the billing there's a place
where you can check the top services
that were utilized it could be free and
it could be paid Service as well top
services that can be utilized it's
actually in the dashboard of the cost
Management console so that table here
shows the top five most used services so
looking at it you can get it all right
so I'm using a lot of storage I'm using
a lot of ec2 why is storage high you can
go and try to justify that and you will
find if you are storing things that
should be storing and then clean it up
why is compute capacity so high why is
data transfer so high so if you start
thinking in those level levels you'll be
able to dig in and clean up unnecessary
and be able to save your bill and there
are cost Explorer services available
which will help you to view your usage
pattern or view your spending for the
past 13 months or so and it will also
forecast for the next three months now
how much will you be using if your
pattern is like this so that will
actually help and will give you a
visibility on how much you have spent
how much you will be spending if the
trend continues budgets are another
excellent a way to control cost you can
actually set up budget all right this is
how much I am willing to spend for this
application for this team or for this
month for this particular resource so
you can actually put a budget Mark and
anytime it exceeds anytime it's nearing
you would get an alarm saying that well
we're about to reach the allocated
budget amount stuff like that that way
you can go back and know and you know
that how much the bill is going to be
for that month or you can take steps to
control bill amount for that particular
month so AWS budget is another very good
tool that you could use cost allocation
tags helps in identifying which team or
which resource has spent more in that
particular month instead of looking at
the bill as one list with no
specifications into it and looking at it
as an expenditure list you can actually
break it down and tag the expenditure to
the teams with cost allocation tags the
dev team has spent so much the
production team has spent so much the
training team has spent more than the
dev and the production team why is that
now you'll be able to you know think in
those levels only if you have cost
allocation tags now cost allocation tags
are nothing but the tags that you would
put when you create a result so for
Production Services you would put as a
production you would create a production
tag then you would associate that
resources to it and at a later point
when you actually pull up your bill
that's going to show a detailed a list
of this is the owner this is the group
and this is how much they have used in
the last month and you can move forward
with your investigation and encourage or
stop users using more services with the
cost allocation tax the other famous
question is are there any other tools or
is there any other way of accessing AWS
results other than the console console
is GUI right so in other words other
than GUI how would you use the AWS
resource and how familiar are you with
those tools and Technologies the other
tools that are available that we can
leverage and access the AWS resource are
of course fully you can configure Puri
to access the AWS resources like log
into an ec2 instance an ec2 instance
does not always have to be logged in
through the console you could use putty
to log into an ec2 instance and like the
jump box like the proxy machine and like
the Gateway machine and from there you
can actually access the rest of the
resources so this is an alternative to
the console and of course we have the
AWS CLI in any of the Linux machines or
Windows machines we can install so
that's 0.23 and 4 we can install AWS CLI
for Linux Windows also for Mac so we can
install them and from there from your
local machine we can access run AWS
commands and access provision monitor
the AWS resources the other ones are we
can access the AWS resource
programmatically using AWS SDK and
Eclipse so these are a bunch of options
we have to use the AWS resource other
than the console if you're interviewed
in a company or buy a company that
focuses more on security and want to use
AWS native services for their security
then you will come across this Quest
Justin what services can be used to
create a centralized logging solution
the basic Services we could use are
cloudwatch logs store them in S3 and
then use elasticsearch to visualize them
and use Kinesis to move the data from S3
to elasticsearch right so log management
it actually helps organizations to track
the relationship between operational and
security changes and the events that got
triggered based on those logs instead of
logging into an instance or instead of
logging into the environment and
checking the resources physically I can
come to a fair conclusion by just
looking at the logs every time there's a
change the system will scream and it
gets tracked in the cloud watch and then
Cloud watch pushes it to S3 Kinesis
pushes the data from S3 to elasticsearch
and I can do a time-based filter and I
would get an a fair understanding of
what was going on in the environment for
the past one hour or whatever the time
window that I wanted to look at so it
helps in getting a good understanding of
the infrastructure as a whole all the
logs are getting saved in one place so
all the infrastructure logs are getting
saved in one place so it's easy for me
to look at it in an infrastructure
perspective so we know the services that
can be used and here are some of the
services and how they actually connect
to each other it could be logs that
belongs to One account it could be logs
that belongs to multiple accounts it
doesn't matter you know those three
services are going to work fairly good
and they're gonna inject or they're
gonna like suck logs from the other
accounts put it in one place and help us
to monitor so as you see you have Cloud
watch here that actually tracks the
metrics you can also use cloud trail if
you want to log API calls as well push
them in an S3 bucket so there are
different types of blog flow logs are
getting captured in an instance
application law odds are getting
captured from the same VPC from a
different VPC from the same account from
a different account and all of them are
analyzed using elasticsearch using the
kibana client so step one is to deploy
the ECS cluster step two is to restrict
access to the ECS cluster because it's
valid data you don't want anybody to put
their hands and access their data so
resting access to the ECS dashboard and
we could use Lambda also to push the
data from cloud watch to The
elasticsearch Domain and then hibana is
actually the graphical tool that helps
us to visualize the logs instead of
looking at log as just statements or a
bunch of characters a bunch of files
kibana helps us to analyze the logs in a
graphical or a chart or a bar diagram
format again in an interview the
interview is more concerned about
testing your knowledge on AWS security
products especially on the logging
monitoring even management or Incident
Management then you could have a
question like this what are the native
AWS security logging capabilities now
most of the services have their own
logging in them like have their own
logging like S3 S3 has its own login and
cloudfront has its own logging DS has
its own logging VPC has its own logging
in additional there are account level
logins like cloudtrail and AWS config
services so there are variety of logging
options available in the AWS like Cloud
soil config cloudfront redshift logging
RDS logging VPC flow logs S3 object
logging S3 access logging stuff like
that so we're going to look at two
servers in specific cloudtrail now this
cloud trail the very first product in
that picture we just stopped the
cloudtrail provides an very high level
history of the API calls for all the
account and with that we can actually
perform a very good security analysis a
security analysis of our account and
these logs are actually delivered to you
can configure it they can be delivered
to S3 for long time archivals and based
on a particular event it can also send
an email notification to us saying hey
just got this error thought I'll let you
know stuff like that the other one is
config service now config service helps
us to understand the configuration
changes that happened in our environment
and we can also set up notifications
based on the configuration changes so it
records the cumulative changes that are
made in a short period of time so if you
want to go through the lifetime of a
particular resource what are the things
that happened what are the things it
went through they can be looked at using
AWS config all right the other question
you could get asked is if you know your
role includes taking care of cloud
security as well then the other question
you could get asked is the native
services that Amazon provides or to
mitigate DDOS which is denial of service
now not all companies would go with
Amazon native services but there are
some companies which want to stick with
Amazon native Services just to save them
from the headache of managing the other
softwares or bringing in another tool a
third-party tool into managing DDOS they
simply want to stick with Amazon
proprietary Amazon native services and a
lot of companies are using Amazon
service to prevent DDOS denial of
service now denial of service is if you
already know what denial of service is
well and good if you do not know then
let's know it now denial of service is a
user trying to or maliciously making
attempt to access a website or an
application the user would actually
create multiple sessions and he would
occupy all the sessions and he would not
let legimate users access the servers so
he's in turn denying the service for the
user a quick picture review of what
denial of services now look at it these
users instead of making one connection
they are making multiple connections and
there are cheap software programs
available that would actually trigger
connections from different computers in
the internet with different Mac
addresses so everything kind of looks
legimate for the server and it would
accept those connections and it would
keep the sessions open the actual users
won't be able to use them so that's
denying the service for the actual users
denial of service all right and
distributed denial of service is
generating attacks from multiple places
you know from a distributed environment
so that's distributed denial of service
so the tools the native tools that helps
us to provide event the denial of
service attacks in AWS is cloud shield
and web access firewall AWS web now they
are the major ones they are designed to
mitigate a denial of service if your
website is often bothered by denial of
service then we should be using AWS
Shield or AWS Waf and there are a couple
of other tools that all showed us when I
say that also does denial of service is
not their primary job but you could use
them for denial of service route 53's
purpose is to provide DNS cloudfront is
to provide caching elastic load balancer
elb's work is to provide load balancing
VPC is to create and secure a virtual
private environment but they also
support mitigating denial of service but
not to the extent you would get in AWS
shield and AWS web so AWS shield and Waf
are the primary ones but the rest can
also be used to mitigate distributed
denial of service the other tricky
question is this actually will test your
familiarity with the region and the
services available in the region so when
you're trying to provision a service in
a particular region you're not seeing
the service in that region how do we go
about fixing it or how do we go about
using the service in the cloud it's a
tricky question and if you have not gone
through such situation you can totally
blow it away you really need to have a
good understanding on regions the
services available in those regions and
what a particular service is not
available how to go about doing it the
answer is not all services are available
in all regions anytime Amazon announces
a new service they don't immediately
publish them on all regions they start
small and as in when the traffic
increases as and when it becomes more
likeable to the customers they actually
move the service to different regions so
as you see in this picture within
America North Virginia has is more
services compared to Ohio or Compared to
North California So within not America
itself North Virginia is the preferred
one so similarly there are preferred
regions within Europe Middle East and
Africa and prefer regions within Asia
Pacific so anytime we don't see a
service in a particular region chances
that the service is not available in
that region yet we got to check the
documentation and find the nearest
region that offers that service and
start using the service from that region
now you might think well if I'm looking
for a service in Asia let's say in
Mumbai and if it is not available why
not simply switch to North Virginia and
start using it you could but you know
that's going to add more latency to your
application so that's why we need to
check for application which is check for
region which is very near to the place
where you want to serve your customers
and find nearest region instead of
always going back to North Virginia and
deploying an application in North
Virginia again let's is a place there's
a link in aws.com that you can go and
look for services available in different
region and that's exactly what you're
seeing here and if your service is not
available in a particular region switch
to the other region that provides your
service the nearest other region that
provides that service and start using
service from there with the coming up of
cloud a lot of companies have turned
down their monitoring team instead they
want to go with the monitorings that
cloud provides you know nobody wants to
or at least many people don't want to go
through the hassle of at least new
startups and new companies that are
thinking of having a monitoring
environment that they don't want to go
with traditional knock monitoring
instead they would like to leverage AWS
monitorings available because it
monitors a lot of stuff not just the
availability but it monitors a lot of
stuff like failures errors it also
triggers emails stuff like that so how
do you actually set up a monitor to
website how to set up a Monitor to
Monitor the website metrics in real time
in AWS the simple way anytime you have a
question about monitoring cloudwatch
should strike your mind because
cloudwatch is meant for monitoring is
meant for collecting metrics is is meant
for providing graphical representation
of what's going on in a particular
Network at a particular point of time so
cloudwatch cloudwatch helps us to
monitor applications and using
cloudwatch we can monitor the state
changes not only the state changes the
auto scaling life cycle events anytime
there are more services added there is a
reduction in the number of servers
because of less usage and very
informative messages can be received
through cloudwatch any cloudwatch can
now support schedule events if you want
to schedule anything cloudwatch has an
event that would schedule an action all
right schedule a Trigger Time based not
incident based you know anything
happening and then you get an action
happening that's incident based on the
other hand you can simply schedule few
things on time based so that's possible
with Cloud watch So This Cloud watch
integrates very well with a lot of other
services like notifications for
notifying the user or for notifying the
administrator about it and it can
integrate well with Lambda so to trigger
an action anytime you're designing an
auto healing environment This Cloud
watch can actually monitor and send an
email if we are integrating it with SNS
simple notification service or this
Cloud watch can monitor and based on
what's Happening it can trigger an event
in Lambda and that would in turn run a
function till the environment comes back
to normal so cloudwatch integrates well
with a lot of other AWS Services all
right so cloudwatch has three statuses
green when everything is going good Lo
when the service is degraded and red
when the service is not available green
is good so we don't have to do anything
about it but anytime there's an Lo the
picture that we're looking at is
actually calling an Lambda function to
debug the application and to fix it and
anytime there's a red alert it
immediately notifies the owner of the
application about well this service is
down and here is the report that I have
here is the metrics that I've collected
about the service stuff like that if the
job role requires you to manage the
servers as well there are certain job
roles which are on the system side there
are certain job roles which is
development plus system side now you're
responsible for the application and the
server as well so if that's the case you
might be tested with some basic
questions like the different types of
virtualization in AWS and what are the
difference between them all right the
three major types of virtualization are
hvm which is Hardware virtual machine
the other one is PV para virtualization
and the third one is PV on hvm para
virtualization on Hardware virtual
module all right the difference between
them are actually describing them is
actually the difference between them hvm
it's actually a fully virtualized
Hardware you know the whole Hardware is
virtualized and all virtual machines act
separate from each other and these VMS
are booted by executing Master boot
record in the root block and when we
talk about para virtualization paragraph
is actually the special boot loader
which boots the PV Amis and when we talk
about PV on hvm it's it's actually the
marriage between hvm and PV and this
para virtualization on hvm in other
words PV on hvm it actually helps
operating system take advantage in
storage and the network input output
available through the host another good
question is name some of the services
that are not region specific now you've
been thought that all services are
within a region and some services are
within an availability zone for example
ec2 is within an availability Zone EBS
is within an availability Zone S3 is
region specific dynamodb is region
specific stuff like that VPC is both
availability and region specific meaning
you know subnets are availability Zone
specific and vpc's region specific stuff
like that so you might have thought you
might have learned in that combination
but that could be some tricky questions
that tests you how well you have
understood the region non-region and
availability non-availability Services I
should say there are services that are
not region specific that would be IAM so
we can't have IM for every availability
Zone and for every region which means
you know users will have to use one
username and password for one region and
anytime they switch to another region
they will have to use another username
and password that that's more work then
that's not a good design as well
authentication has to be Global so IM is
a global Service and which means it's
not region specific on the other hand
Route 53 is again a regional Pacific so
we can't have Route 53 for every region
Route 53 is not a region specific
service it's a global Service and it's
one application users access from
everywhere or from every part of the
world so we can't have one URL or one
DNS name for each region if your
application is a global application and
then web application firewall works well
with cloudfront then cloudfront is a
region based service so the web
application firewall it's not region
specific service it's a global Service
and cloudfront is again a global Service
though you can you know cache content on
a continent and country basis it's still
considered a global Service all right
it's not bound to any region so when you
activate cloudfront you're activating it
away from region or availability zone so
when you're activating a web application
firewall because it's not a region
specific service you're activating it
away from availability Zone and regions
so a quick recap I am users groups roles
and accounts they are Global Services
they can be used globally around 53
services are offered at Edge locations
and they are Global as well web
application firewall a service that
protects our web application from common
web exploits they are Global Services
well Cloud front Cloud front is global
content delivery Network CDN and they
are offered at Edge locations which are
a global Service in other words
non-region specific service or Beyond
region service all right this is another
good question as well in the project
that you are being interviewed if they
really want to secure their environment
using Nat or if they are already
securing their environment using Nat by
any of these two methods like Nat
Gateway or Nat instances you can expect
this question what are the difference
between a Nat Gateway and that instances
now they both saw the same thing right
so they're not two different Services
trying to achieve two different things
they both serve the same thing but still
they do have differences in them all
right on a high level they both achieve
providing nothing for the service behind
it but the difference comes when we talk
about the availability of it not Gateway
is a managed service by Amazon whereas
not instance is managed by us now I'm
talking about the third Point
maintenance here Nat Gateway is managed
by Amazon that instance is managed by us
and availability of nat Gateway is very
high and availability of nat instance is
less compared to the NAT Gateway because
it's managed by us you know it's on an
ec2 instance which could actually fail
and if it fails we'll have to relaunch
it but if it is not Gateway if something
happens to that service Amazon would
take care of reprovisioning it and
talking about bandwidth it can burst up
to 75 gigabits now traffic through the
nag Gateway can burst up to 75 gigabits
but for that instance it actually
depends on the server that we launch and
if we are launching a T2 micro it barely
gets any bandwidth so there's a
difference there and the performance
because it's highly available because of
the bigger pipe 75 gigabits now the
performance of the NAT Gateway is very
high but the performance of the NAT
instance is going to be average again it
depends on the size of the NAT instance
that we pick and billing a billing for
Nat Gateway is the number of gateways
that we provision and the duration for
which we use the NAT Gateway but billing
for Nat instance is number of instance
and the type of instance that we use of
course number of instance duration and
the type of instance that we use
Security in that Gateway cannot be
assigned meaning it already comes with
full backed security but in that
instance security is a bit customizable
I can go and change the security because
such a server managed by me or managed
by us I can always change the security
well allow this allow don't allow this
stuff like that size and load of the NAT
Gateway is uniform but the size and the
load of the NAT instance changes as per
around Gateway is a Fix-It product but
in that instance can be small instance
can be a big instance so the size and
the load through it varies
right the other question you could get
asked is what are the difference between
stopping and terminating an ec2 instance
now you will be able to answer only if
you have worked on environments where
you have your instance stopped and where
you have your instance terminated if you
have only used lab and are attending the
interview chances are that you might you
always lost when answering this question
it might look like both are the same
well stopping and terminating both are
the same but there is a difference in it
so when you stop an instance it actually
performs a normal shutdown on the
instance and it simply moves the
instance to the stop state but when you
actually terminate the instance the
instance is moved to this stop State the
EBS volumes that are attached to it are
deleted and removed and will never be
able to recover them again so that's a
big difference between stopping and
terminating an instance if you're
thinking of using the instance again
along with the data in it you should
only be thinking of stopping the
instance but you should be terminating
the instance only if you want to get rid
of that instance forever
if you are being interviewed for an
architect level position or a junior
architect level position or even an
Cloud consultant level position or even
in an engineering position this is a
very common question that get asked what
are the different types of ec2 instances
based on their cost or based on how we
pay them right they're all compute
capacity for example the different types
are on-demand stances Port instances and
reserved instances it kind of looks the
same they all provide the compute
capacity they all provide the same type
of Hardwares for us but if you are
looking at Cost saving or optimizing
cost in our environment we're going to
be very careful about which one are we
picking now we might think that well
I'll go with on-demand instance because
I pay on a per hour basis which is cheap
you know I can use them anytime I want
and anytime I don't want I can simply
get rid of it by terminating it you're
right but if the requirement is to use
the service for one year the requirement
is to use the service for three years
then you'll be wasting a lot of money
buying on-demand instances you'll be
wasting a lot of money paying on an
hourly basis instead we should be going
for reserved instance where we can
reserve the capacity for the complete
one year or complete three years and
save huge amount in buying reserved
instances alright so on-demand is cheap
to start with if you are only planning
to use it for a short while but if
you're planning to run it for a long
while then we should be going for
reserved instance that is what is cost
efficient so spot instance is cheaper
than on-demand instance and there are
different use cases for spot instance as
well so let's look at one after the
other the on-demand instance the
on-demand instance is purchased at a
fixed rate per hour this is very short
term and irregular workloads and for
testing for development on demand
instance is a very good use case we
should be using on demand for production
spot instance spot instance allows users
to purchase ec2 at a reduced price and
anytime we have more instances we can
always go and sell it in spot instances
I'm referring to anytime we have more
reserved instances we can always sell
them in spot instance catalog and the
way we buy Spot instance is we actually
put a budget this is how much I'm
willing to pay all right would you be
able to give service within this cost so
anytime the price comes down and meets
the cost that we have put in will be
assigned an instance and anytime the
price shoots up the instance will be
taken away from us but in case of
on-demand instances we have bought that
instance for that particular R and it
stays with us but with spot instances it
varies based on the price if you meet
the price you get the instance if you
don't meet the price it goes away to
somebody else and the spot instance
availability is actually based on supply
and demand in the market there's no
guarantee that you will get support
instance at all line all right so that's
a caveat there you should be familiar
with that's a caveat there you should be
aware when you are proposing somebody
that we can go for spot instance and
save money it's not always going to be
available if you want your spot instance
to be available to you then we need to
carefully watch the history of the price
of the spot instance now how much was it
last month and how much was it how much
is it this month so how can I code or
how much can I code stuff like that so
you got to look at those history before
you propose somebody that well we're
going to save money using spot instance
on the other hand reserved instance
provide cost savings for the company or
we can opt for reserved instances for
you know one year or three years there
are actually three types of reserved
instances light medium and heavy
reserved instances they are based on the
amount that we would be paying and cost
benefit also depends with reserved
instance the cost benefit also depends
based on are we doing all upfront or no
upfront or partial payment then split
the rest as monthly payments so there
are many purchase options available but
overall if you're looking at using an
application for the next one year and
three years you should not be going for
on-demand instance you should be going
for reserved instance and that's what
gives you the cost benefit
and in an AWS interview sometimes you
might be asked you know how you interact
with the AWS environment are you using
CLI are you using console and depending
on your answer whether console or a CLI
the panelists put a score okay this
person is CLI specific this person is
console specific or this person has used
AWS environment through the SDK stuff
like that so this question tests whether
you are a CLI person or a console person
and the question goes like this how do
you set up SSH agent forwarding so that
you do not have to copy the key every
time you log in if you have used Puri
anytime if you want to log into an ec2
instance you will have to put the IP and
the port number along with that you will
have to map or we will have to map the
key in the Puri and this has to be done
every time that's what we would have
done in our lab environments right but
in production environment using the same
key or mapping the same key again and
again every time it's actually an hassle
it's considered as a blocker so you
might want to Cache it you might want to
permanently add it in your putty session
so you can immediately log in and start
using it so here in the place where you
would actually map the private key
there's a quick button that actually
fixes or that actually binds your SSH to
your pretty instance so we can enable
SSH agent forwarding and that will
actually bind our key to the SSH and
next time when we try to log in we don't
have to always go through mapping the
key and trying to log in all right this
question what are Solaris and ax
operating systems are they available
with AWS that question generally gets
asked to test how familiar are you with
the Amis available how familiar are you
with ec2 how familiar are you with the
ec2 Hardwares available that basically
tests that now the first question or the
first thought that comes to your mind is
well everything is available with AWS
I've seen Windows I've seen Ubuntu I've
seen Red Hat I've seen Amazon Amis and
if I don't see my operating system there
I can always go to Marketplace and try
them if I don't final Marketplace I can
always go to community and try them so a
lot of Amis available there are a lot of
operating systems available I will be
able to find Solaris and ax but that's
not the case Solaris and ax are not
available with AWS that's because
Solaris uses a different I mean Solaris
does not support the architecture does
not support public Cloud currently the
same goes for ax as well and they run on
power CPU and not on Intel and as of now
Amazon does not provide Power machines
this should not be confused with HPC
which is a high performance Computing
should not be confused with that now
these are different Hardwares different
CPU itself that the cloud providers did
do not provide yet another question you
could get asked in organizations that
would want to automate the their
infrastructure using Amazon native
Services would be how do you actually
recover an ec2 instance or Auto recover
an ec2 instance when it fails oh well we
know that ec2 instances are considered
as immutable meaning irreparable we
don't spend time fixing bugs in an OS
stuff like that you know once an ec2
instance crashes like it goes on a over
spanic or there are various reasons why
it would fail so we don't have to really
worry about fixing it we can always
relaunch that instance and that would
fix it but what if it happens at two
o'clock in the night what if it happens
that during a weekend when nobody is in
office now looking or monitoring those
instances so you would want to automate
that not only on a weekend or during
midnights but it's general practice good
to automate it so you could face this
question how do you actually automate an
ec2 instance once it fails and the
answer to that question is using Cloud
watch we can recover the instance so as
you see there is an alarm threshold a
set in Cloud watch and once the
threshold is met meaning if there is an
error if there is a failure if the ec2
instance is not responding for a certain
while we can set an alarm and once the
alarm is met let's say the CPU
utilization stayed high for 5 minutes
all right it's not taking any new
connections or the instance is not
pinging for five minutes or in this case
it's two minutes it's not pinging so
it's not going to respond connection so
in those cases you would want to
automatically recover that ec2 instance
by rebooting the instance all right now
look at this the take this action
section under the action so there we
have a bunch of options like recover
this instance meaning reboot the
instance so that's how we would recover
the other two options are beyond the
scope of the question but still you can
go ahead and apply just like I'm gonna
do it so the other option going to stop
the instance that's very useful when you
want to stop instances that are having
low utilizations nobody is using the
system as of now you don't want them to
be running and wasting the cloud
expenditure so you can actually set an
alarm that stops the ec2 instance that's
having low utilization so somebody was
working in an instance and they left it
without or they forgot to shut down that
instance and it gets I mean they will
only use it again the next day morning
so in between there could be like 12
hours that the system is running idle
nobody is using it and you're paying for
it so you can identify such instances
and actually stop them when the CPU
utilization is low meaning nobody is
using it the other one is to terminate
let's say you want to give system to
somebody temporarily and you don't want
them to hand the system back to you all
right this is actually an idea in other
words this is actually the scenario so
you hand over a system to somebody and
when they're done they're done we can
actually terminate the system so you
could instruct the other person to
terminate the system when they're done
and they could forget and the instance
could be running forever or you can
monitor the system after the specified
time is over and you can terminate the
system or best part you can automate the
system termination so you assign a
system to somebody and then turn on this
cloudwatch action to terminate the
instance when the CPU is low for like
two hours meaning they've already left
or CPU is low for 30 minutes meaning
they've already left stuff like that so
that's possible and if you're getting
hired for an system side architect or
even on the sysop site you could face
this question what are the common and
different types of Ami designs there are
a lot of Ami designs the question is the
common ones and the difference between
them so the common ones are the full
back Amis and the other one is just
enough OS Ami je OS Ami and the other
one is hybrid type Amis so let's look at
the difference between them the full
backed Ami just like the name says it's
fully baked it's ready to use Ami and
this is the simplest Ami to deploy can
be a bit expensive it can be a bit a
cumbersome because you'll have to do a
lot of work beforehand you could use the
Amis a lot of planning a lot of thought
process will go into it and the Ami is
ready to use right you hand over the Ami
to somebody and it's ready to use or if
you want to reuse the Ami it's already
ready for you to use so that's full
baked Ami the other one is just enough
operating system Ami just like the name
says it has I mean as you can also see
in the diagram or in the picture it
covers a part of the OS all bootstraps
are already packed properly and the
security monitoring logging and the
other stuff are configured at the time
of deployment or at the time you would
be using it so not much thought process
will go in here the only focus is on
choosing the operating system and what
goes the operating system specific
agents or bootstraps that goes into the
operating system that's all we worry
about the advantage of this is it's
flexible meaning you can choose to
install additional softwares at the time
of deploying but that's going to require
an additional expertise on the person
who will be using the Ami so that's
another overhead there but the advantage
is that it's kind of flexible I can
change the configurations during the
time of deployment the other one is the
hybrid Ami now the hybrid Ami actually
falls in between the fully baked Ami and
just enough operating system options so
these Amis have some features of the big
type and some features of the just
enough OS type so as you see the
security monitoring logging are packed
in that Ami and the runtime environments
are installed stalled during the time of
a deployment so this is where the strict
company policies would go into the Ami
company policies like you got to lock
this you got to monitor this these are
the ports that generally gets open in
all the systems stuff like that so they
strictly go into the Ami and sits in an
AMF format and during deployment you
have the flexibility of choosing the
different runtime and the application
that sits in an ec2 instance another
very famous question you would face in
an interview is how can you recover
login to an ec2 instance to which you
lost the key well we know that if the
key is lost we can't recover it there
are some organizations that integrate
their ec2 instances with an ad that's
different all right so you can go and
reset the password in the ad and you
will be able to log in with the new
password but here the specific tricky
question is you are using a key to log
in and how do you recover if you have
lost the key generally companies would
have made a backup of the key so we can
pick from the backup but here the
specific question is we have lost the
key literally no backups on the key at
all so how can we log in and we know
that we can't log into the instance
without the key present with us so the
steps to recover is that make the
instance use another key and use that
key to log in once the key is lost it's
lost forever we won't be able to recover
it you can't raise a ticket with Amazon
not possible they're not going to help
it's beyond the scope so make the
instance use another key it's only the
key that's the problem you still have
valid data in it you got to recover the
data it's just the key that's having the
problem so we can actually focus on the
key part alone and change the key and
that will allow us to log in so how do
we do it step by step procedure so first
verify the ec2 config service is running
in that instance if you want you can
actually beforehand install the ec2
config in that service or you can
actually make the easy to config run
through the console just couple of
button clicks and that will make the
easy to configure run in that easy to do
instance and then it detach the root
volume for that instance of course it's
going to require a stop and start to
detach the root volume from the instance
attach the root volume to another
instance as a temporary volume or it
could be a temporary instance that
you've launched only to fix this issue
and then login to that instance and to
that particular volume and modify the
configuration file configuration file
modify it to use the new key and then
move the root volume back to its
original position and restart the
instance and now the instance is going
to have the new key and you also have
the new key with which you can log in so
that's how we go ahead and fix it now
let's move on to some product specific
or S3 product specific questions a
general perception is S3 and EBS can be
used interchangeably and the interviewer
would want to test your knowledge on S3
and EVS well if EBS uses S3 that's true
but they can't be interchangeably used
so you might face this question what are
some key differences between AWS S3 and
EBS well the differences are S3 is an
object store meaning you can't install
anything in it you can store drive files
but you can't actually install in it
it's not a file system but ABS is a file
system you can install Services I mean
install applications in it and that's
going to run stuff like that and talking
about performance S3 is much faster and
abs is super faster when accessing from
the instance because from the instance
if you need to access S3 you'll actually
have to go out through the internet and
access the S3 or S3 is an external
service very external service you'll
have to go through or you'll have to go
outside of your VPC to access S3 S3 does
not come under a VPC but EBS comes under
a VPC it's on the same VPC so you would
be able to use it kind of locally
compared to S3 EBS is very local so that
way it's going to be faster and
redundancy talking about redundancy of
S3 and abs S3 is replicated the data in
S3 is replicated across the data centers
but EBS is replicated within the data
center meaning S3 is replicated across
availability zones EBS is within an
availability zone so that way redundancy
is a bit less in EVS in other words
blurrency is higher in S3 than EBS and
talking about security of S3 is3 can be
made private as well as public meaning
anybody can access S3 from anywhere in
the internet that's possible with S3 but
EBS can only only be accessed when
attached to an ec2 instance right just
one instance can access it whereas S3 is
publicly directly accessible the other
question related to S3 security is how
do you allow access to a user to a
certain a user to a certain bucket which
means this user is not having access to
S3 at all but this user needs to be
given access to a certain bucket how do
we do it the same case applies to
servers as well in few cases there could
be an instance where a person is new to
the team and you actually don't want
them to access the production service
now he is in the production group and by
default he or she is granted access to
that server but you specifically want to
deny access to that production server
till the time he or she is matured
enough to access or understand the
process understand the do's and don'ts
before they can put their hands on the
production server so how do we go about
doing it so first we would categorize
our instances well these are critical
instances these are normal instances and
we were actually put a tag on them
that's how we categorize right so you
put a tag on them put attacks saying
well they are highly critical they are
medium critical and they are not
critical at all still they're in
production stuff like that and then you
would pick the users who wants to or who
should be or should not be given access
to a certain server and you would
actually allow the user to access or not
access servers based on a specific tag
in other words you can use actually tags
in in the previous step we put tags on
the critical server right so you would
Define that this user is not going to
use this tag all right this user is not
allowed to use the resources for this
stack so that's how you would make your
step forward so you would allow or deny
based on the tags that you have put so
in this case he or she will not be
allowed to servers which are TAG
critical servers so that's how you allow
deny access to them and the same goes
for bucket as well or if an organization
is excessively using S3 for their data
storage because of the benefit that it
provides the cost and the durability you
might get asked this question which is
organizations would replicate the data
from one region to another region for
additional data durability and for
having data redundancy not only for that
they would also do that for Dr purposes
for Disaster Recovery if the whole
region is down you still have the data
available somewhere else and you can
pick and use it some organizations would
store data in different regions for
compliance reasons to provide low
latency access to their users who are
local to that region stuff like that so
when companies do replication how do you
make sure that there is consistency in
the replication how do you make sure
that a replication is not failing and
the data gets transferred for sure and
there are logs for that replication this
is something that the companies would
use where they are excessively using S3
and they are fully relying on the
replication in running their business
and the way we could do it is we can set
up a replication monitor it's actually
set of tools that we could use together
to make sure that the cloud replication
a region level replication is happening
properly so this is how it happens now
on this side on the left hand side we
have the region one and on the right
hand side we have Region 2 and region 1
is the source bucket and region two is
the destination bucket all right so
object is put in the source bucket and
it has to go directly to the region to
bucket or or made a copy in the region
two bucket and the problem is sometimes
it fails and there is no consistency
between them so the way you would do it
is connect these Services together and
create an cross replication or cross
region replication monitor that actually
monitors that actually monitors your
environment so there are cloudwatch that
make sure that the data is moved no data
is failing again there's Cloud watch on
the other end make sure that the data is
moving and then we have the logs
generated through cloudtrail and that's
actually written in dynamodb and if
there is an error if something is
failing you get notified through an SMS
or you get notified through an email
using the SNS service so that's how we
could leverage these tools and set up
and cross region replication monitor
that actually monitors your data
replication some common issues that
company companies face in VPC is that we
all know that I can use Route 53 to
resolve an IP address externally from
the internet but by default the servers
won't connect to the other servers using
our custom DNS name that does not do
that by default so it's actually a
problem there are some additional things
that as an administrator or as an
architect or as a person who uses it you
will have to do and that's what we're
going to discuss so the question could
be a VPC is not resolving the server
through the DNS you can access it
through the IP but not through the DNS
name and what could be the issue and how
do you go about fixing it and you will
be able to answer this question only if
you have done it already it's a quick
and simple step by default VPC does not
allow that's the default feature and we
will have to enable the DNS hostname
resolution before now this is for the
custom DNS not for the default DNS that
comes along this is for the custom DNS
so we will have to enable the DNS
hostname resolution so our will have to
enable DNS hostname resolution so they
actually resolve let's say I want to
connect to a server one dot
simplylearn.com by default it's not
allowed but if I enable this option then
I will be able to connect to server one
simplylearn.com if a company has vpcs in
different regions and they have head
office in a central place and the rest
of them are Branch offices and they are
connecting to the head office for Access
or you know for saving data or for
accessing certain files or certain data
or storing data all right so they would
actually mimic The Hub and spoke
topology where you have the VPC which is
centrally in an accessible region a
centrally accessible region and then you
would have a local vpcs or Branch
offices in different other regions and
they get connected to the VPC in the
central location and the question is how
do you actually connect the multiple
sites to a VPC and make communication
happen between them by default it does
not do that we know that vpcs they need
to be paired between them in order to
access the resources let's look at this
picture right so I have like a customer
Network or Branch offices in different
parts and they get connected to a VPC
that's fine so what we have achieved is
those different offices the remote
offices they are connecting to the VPC
and they're talking but they can't
connect or they can't talk to each other
that's what we have built but the
requirement is the traffic needs to or
they should be able to talk to each
other but they should not have direct
connection between them which means that
they will have to come and hit the VPC
and then reach the other customer
Network which is in Los Angeles or which
is in New York right that's the
requirement so that's possible with some
architecting in the cloud so that's
using VPN Cloud Hub you look at this
dotted lines which actually allows
customers or which actually allows the
corporate networks to talk to each other
through the VPC Again by default it
doesn't happen cloudhub is an
architecture that we should be using to
make this happen and what's the
advantage of it as a central office or
as the headquarters office which is in
the VPC or headquarters data center
which is in the VPC you have control or
the VPC has control on who talks to who
and what traffic can talk to I mean what
traffic can be routed to the other head
office stuff like that that centralized
control is on the VPC the other question
person you could get asked is name and
explain some security products and
features available in VPC well VPC
itself is a security service it provides
a security service to the application
but how do you actually secure the VPC
itself that's the question and yes there
are products that can actually secure
the VPC or the VPC delivers those
products to secure the application
access to the vpcs restricted through a
network access control list all right so
that's a security product in VPC and a
VPC has security groups that protects
the instances from unwanted inbound and
outbound traffic and network access
control list protects the subnets from
unwanted inbound and outbound access and
there are flow logs we can capture in
VPC that captures incoming and outgoing
traffic through a VPC which will be used
for later analysis as in what's the
traffic pattern what's the behavior of
the traffic pattern stuff like that so
there are some security products and
features available in VPC now how do you
monitor VPC VPC is a very important
concept very important Service as well
everything sits in a VPC most of the
service sits in a VPC except for Lambda
and S3 and dynamodb and couple of other
services most of them sit in a VPC for
security reasons so how do you monitor
your VPC how do you gain some visibility
on your VPC well we can gain visibility
on our VPC using VPC flow log that's the
basic Service as you see it actually
captures what's allowed what's not
allowed stuff like that which IPS
allowed which IP is not allowed stuff
like that so we can gather it and we can
use that for analysis and the other one
is cloud watch and Cloud watch logs the
data transfers that happen so this is
you know who gets allowed and who does
not get allowed I mean the flow logs is
who is allowed and who's not allowed
that kind of detail and Cloud watch
gives information about the data
transfer how much data is getting
transferred we can actually pick unusual
data transfers if there is a sudden hike
in the graph there's a sudden hike and
something happens at 12 on a regular
basis and you weren't expecting it
there's something suspicious it could be
valid backups it could be a malicious
activity as well so that's how you know
by looking at Cloud watch logs and
cloudwatch dashboard and with that we
have come to end of this video on cloud
computing expert full course I hope you
found it useful and entertaining please
ask any question about the topics
covered in this video in the comments
box below our experts will assist you in
addressing your problems thank you for
watching stay safe and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign