foreign
the demand for devops professional is at
an all-time high with companies looking
to accelerate their software development
processes the need for devops skills has
become crucial in the market
the job roles in devops are diverse and
in demand with positions ranging from
devops engineered to automation
architect these roles offer competitive
salaries with the average salary for a
dubs engineer in the U.S being around
120 000 per year according to Glassdoor
this makes devops one of the highest
paying I.T roles in the market today if
you are looking to enter the world of
devops Simply learns postgraduate
program in devops is the perfect place
to start this comprehensive program is
designed to equip you with the skills
and knowledge you need to succeed in a
devops role with over 500 hours of
learning content Hands-On projects and
Industry relevant case studies you will
gain practical experience in tools like
git Docker kubernetes and more so
whether you are an ID professional
looking to upskill or a fresh graduate
looking to start a career in devops
don't miss out on this opportunity to
join the ranks of successful devops
professional and roll now and take the
first step towards a brighter future now
let's check out what we have in store
for you for this day of sport cam
we will start our discussion with what
devops is and why do we use devops then
we will discuss various tools starting
with Git you will see the get
installation what is git and explore
some git commands and after that we have
another tool that is cradle we'll see
its installation and we'll see what is
Gradle and we'll see a project on Gradle
then we have another tool that is
selenium you will learn how to install
selenium and after that we'll have a
particular tutorial on selenium
then we have another tool that is talker
first we will see how to install it on
Windows and Ubuntu and after that we
have a dedicated tutorial on docker then
there's come another tool that is Chef
first we'll see its installation and
then we will learn all about Chef
and after that we have another tool that
is ansible we'll see its installation
and a particular tutorial and after that
we have another tool that is puppet
first we'll see its installation then we
have a dedicated tutorial on power
and then we will draw some comparisons
between Chef puppet ansible and salt
stack and then we'll talk about a
monitoring tool called neck use then we
will Deep dive into a discussion on
Jenkins and implement the CI CD pipeline
using Jenkins finally we'll wrap up this
stuff's bootcamp with some interview
questions on devops
meet Tim timbal's a robot in his lab
a climate-controlled and pollution-free
environment
once he's done he drops the robot off at
his project partner Mia's house Mia
takes it out to her backyard to ensure
that the robot meets the requirements
but here is where the problem arises the
change in the environment causes the
robot to malfunction Mia is now really
annoyed and she has a lot to correct and
it seems to her as though Tim didn't
really do much this wall between them
leaves a poor robot to bite the dust
well what if he broke this wall Tim and
Miana work together in a common space
Tim develops each block of functionality
of the robot which is then immediately
checked by Mia both are now working
simultaneously instead of waiting on the
other to finish their task as a man a
feature is ready for use they are put
together to build the final product they
develop a common mindset and share ideas
to further speed up the process they use
several tools which can automate every
stage this means that the robot is now
ready sooner with less iterations and
manual work from an organization
perspective Tim would be the developer
while Mia the operations their Union is
the core of the devops approach devops
has several stages and set of tools to
automate each of these stages let's have
a look at these to him first puts down a
plan in terms of software this could
mean deciding on the modules and the
algorithms to use once he has the plan
he now codes the plan with tools such as
git Tim has a repository for storing all
the codes and their different versions
this is call Version Control next this
code is fetched and made executable this
is the build stage tools such as Gradle
and Maven will sort this out now before
deployment the product is tested to
catch any bugs the most popular tool
automating testing is selenium once the
products are tested Mia must deploy it
the deployed product is then
continuously configured to the desired
State ansible puppet and Docker are some
of the most common tools used that
automate these stages
now every product is continuously
monitored in its working environment
nagios is one such tool that automates
this phase and the feedback is fed back
to the planning stage and finally we
have the core of the devops life cycle
the integration stage tools such as
Jenkins is responsible for sending the
code for build and test if the code
passes the tests it's further sent for
deployment this is called continuous
integration let's now have a look at an
organization that has adopted the devops
approach which of the below sequence of
steps would they follow to develop a
software Lydia answers in the comment
section keep an eye out for the right
answer on the comment section or our
YouTube Community
Giants such as Amazon Netflix Target
Etsy and Walmart have all adopted devops
and seen a considerable increase in
delivery and quality tea in 2014 an hour
of downtime for Netflix would cost two
hundred thousand dollars it became
absolutely crucial that Netflix prepared
themselves for any sort of failure and
so they took to the devops approach and
implemented it in the most unique way
they developed a tool called Simeon Army
this tool created failures and
automatically deployed them in an
environment that did not affect the
users the team would troubleshoot these
failures and this gave them enough
experience to deal with any degree of
collapse with everything being automated
and happening simultaneously
organizations can now deliver at a much
faster pace so considering the benefits
of devops and its Divergence from the
traditional methods would devops be the
future we're going to go through what we
need to be able to do to go to devops
what the arguments are and why you need
to do devops and they will have you go
through all the individual tools you
need to be able to successfully
Implement devops within your
organization in addition to that we're
also going to take time and go through
each of those tools so you get a good
understanding of a step-by-step
instructions on how to do basic setup of
each of those tools so let's get started
so what was devops before so what was
the process that we took by doing
delivery before devops well it was a
model called waterfall and waterfall was
a very traditional approach to actually
building out Solutions and the reason
why it's called waterfall is that you
bring out all the individual
requirements and individual sections of
a project and they Cascade off each
other so if we look at the breakdown we
have requirements design and we have
implementation we have verification we
have maintenance you'll have user
acceptance testing now this is all based
on the software development lifecycle
model or sdlc and it's been around for
quite some time and it's still used by a
lot of companies today okay the
challenge you've had with the waterfall
model is that it really is a very long
drawn out model for actually building
and delivering Solutions so it took a
very long time to actually write code
and then deploy the code and it was very
difficult to actually identify problems
within the code and provide feedback to
the development team on what to fix and
this really was a very time consuming
we're talking about months sometimes
years for projects to be actually go
through a war film model process so
Along came a new method of being able to
do delivery and it's called agile and
the agile approach is a way of being
able to take the actual work that's done
in a waterfall model and compress it
down into small iterations and what we
would do is a fundamental changes that
you would actually take teams that were
disparate and as part of the individual
Cascades within a waterfall project and
you actually bring them together so you
have your requirements team in person
design developer and release management
team all together in one group working
on an iteration the great thing about
agile is that you took a process that
was weeks or months or even years in
length as it was with waterfall and you
reduce it down to two or four week
Sprint depending on the Cadence for your
team typically you have a two-week
spring and then the goal is is that at
the end of each Sprint or sometimes
every other Sprint you would do a
software release and so that customers
were getting the software much faster
the problem that we still ran into
though with um agile is fundamentally
similar to what we were having with
waterfall you have your devops person
working on code on their system and
you'd be working great on their computer
and then you have the operations person
who's migrating the code from the
developers environment the test
environment to for the production
environment and you're going run into
issues where the code simply wasn't
working there's a lot of reasons why
that would happen the actual developer
environment would often be very
different or would have different
dependencies in it so the the hardware
the the software there may be additional
uh applications that were installed on
the operating system that simply hadn't
been transferred over to the operations
environment and so what you would have
is a disconnect between the developer
environment and the operations
environment making it difficult and to
actually roll out code so you'd run into
a program where when you rolled out code
you'd have to have a rollback plan in
case the code wouldn't work in
production and so each release became an
event where everybody got very stressed
about the actual event of releasing code
because you didn't know whether it was
going to work so Dev amps really looks
to address and solve a lot of these
problems so the key word that you'll
often hear with devops is continuous
integration and what that means is
essentially that as a developer is
working on their code the code is
constantly being tested against not just
the actual code itself with unit testing
but the environment with which is going
to be released in and the goal from a
devops model is that the breakdown of
communication that happens with
waterfall and agile where Dev developers
and operations teams aren't working in
the same environment is being removed
and you're able to provide a continuous
and contiguous environment between the
developer and the actual operating model
so the reality is when the developer is
working on their code they're actually
working in an environment that is
identical to the production environment
and so when the actual operations person
comes to actually do releases for the
code and you can see some teams are
doing as many as 20 to even up to 50
releases to production environments
every single day you're able to
guarantee that the actual code itself
will work and releases go from being a
stressful event to a byproduct of good
testing and good setup and structure for
how you actually build out your
Solutions so what we're seeing here so
the goal is that as a developer and as
our operations person that the code is
working continuously in both
environments you have continuous
integration and continuous delivery so
simply put what we're able to do is
we're able to eliminate the problem of
the operation environment not being in
sync with the development environment
and this is a Improvement on agile this
is not to say that waterfall or agile
are wrong as delivery models what it is
is just a maturity of the ability to
deliver Solutions and devops is just
another rung in that maturity curve
using tools that are available available
to us now that five ten years ago simply
weren't available so the goal is for you
as a team to move to a devops model
where you can Implement continuous
releases on your software as long as
you're using the tools that are
available and the good news is those
tools are open source tools so let's go
through some of the benefits of why
you'd want to go and use devops so you
know essentially what's in it for you so
let's over the next few sides we're
going to go through what is devops we're
going to go through the benefits of
devops so in the last few slides you've
actually seen you know what is devops
and the benefits of devops along with
the life cycle but we're also going to
start digging into the tools that you
have that are useful for devops and
we're going to focus in on seven tools
that can provide an end-to-end
infrastructure for delivering devops
Solutions there are significantly more
tools available on the market but these
are seven of the most popular uh for
each of their categories so demo hopes
really is an essential collaboration
between the development team and the
operations team these are teams that
have in past been somewhat at conflict
with each other and what you have now is
an opportunity where those teams can now
work continuously with each other the
expectation with devops is that it will
continue to mature indeed you're
actually even seeing some groups which
are now called devsecops where they're
integrating security as part of the
delivery between the development team
and the operations team the bottom line
is a devops engineer is highly in demand
the demand for devops engineer is
literally going through the roof with
salaries going up exponentially around
that so let's dig into some of the
benefits of devops it's not just a new
catchphrase it's actually got
significant value and how you can speed
up delivery of your software so the
benefits of devops can really broken up
into a number of key areas first of all
we have continues to delivery of
software which allows you to
continuously release new features with
the security and understanding that the
software going out is of high quality it
allows the teams that are working on the
software delivery within your
organization to more effectively
collaborate with each other so that
you're all talking from the same page
and understanding of what needs to be
delivered the deployment process itself
moves from being an event where there's
a lot of stress and there's a lot of
contingency plans to being a much easier
deployment the efficiency within the
actual code that you're writing and the
ability to scale up using the different
tools are available it allows you to be
able to bring in and scale up and reduce
the teams you have running the software
as needed errors can be fixed much
earlier and more quickly and can be
caught before anything gets pushed out
to the production environment and
fundamentally what we're looking for is
improving the security of the actual
releases so the actual concept of
security is Center to all the work
you're doing and then finally what
really allows you to reduce the number
of Errors is that there is much less
manual intervention there is a greater
Reliance on scripted environments that
you can actually test and validate for
their security reliability and uptime
efficiency so let's talk a little bit
about the life cycle of a devops so the
very first step that you'll take is to
actually build out a build and test
environment and this is a continuous
building test environment and this is
managed probably the first step of your
source code once you move through that
and you're looking at continuous
integration which means that every time
somebody checks in their code they're
validating that the code actually can
run in the production environment once
you've actually then passed the end
continuous integration and the testing
that you have with your code you're
looking at continuous deploy employment
if the code works and is available to be
released into the production environment
let's go ahead and release it and once
you actually have release code then you
want to be able to validate that your
environment is working efficiently you
may release code that is a new feature
within your application and customers
May then gravitate immediately to that
new feature if they do you want to be
able to ensure that the code is working
and more importantly but the
infrastructure is there to support and
then finally you're looking at software
released as a continuous event and then
you go back to the beginning you start
working on more code you run it through
your build environments and continuous
integration deployment continuous
monitoring and keep that cycle moving so
let's dig into the tools that you as a
devops engineer would need to learn if
we break down the environment that we
have all the way from source code
management to software release and there
are a number of key tools that you want
to be able to use so for instance source
code management and get is an open
source tool that you would want to use
for managing your code The Continuous
build and test environment will be
managed with Maven and selenium
integration with the environments that
you're working on is managed through
Jenkins the actual deployment to your
production environments will be managed
with products such as ansible and Docker
and then the monitoring of your network
would be used with tools like neglios
the thing that you have to remember with
all these tools is that they're open
source tools there is no licensing that
you have to purchase some of the tools
will have our Pro level licensing that
you can choose to select but to get
started all of these are open source
tools you can actually start using for
free right now so we'll start by
downloading and installing it on our
system we'll then have a look at the git
bash interface we'll type in some basic
git commands next we'll create a local
repository that is we create a
repository on a local machine we'll then
connect to a remote repository and
finally we'll push the file onto GitHub
first things first we need to download
and install git so download git for
Windows and I'll select the second link
so 2.19.1 which is the most latest
version of git that's the one we want
for Windows system choose your version
so mine is a 64-bit system and it's
downloading so this will take a while so
git is finally downloaded now we need to
install the sonar system
click here run
so go to next we don't have to change
this path uh just so just click on in
quick launch and on desktop next next
again next nothing to change here either
and install
so now git is getting installed on our
system
so we don't need to view the release
notes we just want to launch the git
Bash
so let's just stick that and then click
on finish
and your git bash interface appears here
so we are on the master Branch the first
thing we do is we'll check the version
for our git so the command is git dash
dash version and as you can see version
2.19.1 on our Windows system which is
exactly what we just downloaded we'll
now explore the help command so let's
just type get help config so config is
another command and as I hit enter the
manual page for the second command
opened up which is config so what help
command does is that it provides the
manual or the help page for the command
just following it so in case you have
any doubts regarding how a command is
used what a command is used for all the
various syntax of the command you can
always use the help command now there's
another Syntax for using the help
command itself which is git config dash
dash help enter this does the exact same
thing as you can see
now that we looked at the help command
let's begin by creating a local
directory so mkdir test now test is my
new directory I'll move into this
directory so CD test
great so now that we are inside our test
directory let's initialize this
directory so get init is the command for
initializing the directory and as you
can see as you can see the path here
this is the local path where a directory
is created so I'll just show you the
directory test and it's completely empty
what we do now is we'll create a text
file within this new directory that we
created so new text document and I'll
just name this demo I'll open this and
just put in some dummy content so hello
simply learn
save this file and go back to your bash
interface let's just check the status
now so git status and as you can see our
file has appeared it's visible but
nothing is committed yet so this means
that we have not made any change to a
file through the git tool itself so the
next thing that we are going to do is
we'll be adding demo to our current
directory the next command that we'll be
applying is the commit command and when
you add certain files to the current
directory the commit command is applied
on all the above directories so git
commit minus M and a message that will
appear once the file is committed
so as you can see one file is changed
and one insertion I'll just clear the
screen next thing we need to do is we
need to link our git to our GitHub
account so the command for doing that is
git config Global user dot username
and this will be followed by our
username so let me just show you my
GitHub account
so this is my GitHub profile and my
username is simply learn Dash GitHub so
guys before you begin this procedure
just make a GitHub account type in my
username here simply learn Dash GitHub
and enter and there you go a git is
successfully linked with GitHub next
thing we do is we'll just open our
GitHub and we'll create a new repository
give a repository name so I'll get name
it test underscore demo and create
Repository
great so our repository is created this
is our remote repository what we do next
is just copy the link and then go back
to your bash interface
now we need to link our remote and a
local Repository
so git remote origin and then paste
the HTTP link and now that our local
repository and a remote repository are
linked we can push a local file onto a
remote repository so the command for
doing that is git push origin Master as
we are on the master branch
and that's done so now let's move back
to GitHub I'll just click on test demo
and as you can see here our local file
has been pushed to our remote repository
with that we have successfully completed
our demo so we're going to introduce the
concept of Version Control that you will
use within your devops environment then
we'll talk about the different tools
that are available in a distributed
Version Control System we'll highlight a
product called git which is typically
used for Version Control today and
you'll also go through what are the
differences between git and GitHub you
may have used GitHub in the past or
other products like gitlab and we'll
explain what are the differences between
git and git and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like and how do you go through and
create forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally I'll take you through
a demo on how you can actually run git
yourself and in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about Version Control Systems
so you may have already been using a
virtual control system within your
environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a Version Control System allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program such as a website or an
application you would store all of your
Version Control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there would be an
addendum added to that so there will be
the version one changes that you had
then the person would then later on
check out that code and then be a
version two and added to that code and
so you keep adding on versions of that
code the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the charge you're running is that it's
very difficult for large groups to work
simultaneously within a project the
benefits of a VCS system a Version
Control System demonstrate that you're
able to store multiple versions of a
solution in a single repository now
let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are working on the
latest piece of code so a key part of a
distributed Version Control System
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed VCS server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those our clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective disk distributed VCS
environment is get now you may remember
that we actually covered get in a
previous video and we'll reference that
video for you so we start off with our
remote git repository and people are
making updates to the copy of their code
into a local environment that local
environment can be updated manually and
then periodically pushed out to the git
repository so you're always pushing out
the latest code that you've code changes
you've made into the repository and then
from the repository you're able to pull
back the latest updates and so you'll
get repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get get
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
clients to that some remote Hub server
now git is used to track the changes of
the source code and allows large teams
to work simultaneously with each other
it supports a non-linear development
because of thousands of parallel
branches and has the ability to handle
large projects efficiently so let's talk
a little bit about git versus GitHub so
get is a software tool whereas GitHub is
a service and I'll show you how those
two look in the moment you install the
software tool for git locally on your
system whereas GitHub because it is a
service it's actually hosted on a
website git is actually the software
that used to manage different versions
of source code whereas GitHub is used to
have a copy of the local Repository
stored on the service on the website
itself git provides command line tools
that allow you to interact with your
files whereas GitHub has a graphical
interface that allows you to check in
and check out files so let me just show
you the two tools here so here I am at
the get website and this is the website
you would go to to download the latest
version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
Hub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide made
similar service to GitHub gitlab is
another popular service but you also
find that development tools such as
Microsoft Visual Studio are also
incorporating git commands into their
tools so the latest version of Visual
Studio team Services also provides this
same ability but GitHub it has to be
remembered is a place where we actually
store our files and can very easily
create public and shareable is a place
where we can store our files and create
public shareable projects you can come
to GitHub and you can do a search on
projects you can see at the moment I'm
doing a lot of work on blockchain but
you can actually search on the many
hundreds of projects here in fact I
think there's something like over a
hundred thousand projects being managed
on GitHub at the moment that number is
probably actually much larger than that
and so if you are working on a project I
would certainly encourage you to start
at GitHub to see if somebody's already
maybe done a prototype that they're
sharing or they have an open source
project that they want to share that's
already available and in GitHub
certainly if you're doing anything with
them Azure you'll find that there are
thousands 45
000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's get and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your get
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready commit
your files back to the main branch on
your git project you want to push out
all your changes to a local repository
after you've made your changes and these
will commit those files and get them
ready for synchronization with the
service and we'll then push your
services out to the remote repository an
example of a remote repository would be
GitHub later when you want to update
your code before you write any more code
you would pull the latest changes from
the remote repository so that your copy
of your local software is always the
latest version of the software that the
rest of the team is working on one of
the things that you can do is as you're
working on new features within your
project you can create branches you can
merge your branches with the mainline
code you can do lots of really creative
things that ensure the that Aid the code
remains at very high quality and B that
you're able to see famously add in new
features without breaking the core code
so let's step through some of the
concepts that we have available and get
so let's talk about forking and cloning
and kit so both of these terms are quite
old terms when it comes to development
but forking is certainly a term that
goes way way back long before we had
distributed CVS systems such as the ones
that we're using with get to Fork a
piece of software is a particularly open
source project you would take the
project and create a copy of that
project and but then you would then
associate a new team and new people
around that project so it becomes a
separate project in entirety a clone and
this is important when it comes to
working with get a clone is identical
with the same teams and same structuring
as the main project itself so when you
download the code you're downloading
exact copy of that code with all the
same security and access rights as the
main code and then you can then check
that code back in and potentially your
code because it is identical could
potentially become the mainline code in
the future now that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborators that can
work on the project which is essential
for projects where particularly where
you have large teams and this works
really well when you have product teams
where the teams themselves are
self-empowered you can do a concept
what's called branching and get and so
say for instance you are working on a
new feature that new feature and the
main version of the project have to
still work simultaneously so what you
can do is you can create a branch of
your code so you can actually work on
the new feature whereas the rest of the
team continue to work on the main branch
of the the project itself and then later
you can merge the two together pull from
remote is the concept of being able to
pull in Services software the team's
working on from a remote server and git
rebase is the concept of being able to
take a project and re-establish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for quite some time on different
areas and maybe you kind of losing
control of what the true main branch is
you may choose to rebase your project
and what that means though is that
anybody that's working on a separate
Branch will not be able to Branch their
code back into the mainline Branch so
going through the process of a get
rebase essentially allows you to create
a new start for where you're working on
your project so let's go through forks
and clones so you want to go through the
process so you want to go ahead and Fork
the code that you're working on so this
is a scenario that one of your team
wants to go ahead and add a new change
to the project the team member may say
yeah go ahead and you know create a
separate Fork of the actual project so
what does that look like so when you
actually go ahead and create a fork of
the repository you get to go and you can
take the version of the mainline Branch
but then you take it completely offline
into a local repository for you to be
able to work from and you can take the
mainline code and you can then work on a
local version of the code separate from
the mainland Branch it's now a separate
Fork collaborators is the ability to
have team members working on a project
together so if you know if someone is
working on a piece of code and they see
some errors in the code that you've
created none of us are perfect at
writing code I know I've certainly made
errors in my code it's great to have
other team members that have your bag
and can come in and check and see what
they can do to improve the code so to do
that you have to then add them as a
collaborator now you do that in GitHub
you can give them permission within
GitHub itself that's really easy to do
super visual um interface that allows
you to do the work quickly and easily
and depending on the type of permissions
you want to give them sometimes it could
be very limited permissions it may be
just to be able to read the files
sometimes it's being able to go in and
make all the changes you can go through
all the different permission settings on
GitHub to actually see what you can do
but you'll be able to make changes so
that people can actually have access to
your repository and then you as a team
can then start working together on the
same code let's step through branching
and get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new branch and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub run crunch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now the benefit you
have here is that by default we're all
working on the master Branch so we
always have the latest code the circles
that we have here on the screen show
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
they can be many branches in git so git
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want to have everything merge back into
the master Branch let's step through git
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate Branch we'll be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project so get rebase is the
process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase and you unplug a branch and
re-plug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote Suppose there are two developers
working together on application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the Repository because
they'll be able to pull the latest
changes from that remote repository so
that they are ensuring that as
developers they're always working on the
latest code so you could pull any
changes that you have made to your full
remote repository to your local
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in get so get and it
initializes a local git repository on
your hard drive get ads adds one or more
files to your staging area get commit
Dash M commit message is it commit
changes the git command commits changes
to head up to the get command commits
changes to your local staging area get
status checks status of your current
repository and lists the files you have
changed get log provides a list of all
the commits made on your current Branch
get diff the user changes that you've
made to the file so you can actually
have files next to each other you can
actually see the differences between the
two files get push origin Branch name so
the name of your branch command will
push the branch to the remote repository
so that others can use it this is what
you do at the end of your project git
config Dash Global username will tell
get Who You Are by configuring the
author name and we'll go through that in
a moment git config Global user email
will tell get the author of by the email
ID git clone creates a get repository
copy from a remote Source get remote ad
origin server connects the local
repository to the remote server and adds
the server to be able to push to it git
branch and then the branch name will
create a new brand lunch for you to
create a new feature that you may be
working on get checkout and then the
branch name will allow you to switch
from one branch to another Branch git
merge Branch name Will merge a branch
into the active Branch so if you're
working on a new feature you're going to
merge that into the main branch a git
rebase will reapply commits on top of
another base tip and get rebase or
reapply commits on top of another base
tip and these are just some of the
popular git commands there are some more
but you can certainly dig into those as
you're working through using get so
let's go ahead and run a demo using get
so now we are going to do a demo using
get on our local machine and GitHub as
the remote repository for this to work
I'm going to be using a couple of tools
first I'll have the deck open as we've
been using up to this point the second
is I'm going to have my terminal window
also available and let me bring that
over so you can actually see this and
the terminal window was actually running
git bash as the software in the
background which you'll need to download
and install you can also run get batch
locally on your Windows computer as well
and in addition I'll also have the
GitHub repository that we're using for
simply learn and already set up and
ready to go all right so let's get
started so the first thing we want to do
is create a local repository so let's go
ahead and do exactly that so the local
repository is going to reside in my
development folder that I have on my
local computer and for me to be able to
do that I need to create a drive in that
folder so I'm going to go ahead and
change the directory so I'm actually
going to be in that folder before I
actually create to make the new folder
so I'm going to go ahead and change
directory
and now I'm in the development directory
I'm going to go ahead create a new
folder
and let's go ahead and created a new
folder called hello world
I'm going to move my cursor so that I'm
actually in the hello world folder
and now that I'm in the hello world
folder I can now initialize this folder
as a git Repository
so I'm going to use the get command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my Hello
wall folder that I've created and you'll
now see that we have a hidden folder in
there which is called dot gate now we
expand that we can actually see all of
the different subfolders that git
repository will create so let's just
move that over a little bit so that we
can see the rest of the work
and now if we check on our folder here
we actually see this is users Matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt in our folder so
here is our hello world folder and I'm
going to go ahead and using my text
editor which happens to be Sublime
I'm going to create a file and it's
going to have in there the text hello
world and I'm going to call this one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
foreign
for us to actually commit the file we
have to go into our terminal window and
we can use the get status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead I'm just going to
select at git add so the git command is
ADD
[Music]
readme.txt so that then adds that file
into our main project and we want to
then commit those files into the main
repositories history and so it's that do
that we'll have the the get command
commit and we'll do a message in that
commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and I can change this so we
can go hello get
get is a very popular
Version Control solution
and we'll
we'll save that now what we can do is we
can actually go and see if we have made
differences to the readme text so to do
that we'll use the diff command forget
so we do get
diff
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on GitHub we already have one so what
we're going to do is we're going to
match the account from GitHub with our
local account so to do that we're going
to go ahead and say get config
and we're going to do Dash and it's
going to be a
globaluser.name and we'll put in our
username that we use for GitHub and this
instance we're using the simply learn
Dash
GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we called the
repository hello dash world
and what we want to do is connect the
local GitHub account with the remote
hello world.get account and we do that
by using this command from get which is
our remote connection and so let's go
ahead and type that in open this up so
we can see the whole thing so we can
type in git remote add origin https
GitHub
.com slash
simply learn
Dash GitHub and you have to get this
typed in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the git push command commit git push
origin
master
so we're going to go ahead and connect
to our local remote GitHub so I'm just
going to bring up my terminal window
again and so let's select get remote add
origin
and we'll connect to the remote location
github.com slash
simply learn
Dash GitHub
slash
hello dash world dot get
oh we actually have already connected so
we're connected to that successfully and
now we're going to push the master Gish
so get
push origin
master and everything is connected and
successful
and if we go out to GitHub now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and Fork a project from GitHub and
clone it locally so we're going to use
the fork tool that's actually available
on GitHub let me show you where that is
located and here is our branching tool
it's actually changed more recently with
a new UI interface
and once complete we'll be able to then
pull a copy of that to our account using
the forks new HTTP URL address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
Fork options in the top right hand
corner of the screen now right now I'm
actually logged in with the default
primary count for this project so I
can't actually Fork the project because
I'm working on the main branch however
if I come in with a separate ID and here
I am I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
created the fork
so you want to say clone or download
with this and so this is the I select
they'll actually give me the web address
I can actually show you what that looks
like I'll open up my text editor
that's the correct
I guess that is correct so I'm going to
copy that
and
I can Fork the project locally and clone
it locally I can change the directory so
I can create a new directory that I'm
going to put my files in and then post
in that content into that folder so I
can now actually have multiple versions
of the same code running on my computer
I can then go into default content and
use the patchwork command
20 so I can create a copy of that code
that we've just created and we call it
that's a clone and we can create a new
folder that we're actually putting the
work in and we could for whatever reason
we wanted to we could call this where
folder Patchwork and that would be maybe
a new feature and then we can then paste
in the URL of the new directory that has
the forked work in it and now at this
point we've now pulled in and created a
clone of the original content
and so this allows us to go ahead and
Fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin Branch
down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
GitHub name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes and merge it into the
master Branch now to do that you would
create a folder and then that folder in
this instance would be called test we
would then move our cursor into the
folder called test and then initialize
that folder so let's go ahead and do
that so let's call create a new folder
and we're going to first of all change
our root folder and we're going to go to
development
I'm going to create a new folder
call it test and we're going to move
into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
call this one test one
and then we're going to do file save as
and this one's gonna be test
two
and now we're going to commit those
files
I could add and then we'll use the
dot to pull in all files
and then git commit
Dash m
files
Limited
make sure I'm in the right folder here I
don't think I was
and now that I'm in the correct folder
let's go ahead and
and get commit
and it's gone ahead and added those
files and so we can see the two files
that were created have been added into
the master
and we can now go ahead and create a new
branch and call this one get branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as we'll call this one
test three dot text
and we'll go ahead and add
that file I need to get add
test three Dot txt
and we're going to move from the master
Branch to the test ramp branch
kit
check
out test underscore
branch
I switched to the test branch
and we'll be able to list out all of the
files that are in the in that Branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do git merge test underscore
branch
as well we've already updated everything
so that's good so otherwise it would
tell us what we would be merging
and now all the files are merged
successfully into the master branch
there we go all my merged together
fantastic
and so what we're going to do now is
move from must to Branch to test branch
so get
check out
test underscore branch
and we can modify the files the test3
file that we took out
and pull that file up
and we can
now what if I
right
and we can then
commit
that file
back
in and we've actually been able to then
commit the file with one changes and now
we've seen as the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
the workers kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future so let's switch to it by to our
test branch
which I think we're actually on we're
going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into our
branch that we're working on so we're
going to go and select get add Dash a
and we're going to commit those files
get
commit Dash a dash m
adding
2 new files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch them to our Master
Branch we want to rebase the master
so we could do git rebase
master
and that will then give us the command
that everything is now completely up to
date
we can go
get
check out
Master to switch to the master account
this will allow us to then continue
through and rebase the test branch and
then list all the files that are all in
the same area
so let's go get rebase
test underscore
branch
and now we can list and there we have
all of our files listed incorrectly now
we will be talking about it how exactly
we can make use of Maven here in
performing various kind of operations
whether it's a burial compilation with
this kind of stuff there so let's talk
about it why exactly we use Maven here
now Maven is something which is a kind
of a build tool which is there for most
of the job-based projects here so this
tool also helps us in building up the
source code by downloading some couple
of dependencies these dependencies are
something which is being configured by
the developers which a developer feels
that the project is dependent on and he
requires those dependencies to be
downloaded so uh this tool is
specifically used to build and manage
any kind of a jar based project whatever
the complexity of the topic or the
project is there so this tool will be
able to handle that particular
requirement or that project so it
simplifies the day-to-day work of a Java
developer and help them in their
projects for performing daily to daily
activities
right now Maven also helps us in getting
the uh specific jar files for each and
every project so if you are going you
can pretty much make a jar file War file
or VR file any kind of a package
mechanism you can follow for your Maven
there so to download the dependencies we
don't have to actually go to the
official website of each and every
software you can easily get it uh the
dependencies of each and every third
party binaries on the mvnreepository.com
this is a website which is present on
which if you visit you will be able to
download the dependencies which is
required by your project or by your
specific project over here so it totally
depends on how exactly you want to
download and you want to get together a
specific repository over here so mvin
repository.com is the one if you visit
you will be able to download and go
through all the jar files dependent jar
files as independencies which you
require for your current project
right so now what exactly is on Maven
all about so let's see on that part so
Maven is a popular open source Belt Tool
which is uh developed by the Apache
group and the primary ownership of this
tool is to build publish and deploy
several projects at once Maven is
written in Java and it can be used to
build projects written in c-sharp Scala
Ruby Etc so apart from java these are
the different programming languages or
the tools which is supported by the
maven here for performing the build
activity so the tool is used for build
and management of any kind of a jar
based project it simplifies the
day-to-day work of a Java developer and
helps them to automate most of the
compilation or build perspective tasks
on the projects
right now Maven is a kind of a bomb
based project so it also known as in
Project object model and it focuses on
of simplification and standardization of
the building process in the process it
take care of all the followings here
builds dependencies reports distribution
releases mailing list so all these uh
things are being taken care as in this
particular process here and ultimately
it's an availability inside the maven
that how the overall build process needs
to be automated and standardized here so
all these particular mechanisms is can
be followed as such over here so that we
can have a good standards and the
simplifications implemented as such here
now in this one we are going to talk
about that how we can work on Maven
installation on Windows and Ubuntu
platform here so first of all we'll do
it on the Windows platform here and
we'll see that how the installation
really works so for this one I will be
doing the installation into my local
system now before even going for Maven
Maven is in kind of a built tool which
is available there primary for
performing any kind of build automation
for the Java based projects now since
it's used for Java we also require the
jdk installation onto our system so for
that both of the installations we will
be covering over here and we'll see that
how the installation really works as
such over here right so before even
going for Maven we have to install the
jdk onto our Windows machine so that we
can proceed with Maven because Maven
requires the jdk installation to be
there now there are two ways of jdeg
installation either you go for the exe
file or you'd extract the binaries of
jdka to a directory and then set up the
Java underscore home path for that but
if you are using the execute table that
will perform the path configurations for
you also so in that case you don't have
to reconfigure it again and again so
that's what we are going to see here so
first of all we will try to do the
installation of jdk so for that we'll go
to the website of jdk and then we have
to click on this one to access the maven
installations part here so that's what
we are going to do so let's proceed and
let's open the Chrome and try to go
through these two URLs so that we can do
the installations one by one
so here I have to just search for jdk8
download so it will show me the URL for
oracle.com so here you will get the jdk
for the different platforms for
different operating systems now here
what you can see here that you have the
configurations in which you have the jdk
installation so we have to so here I'll
go for the windows 64 version because
that is something I will be using to do
the installation so this is the complete
executable link so here you can download
so we are downloading the sdka
u251 so just search for Windows 64 yeah
this is the one so let me click on this
one
quickly log into the web page so once
the login is done you will be able to
see that it's trying to download the exe
file so that was for the 3dk for the
Java now for the May one I am going to
go for the maven download over here so
in this one the maven repo link is there
maven.apache.org is the link for this
one now here we have the different
options you can see here that the binary
zip archive is there then so save
archive is there so depending on the
installation which you want to perform
you should be able to download that file
and it works there so let's first
install the jdk portion and then we'll
go with this one
so we'll click on this one also anyways
because uh it's something we have to
anyways do it so we'll click on this so
that uh this Maven is also getting
started install downloading over here so
once both of them are downloaded we will
proceed further with the setup so let's
wait for another one minute to do this
one
so the Apache Maven is kind of uh
downloaded over here so we wait for the
jdk download also over here so next 5-10
seconds only left out
so first jdk will be there and then
Maven installation configurations will
be there so for me when we just have to
extract into a directory and give the MV
in-home path into the properties uh into
the variables so that it can pick it up
over there okay so we have to extract
the zip file form A1 give the path and
then we will treble check with the maven
command whether it's working fine or not
so let's try to install the executive so
we'll open the directory where the
executables are there so first of all
we'll install the jdk so we'll double
click the installer for jdk it's of
around 200 MB so the complete jdk will
be installed
so we have to just go for next now when
you go with the installer so usually the
Java home paths and all that stuff is
already configured so you don't really
have to change the passwords or Java
home any kind of variable you have to
put it up all those things will be
automatically cleaned up
so it's running the installation
close so then we have to extract the
maven so we have to extract this
directory because here the maven
complete executables and libraries are
there so we just have to extract it and
place into a directory so that we can
further on use it and access the path so
once the axis is done yeah so this is
done there so I can rename it to like
miven so I can rename it like to ask me
one and I can cut it down and put it
into the C drive here now this is the
maven Home Path which is going to be
there right so all I have to do is that
I have to close this one and this one
and this one and this
so what we have to do is that we have to
really go to the system properties so
that we can extract it so I'm going to
open the system properties over here
properties now here the advanced system
settings now in this one we will be
setting up the paths so environment
variable will go now here if you scroll
down you will see all the different
paths which is available there as such
over here now if we want to see that if
the Java is fine or we require for jdk
also so all we have to do is that we
have to open the CMD prompt window R we
are providing so using the window R I'm
opening the Run window where I can say
like CMD now in this CMD I'm going to
say like Java
hyphen version so Java is there so which
means that we don't have any problem as
such now I'll just go here and set up a
new one called mvn underscore home so
here the same path so you can browse for
directory also but I am using the
ambient home over here in this one
now I have just added that and in the
path variable in the last we have to
actually add the
bin directory also so that uh whenever
we run the mvn command so that should
also be resolved so mvn underscore home
is not the only variable which you will
be configuring in fact you will be
configuring the path variable also that
you can go ahead and put up a new
directory entry for the CMD here so CMD
tell the CMD directory you will go there
and whatever the executable is there you
should be able to resolve that so for
that again you have to open the window R
CMD prompt Java hyphen version and then
you can run mvn hyphen version here when
you run the ambient hyphen version it
will let you know that which particular
Java runtime you are using
1.8.0 underscore 251 that's the same one
which we have configured and it also let
us know that where exactly the
executable is there and we can now
proceed further with the execution of
the maven build so you just have to
check out your source code or go to the
directory where source code is there and
you can run the mvn clean install so
that will ultimately go for the build
process so this is the way that how we
can do the installation of Maven on the
Windows platform now since this is done
now we will go ahead with the second
installation where we are going to do
the installation of Maven and jdk onto
the Ubuntu machine so for that we will
log into the particular Cloud lab of
Simply learn in LMS so let's log into
the LMS here so now we are going to do
the installation of jdk and Maven on the
Ubuntu virtual machine here so for that
we will log into the LMS so let's log
into that so this is the uh elements
over here here I have already started
the lab here so all I have to do is that
I have to just click on this link here
so it will open the lab into a new tab
it's a GUI mode of Ubuntu Server so
which we will be using to configure it
so uh here uh first of all I can open
the Chrome browser also so let me open
the browser here so that I can download
the me1 executables as such over here
so GDK we can do it using the package
installation so using the APT executable
we will be doing the installation so I
have to just open the terminal here so
once the terminal is opened you have to
login through the root ID now why we
require root ID so that we should be
able to do the package installations and
other configurations so that requires
root access
now once that is done you have to run
the particular app update command next
you have to install the installer so the
Oracle installer we have to install now
uh once the setup is done so let's down
install the latest package over here for
iCal Oracle hyphen Java 14 hyphen
installation so this will install the
gdk14 over here so for that
configurations now at the same time
we'll go for Maven download also
so here uh we'll go for download me even
so this is the tar file for executable
so I'm going to copy this URL here
and over here we'll say yes yes
so now uh the Java should be configured
and could should be set up over here in
this one so what we really need to do is
that we need to run the Java command to
see that if the installation is done
properly or not so uh let's open the
terminal again another terminal so that
I can perform these steps related to
Maven so I have got the URL so I'm going
to the opt directory which is always a
optional directory where we can do the
setup and all those configurations
so I'm going to run the wget command and
then the URL which I have downloaded so
this should uh download the tar file
over here of Maven now once that is done
so I have to extract it so uh tar hyphen
x v z f and the tar file name this is a
target zip file so I have to use tar
hyphen XP ZF parameter over here to
extract it now uh what exactly I have
got I have got the Apache Maven
directory over here so I'm going to
rename it in order to rename it what we
need to do we need to actually go for uh
the particular move command so move
whatever the existing folder you have
and then I'm going to rename it with the
maven directory now once that is done I
am going to the maven directory and this
is the path or Maven underscore Home
Path which is available as such over
here in this one so here all the bin
directories executables all this stuff
is there so now I can use it like mvn
underscore home and the same
configuration just like we did in the
window so here also it the same process
we have to follow so Java hyphen version
should be let me know that which version
is deployed so 14.01 version is deployed
over here so for Java I know that don't
have to set up the path again here
because I am viewing for the installer
so I don't have to set up the path
right so for Maven now what exactly I
have to do I have to go to the ETC
profile D now here I am going to put up
a particular file now here you can see
that we have the jdk file here which
means that uh the path variables and
everything is configured as such over
here so you can see that Java underscore
Home Path all these variables are copied
here so I'm going to also create a same
file similar to this one here but in
this case what I'm going to use it that
I'm going to remove some of the values
so I'm going to have some of the
parameters added so in order to do it
effectively I can do the Eco command so
that I can copy the things from here and
I can simply put it up into the file
here so I'm going to have the path
variable added up over here now why I'm
putting up the paths because it's the
same configuration which we did over
there also so opt slash May 1
slash pin so all the bin directory
should be added to the specific path
variable so I'm just trying to create a
file called maven.sh just like a jdk DOT
sh over here I'm also creating similar
kind of file over here called maven.sh
now I'm not going in the vi mode or
other editors because I just want to
make you understand that how I am
copying it the content I'm taking the
jdk.sh as in reference and putting up
the details over here in this one and
then I'm going to export mvn underscore
home over here which is related to like
opt
slash
Maven so this is what we are going to
put up over here and then Maven in the
end Maven this is the file which you
should be having now why we are using it
because of the fact that it's being used
or it will be utilized in such a way
that you can perform the changes or
whatever configurations you want to
perform this will help you to set up the
certain parts like a path and ambient
home so that you can run these scripts
as such over here so I'm going to give
the executable permissions to this one
and that's it so now what exactly I have
to do is that I have to do the source
here
profile
dot d maven once that is done I will be
able to run the mvn hyphen version over
here and the jdk part is already open so
it's going to refer to that part so this
is the way that how we configure them
even and the jdk specifically on the
Ubuntu virtual machine so pretty much
that's it for this demo in this demo we
primarily talked about it how we can do
the installation of jdk and Maven on
Windows and how we can do the similar
task on the maven on the Ubuntu virtual
machine also here hello everyone welcome
to this demo in this demo we are going
to talk about it how exactly we can do
the installation of Maven on the Mac
operating system so here we are going to
talk about it how we can do for a
specific setup where we can do for the
installation of Maven in a Mac operating
system so let's see so there are two
ways of doing the installation here
first is through the blue executable on
the Mac now this blue utility on Mac is
just like apt or yum utility which is
present in most of the environment
ensure most of the platforms here but
here we are using this preview to do the
installation of Maven on the Mac
operating system and again the next one
is just in a particular mechanism where
you should be able to have these set up
like you can download the binders from
the Apache website and then you can
export the M2 underscore home and the
path variable so that you can start
using the maven executable so we'll talk
about these things and we'll see that
how exactly we can go for this setup so
let's login uh to a particular machine
or Mac operating system so where we will
be able to do all this activity
so this is the Mac system which we have
here now in this one what we are going
to do is that we are going to open the
terminal so we are going to open a
terminal here so this is my terminal so
usually in Mac you don't require any
other mechanism so the inbuilt terminal
is more than sufficient for doing the
stuff because it's similar to a Linux
based platform here so I'm going to run
the command here so first of all I'll
just double check that if my Brio
executable is working fine so I'll just
run the Brew help command to see that uh
if my Brew is working fine as such over
here in this one now I have the command
called preview install so that is the
command which we need to use now in this
one we have to actually give the
specific package which we want to
install so we will say like Brew install
maven
so it will download uh the uh specific
executables on the system
from the website it will download the
maven executables so it will take some
time but ultimately it's trying to
download it under the particular tar
file so you can see here that it's
downloading 3.6.3 tar file over here so
uh I can see that the installation is
done so the 3.6.3 version should be
installed over here so when I do like
Maven uh mvn under hyphen iPhone version
over here so you can see that the
3.6.3 version is deployed as such over
here now this is something which is done
with the purely command over here called
Brew which really helps us to do any
kind of package installations not only
Maven other packages also can be
installed pretty much with the help of
this executable now in order to see the
manual stuff what we really need to do
is that we need to actually clean this
specific installation so that I should I
will be able to show you the manual
installation also over here so what I'm
going to do is that I'm going to run the
command called
Brew uninstall Maven so that the maven
should be removed from your system so we
are just trying to remove the maven from
here and uh you can see that the maven
executable is not working as such over
here so which means that the maven is
uninstalled in this case but why I have
uninstalled here so that I should be
able to go ahead and download some
custom Maven over here in this one so I
have opened the browser here so I'll go
for the maven educatable so we'll go for
Maven download so that we should be able
to download a specific the executable
here so I'm going for this directory so
here uh 3.6.3 version same is available
here so I'm going to download the bin.z
which was the file we have so this is
also a kind of a Linux platform right so
that's the reason why uh here the tar
file should work perfectly fine so here
I'm going to the opt directory where we
can have all the optionals related
packages so if it is not there so I can
create a directory called slash opt
so I can log in with
root
so if you don't have the root access so
you can go for the applications in the
applications you can actually create a
directory called maven so here I can go
for me one
yeah
so here we will be downloading the VAR
file here so the tar.gun zip file we are
downloading over here so I'll give the
path here so once the war file or
duplicate command I am using to do the
installation but sometimes the obligate
is not available so I can go for the
Brew install wget also if that package
is available on preview the installation
will be done so we'll try to do the
installation using replicate so that we
will be able to get a tar file generated
or downloaded there are other ways also
you can download it from the browser
also yeah you can go to the browser and
here you can actually click and download
it and then you can transfer it locally
to this specific directory also so both
the possibilities are there but I'm just
trying to download The W gate command so
that it will be easy for me to do the
setup and to do the download over here
so I'm going to run the same command
again
so it will download the tar file so this
is the tar file which is available over
here so I'm going to extract it over
here
so once the extraction is done so I'm
going to rename it like to Apache maven
so that I should not have any kind of
versioning or these kind of parameters
over here and the Apache dot uh Maven
hyphen the star file I'm trying to
remove as such over here so this is what
I have got as an end result over here
now this is the current directory in
which I have these uh applications and
these setups going forward over here so
if I run the mvn hyphen iPhone version
command
so that will not be giving me any kind
of output over here because uh still I
have extracted the tar file but that is
not the activity is not complete I have
to exactly set up some environment
variables so that I should be able to
run this mvn command so what I really
need to do over here is that we have to
export certain variables
application slash me one
Apache Maven so this is the home
directory which I am trying to configure
over here and then next thing is that
path variable so I'm going to have we
are going to coordinate the path
variable with the folder so I'm going to
have the folder structure here so
the bin directory we have to provide so
I'm going to copy here and paste and
then I'm concatenate it with the bin
here so now my environment variables are
established so my mvn hyphen iPhone
version command as you can see here is
that uh you know it's being picked up
over here but it's saying that the
particular Java runtime is not available
so I can go for Java blue install Java
so that should be enough there to
install a Java package there so you can
see that it says that it's already
installed so and up to date so that's
the reason why it's not able to do the
installation so I'm going to reinstall
open jdk
because what happens that when we do the
installation some environment variables
related to Java should be formatted or
should be extracted over here right so
as you can see that uh here it's saying
that if you want to really use the
executable of java you have to
concatenate the path like this so let's
run that and uh then we can see that the
particular executable of java will also
be
find out over here so Java hyphen
version showing that yes it's available
there and now my command
should be able to show me that yes it's
working fine the GDK was already
installed but the problem with that is
that since it's installed into a
particular location so you have to
actually override your path value so
that you should be able to see in that
directory and find out the Java
executable so that I have got by doing
the de-installation I got the command
over there in the output saying that
this is the installation which is done
and now you just have to extract or you
have to just you know put that bar
directory into the specified path
variable and that's how you got the
maven and Java both command lines
working over here so this is how you
will be doing the maven installations
all together how the overall
installation of Maven and the jdk really
works on Mac operating system
now what exactly is in Maven repository
all about
so Maven repository is something which
is uh kind of a directory or it's kind
of a location where we have all the jar
files packaged all together in a single
location so depending on the software
which you want to download the dependent
Java files can be downloaded from there
so the metadata refers to the pump files
relevant to each project here this
metadata is what allows Maven to
download the dependencies now there are
three type of repositives from where we
can download the dependencies the first
one is the local repository second one
is the remote repository and the third
one is the central repository so these
are the primary three type of
repositories which is available there
for our specific Maven over here so
whatever repository you want to follow
you want to access you can pretty much
access it and you can go with that
right now the local repository will be
primary present onto your PC that's the
way that how it's present there and the
particular remote repository is
something which you can store on any
remote machine or remote server and the
central repository is something which
you can have it on to the Internet so
usually the central repositories is
something in which we can actually host
our different kind of dependencies and
the jar files over there
so local repository on the machine of
the developer where the project material
is saved so all the particular
dependencies jar file will be available
on your local machine remote machine is
the location where you can actually
store all your particular dependencies
from where you can download these
dependencies whenever you require it so
this and repositories work similar to
the central repository whenever anything
is needed from remote repository it's
first downloaded into your local
repository and then it will be utilized
so if you feel that the dependency is
something which is not there in the
local it will be first downloaded from
the remote repository and then it will
be utilized or will be referenced onto
the local machine there
and Central repository is refers to the
memory Community repository there where
you will be able to see each and and
every dependency present on that
repository so Maven downloads the
repositories from here in the local
repository whenever they're needed or
whenever required all these dependencies
are cached locally from where it will be
referenced in the next or you know
whenever we want to refer that we will
be able to do that particular reference
easily on that part
so this is the way that how the three uh
particular repositories are Maven
repositories really helps us for
performing various kind of automations
as such here
right now what are the basic concepts of
Maven here so some of the couple of
basic concepts which is present in Maven
is like for the first time we have the
project object model so project Palm
reference to the XML file which has all
the information regarding the project
how this particular project should be
built up how the different build process
should be automated there it also has a
description of the project details
regarding the versioning versioning
information is also stored as such in
this XML file in this particular form
reference file as such over here the XML
file is which is there in the project
home directory so Maven is uh something
which search for the pump file in the
current directory if it's able to find
that particular pump file so it will
execute or it will proceed with the
build step otherwise the it will hold
there itself so this Pawn file is very
important because it will act as a kind
of a build script and primarily the
maven tool will be able to process each
and everything which is configured as
such in this specific XML file there and
according to that only it will function
or it will proceed further
right now second thing is the
dependencies and repositories so
dependencies are usually refers to the
jar files so the Java libraries which we
need in our into our particular project
so while working on the project we may
be having the dependency on couple of
dependencies so these dependencies we
have to actually download or we have to
actually refer and once these are a
particular information or these details
are referred so what will happen that uh
we need to download these uh specific
dependencies from the particular Maven
repositories so if the dependencies are
not present in your local repository
then Maven will try to download it from
the central repository and cache it into
your local repository but first of all
it will look on into the local
repository and if it is not present in
local then only it will go to the remote
or the central Repository
then we have build life cycles and phase
and goals there so build life cycles
consists of the sequence of build phases
and each field phase consists of
sequence of goals here now each goal is
responsible for performing a specific
task when a phase is uh running all the
goals related to that phase and its
plugins are also used to performing the
compilation preparing the artifacts
downloading the dependencies all
different things being done by the
plugin cm then we have the build profile
so build profiles refers to a set of
configuration values now generally we
have the generic build process for all
the environments all the particular code
base there but sometimes we do have some
kind of a differences there so that is
where we can go for the profiles because
a specific profile can have its own
configuration and whatever it's required
for the build process we can have
different different configurations
stored inside the maven's build script
here
then with different build profiles are
added to the pump files and which
enables the different uh build as such
here so depending on these profiles we
can actually decide that how we need to
proceed with the build process and how
the different kind of build order
missions needs to be performed here then
a build profile helps in customizing the
build for the different environments so
that also we can perform as such while
work forming or while working on the
different components and the different
management here so a build profile is
something which you can configure you
can utilize and depending on that build
profile we will be deciding whether to
proceed further or how to design on that
particular Factor so these are something
which we need to take here or we need to
work according to see which we can
decide that how the specific
configurations and specific automation
needs to be performed here
now usually when we go for the uh Maven
there so we have to use some couple of
Maven plugins now the goal of using
these uh specific plugins is to automate
some of the basic stuff like compilation
creating the bar file creating the jar
file so when we are using the plugins we
don't have to configure the steps or
write the steps that how these uh
specific plugins should be installed or
should be referenced it's ultimately the
internal configurations which we are
looking forward or which we are using
while working on these specific plugins
so you can download some couple of
plugins using which we will be able to
decide that how a specific goals needs
to be achieved here so Maven has its own
standard plugins that can be utilized so
if uh we know want to go for a custom
one so you can do that but pretty much
you can go for the default one and you
can pretty much have some specific goals
executed with the help of these build
plugins
now next thing is the maven architecture
so uh in Maven what happens that uh
let's talk about the basic architecture
of Maven here so the maven executable
when it's deployed so it usually uh
reads the palm.xml file which is the
build script here now once the specific
download XML file is being read out so
it will process the components it will
uh you know download or it will process
the dependencies plugins life cycles
phases and goals and even the build
profiles and uh if any kind of
interaction is required so it will try
to connect to the central repository or
remote repository and according to that
it will provide the information and
provide the details as such so Maven
repository is something which is very
important because it's ultimately trying
to connect to the maven repository and
try to achieve the various kind of
automation here so it's very important
because ultimately it's trying to help
us to go for the complete end-to-end
build automation when we'll go for a
specific Palm based build automation
so uh Maven is going to read the
palm.xml file then download the
dependencies defined into the power XML
file and that dependencies are cached
locally from the remote or the central
repository now then create and generate
a remote a report according to the
requirement and execute the life cycles
phases goals plugins Etc so it will
process the power XML file there and
whatever the plugins and goals or the
life cycles which is configured there
step by step each of them will be
executed into a sequence here
now let's talk about the overall build
life cycle of the maven here so what
happens that uh the maven build
lifecycle is something which is a kind
of a collection of different steps here
and this is something which is followed
uh to perform a build automation for a
specific project so that they are pretty
much three steps which is being done so
first one is the default so it handles
the project deployment there second
handles the project cleaning which is a
clean day and then we have site so it
handles the creation of the project
sites documentation there
so these are the three built-in
particular life cycles which is
available there at the beaver level
which you can utilize to perform a
various kind of particular steps
execution for your project
right so the build cycle has the
different build phases or stages there
because when we are performing a build a
variety of things needs to be performed
over here so the first one is the
compile then test compile is there so
compilation is there for the source file
and test compile is there for the test
cases test will execute the test cases
package will package the bundle then we
have the integration test to run the
integration test cases then we have the
verify then install goal is there and
the last one is the deploy which is
there to deploy the artifacts to the
artifactory so all these things are the
different phases and the stages which is
being followed as per a specific build
here so during these builds these are
the different phases and these stages
which we normally follow for performing
the build automation
now what is the exactly advantage of a
specific Maven here so that is something
which we need to talk about that how the
particular automation needs to be done
or how we can actually go for the
configurations where we feel that yes we
want to perform or we want to take
certain benefits out of the
implementation of Maven here so for that
what happens that we have to see that
the various kind of benefits the first
one is that Apache Mi one helps manage
all the processes there such as building
documentation releasing and distribution
in project management here so the tool
simplifies the process of project
building it increases the performance of
the project and the building process all
together and the task of downloading jar
files and other dependencies is also
done automatically we don't have to
indulge in any kind of download part as
such manually over here
right and Maven provides easy access to
all the required information so Maven
makes it easy for a specific developer
to build a project in the different
environments so we don't have to worry
about the environment we don't have to
worry about the infrastructure nothing
is required everything is available
there inside this package and you know
depending on the particular you can
write the source code on one machine and
you can pretty much do the build on
another machine and this is the reason
how the build automation really happens
on the first place because the developer
is writing the source code in one
location and the same code is deployed
onto the particular or built onto this
Jenkins or any kind of build tool there
right in Maven you can easily add new
dependencies so you must write the
particular dependencies in the form file
so if you feel that you want to download
some particular dependencies so all you
have to do is that you have to put that
dependency into the specific palm.xml
file there and depending on that the
execution will be really performed and
executed as a chair right so let's talk
about the demo now so let's see that how
exactly we can go for this demo and we
can perform the various kind of
automations
so this is the virtual machine which we
have here on which the maven is already
installed so we can run like mvn
so Maven will be available as in 3.6.3
here now I'm going to run a particular
command called mvn archetype generate
here
let me create a directory here
our temp directory
and to perform this activity over there
so mvn
type
generate now once we run that so what
will happen that it will download some
of the bin entities there because uh
ultimately what we are trying to do is
that we are trying to generate a new
project like a maven project so couple
of a particular plugins will be
downloaded by the maven executable so
that it can achieve that particular
execution so we just have to wait for
downloading all these values now here
it's trying to give us a particular
attributes like it's asking the
different attributes over here so what
exactly we want to configure so if you
want to configure you can provide that
details otherwise you can perform or
whatever the setup you want to perform
now here it's asking for the version so
which kind of version we want to follow
so I am going to follow like five here
so I'll press five
then a group ID which is there so it's
basically a kind of a group mechanism so
I can say like com dot simply learn so
that's a value which I'm providing here
artifact ID I can make it like a sample
project or something like that I can do
so that will be the artifact uh ID which
is there so version I am keeping the
same only so and uh yeah so package same
here
so I just want to create so I'll just
provide the value called yes and enter
so with this what will happen that our
sample project will be created here
right so whatever the artifact ID you
provided so according to that the
project is created in this directory so
you have to go into this directory and
see that what exactly the files are
created there so you have the pound.xml
file now this Palm Direction file when I
open so you can see here that there are
some attributes like you can have the
values uh related to what version or
group ID you want to follow so this is
the group ID this is the artifact ID so
this is jar file by default you can
change it according to your requirement
then this is the version and if you feel
that you want to do the changes to the
name that also you can perform here so
by default the J unit dependency is
added but if you want to keep on adding
your own custom dependencies you will be
able to do that now in this case if you
run like mvn clean install so it will be
considered as in particular Maven
project a power.xml file is already
there present in the local directory so
according to that the execution of the
steps will be performed and according to
that you will be able to get some
desired values here so ultimately in the
Target directory you will be able to see
that some couple of Jar file or a
specific jar file is generated here so
you can see that in the Target directory
this jar file or this artifact is
generated here so this is a way that how
we can actually go for a generic one
like a new particular project and later
on you can depending on your uh
understanding you can keep on adding or
you can keep on modifying the
dependencies and that's how you can get
the final result there
so that's it for this demo in which we
have find out that how exactly we can go
for a particular project preparation
with the help of mwin executable
if you are looking to enter the world of
devops Simply learns postgraduate
program in devops is the perfect place
to start this comprehensive program is
designed to equip you with the skills
and knowledge you need to succeed in a
devops role with over 500 hours of
planning content Hands-On projects and
Industry relevant case studies you will
gain practical experience in tools like
git Docker kubernetes and more so
whether you are an ID professional
looking to upskill or a fresh graduate
looking to start a career in devops
don't miss out on this opportunity to
join the ranks of successful devops
professional and roll now and take the
first step towards a brighter future
welcome everyone to this topic in which
we are going to talk about that what
exactly is the different Maven interview
questions here now in this one we are
going to talk about what are the
different questions some couple of
questions we are going to go through and
we'll try to understand that what
exactly the answers are now let's talk
about the first question over here so
what exactly is an Maven here so Maven
is nothing but a kind of a popular open
tool open source build tool which is
available there now before May 1 there
were a couple of build tools which was
present like and and you know a lot of
other Legacy tools was present there but
after that Maven is something which was
uh released as an open source tool and
it really helps the organization to uh
automate some couple of build processes
and you know have some particular
mechanisms like build publish and
deploys of different different projects
at once itself so it's a very powerful
tool which can really help us to do the
build automations we can integrate them
with the other tools like Jenkins and
you know we can automate them we can
schedule the builds so a lot of various
advantages we can get with the help of
this tool here it's primary written in
Java and it can be used to build up
various other kind of projects also like
c-sharp Scala Ruby Etc so all these
other typical tools can also be built up
with the help of this tool so this tool
is primarily used to do the particular
development and management of the
artifacts in the Java based projects so
for most of the jar based projects
nowadays this is the default tool and
it's already integrated with the eclipse
also so when you go for a new waiver
project automatically uh it will be
created for a job project you can use it
for other languages also but yes default
choice of java of of the Java
programming language is Maven build tool
only now let's talk about next question
so what does the maven help with so
Maven Apache Maven helps to manage all
the processes such as build process
documentation release process
distribution deployment preparing the
artifact so all these tasks is being
primary taken care by the Apache mav1 so
this tools amplifies the process of
project building it also increases the
performance of the project and the
overall building process so all these
things are something which is being
taken care by these specific Maven tool
here so it also uses the particular uh
you know it downloads the jar files of
the different dependencies for example
if your source code is dependent on some
of the Apache web service uh jar files
or some of the other third-party jar
files in that case you don't have to
download those jar files and keep in
some repository or keep it in some Live
directory you just have to mention that
dependency in the maven and that jar
file will be downloaded during the build
process and will be cached locally so
that's the biggest Advantage which we
get with Maven that you don't have to
take care of all these dependencies
anywhere into your Source Code system so
Maven provides easy access to all the
required information it uh helps the
developer to build the projects and
without you know even worrying about the
dependencies processes or different
environments or different because it's
in a kind of a tool which can be used in
any platform Linux or Windows so they
don't have to do any kind of conver
versions so all they have to do is that
they have to just add new dependencies
and that should be updated into the Palm
file and depending on that dependencies
the source code will be built up and
they don't have to refer any kind of
third-party jar files so they don't have
to play with the class bars during the
build process so no customizations is
actually required with this one now the
next question is that what are the
different elements that Maven take case
off so there are different kind of
elements which is being taken care by
Maven so uh the these particular
parameters are elements are builds
dependencies reports distribution
releases and mailing list so these are
the typical different different elements
which is being taken care by the May one
during the build process and during the
preparation of the builds here so all
these things you can they can explore
they can extract on that part and they
can fully understand that how they can
work on all these different different
processes now next question is that what
is the difference primary difference
between the and and Maven first of all
both of them are time to use for the
Jarvis Project so and is the older
version and Maven is something which was
launched after the ant here so and has
no formal conventions like so which can
be uh you know coded into the build.xml
file there but yes the maven has
convention so information is not
required as such in the palm.xml file
there so and is procedural whereas Maven
is declarative over here so and does not
have any kind of life cycle so it
depends on you that how you program the
and there but Maven is having a lot of
life cycles there which we can configure
we can utilize so the uh ant related
scripts are not reusable because you
cannot reuse it and you have to do some
kind of customizations in order to work
on that but yes Maven is not having much
of the project related any kind of
dependencies they can be easily reusable
there because there is nothing about the
palm.xml file it's just the artifact
name and the dependencies which is uh
something we can override or we can
change and then the same form protection
file we can reuse assets for the new
project also so that is where the
reusability comes into the picture now
ant is a very specific build tool so we
don't have to there is no plugins as
such which is available there you just
have to code everything that what build
process you want to prepare whereas in
case of Maven we have the concept of
plugins which can really help us to
understand that how we can make or use
of these plugins so that we can have the
reusability implemented so these are
some of the differences which is
available there between the ant and
Maven here now next thing is that what
exactly is in Palm file all about so
Palm file is nothing but its kind of a
XML file which is available there and
it's have having all the information
regarding the project and the
configuration details so it primarily
used over here that how the
configuration needs to be done and how
the setup should be performed as such
here so power.xml file is the build
script which we prepare uh you can
prepare it using a particular component
or you can have a particular mechanisms
or if you feel that you want to have
some kind of setup so all these things
typically can be implemented can be done
with the help of pill tool so build
tools can be really helpful for us to do
the automations and it can really help
us to understand that how some build
processes we can automate with simply
with the help of palm.xml file here so
the developers usually put up everything
inside these uh dependencies in the XML
file here so so this is the file which
is uh usually present in the home
directory it's in the current directory
so that once the build is triggered it
will be picked up from that directory
and according to the steps according to
the content of the palm.xm file the
build will be processed or will be
created here now what I'll encoded into
the pump file here so the different
components which is included into the
Palm dot XML file here is the
dependencies
developers and contributors plugins
plugin configuration and resources so
these are the typical components which
is a part of a pomodexo file which can
be a same form a lot of projects you can
do some customization and then the same
pump file can be reused for the other
projects also now one of the minimum
requirement of the elements which is
there for a palm for Palm XML file so
without which the pound XML file will
not be validated and will will be
getting a kind of validation errors so
the minimum required elements are
project root
so it should be 4.0.0 the group ID of
the project the artifact ID of the
project and the version of the artifact
these are the minimum things which we
want to Define so that we can understand
that what kind of artifact we are trying
to prepare or we are trying to create
here so these are the minimum required
Elements which is required in the Palm
decimal file without which the
validation of the pump file will fail
and the build will also fail here
now what exactly is the mint uh with the
term called build tool so build tool is
an essential tool is a kind of a process
for building or compiling the source
code here so it's needed or it's
required for the below for generated
processes if you want to generate the
source code if you want to compile the
source code you want to generate the
source code you want to generate some
documentation from the source code you
want to compile the source code or you
want to package the source code whether
it's a jar file it's a war file or it's
a ER file so whatever the packaging mode
you want to select you will be able to
do it with the help of the particular
build tool here and if you feel that you
want to upload these particular
artifacts to the artifact tree whether
it's on remote machine or locally there
so that also you can do it with the help
of this particular build tools here so
build tools can be helpful in doing a
lot of activities for the Developers
now one of the different steps which is
involved to install Maven on Windows now
all you have to do is that you have to
just first of all download the tar file
from the maven Apache Maven repository
there once that is done so what happens
that you have to set up some couple of
environment variables now if you
download the Java jdk using the exe file
in that case the Java undersco home will
be configured automatically but if it is
not available and you are not able to
run the Java command line in that case
you have to set up the Java underscore
home and then similarly for Maven you
have to go for the maven underscore home
that particular variable you have to
configure now once that is done all you
have to do is that you have to edit the
path variable so the bin directory of
the maven extracted folder you have to
put it up into the path variable and
once that is done what will happen that
you will be able to check the latest
version the version of the maven over
there if it is like some old version
again you have to extract the latest
version and do the steps all together
again so these are some of the ways that
in which you can actually go for the
installation or the configurations of
Maven on the Windows platform
now what are the different steps which
is involved for the installation of
Maven and Ubuntu so Ubuntu it's fine you
just download the package of java jdk
there once the jdk is installed over
there what you can do you can simply go
and say that yes I want to search for a
particular Maven package which is
available there so once the jdk is
installed all you have to do is that you
have to configure the Java underscore
home M3 underscore home main one disco
home and the path variable once all the
path variables are something which is
configured then you will be able to
check the latest version like whether
it's a the version is correct or we are
getting the standard version over here
or not over here so that's the main
mechanism that how we will be able to
you know configure the maven on Ubuntu
km now what exactly is the command to
install to char into the locals
repository now sometimes what happens
that we are not able to fetch like some
dependencies not present on the uh
particular Central repository memo
repository or your artificial repository
in that case you have some third party
jar which we want to install locally
onto your repository so in that case we
can go for the uh but we can download
the jar file there and then we can run
the command called mvn install install
hyphen file and then we are giving the
path like hyphen D file where the path
of the file should be provided now once
that is done so what will happen that in
the local dot M2 directory these
specific artifact will be downloaded and
will be installed here so this is a
mechanism where you will be able to
configure or you will be able to set up
the artifacts locally the Java file
locally here in the local repository so
next question is that how do you know
that the version of the maven being used
here so the version of the maven is
pretty easy to calculate so all you have
to do is that you have to just go for
mvn and space hyphen iPhone version the
moment you do that it will let you know
that what jdk or what Java version you
are using and it will also show you that
what particular Maven version your going
to use here so all that details you will
be able to get with that particular
command here
now what exactly is the clean default
and write-in variable here so these are
the build Cycles which is available
there in within Maven so these are the
built-in build Cycles so for clean what
happens that this life cycle will help
you to perform the project cleaning so
usually during the build there are some
files which is created into the target
directory so the clean life cycle is
essentially helping us to clean up all
that directory all that particular
Target directory and when we talk about
the specific default so default a
lifecycle handles the projector
deployment that is the default life
cycle and site is something which is uh
you know helpful for creating the site
documentation
you know it's kind of a life cycle which
is available there so clean default and
site other different life cycles which
can perform different different kind of
attributes or different tasks here next
question
what exactly is an Maven repository so
Maven repository refers to the
directories of the package jar files
that contain metadata now the metadata
refers to the Palm files relevant to
each project so here you can able to get
your artifacts uh stored there you can
download these artifacts also during the
maven build if you put up that
dependency you will there are different
kind of repositories which is available
there one is the local repository one is
a remote repository and one is the
central repository so these are the
different typical type of repositories
which we have where we can store the
artifacts also and from where we can
download the artifacts also whenever
required now the first one is the local
repository so local repository refers to
the machine of the developers itself
where all the project related files are
stored there now whenever we work on the
uh particular Maven so there is an in
the home directory.m2 folder is created
now usually whatever the artifacts
downloaded from artifactory or from the
maven repository it gets cached locally
there and once it is downloaded next
time it will not download the same
artifacts of the same dependency all
together again so this local repository
is something which is available locally
on the developers machine only so it
contains all the dependent jars which a
particular developer is downloading
during the maven build now remote
repositories refers to the repository
which is present on the server and from
where we will be downloading the
particular dependencies so the when when
we are running the maven build on a
fresh machine so usually over there the
local repository does not exist so in
that case what happens that the dot M2
directory is empty but the moment you
run the build what will happen that the
artifacts or the dependencies will be
downloaded from the remote repository
and uh once it is done once it's
downloaded it will be added or it will
be downloaded cached locally there and
it will be helpful in the future run so
that will be considered as a local
Repository because all the artifacts all
the dependencies are downloaded there
and Central repository is something
which is known as the maven Community
where all the artifacts is available
there so usually we cache or we mirror
these Central repositories as our
particular remote repositories because
it could be a possibility that these
remote repositories are something which
we are hosting into our organization and
Central repository is something which is
available centrally for everyone to use
it so these are something you know some
kind of repositories where each and
every artifacts will be stored and
anyone will be able to have the access
to these uh particular artifacts here so
these artifacts are every artifices
every open source uh artifits is
something which is available over there
to these Central repositories now how
does the maven architecture really work
here so the maven architecture really
works in the three steps the very first
step is that it reads the palm.xml file
here that's the very first step second
in it downloads the dependencies defined
in the pound of XML file into the local
repository from the central or the
remote repository here once that is done
so it will uh you know create or
generate the reports according to the
life cycles which you have configured
whether it's a clean install site deploy
package or whatever the life cycle you
want to trigger you will be triggering
that particular life cycle and
corresponding to that the build or a
specific task will be performed so these
are the three steps in which the overall
build or any kind of execution of power
XML file really happens here now what
exactly is the maven build lifecycle so
Maven lifecycle isn't nothing but
collection of steps here that needs to
be uh followed for doing a proper build
of a project here so there are primary
three built-in Cycles which is available
there default which handles the project
deployment clean which handles the
project uh cleaning there and site which
handles the create version of the
project sites documentation so these are
the three primary built-in built Cycles
life cycles which is available as such
now so build lifecycle has you know
different kind of phases or the stages
here because in the previous uh
particular slide we were talking about
what what are the different uh
particular build life cycles which is
available there but these are the
different phases like what are the
different step-by-steps executions like
further deep down which is available
there inside a specific Maven build life
cycle so here you can see that it's
compiling then the test compile test
execution is there then package
integration test to verify install and
the lastly deploy here so these are the
different build phases which is
available as such over here so what
exactly is the command to use to do a
particular Maven side so MV Insight is
something which is used to create a
maven site here now usually whatever the
artifacts is prepared that will be
prepared in the Target directory so here
also you will be able to see a site
directory which is available there in
the Target directory which you can refer
for the site documentations what is the
different conventions used while main
naming a project in May 1. so usually it
involves three components so the full
name of a project in Maven includes
first of all the group ID for example
count at Apache com dot uh example so
these are some of the uh particular way
that where you can provide the group ID
artifact ID can be exact project name
like Maven project or whatever the
project you are creating so sample
project example project so these kind of
things will be there in the artifact ID
and lastly is a version like which
version of your artifact you want to
prepare like 1.0.0 hyphen snapshot
2.0.0 so like this information you are
providing that what particular version
you are trying to configure here now
let's move on to the intermediate level
where we will be having a little bit
more complex quotients related to the
maven here now what exactly is in Maven
artifact now usually what happens that
when we do a build process as an end of
result of the build process we will get
some artifacts for example when when we
build a dotnet project so there we will
be able to have a exe or dll files as an
artifacts similarly in case of May 1
when we do a build process there we get
the different kind of artifacts like
depending on the packaging mode like jar
file War files or the ER files here so
these are something which is you know
getting generated during the build
process during the moving process and
you can store them into your local
repository or you want to push them to
the remote repository it's something
that totally depends on you so Maven is
a tool which can help you to create all
the artifacts whether it's a jar file
whether it's a VAR files or whether it's
a ER file here
and every artifact is having three
attributes the first one is the group ID
the artifact ID and a particular version
and that's how you will be able to
identify a full-fledged artifact as such
in Maven so Maven is not about only the
name of the Char file it's actually
referring to the attributes like crop ID
the artifact ID and the version of the
artifact here now what are the different
phases of the clean life cycle here so
clean is something which is being used
to clean the target directory so that a
fresh build can be triggered there so
there are three steps pretty clean clean
and post clean here so if you wish that
you want to override the particular
lifecycle configurations and you want to
run some particular steps before the
clean activity so you can do it into the
pre-clean and if you want to do it like
some steps after clean then post clean
can be utilized now what are the
different phases of the site life cycle
so pre-site site poor site and site
deploy so these are the different phases
which is available there in the site
life cycle what is exactly we meant by
the maven plugin now this is the huge
difference between the ant and Maven
here because in and we were not having
this that much support of the plugins
and that's the reason why we had to deal
with all the build configurations so we
have to Simply put the overall build
process that how the build should be
triggered but that is not something
which is there in case of Maven in May 1
we have a lot of flexibility because we
can actually put up what exactly build
configurations we want to put here we
can put some features like important
features over here in Maven and these
plugins we can utilize for example I
want to perform a compilation now I
don't really want to do any kind of
configurations in this one so what I can
do is that I can simply use the
compilation plugin in May 1 and that can
really help me because I don't have to
unnecessarily write or rewrite the
configuration that how the compilation
should be done it's something which is
pre-configured or pre-written in this
plugin is that I can simply import the
plugin and I can do the build process or
the compilation process in a pretty
standard mode so I don't really have to
do any kind of workarounds with that and
simply with a small automations I will
be able to reach that how this Maven
plugins can be integrated into my palm
or XML file and I can desire or I can
have some particular procedures and some
straps executed there so that's the
biggest benefit which we really get with
the help of Maven plugins now why
exactly the maven plugins are utilized
so to create a jar files to create the
war files to compile the code files to
perform the unit testing to create the
project documentation and to create the
project reports so there are variety of
things in which we can actually use this
Maven plugins through the Integrations
within the pound XML file there so it's
all about the plugins you just import
the plugin and that desired activity
will be performed there now what are the
different type of plugins which is there
so you can have either build projects
for performing the build activities you
can have some build programs for
reporting plugins also there which can
be only generated or utilized to
generate the reports to process the
reports and do any kind of formatting or
any kind of processing on the reports
here so that is where the reporting
plugins are utilized now what is exactly
the difference between the convention
and the configuration in May 1. so
convention is in particular process when
the developers are not required to
create the build processes so
configuration is when you know the
developers are supposed to create the
build processes so the users do not have
to rectify the configuration in detail
and once the project is created it will
automatically create a structure so they
must specify every in case of
configuration you have to provide each
and every details so that's how the
configurations really happens because
you have to put every detail into the
power XML file and that's how the
particular configurations really work as
such so this is the huge difference
between the conventions and the
configurations here now so why exactly
said that Maven uses conventions over
the configuration so maybe it pretty
much does not puts any efforts like on
the particular developers that they have
to put each and every configuration so
there are some ready-made plugins which
is available there and pretty much we
are making use of that so that in such a
case we don't have to worry about the
executions and we will be able to pretty
much work on that so conventions like
Maven uses the conventions incident of
the configurations so the developers
does no they don't just have to create
the maven project the rest of the
particular structure will be taken gear
automatically so they are not uh you
know expecting that the developers
should be doing the configuration work
and everything should be taken care in
such a way that you just have to start
the things and rest of the things should
be taken care by the maven itself so
Maven will be responsible because the
due to the plugins it will be
responsible to set up the default
architecture the default folder
structures and all you have to do is
that you have to just place these scored
in the desired folder structure here so
that's something which you need to do as
in particular developers so what exactly
is the maven order of inheritance here
so the order of inheritance is the
settings CLI parameters parent form and
the project Palm which means that if you
have some configuration and settings
that will be the highest value then the
CLI parameters are there then the parent
Palm is there and then the project Palm
so this is the way that how the
particular parameters or the
configurations will be picked up by the
May one so that's the order so what does
the build life cycles and the phases
imply in the basic concept of Maven so
build life cycles consist of a sequence
of build phases and each build phase
consists of a sequence of goals when a
phase is run all the goals related to
that phase and its plugins are also
compiled so you will be able to have a
lot of particular goals which is
residing inside of phase there and
similarly life cycle is nothing but a
kind of a sequence of the different
phase so life cycle comes in the top
then it comes on the phases and then it
comes on the goals here now what is the
terminology called goal in case of Maven
the term a terminology goal refers to
the specific task that makes it possible
for the the project to be built and
organized so it's something which we can
run so it's a actual implementation
which is going on there for example in
the build process in the live build
phase I have a different goals like
clean install package deploy these are
the different typical goals which is
available there which I can execute into
the maven here so these are the
different goals like which we can
execute and which we can run during a
maven build next question is what is
exactly meant by the term dependencies
and the repositories in Maven here so
dependencies refer to the Java libraries
which we usually put up into the pound
or XML file there now what happens that
sometimes our source code is requiring
some jar files like a secondary jar
files for performing the build process
so instead of downloading it and storing
it into the class path for during the
build process we just have to specify
the dependency of that artifact what
dependency we need to put and once that
dependency is put up there we will be
able to have that jar file downloaded
and cached into the local repository
during the maven build project now if
the dependencies are not present into
your local repository then Maven will
try to download it from the central
repository and again if it is not a uh
you know it's something which is
available which is downloaded from the
central repository then it will be
cached locally into the local repository
so that's a cycle which is being
implemented and utilized during this
process
now what exactly isn't snapshot in Maven
so snapshot refers to the version
already available in the moment report
repository it signifies the latest
development copy that's what we do with
the case of snapshot here so Maven
checks for a new version of snapshot in
the remote repository for every new
build so during the build process like
uh you know a new snapshot version is
being downloaded and the snapshot is
updated by the data service team which
with updated source code every time to
the repository for each Maven page so
snapshot is something which we will be
using like very frequently we will be
updating to that and frequently we will
be updating the version to that and we
will try to explore and we'll try to do
the modifications now what are the
different type of projects available in
Maven so there are thousands of java
projects which you know can be utilized
or we can be implemented by my one here
so this helps the as the user that they
as they no longer have to remember every
configurations to set up a particular
project for example spring boot spring
MVC spring boot Etc these are the
different projects which is already
available in Maven so most of the we
have already discussed that for the jar
based projects maybe something which is
you know considered as by default so a
lot of organizations are actually using
it for you know storing or utilizing it
for the particular Maven project now
what exactly is the maven uh archetype
over here so Maven octave refers to a
maven plugin that is uh entitled to
create a for a project structure as per
its template these archives are just
project templates that are generated by
Maven when any new project is created
there so this is something which we are
using so that we will be able to create
a fresh new projects right so let's go
on to the advanced level of this Maven
questions now what exactly is the
command create a new project based on an
archive type so mvn archetype generate
is used to create a new Java Project
based on the archetype now this will
take a of some parameters from as an end
user from you and depending on that
parameters it will create the palm.xml
file it will create the source
directories inside that main Java test
all these different couple of
directories relative structures will be
automatically created now why we require
this command so that if you are going to
create a project from scratch from the
from the day one this command will help
you to have all the folder structures
created and then further on you can put
up your source code and those files as
such in this folder structure so that's
how is the mechanism that we will be
able to see that how the setup can be
performed really over here now what does
mavenclean implies now we will clean is
a plugin that suggests that it's going
to clean the files and directories there
so whenever we do a build process
usually in the Target directory we have
some class files some jar files or
whatever the generated source code which
is available that will be present in the
docket directory so the maven clean is
something which is available which is
going to clean all these directories and
why we are doing this directly structure
cleanup so that we will be able to do a
fresh uh build process and there should
not be any kind of issues as such over
here so that's the main reason why we
are looking forward for this particular
mechanism or for this particular changes
as such here now so what exactly isn't
build profile all about so build
profiles refers to the set of
configurations uh where we can have like
typically two different kind of build
processes there so if you feel that the
same pump detection file you can use you
want to run for different different
particular configurations so that you
will be able to do pretty much with the
help of this component so build profile
is used to do a customization processes
so that you will be able to have the
process and you will be able to perform
the configurations and the setups all
together there so that's a very
important aspect to be considered that
which we need to uh discuss when we talk
about the build profile so build profile
whenever you feel that you want to do
some customizations and you want to
proceed with the setup so that's where
it's utilized next thing is that what
are different type of build profiles
which is available there so the build
profiles can be done on for a particular
project like per project you can do you
can uh even do the build profiles in the
settings.xml file also and if you feel
that you want to do it into the global
settings.xml file so that also you can
do as such over here so there are
different ways in which you can do the
customization and once the customization
is done you will be able to have the
different ways of doing the setups and
the configurations over there so what
exactly is meant by the uh particular
system dependencies here so let's talk
about that also so system dependencies
refers to the particular mechanisms
where we feel that uh how the
dependencies should be uh you know
present there so that is something which
is having a scope of system there so
these depends senses are commonly used
to help Maven know the dependencies that
is being provided by the jdk system
dependencies are mostly used to resolve
the dependencies on the artifacts that
are provided by the jdk so these
dependencies are somewhat which is being
utilized and used over here so that we
will be able to implement and go ahead
through the system dependencies what is
the reason for using an optional
dependency here so optional dependencies
are used to decrease the transitive
burden of some libraries so what happens
that when you download an artifact when
you put up a dependency so it could be a
possibility that some dependencies as in
particular optional can also be
downloaded now these are not always
required but yes sometimes what happens
that these are downloaded so that you
don't have to put each and every uh
artifact or dependency into the pound or
XML file for example you're trying to
download some Apache tool and with that
some like three four jar files or three
four another dependencies are also
getting downloaded now if you are using
that dependencies that totally create
because you don't have to put that a
list out that entry in the dependency
list in the pound or XML file and that
can really save your time but if you
feel that you don't want to have them
and you these are the optional ones and
you really want to get rid of that so
that also you can exclude while
downloading any kind of dependency so
these are the optional ones which
depending on your requirement you can
utilize you can process and if you feel
that you don't want to get it you won't
want to process it you can simply ignore
it and you can get rid of that also now
what is a dependency scope and how many
type of dependencies scope are there so
there are different type of dependencies
course which is there which is used on
each and every stage of the build here
so compile provided runtime test system
import these are the different kind of
dependency Scopes which we have using
which we can Define that when exactly we
want to go ahead for a specific build
process so depending on your requirement
comment you can explore all these build
Scopes and you can get benefits out of
that what is exactly as intransitive
dependency in May 1. so Maven avoids the
need to find out and specify libraries
that our own dependencies required by
including the transitive dependencies
automatically so transitive dependencies
says that if he depends if x depends on
Y and Y depends on Z then X depends on Y
and both there
so which means that you are not
depending on one artifact you also need
the Z artifact there with the Y artifact
so that is what you need to do so that
you will get both the dependencies there
because this is normal that if you are
trying to download some particular
artifacts or download some dependency
and that dependency is also dependent on
some other artifact or some other Char
file then you have to include both of
them so this is something which you will
be able to get so that you can easily
download all these dependent jar files
also and the maven build can be success
how can a maven build profile can be
activated so Maven build profile can be
activated through different ways so
explicitly using the command uh command
line you can talk about that which
profile you want to execute through
Maven settings you can do uh based on
environment parameters OS settings and
present and missing files so these are
the different ways in which you can
actually activate that which but profile
you want to have so profiles
configurations can also be saved in
various situations and various files and
from there you will be able to refer
that which file you want to refer as
such now what is meant by the dependency
exclusion the exclusion is used to
exclude any transitive dependency
because you never know that if you are
trying to put up a dependency of entry
in the power X1 file that artifact is
also further dependent on another
artifact so in order to feel in order to
see that you want to exclude that
dependent artifact which is being
automatically downloaded that also we
can exclude with the help of exclusion
so you can avoid the transitive
dependency with the help of dependency
exclusions here so what exactly is in
Mojo So Mojo is nothing but Maven plane
old Java object here so it's an
executable goal in Maven and a plugin
refer to the distribution of such Moses
so Moses enable the maven to extend its
functionality that already is not
founded in so it's kind of an extension
which is there and using this we can get
some additional benefits and some
executions over there so what is the
command to create a new project based on
a hard drive so again archetype is
something which we normally use to
create the new projects now you can give
the parameters in the command itself or
you want it to in in kind of an
interactive mode where it will take the
parameters from the end user and
according to that the project will be
created onto hard drive or onto server
wherever you wish you want to create you
can create a new project
so explain about the maven settings.xml
file so Maven settings.xml file contains
the elements that are used to define
that how the maven execution should be
there so there are different uh settings
like local remote Central all these
different repositories are configured as
such over here now in this case what
happens that uh usually the
configurations are done in such a way
that it can you know go for the
executions it can go for the build
process and the complete executions can
be involved and can be achieved as a
chair so all these executions are
something which we can really perform
and uh here we can put some credentials
how to connect to the remote repository
how to connect to remote repository all
that stuff is something which we talk
about over here what exactly is the
meant by term super form here so super
form refers to the default form of Maven
so the moms of Maven can derive from so
it's nothing but a reference to a parent
form which is available there that is
the Super Pawn so if you define some
dependencies in that super form
automatically the uh child form will
also be able to inherit all those
dependencies so we can put some
executions like we can put some
configuration in the super pump so that
if multiple projects are going to refer
that they should be able to refer that
easily so that's the reason why we
primarily use the super pump so that we
can have the execution some processes
put up over there and all the other
projects should be effort to refer or
inherit from there so where exactly the
dependencies are stored so dependencies
are stored like in different locations
like you have the local repository
remote repository is there local
repositories on the local developers
machine and remote repository something
which is available on a server in form
of artifactory now let's talk about the
Gradle installation because this is a
very important aspect to be done because
when we are doing the installation we
have to download the Cradle executables
right so let's see that what are the
different steps is involved in the
process of the Gradle installation
so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
uh or you can go for the Oracle Java so
you can do the installation of the jdk
on your system so jdk8 is something we
can uh most commonly use nowadays so you
can install that once the Java is
downloaded and installed then you have
to do the Cradle uh download cradle
there now once the Gradle boundaries are
executable uh or the user file gets
downloaded so you can add the
environment variables and then you can
validate if the Gradle installation is
working fine as expected not so we will
be doing the Gradle installation into
our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the Gradle installation so this is the
website of the jdk of a Java Oracle Java
now here you have different jdk so from
there you can do whatever the option you
want to select you can go with that so
jdk8 is something which is most commonly
used nowadays like it's most comfortable
or compatible version which is available
so in case you want to see that if the
jdk is installed into your system all
you have to do is that you have to just
say like Java hyphen version and that
will give you the output at whether the
Java is installed into your system or
not so in case my system the Java is
installed but if you really want to do
the installation you have to download
the jdk installer from this website from
this Oracle website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
Gradle is something the which will be
performing at the build automations and
all that stuff so you have to download
the binders like a zip file probably in
which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the System modified over there so
right now we have got like the
prerequests as in Java version installed
now the next thing is that we have to
install or download the executables so
uh in order to download the latest
Gradle distribution so you have to click
on this one right now over here there
are different options like you want to
go for 6.7 now it's they're having like
binary only or complete we'll go for the
binary only is because we don't want to
have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the complete output to us over here
in this case so it may take some time
and once the uh particular modifications
and the download is done then we have to
extract it and once the extraction is
done so we will be able to go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like we can set up this path into our
environment variables and once the path
is configured and established we will be
able to start further on that part on
the execution so meanwhile these the
files are getting started let's see so
we already the folder structure over
here and we will see like we will give
this path here there is two environment
variables we have to configure one is
the Gradle underscore home and one is
the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there you have to go for the advanced
systems settings then environment
variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the Cradle executable should be there
when I am running the Cradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see check the version of the Cradle
which is installed and now you can see
that it saves that 6.7 version is being
installed over here in this case so
that's the way that how we are going to
have the Cradle installation performed
into our particular system and in this
one uh we will be also working on some
demos and some Hands-On to understand
that how we can make use of Gradle for
performing the build activity so let's
begin with the the first understanding
that what exactly is incredible all
about
now griddle is in kind of a bell tool
which can be used for the build
automation performance and it can be
used for various programming languages
primarily it's being used for the Java
based applications it's some kind of
build tool which can help you to see
that how exactly automatically you can
prepare the builds you can perform the
automations earlier we used to do the
build activity from the eclipse and we
used to do it manually right but with
the help of this build tool we are going
to do it like automatically without any
manual efforts as such here there are
like a lot of activities which we will
be doing during the build process
primarily there are different activities
like compilations linkage packaging
these are the different tasks which we
perform during the build process so that
we can understand that how the build can
be done and we can perform the
automations uh this uh process also it's
kind of a standardized because again if
you want to automate something standards
or a standard process is something which
we require for that before we been going
ahead with that part so that's the
reason why we are getting this well tool
because this build tool helps us to do a
standardization process to see that how
the standards can be met and how we can
proceed further with that part
also it's something which can be used
for variety of languages programming
languages Java is the primary language
for which we use the Gradle but again
other languages like Scala Android CC
plus groovy these are some of the
languages for which we can use the same
tool now it's actually using like it's
referring to as an trophy based domain
specific language rather than XML
because ant and Maven these are the XML
based build tools but this one is not
that dependent on XML it's using The
Groovy based domain specific language
DSL language is being used here right
now again it's something which can be
used to do the build it can further on
be used to perform the test cases Auto
motions also there and then further on
you can deploy to the artifactory also
that okay I want to push the artifacts
to the artifactory so that also that
part also you can get it done over here
so uh primary this tool is known for
doing the build automations for the big
and large projects the projects in which
the source code the amount of source
code and the efforts is more so in that
case this particular tool makes sense
now griddle includes both the pros of
Maven and ant but it removes the
drawbacks or whatever the issues which
we face during these two build tools so
it's helping us to remove all the cons
which we face during the implementation
of ant and Maven and again again all the
pros of ADD and Maven is implemented
with this cradle tool
now let's see that why exactly this
griddle is used because that's a very
valid question that what is the activity
like what is the reason why we use the
Gradle because
um the first one is that it resolves
issues faced on other build tools that's
a primary reason because we all already
having the tools like Maven and and
which is available there but primarily
this griddle is something which is
removing all the issues which we are
facing with the implementation of other
tools so these issues are getting uh
removed as such second one is that it
focuses on maintainability performance
and flexibility so it's giving the focus
on that how exactly we can manage the
big large projects and uh we can have
flexibility that what different kind of
approaches I want to build today I want
to build in different ways tomorrow the
source code modifies gets added up so I
have the flexibility that I can change
this build script so I can perform the
automations so a lot of flexibility is
available which is being supported by
this tool and then the last one is like
it provides a lot of features a lot of
plugins now this is one of the benefit
which we get in the case of Maven also
that we get a lot of features but again
when we talk about cradle then it
provides a lot of plugins like let's say
that normally in a build process we do
the compilation of the source code but
sometimes let's say that we want to
build an angular or a node.js
application now in that case we may be
involved in running some command line
executions some command line commands
just to make sure that yes we are
running the commands and we are getting
the output so there are a lot of
features which we can use like uh there
are a lot of plugins which is available
there and we will be using those plugins
in order to go ahead and in order to
execute those builds process and doing
the automations now let's talk about the
cradle and Maven because again when we
talk about Maven like it was like
something which was primary used for the
Java but again when we are talking about
cradle so again it's just uh being used
primary for the Java here but what is
the reason that we prefer Gradle over
the create me one so what are the
different reason for that let's talk
about that part because this is very
important we need to understand that
what is the reason that cradle is
preferred as a better tool for the Java
as compared to Maven when we talk about
for the build automation name
now the first one is that the Gradle
using The Groovy DSL language domain
specific language whereas the maven is
considered as a project management tool
which is uh creating the palms or XML
file format files so it's being used for
the Java project but XML format is being
used here and on the other hand griddle
is something which is not using the XML
formats and whatever the build scripts
you are creating that is something which
is there in the groupie based DSL
language and on the other hand in the
Palm we have to create the xmls
dependencies whatever the attributes you
are putting up in the mav1 that's
something which is available there in
the format of XML the overall goal of
the griddle is to add functionality to a
project whereas the goal of the maven is
to you know to complete a project phase
like to work on different different
project phase like compilation test
executions uh then uh packaging so then
deploying to artifactory so these are
all different phases which is available
there into the maven but on the other
hand griddle is all about about adding
the functionality that how you want to
have some particular features added up
into the build scripts in griddle there
are like we usually specify that what
are the different tasks we want to
manage so different different tasks we
can add up into the case of Gradle and
we can override those tasks also in case
of Maven it's all about the different
phases which has been happening over
here and it's in sequence manner so
these phases happens in the sequence
order that how exactly you can build up
the sequence there but in case of
griddle you can have your own tasks
custom tasks also and you can disrupt
the sequence and you can see that how
the different steps can be executed in a
different order so Maven is something
which is a phase mechanism there but
Gradle is something which is according
to the features or the flexibilities now
griddle works on the tasks whatever the
task you want to perform you uh it works
directly on those tasks there on the
other hand uh Maven is something does
not have any kind of inbuilt cache so
every time you're running the build so
separate uh things or the the plugins
and all this information gets loaded up
which takes definitely a lot of time on
the other hand gradually something which
is uh using its own internal cache so
that it can make the bills a little bit
faster because it's not something which
is doing the things from the scratch
whatever the uh things is already being
available in the cache so it's just pick
that part and from there it will proceed
further on the build Automation and
that's the reason why cradle performance
is much faster as compared to Maven
because it uses some kind of a cache in
there and then helps to improve the
overall performance now let's talk about
the Gradle installation because this is
a very important aspect to be done
because when we are doing the
installation we have to download the
Cradle executables right so let's see
that what are the different steps is
involved in the process of the Gradle
installation
so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
uh or you can go for the Oracle Java so
you can do the installation of the jdk
on your system so jdk8 is something we
can uh most commonly use nowadays so you
can install that once the Java is
downloaded and installed then you have
to do the Cradle uh download cradle
there now once the Gradle boundaries are
executable uh or the user file gets
downloaded so you can add the
environment variables and then you can
validate if the Gradle installation is
working fine as expected not so we will
be doing the Gradle installation into
our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the Gradle installation so this is the
website of the jdk of a Java Oracle Java
now here you have different jdk so from
there you can do whatever the option you
want to select you can go with that so
jdk8 is something which is most commonly
used nowadays like it's most comfortable
or compatible version which is available
so in case you want to see that if the
jdk is installed into your system all
you have to do is that you have to just
say like Java hyphen version and that
will give you the output at whether the
Java is installed into your system or
not so in case my system the Java is
installed but if you really want to do
the installation you have to download
the jdk installer from this website from
this article website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
Gradle is something the which will be
performing at the build automations and
all that stuff so you have to download
the binders like uh the zip file
probably in which we have the
executables and all and then we have to
have have some particular environment
variables configured so that we will be
able to have the System modified over
there so right now we have got like the
prerequests as in Java version installed
now the next thing is that we have to
install or download the executables so
uh in order to download the latest
Gradle distribution so you have to click
on this one right now over here there
are different options like you want to
go for 6.7 now it's they're having like
binary only is or complete we'll go for
the binary only is because we don't want
to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the complete output to us over here
in this case so it may take some time
and once the uh particular modifications
and the download is done then we have to
extract it and once the extraction is
done so we will be able to go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables this one so once
the installation and the extraction is
done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like we can set up this path into our
environment variables and once the path
is configured and established we will be
able to start further on that part on
the execution so meanwhile these files
are getting extracted let's see so we
already got the folder structure over
here and we will see that we will give
this path here there is two environment
variables we have to configure one is
the Gradle underscore home and one is
the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there we have to go for the advanced
systems settings then environment
variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the Cradle executable should be there
when I'm running the Gradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see check the version of the Cradle
which is installed and now you can see
that it saves that 6.7 version is being
installed over here in this case so
that's the way that how we are going to
have the Cradle installation performed
into our particular system so let's go
back to the content let's talk about the
Cradle Core Concepts here now in this
one we are going to talk about what are
the different Core Concepts of cradle
are all about the very first one is the
projects here now a project represents a
item to be performed over here to be
done like deploying an application to a
staging environment performing some
build so Gradle is something which is
required uh the projects
um the Gradle project which you prepare
is not having multiple tasks which is
available there which is configured and
all these tasks all these different
tasks needs to be executed into a
sequence now sequences again is a very
important part because again if the
sequence is not meant properly then the
execution will not be done in a proper
order so that's the very important
aspect here
tasks is the one in which is a kind of
an identity in which we will be
performing a series of steps these tasks
may be like compilation of a source code
preparing a jar file preparing a web
application archive file or ER file also
we can have like in some tasks we can
even publish our artifacts to the
artifactory so that we can store those
artifacts into a shared location so
there are different ways in which we can
have this uh particular tasks executed
now build scripts is the one in which we
will be storing all this information
what are the dependencies what are the
different tasks we want to refer it's
all going to be present in the
build.gradle file there build.gradle
file will be having the information
related to what are the different
dependencies you want to download and
you want to store there so all these
things will be a part of the build
Scripts
now let's talk about the features of
cradle what are the different features
which we can use in case of cradle here
there are different like type of
features which is available there so
let's talk about them one by one so the
very first one over here is the high
performance then high performance is
something which we can see that we
already discussed that in case you are
using a large projects so Gradle is
something which is in better approach as
compared to Maven because of the high
performance which we are getting it uses
an internal cache which makes sure that
you are using like you are doing the
builds faster and that can give you a
higher performance over there second one
is the support it provides the support
so it yes definitely provides a lot of
support on how you can perform the belts
and it's being the latest tool which is
available there so the support is also
quite good in terms of how you want to
prepare the build how you want to
download the plugins different plugin
supports and the dependencies uh
information also there next one is
multi-project build software so using
this one you can have multiple projects
in case in your repository you have
multiple projects say so all of them can
be easily built up with the help of this
part particular tool so it supports
multiple project to be built up using
the same Gradle project and griddle
scripts so that support is also
available with this cradle a build Tool
uh incremental builds are also something
which you can do with the help of cradle
so if you have done only the incremental
changes and you want to perform only the
incremental build so that can also be
possible with the help of a griddle here
the build scans so we can also perform
the build scans so we can use some
Integrations with sonar Cube and all
where we can have the scans down to the
source code on understand on how the
build happens or how the source code
really happens on there so that code
scan or the build scans can also be
performed with this one and then it's a
familiarity with Java so for Java it's
something which is considered as in by
default not even Java in fact Android
which is also using the Java programming
language is using the particular Gradle
over here so that the build can be done
and it can gain benefits out of that so
in in all the manners in all the
different ways it's basically helping us
to see that how uh we can make sure that
this tool can help us in providing a lot
of features and that can help us to make
a reliable build tool for our Java based
projects or any other programming based
project here right now let's see that
how we can invert a Java project with a
Gradle here and for that we have to go
back and readily something which is
already installed we just have to create
a directory where we can have like how
we can perform some executions we can
prepare some build scripts and we can
have a particular execution of a Gradle
build happened over there so let's go
back to the machine okay so we are going
to open the terminal here and we'll see
that how we can create it so first of
all I have to create a directory
structure let's say that we'll say like
griddle
hyphen project now once the project is
created so we can go inside this
directory so to create some critical
related projects and preparing the files
now uh in this one let's first create a
particular one so we will be saying like
VI
build
dot Gradle so in this one we are going
to put like uh two plugins we are going
to use so we are going to select apply
frogging
Java and then we are going to say like
apply
plugin
application
so these two plugins we are going to use
and when we got this file over here in
this one so it shows like build.grill
which is available there in this case so
two these files are available now if you
want to learn like you know what are the
different tasks you can run like griddle
tasks command over there so griddle task
will help you know that what are the
different tasks which is available over
here by processing the build scripts and
all so um this will definitely help you
to understand on giving you the output
so here all the different tasks are
being given and it will help you to
understand that what are the different
tasks you can configure and you can work
over here just like jar files clean and
all that stuff build compile then uh in
it is there then all these different
executions assemble then Java doc then
build then check test all these
different tasks are there and if you
really want to run the Gradle build so
you can run like Gradle clean to perform
the clean activity because right now you
are doing like if a build so before that
you can have a clean and then you can
run a specific command or you can run
the Gradle clean build which will
perform the cleanup also and it will at
the same time will have the build
process also performed over there so
build and clean up both will be executed
over here and what is the status whether
it's a success was a failure that will
be given back to you now in this case in
the previous one if you see that when
you run the clean the Cradle clean it
was only running one task but when you
go for the build uh process when you run
the Gradle clean build it's going to
give you a much more information in fact
you can also give me uh further
information like you can have the hyphen
iPhone Info flag also there so that if
you want to get the details about the uh
different uh tasks which we which is
being executed over here so that also
you're going to get over here in this
one so you just have to put like hyphen
iPhone Info and then all these steps
will be given back to you that how these
tasks will be executed and the response
will be there so that's the way that how
you can provide a pretty much simple
straightforward project in form of
Gradle which can definitely help you to
run some couple of creative commands and
then you can understand that what are
the basic commands you can run and how
the configurations really works on there
right let's go back to the main content
right now let's move on to the next one
so in the next one we are going to see
that how we can prepare a griddle build
project in case of eclipse now we are
now using the local system we are not
directly creating the folders and the
files here we are actually using the
eclipse for performing the creating a
new Gradle project over here so let's
move on that part okay so now the
eclipse is open and I have opened in
this one the very first thing is that we
have to do the Gradle plugin
installation so that we can create new
projects on cradle and then we have to
configure the path that's how the Gradle
plugin can be configured on the previous
uh preferences and all that stuff and
then we will be doing the build process
so the very first thing is that we have
to go to the eclipse Marketplace
in there we have to search for griddle
so once the search is done
it will show us the plugins we have to
go for build ship Gradle integration so
we'll click on the install
it will proceed with installation it
will download it in some cases maybe
it's part of the eclipse as in uh in the
ID so you can go to the install Tab and
you can see that also that if this
plugin is already installed or not but
in this case we are installing it and uh
once the installation is done we just
have to restart the uh specific uh once
we have to restart this Eclipse so that
the changes can be reflected
so it's downloading
it's downloading the Cradle here and
once that is installed we will be able
to use it over here in this case in this
scenario so we have to just wait for
that part so still downloading the jar
files
so once the jar file is done it's now
over the areas and download it so after
that we will be able to proceed further
on that download pass bet so it's going
to take some time to download it and
once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now so uh before that uh we just have to
click on restart now and then the
eclipse will be restarted all together
again here so you can do it manually or
you can go for that options you just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the you know the
configuration where we can have the
system so we can go for the Gradle
configuration so we can go for Windows
and then preferences
now in this case we have to go for the
uh for the ones in which the Cradle
option is available there so cradle is
what we are going to select now user
home the Gradle user home is what we
need to use right so you want to go for
the Cradle you want to go for local
installation so so all these options you
can use you can if if you go for the
griddle wrapper then it will be
downloading the Gradle locally and it is
going to use the Cradle W or
griddlew.bat file but if you already
have an installation locally so you can
prefer that also right now in the
previous demo we have already got the
Gradle uh extracted so we just have to
go for the downloads in the downloads
already Gradle is available so we are
going to select that part here so this
is what we are going to select
right so this represents that this is
the directory structure in which we are
having the mechanism so you can either
go for the pill scan so you can select
the build scan also so once this is
enabled then all the projects will be
scanned and will be you know published
and uh it's in kind of an additional
option which is available if you really
want to disable it you can disable it
also and you can go with this
configuration also so uh this is where
the particular Gradle folder is being
put over here in this case and then we
have to just click on apply
and we just have to click on apply and
close so with this one the particular
execution is done now we will be going
for the project creation so you can
right click over here or you can go to
the file also so here we are going to go
for the job project and in this we are
going to have a Gradle project so Gradle
project is what we are going to create
here
and next
so we are going to say like cradle
project
and then next
so once that is done so finish
so uh with this one when you create the
project so what will happen that uh
automatically there will be a folder
structure will be available there right
and uh there are some uh Gradle scripts
which will also be created there so we
will be doing the modifications there
and we'll see that how the uh particular
Gradle build script looks like and how
we can we will be adding some couple of
uh selenium related dependencies and
we'll see that how we can have more and
more dependencies added and what will be
the impact of those dependencies on the
overall project so that also it's very
important aspect to be considered so let
this processing be happen over there
just creating and uh some plugins and
binaries are getting installed and
getting downloaded so we'll see that
once the project is imported completely
executed over here and got created we
can extract that now if you see here the
particular option is available about the
Gradle tasks so you can extract it also
and you will be able to know that what
are the different tasks which is
available there let's see that in the
build they are running like build these
are the different tasks which is
happening inside the build process so
gradual executions will be also
available over here in this case and
Gradle tasks will be different it will
be represented over here in this one so
you just have to extract on the Gradle
project okay so this is the library
which is available now uh what happens
that you will be able to have like
settings.gradle in this one you will be
able to have like okay Gradle hyphen
project is something which is available
there in this one so that's what you're
being aren't referring then we have over
here as in these folder structures which
is created like Source main Java this is
the one source test Java is the one
which is available as an on the folder
structure and so test resources are also
available here so the main source main
resources are also available now in this
case what happens that these are the
dependencies project and external these
are the different dependencies are
available there so let's see let's add a
dependency over here in this one in the
Bell dot uh griddle script and see that
how we can do that if we open
build.gettle file so you can see that
these dependencies are there like test
implementation junit is available there
right and then we have implementations
of this one which is available now these
jar files which you put up you it will
automatically be added up as in part of
this one as in part of the particular
dependencies over here and which means
that you don't have to store them as a
now within the repository and
automatically they can be happened over
there so let's open a dependency page so
we will be going to MN repository where
we will be opening a dependency link so
this is the dependency link here so
surname iPhone Java is available and it
can give you the dependency for all the
different options now we have for Maven
this is the one and for Kettle this is
the one here so we have to just copy
this one and we have to use it as
independency so this is the group and
this is the name and the version which
we are using here now we have copied
this one so we will go back to the
Eclipse so here we have to just put that
dependency
and we have to just save it so uh this
is something which is providing like
selenium dependencies which is available
so now we have to just refresh the
project so right click over here then
you will be able to see the options in
the grader saying that refresh credible
project now once the moment you do that
so you will be able to do like for the
first time maybe it will take some time
to download all the dependencies which
is related to selenium but after that
you will be able to select the
dependencies will be simply added up
over here in this case so you can see
that all the selenium related
dependencies are added up for any reason
if you comment these ones
and you say like
synchronize again
so you will see that all the
dependencies which you are adding up
from this selenium represent uh from the
selenium perspective will be gone back
again so this is the way that how you
can keep on adding the dependencies
which is required for preparing your
bill for your source code and from there
you will be able to proceed further on
the execution part so that's the best
part about this cradle here so that's
the way that how we are going to prepare
a Gradle project within the eclipse and
now you can keep on adding like the
source code in this one and that's the
way that how the code base will be added
up over here right so that's the way
that how the uh particular executions or
this Gradle project is being prepared in
case of eclipse selenium installation is
a three-step process so it has certain
prereqs the first prerequis you need to
have Java on your system so we will be
installing Java first and then we will
be working with Eclipse ID so we will be
installing eclipse and then we will
install selenium for Java we will
install the version Java 8 and for
Eclipse we have a version 4.10 this was
the last stable version which was
released in December last year so I'll
be using that version and selenium we
will download the latest 3.14 version
okay so let's get started with our first
step which is the Java installation so
to install Java let's go to the browser
and simply just search for Java 8
download
so now you will see that there is an
oracle site which is listed there and
that is where you would be downloading
all your Java package so go ahead and
click on that and for you to download
any jdk package from the Oracle site you
need to create an account so if you
already have one you just need to log in
using that account and then you can
download any of the jdks and if you do
not have one please go ahead create a
new account on the Oracle log into that
account and then you can just download
the Java 8. so since I already have an
account and I have already downloaded
the package but I'll show you how and
where to download it from so in this
page if you scroll down so you will see
this Java development kit 8211 so this
is the version we'll be downloading it
so click on the accept license agreement
and then since we are working on the
Windows system today so we will be
downloading this the windows package so
just click on that and it will get
downloaded in your downloaded folder and
as I said I've already downloaded the
packages so here it is what I've done is
I've just created a directory called
installers and I'm going to be keeping
all my installables here so here I have
a folder called Java installer and this
is where my installable is so now that
we have this file so we will just go
ahead double click on it and launch this
installer the installer is launched and
just click on run so this will take a
few minutes to install Java the
installer is launched now just click on
the next button here so here for the
installation directory you can change
the directory to the choice of whatever
drive and the folder structure you want
to I would like to leave it as default
here and we'll just go and click on next
and then the Java installation is in
progress so let's wait until this is
completed it really shouldn't take too
much time maybe just a few more minutes
here
okay accept the license term just click
on next we leave the destination folder
as it is
so jdk8 is successfully installed on
your system so close the installer now
and let's go ahead and check whether the
installation is done properly so for
that what I'll do is I'll go to my
command prompt and I'll just say Java
minus version so it says Java version
1.8 and this tells us that the Java is
installed successfully now after this
installation there are couple of
configurations which we need to do and
what are those configuration one is you
need to set the path variable and then
we are also going to set a Java home
directory so for that first let's go
ahead and check where is the Java
installed actually let's figure out the
directory first so if you remember the
directory structure where the Java got
installed was in program files Java I
have there are certain previous versions
which are installed and then uninstalled
it so that is why you see some residuals
here sitting here that's not worry too
much about that instead let me go to the
latest one what I have installed which
is this okay and there is a bin folder
here and this is the path which we need
to set in our path variable so what I
will do is I'll just copy this path and
then go to your control panel here go to
your where is my system yeah so click on
the system go to Advanced system setting
and here in the environment variables
find the PATH variable okay and then say
edit now what are we doing here in the
path variable is we are going to add the
Java bin directory to the path be very
careful whenever you are editing your
path variable do not overwrite anything
always go into the edit mode go towards
the end here and then just say Ctrl V
paste the path which you have just
copied from the Explorer window that's
it now just say okay done so your path
setting is done so what's the next one
we need to do we need to add a new
environment variable called the Java now
what I'll do for that is I just say new
I'll just type Java home here
and what is the value of this we need to
set we need to set the same path but
without the print directory so we just
need to set the path till your Java
directory that is this so we'll just
copy the path again and paste it here
that is off just say Okay click on Okay
click on OK here and we are done so
again let's go to our Command Prompt and
just say Java minus version
so everything seems to be fine so now
successfully we have installed Java on
the system so what is our next
installation step what we have now we
need to install the eclipse so let's go
back to the browser again so to download
Eclipse we will be downloading the
package from the eclipse.org so when you
go here to eclipse.r you can see the
latest version which is available and
the latest version available when this
video was made was
20906. so especially with Eclipse since
it's an open source I prefer to work
with the last stable version and so does
most of the developers do and hence that
is the reason why I have picked up the
version which is like last year's
version which is uh 4.10 which was
released in last December so you can
always choose to go with the latest
version but then if there are any issues
and if you are like first time working
with the eclipse you're going to get
confused as where these issues are
coming from right so I would still
recommend that you use the last stable
version which is available with your
Eclipse so now to get the last stable
version what you need to do is go and
click on this download packages and here
if if you scroll down this page you can
see here more downloads so there is a
list of all the previous releases of app
Clips which is available and this is
what we need to download so just click
on that 4.10 version and then click on
the OS on which you want to install
eclipse for us it is Windows so I'll
just click here on the 64-bit windows
and then click on the download and you
will be downloading the complete package
so once you download this is what it
will look like so let's go back to our
directory of installers so this is the
installer for the Eclipse which I got
now what's the next step I need to do
just launch this installer and install
Eclipse so I'll just say double click on
this I'll say run
so here you will see multiple options
here for Eclipse installation so
depending on your requirement you can go
ahead and install any of these packages
so for us we just need an eclipse ID for
Java developer so I'll select this and
I'll say install so again you'll have a
choice of directory where you want to
install so I have chosen D drive here
this is a default directory name it
takes which is okay we can leave it as
it is and then also you have an option
to create a start menu entry and desktop
shortcut so just leave the default
selection as it is and go ahead and
click on install so this will take a
while to install the eclipse
this is select all you can close this
window just say select all and accept it
okay so the installation has been
completed successfully so let's go and
click on this launch and let's see the
first window what opens
when you launch the eclipse you need to
specify a workspace directory now what
is this workspace directory so this is a
directory or a folder wherein all the
Java files or any programs or any
artifacts which are going to create
through Eclipse will be stored in this
particular folder so this could be any
location on your system so this is you
can go ahead browse the location and
change it so for in our case what we
will do is I'll go to the D drive and I
already have a directory so here I'll
create I'll just create select this
folder and then create a folder called
workspace I'll say my workspace and then
I'll say launch so every time I open the
eclipse right so if this is going to
take as my default workspace and all my
programs all my javascripts or my
automation scripts are getting are going
to be stored in this particular location
so we'll say launch
so this is a welcome window which opens
we can just close this and there we go
the eclipse is open with a certain
perspective so there are certain Windows
here which we do not need let's just
close them so now the first thing what
you do after launching the eclipse is go
ahead and create a new project so I'll
say file new and since I'm going to be
using Java with selenium I'll say create
Java project so give a project name
let's say my first project now you have
an option here to select the JRE which
you want to use so we just install this
jdk 1.8 okay so I'm going to click on
use default GRE otherwise you also have
an option to use a Project Specific gr
for example I could have two different
projects where one project I'm going to
be working with GRE 1.8 and there is
another project which I want to work
with the latest Java maybe Java 12 and I
can have more than one Java installed on
the machine so this gives me an option
to select whichever Java I want to work
with so if you have another Java
installed here it will show up in this
list so and you can just go ahead and
select that now since we have only one
Java installed on our machine which is
Java 1.8 I will say use default GRE
which is 1.8 and I will click on finish
now if You observe this folder structure
the project which is created see all the
reference libraries to this particular
Java have been created here now we are
ready to create any kind of java
programs in this project so now we have
successfully done the second step of our
installation which is the eclipse
installation after this we need to
install the selenium so again let's go
back to the browser and see what files
we need to download to install selling
it so let me go to my browser and here I
will be going to the
seleniumhq.org so if you are working
with selenium this particular website
the seleniumhq.org is going to be a
Bible everything and anything related to
selenium is available in this website
whether you want to download the files
whether you want to refer to the
documentation anything regarding to
selenium is available here so what we
want now is the installables for
selenium so here go to the download tab
now for you to install selenium and
start working with selenium there are
three things which are required for you
to download one is a standalone selenium
server so this is not required
immediately when you get started with
selenium however when you start working
with remote selenium Webdriver you would
be requiring this when you have a grid
setup you will be requiring the
Standalone server so for that what you
can do is you can just download the
latest version available here so when
you click on that it'll download the
file into your download folder so this
is one particular file which you need to
keep next selenium client and Webdriver
language bindings now in today's demo we
will be looking at selenium with Java so
that means my client package of java is
what I need to download so whatever
programming language selenium supports
we have respective downloadables
available with that so if you're working
with python then you need to download
your client library for Python and since
we are working with Java you need to
download this package so simply what you
need to do click on this link and it
will download the Java package for you
which are basically the jar face so we
have client libraries now and then there
is another component what we need now
with a linear you're going to be
automating your web browser applications
correct and you also want your
applications to run on multiple browsers
so that means your scripts the
automation scripts which you create
should be able to run on any browser
selenium works with multiple browsers
like Edge Safari Chrome Firefox and
other browsers even it has a support for
headless browser now every browser which
it supports comes with its own driver
file now say for example we want to say
work with Firefox driver so that means
for us to start working with Firefox
browser we need to download something
called as a gecko driver here and if you
want to work with Chrome browser you
need to install the Chrome driver so
depending on what browsers you'll be
testing with go ahead Click on each of
this link and download the latest driver
files now since we are going to be
working with Firefox in this demo what I
need to do is I just need to click here
on the latest link so when I click on
the latest link it is going to take me
to this driver files so driver files are
specific to each of the operating system
so if you go down here you will see
there is a separate driver file
available for Linux for mac and for
Windows so depending on which operating
system where you'll be running your test
download that particular driver file and
this is the driver file I need because
we are working on Windows machine so
these are the three different packages
which we need to download from the
selenium hq.org for us to install
sending so let me show you the folder
where I have already downloaded all this
so if you see here selenium Java
3.141.59 okay this is nothing but our
client Library which we saw here let's
go back to the main page here that is
this so once I download this this is a
ZIP file after I unzip the file this is
the folder structure I see and let's see
what is there in this folder selector so
there are two jar files here and then in
the lips there are multiple jar files
and we will need all this to work with
selenium and then we also downloaded the
driver files so what I did was after
downloading those driver files for the
browser I created a directory here
called drivers and I've kept all my
browser drivers here so I have a driver
file downloaded for Chrome I want to
work with Firefox so I have a gecko
driver here and then for Internet
Explorer that's it so this is all we
need so once we have all this what you
need to do is go to your eclipse in the
eclipse right click on the project which
you have created and then go to the
build path and say configure build path
go to the libraries tab here now do you
see this JRE library is here this is
what got installed first and now
similarly we are going to add the
selenium Jazz to the slide British and
how do we add that on your right you can
see this add external jars click on ADD
external jars go to your folder where
you have downloaded your selenium which
is this select all the jar files which
is available so I have two jar files
here I'll just say click open again I
will click on ADD external jars now from
the lips folder I will select all this
five so select all the five jars and
click on open so you should see all the
seven jar files here so once you have
this just say apply and close now if you
look into your project directory here
you'll see some a folder called
referenced library and this is where you
will see all the selenium charts here
this is a very simple installation in
Eclipse when you want to install
selenium you just need to export all the
jars of the selenium into eclipse and
now your system is ready to start
working with selenium scripts all right
so now let's just test our installation
by writing a small selenium test script
so for that what I will do is I'll go to
the source folder right click new and
I'll say Java class so let's name this
as say first selenium test and I will
select this public static white Main and
I will click on finish all right so now
let's create a use case say we want to
launch a Firefox browser and then we
want to launch the Amazon site so these
will be just two simple things which we
will be doing in this test script so for
me to do that what I usually do is I
create a method for any functionality
which I want to create here so now I
want to do a launch browser so I'll
create a method here called launch
browser now whenever you start writing
your selenium scripts the first line
what you need to do is you need to
declare an object of Webdriver class so
here I'll say
Webdriver driver driver so now if you
hover over this error what it is showing
it says import web driver from or dot
open QA dot selling it so if you
remember when we installed the selenium
we imported all these jars right so that
means so what whenever we want to use a
web driver we need to import this class
from these packages so just go ahead and
click on this import State done now next
step now for us to launch a Firefox
browser it is a two steps process which
is involved here one is you need to set
the system property and then you need to
launch the driver so let's do that I'll
say system dot set property so use this
method set property so this takes two
arguments the key and the value pair now
what is the key I'm going to mention
here I am going to be mentioning the
gecko driver and the path for the gecko
driver okay because since I'm working
with the Firefox so in double quotes
I'll say
webdriver.geco.driver this is my key and
the value is going to be sorry the fully
qualified path for your driver files and
you know know where we have kept our
driver files let's go to that driver
files in D colon I have selenium
tutorial in installers I have driver
folder okay so I'm just going to copy
the complete path from here Ctrl C and I
paste it here Ctrl V along with this I
need to provide the file name for the
gecko driver which is gecko driver dot
exe and let's complete this step next so
once I've set the property I need to
provide a command for launching my
Firefox driver and how do I do that I
simply use this driver object which I've
created driver equal to
new Firefox driver again similarly the
way we imported packages for Webdriver
we also need to import the package for
Firefox driver so just hover over the
mouse over that and select import
Firefox track with these two statements
we will be able to launch the Firefox
browser and as I said in our use case
what is the next thing we want to do we
want to launch say amazon.in website for
that there is a command in selenium
which says driver dot get and you pass
the URL here so for me to write the URL
what I usually do is I go to my browser
I open the website which I want to work
with in our case it's amazon.in and I
just simply copy this fully formed URL
go to my eclipse and just paste it here
now this ensures that I don't make any
mistakes in typing out the URL let's
complete this statement and we are done
and now in the main function I'll just
create an object of this and we will
call this method so I'll copy this class
for selenium test say obj equal to new
for selenium test and now I'll say say
obj dot this is our function launch
process so let's save this and execute
this Ctrl C right click run as Java
application okay so the Mozilla Firefox
has been launched now it should launch
your amazon.bingo so there goes our
first test script which runs
successfully before you start
understanding any automation tool it's
good to look back into what manual
testing is all about what are its
challenges and how automation tool
overcomes these challenges challenges
are always overcome by inventing
something new so let's see how selenium
came into existence and how did it
evolve to become one of the most popular
web application automation tool selenium
Suite of tools selenium is not a single
tool it has multiple components so we
will look into each of them and as you
know every automation tool has its own
advantages and limitations so we will be
looking at what the advantages are and
the limitations of selenium and how do
we work around those limitations all
right so let's get started manual
testing a definition if you can say a
manual testing involves the physical
execution of test cases against various
applications and to do what to detect
bugs and errors in your product it is
one of the Primitive methods of testing
a software this was the only method
which we knew of earlier it is execution
of test cases without using any
automation tools it does not require the
knowledge of a testing tool obviously
because everything is done manually also
you can practically test any application
since you are doing a manual testing so
let's take an example so say we have a
use case you are testing say a Facebook
application and in Facebook application
let's let's open the Facebook
application and say create an account
this is your web page which is under
test now now as a tester what would you
do you would write multiple test cases
to test each of the functionalities on
this page you will use multiple data
sets to test each of these fields like
the first name the surname mobile number
or the new password and you will also
test multiple links what are the
different links on this page like say
forgotten account or create a new page
so these are the multiple links
available on the web pages also you look
at each and every element of the web
page like your radio buttons like your
drop down list apart from this you would
do an access disability testing you
would do a performance testing for this
pitch or say a response time after you
say click on the login button literally
you can do any type of tests manually
once you have this test cases ready what
do you do you start executing these test
cases one by one you will find bugs your
developers are going to fix them and you
will need to rerun all these test cases
one by one again until all the bugs are
fixed and your application is ready to
ship now if one has to run test cases
with hundreds of transactions or the
data sets and repeat them can you
imagine the amount of effort required in
that now that brings us to the first
demerit of the manual testing manual
testing is a very time consuming process
and it is very boring also it is very
highly error Pro why because it is done
manually and human mistakes are bound to
happen since it's a manual executions
tester's presence is required all the
time one is to keep doing manual Steps
step by step again all the time he also
has to create manual reports group them
format them so that we get good looking
reports also send this reports manually
to all stakeholders then collection of
logs from various machines where you
have run your test consoliding all of
them creating repositories and
maintaining them and again since it's
all as a manual process there is a high
chance of creating manual errors there
scope of manual testing is limited for
example let's say regression testing
ideally you would want to run all the
test cases which you have written but
since it's a manual process you would
not have the luxury of time to execute
all of them and hence you will pick and
choose your test cases to execute that
way you are limiting the scope of
testing also working with large amount
of data manually is Impractical which
could be the need of your application
what about performance testing you want
to collect metrics on various
performance measures as a part of your
performance testing you want to simulate
multiple loads on application under test
and hence manually performing these kind
of tests is not feasible and to top it
all up say if you're working in an agile
model where code is being churned out by
developers testers are building their
test and they are executing them as and
when the bills are available for testing
and this happens iteratively and hence
you will need to run this test multiple
times during your development cycle and
doing this manually definitely becomes
very tedious and burning and is this the
effective way of doing it not at all so
what do we do we automate it so this
tells us why we automate one for faster
execution two to be less error prone and
three the main reason is to help
frequent execution of our test so there
are many tools available in the market
today for automation one such tool is
selenium birth of selenium much before
selenium there were various tools in the
market like say rft and qtp just to name
a few popular ones selenium was
introduced by gentlemen called Jason
Huggins way back in 2004. he was an
engineer at thoughtworks and he was
working on a web application which
needed frequent testing he realized the
inefficiency in manually testing this
web application repeatedly so what he
did was he wrote a JavaScript program
that automatically controlled the
browser actions and he named it as
JavaScript test later he made this open
source and this was renamed as the
selenium core and this is how selenium
came into existence and since then
selenium has become one of the most
powerful tool for testing web
applications so how does selenium help
so we saw all the demerits of manual
testing so we can say by automation of
test cases one selenium helps in Speedy
execution of test cases since manual
execution is avoided the results are
more accurate No human errors since your
test cases are automated Human Resources
require to execute automated test cases
is far less than manual testing because
of that there is a lesser investment in
human resources it saves time and you
know time is money it's cost effective
as selenium is an open source it is
available free of cost early time to
Market since you save effort and time on
manual execution your clients will be
merrier as you would be able to ship
your product pretty fast lastly since
your test cases are automated you can
rerun them any point of time and as many
times as required so if this tool offers
so many benefits we definitely want to
know more detail about what selenium is
selenium enables us to test web
applications on all kind of browsers
like Internet Explorer Chrome Firefox
Safari Edge Opera and even the Headless
browser selenium is an open source and
it is platform independent the biggest
reason why people are preferring this
tool is because it is free of cost and
the qtp and the rft which we talked
about are chargeable selenium is a set
of tools and libraries to facilitate the
automation of web application as I said
it is not a single tool it has multiple
components which we'll be seeing in
detail in some time and all these tools
together help us test the web
application you can run selenium scripts
on any platform it is platform
independent why because it is primarily
developed in JavaScript it's very common
for manual testers not to have in-depth
programming knowledge so selenium has
this record and replay back tool called
the selenium ID which can be used to
create a set of actions as a script and
you can replay the script tag however
this is mainly used for demo purposes
only because selenium is such a powerful
tool that you should be able to take
full advantage of all its features
selenium provides support for different
programming languages like Java python
c-sharp Ruby so you can write your test
scripts in any language you like one
need not know in depth or Advanced
knowledge of these languages also
selenium supports different operating
systems it has supports for Windows Macs
Linux even Ubuntu as well so so you can
run your selenium test on any platform
of your choice and hence selenium is the
most popular and widely used automation
tools for automating your web
applications selenium set of tools so
let's go a little more deeper into
selenium as I said selenium is not a
single tool it is a suite of tools so
let's look at some of the major
components or the tools in selenium and
what they have to offer so selenium has
four major components one selenium ID
it's the most simplest tool in the suite
of selenium it is integrated development
environment earlier selenium IDE was
available only as a Firefox plugin and
it offered a simple record and Playback
functionality it is a very simple to use
tool but it's mainly used for
prototyping and not used for creating
Automation in the real-time projects
because it has its own limitations like
any other record and replay tool
selenium RC this is nothing but selenium
remote control it is used to write web
application test in different
programming language what it does it
basically interacts with the browser
with the help of something called as RC
server and how it interacts its uses a
simple HTTP post get request for
communication this was also called as
selenium 1.0 version but it got
deprecated in selenium 2.0 version and
was completely removed in 3.0 and it was
replaced by Webdriver and we will see in
detail as why this happened selenium
Webdriver this is the most important
component in the selenium Suite it is a
programming interface to create and
execute test cases it is obviously the
successor of the selenium RC which we
talked about because of certain
drawbacks which RC hat so what Webdriver
does is it interacts with the browsers
directly unlike RC where the RC required
a server to interact with the browser
and the last component is the selenium
grid so selenium grid is used to run
multiple test scripts on multiple
machines at the same time so it helps
you in achieving parallel execution
since the selenium Webdriver with you
can only do sequential execution grid is
what comes into picture where you can do
your parallel execution and why is
parallel execution important because in
real time environment you always have
the need to run test cases in a
distributed environment and that is what
grid helps you to achieve so all this
together helps us to create robust web
application test Automation and we will
go in detail about each of these
components so before that let's look at
the history of selenium version so what
did selenium version comprised of it had
an IDE RC and grid and as I said earlier
there were some disadvantages of using
RC so RC was on its path of deprecation
and web driver was taking its path so if
you look at selenium 2 version it had an
earlier version of Webdriver and also
the RC so they coexisted from three dot
onwards RC was completely mode and
Webdriver took its place there is also a
four dot version around the corner and
it has more features and enhancements
some some of the features which are
talked about are w3c Webdriver
standardization improved ID and improved
grid now let's look at each of the
components in the selenium Suite
selenium IDE is the most simplest tool
in the suite of selenium it is nothing
but an integrated development
environment for creating your automation
scripts it has a record and Playback
functionality and is a very simple and
easy to use tool it is available as a
Firefox plugin and a Chrome extension so
you can use either of this browser to
record your test scripts it's a very
simple user interface using which you
can create your scripts that interact
with your browser the commands created
in the scripts are called selenis
commands and they can be exported to the
supported programming language and hence
this code can be reused however this is
mainly used for prototyping and not used
for creating automation for your
real-time projects why because of its
own limitation which any other recorded
replay tool has so a bit history of
selenium ID so earlier selenium ID was
only a Firefox extension so we saw that
IDE was available since the selenium
version one selenium ID died with the
Firefox version 55 that was ID was
stopped supporting from 55 version
onwards and this was around 2017 time
frame however very recently all new
brand selenium ID has been launched by
apply tools and also they have made it a
cross browser so you can install the
extension on Chrome as well as as an
add-on on Firefox browser so they
completely revamped this IDE code and
now they have made it available on the
GitHub under the Apache 2.0 license and
for the demos today we'll be looking at
the new ID now with this new ID also
comes a good amount of features
reusability of test cases better
debugger and most importantly it
supports pattern test case execution so
they have introduced a utility called
selenium side Runner that allows you to
run your test cases on any browser so
you can create your automation using IDC
on Chrome or Firefox but through command
prompt using your site Runner you can
execute these test cases on any browser
thus by achieving your cross browser
testing control flow statement so
initially in the previous versions of
idea there were control flow statements
available however one had to install a
plugin to use them but now it is made
available out of box and what are these
control flow statements these are
nothing but your if else conditions the
while Loops the switch cases so on it
also has an improved locator
functionality that means it provides a
failover mechanism for locating elements
on your web page so let's look at how
this ID looks and how do we install it
and start working on that so for that
let me take you to my browser so say
let's go to the Firefox browser so on
this browser I already have the ID
installed so when you already have an ID
installed you will see an icon here
which says selenium ID and how do you
install this you simply need to go to
your Firefox add-ons here whether it
says find more extension so just type in
selenium ID and search for the second so
in the search results you see the
selenium ID just click on that and now
since I've already installed here it
says remove otherwise for you it is
going to give you an add button here so
just click on the add button it will
install this extension once it is
installed you should be able to see this
selenium ID icon here okay so now let's
go ahead and launch this ID so when I
click on that it is going to show me a
welcome page where it's going to give me
a few options the first option is it
says record a new test case in a new
project so straight away if you choose
this option you can start recording a
test case in which case it's going to
just create a default project for you
which you can save it later then open an
existing project so you can open if you
already have a saved project create a
new project and close so I already have
an existing project with me for the demo
purpose so I'll go ahead and open that
so I'll say open existing project and I
have created a simple script what the
script does is it logs me into the
Facebook using a dummy user mail sorry
username and password that's all it's
very simple script with few lines and
this is what it's going to do so what we
will simply do is we'll just run the
script and see how it works for that I'm
just going to reduce the test execution
speed so that you should be able to see
every step of execution here all right
so what I'll do now here is I'll just
adjust this window and I'll just simply
say run current test all right so I'll
just get this side by side so that you
should be able to see what exactly the
script is doing okay so now you are able
to see both the windows okay so now it's
going to type in your user email here
there you go and now it'll enter the
password and it has clicked on the login
button so it's going to take a while to
say login and since these are the dummy
IDs it is you are not able to log in
here and you are going to see this error
window fine that is what is the expected
output here now on the ID if you look
here after I execute the test case every
statement or every command which I have
used here is color coded in green so
that means this particular step was
executed successfully and then here in
the log window it will give you a
complete log of this test case right
from the first step till the end and
your end result is it says FB login
which is my test case name completed
successful let's look at few components
of this ID the first one is the menu bar
so let's go to our ID all right so the
menu bar is right here on the top so
here is your project name so either you
can add a new project here or rename
your project so since we already have
this project which is named as Facebook
and then on the right you have options
to create a new project open an existing
project or save the current project and
then comes on toolbar so using the
options in this toolbar you can control
the execution of your test cases so
first one here is the recording button
so this is what you use when you start
recording your script and then on the
left you have two options here to run
your test cases the first one is run all
tests so in case you have multiple test
cases written here you can execute them
one by one sequentially by using this
run all test else what you can do is if
you just want to run your current test
this is what you would use then idea has
this debugger option which you can use
to do a step execution so say for
example now whenever I run the script
it's going to execute each and every
command here sequentially so instead if
I just select the first command and say
do step execution all right so what it
does is the moment it finishes the First
Command which is opening of Facebook
right I think which is already done here
yeah all right so once this is done it
is going to wait immediately on the
second command and it says pause
debugger so from here you can do
whatever you would like to do in case
you want to change the command here you
can do that you can pause your execution
you can resume your execution here right
you can even completely stop your test
execution or you can just select this to
run the rest of the test case so if we
say run the test case what it is going
to do is it's just going to Simply go
ahead and complete the com complete the
test case now there is another option
here which is you see the timer there
which says test execution speed so to
execute your test cases in the speed you
want say whenever you're developing an
automation script right and say you want
to give a demo so you need to control
the speed sometimes so that the viewer
is able to exactly see all the steps
which is being performed and this gives
you an option to control that complete
execution right so do you see the
grading here so we have somewhere from
Fast to completely slow execution so the
previous demo which I showed was I
control the speed and then I executed it
so that we could see every command how
it has been executed all right so what's
the next this is called as an address
bar so whichever Wherever Whenever you
enter an URL here that is where you want
to conduct your test and another thing
what it does is it gives a history of
all the URLs which I've used for running
your test then here is where your script
is recorded So each and every
instruction is displayed here in the
order in which you have recorded the
script and then if you look here you
have something called as login reference
so now log is an area where it records
each and every step of your command as
in when they get executed right so if
you see here it says open https
facebook.com and ok so that means this
command was executed successfully and
after the complete test case is done it
gives you whether the test case passed
or filled so in case there is a failure
you'll immediately see this test case is
filled in red color also there is
something called as reference here for
example say if I click on any of this
command the reference tab what it is
going to show me is a details of this
command which I have used in the script
it gives you the details of the command
as well as what the arguments have been
used or how how is that you need to be
using this particular command okay so
now what we'll do is let's go ahead and
write a simple script using this ID so
with this you'll get an ideas how do we
actually record scripts in ID so for
that I have a use case here a very very
simple use case so what I will do is we
will open amazon.in then we'll search
simply search for say a product iPhone
and once we get that search page where
all your iPhones are displayed we will
just do an assert on the title of the
page simple all right so let's do that
so first thing what I need is an URL
okay so first let me go to my Firefox
browser here and say amazon.in so why
I'm doing this just to Simply get the
right URL absolute URL path here and so
that I don't make any mistakes while
typing in the URL okay so I got this so
let me close all this windows I don't
need any of this let's minimize this all
right so here what I will do in the
tests tab I'll say add a new test and
name this test as uh Amazon search done
I'll say add now I'll enter this URL
which I just copied it from my browser
okay and then I'll just say start
recording so what it does did was since
I have entered the URL in this address
box it just opened the amazon.in URL now
let's do the test case so in my test
case what I said was I want to search
for iPhone once I have this I'm just
going to click on my search button so
now this gives me a list of all iPhones
and then I said I want to add an
assertion on the title of this page so
for me to do that what id gives me an
option is I have to just right click
anywhere on this page and you'll see the
selenium ID options here so in this I
will select assert title and then I will
close this browser so that kind of
completes my test case so now take a
look at all the steps which has created
for me so it says open slash because
I've already provided the URL here so
either you can replace it with your
regular URL or you can just leave it as
it is so what I will do is since this is
going to be a proper script and I might
be using this to run it from a command
prompt also so I'll just replace this
target with the actual URL and then what
it is doing it is setting a window size
then there are whatever I did on that
particular URL on that website it has
recorded all the steps for me so this is
where it says type into this particular
text box which is my search box and what
did it type iPhone this was the value
which I entered now there was one more
feature which I told you in this new ID
which had which I said it has a failover
mechanism for your locating techniques
now that is what this is now if you look
here this ID is equal to two tab search
text block this is nothing but the
search box where we entered the text
iPhone and it has certain identification
through which this IDE identifies that
web element and that has multiple
options to select that particular search
box so right now what it has used is ID
is equal to two tab search box however
if you know the different locating
techniques you will be able to see here
that it has other techniques also which
it has identified like the name and the
CSS and the XPath so how does this help
in failovers say tomorrow if Amazon dot
in website changes the ID of this
element right you are not going to come
and rewrite the scripts again instead by
using the same script what it will do is
if this particular ID fails if it is
unable to find the element using the
first locator which is the ID it simply
moves to the next available ones and it
tries to search for that element until
one of this becomes true that is what
was the failure mechanism which has got
added now it's a very brilliant feature
because most of our test cases break
because of element location techniques
well let's come back to this so then we
added an assert title right so what is
assert Title Here it simply captures the
title of that particular page and it
checks this is all a very simple test
case so what we will do now is we will
stop the recording and then I've also
given a closed browser so right now what
I will do is I'll just comment this out
why because if I just run this test case
it's going to be very fast and you might
not be able to catch the exact command
execution what has happened all right so
right now I'll just disable it so that
it will just do all these test cases and
it just stays there without closing the
browser so now I'll just say run the
current tested so your Amazon in is
launched okay it is typed in the iPhone
it's also clicked on the search so it is
done so now if you look here since we
are in the reference tab it is not able
to show so let's go to the log and now
let's see the log so it's going to be a
running log so if you notice here the
previous examples which we have run for
Facebook is also in the same lock so we
will have to see the log from running
Amazon search because that's our test
case so if you see here every command
line right was executed successfully
assert title was also done and your test
case was executed successfully so it
passed now what we will do is on this
assert title I'll just modify this and
let's say just add some text I'll just
add double s here now this by
intentionally I am going to fail this
test case just to show you that whenever
there is a test case failure how does
the ID behaves and how do you get to
know the failures all right so I'll just
run the test test case again so before
that let's close the previous window all
right done and now here I'll also
uncomment the close because anyway it's
a failure which I'm going to see which I
should be able to see it in the logs so
I'll close the browser after the
execution of test case Okay so let's
simply go and run the test case Okay
amazon.in is launched it should search
for iPhone now yeah there you go all
right now it should also close the
browser yes let us close the browser and
it has failed now see here now this is
the line where our Command fit why
because the expected title was not there
and if you look in the logs it says your
assert title on Amazon dot in failed at
actual result was something different
and it did not match with what we had
asked it for so this is how simple it is
to use your ID to create your automation
scripts so we saw all the components of
ID we saw the record button then I
showed you the toolbar I showed you the
editor box and also the test execution
log so now let's come to what are the
limitations of this ID with IDE you
cannot export your scripts your test
scripts to web driver scripts this
support is not yet added but it is in
The Works Data driven testing like using
your Excel files or reading data from
the CSV files and passing it to the
script this capability is still not
available also you cannot connect to
database for reading your test data or
perform any kind of database testing
with selenium Webdriver yes you can also
unlike selenium Webdriver you do not
have a good reporting mechanism with the
ID like say for example test NG or
report NC so that brings us to the next
component of The Suite which is selenium
RC a selenium remote control so selenium
RC was developed by Paul Hammond he
refactored the code which was developed
by Jason and was credited with Jason as
a co-creator of selenium selenium server
is written in Java it is used to write
web application tests in different
programming languages as it supports
multiple programming languages like your
Java C sharp Pearl Python and Ruby it
interacts with a browser with the help
of an RC server so this RFC server uses
a simple HTTP get and post request for
communication and as I said earlier also
selenium RC was called as selenium 1.0
overshill but it got deprecated in
selenium 2.0 and was completely removed
in 3.0 and it got replaced by what web
driver and we'll see why this happened
and what was that issue which we had
with the RC server so this is the
architecture of selenium remote control
at a very high level so when Jason
Huggins introduced selenium you know the
tool was called as JavaScript program
and then that was also called as a
selenium core so every HTML has a
JavaScript statements which are executed
by web browser and there is a JavaScript
engine which helps in executing this
command now this RCA had one major issue
now what was that issue say for example
you have a test script say test dot
JavaScript here which you are trying to
access elements from anywhere from the
google.com domain so what used to happen
is every element which is accessible are
the elements which can belong only to
google.com domain like say for example
mail the search or the drive so any
elements from this can be accessible
through your test scripts however
nothing outside the domain of say
google.com in this case was accessible
say for example if your test scripts
wanted to access something from
yahoo.com this was not possible and this
is due to the security reasons obviously
now to overcome that the testers what
they had to do was they had to install
the selenium core and the web server
which contained your web application
which is under test on the same machine
and imagine if you have to do this for
every machine which is under test this
is not going to be feasible or even
effective all the time and this issue is
called as the same origin policy now
what the same origin policy issue says
is it prohibits a JavaScript from
accessing elements or interacting with
scripts from my domain different from
where it is launched and this is purely
for the security measure so if you have
written a scripts which can access your
google.com or anything related to
google.com these scripts cannot access
any elements outside the domain like as
we said in the example yahoo.com this
was the same origin policy now to
overcome this what this gentleman did
was he created something called as
selenium remote control server to trick
the browser in believing that your
course your selenium core and your web
application under test are from the same
domain and this is what was the selenium
remote control so if you look at again a
high level architecture or how did this
actually work first you write your test
scripts which is here right in any of
the supported language like your PHP or
your Java or Python and before we start
testing we need to launch this RC server
which is a separate application so this
selenium server is responsible for
receiving the Cellini's commands and
these Cellini's commands are the ones
which you have written in your script it
interprets them and reports the result
back to your test so all that is done to
your RC server the browser interaction
which happens through RC server right
from here to your browser so this
happens through a simple HTTP post and
get request and that is how your RC
server and your browser communicate and
how exactly this communication happens
this RC server it acts like a proxy so
say your test scripts asked to launch a
browser so what happens is this commands
goes to your server and then your RC
server launches the browser it injects
the JavaScript into the browser once
this is done all the subsequent calls
from your test script right from your
test scripts to your browser goes
through your RC and now upon upon
receiving these instruction your
selenium core executes this actual
commands as JavaScript commands on the
browser and then the test results are
displayed back from your browser to your
RC to your test scripts so the same
cycle gets repeated right until the
complete test case execution is over so
for every command what you write in your
JavaScript here or your test script here
goes through a complete cycle of going
through the RC server to the browser
collecting the results again from the RC
server back to your test scripts so this
cycle gets repeated for every command
until your complete test execution is
done so RC had definitely lot of
shortcomings and what are those so RC
server needs to be installed before
running any test scripts which we just
saw so that was an additional setup
since it acts as a mediator between your
commands which is your selling is
commands and your browser the
architecture of RC is complex educated
why because of its intermediate RC
server which is required to communicate
with the browser the execution of
commands takes very long it is slower we
know why because every command in this
takes a full trip from the test script
to your RC server to the core engine to
the browser and then back to the same
route which makes your overall test
execution very slow lastly the API is
supported by RC are very redundant and
confusing so RC does have a good number
of apis however it is less object
oriented so they are redundant and
confusing say for example say if you
want to write into a text box how and
when to use a type key command or just a
type command is always confusing another
example is some of the mouse commas
using a click or a mouse door both kind
of you know almost providing a similar
functionality so that is the kind of
confusion which developers use to create
hence
selenium RC got deprecated and is no
more available in later selenium
versions it is obsolete now now to
overcome these shortfalls web driver was
introduced so while RC was introduced in
2004 web driver was introduced by Simon
Stewart in 2006. It's a class platform
testing platform so Webdriver can run on
any platform like say Linux Windows Mac
or even if you have a Ubuntu machine you
can run your selenium scripts on this
machine it is a programming interface to
run test cases it is not an ID and how
does this work actually so test cases
are created and executed using web
elements or objects using the object
locator and the web driver method so
when I do a demo you will understand
what this Webdriver methods are and how
do we locate the web elements on the web
page it does not require a core engine
like RC so it is pretty fast why because
web driver interacts directly with the
browser and it does not have that
intermediate server like the RC hat so
each browser in this case what happens
is each browser has its own driver on
which the application runs and this
driver is responsible to make the
browser understand the commands which
you will be passing from the script like
say for example click of a button or you
want to enter some text so through your
script you tell which browser you want
to work with say Chrome and then the
Chrome driver is responsible for
interpreting your instructions and to
execute it on the web application
launched on the Chrome browser so like
RC Webdriver also supports multiple
programming languages in which you can
write your test Scripts
so another advantage of web driver is it
supports various Frameworks like test NG
junit and unit and Report entry so when
we talk about the limitations of
Webdriver you will appreciate how this
support for various Frameworks and tools
help in making the selenium a complete
automation solution for web application
so let's look at the architecture of
Webdriver at a high level what is in
Webdriver so Webdriver consists of four
major components
the first one is we have got client
libraries right or what we also call it
as language bindings
so since selenium supports multiple
language and you are free to use any of
the supported languages to create your
automation script these libraries are
made available on your selenium website
which you need to download and then
write your scripts accordingly so let's
go and see from where do we download
this so if I go to my browser
so
seleniumhq.org right so if you're
working with selenium this website is
your Bible so anything and everything
you need to know about selenium right
you need to come here and use all the
stabs here in this website so right now
what we are going to look at is what are
those language binding so for that I'll
have to go to this download tab here
okay and if you scroll down here you
will see something like selenium client
and web driver language bindings and for
each of the supported language of
selenium you have a download link
right so say for example if you're
working with Java here what you need to
do is you need to download your Java
language binding so let's go back to the
presentation so this is where your
language bindings are available next so
selenium provides lots of apis for us to
interact with the browser and when we do
the demo I'll be showing you some of
this APS right and these are nothing but
the rest apis and everything whatever we
do through the script happens through
the rest course then we have a Json wire
protocol what is Json JavaScript object
notation it is nothing but a standard
for exchanging data over the web so for
example you want to say launch a web
application through your script
so what selenium does it it creates a
Json payload and posts the request to
the browser driver that is here and then
we have this browser drivers themselves
and as I said there is a specific driver
for each browser
as you know every tool has its own
limitation so does selenium so let's
look at what these limitations are and
if there are any workarounds for them
cannot test mobile applications requires
framework like APM
selenium is for automating web
application it cannot handle mobile
applications mobile applications are
little different and they need its own
set of automation tool however what
selenium provides is a support for
integrating this APM tool which is
nothing but a mobile application
automation tool and using APM and
selenium you can still achieve mobile
application automation
and when do you usually need this when
your application under test is also
supported on mobile devices you would
want a mechanism to run the same test
cases on web browser as well as your
mobile process right so this is how you
achieve it the next limitation so when
we talked about the components of
selenium I said that with Webdriver we
can achieve only sequential execution
however in real time scenario we cannot
just live with this we need to have a
mechanism to run our test cases
parallelly on multiple machines as well
as on multiple browsers so though this
is a limitation of web driver but what
selenium offers is something called as
grid which helps us achieve this and we
will see in shortly what the selenium
grid is all about also if you want to
know more details as how do we work with
the grid how do you want to install that
grid so do check out our video on simply
learn website on selenium grid third
limitations so limited reporting
capability so selenium Webdriver has a
limited reporting capability it can
create basic reports but what we
definitely need is a more
so it does support some tools like say
test NG report NG and even extend
reports which you can integrate with
selenium and generate beautiful reports
powerful isn't it also there are other
challenges with selenium like selenium
is not very good with image testing
especially for the ones which are
designed for web application automation
but then we have other tools which can
be used along with selenium like autoit
and securely so if you look at all this
selenium still provides a complete
solution for your automation needs and
that's the beauty of selenium and that
is why it makes the most popular tool of
today for automation
okay let's do a quick comparison between
the selenium RC and the Webdriver
so RC has a very complex architecture
you know why because of the additional
RC server whereas due to direct
interaction with the browser web driver
architecture is pretty simple execution
speed
it is slower in RC and much faster than
Webdriver why because in Webdriver we
have eliminated that complete layer of
selenium server write that the RC server
and we established a direct
communication with the browser through
browser drivers it requires an RCA
server to interact with the browsers we
just talked about it and whereas
Webdriver can directly interact with the
browser
so RC again we talked about this as one
of the limitations that we have lot of
redundant ABS which kept developers
guessing as which API to use for what
functionality however Webdriver offers
pretty clean apis to work with
RC did not offer any support for
headless browser whereas in Webdriver
you do have a support for using headless
browsers
let's see the web driver in action now
now for the demo we will use this
particular use case and what this use
case says is navigate to the official
simply learn website then type the
selenium in search bar and click on it
and click on the selenium 3.0 training
so we are basically searching for
selenium 3.0 training on the simply
learn website first let's do the steps
manually and then we will go ahead and
write the automation script so let's go
to my browser on my browser what I'll do
is let me first launch the simply learn
website
okay and here what my use case step says
is I need to search for selenium and
click on the search button so once I do
that it is going to give me a complete
list of all kind of selenium trainings
which is available with simpler and what
I'm interested in is the selenium 3.0
training here once I find this on the
web page I need to go and click on that
all right so this is all the steps which
we are going to perform in this use case
okay now so for writing the test cases
I'll be using an ID which is Eclipse I
have already installed my eclipse and
also I have installed selenium in this
instance of my Eclipse all right so if
you can see the reference library folder
here you will see all the jars which are
required for the selenium to work next
another prereq which is required for
selenium and that is your driver files
now every browser which you want to work
with has its own driver file to execute
your selenium scripts and since for this
demo I'll be working with the Firefox
browser I will need a driver file for
Firefox now driver file for Firefox is
the gecko driver which I have already
downloaded and placed in my folder
called drivers now where did I download
this from let's go ahead and see that so
if I go back to my browser and if you go
to your selenium HQ dot website you have
to go to this download tab here in the
download tab when you scroll down you
will see something like third party
drivers bindings and plugins in this
you'll see the list of all the browsers
which is supported by selenium and
against each of this browser you will
find a link which has the driver files
now since we'll be using the gecko
driver
this is the link where you need to go to
and depending on which operating system
which you're working on you need to
download that particular file now since
I am working on Mac this is the file
which I'm using if you're a Windows user
you need to download this ZIP file and
unzip it so once you unzip that you
would
get a file called gecko driver for your
Firefox or a chrome driver for your
Chrome browser and then what you do is
you just create a directory called
drivers under your project and just
place the driver files here so these are
the two prereqs for your selenium one is
importing your jar files like this and
then having your drivers downloaded and
keep them under a folder where you can
reference to okay so now we'll go ahead
and create a class I already have a
package created in this project so I'll
use this project and create a new class
so I'll say create new Java class and
let's call this as search training I'll
be using a public static wordman and
I'll click on finish so let's remove
this Auto generated lines as we do not
need them all right now the first
statement which you need to write before
even you start writing the rest of your
code is what you need to do is you need
to define or declare your driver
variable using your class web driver so
what I would do is I'll say Webdriver
driver
done all right now you will see that
this ID is going to flash some errors
for you that means it is going to ask
you to import certain libraries which is
required by the web driver so simply
just go ahead and say import Webdriver
from
org.opensq.cellaneous this is the
package which we will need all right so
you have a driver created which is of
the class web driver and now after this
I'm going to create three methods all
right so first method I will have for
launching the Firefox browser okay and
then I will write a simple method for
searching
selenium training and clicking on it
this is the actual use case what we'll
be doing and then third method I'm going
to write is just to close the browser
which I'm going to be opening right so
these are the different methods which
I'll be creating and from the public
static wordman I will just call these
methods one after the other okay so
let's go ahead and write the first
method now my first method is launching
the Firefox browser so I'll say public
void since my return type is null or
there is no return type for this let's
call it as launch browser okay all right
now in this for launching any browser I
need to mention two steps now the first
step is where I need to do a system.set
property okay let's do that first and
then I'll explain what this does I'll
just say system dot set property so this
accepts a key and a value pair so what
is my key here my key here is Webdriver
dot gecko dot driver and I need to
provide a value so value is nothing but
the path to the gecko driver and we know
that this gecko driver which I'm going
to use here is right here in the same
project path under the drivers folder
okay and that is what the path which I
am going to provide here so here simply
I need to say drivers slash gecko driver
gec KO all right done and let me close
this sentence all right now since I am a
Mac User my gecko driver installable is
just the name gecko driver if your
windows use and if you are running your
selenium scripts on the Windows machine
you need to provide a complete path to
this including dot exe because driver
executable on your machines is going to
be gecko driver.exe all right so just
make sure that your path which you
mentioned here in the system.set
property is the correct path okay then
the next thing what we need to do is I
need to just say driver is equal to new
Firefox driver okay so this command new
Firefox driver creates an instance of
the Firefox browser now this is also
flagging me error why because again it's
going to ask me to import the packages
where the Firefox driver class is
present okay we did that now these two
lines are responsible for launching the
Firefox browser form so this is done so
what's my next step in the use case now
I need to launch the website simply
learn so for that we have a command
called driver dot get driver dot get
what it does is whatever URL you're
going to give it here in this double
quotes as an argument it is going to
launch that particular website and for
us it's a simply learn website so what I
do as a best practices instead of typing
out the URL I go to my browser launch
that URL which I want to test and I
simply copy it come back to your eclipse
and just simply paste it so this ensures
that I do not make any mistakes in the
URL okay so done so our first method is
ready where we are launching the browser
which is our Firefox browser and then
launching the simply learn website now
the next method what is my next method
in my next method I need to give the
search string to search selenium
training on this particular website now
for that we need to do few things what
are those few things let's go to the
website again all right so let me
relaunch this let's close this okay let
me remove all this and let's go to the
home page first okay this is my home
page so as you saw when I did a manual
testing of this I entered the text here
so now since I have to write a script
for this first I need to identify what
this element centers for that what I am
going to do is I'm just going to say
right click here and I'll say inspect
element all right now this element let's
see what attribute it has which I can
use for finding this element so I I see
that there is an ID present so what I'm
going to do is I'm just going to Simply
use this ID and then I'll just copy this
ID from here go back to Eclipse let's
write a method first so I'll say public
void and what do we give the method name
say search training or just search all
right now in this I need to use a
command called driver dot find element
by ID is what I'm going to use as a
locating technique and in double quotes
the ID which I copied from the website
is what I'm going to paste here okay and
then what am I going to do on this
element is I need to send that text the
text which I'm going to search for which
is selenium so I'll just say send keys
and whatever text I want to send I need
to give it in double quotes So for that
selenium so this is done so now I've
entered the text sure and after entering
the text I need to click on this button
so for that I need to first know what
that button is so let's inspect that
search button okay now if you look at
the search button other than the tag
which is span and the class name I do
not have anything here all right so what
I can do is I can either use the class
name or I can write an X path since this
is a demo which we have already used ID
locating technique I would go ahead and
use the X path here so for me to
construct an X path uh I will copy this
class first okay and then I already have
a crow path installed on my Firefox so
I'll use the crow path and first test my
X bar so I'll just say double slash
let's see what was that element it has a
span tag okay so I'll have to use span
and at class equal to and I'll just copy
the class name here and let's see if it
can identify that element yeah so it is
able to identify so I'll just use this x
path in my code so I'll go back to
eclipse and I'll say driver dot find
element by y dot X path and the X path
which I just copied from cropath is what
I'm going to paste here and what is the
action I need to do here I need to say
click done so I have reached a stage
where I have entered this selenium okay
and then I have clicked on the search
button once I do this I know that
expected result is I should be able to
find this particular link here selenium
3.0 training okay and I should be able
to click on that so for that again I
need to inspect this so let's inspect
this selenium 3.2 all right so now what
are the elements this has now this
particular element has attributes like
it has a tag H2 then it has got some
class name and some other attributes so
I would again would like to use a x path
here now this time while using the X
path I am going to make use of a text
functionality so that I can search for
this particular text so I'll simply copy
this I'll go to my Crow path the tag is
H2 so I'll say simply H2 okay and here
I'll say text equal to and this is the
text which I copied I missed all that
yes there so I'm just going to add an S
okay so let's first test here whether it
is able to identify that element yeah so
it is able to identify so can you see
your blue dotted line it is able to show
us which element it is identified so
I'll copy this x path now and let's go
to my ID Eclipse so now here what I need
to do is I'll have to again simply say
driver dot find element by dot X path
and paste the X path which we just did
and then again I have to do a click
operation done all right so technically
we have taken all the steps of the use
case and we have written the commands
for that now let's add an additional
thing here say after coming to this page
after finding this we want to uh say
print the title of this page now what is
the title of this page if you just hover
your mouse on this it says online and
classroom training for professional
certification courses simply now so what
I will do is after doing all these
operations I will just print out this
page title on our console so for that I
have to just do this try fiber dot so
let's do a sys out so I'll say sys out
system.out.println okay and here I would
say let's add a text here the page title
is and then let's append it with driver
dot get title so this is the command
which we'll be using to fetch the page
title done now what is the last method I
need to add just to close the browser
all right so let me add a method here
I'll say public void close browser and
it's a one single command which I need
to call I'll say driver dot quit Okay
and then I need to call all these
methods from my public static void main
so I let me use my class name which is
this so I'm going to create an object
obj is equal to new class name and then
using this object first is I need to
call the method launch browser and then
I'll call the method search right and
then I'll call the method close browser
so technically our script is is ready
with all the functionality which we
wanted to cover from our use case now
there are few other tweaks which I need
to do this and I'll tell you why I need
to do this now for example after we
click here right after we click on the
search if you observed on your website
it took a little while before it listed
out all the selenium trainings for us
and usually when you are actually doing
it you wait for the selenium 3.0
training to be available and then you
click on that now same thing you also
need to tell your scripts to do that you
need to tell your scripts to wait for a
while until you start seeing the
selenium 3.0 training or it appears on
your web page there are multiple ways to
do that in your script and it is a part
of overall synchronization what we call
where we use kind of implicit and
explicit kind of image now since this is
a demo for demo purpose what I'm going
to do is I am going to use a command
called
thread.sleep and I'm just going to give
an explicit weight of say three seconds
so you can use this Main only for the
demo purposes you can use a thread.sleep
command now this thread.sleep command
needs us to handle some exceptions so
I'm just going to click on ADD throws
declaration and say interrupted
exception now same thing I'll have to do
it in my main function also okay so
let's do that and complete it all right
so this is done so by doing this what am
I doing I'm ensuring that before I click
on the selenium 3. training we are
giving enough time for the script to
wait until the web page shows this link
to the selenium 3.0 training that's one
thing I'm doing all right and also now
since you are going to be seeing this
demo through the video recording the
script when it starts running it is
going to be very fast so you might just
miss out saying how it does the send
keys and how did it click on the search
button for us to enable us to see it
properly I'll just add some explicit
weights here just for a demo purpose so
after entering the keys right so what
I'll do is I'll just give a simple
thread.sleep here now
okay so probably a three seconds or two
seconds which should be good enough okay
at three seconds weight should be good
enough here so that we should be able to
see how exactly this works on your
browser when we execute this okay now
our complete script is ready so what
I'll do is I'll just save the script and
then we will simply run the script so to
run the script I just say right click
run as Java application okay it says ask
me to select and save I have saved the
script now so let's observe how it runs
okay the simply learn.com the website is
launched so the selenium text has been
entered in the search box it has clicked
on the search okay all right so now it
did everything whatever we wanted it to
do all right so since we are closing the
browser you are unable to see whether
the selenium three dot training was
selected or not however what I have
given here is to fetch the title after
all these operations were complete and
if you see here the complete operations
was done and and we were able to see the
page title here okay so now what we'll
do since we are unable to see whether it
clicked on the selenium 3. training or
not I'll just comment out the closed
browser uh the command okay so we will
not call the closed browser so that the
browser remains open and we get to see
whether did it really find the training
link or not okay so let me close this
window we don't need this Firefox window
close all tabs and then I'll just
re-execute this script so I'll say run
as Java application so save the file
okay simplylearn.com is launched so
search text is entered now it's going to
click on the search button yes all right
so we've got the search results it
should click on selenium 3.0 training
and yes it is successfully able to click
on that all right so now it's not going
to close the browser because we have
commented on that line however it did
print us the title here all right so
this is a simple way of using the
selenium Scripts selenium grid so grid
is used to run multiple test scripts on
multiple machines at the same time with
Webdriver you can only do sequential
execution but in real time environment
you always have the need to run test
cases in distributed environment and
that is where selenium grid comes into
picture so grid was conceptualized and
developed by Patrick the main objective
is to minimize test execution type and
how by running your test parallelly so
design is in such a way that commands
are distributed on multiple machines
where you want to run tests and all
these are executed simultaneously what
do you achieve by this methodology of
course the parallel execution on
different browsers and operating system
grid is pretty flexible and can
integrate with many tools like say you
want a reporting tool integrated to pull
all the reports from the multiple
machines where you're running your test
cases and you want to present that
report in a good looking format so you
have an option to integrate such report
okay so how does this grid work so grid
has as a hub and node concept which
helps in achieving the parallel
execution let's take an example say your
application supports all browsers and
most of the operating systems like as in
this picture you could say one of them
is a Windows machine one of them is a
Mac machine and another one is say a
Linux machine so your requirement is to
run the test on all supported browsers
and operating systems like the one which
is depicted in this picture so what you
have to do is first thing is you
configure a Master machine or what you
also call it as a hub by running
something called a selenium Standalone
server and this Talent Standalone server
can be downloaded from the selenium HQ
website using the server you create a
hub configuration that is this node and
then you create nodes specific to your
machine requirement and how are these
nodes created you again use the same
server which is your Standalone selenium
server to create the node configuration
so I'll show you where the selenium
server can be downloaded so if we go
back to our selenium HQ website so you
can see here right on the top it says
selenium Standalone server welcome
everyone to our one another demo on
which we are going to see that how
exactly we can do the installation of
Docker on the Windows platform
specifically on Windows 10. now Docker
is something which is available for most
of the operating systems different
different platforms so it supports both
the Unix and the windows platform as
such so Linux through various commands
we can do the installation but in the
case of Windows you have to download the
exe file and a particular installer from
the docker Hub websites you can simply
Google it and you will get a kind of a
link from where you will be able to
download the package so let's go to the
Chrome and try to search on for the
windows string uh particular installer
you will get a link from Docker Hub you
download it you get the stable version
you get the edge version whichever
version you want you wish to download
you can download it so let's go back to
the Chrome so here you have the docker
next for Windows so you can go for this
table or you can go for the edge right
so you also have the comparison that
what is the difference between these two
versions right so um the particular Edge
version is something which is getting
releases every month and uh the stable
version is getting the releases every
quarter so they are not doing much of
the changes to the stable version as
compared to the edge there so you just
have to double click on the installer
and that will help you to do the
installation of the process so let's get
started so you just click on the get in
stable version so when you do that the
particular installer is going to install
now it's going to take like around 300
MB there so that's the kind of installed
which is available so once the installer
is downloaded so what you can do is that
you can actually go ahead and you can
proceed with the doing the double click
on this installer when you double click
on that you have to proceed with some of
the steps like you know from the GUI
itself you are going to proceed with
these steps so we'll wait for 10 to 20
seconds more and then the installer will
be done and then we can do the double
click and the installation will proceed
so another thing is that uh there is a
huge difference between the installer
like for example in case of Unix they
install it is a little bit less but in
case of windows it's a GUI is also
involved and there are a lot of binders
which is available there so that's the
reason why you know the Hue size is
there now it's available for free that's
for sure and it also requires the
Windows 10 professional or Enterprise
64-bit there so um if you are working on
some previous uh version of operating
systems like Windows 7 and all you have
the older version called Docker toolbox
so they used to call it as like Docker
toolbox earlier but now they are calling
it as in Docker desktop with a new
Docker Windows 10 support as such here
so another couple of seconds and then
the installer will be done and then we
will be able to proceed with the
installation
so let's see that how much progress is
there to the download so we'll click on
the downloads and here still we have
some particular installations or some
download going on so we'll wait for some
time and once the installation is done
then we'll go back and we'll proceed
with installation
so couple of seconds
so it's almost done so I'll just click
on this one you can go to the directory
to the downloads and you can double
click on that also but if you want to do
the installation you can click on this
one also and it will ask for the
approval yes or no you have to provide
now once that is done so um a desktop a
kind of a GUI component will open there
so it will start proceeding with the
installation so it's asking whether you
want to add a desktop the shortcut to
desktop so you can say okay I'm going to
click on OK so it will unpack the files
all the files which is required for
Docker to successfully install that is
getting unpacked over here so it will
take some time to do the installation
because it's doing a lot of work here so
you can just wait for till the execution
of the installer to be completed and
once the installer is done you can open
your command line and start working on
the docker
so taking some time to extract the files
now it's asking us to you know do the
close and do the restart so once that is
done you will be able to proceed further
and you can just you know run the
command line and any Docker command if
you can run so that will give you the
response whether the docker is installed
or not so you can see here that Docker
is you know something which is installed
so you can run like Docker version you
will be able to get a version of the
client when you do the restart of the
machine then add that moment of time the
docker server will also be started and
then this particular error message will
go off right now the docker demon is not
up and running because the installation
requires a restart and when you close on
this one and go for the restart the
machine will be restarted here so this
is the way that how exactly we can go
for a Docker installation and we can go
on that part so now let's begin with the
demo we'll be installing Docker on an
Ubuntu system so this is my system I'll
just open the terminal so the first
thing you can start with is removing any
Docker installation that you probably
already have present in your system if
you want to start from scratch so this
is the command to do so sudo app get
removed docker
dock engine Docker dot IO enter your
password
and Docker is removed so now we'll start
from scratch and we'll install Docker
once again before that I'll just clear
my screen
okay so before I install Docker let me
just ensure that all these softwares on
my system currently is in its latest
state
so sudo app get update
trade so that's done next thing we'll
actually install our docker
so type in sudo apt
get
install
docker
now as you can see here there's an error
that's occurred so sometimes it's
possible that due to the environment of
the machine that you're working in this
particular command does not work
in which case there's always another
command that you can start with
just type Docker install
and that by itself will give you the
commands you can use to install docker
so as it says here sudo apt install.io
is a command that we will need to
execute to install docker
and after that we'll execute the sudo
snap install Docker so sudo apt install
Docker dot IO first
and this will install your docker
after that's done we will have sudo snap
install Docker so snap install Docker
installs a newly created snap package
there are basically some other
dependencies for Docker that you'll have
to install
of course since this is the installation
process for the entire Docker i o it
will take some time
great so our Docker is installed the
next thing we do as I mentioned earlier
is that we need to install all the
dependency packages so the command for
that is
sudo snap install talk
enter your password
so with that we have completed the
installation process for Docker but
we'll perform a few more stages where we
will test if the installation has been
done right
so before we move on with the testing
for Docker let's once again just check
the version that we have installed
so for that the command is Docker
version
and as you can see Docker version
17.12.1 CE has been installed next thing
we do is we pull an image from the
docker hub so Docker run
hello
world
now hello world is a Docker image which
is present on the docker Hub Docker Hub
is basically a repository that you can
find online so with this command the
docker image hello world has been pulled
onto your system
so let's see if it's actually present on
your system now the command to check
this
is sudo Docker images
and as you can see here hello world
repository this is present on our system
currently so the image has been
successfully pulled onto the system and
this means that our Docker is working
now we'll try out another command
this displays all the containers that
you have pulled so far so as you can see
here there are three hello world images
displayed and all of them are in exited
state so I did this demo previously too
which is why the two hello worlds which
is created two minutes ago is also
displayed here and the first hello world
which has been created a minute ago is
the one we just did for this demo now as
you have probably noticed that all the
hello world images over here all these
containers are in the exited state so
when you give the option for Docker PS
minus a where minus a stands for all it
displays all the containers whether they
are in exited or running state if you
want to see only those containers which
are in their running State you can
simply execute sudo Docker PS
sudo docker
yes
and as you can see no container is
visible here because none of them are in
running state if you are looking to
enter the world of devops Simply learns
postgraduate program in devops is the
perfect place to start this
comprehensive program is designed to
equip you with the skills and knowledge
you need to succeed in a devops role
with over 500 hours of learning content
Hands-On projects and Industry relevant
case studies you will gain practical
experience in tools like git Docker
kubernetes and more so whether you are
an ID professional looking to upskill or
a fresh graduate looking to start a
career in devops don't miss out on this
opportunity to join the ranks of
successful devops professional and roll
now and take the first step towards a
brighter future in this presentation
we're going to go through a number of
key things we're going to compare Docker
versus traditional virtual machines and
what are the differences and why you'd
want to choose Docker over a virtual
environment we'll go through the
advantages of working with Docker and
the structure and how you would build
out a Docker environment and during that
structure we'll dig through the
components and the advanced components
within Docker at the end of the
presentation we'll go through some basic
commands and then show you how those
basic commands can be used in a live
demo so with all that said let's get
started so let's first we'll compare
Docker with a traditional virtual
machine so here we have the architecture
on the left and right of a traditional
Virtual Machine versus a Docker
environment and there are some things
that you'll probably see immediately
that are big differences one is that the
virtual environment has hypervisor layer
whereas the docker environment has a
Docker engine layer and then in addition
to that there are additional layers
within the virtual machine each of these
really start compounding and creating
very significant differences between a
Docker environment and a virtual machine
environment so with a virtual machine
the actual memory usage is very high
where is with the darker environment the
memory usage is very low if we look at
performance virtual machines when you
start building out particularly more
than one virtual machine on a server the
performance starts aggregating and
starts getting poorer whereas with
Docker the performance always stays
really good this is largely due to the
lightweight architecture used to
construct the docker containers
themselves if we look at portability
virtual machines just are terrible for
portability they're still dependent on
the host operating system and there's
just a lot of problems that happen when
you are using virtual machines for
portability in contrast Docker was
designed for portability so you can
actually build Solutions in a Docker
container environment and have the
guarantee that the solution will work as
you have built it no matter where it's
hosted finally boot up time at the boot
up time for a virtual machine is fairly
slow in comparison to the boot up time
for a docker environment which is almost
instantaneous so we look at these in a
little bit more detail one of the other
challenges that you have with a virtual
machine is that if you have unused
memory within the environment you cannot
reallocate that memory so if you set up
an environment that has nine gigs of
memory that's being used we have six
gigs that are free you can't do anything
with it though that whole nine gig has
been allocated to that virtual machine
in contrast with Docker if you have nine
gigs and six gigs becomes free that free
memory can then be reallocated and
reused across other containers used
within that Docker environment another
challenge is running multiple virtual
machines in a single environment I can
lead to instability and performance
issues whereas Docker is designed to run
multiple containers in the same
environment and actually gets better the
more containers you run in that hosted
single Docker engine portability issues
with a virtual machine is the software
can work on one machine but then when
you move that VM to another machine
suddenly some of the software won't work
because there are some dependencies that
haven't been inherited correctly whereas
Docker itself is designed specifically
to be able to run across multiple
environments and to be deployed very
easily across systems and again the
actual boot up time for a VM it just
takes a long time you're talking about
minute in contrast to the milliseconds
that it takes for a Docker environment
to boot up so let's dig into what Docker
actually is and what allows for these
great performance improvements over a
traditional VM environment so Docker
itself is an OS virtualized software
platform and it allows it organizations
to really easily create deploy and run
applications as what are called Docker
containers that have all the
dependencies Within contains very easily
and the container itself is really just
a very lightweight package that has all
the instructions and dependencies such
as Frameworks libraries bins Etc all
within that container and that container
itself can then be moved from
environment to environment very easily
if we to look in our devops life cycle
the place where Docker really shines is
in deployments because when you're
actually at the point of deploying Your
solution you want to be able to
guarantee that the code that has been
tested will actually work in the
production environment but in addition
to that what we often find is that when
you're actually building the code and
you're actually testing the code having
a container running the solution at
those stages is also a really good plus
because what happens is that the people
building the code and testing the code
are able to validate their work in the
same environment that would be used for
the production environment so really you
can use Docker in multiple stages within
your devops cycle but it becomes really
valuable in the deployment stage so
let's look at some of the key advantages
that you have with Docker some things
that we've already covered is that you
can do rapid deployments and you can do
it really fast the environment itself is
highly portable and was designed for
that in mind the efficiencies that
you'll see will allow you to run
multiple Docker containers in a single
environment as compared to more
traditional VM environments the
configuration itself can be scripted
through a language called yaml which
allows you to be able to write out and
describe the docker environment that you
want to create this in turn allows you
to be able to scale your environment
very very quickly but with all of these
advantages probably the one that is most
critical to the type of work that we're
doing today is security you have to
ensure that the environment you are
running is a highly secure but highly
scalable environment and I'm very
pleased to say that docker it takes
security very seriously so you'll see it
as one of the key tenants for the actual
architecture of the system that you're
implementing so let's look at how Docker
actually works within your environment
so Docker works out there is a what's
called a Docker engine the docker engine
is really comprised of two key elements
that you have a server and a client and
the communication via the two is via
rest API the server as you can imagine
has the instructions that are
communicated out to the client and
instructs the client on what to do the
connection between the client and the
server the communication is via a rest
API on older systems you can take
advantage of the docker toolbox which
allows you to go ahead and control the
docker engine the docker machine Docker
compose and kitomatic so let's now go
into what the actual root components
though of Docker are so let's have a
look at those key components there are
four component we're going to go through
we have the docker clients and server we
have Docker images we have the docker
registry and the docker container we're
going to step through each of these one
by one so let's look at the docker
clients and server first so the docker
clients and server is a command line
instructed solution where you would use
terminal on your Mac or command line on
your PC or Linux system to be able to
issue commands from the docker Daemon
the communication between the docker
client and the docker host and is via a
rest API so you can do Sim communication
such as a Docker pool command which
would send an instruction to the Daemon
which would then form the interaction of
pulling in the correct components such
as an image or container or registry to
the docker client the docker Daemon
itself is actually a service which
actually performs all sorts of operating
and Performance Services and as you'd
imagine the docker Daemon is constantly
listing across the rest API to see if it
needs to perform any specific requests
if you want to trigger and start the
whole process you what you want to do is
use the command dockered within your
Docker Daemon and that will start all of
your performances and then you have a
Docker host which actually runs the
docker Daemon and registry itself so now
let's look into the actual structure of
a Docker image so a Docker image itself
is a template which contains
instructions for the docker container
and that template is written with a
language called yaml and yaml stands for
yet another markup language it's very
easy to learn the docker image itself is
built within that yaml file and then
hosted as a file in the docket registry
the image is really comprised of several
key layers and you start with your base
layer which will typically have your
base image in this instance is your base
operating system such as Ubuntu and then
you then have layer of dependencies
above that this would then comprise the
instructions in a read-only file that
would become your Docker file so I said
you go through and look at what one of
those sets of instructions would look
like so here we have four layers of
instructions we have a from pull run and
then command so what does that actually
look like in our layers so to break this
down the from creates a layer which is
based on Ubuntu and then what we're
doing is we're adding in files from the
docker repository onto that base command
that base layer and then what we want to
be able to do is then say okay what are
the wrong commands so we can actually
then build the container within the
environment and then we want to be able
to then have a command line that
actually executes something within that
container and in this instance the
command is to run python so one of the
things that we will see is that as we
set up multiple containers each new
container is is a new layer with new
images within the docker environment
each container is completely separate
from the other containers within your
Docker environment so you're able to
create your own separate read write
instructions within each layer what's
interesting is that if you delete a
layer then the layer above it will also
get deleted so what happens when you
pull in a layer but something has
changed in the the core image what's
interesting then is that the actual main
image of itself cannot be modified once
you've copied the image you can then
modify it locally but you can never
modify the actual base image itself so
here are some callouts for the
components within a Docker image so the
base layer are in read-only format the
layers can be combined in a union file
system to create a single image the
union file system saves memory space by
avoiding duplication of files and this
allows a file system to appear as a
writable but without modifying the file
which is known as a copy on right the
actual base layer is themselves are read
only so to be able to get around this
structure within a Docker container the
docker's environment itself uses what's
known as a copy and write strategy
within the images and the containers
themselves and so what this allows you
to do is you can actually copy the files
for better efficiency across your entire
container environment the copy and write
strategy does make Docker super
efficient and what you're able to do all
the time is keep reducing the amount of
disk space you're using and the amount
of performance that you're taking from
the server and that's really again a key
element for Docker it's just this
constant ability to be able to keep
improving the efficiency within the
actual system itself alright so let's go
on to item number three which is the
docker registry so the docker registry
itself is the place where you would host
and distribute the different types of
images that you have created or you want
to be used within your environment the
actual repository itself is just a
collection of Docker images and those
Docker images uh built on instructions
that you would write with yaml and are
very easily stored and shared and what
you can actually do is you can actually
associate specific name tags to the
actual Docker images themselves so it's
easy for people to be able to find and
share that image within the docker
registry itself one of the things you
actually see is we go through the demos
you actually see us actually using the
the tag name and you'll see how it is in
alphanumeric identifier and how we
actually use it to actually create the
actual container itself one of the
things you can do to start off how you
would manage a registry is you can
actually use the publicly accessible
Docker Hub registry which is available
to anybody but you can also create your
own registry for your own use internally
the actual registry that you create
internally can have both public and
private images that you create and this
may be for various reasons of how you
structure your environment the actual
commands you would use to actually
connect to the registry are both push
and pull push is to actually push a new
container environment you've created
from your local manager node to the
remote registry and a pool allows you to
pull a new client that has been created
and is being shared so again full
command and it pulls and retrieves a
Docker image from the docker registry
and makes it very easy for people to
share different images consistently
across teams and a push command allows
you to take a new command that you've
created a new container that you've
created and push it to the registry
whether it's Docker Hub or whether it's
your own private registry and allow it
to be shared across your teams some key
did you knows in Docker registry
deleting a repository is not reversible
action so if you delete a repository
it's gone so let's go into the final
stage here which is the actual Docker
container itself so the dock container
itself
um is an executable package of
applications and its dependencies
bundled together so it gives all the
instructions that you would have for the
solution that you're looking to run it's
actually really lightweight and again
this is because of the redundancy that's
built into how you structure the
container and the container itself is
then inherently also extremely portable
what's really good about running a
container though is that it does run
completely in isolation so you're able
to share it very easily from group to
group and you are guaranteed that even
if you are running a container it's not
going to be impacted by any host Os
peculiarities or unique setups as you
would have in a VM or a
non-containerized environment the actual
memory that you have on a Docker
environment they can be shared across
multiple containers which is really
useful typically when you have a vamp
you would have a defined amount of
memory for each VM environment the
challenge you start running into though
is that you can't share that memory
whereas with Docker you can easily share
the memory and for a single environment
across multiple containers the actual
container is built using Docker images
and the command to actually run those
images is a run command and so this can
actually go through a basic structure of
how you would run a Docker image so you
go into terminal window and you would
write a Docker run redis and then it
would run a container called redis so
we're going to go in and if you don't
have the redis image locally installed
it will then pull it from the registry
then the new Docker container redis will
be then available within your
environment so you can actually start
using it so let's look at why containers
are so low lightweight they're so
lightweight because they really have
been able to get away from some of the
additional layers that you have in
virtualization within VMS and the
biggest one is the hypervisor and the
need to run on a host operating system
those are two two big big elements so
you can get rid of those then you're
doing great so let's look at some of the
more advanced concepts within the docker
environment and we're going to look at
two Advanced components one is Docker
compose and the second is Docker swarm
so let's look at Docker compose Docker
compose is really designed for running
multiple containers as a single service
and it does this by running each
container in isolation but allowing the
containers to interact with each other
as was stated earlier on you would
actually write the compose environment
using yaml as the language in the files
that you would create so where would you
use something like Docker compose so an
example would be if you are running an
Apache server with my SQL database and
you need to create a additional
containers to run additional services
without the need to start each one
separately and this is where you would
write a set of files using Docker
compose to be able to help balance out
that demand so let's now look at Docker
swarm so Docker swarm is a service that
allows you to be able to control
multiple Docker environments within a
single platform so what you actually are
looking at doing is within your Docker
swamp is we're treating each node as a
Docker Daemon and we're actually having
an API that's interacting with each of
those nodes there are two types of nodes
that you're going to be getting
comfortable working with one is the
manager node and the second is the
worker node and as you'd expect the
manager node is the one sending out the
instructions to all of the worker nodes
but there is a two-way communication
that is happening the communication
allows for the manager node to be able
to manage the instructions and then
listen to and receive updates from the
working node so if anything happens
within this environment the major node
can react and adjust the architecture of
the the worker node so it's always in
sync it was really great for a large
scaled environments so finally let's go
through what are some of the basic
commands you'd with use within Docker
and once we've gone through all these
basic commands we'll actually show you a
demo of how you'd actually use them as
well so if we're going to go in probably
the first command is to install the
docker and so if you have yam installed
you just do yum install Docker and
you'll install Docker onto your computer
to start the docker Daemon is you want
to do system CTL start Docker the
command to remove a Docker image is
Docker RMI and then the image ID itself
and that's not the image name that's the
actual alphanumeric ID number that you
want to grab the command line to
download a new image is Docker pull and
then the name of the image you'd want to
pull and by default you're going to be
pulling from the docker default registry
that will then connect to your Docker
Daemon and download the images from that
registry the command line to run an
image is Docker run and then the image
ID and then we have the if we wanted to
pull specifically from Docker Hub then
we would have a Docker pull and then the
image name and colon It's tag to pull
build an image from a Docker file you
would do Docker build Dash T and then
the image name and colon tag to shut
down the container you do Docker stop
container ID the access for running a
container is Docker exec it container ID
bash so we've gone through all the
different commands but let's actually
see how they would actually look and
we're going to go ahead and do a demo so
welcome to this demo where we're going
to go ahead and put together all of the
different commands that we have outlined
in the presentation for Docker first is
just to list all of the docker images
that we have so we do sudo Docker images
and we enter an password
and this will Now list out the images
that we've created already and we have
three images there
so let's go ahead and pull a Docker
image so to do that we'll we'll go ahead
and type sudo docker
and actually we don't want to do image
we want to select pull and then the name
of the image that we want to pull which
is going to be my SQL
and by default this is actually going to
go ahead and use the latest MySQL
command MySQL image that we have so it's
now going to head and pull this image
it's going to take a few minutes
depending on your internet connection
speed it's kind of a large file that has
to be downloaded so we'll just wait for
that to download
you can see the others have completed
just move this last file to download
almost there once that's done we're
going to go ahead and do as we'll
actually run the docking container and
create the new container using the image
that we just downloaded but we have to
wait for this to download First
all right so the image has been pulled
from Docker hub
and let's go ahead and create the new
Docker container so we're going to do
sudo docker
run Dash D
Ash p
0.0.0.0
[Music]
colon 80
column 80.
and then put in MySQL
call on latest so we have the latest
version
and we have our new token
and that shows our new Docker container
has been created
now let's go ahead and see if the
container is running
and we'll do sudo docker
PS
to list all the running containers and
what we see is that the container is not
listed there which means it's probably
not running so let's go ahead and list
out all of the images that we have
within Docker so we can see whether it's
actually listed there so we'll do PS
Dash a and yes there we are we can see
that we do have our new container we
want to see called latest and it was
created 36 seconds ago but it's in the
exited mode so what we have to do is we
have to change that status so it's
actually running
so let's change that to running state
we'll do sudo
docker
run
Dash
it
Dash Dash
name
and we can name it
SL
Q
MySQL
slash bin slash
[Music]
and that's now going to be in the root
and we'll exit out of that and now if we
list out the docking containers we
should see it is now an active container
pseudo docker
start
and then we'll start the scene and then
and we should now see it
there we are it's now in the running
State excellent
and we can see that it was updated six
seconds ago
we're gonna go ahead and we're going to
clear the screen
okay now what we want to do is remove
the docker container so we're going to
do is check the list of images that we
have
and so sudo Docker images
here are the images that we have and we
have my sqls listed and what we want to
do is delete my SQL and to do that we're
going to type in sudo Docker RM Dash F
image MySQL
run that command and what we'll find is
the image there's no such image oh okay
so what we actually have to do is we
have to go and see that the image is now
gone it's been removed excellent
it's exactly what we wanted to see
and we can also delete an image by its
image ID as well
however if an image is running and
active we have to kill that image first
so I'm going to go ahead and we're going
to select the image ID we'll copy that
and it's going to
replace that it won't be able to
actually run correctly because the image
is active so what we have to do now is
stop the image and then we can kill it
so it's in the running state
so we have to
so we do pseudo Docker kill
and kill SL that will kill the container
and now we'll see that the container has
gone
and now we can delete the image
and that's going to be the image gone
with image ID but boom easy peasy
okay let's go ahead on to the next
exercise which is to
so here we are we've listed all of the
uh containers and they're all gone so
let's go into the next exercise final
exercise which is to actually create a
batch image and we're going to do a
batch HTTP image so let's go ahead and
write that out so it's going to be
Docker run
Dash did
dash dash name
white is that's going to be the name of
this HTTP service Dash p
ad 8080 colon 80 Dash V
open quotes
dollar sign PWD
close quotes
colon
slash USR
slash local
slash apache2
slash htdogs
slash
httpd semicolon 2.4
let me run that
open in our password again
so what we see is the port is already
being used so let's go ahead and see
which ports let's go see if we can
change the port or see what supports are
running so let's do pseudo images and
see which ports are being used because
so the the port or the name hasn't been
put in correctly so pseudo Docker images
PS
or docker PS Dash a
and yep let's put 80 there
so clear the screen
so we're going to change the container
name because I think we actually have
the wrong container name here so let's
go and change that and we'll paste that
in and voila there we go
now working and we'll just double check
and make sure everything's working
correctly so to do that we'll go into
our web browser and we'll type in since
Firefox opens up
type in localhost
colon 8080
which was the the port that we created
and there we are it's a list of all the
files which shows that the server is up
and running
and today we'll be looking at the
installation for the tool Chef as you
probably already know Chef is a
configuration management tool so that
basically means that Chef is a tool
which can automate the entire process of
configuring multiple systems it also
comes with a variety of other
functionalities which you can check out
in our video on what is chef and the
chef tutorial so before we move on to
the installation process let me just
explain to you in brief the architecture
of Chef so Chef has three components
there's the workstation which is where
the system admin sits and he or she
writes the configuration files here your
second system is the server the server
is where all these configuration files
are stored and finally you have the
client or the node systems so these are
the systems that require the
configuration you can have any number of
clients but for a demo to keep it simple
we'll just have one client now I'm using
my Oracle VM virtualbox manager as you
can see here I'll have two machines the
master and the node both of these are
sent to S7 machine genes as of the
server we'll be using this as a service
on the cloud so let's begin let's have a
look at our Master System first this is
my Master System the terminals open over
here and the terminal color here it's
black background with green text and
this is my note system so the terminal
here has a black background with white
text so you can differentiate between
the both so we start at our Master
System the first thing we need to do is
we need to download the chef DK so you
can write wget which is the command for
downloading and then go to your browser
and just type Chef DK here
the first link
so here you have different versions of
Chef TK depending on the operating
system that you are using you need to
select the appropriate one I'm using the
red hat Enterprise version and that's
number seven so I'm using send to S7 so
this is my link for downloading Chef DK
just copy this link and go back to your
terminal
and paste it here
so your Chef DK is being downloaded this
will take a while right after we
download the chef DK our next step is to
install it on our system
swash Chef DK is downloaded now let's
install it
so guys this is the version of Chef TK
that you have downloaded so make sure
this is exactly what you type down here
too
so great our Chef DK is installed so
basically our installation for the
workstations done right now but just so
you understand how the flow is we'll
also write a sample recipe on our
workstation so before we do that let's
first create a folder
my folder name Chef repo basically the
chef Repository
and let's move into this folder okay so
we're in next what we need to do is as I
mentioned earlier all your recipes will
be within a cookbook so let's create a
folder which will hold all our cookbooks
and let's move into this too
okay so our next stage is to create the
actual cookbook within which we'll have
our recipe so the command for creating
the cookbook is Chef generate
cookbook sample quiz to sample is the
name of my cookbook so guys please
notice here cookbooks is the directory
that I created which will hold all our
cookbooks and here cookbook is the
keyword so sample is that one cookbook
that we are creating under a folder
cookbooks
and a cookbooks being created great so
that's done moving into a cookbook okay
so when a cookbook sample was created
automatically there's this hierarchical
structure associated with it so let's
have a look at this hierarchical
structure to understand what our
cookbook sample exactly is before we
move on so the command for looking at a
hierarchical structure is tree so as you
see here within our cookbook we have a
folder recipes and under this this the
default.rb recipe this is where we'll be
creating a recipe so we'll just alter
the content of default.rb so let's move
on to finally writing our recipes so
we'll move into this recipes folder
first
so now we'll open our recipe default.rb
in G edit
so the recipe for this particular demo
is to install the httpd package on our
client node that is basically your
Apache server and we'll also be hosting
a very simple web page so let's begin
so the recipes in Chef is written in
Ruby
so I'll explain you the recipe in a
while
okay so the first line is where you
install httpd the second line for
service is where you start or enable the
httpd service on the client node that's
our first task the second path where we
need to create our web page
so this is the path where your web page
will be stored
if you have written any HTML file
previously you know that this is
probably like a default path where our
web pages are created
yep that's it so this is the content
that will be displayed on your web page
if everything works right and I'm pretty
sure it will so now we can save our
recipe and that's done close your G
array so now that we have created the
recipe all our work at the workstation
is completed the next thing we do is we
move on to the server so as I mentioned
earlier we'll be using the server as a
service on the cloud so go to your
browser and here just type
manage.chef.io
so this is the home page of your Chef
server click here to get started we need
to First create an account for using the
chef server this completely free we just
need to give our email ID and a few
other details it's in fact a lot like
creating an account on Facebook or
Instagram
fill in all the details check the terms
of service box
so the next thing you need to do is go
back to your inbox and verify your email
ID so I have my inbox opened here on my
Windows machine so this is my inbox
you'd have received a mail from Chef
software just click on this link to
verify it and create your password
and that's done so let's continue this
on our workstation machine
so type in your username and password
so the first time you log into your Chef
server you'll have this pop-up appear
where you need to create a new
organization so create your organization
so this organization is basically the
name that will be associated with a
collection of the client machines first
thing you do go to your Administration
Tab and download the starter kit so guys
when you're doing this part make sure
that you're on your workstation that is
your opening your Chef server on the
workstation because you need this folder
to be installed here
you save the file so this gets
downloaded so the shift starter kit is
the key to connecting your workstation
with the server and the server with the
node so basically it has a tool called
knife which we'll come across later in
our demo this knife is what takes care
of all the communication and the
transferring of cookbooks between the
three machines in our case the two
machines the workstation and the node
and the one server so let's go back to
our root directory so our Chef starter
zip file is within a downloads folder
what we do first is we'll move the zip
folder into our cookbooks folder and
then we'll unzip it there because our
cookbooks folder is the one that
contains the recipe and that is where we
require knife tool command to be present
so we can send this recipes over to the
server
so we'll just check the contents of our
cookbooks right now to ensure that our
chefstarter.zip file is within the
cookbooks
yep so it's here so next thing we do is
we need to unzip this folder
great so that's unzipped and this means
that our workstation and our server are
now linked so we just need to use the
knife command tool to transfer or to
upload our recipes which we created on
the workstation onto the server so
before we execute this command we need
to move into our cookbooks directory as
you know that is where we unzipped our
Chef starter kit so that is where our
knife command is present to
and now let's execute the knife command
so it's knife
cookbook upload and Sample so as you
probably recall sample is the name of
the cookbook that we created and within
sample we created RSD which is
default.rb so we're uploading the entire
cookbook onto the server execute the
command
great so our cookbooks uploaded now
let's check this on our server so move
to your browser where you opened your
Chef server and go to policy so here you
go this is the cookbook we uploaded
sample and it's the first time we
uploaded it so the version
0.1.0 the first version now what you
would notice is if you go to the notes
tab there are no notes present so if you
have no nodes you basically have no
machine to execute your cookbooks and
the nodes are not seen right now because
we have not configured them yet so
that's the next thing we need to do all
this so far was done on your master
machine now we'll move on to the node
machine
so before moving on let's just check the
IP of our node machine
so that's our IP note this down
somewhere and now we move back to our
Workstation
as we already saw that we uploaded a
sample workbook next thing we need to
make sure that our server and node are
able to communicate with each other so
again we use the knife tool for this
tool the command here is knife bootstrap
and enter the IP address of your node
which we just checked
will be logging in there so we'll be
using the node as the root user and then
we also need to specify our root
password for the node
and we give a name to this node so this
is the name by which we'll be
identifying our node at the server
so as you have probably noticed here we
are using the term SSH which is a secure
shell so it basically provides a Channel
of secure communication between two
machines in an unsafe environment
okay so it's done so if your command has
executed right which in our case as we
can see has a chef server and a chef
node must be able to communicate with
each other so if this is so we should be
able to send the cookbook that we
previously uploaded from our workstation
onto the server now from a server to our
node so to do that before we move on to
the node machine we need to go back to
our Chef server let's refresh this page
and as you see here previously under the
notes tab we did not have any node
mentioned now we do Chef node which is
the node we wanted to identify our node
by which is a Centos platform and that's
our IP so it's active for two hours
that's the uptime last check-in the last
time we checked into our node was a
minute back and yeah that's pretty much
it so now we'll create a run list and
we'll add our sample to this run list so
just click on your node and you'll see
the small Arrow here in the end click on
that edit run list and under available
recipes we have a cookbook sample
present so drag and drop this to the
current run list and accept it okay so
now that we updated our run list our
recipe is sent to our node what we next
need to do is that we need to execute
this at our node so now we'll move on to
our node machine
shift client is the command to execute
your
so while this recipe is executing you
can see what exactly is happening a
recipe was to install httpd package
first which is your Apache server so the
first line that's done and it's up to
date the second line it's enabled third
line the service is started and the
fourth line is where your contents
created for the web page at this very
location so by the look of this
everything should work fine so how do we
check this we can just go to our browser
and the search bar just type localhost
and there you go so our httpd package
which is the patchy server is installed
and our sample web page is also hosted
congratulations on completing the chef
demo today we'll dive into a tutorial on
the configuration management tool Chef
so if you look at the devops approach or
the devops lifecycle you will see that
Chef falls under operations and
deployment so before we begin let's have
a brief look at all that you learned
today first we'll get to know why should
we use chef and what exactly is the chef
two of the most common terms used with
Chef configuration management and
infrastructure as code we'll have a
brief look at these we'll also have a
look at the components of chef and the
chef architecture quickly go through the
various flavors of Chef and finally
we'll wrap it up with the demo a demo on
the installation of Apache on our notes
so let's begin guys why should we use
Chef well consider a large company now
this company caters to a large number of
clients and provides a number of
services or Solutions of course to get
all of this done they need a huge number
of servers and a huge number of systems
basically they will have a huge
infrastructure now this infrastructure
needs to be continuously configured and
maintained in fact when you're dealing
with an infrastructure that size there's
a good chance systems may be failing and
in the long run as your company expands
new systems may even get added so what
do you do well you could say the company
has the best system administrator out
there but all by himself could he
possibly take care of an infrastructure
that size no he can't and that's where
Chef comes in because Chef automates
this entire process so what does Chef
provide Chef provides continuous
deployment so when you look at the
market space today you see how products
and their updates are coming out in a
matter of days so it's very important
that a company is able to deploy the
product the minute it's ready so that
once it's out it's not already obsolete
Chef also provides increased system
robustness as we saw Chef can automate
the infrastructure but in spite of this
automation there's a good possibility
that errors do creep in Chef can detect
all these bugs and remove them before
deploying them into the Real Environment
not only this Chef also adapts to the
cloud we all know how today the services
tools Solutions everything is revolving
around the cloud so Chef does really
play along by making itself easily
integratable with the cloud platform so
now that you know why to use Chef let's
look at what exactly is chef chef is an
open source tool developed by Ops Code
of course there are paid versions of
Chef such as Chef Enterprise but other
than that most of it is freely
accessible Chef is written in Ruby and
erlang if you would have gone through
any previous material on Chef I'm sure
you would have come across Ruby being
related to chef but not erlang so this
is why because Ruby and erlang are both
used to build chef but when it comes to
actually writing the codes in Chef it's
just Ruby and these are the codes that's
deployed onto your multiple servers and
does the automatic configuration and
maintenance and this is why Chef is a
configuration management tool so I've
used this term configuration management
a couple of times what exactly does this
mean let's start with the definition of
configuration management configuration
management is a collection of
engineering practices that provides a
systematic way to manage entities for
efficient deployment so let's break this
down configuration management basically
is a collection of practices and what
are these practices for these practices
are for managing your entities the
entities which are required for
efficient deployment so what are these
entities that you need for efficient
deployment they are code infrastructure
and people code is basically the core
the system administrators right for
configuring your various systems
infrastructure is the collection of your
systems and your servers and then
finally you have the teams that take
care of this infrastructure so codes
need to be updated whenever your
infrastructure needs a new configuration
or some sort of updation in the
operating system or the software
versions your code needs to be updated
at first and as the requirements of the
company change the infrastructures
configuration needs to change and
finally of course the people need
coordination so if you have a team of
system administrators and say person a
makes some change to the code person B C
D and so on need to be well aware when
the change is made as as to why it was
made what was the change made and where
exactly this change was made so there
are two types of configuration
Management on our left we have the push
configuration here the server that holds
the files with instructions to configure
your notes pushes these files onto the
loan so the complete control lies with
the server on your right side we have
the pull configuration in case of pull
configuration the nodes pull against the
server to first check if there's any
change in the configurations required if
there is the nodes themselves pull these
configuration files Chef follows pull
configuration and how it does this we'll
see further in our video another
important term often used with Chef
infrastructure as code so let's
understand what this term infrastructure
as code means through this small story
so here's dim dims the system
administrator at a large company now he
receives a task he has to set up a
server and he has to install 20 software
applications over it so he begins he
sets up the server but then it hits him
it would take him the entire night to
install 20 software applications
wouldn't things have been much simpler
if he just had a code to do so well of
course codes do make things much simpler
codes have a number of advantages
they're easily modifiable so if today
Tim is told we need Maya skill installed
on 20 systems Tim simply writes a code
to do so and the very next daytime is
told we changed our mind we don't need
Maya skill I think we'll just use Oracle
this does not bothered him because now
he just opens the file he makes a few
Corrections in his code and that should
work just fine code is also testable so
if Tim had to write 10 commands to do
something and at his 10th command he
realized the very first command he wrote
there was something not right there well
that would be quite tiresome wouldn't it
with codes however you can test tested
even before running it and all the bugs
can be caught and corrected codes are
also Deployable so they're easily
Deployable and they're Deployable
multiple times so now that we saw the
various advantages of having codes let's
say in what infrastructure as code
exactly is here's the definition
infrastructure as code is a type of it
infrastructure where the operation team
manages the code rather than a manual
procedure so infrastructure as a code
allows the operations team to take care
of a code which automatically performs
various procedures rather than having to
manually do those procedures so with
this feature all your policies and your
configurations are written as code let's
now look at the various components of
Chef so our first component is the
workstation the workstation is the
system where the system administrator
sits he or she creates the codes for
configuring your nodes now these codes
which in case of Chef are written in
Ruby are called the recipes and you'll
have multiple number of recipes so a
collection of recipes is called a
cookbook now these cookbooks are only
created at the workstation but they need
to be stored at the server so the knife
is a command line tool so it's basically
a command that you will see us executing
in one of our demos that shifts these
cookbooks from the workstation over to
the server a second component is the
server so servers like the middleman it
lies between your workstation and your
notes and this is where all your
cookbooks are stored because as you saw
previously the knife sends these
cookbooks over from the workstation to
the server the server can be hosted
locally that's on your workstation
itself or it can be remote so you can
have your server at a different location
you can even have it on the cloud
platform and a final confidence the node
so nodes are the systems that require
the configuration in a chef architecture
you can have a number of nodes or high
is a service which is installed on your
node and it is responsible for
collecting all the information regarding
your current state of the node this
information is then sent over to the
server to be compared against the
configuration files and check if any new
configuration is required shift client
is another such service on your node
which is responsible for all the
communications with the server so
whenever the node has a demand for a
recipe the shift client is responsible
for communicating this demand to the
server since you have a number of nodes
in a chef architecture it's not
necessary that each node is identical so
of course every node can have a
different configuration let's now have a
look at the chef architecture so here we
have a workstation one server machine
and two nodes you can have any number of
nodes first things first the system
administrator must create a recipe so
the recipes that are mentioned in our
Chef architecture are just dummy recipes
we look into actual functioning recipes
later in our demo so you have one recipe
two recipes three recipes and a
collection of recipes forms a cookbook
so guys if you look at the recipe in
sauce you have simply learned three dot
Erb Erb is the extension for your Ruby
files so the cookbooks are only created
at the workstation they now need to be
sent over to the server where they are
stored and this is the task of the knife
knife is a command line tool which is
responsible for transferring all your
cookbooks onto the server from the
workstation here's the command for
running your knife knife upload simply
Dash DB where simply Dash DB is the name
of the cookbook we then move on to our
node machines at our nodes we run the
Ojai service the Ojai service will
collect all information regarding the
current state of your notes and send it
over to the chef client when you run the
chef client these informations are sent
over to the server and they are tested
against the cookbooks so if there is any
discrepancy between the current state of
your nodes and the cookbook that is if
one of the notes does not match the
configurations required The cookbook is
then fetched from the server and
executed at the node this sets the node
to the right State there are various
flavors of Chef we'll quickly go through
these first we have Chef solo with Chef
solo there's no separate server so your
cookbooks are located on the Node itself
now this kind of configuration is used
only when you have just a single note to
take care of the next flavor is a hosted
chef with hosted Chef you still have
your workstation and your node but your
server is now used as a service on the
cloud this really makes things simple
because you don't have to set up a
server yourself and it still performs
all the functioning of a typical Chef
this is the configuration you will
notice that we'll be using in our demo
shift client server with shift line
server you have a workstation you have
server and you have and number of nodes
now this is the traditional Chef
architecture this is the one we have
used for all the explanations previously
and finally we have private Chef private
Chef is also known as Enterprise Chef in
this case your workstation server and
node all are located within the
Enterprise infrastructure this is the
main difference between Chef client
server and private Chef in case of Chef
client server all these three machines
could be dispersed the Enterprise
version of Chef also provides the
Liberty to add extra layers of security
and other features and we reach the
final part of our video where we'll have
the Hands-On so before we dive into our
demo let me just quickly give you an
introduction to it we'll be using two
virtual boxes both sent to S7 one will
be used as workstation while the other
will be a node so we are just using one
node to make things simple the server
will be used as a service on the cloud
now these are the steps we'll be
performing during our demo we'll first
download and install the chef DK on our
workstation we then make an empty
cookbook file and we'll write a recipe
into it we need to then set up the
server so as I mentioned earlier server
will be a service on the cloud so you'll
have to create a profile but this will
be completely free we then link the
workstation to the server and we'll
upload the recipe to the server the
nodes will now download the cookbooks
from the server and configure themselves
so now that you have some idea about
what we'll be doing let's move on to the
actual demo We Begin our demo here's my
Oracle VM virtualbox manager I have two
machines here I've already created my
workstation and node both of these are
sent to S7 machines just for you to
differentiate this is my terminal and
for my workstation it's a black
background with white text and as of my
node it's a black background with green
text
the first thing you do is you go to your
workstation box and open a web browser
search for Chef DK installation
go to the first link which is your
Chef's official page a very warm welcome
to all our viewers I'm Anjali from
Simply learn and today I'll be showing
you how you can install the
configuration management tool ansible so
let's have a brief about why one would
use ansible and what exactly is ansible
so if you consider the case of an
organization it has a very large
infrastructure which means it has more
than probably hundreds of systems and
giving one or even a small team of
people the responsibility to configure
all these systems makes their work
really tough repetitive and as you know
manual work is always prone to errors so
ansible is a tool which can automate the
configuration of all these systems with
ansible a small team of system
administrators can write simple code in
yaml and these codes are deployed onto
the hundreds and thousands of servers
which configures them to the desired
States so ansible automates
configuration management that is
configuring your systems it automates
orchestration which means it brings
together a number of applications and
decides an order in which these are
executed and it also automates
deployment of the applications now that
we know what ansible does let's move on
to the installation of ansible so here
is my Oracle VM virtualbox manager I'll
be using two systems there's the node
system which is basically my client
system and there's a server system or
the Master System so let's begin at our
server system
so this is my Master System guys so the
first thing we do is we download our
ansible tool
so one thing we must remember with
ansible is that unlike Chef or puppet
ansible is a push type of configuration
management tool so what this means is
that the entire control here lies with
your master or your server system this
is where you write your configuration
files and these are also responsible for
pushing these configuration files onto
your node or client system as and when
required
Creator ansible tool is installed now we
need to open a ansible host file and
there we'll specify the details of our
node or client machine
host file as you can see here the entire
file is commented but there's a certain
syntax that you'd observe for example
here we have a group name web servers
under which we have the IP addresses or
certain host name so this is about how
we'll be adding the details for our
client system first we need to give a
group name
under this group basically we add all
the clients which require a certain type
of configuration since we are using just
one node we'll give only the details for
that particular node first we need to
add the IP address of our client machine
so let's just go back to our client
machine
and this here is the IP address
192.168.2.11
once you have typed in your IP address
give a space and then we'll specify the
user for a client machine
so all Communications between the server
or the Master System and the client or
the node system takes place through SSH
ssh basically provides a secure channel
for the transfer of information
follow this up with your password in my
case it's the roots password
and that's it we are done so now we save
this file and go back to our terminal
so now that our host file is written the
next thing we do is we write a Playbook
The Playbook is the technical term used
for all the configuration files that we
write in ansible now playbooks are
written in yaml yaml's extremely simple
to both write and understand it's in
fact very close to English
so now we'll write our Playbook The
Playbook or any code in yaml first
starts with three dashes this indicates
the beginning of your file next thing we
need to give a name to our Playbook
so name
and I'm going to name my playbook sample
book
we next need to specify our host systems
which is basically the systems at which
the configuration file or the playbook
in our case will be executed so we'll be
executing this at the client machines
mentioned under the group ansible
servers so we had just one client
machine under it we'll still mention the
group name
we next need to specify the username
with which we'll be logging into our
client machine
which is Root in my case
and become true specifies that you need
to become the route to execute this
Playbook so becoming the roots called a
privilege escalation next we need to
specify our tasks so these are basically
the actions that the Playbook will be
performing so you would have noticed
everything so far is aligned that is
name host remote user become and task
because these are at one level now
whatever comes under task will be
shifted slightly towards the right
although yamil is extremely simple to
understand and read both it's a little
tricky while writing because you need to
be very careful about the indentations
and the spacing
so my first task is install httpd which
is basically a Apache server
so now my command yum and this will be
installing the httpd package
and the latest date of it will be
installed so that's our first task now
our second task would be running our
Apache service
so name run httpd
and the action which is service
will be performed on httpd hence the
name httpd
and state must be started
now we come to our third task
so here we'll create a very simple web
page that will be hosted
so create content is the name of our
task
and the content that we are creating
here will basically be copied to a node
system at a particular file location
that will provide
a Content will be congrats
and then we'll provide the destination
at which this file will be copied
so this is the default location for Ola
HTML files
and that's it we are done writing our
Playbook
just save this and go back to your
terminal
before we execute the Playbook or push
the Playbook onto our node system let's
check the syntax of our Playbook so the
command for doing so is
and if everything is fine with your
playbook the output would be just your
playbook name so our syntax is perfectly
fine now we can push on the Playbook to
our node or the client machine
and that's the Syntax for doing so now
as your playbook is being sent over to
the client machine you can see that
first the facts are gathered that is the
current state of your client machine is
first fetched to check what all is to be
changed and what is already present so
the first thing is installing httpd so
our system already had httpd so it says
okay because this does not need to be
changed our next task was running httpd
now Although our system had the Apache
service it was not running so that is
one thing that was changed the next was
there was no content available so the
content was also added so two tasks were
changed and four things were okay now
everything seems fine and before you
move any forward it is very important
that you check this one line of
documentation provided by ansible you
have all kind of information available
here regarding which all tasks were
executed if your client machine was
reachable or unreachable and so on so
now everything's fine here we can move
on to our node system
and we'll just go to our browser so if
our Playbook has been executed here what
should happen is that the httpd service
must be in the running State and the
webpage that we created should be hosted
so let's just type localhost
and great everything's working fine so
our web page is displayed here so we
come to an end for our installation and
configuration video for the
configuration management tool ansible if
you have any doubts please post them in
the comment section below and we'll
definitely get back to you as soon as
possible if you are looking to enter the
world of devops Simply learns
postgraduate program in devops is the
perfect place to start this
comprehensive program is designed to
equip you with the skills and knowledge
you need to succeed in a devops role
with over 500 hours of learning content
Hands-On projects and Industry relevant
case studies you will gain practical
experience in tools like git Docker
kubernetes and more so whether you are
an IIT professional looking to upskill
or a fresh graduate looking to start a
career in devops don't miss out on this
opportunity to join the ranks of
successful devops professional and roll
now and take the first step towards a
brighter future thanks Anjali now we
have Matthew and Anjali to take us
through how to work with ansible ansible
today as one of the key tools that you
would have within your devops
environment so the things that we're
going to go through today is we're going
to cover why you would want to use a
product like ansible what ansible really
is and how it's of value to you in your
organization the differences between
ansible and other products that are
similar to it on the market and what
makes ansible a compelling product and I
want to dig into the architecture for
ansible we're going to look at how you
would create a Playbook how you would
manage your inventory of your server
environments and then what is the actual
workings of ansible there's a little
extra we're going to also throw in
ansible Tower one of the secret Source
solutions that you can use for improving
the Speed and Performance of how you
create your ansible environments and
finally we're going to go through a use
Case by looking at HootSuite social
media management company and how they
use ansible to really improve the
efficiency within their organizations so
let's jump into this so the big question
is why ansible so you have to think of
ansible as another tool that you have
within your devops environment for
helping manage the servers and this
definitely falls on the operations side
of the devops equation so if we look
here we have a picture of Sam and like
yourselves Sam is a system administrator
and he is responsible for maintaining
the infrastructure for all the different
servers within his company so some of
the servers that he may have that he has
to maintain could be web servers running
Apache it could be database servers
running my Sequel and if you only have a
few servers then that's fairly easy to
maintain I mean if you have three web
servers and two database servers and
let's face it will we all love just to
have one or two servers to manage it
would be really easy to maintain the
trick however is as we start increasing
the number of servers and this is a
reality of the environments that we live
and operate in it becomes increasingly
difficult to create consistent setup of
different infrastructures such as web
servers and databases for the simple
reason that we're all human as if we had
to update and maintain all of those
servers by hand there's a good chance
that we would not set up each server
identically now this is where ansible
really comes to the rescue and helps you
become an efficient operations team
ansible like other system Solutions such
as chef and puppet uses code that you
can write and describe the installation
and set up of your servers so you can
actually repeat it and deploy those
servers consistently into multiple areas
so now you don't have to have one person
redoing and re-following setup
procedures you just write one script and
then each script can be executed and
have a consistent environment so we've
gone through why you'd want to use
ansible let's step through what ansible
really is so you know this is all great
but you know how do we actually use
these tools in our environment so
ansible is a tool that really allows you
to create and control three key areas
that you'd have within your operations
environment first of all there's it
automation so you can actually write
instructions that automate the it setup
that you would typically do manually in
the past the second is the configuration
and having consistent configuration
imagine setting up hundreds of Apache
servers and being able to guarantee with
Precision that each of those Apache
servers is set up identically and then
finally you want to be able to automate
the deployment so that as you scale up
your server environment you can just
push out instructions that can deploy
automatically different servers the
bottom line is you want to be able to
speed up and make your operations team
more efficient so let's talk a little
bit about pull configuration and how it
works with ansible so there are two
different ways of being able to set up
different environments for Server Farms
one is to have a key server that has all
the instructions on and then on each of
the servers that connect to that main
Master server you would have a piece of
software known as a client install on
each of those servers that would
communicate to the main Master server
and then would periodically either
update or change the configuration of
the slave server this is known as a pull
configuration an alternative is a push
configuration and the push configuration
is slightly different the main
difference is as with a pull
configuration you have a master server
where you actually put up the
instructions but unlike the pull
configuration where you have a client
installed on each of the services with a
push configuration you actually have no
client installed on the remote servers
you simply are pushing out the
configuration to those servers and
forcing a restructure or a fresh clean
installation in that environment so
ansible is one of those second
environments where it's a push
configuration server and this contrasts
with other popular products like chef
and puppet which have a master's slave
architecture with a master server
connect acting with a client on a remote
slave environment where you would then
be pushing out the updates with ansible
you're pushing out the service and the
structure of the server to remote
hardware and you are just putting it
onto the hardware irrelevant of the
structure that's out there and there are
some significant advantages that you
have in that in that you're not having
to have the extra overhead weight of a
client installed on those remote servers
having to constantly communicate back to
the master environment so let's step
through the architecture that you would
have for an ansible environment
so when you're setting up an ansible
environment the first thing you want to
do is have a local machine and the local
machine is where you're going to have
all of your instruction and really the
power of the control that you'd be
pushing out to the remote server so the
local machine is where you're going to
be starting and doing all of your work
connected from the local machine are all
the different nodes pushing out the
different configurations that you would
set up on the local machine the
configurations that you would write and
you would write those in code within a
module so you do this on your local
machine for creating these modules and
each of these modules is actually
consistent playbooks the local machine
also has a second job and that job is to
manage the inventory of the nodes that
you have in your environment the local
machine is able to connect to each of
the different nodes that you would have
in your Hardware Network through SSH
clients so a secure client let's dig
into some of the different elements
within that architecture and we're going
to take a first look at playbooks that
you would write and create for the
ansible environments so the core of
ansible is the Playbook this is where
you create the instructions that you
write to define the architecture of your
Hardware so the Playbook is really just
a set of instructions that configure the
different nodes that you have and each
of those set of instructions is written
in a language called yamo and this is a
standard language used for configuration
server environments did you know that
yaml actually stands for yaml a markup
language that's just a little tidbit to
hide behind your ear so let's have a
look or one of these playbooks it looks
like and here we have a sample yaml
script that we've written so you start
off your gamma script with three dashes
and that integrates the start of a
script and then the script itself is
actually consistent of two distinct
plays at the top we have play one and
below that we have play two within each
of those plays we Define which nodes are
we targeting so here we have a web
server in the top play and in the second
play we have a database server that
we're targeting and then within each of
those server environments we have the
specific tasks that we're looking to
execute so let's step through some of
these tasks we have an install patchy
task we have a starter patchy task and
we have it installed my SQL task and
when we do that we're going to execute a
specific set of instructions and those
instructions can include installing
Apache and then setting the state of the
Apache environment or starting the
Apache environment and setting up and
running the MySQL environment so this
really isn't too complicated and that's
the really good thing about working with
yaml is it's really designed to make it
easy for you as an operations lead to be
able to configure the environments that
you want to consistently create so let's
take a step back though we have two
hosts we have web server and database
server why do these names come from well
this takes us into our next stage and
the second part of working with ansible
which is the inventory management part
of ansible so the inventory part of
ansible is where we maintain the
structure of our Network environment so
what we do here is part of the structure
and creating different nodes is we've
had to create two two different nodes
here we have a web server node and a
database server node and under web
server node we actually have the names
that were actually pointed to specific
machines within that environment so now
when we actually write our scripts all
we have to do is refer to either web
server or database server and the
different servers will have the
instructions from the yamascript
executed on them this makes it really
easy for you to be able to just point to
new services without having to write out
complex instructions so let's have a
look at how ansible actually works in
real world so the real world environment
is that you'd have the ansible software
installed on a local machine and then it
connects to different nodes within your
network on the local machine you'll have
your first your playbook which is the
set of instructions for how to set up
the remote nodes and then to identify
how you're going to connect to those
nodes you'll have an inventory we use
secure SSH connections to each of the
servers so we are encrypting the
communication to those servers we're
able to grab some basic facts on each
server so we understand how we can then
push out the Playbook to each server and
configure that server remotely the end
goal is to have an environment that is
consistent so this asks you a simple
question what are the major
opportunities that ansible has over chef
and puppet really like to hear your
answers in the comments below pop them
in there and we'll get back to you and
really want to hear how you feel that
ansible is a stronger product or maybe
you think it's a weaker product as it
compares to other similar products in
the market here's the bonus we're going
to talk a little bit about ansible Tower
so ansible Tower is an extra product
that red hat created it that really kind
of puts the cherry on the top of the ice
cream or is the icing on your cake add
ible by itself is a command line tool
however asphalt Tower is a framework
that was designed to access ansible and
through the ansible tower framework we
now have an easy to use GUI this really
makes it easy for non-developers to be
able to create the environment that they
want to be able to manage in their
devops plan without having to constantly
work with the command prompt window so
instead of opening up terminal window or
a command window and writing out complex
instructions only in text you can now
use drag and drop and mouse click
actions to be able to create your
appropriate playbooks inventories and
pushes for your nodes alright so we've
talked a lot about ansible let's take a
look at a specific company that's using
ansible today and in this example we're
going to look at HootSuite now HootSuite
if you not already use their products
and they have a great product HootSuite
is a social media management system they
are able to help with you managing your
pushes of social media content across
all of the popular social media
platforms they're able to provide the
analytics they're able to provide the
tools that marketing and sales teams can
use to be able to assess a sentiment
analysis of the messages that are being
pushed out really great tool and very
popular but part of their popularity
drove a specific problem straight to
HootSuite the chance they had at
HootSuite is that they had to constantly
go back and rebuild their server
environment and they couldn't do this
continuously and be consistent there was
no standard documentation and they had
to rely on your memory to be able to do
this consistently imagine how complex
this could get as you're scaling up with
a popular product that now has tens of
thousands to hundreds of thousands of
users this is where ansible came in and
really helped the folks over at
HootSuite today the devops team at
HootSuite write out playbooks that have
Specific Instructions that Define the
architecture and structure of their
Hardware nodes and environments and are
able to do that as a standard product
instead of it being a problem in scaling
up their environment they now are able
to rebuild and create new servers in a
matter of seconds the bottom line is
ansible has been able to provide
HootSuite with it automation consistent
configuration and free up time from the
operations team so that instead of
managing servers they're able to provide
additional new value to the company I've
very warm welcome to all our viewers I
am Anjali from Simply learn and today
I'll be taking you through a tutorial on
ansible so ansible is currently the most
trending and popular configuration
management tool and it's used mostly
under the devops approach so what will
you be learning today you learn why you
should use ansible what exactly is
ansible the ansible architecture how
ansible works the various benefits of
ansible and finally we'll have a demo on
the installation of Apache or the httpd
package on our applying systems we'll
also be hosting a very simple web page
and during this demo I'll also show you
how you can write a very simple playbook
in Yaman and your inventory file so
let's begin why should you use ansible
let's consider a scenario of an
organization where Sam is a system
administrator Sam is responsible for the
company's infrastructure a company's
infrastructure basically consists of all
its systems this could include your web
servers your database servers the
various repositories and so on so as a
system administrator Sam needs to ensure
that all the systems are running the
updated versions of the software now
when you consider a handful of systems
this seems like a pretty simple task Sam
can simply go from system to system and
perform the configurations required but
that is not the case with an
organization is it an organization has a
very large infrastructure it could have
hundreds and thousands of systems so
here is where Sam's work gets really
difficult not only does it get tougher
Sam has to move from system to system
performing the same task over and over
again this makes Sam bored not just that
repeating the same task leaves no space
for Innovation and without any ideas or
innovation how does the system grow and
the worst of it all is manual labor is
prone to errors so what does Sam do well
here is where and principle comes in use
with ansible Sam can write simple codes
that are deployed onto all the systems
and configure them to the correct States
so now that we know why we should use
ansible let's look at what exactly is
ansible ansible is an I.T engine that
automates the following tasks so first
we have orchestration orchestration
basically means bringing together of
multiple applications and ensuring an
order in which these are executed so for
example if you consider a web page that
you require to host this webpage stores
all its values that it takes from the
user into a database so the first thing
you must do is ensure that the system
has a database manager and only then do
you host your web page so this kind of
an order is very crucial to ensure that
things work right next ansible automate
configuration management so
configuration management simply means
that all the systems are maintained at a
consistent desired state are the tools
that automate configuration management
include puppet and Chef and finally
ansible automates deployment deployment
simply means the deploying of
application onto your servers of
different environments so if you have to
deploy an application on 10 systems with
different environments you don't have to
manually do this anymore because ansible
automates it for you in fact ansible can
also ensure that these applications all
the code are deployed at a certain time
or after regular intervals now that we
know what exactly ansible is let's look
at ansible's architecture ansible has
two main components you have the local
machine and you have your node or the
client machine so the local machine is
where the system administrator sits he
or she installs ansible here and on the
other end you have your node or the
client systems so in case of ansible
there's no supporting software installed
here these are just the systems that
required to be configured and they are
completely controlled by the local
machine at your local machine you also
have a module a module is a collection
of your configuration files and in case
of ansible these configuration files are
called playbooks playbooks are written
in yaml yaml stands for yaml Inc or
markup language and it is honestly the
easiest language to understand and learn
since it's so close to English we also
have the inventory the inventory is a
file where you have all your nodes that
require configuration mentioned and
based on the kind of configuration they
require they are also grouped together
so later in the demo we'll have a look
at how the Playbook and the inventory is
written and that will probably make it
clearer so of course a local machine
needs to communicate with the client and
how is this done this is done through
SSH ssh is your secure shell which
basically provides a protected
Communication in an unprotected
environment okay so we saw the various
components of ansible now how does
ansible exactly work you have your local
machine on one end this is where you
install ansible if you've gone through
any previous material on ansible you
would have come across the term
agentless often being associated with
this tool so this is what agentless
means you're installing ansible only on
your local machine and there's no
supporting software or Plugin being
installed on your clients this means
that you have no agent on the other end
the local machine has complete control
and hence the term agentless another
term that you would come across with
ansible is push configuration so since a
local machine has complete control here
it pushes The Playbook books onto the
notes and thus it's called a push
configuration tool now the playbooks and
the inventory are written at the local
machine and the local machine connects
with the nodes through the SSH client
this step here is optional but it's
always recommended to do so it's where
the facts are collected so facts are
basically the current state of the node
now all this is collected from the node
and sent to the local machine so when
the Playbook is executed the tasks
mentioned in the Playbook is compared
against the current status of the note
and only the changes that are required
to be made further are made and once the
playbooks are executed your notes are
configured to the desired States so as I
mentioned before ansible is currently
the most trending tool in the market
under the configuration management
umbrella so let's have a look at the
various benefits of ansible which gives
it this position well ansible is
agentless it's efficient it's flexible
simple in important and provides
automated reporting how does it do all
this let's have a look at that agentless
as I already mentioned before you
require no supporting software or Plugin
installed on your node or the client
system so the master has complete
control and automatically this means
that ansible is more efficient because
now we have more space in our client and
node systems for other resources and we
can get ansible up and running real
quick ansible is also flexible so an
infrastructure is prone to change very
often and ansible takes no amount of
time to adjust to these changes ansible
cannot get any simpler with your
playbooks written in a language such as
yaml which is as close to English as you
can possibly get
it important basically means that if you
have a Playbook which needs to be run n
number of systems it would have the same
effect on all of these systems without
any side effect and finally we have
automated reporting so in case of
ansible your playbook has a number of
tasks and all these tasks are named so
whenever you run or execute your
playbook it gives a report on which
tasks ran successfully which failed
which clients were not reachable and so
on all this information is very crucial
when you're dealing with a very large
infrastructure and finally we reach the
most exciting part of our tutorial the
Hands-On before we move on to the actual
Hands-On let me just brief you through
what exactly we'll be doing so I'll be
hosting two virtual boxes both Centos 7
Operating Systems one would be my local
machine and other my node or the client
machine so on my local machine first
I'll install ansible we then write the
inventory and the Playbook and then
simply deploy this Playbook on the
client machine there's just one thing
that we need to do is that we need to
check if the configurations that we
mentioned in our Playbook are made right
so we'll now begin our demo this is my
Oracle virtual box here I have my master
system which is the local machine and
this is the client machine so let's have
a look at these two machines this is my
client machine the terminal is open
right now so the client machine terminal
has a black background with white text
and the Master machine terminal has a
white background with black text just so
you can differentiate between the two so
we'll start at the Master machine the
first thing to do is we need to install
our ansible so yum install
ansible hyphen Y is the command to do so
so this might take some time
yeah so ansible's installed the next
step we go to our host file so host file
here is basically the inventory it's
where you'll specify all your nodes
in our case we just have one node
that's the path to your host file
as you'll see everything here is
commented so just type in the group for
your client notes
so I'm going to name it ansible clients
[Music]
and here we need to type the IP address
of our client machine so my client
machines IP address is
192.168.2.127 so before you come to this
it's advice that you check the IP
address on your client machine the
simple command for that is ifconfig now
once you type the IP address put a space
and here we need to mention the username
and the password for our client
so I'll be logging in as the root user
so this is the password
and then the user
which is Root in my case
that's it now you can save this file
just clear the screen
next we move on to our Playbook we need
to write the Playbook
so the extension for our Playbook is yml
which stands for yaml
and as you can see here I have already
written my playbook but I'll just
explain to you how this is done so a
yaml file always begins with three
dashes this indicates the start of your
yaml file
now the first thing is you need to give
a name to the entire Playbook
so I have named it sample book host is
basically where this would be executed
so as we saw earlier in our inventory I
mentioned client's group name as ansible
clients so we use the same name here
the remote user is the user you'll be
using at your client so in my case
that's root and become true is basically
to indicate that you need to set your
privileges at root so that's called the
privilege escalation now A playbook
consists of tasks
so we have here three tasks the first
task I've named it to install httpd so
what we are doing here is we are
installing our httpd package which is
basically the Apache server and we are
installing the most latest version of it
hence the state value is latest the next
task is running httpd so for the service
the name is httpd because that's the
service we need to start running and the
state is started our next task is
creating content so this is the part
where we are creating our web page
so copy because this is the file that
will be created at the client
the content will be welcome and the
destination of the file will be via
www.html index.html as you know this is
like a default path that we use to store
all our HTML files now as you can see
here there's quite a lot of indentation
and when it comes to yaml although it's
very simple to write and very easy to
read
the indentation is very crucial
so the first Dash here represents the
highest stage that is the name of the
Playbook and all the dashes under tasks
are slightly shifted towards the right
so if you have two dashes at the same
location they basically mean that
they're siblings so the priority would
be the same so to ensure that all your
tasks are coming under the tasks label
make sure they are not directly under
name so yeah that's pretty much it so
when you write your yaml file the
language is pretty simple very readable
indentations absolutely necessary make
sure all your spaces are correctly
placed we can now save this file
next thing we need to check if the
syntax of a yaml file is absolutely
right because that's very crucial
so the command to check the syntax of
the yaml file is ansible
Playbook
the name of your playbook
syntax check
so we have no syntax errors which is why
the only output you receive is sample
dot yml which is the name of your
playbook
so our Playbook is ready to be executed
the command to execute the Playbook is
ansible Playbook
and the name of your playbook
so a playbooks executed as you can see
here Gathering facts that's where all
the facts of the know that the present
state of the note is collected and sent
to the local machine so it's basically
to check that if the configuration
changes that we are about to make is
already made
so it's not made we do not have the
httpd package installed on our node so
this is the first change that's made
also if it's not installed of course
it's not running that's the second
change that's made so it's put into the
running state
and a final task which is create content
is under the okay State this means that
the content is already present in the
client machine so I made it this way so
that you can at least see the different
states that's present
so over here we have okay four so four
things are all fine the facts are
gathered two things are changed and one
is already present two changes are made
zero clients are unreachable and zero
tasks have failed so this is the
documentation that I was referring to
previously that ansible provides
automatically
and is very useful as you can see
so our next step we need to just check
on a client machine if all the changes
that we desired are made so let's move
to our client
so this is my client machine so to check
this since we are installing the httpd
package and hosting a web page the best
way to do it is open your browser
and type in localhost
so there you go your Apache server is
installed and your web page is hosted
today I'll be showing you the
installation procedure for the
configuration management tool puppet so
what exactly is the use of puppet if you
consider the scenario of an organization
which has a very large infrastructure
it's required that all the systems and
servers in this infrastructure is
continuously maintained at a desired
State this is where puppet comes in
puppet automates this entire procedure
thus reducing the manual work so before
we move on to the demo let me tell you
what the architecture of puppet looks
like so puppet has two main components
you have the puppet master and the
puppet client the Puppet Master is where
you write the configuration files and
store them and the puppet client are
basically those client machines which
require the configuration in case of
puppet these configuration files that
you write are called manifest so let's
move on to the demo so here are my two
machines the first is the server system
which is basically a master where you'll
write your account configuration files
and the others the node or the client
system so let's have a look at both of
these machines this is my node system
the terminals open here and the terminal
has a black background with white text
and as of my server or the Master
machine it has a black background with
green text
so we started a server machine the first
thing that we need to do is we need to
remove the firewall so in a lot of cases
there are chances that the firewall
stops the connection between your server
and your node now since I'm doing a demo
and I'm just showing you how puppet
Works between two virtual boxes I can
safely remove the firewall without any
worries but when you're implementing
puppet in an organization or a number of
systems on a local network be careful
about the consequences of doing so
so a firewall is disabled next thing
that we do is we'll change the host name
of our server system now while using the
puppet tool it's always advisable that
you name your servers host as puppet
this because the puppet tool identifies
the hostname puppet by default as the
host name for the master or the server
system
let's just check if the host name is
changed successfully yep so that's done
so as you see still localhost is
appearing as the host name so just close
your terminal and start it again
and you see here the host team has been
changed to puppet okay so the next thing
that we have to do is we install our
Puppet Labs make sure your system is
connected to the net
right so Puppet Labs is installed next
we need to install the puppet server
service on our server system
now that our puppet server Services
installed we need to move into the
system configurations for our puppet
server so the path for that is ETC
sysconfig puppet server
so this is a configuration file for the
puppet server now if you come down to
this line now this line here this is the
line which allocates memory for your
puppet server now you must remember that
puppet is a very resource extensive tool
so just in case to ensure that we do not
encounter any errors because of out of
memory we will reduce these sizes so as
of now we have 2GB allocated by default
we'll change to 512 MB
now in a lot of cases it may work
without doing so but just to be on the
safer side we make this change
save it
and go back to your terminal we are now
ready to start our puppet server service
the first time you start your puppet
server service it may take a while next
we need to enable this
and if your puppet server Services
started and enabled successfully this is
the output that you would get in case
you're still not sure you can always
check the status at any point of time
and as you see here it's active so
everything is fine as of now next thing
we do is we'll move on to our agent
system or a client or node system
so here too we'll have to install Puppet
Labs but before we do so we need to make
a small change in our host file so let's
open the host file
yeah so this is a host file we need to
add a single line here which specifies
our puppet master so first we put our
Puppet Masters IP address followed by
the hostname and then we'll add a DNS
for our puppet server so let's just go
back
to a server system and find out its IP
address
and that's my IP address for the server
system
now the host name of our puppet server
and a DNS for it
save this file
and return to your terminal
so now we can download our Puppet Labs
on the node system is the exact same
procedure that you followed for
downloading Puppet Labs on your server
system too
so in my node system the Puppet Labs is
already downloaded so the next thing is
we need to install a puppet agent
service
so puppet is a pull type of
configuration tool what this means is
that all your configuration files that
you'll be writing on your server is
pulled by the node system as and when it
requires it so this is the
co-functionality of the agent service
which is installed on your client node
or agent system so my puppet agent
service is installed so next I'll just
check if my puppet server is reachable
from this node system
so 8140 is a port number that the puppet
server must be listening on and it's
connected to puppet so that guarantees
that your server is reachable from the
node system
so now that everything's configured
right we can start our agent service
so guys you would have noticed that the
command for starting the agent service
is a little more complex in the command
for starting your server service this is
because when you start your agent
service you're not just starting a
service but you're also creating a
certificate this is the certificate that
will be sent over to your master system
now at the Master System there's
something called the certificate
Authority this gives the master the
rights to sign a certificate if it
agrees to share information with that
particular node so let's execute this
command which does both the function of
sending the certificate and starting
your agent service so as you can see
here our services started successfully
it's in a running State now we'll move
to our Master System or the server
system
so first we'll have a look at the
certificates that we received the
certificate should be in this location
so as you can see here this is the
certificate that we just received from
our agent service so this here within
codes is the name of our certificate so
next when we are signing the certificate
this is the name we'll provide to
specify that this is the particular
certificate that we want to sign so the
minute we sign a certificate the node
that send the certificate gets a
notification that the master has
accepted your request so after this we
can begin sharing our manifest files now
here's the command for signing this
certificate
okay so our certificate is signed which
means that the nodes request is approved
and the minute the certificate is signed
the request is removed from this list so
now if we execute the same command as we
did to check the list of all the
certificates we will not find this
certificate anymore let's just check
that so as you see now there are no more
requests pending because we have
accepted all the requests if you want to
have a look at all the certificates that
is signed or unsigned you can use the
same command with the addition of all
and all the certificates received so far
will be listed as you can see here the
plus sign indicates that the Certificate
request has already been accepted so now
that our certificate is signed the next
thing we do is we'll create a sample
manifest file
so this is the path that you create your
manifest files in
a file name is sample.pp and our files
created so right now we have no content
in this file we'll just check if the
agent is receiving it and once that's
confirmed We'll add some content to the
file so let's move to our agent system
now this is the command to execute at
the agent system to pull your
configuration files
so a catalog is applied in 0.02 seconds
so now that the communication between
our agent system and our Master system
is working perfectly fine let's add some
content to the previous placeholder file
that we created on our Master System
so now we open the same file in an
editor
okay so we are going to write a code for
installing the httpd package on our node
system which is basically a Apache
service node and then within codes
insert the hostname of your node system
so my note system Source name is client
the package you wish to install which in
our case is httpd
and the action to be performed
and that's it a very small and simple
code save this file
now let's go back to our node system
and let's pull this second version of
the same configuration file so every
time you execute this command as we did
previously to what happens is that the
agent service so the agent service
basically checks on your master system
if there's any new configuration file
added or if there's any change to the
previous configuration file made if so
then the catalog is applied once again
so now our catalogs applied in 1.55
seconds so now to check if our catalog
served its purpose let's just open a
browser
just type localhost here
and as you can see if your httpd package
has been successfully installed the
Apache testing page will appear here so
in this session what we can do is we're
going to cover what and why you would
use puppets what are the different
elements and components of puppet and
how does it actually work and then we'll
look into the companies that are
adopting puppet and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write a manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump onto
that server and you can fix it but what
if the scenario changes and you have
multiple servers going down so here is
where puppet shows its strap with
puppets all you have to do is write a
simple script they can be written with
Ruby and write out and deploy to the
servers your settings for each of those
servers the code gets pushed out now to
the servers that are having problems and
then you can choose to either roll back
to those servers to their previous
working States or set them to a new
state and do all of this in a matter of
seconds and it doesn't matter how large
your server environment is you can reach
to all of these servers your environment
is secure you're able to deploy your
software and you're able to do this all
through infrastructure as code which is
the advanced devops model for building
out Solutions so let's dig deeper into
what puppet actually is so puppet is a
configuration management tool maybe
similar tools like Chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable state public can
also be used as a deployment tool for
software or automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of Puppets so
here we have a breakdown of the puppet
environment and on the top we have the
main server environment and then below
that we have the client environment that
would be installed on each of the
servers that would be running within
your network so if we look at the top
part of the screen we have here our
puppet master store which has and
contains our main configuration files
and those are comprised of manifests
that are actual codes for configuring
their clients we have templates that
combine our codes together to render a
final document and you have files that
will be deployed as content that could
be potentially downloaded by the clients
wrapping this all together is a module
of Mana office templates and files you
would apply a certificate authority to
sign the actual documents so that the
clients actually know that they're
receiving the appropriate and authorized
modules outside of the master server
where you'd create your manifest
templates and files you would have
public clients as a piece of software
that is used to configure a specific
machine there are two parts to the
client one is the agent that constantly
interacts with the master server to
ensure that the certificates are being
updated appropriately and then you have
the fact that the current state of the
client that is used and communicated
back to through the agent so let's step
through the workings of puppet so the
puppet environment is a master sleeve
architecture the clients themselves are
distributed across your network and they
are constantly communicating back to a
Master server environment where you have
your puppet modules the client agent
sends a certificate with the ID of that
server back to the master and then the
master will then sign that certificate
and send it back to the client and this
authentication allows for a secure and
verifiable communication between client
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those catalogs are sent
back to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular intervals ensuring all client
systems are up to date so let's have a
look at companies that are using puppets
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using property today includes
Spotify Google ATT so why are these
companies choosing to use puppet as
their main configuration management tool
the answer can be seen if we look at a
specific company Staples so Staples
chose to take and use puppet for their
configuration management tool and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enabled them to open up time to allow
them to expound with other and new
projects and assignments a real tangible
benefit to a company so let's look at
how you write a manifest in puppets so
so manifests are designed for writing
out in code how you would configure a
specific node in your server environment
the manifests are compiled into catalogs
which are then executed on the client
each of the Manifest are written in the
language of Ruby without dot PP
extension if we step through the five
key steps for writing a manifest they
are one create your manifest and that is
written by the system administrator two
compile your manifest it is compiled
into a catalog three deploy the catalog
is then deployed onto the clients for
execute the catalogs are run on the
client by the agent and then five and
clients are configured to a specific and
desired state if we actually look into
how manifest is written it's written
with a very common syntax if you've done
any work with Ruby or really
configuration of systems in the past
this may look very familiar to you so we
break out the work that we have here you
start off with a package file or service
as your resource type and then you give
it a name and then you look at the
features that need to be set such as IP
address then you're actually looking to
have a command written such as present
or star the Manifest can contain
multiple resource types if we continue
to write our manifesting puppet the
default keyword applies a manifest to
all clients so an example would be to
create a file path that creates a folder
called sample in a main folder called
Etc the specified content is written
into a file that is then posted into
that folder and then we're going to say
we want to be able to trigger an Apache
service and then ensure that that Apache
service is installed on a node so we
write the Manifest and we deploy it to a
client machine on that client machine a
new folder will be created with a file
in that folder and an Apache server will
be installed you can do this to any
machine and you will have exactly the
same results on those machines we're
going to decide which is better for your
operations environment is it Chef puppet
and support or salt stack all four are
going to go head to head so let's go
through the scenario why you'd want to
use these tools so let's meet Tim he's
our system administrator and Tim is a
happy camper putting and working on all
of the systems and his network but what
happens if a system fails if there's a
fire a server goes down well Tim knows
exactly what to do he can fix that fire
really easily the problems become really
difficult for Tim however if multiple
servers start failing particularly when
you have large and expanding networks so
this is why Tim really needs to have a
configuration management tool and we
need to now decide what would be the
best tool for him because configuration
management tools can help make Tim look
like a superstar all he has to do is
configure the right codes that allows
him to push out the instructions on how
to set up each of the servers quickly
effectively and at scale all right let's
go through the tools and see which ones
we can use the tools that we're going to
go through are Chef puppet and support
and salt stacks and we have videos on
most of these software and services that
you can go and view to get an overview
or a deep dive in how those products
work so let's go and get to know our
contestants so our first contestant is
Chef and Chef is a tool that allows you
to configure very large environments it
allows you to scale very effectively
across the entire ecosystem and
infrastructure Chev is by default an
open source code and one of the things
that you find is a consistent metaphor
for the tools that we recommend on
simply learn is to use open source code
the code itself is actually written in
language of Ruby and erline and it's
really designed for heterogeneous
infrastructures that are looking for a
mature solution the way that Chef works
is that you write recipes that are
compiled into cookbooks and those
cookbooks are the definition of how you
would set up a node and a node is a
selection of servers that you have
configured in a specific way so for
instance you may have Apache Linux
servers running or you may have a MySQL
server running or you may have a python
server running and Chef is able to
communicate back and forth between the
nodes to understand what nodes are being
impacted and we need to have
instructions sent out to them to correct
that impact you can also send
instructions from the server to the
nodes to make a significant update or a
minor update so there's great
communication going back and forth if we
look at the pros and cons the pros for
Chef is that there is a significant
following for chef and that has resulted
in a very large collection of recipes
that allow you to be able to quickly
stand up environments there's no need
for you to have to learn complex recipes
the first thing you should do is go out
and find the recipes that are available
it integrates with Git really well and
provides for a really good strong
Version Control some of the conso are
really around the learning speed it
takes to go from beginner user with Chef
to being an expert there is a
considerable amount of learning that has
to take place and it's compounded by
having to learn Ruby as the programming
language and the main server itself
doesn't really have a whole lot of
control it's really dependent on the
communication throughout the whole
network all right let's look at our
second Contender puppet and puppet is
actually in many ways very similar to
Chef there are some differences but
again puppet is designed to be able to
support very large heterogeneous
organization it is also built with Ruby
and uses DSL for writing manifests so
there are some strong similarities here
to Chef as with a chef there is a Master
Slave infrastructure with puppet and you
have a master server that has the
manifests that you put together in the a
single catalog and those catalogs are
then pushed out to the clients over an
SSL connection some of the pros with
public is that as with Chef there is a
really strong Community around puppies
and there's just a great amount of
information and support that you can get
right out of the gate it is a very well
developed reporting mechanism that makes
it easier for you as an administrator to
be able to understand your
infrastructure one of the cons is that
you have to really be good at learning
Ruby again as with shav you know the
more advanced tasks really need to have
those Ruby skills and as with Chef the
server also doesn't have much control so
let's look at our third Contender here
ansible and so ansible is slightly
different it is the way the ansible
works is that it actually just pushes
out the instructions to the server
environment there isn't a client server
or Master Slave environment where
ansible would be communicating backwards
and forwards with its infrastructure it
is merely going to push that
instructions out the good news is that
the instructions are written in yaml and
yowel stands for yaml eight markup
language yaml is actually pretty easy to
learn if you know XML and XML is pretty
easy if you know XML you're going to get
yamo up really well ansible does work
very well on environments where the
focuses are getting servers up and
running really fast it's very very
responsive and can allow you to move
quickly to get your infrastructure up
quick very fast and we're talking
seconds and minutes here really really
quick so again the way that ansible
works is that you put together a
Playbook and an inventory or you have a
Playbook so the way that ansible works
is that you have a playbook in the
Playbook it then goes against the
inventory of servers and will push out
the instructions for that Playbook to
those servers so some of the pros that
we have for ansible we don't need to
have an agent install on the remote
notes and servers it makes it easier for
the configuration yaml is really easy to
learn you can get up to speed and get
very proficient with yaml quickly the
actual performance once you actually
have your infrastructure up and running
is less than other tools that we have on
our list now I do have to add a Proviso
this is a relative less it's still very
fast it's going to be a lot faster than
individuals manually standing up servers
but it's just not as fast as some of the
other tools that we have on this list
and yaml itself as a language while it's
easy to learn it's not as powerful as
Ruby Ruby will allow you to do things
that at an advanced level that you can't
do easily with the ammo so let's look at
our final Contender here soot stack so
salt stack is a CLI based tool it means
that you will have to get your command
line tools out or your terminal window
out so you can actually manage the
entire environment environment via salt
sack the instructions themselves are
based on python but you can actually
write them in yaml or DSL which is
really convenient and as a product it's
really designed for environments that
want to scale quickly and be very
resilient the way that sort stat works
is that you have a master environment
that pushes out the instructions to what
they call grains which is your network
and so let's step through some of the
pros and cons that we have here with
salt stag so salt stack is very easy to
use once it's up and running it has a
really good reporting mechanism that
makes your job as an operator in your
devops environment much much easier the
actual setup though is a little bit
tougher than some of the other tools and
and it's getting easier with the newer
releases but it's just a little bit
tougher and related to that is that salt
stack is fairly late in the game when it
comes to actually having a graphical
user interface for being able to create
and manage your environment other tools
such as ansible have actually had a UI
environment for quite some time all
right so we've gone through all four
tools let's see how they all stack up
next to each other so let the race begin
let's start with the first stage
architecture so the architecture for
most of our environments is a server
client environment so for Chef puppet
and salt snack so very similar
architecture there the one exception is
ansible which is a client Only Solution
so you're pushing out the instructions
from a server and pushing them out into
your network and there isn't a client
environment there isn't a two-way
communication back to that main client
for what's actually happening in your
network so let's talk about the next
stage ease of setup so we look at the
four tools there is one tool that really
stands out for ease of setup and that is
ansible it is going to be the easiest
tool for you to set up and if you're new
to having the these types of tools in
your environment you may want to start
with ansible just to try out and see how
easy it is to create automated
configuration before looking at other
tools now and so with that said Chef
puppet and Source deck aren't that hard
to set up either and you'll find there's
actually some great instructions on how
to do that setup in the online community
let's talk about the languages that you
can use in your configuration so we have
two different types of language with
both chef and ansible being procedural
in that they actually specify at how
you're actually supposed to do the task
in your instructions with puppet and
saltstack it's decorative where you
specify only what to do in the
instructions let's talk about
scalability which tools scale the most
effectively and as you can imagine all
of these tools are designed for
scalability that is the driver for these
kind of tools you want them to be able
to scale to massive organ organizations
what did the management tools look like
for our four contenders so again we have
a two-way split with ansible and salt
stack the management tools are really
easy to use you're going to love using
them they're just fantastic to use with
puppet and Chef their management tools
are much harder to learn and they do
require that you learn some either the
puppet DSL or the Ruby DSL to be able to
be a true master in that environment but
what does interoperability look like
again as you'd imagine with the similar
to scalability interoperability with
these products is very high in all four
cases now let's talk about Cloud
availability this is increasingly
becoming more important for
organizations as they move rapidly onto
cloud services well both ansible and
salt stack have a big fail here neither
of them are available in the most
popular Cloud environments and puppet
and Chef are actually available in both
Amazon and Azure now we've actually just
haven't had a chance to update our Chef
link here but Chef is now available on
Azure as well as Amazon so what does
communication look like with all of our
four tools so the communication is
slightly different with them Chef has
its own knife tool and whereas puppet
uses SSL to secure sockets layer and
ansible and salt stack use secure socket
hashing SSH as their communication tool
bottom line all four tools are very
secure in their communication so who
wins well here's the reality all four
tools are very good and it's really
dependent on your capabilities and the
type of environment that you're looking
to manage that will determine which of
these four tools you should use the
tools themselves are open source so go
out and experiment with them there's a
lot of videos our team has done other
ton of videos on these tools and so feel
free to find out other tools that we
have then covered so you can learn very
quickly how to use them but consider the
requirements that you have and consider
the capabilities of your team if you
have Ruby developers or you have someone
on your team that knows Ruby your
ability to choose a broader set of tools
becomes much more interesting if however
you're new to coding then you may want
to consider yaml based tools again the
final answer is going to be up to you
and we'll be really interested on what
your decision is
monitoring as the term says you're
monitoring you're watching your
logging your production environment so
of course there are a whole bunch of
management tools they become an
important part of your production
environment and a lot of these
monitoring tools are also I've seen that
also being used especially in your UAP
environment and you can optionally have
them for some time even in your you know
development environment no not not
development business development
services are usually not very uh
high-end configurations but you know
maybe a decent development slash
integration
server especially if you have a long
running scripts and if you have programs
that use a lot of servers uh you know
maybe CPU or processing power so then
you can have monitoring tools when
you're writing such scripts and you know
uh you are doing the UA unit testing for
those scripts so that you know to see uh
what kind of server utilization
happens when you run this script you
know if you'll put this in production
will it actually you know slow down your
production server and what kind of uh
impact that will have on the you know
your rest of your application or other
applications running on that server
like the this particular chapter is more
in context with production environment
so these uh tools they basically monitor
your server they monitor your searches
of course they want to your applications
and any services that you have deployed
on your servers and they generate alerts
when something goes wrong and that's the
whole job of monitoring it is
continuously watching continuously
looking at what is running what is
happening what is going up what is going
down when it's uh CPU peaking when it's
memory peaking and all that so that you
can you typically send limits for these
uh all these different parameters and
anytime any of these parameters goes
outside of that limit you know even more
than that or less than that these
monitoring tools usually send out an
alert and these alerts could again be
SMS alerts or email alerts and there are
there are usually people monitoring
these monitoring to tools to look look
out for any
issues reported
and they also generate alerts when the
problem has been resolved so it was
worth it
scenarios is an open source monitoring
tool and it can
even monitor your network services
it's a little diagram here which is
too small
but here is somewhere what I can read
and stated
these are different devices
[Applause]
I think no no no yeah these are
different devices to which nagios is
sending the status there's a browser
there's an SMS there's an email and then
there's a graph also and these are
different objects that Nigeria's is
basically monitoring this is an SMTP so
I can read SMTP
I don't know PCP IP no I don't know some
database server okay this is a database
server and this is an application server
then you switch router okay okay I can
read that now so this is the different
kind of object these are different kind
of servers that naziers monitors and
these are the different kind of uh
devices or statuses that it can send
so it helps monitor your CPU usage your
dispute page and you know even your
system logs and it uses the plugin
script that can be written you know in
any scripting language actually
do not use remote plugin executors are
basically agents that allow remote
scripts to be executed as well and these
scripts are usually executed to monitors
again your CPU just to say number of
users logged in who is logged in who's
logged in what time logged out at what
time and all these happening so all
these monitoring tools work on the
concept of polling so
polling is more like you know they
so the NRP agent is a program that will
continuously keep polling a machine for
certain parameters that are configured
energies to be monitored so this program
continues to keep pinging the server
bringing the program uh you know to keep
checking for what it has been asked to
check so in case of logged in users you
keep checking uh at a you know like
maybe every 30 seconds or every one
minute you keep uh pinging uh to see how
many users have logged in onto this
server and who are the users who have
logged in what time they logged in what
time they logged out and things like
that
Asian program that can you know help you
uh
all or paint even remote machine
the nadios remote data processor is an
agent that allows the you know flexible
data transports and you know it uses the
HTTP and XML protocols to do that
and we're talking about uh essentially
your databases and
data server usage is like you know so
then if you have an Oracle database how
many database instances are there you
know how
your load balancing is set up on that
how data is moving between different uh
database servers within Oracle and how
data is moving within the load balancer
and there's always a DRP there's always
a backup with database so that's why
preemie mentioned trp as soon as I say
the word database and if there's a
backup plan you know uh how how's the
data moving how much time did the backup
take did it take too much time and why
you know why did it take should help you
do all those kind of monitoring
the NS client is basically mainly used
to monitor Windows machines and
typically when we talk about servers the
we end up talking more about you know
Unix on Linux servers of course now with
a lot of Microsoft Technologies being
robust than they were you know uh like
SharePoint or
things like that there are windows
servers too but 10 years ago if you
would talk about having the Windows
Server it was actually kind of round the
pound especially for production
and again you know this has a few
monitors usually your CPU your this
usage and it pulls the plugin and this
particular
agent listens to this particular Port
always so that's what is the port
and usually your system administrators
or server administrators know all these
things today let's get started with
Jenkins Jenkins in my opinion is one of
the most popular continuous integration
servers of recent times what began as a
hobby project by a developer working for
Sun Microsystems way back in early or
mid-2000s has gradually and eventually
evolved into a very very powerful and
robust automation servers it has a wider
option since it is released under MIT
license and is almost free to use
Jenkins has a vast developer community
that supports it by writing all kinds of
plugins plugins is the heart and soul of
Jenkins because using plugins one can
connect Jenkins to anything and
everything under the sun with that
introduction let's get into what all
will be covered as a part of this
tutorial I will get into some of the
prerequisites required for installing
Jenkins post which I will go ahead and
install Jenkins on a Windows box there
are few first-time configurations that
needs to be be done and I will be
covering those as well so once I have
Jenkins installed and configured
properly I will get into the user
administrative part I create few users
and I will use some plugins for setting
up various kinds of access permissions
for these users I will also put in some
freestyle jobs freestyle job is nothing
but a very very simple job and I will
also show you the powerfulness of
Jenkins by scheduling this particular
job to run based upon time schedule I
will also connect Jenkins with GitHub
GitHub is our source code where source
code repository where I've got some
repositories put up there so using
Jenkins I will connect to GitHub pull up
a repository that is existing on GitHub
onto the Jenkins box and run few
commands to build this particular
repository that is pulled from GitHub
sending out emails is a very very
important configurations of Jenkins or
any other continuous integration server
for that matter Whenever there is any
notifications that has to be sent out as
a part of either a build going bad or
building code or building propagated to
some environment and all these scenarios
you would need the continuous
integration servers to be sending out
notifications so I will get into a
little bit details of how to configure
Jenkins for sending out emails I will
also get into a scenario where I would
have a web application a maven-based
Java web application which will be
pulled from a GitHub repository and I
will deploy it onto a tomcat server the
Tomcat server will be locally running on
my system eventually I will get into one
other very very important topic which is
the Master Slave configuration it's a
very very important and pretty
interesting topic where distributed
builds is achieved using a Master Slave
configuration so I will bring up a slave
I will connect the slave with the master
and I'll also put in a job and kind of
delegate that particular job to the
slave configuration finally I will let
you know how to use some plugins to
backup your Jenkins so Jenkins has got a
lot of useful information set up on it
in terms of the build environments in
terms of workspace all this can be very
very easy easily backed up using a
plugin so this is what I'm going to be
covering as a part of this tutorial
Jenkins is a web application that is
written in Java and there are various
ways in which you can use and install
Jenkins I have listed popular three
mechanisms in which Jenkins is usually
installed on in any system the topmost
one is as a Windows or a Linux Based
Services so if at all you have Windows
like the way I have and I'm going to use
this mechanism for this demo so I would
download a MSI installer that is
specific to Jenkins and install this
service so whenever I installed as a
service it goes ahead and nicely
installs all that is required for my
Jenkins and I have a service that can be
started as stopped based upon my need
any flavor of Linux as well one other
way of running Jenkins is downloading
this generic War file and as long as you
have jdk installed you can launch this
war file by the command opening up a
command prompt or shell prompt
difficulty on Linux box specifying Java
hyphen jar and the name of this war file
it typically brings up your web
application and you know you can
continue with your installation the only
everything being if at all you want to
stop using Jenkins you just go ahead and
close this prompt you either do a Ctrl C
and then bring down this prompt and your
Jenkins server would be down other older
versions of Jenkins were run popularly
using this way in which you already have
a Java based web server running up and
running so you kind of drop in this war
file into the root folder or the httpd
root folder of your web server so
Jenkins would explode and kind of bring
up your application all user credentials
or user Administration is all taken care
of by the Apache or the Tomcat server or
the web server on which Jenkins is
running this was a very older way of
running but still some people use it
because if they don't want to maintain
two servers if they already have a Java
web server which it's been nicely
maintained and backed up Jenkins can run
attached to it all right so either ways
it doesn't matter however you're going
to bring up your Jenkins instance the
way we're going to operate Jenkins is
all going to be very very same or
similar one with the subtle changes in
terms of user Administration if at all
you're launching it through any other
web server which will take care of the
user Administration otherwise all the
commands are all the configuration or
the way in which I'm going to run this
demo it is going to be same across any
of these installations all right so the
prerequisites for running Jenkins as I
mentioned earlier Jenkins is nothing but
a simple web application that is written
in Java so all that it needs is Java
preferably jdk 1.7 or 1.8 2GB Ram is the
recommended RAM for running Jenkins and
also like any other open source toolsets
when you install jdk ensure that you set
in the environment variable Java home to
point to the right directory this is
something very specific to jdk but for
any other open source tools that you've
installed there is always a preferred
environment variable that you got to set
in which is specific to that particular
tool that you are going to use this is a
generic thing that is there for you know
for any other open source projects
because the way open source projects
discover themselves is using this
environment variables so as a general
practice or a good practice always set
this environment variables according so
I already have jdk 1.8 installed on my
system but in case you do not what I
would recommend is just navigate on your
browser to the Oracle home page and just
type in or search for install jdk 1.8
and navigate to The Oracle home page
you'll have to accept the license
agreement and there are a bunch of
installers that is okay that you can
pick up based upon the operating system
on which you're running so I have this
windows 64 installer that is already
installed and running on my system so
I'm not going to do the details of
downloading this or installing it let me
show you once I install this what I've
done with regard to my path
so if you get into environment variables
all right so I have just set in a Java
home variable
if you say this C colon program files
Java jdk1.8 this is where my my Java is
located C program files C program files
Java
okay so this is the home directory of my
jdk so that is what I've been I've set
it up here in my environment variable
so if you see here this is my Java home
alright one other thing to do is ensure
that in case you want to run Java or
Java C from a command prompt and show
that you also add that path into this
path variable so if you see this
somewhere I will see yes there you go
see colon program files Java jdk 1.8 pin
so with these two I'll ensure that my
Java installation is nice and you know
good enough so to check that to double
check that or to verify that let me just
open up a simple command prompt
and if I type in Java iPhone version
all right and Java C
yphen version
so the compiler is on the path Java is
on the path and if at all I do this
even the environment variable specific
to my Java is installed correctly so I
am going to go ahead with my Jenkins
installation
now that I have my prerequisites
all set for installing Jenkins let me
just go ahead and download Jenkins
so let me open up a browser and say
download
Jenkins
all right LTS is nothing but the long
term support these are all stable
versions weeklies I would not recommend
that you try these unless until you have
a real need for that long term support
is good enough and as I mentioned there
are so many flavors of Jenkins that is
available for download
you also have a Docker container wherein
you know you can launch Jenkins as a
container but I'll not get into details
of that in this tutorial
all right so what I want is yes this is
the war file which is generic War file
that I was talking to you earlier and
this is the windows MSI installer so go
ahead and download this MSI installer I
already have that downloaded so let me
just open that up
all right so this is my downloaded
Jenkins instance or other installer this
is a pretty maybe a few months old but
this is good enough for me
before you start Jenkins installation
just be aware of one fact that there is
a variable called Jenkins home this is
where Jenkins would store all this
configuration data jobs deposit
workspace and all that specific to
Jenkins so by default if at all you
don't set this to any particular
directory if at all you install an MSI
installer all your installation gets
into SQL and program files 86 and
Jenkins folder if at all you run a war
file depending upon the user ID with
which you're running your War file the
Jenkins folder there's a DOT Jenkins
folder that gets created inside the user
home directory so in case you have any
need wherein you want to backup your
Jenkins or you want Jenkins installation
so get into some specific directories go
ahead and set this Jenkins home variable
accordingly before even begin your
installation for now I don't need to do
any of this thing so I've already
downloaded the installer let me just go
ahead with the default installation all
right so this is my Jenkins MSI
installer I would just I don't want to
make any changes into the Jenkins
configuration C colon program files is
good for me yeah this is where all my
destination folder and all the
configuration specific to it goes I'm
happy with this I don't want to change
this I would just say go ahead and click
installation
foreign
so what typically happens once the
Jenkins installation gets through is it
will start installing itself and there
are some small checks that needs to be
done
so and by default Jenkins Launches on
the port 8080 so let me just open up
localhost 8080
there's a small checking that will be
done as a part of the installation
process wherein I need to type in the
hash key all right so there's a very
very simple hash key that gets stored
out here so I'll have to just copy this
path
if at all you're running as a war file
you would see that in your logs
all right so this is a simple hash key
that gets created every time when you do
a Jenkins installation so as a part of
the installation it just asks you to do
this
so if that is not correct it will crib
about it but this looks good so it's
going ahead
all right one important part during the
installation so you would need to
install some recommended plugins what
happens is the plugins are all related
to each other so it's like the typical
RPM kind of a problem where you try to
install some plugin and it's got a
dependency which is not installed and
you get into all those issues in order
to get rid of that what Jenkins
recommends is a bunch of plugins that is
already recommended so just go ahead and
blindly click that install recommended
plugin so if you see there is a whole
lot of plugins which are bare essential
plugin that is required for Jenkins in
order to run properly so Jenkins as a
part of the installation would get all
these plugins and then install it for
you this is a good combination to kind
of begin with and mind you at this
moment Jenkins needs lots of bandwidth
in terms of network so in case your you
know your network is not so good few of
these plugins would kind of fail and
these plugins are all you know all
available on openly or or mirrored sites
and sometimes some of them may be down
so do not worry in case some of these
plugins kind of fail to install you'll
get an option to kind of retry
installing them but just ensure that you
know at least most or 1995 of all these
plugins are installed without any
problems let me pause the video here for
a minute and then get back once all
these plugins are installed my plugin
installation is all good there was no
failures in any of my plugins so after
that I get to create this first admin
user again this is one important point
that you got to remember
given any username and password but
ensure that you kind of remember that
because it's very hard to get back your
username and password in case you forget
it alright so I'm going to create a very
very simple username and password
something that I can remember
I will
that's my name and an email ID is kind
of optional but it doesn't allow me to
go ahead in case I don't so I just given
an admin and I get a password I've got I
remember my password this is my full
name all right I say seven finish
all right that kind of completed my
Jenkins installation
it was not that tough was it
now that I have my Jenkins installed
correctly let me quickly walk you
through some bare minimal configurations
that is required these are kind of a
first time configuration that is
required so and also let me warn you the
UI is little hard for many people to
wrap their head around it specifically
the windows guys but if at all you're a
Java guy you know how painful it is to
write UI in Java you would kind of
appreciate you know all the effort that
has gone into the UI bottom line UI is
little hard to you know wrap your head
around it but once you start using it
possibly you'll start liking it all
right so let me get into something
called as manage Jenkins
this can be viewed like a main menu for
all Jenkins configuration so I'll get
into some of those important ones
something called as configure system
configure system this is where you kind
of put in the configuration for your
complete Jenkins instance few things to
kind of look out for this is a home
directory this is a Java home where all
the configurations all the workspace
anything and everything regarding
Jenkins is stored out here
system message you want to put in some
message on the system you just type in
whatever you want and it's possibly show
up somewhere up here on the menu number
of executors very very important
configuration this just lets Jenkin know
at any point in time how many jobs or
how many threads can be run you can you
can kind of visualize it like a thread
that can be run on this particular
instance as a thumb rule if at all
you're on a single core system number of
executors two should be good enough in
case at any point in time if there are
multiple jobs that kind of get triggered
the same time in case the number of
executors are less compared to the
number of jobs that have that woken up
no need to panic because they will all
get queued up and eventually Jenkins
will get to running those jobs just bear
in mind that whenever a new job kind of
you know gets triggered the CPU usage
and the memory usage in terms of the
disk right is very high on the Jenkins
instance
so that's something that you got to kind
of keep in mind all right but number of
executors two for my system is kind of
good label for my Jenkins I don't want
any of these things usage how do you
want to use your Jenkins this is good
for me because I only have a primary uh
server that is running so I want to use
this node as much as possible quite
better each of these options I've got
some bare minimal help kind of a thing
that is that is out here by clicking on
these question marks you will get to
know as to what are these particular
configurations
all right so this all look good what I
want to show you here is there's
something regarding the docker
timestamps git plugin SVN email
notifications I don't want that what I
want the yes I want this SMTP server
configuration remember I mentioned
earlier that I would Bond Jenkins to be
sending out some emails and what I've
done here is I've just configured the
SMTP details of my personal email ID
in case you are in a in an organization
you would have some sort of an email IDs
that is set up for a Jenkins server so
you can specify the SMTP server details
of your company so that you know you can
authorize Jenkins to kind of send out
emails but in case you want to try it
out like me I have configured my
personal email ID which is on my Gmail
for sending out notifications so the
SMTP server would be
smtp.gmail.com I'm using the SMTP
authentication I have provided my email
ID and my password I'm using the SMTP
Port which is 465 and I'm you know reply
to address is the same as mine I can
just send out an email and see if at all
this configuration works again Gmail
would not allow you to allow anybody to
send out notifications on your behalf so
you'll have to lower the security level
of your Gmail ID so that you can allow a
programmatically somebody to send out
email notifications on your behalf so
I've done already that I'm just trying
to see if I can send a test you email
with the configuration that I've set in
yes
all right so the email configuration
looks good
so this is how you configure your you
know your Gmail account in case you want
to do that if not put in your
organization SMTP server details which
are with a valid username and password
and should all be set
all right so no other configurations
that I'm going to change here all of
these look good
all right so I come back to manage
inkins okay one other thing that I want
to kind of go over is the global tool
configuration
imagine this scenario or look at it this
way Jenkins is a is a continuous
integration server it doesn't know what
kind of a code base it's going to pull
in what kind of a tool set that is
required or what is the code that is
going to pull in and how is it going to
build so you would have to put in all
the tools that is required for building
the appropriate kind of code that you're
going to pull in from you know your
source code repositories so just to give
an example in case your source code is a
Java source code and assuming that you
know because in this a demo this is my
laptop and I've put in all the
configurations jdk everything on my
laptop because I'm a developer I am
working on the laptop but my continuous
integration server would be you know a
separate server without anything being
installed on it so in case I want
Jenkins to you know run a Java code I
would need to install jdk on it I need
to specify the jdk location of this out
here this way
okay since I already have the jdk
installed and I've already put in the
Java home directory or rather the
environment variable correctly I don't
need to do it git if at all I want the
Jenkins server to use git a git is a you
know command Dash or the command prompt
for for running git and connecting to
any other git server so you would need
git to be you know installed on that
particular system and set the path
accordingly
Gradle and Maven if at all you have some
mavens as well you want to do this any
other tool that you're going to install
on your system which is your continuous
integration server you will have to come
in here and configure something
in case you don't configure it when
Jenkins runs it will not be able to find
these tools for building your task and
it'll crib about it
that's good I don't want to save
anything
managing games let me see what else is
required yes configure Global Security
all right the security is enabled and if
you see by default it's the security
access control is set to Jenkins own
user database so what does this mean you
know Jenkins by default it uses file
system where it stores all the usernames
with hashes up these usernames and kind
of stores them
so as of now it Jenkins is configured to
use its own database assuming that you
are running in an organization you would
probably want to have a you know some
sort of an ad or an ldap server using
which you would want to control access
to your Jenkins repository rather
Jenkins tool so you would specify your
ldap server details the root DN password
or the manager DN and the manager
password and all these details in case
you want to connect your Jenkins
instance with your ldap or ad or any of
the authentication servers that you have
in your organization but for now since I
don't have any of these things I'm going
to use this own database that's good
enough
all right so I will set up some
authorization methods and stuff like
that once I put in few jobs so for now
let me not get into any of these details
of this just be aware that Jenkins can
be connected for authorization to an
ldap server or you can have Jenkins
managing its own servers which is
happening as of now so I'm going to save
all this stuff that's good for me so
enough of all these configurations let
me put in a very very simple job
all right so job new item you know
little difficult to kind of figure out
but then that's the new item so I will
just say you know first job this is good
for me I just give a name for my job I
would say it's a freestyle project
that's good enough for me I don't want
to choose any of that so unless until
you choose any of this this particular
button would not become active so choose
the freestyle project and say okay
at a very high level you will see
General source code management build
triggers build environment build and
post build in case you install more and
more plugins you will see a lot more
options but for now this is what you
would see so what am I doing at the
moment I'm just putting up a very very
simple job and the job could be anything
and everything so I don't want to put in
a very complicated job for now for the
demo purpose let me just put in a very
very simple job I'll give a description
this is an optional thing this is my
first Jenkins job all right I don't want
to choose any of these again there are
some helps available here
I don't want to choose any of this I
don't want to connect it into any source
code for now I don't want any triggers
for now I'll come back to this in in a
while build the environment I don't want
any build environment as a part of this
build step you know I just want to you
know run few things so that I kind of
complete this particular job so since
I'm on a Windows box I would say execute
Windows batch command
all right so what do you want to do I
will let me just Echo something Echo
uh hello this is my first Jenkins job
and possibly I would want the date and
the timestamp
pertaining to the job I mean the date
and time in which this job was run all
right very very simple command that says
you know this is my first job it just
puts out something along with the date
and the time
all right I don't want to do anything
else I want to keep this job as simple
as this so let me save this job
all right so once I save this job you
know the job names comes up here and
then I need to build this job and you
would see some build history out here
nothing is there as of now because I've
just put in a job I have not run it yet
all right so let me try to build it now
you see a build number you would see a
date and a timestamp so if I click on
this you will see a console output if I
go here
okay as simple as that and where is all
the job details that is getting into if
you see this
if I navigate to this particular
directory
all right so this is the directory what
I was mentioning earlier regarding
Jenkins home so all the job related
stuff that is specific to this
particular Jenkins installation is all
here
all the plugins that is installed the
details of each of those plugins can be
found here
all right so the workspace is where all
the jobs that I've created whichever I'm
running would you know there will be at
individual folders specific to the jobs
that has been put up here all right so
one job one quick run
that's what it looks like pretty simple
okay let me do one thing let me put up a
second job
I would say second job
I would say freestyle project
all right this is my second job
I just want to demonstrate the
powerfulness of the automation server
and how simple it is to automate a job
that is put up on Jenkins
which will be triggered automatically
remember what I said earlier about
Jenkins because at the core of Jenkins
is a very very powerful automation
server all right so what I'm going to do
I will just keep everything else the
same I'm going to put in the build
script pretty much similar to
second job
that gets triggered
automatically
every minute all right let me do that
percentage date and now put in the time
all right so I just put in another job
called second job and it pretty much
does the same thing as what I was doing
earlier in terms of printing the date
and the time but this time I'm just
going to demonstrate the powerfulness of
the automation server that is there if
you see here
there's a build trigger so a build can
be triggered using various triggers that
is there so we'll get into this GitHub
uh triggering uh hook or a web hook kind
of a triggering later on but for now
what I want to do I want to ensure that
this job that I'm going to put in would
be automatically triggered on its own
let's say every minute I want this job
to be run on its own
so build periodically is my setting
if you see here there's a bunch of
help that is available for me so for
those of you you have written cron jobs
on Linux boxes you'll find it very very
simple but for others don't panic let me
just put up a very very simple regular
expression for scheduling this job every
minute
all right so that's one
two three four five
right come up come up come up all right
so five stars is all that I'm gonna put
in and Jenkins got a little worried and
she's asking me do you really mean every
minute oh yeah I want to do this every
minute let me save this and how do I
check whether it gets triggered every
minute or not I just don't do anything
I'll just wait for a minute and if at
all everything goes well Jenkins would
automatically trigger my second job in a
minute's time from now
this time around I'm not going to
trigger anything look there you see it's
automatically got triggered
if I go in here
yep second job that gets triggered
automatically you know it was triggered
at 42 1642 which is 442 my time that
looks good
and if everything goes well every one
minute onwards this job would be
automatically triggered
now that I have my Jenkins up and
running a few jobs that has been put up
here on my Jenkins instance I would need
a way of controlling access to my
Jenkins server this is wherein I would
use a plugin called role based access
plugin and create few rules the rules
are something like a global Rule and a
project role Project Specific role I can
have different rules and I can have
users who have signed up or the users
who might create kind of assigned to
these roles so that each of these users
fall into some category this is my way
of kind of controlling access to my
Jenkins instance and ensuring that
people don't do something unwarranted
alright so first things first let me go
ahead and install a plugin for doing
that so I get into manage Jenkins and
manage plugin
a little bit of a confusing screen in my
opinion there's updates available
installed and advanced as of now we
don't have the role based plugin so let
me go to available it'll take some time
for it to get refreshed all right now
these are the available plugins these
are the install plugins all right so let
me come back to available and I would
want to search for my role based access
plugin so I would say search for role
and hit enter
okay role-based authorization strategy
enables user authorization using a
role-based strategy roles can be defined
globally or for particular jobs or notes
and stuff like that so exactly this is
the plugin that I want I would want to
install it without a restart
all right looks good so far yes
go back to the top of the page yes
remember Jenkins is running on a Java
using a Java instance so typically many
things would work the same way
understand until you want to restart
Jenkins once in a while but as a good
practice whenever you do some sort of a
big installations or big patches on your
Jenkins instance just ensure that you
kind of restarted otherwise there would
be a difference in terms of what is
installed on the system and what is
there on the file system you will need
to flush out few of those settings later
on but for now these are all very small
plugins so this would run without any
problems but otherwise if at all there
are some plugins which would need a
restart you know kindly go ahead and
restart your Jenkins instance but for
now I don't need that it looks good I've
installed the plugin so where do you see
my plugin I install the plugin that is
specific to the user control or the
access control so let me go into yes
Global Security
and I would see this role-based strategy
showing up now all right so this comes
in because of my installation of my role
based plugin so this is what I would
want to enable
because I already have my own database
set up and for the authorization part in
the sense that who can do what I'm going
to install I mean I've already installed
a role-based strategy plugin and I'm
going to enable that strategy all right
I would say save okay now I've installed
the role based access plugin I would
need to just set it up and check that
you know I would go ahead and create
some roles and ensure that I assign
users as per these roles all right so
let me go to manage Jenkins
configure all right let me see where is
this configure configure Global Security
is that where I create my rules
nope not here
yes manage and assign roles
okay again you would see these options
only after you install these plugins so
for now I have just enabled the plugin I
have enabled role based access control
and I would go ahead and create some
rules for this particular Jenkins
instance so I would say first manage
rules
so I would need to create some roles
here and the rules are at a very high
level these are Global rules and there
are some project rules and there are
some slave rules I'll not get into
details of all of this at a very very
high level which is a global role let me
just create a row a role can be kind of
visualized like a group so I would
create a role called developer typically
the Jenkins instance or the ca instance
are kind of owned up or controlled by QA
guys so QA guys would need to provide
some sort of a you know limited access
to developers so that's why I'm creating
a role called developer and I'm adding
this role at a global role level so I
would say add this here and you would
see this developer role that is there
and each of these options if we hover
over it you would see some sort of a
help on what what are these uh you know
permissions specific to so what I want
is like you know it sounds a little you
know different but I would want to give
very very little permissions for the
developer so from an Administration
perspective I would just want him to
have a read
um kind of a role credentials again I
would just want to view kind of a role I
don't want him to create any agents and
all that stuff that looks good for me
for a job I would want him to just
possibly read I don't want him to build
I don't want him to cancel any jobs I
don't want him to configure any job I
don't even want him to create any job I
would just want him to read few things I
would not give him possibly a role to
the workspace as well I mean I don't
want him to have access to the workspace
I would just want him to read a job
I'll check you know have read-only
access to the job run
um no I don't want him to give him any
any particular access which will allow
him to run any jobs view configure yeah
possibly create yeah delete I don't want
to read yes definitely and this is the
specific role so what I'm doing I'm just
creating a global role called developer
and I'm giving him very very limited
roles in the sense that I don't want
this developer to be able to run any
agents nor create jobs or build jobs or
cancel jobs or configure jobs at the max
I would just want him to read a job that
is already put up there
okay so I would save
now I created a rule I still don't have
any users that is there on the system so
let me go ahead and create some user on
the system
that's not here I will say configure
managing cans manage users
okay let me create a new user
I would call this user as yeah developer
one sounds good
some password
some password that I can remember
okay his name is developer One D at
d.com or something like that
okay so this is the admin with with
which I kind of configured out of the
system and developer one is a user that
I have configured so still have not set
any rules for this particular user yet
so I would go to manage and gains
I would say
manage and assign rules I would say
assign rules
okay so if you see what I'm going to do
now is assign a role that is specific to
that particular deal I will find the
particular user and assign him the
developer role that I have already
configured
the role shows up here I would need to
find my user whoever created and then
assign him to that particular role so if
you remember the user that I created was
developer one I would add this
particular user and now this particular
user what kind of a role I want him to
have because this is the global rule
that had created so developer I would
assign this developer one to this
particular Global Rule and I would go
ahead and save my changes
now let me check the permissions of this
particular user by logging out of my
admin account and logging back as
developer one
if you remember this role was created
with very less privileges so there you
go I have gen games but I don't see a
new item I can't trigger a new job I
can't do anything I see these jobs
however I don't think so I'll be able to
start this job I don't have the
permission set for that the maximum I
can do is look at the job see what was
there as a part of the console output
and stuff like that so this is a limited
role that was created and I added this
developer to that particular role which
was a developer role so that the
developers don't get to configure any of
the jobs because the Jenkins instance is
owned by a cure person he doesn't want
to give developer any administrative
rights so the rights that he set out by
creating a developer role and anybody
who is tagged any user who is tagged as
a part of this developer role would get
the same kind of permissions and these
permissions can be you know fine-grained
it can be a Project Specific permissions
as well but for now I just demonstrated
the high level permission that I had set
in let me quickly log out of this user
and get back as the admin user because I
need to continue with my demo with the
developer role that was created I have
very very less privileges one of the
reasons for Jenkins being so popular as
I mentioned earlier is the bunch of
plugins that is provided by users or
Community users who don't charge any
money for these plugins but it's got
plugins for connecting anything and
everything so if you can navigate to or
if you can find Jenkins plugins
you would see index of over so many
plugins that is there all of these are
wonderful plugins whatever connectors
that you would need if you want to
connect Jenkins to an AWS instance or
you want to connect Jenkins to a Docker
instance or any of those containers you
would have a plugin you can go and
search up if I want to connect Jenkins
to big bucket
white bucket is one of the git servers
has so many plugins that is available
okay so bottom line Jenkins without
plugins is nothing so plugins is the
heart of Jenkins for you to connect or
for in order to connect Jenkins with any
of the containers or any of the other
tool sets you would need the plugins if
you want to connect or you want to build
a repository which has got Java and
Maven you would need to install Maven
and GDK on your Jenkins instance if at
all you're looking for a net build or a
Microsoft build you would need to have
MS build installed on your on your
Jenkins instance and the plugins that
will trigger Ms build if at all you want
to listen to some server-side web hooks
from GitHub you would need GitHub
specific plugins if you want to connect
Jenkins to AWS you need those plugins if
you want to connect to a Docker instance
that is running anywhere in the world as
long as you have the URL which is
publicly reachable you just have a
Docker plugin that is installed on your
Jenkins instance sonar cube is one of
the popular static code analyzers so you
can connect Jenkins build build a job on
Jenkins and push it to sonar Cube and
get sonar Cube to run analysis on that
and get back the results in Jenkins all
of these works very well because of the
plugins now with that let me connect our
Jenkins instance to GitHub I already
have very very simple Java repository up
on my GitHub instance so let me connect
Jenkins to this particular GitHub
instance and pull out a job that is put
up there all right so this is my very
very simple uh you know repository that
is there called hello Java and this is
what is there in the report there is a
hello hello.java application that is
here or a simple class file that is
there it's got just one line of system
dot out so this is already present on
github.com at this place and this would
be the URL for this Repository
if I pick up the https URL this is my
https URL so what I would do is I would
connect my Jenkins instance to go to
GitHub provide my credentials and pull
out this repository which is on the
cloud hosted github.com and get it to my
Jenkins instance and then build this
particular Java file I'm keeping the
source code very very simple it's just
at the Java file how do I build my Java
file how do I compile my Java file I
just say Java C and the name of my class
file which is hello.java and how do I
run my Java file I would say Java and
hello okay so remember I don't need to
install any plugins now because uh what
it needs is a git plugin so if you
remember when we were doing the
installation there was a bunch of
recommended plugins so git is already
installed on my system so I don't need
to install it again so let me put up a
new job here it says get job let it be a
freestyle project that's good for me I
would say okay
all right so the source code management
remember in the earlier examples we did
not use any source code because we were
just putting up some Echo kind of jobs
we did not need any integration with any
of the source code systems so now let me
connect this so I'm going to put up a
source code and git put sure because the
plugin is already there SVN perforce any
of those additional source code
management tools if at all you would
need just install those plugins and
Jenkins connects wonderfully well to all
these particular Source control tools
okay so I would copy the https URL from
here I would say this is the URL that
I'm supposed to go and grab my source
code from but all right that sounds good
but what is the username and password so
I'll have to specify a username
and password all right so I would say
the username this is my username and
this is my https credential for my job
okay so this is my username and this is
my password I just save this I say add
and then I would say you know use these
credentials to go to GitHub and then on
my behalf pull out a repository all
right if at all at this stage if there's
any error in terms of not able to
Jenkins not able to find git or the
git.exe or if my credentials are wrong
somewhere down here you would see a red
message saying that you know something
is not right you can just go ahead and
kind of fix that for now this looks good
for me I'm going to grab this URL what
am I gonna do the step would pull the
source code from the GitHub and then
what would be there as a part of my
build step
because this repository just has a Java
file correct hello.java so in order for
me to build this I would just say
execute Windows batch command and I
would say Java C
Hello dot Java that is the way I would
build my Java code and if I have to run
it I would just say Java
hello
pretty simple two steps and this would
run after the repository contents are
fetched from GitHub
Java that sounds good I would say save
this and let me try to run this
okay if you see there's a lot of you
know it executes git on your behalf it
goes out here it provides my credentials
and says you know it pulls all my
repository and by default it will pull
up the master branch that is there on my
repository and it kind of builds this
whole thing Java C hello to Java and it
runs this project Java hello and there
you see this is the output that is there
and if at all you want to look at the
contents of the repository if you can go
here this is my
workspace of my system hang on this is
not right
okay get job if you see here this is my
hello.java this is the same program that
was there on my GitHub repository
okay so this is a program that was there
on the main GitHub repository all right
so this was the same program that was
here and Jenkins on our behalf went over
all the way to GitHub pulled this
repository from there and then you know
it brought it down to my local system or
my Jenkins instance it compiled it and
it ran this particular application
okay now that I've integrated Jenkins
successfully with GitHub for a simple
Java application let me build a little
bit on top of it what I will do is I
have a maven-based web application that
is up there as a repository in my GitHub
so this is the repository that I'm
talking about it's called AMV and web
app it's got It's a maven-based
repository as you would know my one is a
very very simple Java based uh build
tool that will allow you to run various
targets and it will compile it will
based upon the goals that you specify it
can compile it can run some tests and it
can you can build a war file and even
deploy it into some other server for now
what we're going to use Maven is just
for building and creating a package out
of this particular web application it
contains a bunch of things and what is
important is just the index.jsp it just
contains an HTML file that is there as a
part of this web application so from a
perspective of requirements now since
I'm going to connect Jenkins with this
particular repository git we already
have that set we only need two other
things one is Maven because Jenkins will
use Maven so in order to use Maven
Jenkins would have to have a maven
installation that is there on the
Jenkins box and in this case the link is
box is this laptop
and after I have my Maven installed I
also need a tomcat server Tomcat is a
very very simple web server and then you
can freely download I'll let you know
how to quickly uh download and install
the Tomcat all right so download Maven
first
at the various ways in which you can
kind of download this mobile there are
zip files binary zip files and archive
files so what I've done is I've just
already downloaded Maven and if you see
I've unzipped it here so this is the
folder with which I have unzipped my
maven so as you know Maven again is is
one open source build tool so you'll
have to set in a few configurations and
set up the path so mvn hyphen iPhone
version if I specify this after setting
my path my one should work and if at all
I Echo M2 home which is nothing but the
variable environment variable specific
to my one it is always set here so once
you unzip my one just set this m to home
variable to the directory where you
unzip your Maven also just set the path
to this particular directory slash bin
because that is where your Maven
executables are all found
all right so that's with Maven and you
know since I've set the path and the
environment variable Maven is running
perfectly fine on my system I just
verified it okay next one is a tomcat
server
download Apache Tomcat server 8.5 is
what I have on my system
so I'm just gonna show you where to
download this from
this way you download Tomcat server and
um I already have the server downloaded
again this doesn't need any installation
I just unzip it here and it kind of has
a bin and configuration I have made some
subtle changes in the configuration
first and foremost Tomcat server also by
default runs on port 8080 since we
already have our Jenkins server running
on port 8080 we cannot let Tomcat run on
the same port there will be a port Clash
so what I've done I have configured
Tomcat to use a different port so if I
go to this configuration file here there
is a server.xml
let me open this up here
all right okay so this is the port by
default it will be 80 80. I have just
modified it to 8081 so I've changed the
port on which my Tomcat server would run
all right so that's one chain second
change when Jenkins kind of tries to get
into my tomcat and deploy something for
someone he would need some
authentications so that he'll be alloyed
deployment by Tomcat so for that I need
to create a user on tomcat and provide
this user credentials to my Jenkins
instance
so I would go to
tomcatusers.xml file here
I have already created a username called
deployer and the password is deployer
and I've added a role called manager
hyphen script manager hyphen script will
allow programmatic access to the Tomcat
server so this is the role that is there
so using this credentials I will enable
or I'll Empower Jenkins to get into my
Tomcat server and deploy my application
all right only these two things that is
required
let me just start my Tomcat server first
so I get into my bin folder I open
command prompt here and there's a
startup.bat
pretty fast it just takes a few seconds
yes there you go don't get server is up
and running now this is running on Port
8081 so let me just check if that looks
good
so localhost
8821
okay my Tomcat server is up and running
that sounds good the user is already
configured on this that's also fine so
what I'll do as a part of my first job
my one is also installed on my system so
I'm good to use Maven as a part of my
Jenkins so I will put up a simple job
now
I will say job
mbn
web app I call this
freestyle job that's good
okay so this will be a git repository
what is the URL of Market repository is
uh this guy https URL
okay
because this URL I will use the
credentials the old credential that I
set up will work well because it's the
same git user that I'm kind of
connecting into
all right so now the change happens here
where after I get this since I said this
is a simple Maven repository I will have
some Maven targets to run
so a simple Target first is let me run
Maven package this creates a war file
okay so mvn package is the target
package is the target so when whenever I
run this package it kind of creates it
it builds it it tests it and then
creates a package so this is all that is
required maybe let me try to save this
and let me first run this and see if it
connects well if there's any problem
with my War file or the war file gets
created properly
okay
wonderful so it built a word file and if
you see it all shows you what is the
location where this war file was
generated
so this will be the workspace if you see
this this Warfare was successfully built
now I need to grab this particular War
file and then I would need to deploy it
into Tomcat server again I would need a
small plugin to do this because I need
to connect Tomcat with my Jenkins server
let me go ahead and install the plugin
for the container deployment so I will
go to manage plugins
available
type in container
container container
deploy to container okay so let's put
this the plugin that I would need I
would install it without a restart
right seems to be very fast
nope sorry it's still installing
okay
it installed the plugin so if at all you
see this
if you go to my workspace
okay in the Target folder
I will see this web application War file
that is already built so I would need to
configure this plugin to pull up this
war file and deploy it onto the Tomcat
server for deploying onto the Tomcat
server I will use the credentials of the
user that I've created
okay so let me go to configure this
particular project again
and um
okay all this is good
so the package is good I'm going to just
create a package that's all fine now
add a post build step so after the war
file is built as a part of this package
directive let me use this deployment to
container now this will show up after
you install the plugin so deploy this
one to the container now what is that
you're supposed to specify you're
supposed to specify the what is the
location
okay so this is a global uh you know
configuration that is there that will
allow you to
from the root folder it will pick up the
war file that is there so star star
forward slash star dot War that's good
for me
okay what is the context path context
path is nothing but just the name of an
application that you know under which it
will get deployed into the Tomcat server
I will just say mvn web app
that's the name of my thing now I need
to specify what kind of a container that
I'm talking about all right so the
deployment would be for
the stormcat 8.5 is what I need
okay because the server that we have is
a tomcat 8.5 server that I have so this
would be the URL so the credentials yes
I need to add a credential for this
particular server so if you remember I
had created a credential
for my web application so let me just
find that
my Tomcat server yes configuration of
this
okay so deployer and deployer username
is deployer password is deployed okay so
let me use that credential
I would say I would say add a new
credential Jenkins credential the
username is deployer and the password is
replyer
so I would use this deploy credentials
for that and what is the URL of my
Tomcat instance so this is the URL of my
Tomcat instance so take the war file
that is fine found in this particular
folder and then you know context path
the same in web app use the deployer
deployment credentials and get into this
localhost which is there
8081 this is the Tomcat server that is
running on my system and then go ahead
and deploy it
okay so that is all that is required so
I would say just save this and let me
run it now
okay it builds successfully built the
war file it is trying to deploy it and
looks like a deployment went ahead
perfectly well
so
the context path was a main web app so
if I type in this
all right if at all I go ahead into my
Tomcat server there would be a web apps
folder you would see the
you know the date timestamp so this is
the file that get got recently copied
and this is the Explorer version of our
application
so the application was built the source
code of this application was pulled from
the GitHub server it was built locally
on the Jenkins test and then it was
pushed into a tomcat server which is
running on a different port which is
808.1 now for this demo I am running
everything locally on my system but
assuming that you know this particular
Tomcat instance was running on some
other server with some other different
IP address all that you got to go and
change is the URL of the server
so this would be the server in case you
you already have that uh you know if you
have a tomcat server which is running on
some other machine that's all fine with
a different IP that's all good enough
the whole bundle or the war file that
was built as a part of this Jenkins job
gets transferred onto the other server
and gets deployed that's the beauty of
Jenkins and automatic deployments or
other deployments using Jenkins and my
own distributed build or Master Slave
configuration in Jenkins as you would
have seen you know we just have one
instance of Jenkins server up and
running all the time and also I told you
that whenever any job that kind of you
know gets started on the Jenkins server
it is little heavy on in terms of disk
space and the CPU utilization so which
kind of you know if at all you're in an
organization where in your heavily
reliant on the Jenkins server you don't
want your Jenkins server to go down so
that's wherein you kind of start
Distributing the load that is there on
the Jenkins server so you primarily have
a server which is just a placeholder or
like a master who will take in all the
kind of jobs and what he'll do is based
upon trigger that has happened to the
job or whichever job needs to be built
he if at all he can delegate these jobs
onto some of the machines or some of the
sleeves you know that's a wonderful
thing to have okay use case one use case
two assuming that you know if you have a
Jenkins server that is running on a
Windows box or on a Linux one and if at
all you have a need where you need to
build based upon operating systems you
have multiple build configurations to
support maybe you need to build a
Windows you know Windows based.net kind
of a projects where you would need a
Windows machine to build this particular
project you also have a requirement
where you want to build Linux the next
based are systems you also have a Mac
you you support some sort of an apps or
something that is built on Mac OS you
would need to build you know Mac based
system as well so how are you going to
support all these needs so that's
wherein a beautiful concept of Master
Slave or you know primary and
delegations or agent and master comes
into play so typically you would have
one Jenkins server who will just you
know configure with all the proper
authorizations users configurations and
everything is set up on this Jenkins
server his job is just delegations he
will listen to some server triggers or
based upon the job that is coming in he
will if there is a very nice way of
delegating these jobs to somebody else
and you know taking back the results he
can control lot of other systems and
these systems may not have a complete or
there's no need to put in a complete
Jenkins installation all that you got to
do is have a very very simple Runner or
a sleeve that's a simple jar file that
is run as a low priority thread or a
process Within These systems so with
that you can have a wonderful
distributed build server that can be set
up and in case one of the servers goes
down your master would know that what
went down and kind of delegate the task
to somebody else so this is the kind of
distributed build or the Master Slave
configuration so what I'll do in this
exercise or in this demo is I'll set up
a simple slave but since I don't have
too many machines to kind of play around
what I'll do is I will set up a slave in
in one other folder within my hard drive
so I've got the C drive and D drive my
Jenkins is on my C drive so what I do is
I would just use my e Drive and set up a
very very simple slave out there I'll
just show you how to provision a slave
and how to connect to a slave and how to
delegate a job to that slave let me go
back to my Jenkins master and configure
him to you know talk to an agent so
there are various ways in which this
client and server talk to each other
what I'm going to choose is something
called as gnlp Java Network launch
protocol so using this I would ensure
that you know the client and server talk
to each other so for that I need to
ensure that I kind of enable this jnlp
port so let me try to find out where is
that let me try this
okay yes
agents and by default this jnlp agents
thing would be disabled so if you see
here there's a small help on this
so I'm going to use this jnlp which is
nothing but Java Network launch protocol
and you know I'll configure the master
and server to talk to each other using
jnlp so for that I need to enable this
guy so I enable this guy instead of
making the by default the configuration
was disabled so I make him random I make
him you know enabled and I say save this
configuration
all right so now I've configured RS made
a setting for the master so that the jlp
port is kind of opened up so let me go
ahead and you know create an agent
so I'll go to manage nodes so if you see
here there's only one master here so let
me provision a new node here so this is
the way you know in which you bring up a
new node you have to configure it on the
server and Jenkins would put in some
sort of security around this particular
agent and let you know how to launch
this particular engine so that he can
connect to a Jenkins master so I would
say new node I would give a name for my
node I would say windows node because
both of these are windows only so that's
fine I just give an identifier saying
that Windows node I would say this is a
permanent agent I would say okay so if
you see the name let me just copy this
name here with the description
number of executors since it's a slave
node and both of these are running on my
system I'll keep the number of executors
as one
that's fine remote root directory now
this is where let me just clarify this
since I have both my
my master is running on my C drive C
drive program files 86 or hang on not
86.
C colon program files
it is indeed 86.
all right Jenkins so this is where my
master is running so I don't want the C
drive what I'll do is I'll use something
called as a drive I have another Drive
in my system but please visualize this
like you know you're running this on a
separate system altogether so I create a
folder here called Jenkins node and this
is where I'm going to place my or I'm
going to provision my slave and I'm
going to run it from here so this is the
directory in which I am going to
provision my slave node so I'm going to
copy this here and that is the remote
root directory of your particular agent
or sleep so I just copy it here the
label you know probably this is fine for
me
and usage how do you want to use this
guy
so I would don't want him to run all
kinds of jobs I will only build jobs
with label Expressions that match this
particular node and so this is the label
of this node so in order for somebody to
kind of delegate any task to them
they're allowed to specify this
particular level so imagine this way if
I have a bunch of Windows system I name
it as Windows star anything that says
from Windows I can give a regular
expression and say that anything that
matches Windows run this particular task
there if I have some MAC machines I name
all these Mac agents as Mac star or
something like that and I can delegate
all tasks you know saying that start
with whatever starts with Mac in this
node run the Mac jobs there so you
identify a node using the label and then
delegate the task there all right so
launch method you know we will use Java
web start because we're going to
we we're going to use jnlp protocol okay
that sounds good directory I think
nothing else is required availability
yes will keep this agent yeah online as
much as possible that sounds good all
right let me save this
all right I'm just provisioning this
particular node now
so if I click on this node I get a bunch
of commands along with an agent.jar
so this is the agent.jar that has to be
taken down to the other machine or the
slave node and from there I need to run
this along with a small security
credential so let me copy this whole
text here in my notepad
notepad plus plus is good for me
okay I copy this whole
path there I also want to download this
alien.jar I would say yes
this agent.jar is the one that is
configured by our server so all the
details that is required for launching
this agent.jar is found in this sorry
for launching this agent is found this
agent.jar so typically I need to take
this jar file onto the other system
and then kind of run it from there so
I have this 800 jar I copy this or I cut
this I come back to my folder my Jenkins
node I paste it here
okay so now with this provision
agent.jar and I need to use this whole
command Ctrl a control C and then launch
this particular agent so let me bring up
a command from right here and then lost
it so I'm saying in the same folder
where there is agent.jar I'm going to
launch this a particular agent Java
hyphen jar agent.jar jnlp this is the
URL of my server in case the server and
client are on different locations or
different IPS there are specify the IP
address all this anyway would show up
and then the secret and you know the
root folder of your Jenkins or the slave
node
okay
so something ran and then you know it
says it's connected very well it seems
to Connected very well so let me come
back to my Jenkins instance and see you
know if at all you see earlier this was
not connected now let me refresh this
guy
okay now these two guys are connected
provision a Jenkins node and then I
copied all the credentials or the slave
dot jar along with the launch code and
then took it to the other system and
kind of sign it from there since I don't
have another system I've just got a
separate directory in another folder
another drive and I'm launching the
agent from here as long as this
particular agent is up and running or
this command prompt is up and running
the agent would be connected so once I
close this the connection goes down
all right so successfully we have
launched this particular agent now this
would be the home directory of this
Jenkins node or the Jenkins slave so any
task that I'm going to delegate to this
particular slave would all be run here
it will create a workspace right here
all right so good so let me just come
back and let me kind of put up a new
task here I will say that you know
delicate job is good I say freestyle
project I'm going to create a very very
simple job here
I don't want it to connect to gate or
anything like that let me just create a
very very simple
Echo
relegate to the slave negative two I
don't like the word slave delegated to
agent
put this way
all right so delegated to agent sounds
good now how am I going to ensure that
this particular job runs on the agent or
on the slave that I've configured
right you see this if at all you
remember how we provisioned our
particular slave we give a label right
so now I am going to put in a job that
will only match this particular label
so I'm going to say that whatever
matches this you know Windows label run
this job on that particular node so we
have only one node that's matching this
you know Windows node so this job will
be delegated around there so I save this
and let me build this
this is again a very very simple job
there's nothing in this I just want to
demonstrate how to kind of delegate it
to an agent so if you see this it ran
successfully and where is the workspace
the workspace is right inside our
Jenkins node it creates a new workspace
delegate a job it put in here so my old
or the my primary Master job is in SQL
program files under Jenkins and this is
the slave job that was successfully run
very very simple but very very powerful
concept of Master Slave configuration or
distributed build in Jenkins
okay approaching the final section where
we've done all this hard work in
bringing up our Jenkins server
configuring it putting up some jobs on
it creating users and all this stuff now
we don't want this configuration to kind
of go away we want a very nice way of
ensuring that we backup all this
configuration and in case there is any
failure Hardware crash or a machine
crash we would want to kind of restore
from the existing configuration that we
kind of backed up the one quick way to
do that would be or one dirty way to do
that would be just you know take a
complete backup of a colon program files
colon Jenkins directory because that's
where our whole Jenkins configuration is
present but we would don't want to do
that let's use some plugins for taking
up a backup so let me go to manage
Jenkins
and click on available
and let me search for some back there
are a bunch of backup plugins so I would
recommend one of these plugins that I
specifically use so this is the backup
plugin so let me go ahead and install
this plugin
all right so we went ahead and installed
this plugin so let me come back to my
manage plugins so this plugin is there
so hang on Backup Manager so you will
see this option once you install this
plugin so first time I can you know do a
setup I would say back up this
particular I'll give a folder this
folder is pertaining to the folder where
I want to check is to backup some data
and I would say the format should be zip
format is good enough let me give a name
or a template or a file name for my you
know backup this is good I want it in
verbose mode I don't want to shut on my
Jenkins or should I shut it down no okay
one thing that you got to remember is
that whenever a backup happens if there
are too many jobs that is running on the
server
it can kind of slow down your Jenkins
instance because it's it's in the
process of copying few of those things
and the files are being changed at that
moment instead a bit problematic for
Jenkins so typically you backup your
servers only when there is very less
load or typically try to you know bring
it to a shutdown kind of a state and
then take a backup
all right so I'm going to backup all
these things you know I don't want to
exclude anything else I want the history
I want the maven artifacts possibly I
don't want this guy I would just say
save
and then I would say back him up
so this would run a bunch of you know
steps and all the files that is required
as a part of this is pretty fast but
then if at all you have too many things
up on your server for now we didn't have
too many things up on our server but in
case you had too many things to kind of
backup this may take a while so let me
just pause this recording and get back
to you once the backup is complete so
there you go the backup was successful
created a backup of all the workspace
the configurations the users and you
know all that so all this is kind of
hidden down in this particular zip file
so at any instance if at all I kind of
Crash my system for some instance or say
hard disk failure and I bring up a new
instance of Jenkins I can kind of use
the backup plugin for restoring this
particular
configurations so how do I do that I
just come back to my managing cans
come back to backup manager and
obviously restore Hudson or Jenkins
configuration so devops today is being
played by you know most of the major
organizations whether it's a financial
organization whether it's a kind of a
service organization every organization
is somehow looking forward for the
implementation and the adaptation of
devops because it totally redefines and
automate the whole development process
all together and whatever the manual
efforts you were putting earlier that is
simply all gets automated with the help
of these tools here so this is something
which get really implemented because of
some of the important feature like a
cicd pipeline because CI CD pipeline is
responsible for delivering your source
code into reproduction environment in
less duration of time so cicd pipeline
is ultimately the goal which really
helps us to deliver more into the
production environment when we talk
about from this perspective
now let's talk about that what exactly
is a cacd pipeline now when we go into
that part when we go into that
understanding so cicd pipeline is
basically continuous integration and
continuous delivery concept which is
used or which is considered as an
backbone of the overall devops approach
now it's one of the Prime approach which
we Implement when we are going for a
devops implementation for our project so
if I have to go for a divorce
implementation the very first and the
minimum implementation and the
automation which I am looking forward is
actually from the particular cicd
pipelines here so cicd pipelines is
really a wonderful option when we talk
about the devops here
so what exactly is the pipeline term all
about so pipeline is in series of events
that are connected together with each
other it's kind of a sequence of the
various steps like you know typically
when we talk about any kind of
deployment so we have like you know
build process like we compile the source
code we generate the artifacts we do the
testing and then we deploy to a specific
environment all these various steps
which we use to do it like manually that
is something which we can do it into a
pipeline so pipeline is nothing but a
sequence of all these steps
interconnected with each other executed
one by one into a particular sequence
now the pipelines is responsible for
performing a variety of tasks like
building up the source code running the
test cases probably the deployment can
also be added up when we go for the
continuous integration and continuous
delivery there so all these steps are
being done into a sequence definitely
because sequence is very important when
we talk about the pipeline so you need
to talk about the sequence the same way
in which you are working on the
development and in a typical world the
same thing you will be putting up into a
specific pipeline so that's a very
important aspect to be considered now
let's talk about what is the continuous
integration here now continuous
integration is also you know known as
the CI pretty much you can see that a
lot of tools are actually named as CI
but they are referring to the continuous
integration only so continuous
integration is a practice that
integrates the source code into a share
repository and it used to automate the
verification of the source code so it
involves a build automations test cases
automation so it also helps us to detect
the issues and the bugs quite easily and
quite faster that's a very early
mechanism which we can do as such if we
want to resolve all these problems now
continuous Integrations does not
eliminate the bugs but yes it definitely
helps them uh you know easily to find
out because we we are talking about
about the automated process we are
talking about the automated test cases
so definitely that is something which
can help us to find out the bugs and
then you know the development can help
on that and they can you know proceed
with those bugs and they can try to
resolve those things one by one so it's
not in kind of automated process which
will eventually remove the bugs bugs is
something which you have to recode and
you have to fix it by following the
development practice but yes it can
really help us to find those bugs quite
easy and help them to remove
now what is the continuous delivery here
so continuous delivery also known as CD
is in kind of a phase in which the
changes are made uh into the code before
the deployment now in this case what
happens that uh it's something which we
are discussing or we are validating that
what exactly we want to deliver it to
the customer so what exactly we are
going ahead or we are moving to the
customers so that's what we typically do
in case of continuous delivery and the
ultimate goal of the pipeline is to make
the deployments that's the end result
because coding is not the only thing you
code the programs you do the development
after that it's all about the
deployments like how you're going to
that to perform the deployment so that
is a very important aspect you want to
go ahead with the deployments that's
where you can go there and that's the
real Beauty about this because it it's
in kind of a way in which we can
identify that the how the deployments
can be run or can be executed as such
here
right so the ultimate goal for the
pipeline is nothing but to do the
deployments and to proceed further on
that
right so when both these practices are
placed in together in an order so all
these steps could be referred as in
complete automated process and this
process is known as cicd so when we are
talking about like when we are working
on this automation so in that case what
happens that we are looking forward that
how the automation needs to be done and
since it's in kind of a CI CD automation
which we are talking about so it's
nothing but the uh end result would be
like build and deployment automation so
you will be taking care of both the bill
and the test case executions and the
deployments as such when we talk about
as such the cicd here
the implementation of cigd also enables
the team to do the build and deploys
quite quickly and efficiently because
these are things which is you know
happening automatically so there was no
manual efforts involved and there is no
scope of human error also so we have
frequently seen that while doing the
deployments we may miss some boundaries
or some Miss can be there so that is
something which is you know completely
removed as such when we talk about this
the process makes the teams more agile
productive and the uh confident here
because the automations definitely gives
a kind of a boost to the confidence the
TS links are going to work perfectly
fine and there is no issues as such
present now why exactly Jenkins like
Jenkins is what we typically understand
or we you know uh here and there that
it's in ci2 it's a CD tool so what
exactly is Jenkins all about so Jenkins
is also known as a kind of orchestration
tool it's an automated tool which is
there and the best part is that it's
completely open source yes there are
some particular uh paid or the
Enterprise tools are there like Cloud
bees and all but there is no as such of
offering difference between the cloud
bees and the Jenkins here so Jenkins
isn't kind of Open Source tool which a
lot of organizations pretty much
Implement as it is it itself so even if
they don't want to go
um we have seen in a lot of big
organizations where you know they are
not going for the Enterprise tool like
Cloud bees and all and they are going
for the pretty much you know core
Jenkins software as such here
so this Tool uh makes it easy for the
developers to integrate the changes to
the project that is something which is
very important because it can really
help the teams to say that how the
things can be done and how it can be
performed over there so the tools is
very easy for the developers to
integrate and that's the biggest uh you
know benefit which you are getting when
we talk about these tools as such so
Jenkins is a very important tool to be
considered when we talk about all these
automations now Jenkins achieves
continuous integration with the help of
plugins that is also a kind of another
feature or benefit which we get because
there are so many plugins which is
available there as such which is being
used and for example you want to have an
integration with kubernetes code Docker
and all Maybe by default those plugins
are not installed but yes you have the
provisioning that you can go for the
installation of those plugins and yes
those features will start embedded up
and integrated within your Jenkins so
this is the reason this is the main
benefit which we get when we talk about
the Jenkins implementation
so Jenkins uh is you know one of the
best fit which is there for building a
cicd pipeline because of its flexibility
uh open source nature plug-in
capabilities the support for plugins and
it's quite easy to choose and it's very
simple straightforward GUI which is
there which can definitely helps us you
can you know easily understand and go
through the Jenkins and you can grab the
understanding and as an end result you
will be able to have a very robust tool
which using which pretty much any kind
of source code or any kind of
programming language you can Implement
cicd whether it's Android it's a not net
it's a Java it's a node.js all the
languages are having the support for the
Jenkins
so let's talk about the cicd pipeline
with the Jenkins here now to automate
the entire development process a CI CD
pipeline is the ultimate you know
solution which we are looking forward to
build such a pipeline Jenkins is our
best solution and best fit which is
available here
so there are pretty much six uh steps
which is involved when we look forward
for any kind of pipeline it's generic
pipeline which we are looking forward
now it may have like uh any other steps
which is available there probably some
additional steps you're doing like some
other plugins you are installing but
these are the basic steps which is there
like a minimum pipeline if you want to
design these are the steps which is
available there now let's see the first
one is that we have the uh required Java
jdk like a jdk to be available on the
system now most of the operating systems
are already available with the GRE like
a Java GRE but the problem with GRE is
that it's only for the build process it
will not be doing the compilation you
can run the artifacts you can run the
jar files you can you know run the
application run the code base but the
compilation requires in Java C or the
Java jdk kit to be installed onto the
system and that's the reason why for
this one we also require the jdk and
certain Linux commands execution
understanding we need to have because we
are going to run some kind of steps some
installations and you know process so
that's pretty much required now let's
talk about how do cacd Pipeline with
Jenkins now first of all you have to
download the jdk and that is something
which is installed so after that you can
go for the Jenkins download now
jenkins.io download is the website is
the official websites of Jenkins now the
best part is that there you have the
support for different operating systems
and platforms from there you can easily
say that if you want to go for a Java
package like a war file Docker Ubuntu
deviant sentos Fedora Red Hat windows
open Sushi uh FreeBSD ganto Mac
operating system in fact whatever the
different kind of artifacts or different
environment or different uh application
you want to download you will be able to
do that so that's the very first thing
to start up on you download the generic
Java package like a war file then you
have to execute it you have to download
that into a specific folder structure
let's say say that you have you know
created a folder called Jenkins now you
have to go into that Jenkins folder with
the help of CD command and there you you
have to run the command called Java
hyphen jar and the jenkins.bar there so
these are the executables artifacts so
War files can be easily executable jar
files bar files can be easily deployed
so just because with the Java command
you can run them you don't require any
kind of web container or application
container as such so here also you can
see that we are running the Java command
and it runs the applications as such and
once that is done so you can open the
web browser and you can open like
localhost colony80 so Jenkins uses the
ATT Port just like a tomcat Apache so uh
if you know once the deployment is done
installation is done so you can just
open the localhost colonyity now if you
want to get uh the Jenkins up and
running in the browser probably you can
you know go through the public IP
address also there so you can put the
public IP address call in adity and that
can also help you to you know start
accessing the Jenkins application now in
there you will be having an option
called create new jobs so you need to
click on that now once the particular
new job new item new job that's a
different naming conventions which is
available there now all you are going to
do is that you're going to do like you
are proceeding with creating the
pipeline job so you will be having an
option called pipeline job over there
just select that and provide your custom
name what pipeline name or job name you
want to refer or you want to process
there now once that is available so what
happens that it will be in easy task for
us to see that how exactly we can go
ahead and we can perform on that part so
this can really help us to see that how
a pipeline job can be created and you
know performed on this modifications as
such now when the pipeline is selected
and we can give a particular name that
is this is the name which is available
and then we can say okay as such over
there now you can scroll down and find
the pipeline section so there what
happens that when you go over there and
say that okay this is the way that how
the pipelines are managed and you know
those kind of things so you will scroll
down and find the pipeline section and
go with that pipeline script now when
you select that option there are
different options which is available
like how you want to manage these
pipelines now you are you know have the
direct access also like if you want to
directly create the create a pipeline
script you can do that if you feel that
you want to manage like you want to
retrieve the Jenkins files also as code
management tool also can be used there
so you can work on that also so like
this there are so many a variety of
things which is available like which you
can use to work around that how exactly
the pipeline job can be created so
either you can fetch it from the source
code management Tool uh like get
subversion or something like that or you
can directly put the pipeline code as
such over there right now so next thing
is that we can configure an execute a
pipeline job with a direct script so uh
we can one see pipeline is selected so
you can put the particular script like
Jenkins file into your particular GitHub
link so you you may be having like
already a GitHub link so that the where
the Jenkins file is there so you can
make use of that now once you process
the GitHub link so what we can do is
that we can proceed with that and uh
once the processing is done so you can
do the save and you know you can keep
the changes and you know it will be
picking up the pipelines you know the
pipeline script is added up into the
GitHub and you know you have already
specified that uh let's just go ahead
with this Jenkins file pipeline script
from the GitHub repository and proceed
further now once that is done so what
next you can do is that you can go with
the build now process you click on the
bill now and once that is done so what
will happen that you will be able to see
that how the build process will be done
and how the build will be performed over
there so these are pretty much a kind of
a way so you can click on the console
output you will get all the logs that is
happening in the inside that whatever
the pipeline steps are getting executed
all of them you will be able to get or
you will be able to you know get on that
part there so these are the different
steps which is involved as such and the
sixth one is that you know uh yes
whatever the uh particular uh when you
run the build now you will be able to
see that source code will be uh you know
will be checked out and will be
downloaded before the build and you can
proceed with that part now later on if
you want to change the url of this
GitHub you can configure the job again
the existing job and you can change that
URL GitHub link URL whenever you require
you can also clone this uh job whenever
you go ahead and you work on that and
that's also kind of you know the best
part which is available as such right
and uh then you can have the advanced
settings over there so in there you can
put like your GitHub repository you can
say like okay the GitHub repository is
there so I'm just going to put this URL
and you know with that what will happen
that the settings will be available
there and the Jenkins file will be
downloaded as such and when you run the
build now you will be able to have a lot
of steps like a lot of configurations
going on so uh then the checkout sem so
we can have a declaration like checkout
sem which is there so when is there so
it will check out a specific source code
after that you go to the log and you
will be able to see that each and every
stage which is being built up and
executed as such
okay so now we are going to talk about a
demo here so on the pipeline here so
this is a Jenkins portal now you can see
here that there is an option called
create a job you can either click on the
new item or you can click on the new uh
create a job here now here I'm going to
say like a pipeline
and uh then you know you can select the
pipeline uh job type here now you have
the freestyle pipeline uh GitHub
organization multi-ability Branch
pipeline these are the different options
which is available there but I'm going
to continue with the pipeline here as
such so when I selected the pipeline and
say okay so what will happen that I will
be able to see a configuration page
which is related to the pipeline now
here the very important part is that you
have all the uh General build trigger uh
you know options which is similar to the
freestyle but the build step and the
post build step is completely removed
because of the pipeline introduction now
here you either have the option to put
the pipeline script all together you can
also have some particular example for
example let's talk about some GitHub May
one uh particular tool here so you can
see that we have you know got some steps
as such over here and you know it's
pretty much running over there now you
run it it will work smoothly it will
check out some source code but how we
are going to integrate like the version
the Jenkins file into the version
control system because that's the ideal
approach we should be following when we
create a pipeline of a cicad now I'm
going to select a particular pipeline
from a CM here then go with the git here
now in there the Jenkins file is the
name of the file of the pipeline script
and I'm going to put my
repository over here in this one now
this repository is of my gate which is
like having a miven build pipeline which
is available there it's having some
steps related to CI with for the build
and deployments and that's what we can
follow as such over here now in this one
the uh if it is a private repository
definitely you can add on your
credentials but this is a public
repository a personal repository so I
don't have to put any kind of
credentials but you can always add the
credentials with the help of add here
and that can help you to you know set up
whatever the credentials the private
repositories you want to configure now
once you save the configuration here
now what it's going to do is that you
know it's going to give you a particular
page to related to build now uh if you
want to run if you want to delete the
pipeline if you want to reconfigure the
pipeline all these different options are
available there so we are going to click
on the build now here and when I do that
immediately the pipeline will be
downloaded and will be processed now you
may not be able to get the a complete
stage view as of now because it's still
running so yeah you can see that the
checkout code is done then it's going on
to the build okay that's one of the step
which is there now once the build will
be done so it will continue with the
next steps with the next further steps
there so you can also go to the console
output log here like you can click on
this or you can click on the console
output to check the complete log which
is happening there or in fact you can
also see the stage wise logs also uh
because that is also very important when
you go for the complete logs uh it may
you know uh have a lot of steps involved
and you know a lot of logs will be
available there but if you want to see a
specific log of a specific stage that's
where this comes into the picture and as
you can see that all the different steps
like test cases executions personal Cube
analysis the archive artifacts
deployment and in fact the notification
so all this is a part of a complete
pipeline this whole pipeline is done
here and you know you get a kind of a
stage view it's success over here and
the artifacts is also available to
download so you can download this war
file is a web applications as such over
here so this is what a typical pipeline
looks like that how the automation the
complete automations really looks like
as such over here now this is a very
important aspect because it really helps
us to understand that how the pipelines
can be configured can be done and pretty
much with the same steps you will be
able to automate any kind of pipelines
as such so that it was the demo to build
a simple pipeline as such with the
Jenkins and uh pretty much in this one
we understood that how exactly the cicd
pipelines can be configured and we can
use them and we can get hold on that
part if you are looking to enter the
world of devops Simply learns
postgraduate program in devops is the
perfect place to start this
comprehensive program is designed to
equip you with the skills and knowledge
you need to succeed in a devops role
with over 500 hours of learning content
Hands-On projects and Industry relevant
case studies you will gain practical
experience in tools like git Docker
kubernetes and more so whether you are
an ID professional looking to upskill or
a fresh graduate looking to start a
career in devops don't miss out on this
opportunity to join the ranks of
successful devops professional and roll
now and take the first step towards a
brighter future so there are
approximately seven sections that we
cover in devops we go from general
devops questions source code management
with tools such as git continuous
integration and here will focus on
Jenkins continuous testing with tools
such as selenium and then you also have
the operation side of devops which is
your configuration management with tools
such as Chef puppet enhanceable
containerization with Docker and then
continuous marching with tools such as
nagios so let's just get into those
general devops questions so one of the
questions that you're going to be asked
is how is devops different from agile
and the reality is is that devops is a
cultural way of being able to deliver
Solutions that's different from agile if
we look at the evolution of delivery
over the last five to ten years we've
gone from waterfall based delivery to
Agile delivery which is on Sprints to
where we are with continuous integration
and continuous delivery around devops
the whole concept of devops is
culturally very very different from
agile and the difference is is that
you're looking at being able to do
continuous releases what does that mean
the difference is is that you want to be
able to send out code continuously to
your production environment that means
the operations team the development team
have to be working together that means
that any code that gets created has to
be able to go to production very quickly
which means you need to be testing your
code continuously and then that
production environment must also be able
to be tested continuously and any
changes or any errors that come up have
to be communicated effectively and
efficiently back to the dev and opt team
another area in which I see that devops
is different is really the response that
we have for how we engage with the
customer so the customer is coming to
your website to your mobile app to your
chatbot or any digital solution that you
have and has an expectation when you're
going through and actually doing a
devops paradigm the old model would be
that you would capture requirements from
the customer then you do your
development then you do your testing and
there'd be these barriers between each
of those as we move faster through from
waterfall to Agile what we saw is that
with agile we're able to respond much
faster to customer demand so instead of
it being weeks or months sometimes in
some cases years between releases of
software what we saw it would was a
transition to weeks and months for
releases on software now we see with
devops is that the release cycle has
shrunk even further with the goal of
continuously delighting the customer how
further has that release cycle shrunk to
there are companies that have gone from
having releases of once a week or once
every two weeks or once a month to now
having multiple releases a day indeed
some companies have up to 50 releases a
day this isn't something to also bear in
mind is that each of those releases are
tested and verified against test records
so that you can guarantee that the code
that's going to production is going to
be good continuous code so what are the
differences between the different phases
of devops so effectively there are two
main phases of devops there's the
planning encoding phase and then there's
the deploying phase and you have a tool
such as Jenkins that allows you to
integrate between both environments some
of the core benefits that you may have
to devops are going to be some technical
benefits and some business benefits so
when somebody asks you what are the
benefits of devops you can reply that
from a technical point of view you're
able to use continuous software delivery
to constantly push out code that has
been tested and verified against scripts
that have been written and approved you
can be able to push out smaller chunks
of code so that when you have an issue
you're not having to go through massive
blocks of code or massive projects
you're going through just very small
micro services or small sections of code
and you're able to detect and correct
problems faster on the business side the
benefits are absolutely fantastic from a
customer that's coming to your website
and or to your mobile app they're going
to see responses happening continuously
so that the customer is always aware
that you as a company are listening to
their demands and responding
appropriately you're able to provide a
more stable environment and you're able
to scale that environment to the demands
of the number of customers that are
using your services so how you approach
a project that needs to implement devops
so this is really an exciting area for
you to be in so there are effectively
three stages when it comes to actually
working in a devops the first stage is
an assessment stage and think of this as
the back of the napkin ideation stage
this is where you are sitting with a
business leader and they're giving you
ideas of what they would like to see
from feedback that they've had from
their customers this is Blue Sky
opportunity this is thinking of Big
Ideas that second stage and this often
comes as a fast follow to stay age one
is actually proving out that concept so
developing a proof of concept and a
proof of concept can actually be a
multiple different things so it could be
something as simple as a wireframe or it
could be something that is as complex as
a mini version of the final application
depending on the scope of the work that
you're delivering it will really depend
on how complicated you want the POC to
be but with that in mind whatever choice
you make you have to be able to deliver
enough in the POC so that when you
present this to a customer they're able
to respond to that creation that you've
developed and able to give you feedback
to be able to validate that you are
going with the right solution and able
to provide the right product to your
customers that third stage is where you
get into your devops stage and this is
just the exciting part this is where the
rubber hits the road and you start
releasing code based on a backlog log of
features that are being requested for
the solution in contrast to doing agile
delivery where you just continuously
work through a backlog with devops what
you're also looking at is putting an
analytics and sensors to be able to
validate that you are being successful
with the solution that's being delivered
so that once you've actually start
delivering out code that customers can
interact with you want to be able to see
what are the pieces of the solution that
they are using what do they like what is
complicated where are the failure points
and you want to use that data and feed
that back into your continuous
integration and have that as a means to
be able to backfill the demand of work
that gets completed in the bank log so
what is the difference between
continuous delivery and continuous
deployment so continuous delivery is
based on putting out code that can be
deployed safely to production an
insurance that your businesses and
functions are running as you would
expect them to be so it's going through
and completing the code that you would
actually see continuous deployment in
contrast is all about ensuring that
you're automating the deployment of a
production environment so you're able to
go through and scale up your
environments to meet the demands of both
the solution and the customer this makes
software development and release
processes much more faster and more
robust so if we look here we can
actually see where continuous
integration and continuous deployment
come hand in hand so when you actually
start out with the initial pushes of
your code that's where you're doing your
continuous integration and your
continuous delivery and then at some
point you want to get very comfortable
with deploying the code that you're
creating so it's being pushed out to
your production environment so one of
the things that's great about working
with the tools that you use in a devops
continuous integration and continuous
delivery model is that the development
tools that you use the containerization
tools the testing tools should always
reflect the production environment and
what this means is that when you
actually come to deploying solutions to
production there are no surprises
because your development team have been
working against that exact same
environment all the way through so a
question that you'll also be asked is
you know what is the role of a
configuration Management in devops and
so the role of configuration management
really has three distinct areas and the
first and this is really obvious one and
this is the one where you probably
already have significant experiences is
the ability manage and handle large
changes to multiple systems in seconds
rather than days hours or weeks as that
may have happened before the second area
is that you want to also demonstrate the
business reasons for having
configuration management and the
business reason here is that it allows
it and infrastructure to standardize on
resource configurations and this has a
benefit in that you're able to do more
with fewer people so instead of having a
large configuration team you can
actually have a smaller more highly
skilled team that's able to actually
manage an even larger operational
environment and thirdly you want to be
able to highlight the ability to scale
so if you have configuration management
tools you're able to manage a
significant number of servers and
domains that may have multiple servers
in it allows you to effectively manage
servers that are deployed on cloud or
private cloud and allow you to do this
with high accuracy so how does
continuous monitoring help and maintain
the entire architecture of the system so
when this question comes up you want to
dig in and show your knowledge on how
configuration and continuous monitoring
is able to control an entire environment
so the number one topic that you want to
bring up when it comes to continuous
monitoring is that with being able to
effectively monitor your entire network
24 7 for any changes as they happen
you're able to identify and Report those
thoughts or threats immediately and
respond immediately for your entire
network instead of having to wait as it
happens sometimes for a customer to
email or call you and say hey your
website's down nobody wants that that's
an embarrassing thing the other three
areas that you want to be able to
highlight are the ability to be able to
ensure that the right software and the
right services are running on the right
resources that's your number one
takeaway that you want to be able to
give of continuous monitoring the second
is to be able to monitor the status of
those servers continuous see this is not
requiring manually monitoring but having
a agent that's monitoring those servers
continuously and then the third is that
by scripting out and continuously
monitoring your entire environment
you're creating a self-audit trail that
you can take back and demonstrate the
effectiveness of the operations
environment that you are providing so
one of the cloud companies that is a
strong advocate for devops is Amazon's
web services AWS and they have a really
five distinct areas them that you can
zero in on board services so when the
question comes up what is the role of
AWS in devops you want to really hold
out your hand and list of five areas of
focus using your family finger so you
want to have flexible Services built for
scale automation secure and a large
partner ecosystem and having those five
areas will really be able to help
demonstrate why you believe that AWS and
other Cloud providers but AWS is 70 the
leader in this space are great for being
able to provide support for the role of
devops so one of the things that we want
to be able to do effectively when we're
releasing any kind of solution is to be
able to measure that solution and so
kpis are very important so you will be
asked for three important Dev of kpis
and so three that really come to mind
that are very effective the first one is
mean time to failure recovery and what
this talks about is what is the average
time does it take to recover from a
failure and if you have experience doing
this then look at the experience you
have and use a specific example where
you were able to demonstrate that mean
time to failure recovery the second is
deployment frequency and with deployment
frequency you want to be able to discuss
how often do you actually deploy
Solutions and what actually happens when
you're actually doing those deployments
and what does the impact to your Network
look like when you're doing those
deployments and then the third one is
really tied to that deployment frequency
which is around what is the percentage
of failed deployments and so how many
times did you deploy to a server and
something happened where the server
itself failed what we're looking for
when you're going through and being
asked for these kpis is experience with
actually doing a devops deployment and
being able to understand what devops
looks like when you're pushing out your
infrastructure and then the second is
being able to validate that
self-auditing ability and one word of
caution is don't go in there and say
that you have a hundred percent success
the reality is that servers do degrade
over time and you maybe want to talk
about a time when a server did degrade
in your environment and use that as a
story for how you were able to
successfully get over and solve that
degradation so one of the terms that is
very popular at the moment is
infrastructure as code and so you're
going to be asked to explain
infrastructure as code and really it's
it's something that actually becomes a
byproduct of the work you have to do
when you're actually putting together
your devops environment and
infrastructure's code really refers to
the writing of code to actually manage
your environment and you can go through
many of the other tools that we've
covered in this series but you'll see
that XML or Ruby or yaml are used as
languages to describe the configuration
for your environment this allows you to
then create the rules and instructions
that can be read by the machines that
are actually setting up the physical
Hardware versus a traditional model
which is having software and installing
that software directly onto the machine
this is really important when it comes
to cloud computing there really is a
strong emphasis of being able to explain
infrastructure as a service and
infrastructure as code is fundamental to
the foundation to infrastructure as
service and then finally allows you to
be able to talk about how you can use
scripted languages such as yaml to be
able to create a consistent experience
for your entire network all right so
let's now get into the next section
which is source code management and
we're going to focus specifically on git
the reason being is that get is really
the most popular source code management
solution right now there are other
Technologies out there but for the types
of distributed environments that we have
uh source code management with Git is uh
probably the most effective so the first
question you'll be asked when it comes
to git is to talk about the difference
between centralized and distributed
Version Control and if we look at the
way that the two are set up older
Technologies such as older versions of
Team Foundation server though the
current version does actually have git
in it but older versions required a
centralized server for you to check in
and check out of code the developer in
the centralized system does not have all
the files for the application and if the
centralized server crashes then you
actually lose all of the history of your
code now in contrast a distributed model
actually we do check in our code to a
server however for you to be effective
and building out your solution you
actually check out all of the code for
the solution directly onto your local
development machine so you can actually
have a copy of the entire solution
running on your local machine this
allows you to be able to work
effectively offline it really allows for
scalability when it comes to building
out your team so if you have a team that
may be in Europe you can actually then
scale that team with people from Asia
from North America or South America very
easily and not have to worry about
whether or not they have the right code
or the wrong code and in addition to
that if the actual main server where
you're checking in your code does crash
it's not a big deal because you actually
have each person has a copy of the code
so as soon as the server comes back up
you have to check back in and
everybody's running back as if there was
nothing had happened at all so one of
the questions you'll be asked is to give
the answer to some of the commands you
use for working with Git so if you were
to be asked a question is what is the
git command that downloads any
repository from GitHub to your computer
on the screen you have four options we
have git push get Fork get clone and get
commit the answer in this instance would
be get clone now if you want to be able
to push code from your local system to a
GitHub repository using get then first
of all you want to be able to do is
connect the local opacity to a remote
repository and in the example you may
want to talk about using the command get
remote ad origin and then the actual
path to a GitHub repository you could if
you wanted to actually at this point
also talk about other repositories such
as get lab that you can also work with
or a private git repository that would
be used just for the development team
once you've actually then added the
local repository into your local
computer then the second action you want
to use is a push which is to actually
push your local files out to the master
environment so you'd use the command git
push origin master so one question you
may be asked is what is the difference
between a bear repository and a standard
way of initializing a git repository so
let's look through what is the standard
way so the standard way using get in it
allows you to create a working directory
using the command git in it and then the
folder that creates is the folder that
creates all the revision history related
to the work that you're doing in
contrast using the bear way you have a
different command for setting that up so
it would be it in it dash dash bear and
it does not contain any working or
checked out source files locally on your
machine in addition the revision history
is actually stored in the root folder
versus a subfolder that you'd have with
the normal git in it initialization so
which of the following CLI commands
would you be used to rename a file so we
have git RM get MV git RM Dash r or none
of the above well in this instance it
would be gate MV a question that you'll
be asked around commit is going to be
what is the process to revert a commit
that has already been pushed and made
public and there are two ways you can
address this the first is to actually
address the bad file in the new commit
and you can use the command git commit
Dash M and then put in a comment for why
that file is being removed the second is
to actually create a new community it
that actually undoes all the changes
that were made with the bad commit and
then to do that you would use git revert
and then the commit ID and the commit ID
and it could be something such as
560e0938f but you'd have to find that
from the the commit that you had made
but that would allow you to revert any
bad files that you had submitted so the
two ways of being able to get files from
a get repository and you're going to be
asked to explain the difference between
git Fetch and get Paul so get fetch
allows you to fetch and download only
new data from a new repository it does
not integrate any of the new data into
your working files and it can be undone
at any time if you want to break out the
remote tracking branches in contrast get
pull updates the current head Branch
with the latest changes from the remote
server so you get all of the files and
downloaded it downloads new data and
integrates it with the current working
files you have on your system and it
tries to merge remote changes with your
local Wireless so one of the questions
you'll get asked about get is what is a
get stash so as a developer you'll be
working on the con Branch within a
solution but what happens if you come up
with an idea where it's something that
will take a different amount of time for
you to be able to complete but you don't
want to interrupt the mainline Branch so
what you can actually do is you can
actually create a branch that allows you
to start working on your own work
outside of the mainline branch and this
is called git stash allows you to be
able to modify your files without
interrupting the mainline Branch so you
once you start talking about branching
and get be prepared to answer and
explain the concept of branching so
essentially what it allows you to do is
have a Mainline Master branch that has
all the code that the team is checking
in and checking out against but allows
you to have an indefinite number of
branches that allows for new features to
be built in parallel to the mainline
branch and then at some point be
reintroduced to the mainline Branch to
allow the team to add in new features
and so if we look through the merge and
get rebase these are the two features
that you'd be using continuously to be
able to talk about how you take a branch
and merge it back into the mainline
Branch so on the left hand side we have
get merge which allows you to take the
code that you're creating and merge it
back into the master on the right hand
side what you have is slightly different
approach this is for projects where you
reach a point in a project where you go
okay we're going to effectively restart
the project at this point in time and we
want to ignore the complete history
that's happened before that and that's
called get rebase and that would allow
you to rewrite the project history by
creating a brand new Mainline branch
that ignores all other previous branches
that have happened before it you can if
you want to very quickly and easily find
out all the files that have been used to
make a particular commit so when
somebody asks you the question how do
you find a list of files that's been
changed in a particular commit you can
actually say that all you have to go is
find the command get div tree dash R and
then the hash that you would use for the
commit and that would actually then give
you a breakdown of all the files that
have been made with that particular
commit a question that you'll be asked
when you're talking about merging files
is what is a merge conflict in git and
how can it be resolved so essentially a
merge conflict is when you have two more
branches that are competing with commits
in git and you have to be able to
determine which is the appropriate of
files that need to be submitted and this
is where you would go in and to actually
help resolve this issue you'd actually
go in and manually edit the conflicted
files to select the changes you want to
keep in the final merge so let me go
through the steps that you would take to
be able to illustrate this when you're
talking about this particular question
in your interview now there are
essentially four stages the first would
be under the repository name you want to
select a pull request and you're going
to be able to show how that pull request
would be demonstrated inside of GitHub
So within the pull request there's going
to be a highlight of conflict markers
and you'll be able to select which
conflicts you want to keep and which you
want to merge and which ones you want to
change so if it's a step through how you
would actually resolve a merge conflict
and the first step would be under GitHub
you want to be able to pull the
repository name and then the pull
request around that repository in the
pull request list click the pull request
with a merge config and that you'd like
to be able to resolve they'll pull up a
file they'll list out all of the
conflicts for you near the bottom of
that file will be a list of the requests
that need to be resolved and then if you
need to make a decision on which
branches you want to keep or which ones
you want to change that will have to be
something you have to put in
instructions inside of the file you'll
actually see that there are conflict
markers within the instructions which
are going to ask you which files you
want to change which ones you want to
keep if you have more than one merge
conflict in your file scroll down to the
next set of conflict markers and repeat
steps four and five until you resolve
all of the conflicts you will want to
mark your file as resolved in GitHub so
that the repository knows that you are
having everything resolved if you have
more than one file with a conflict then
you want to go then on to the next file
and start working on those files and
just keep repeating the steps we've done
up to this point until you have all the
conflicts result evolved and then once
you have all of the resolutions created
then you want to select the button which
is commit merge and then merge all your
files back into GitHub and this will
take care and manage the resolution of
the merge conflict within GitHub so you
can also do this through command line
and with the command line you want to
use a get bash and so you want to as a
first step open up get bash and then
navigate to the local git repository in
command line by using the CD change
directory and then list out the actual
folder where you actually are putting
all of your code and then you want to be
able to generate a list of the files
that are affected with the merge
conflict and in this instance here you
can actually see the file style guide.md
has a merge conflict in it and as before
with working with GitHub you actually go
through and use a text edit and then use
any text editor but as you go to and
edit out what you want to keep and what
you want to manage in your conflict so
you actually have a resolution that's
been created so that you'll be able to
then once you you're using the conflict
markers you can actually merge your
files together so that the solution
itself will allow you to incorporate
your commits effectively into the
resolution once you've gone through and
applied your changes you're able to then
merge the conflicted commits into a
single commit and able to push that up
to your remote repository all right
let's talk about the next section which
is continuous integration with Jenkins
so the first question you'll be asked
about with Jenkins is explain a master
sleeve architecture of Jenkins so the
way that Jenkins is set up is that the
Jenkins Master will pull code from your
remote git repository such as GitHub and
we'll check that repository every time
there is a code commit it will
distribute the workload of that that's
code and the tests that need to be
applied to that code to all of the
Jenkins slaves and then on requests the
Jenkins master and the Slate will then
carry out all the builds and tests to be
able to produce test reports the next
question you'll be asked is what is a
Jenkins file and simply put a Jenkins
file as a text file that has a
definition of the Jenkins Pipeline and
is checked into a source code repository
and this really allows for three
distinct things to happen one it allows
for a code review and iteration of the
pipeline it permits an audit Trail for
that Pipeline and also provides a single
source of troop for the pipeline which
can be viewed and edited so which of the
following commands runs Jenkins from the
command line is it Java Dash jar
jenkins.war Java Dash War jenkins.jar
Java dot dash jar Jenkins jar Java Dash
War
jenkins.war and the answer is a Java
Dash jar jenkins.war so when working
with Jenkins you're going to be asked
what are the key Concepts and aspects of
working with the Jenkins Pipeline and
you want to really hold out your fingers
here and go through four key areas and
that is pipeline node step and stay so
pipeline refers to the use defined model
of a CD continuous delivery pipeline
node by the machines which is which are
part of that Jenkins environment within
the pipeline step is a single task that
tells Jenkins what to do at that
particular point in time and then
finally stage defines a conceptually
distinct subset of tasks performed
through the entire Pipeline and tasks
could be build test and deploy so which
of the following file is used to Define
dependency in Maven and do we have a
build.xml b palm.xml c e dependency.xml
or D version dot XML and the answer is
palm.xml working with Jenkins you're
going to be asked to explain the two
types of pipeline used in Jenkins along
with the syntax and so a scripted
pipeline is based on groovy script as
the domain-specific language for Jenkins
there are one or more no blocks that are
used throughout the entire Pipeline on
the left hand side you can actually see
what the actual script would look like
and the right hand side shows what the
actual declaration for each section of
that script would be the second time of
Jenkins pipeline is a declarative
Pipeline and a declarative pipeline
provides a simple and a friendly syntax
to Define what the plant line should
look like and then you can actually at
this point use an example to actually
break out how blocks are used to define
the work completed in a decorative
pipeline so how do you create a copy and
backup of Jenkins but to create a backup
periodically back up Jenkins to your
Jenkins home directory and then create a
copy of that directory it's really as
simple as that the question you'll be
asked as well is how can you copy
Jenkins from one server to another well
there are essentially there are three
ways to do that one is you can move a
job from one installation of Jenkins to
another by copying the corresponding job
directory the second would be to create
a copy of an existing job directory and
making a clone of that job directory but
with a different name and the third is
to rename an existing job by renaming a
directory so security is fundamental to
all the work that we do within devops
and Jenkins provides the center core to
all the work that gets completed within
a devops environment there are three
ways in which you can apply security to
authenticate users effectively and when
you're asked about this question of
security within Jenkins the three
responses you want to be able to provide
is a Jenkins has his own internal
database that uses secured user data
from and user credentials B is you can
use a ldap or lightweight directory
access protocol server to be able to
authenticate Jenkins users or C you can
actually configure Jenkins to
authenticate by using such as oauth
which is a more modern method of being
able to authenticate users you're going
to be asked how to deploy a custom build
of a core plugin within Jenkins and
essentially the four steps you want to
go through are first of all copying the
dot HPI plugin file into the Jenkins
home plugins subdirectory you want to
remove the plugins development directory
if there is one you want to create an
empty file called
plugin.hpi dot pinned and once you've
completed these three steps restart
Jenkins and your custom build plugin
should be available how can you
temporarily turn off Jenkins security if
the administrative user has locked
themselves out of the admin console this
doesn't happen very often but when it
does it's good to know how you can
actually get into Jenkins and be able to
resolve the problems of authenticating
effectively into the system as an
administrator so when you want to be
able to get into a Jenkins environment
what you want to be able to do is locate
the config file you should see that it's
set to True which allows for security to
be enabled if you then change the user
security setting to false Security will
disabled allow you to make your
administrative changes and will not be
re-enabled until the next time Jenkins
is restarted so what are the ways in
which a build can be scheduled and run
in Jenkins well there are four ways in
which you can identify the way a build
can be scheduled on Ryan Jenkins the
first is when source code management
commits new code into the repository you
can run Jenkins at that point the second
can be the after the completion of other
builds so maybe you have multiple builds
in your project that are dependent to
each other and when so many other builds
have been executed then you can have
Jenkins run you can schedule build to
run at a specified time so you may have
nightly builds of your code that
illustrate the changes and the solution
you're building and then finally you
also can manually build a environment on
request occasionally you will want to
also restart Jenkins and so it's good
that when a question around how do you
restart Jenkins manually comes up that
you have the answers and there are two
ways in which you can do it one is you
can force a restart without waiting for
bills to complete by using the Jenkins
URL that you have for your environment
slash restart or you can allow all
running bills to complete before restart
are required in which case you would use
the command mind of the URL for your
Jenkins environment slash safe restart
so let's go into the fourth and final
section of this first video which talks
about continuous testing with selenium
so the first question you will be asked
most likely around selenium are what are
the four different selenium components
and again you want to hold open your
fingers because there are four distinct
environments you have selenium
integrated development environment or
selenium IDE you have selenium remote
control or selenium RC you have selenium
Webdriver and then selenium grid you'll
be asked to explain each of those areas
in more detail but let's start off with
by looking at selenium driver what are
the different exceptions in selenium
Webdriver so it's useful to remember
that an exception is an event that
occurs during the execution of a program
that disrupts the normal flow of that
program's instructions and so we have
four we have a timeout exception an
element not visible exception no such
element exception and a session not
found exception and each of those if we
step through them are the four different
types of exceptions that can be thrown
up when using the selenium Webdriver so
as we evolve in our digital world with
the different types of products that are
available for us to be able to build
Solutions onto multiple platforms you're
going to be asked can selenium and other
devops tools run in other environments
and so a good question around this is
cancellium test and application in an
Android web browser and the short answer
is absolutely yes it can we have to use
the Android driver for it to be able to
work so you want to be able to talk
about the three different types of
supported tests within selenium so when
the question comes up what are the
different test types supported by Salam
you can answer that and there are three
different types of tests first is a
functional test second is a regression
test and third is a load testing test
the functional test is a kind of Black
Box testing in which test cases are
based on a specific area or feature
within the software a regression test
helps you find any specific areas that
functional tests or non-functional areas
of the code wouldn't be able to detect
the load testing test allows you to
monitor the response of a solution as
you increase the volume of hits and how
you're using the code are put onto it an
additional question you'll be asked is
how can you get a text of a web element
using selenium well the get command is
used to retrieve tax of a specific web
element and it's important to remember
however that the command does not return
any parameters but just returns a string
value so you want to be able to capture
that stream value and discuss about it a
question you'll be asked around selenium
am is can you handle keyboard and mouse
actions using selenium and the answer is
yes you can but you have to make sure
that you're using the advanced user
interaction API and the advanced user
interaction API is something that can be
scripted into your test and it allows
you for capturing methods such as a
click and hold and drag and drop Mass
events and then keyboard down or
keyboard up a key release events so that
if you want to to capture say the use of
control shift or a specific function
button off the keyboard you'd be able to
capture those of the following four
elements which of these elements is not
a web element method a get text B size C
get tag name d send keys and it's B size
you're going to be asked to explain what
is the difference for when we use find
element or find elements and so if we
look at find element and find element
finds the first element in the current
web page that matches the specified
locator value in contrast find elements
finds all of the elements on the web
page that matches the specified value
when using Webdriver what are the driver
close and Driver quit commands and these
are two different methods used to close
a web browser session in selenium so
driver close will close the current web
browser in which your focus is set and
Driver quick closes all the browser
windows and ends the web driver session
completely the final question that you
are likely to be asked in using selenium
is how can you submit a form using
selenium well in this instance that's
relatively easy the following lines of
code will let you submit a form in
selenium which would be web element El
equals driver dot find element and then
you put in the ID and the element ID and
then L submit open close parentheses
semicolon so let's just get into the
first section which is configuration
management so one of the questions that
you'll get asked right away is why do
you have SSL certificates used for Chef
really fundamentally your immediate
answer should be security SSL provides
for a very high level of private
security and private and public key
pairing this really is essential to
ensure that you have a secure
environment throughout your entire
network the second part should be that
if you're using SSL and you're using the
private public key model within SSL
you're able to guarantee the systems on
your network that the chef that you'll
be able to validate that the nodes
within your network that Chef is
validating against actually are the real
nodes themselves not imposters so you
will also be asked some questions such
as the following which is the following
commands which you use to stop or
disable the HTTP service when the system
boots and you'll typically get four
responses and there'll be hashtag
systemctl disable
httpd.service or is it system disable
http.service system disable httpd or the
final option which is systemctl disable
httpd dot service your answer should be
the first one which is hashtag systemctl
disable
http.service so Chef comes with a series
of tools that allow it to function
effectively and one of the tools that
you're going to be asked about is what
is Test Kitchen and Test Kitchen is
essentially a command line tool that
allows you to be able to test out your
cookbook before you actually deploy it
to a real node so some of the commands
that you would use are for instance if
you want to create an instance of Test
Kitchen you would do kitchen create if
you want to destroy an instance after
you've created it you do kitchen destroy
and if you want to be able to combine
multiple instances you would do kitchen
converge so a question you'll get is
around Chef is how how does Chef apply
differ from Chef client and so
fundamentally the difference between
them is that Chef apply will validate
the recipe that you're working on
whereas Chef client looks to apply and
validate the entire cookbook that's run
in your server environment so one is
focused on the recipe and the other is
focus on the entire cookbook so there
are some differences when you're working
with different command lines so for
instance when you're working with
puppets and you're working with one
version of puppets and you want to do
what is the command to sign a requested
certificate the top example here is for
puppet version 2.7 whereas the lower
option here is for puppet version three
and that's something to bear in mind
when you're going through your interview
process is that the tools that are used
within a continuous integration can
continuous delivery devops model do vary
and so you want to be able to talk
knowledgeably about the different
version of the tools so that when you're
talking to your interviewer you're able
to show you the Deep knowledge that you
have which open source or Community
tools do you use to make puppet more
powerful and essentially this question
is going to be asking you to look beyond
the core Foundation of puppet itself and
so the three options you have is being
able to track configurations with jira
which you should be doing anyway but
it's a great way to be able to clearly
communicate the work that's been done
with puppet our Version Control can be
extended with get and then the changes
should be passed through Jenkins so the
three tools you want to be looking at
integration with jira Git and Jenkins so
what are the resources in public well
fundamentally there are four the
resources are basic units of any
configuration management tool they are
the features of the nodes they are the
written catalog and the execution of the
catalog on a node so as we dig deeper
into puppets one of the things that you
are likely to be asked regarding puppet
is what is a class in puppets and so a
class in public is really the name
blocks in your manifest that contain the
various configurations and this can
include Services files and packages we
have on the screen here example of what
a class would look like when you write
it out and you may want to memorize just
one class don't memorize just a whole
set of classes just memorize one the
person's interviewing you is just really
looking for someone who has a working
knowledge they're not looking for you to
have memorized complete massive classes
but having one small class to be able to
illustrate the experience you have is
extremely valuable to the interviewer
and particularly if it's a technical
interview so as we move into ansible one
of the things that you're going to be
asked around ansible is what is ansible
role so a role is an independent block
of tasks and variable files and
templates embedded with inside made of
the Playbook so the example we have here
on the screen actually shows you one
role within a Playbook and in this role
it is to install Tomcat on a node again
as with previous question within puppet
of a class it's probably good to have
memorized just one or two roles so you
can talk knowledgeably about ansible
when you're having your interview so
when you're working with ansible when
should you be using the curly brackets
and so just as a frame of reference uh
there's often two different ways that
these kind of brackets are referred to
now they're either referred to as French
brackets or curly brackets either way
and what you'll be wanting to ask is
when would you use these specific types
of brackets within and Sport and really
the answer comes down to two things one
is that it makes it easier to
distinguish strings and undefined
variables and the second is for putting
together conditional statements when you
are actually using variables and the
example we have here is this prints the
value of and we have Foo and we have to
then put in the variable conditional
statement of Foo is defined as something
so what is the best way to make content
reusable and redistributable with
ansible and there's a really essentially
three the first is to include a sub
module or another file in your playbook
the second is to import an improvement
of an include which ensures that a file
is added only once and then the third is
roles to manage the tasks within the
Playbook so a question you will be asked
is provide a differences between ansible
and puppets so if we look at ansible
it's a very easy agentless installation
it's based on python you can configure
it with yaml and there are no support
for Windows in contrast puppet is an
agent based installation it's written in
Ruby the configuration files are written
in DSL and it has support on all popular
operating systems so we dig deeper into
the actual architecture ansible it has a
much more simple architecture and it's
definitely a push only architecture in
contrast to puppet it's a more
complicated but more sophisticated
architecture where you're able to have a
complete environment managed by the
puppet architecture so let's get on to
our next section which is
containerization so let's go through and
you're going to be asked to explain what
the architecture of Docker is and Docker
really is the most popular
containerization environment so Docker
uses a client server architecture and
the docker client is a service which
runs in a command line and then the
docker Daemon which is run as a rest API
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an execute real package of
applications and its dependencies
together and then finally the docker
registry is a service to hosts and
distribute Docker images among other
users so you'll also be asked to provide
what are the advantages of Docker over
virtual machine and and this is
something that comes up very
consistently in fact and you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a Docker
or docker-like environment and really
the the arguments for Docker are just
absolutely fantastic you know first of
all Docker does contain an occupy Docker
containers occupy significantly less
space than a virtual machine or a
dedicated machine the boot up time on
Docker is significantly faster than a VM
containers have a much better
performance as they are hosted in a
single Docker image Docker is highly
efficient and very easy to scale
particularly when you start working with
kubernetes easily portable across
multiple platforms and then finally for
space allocation Docker data volumes can
be shared and reused among multiple
containers the argument against virtual
machines is significant and particularly
if you're going into an older
environment where a company is still
using actual dedicated hardware and
haven't moved to a cloud or cloud-like
environment your Arguments for Docker
are going to be very very persuasive be
very clear on what the advantages are
for Docker over a virtual machine
because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to working with the
tools like Docker you need to be able to
effectively share with that team what
the benefits are so how do we share
Docker containers with different nodes
and in this instance what you want to be
able to do is Leverage The Power of
Docker swarm so darker swarm is a tool
which allows the it administrators and
developers to create and manage clusters
of a swarm nodes within the darker
platform and there are two elements to
the node there's the manager node and
then there's the the worker node the
manager node as you'd assume matches the
entire infrastructure and the working
node is actually the work of the agent
as it gets executed so what are the
commands to create a Docker swarm and so
here we have an example of what a
manager node would look like and once
you've created a swarm on your manager
node you can now add worker nodes to
that swarm and again when you're
stepping through this process be very
precise in the execution path that needs
to be taken to be able to effectively
create a swarm so start with the manager
node and then you create a worker node
and then finally when a node is
initializes a major note it can
immediately create a token and that
token is used for the worker nodes and
associating the IP address with the
worker nodes question 17 how to run
multiple containers using a single
service it is possible to run multiple
containers as single service by using
Docker compose and Docker compose will
actually run each of the services in
isolation so that they can interact with
each other the language used to write
out the compose files that allow you to
run the service is called yaml and yaml
stands for yet another markup language
so what is the use of a Docker file so a
Docker file actually is used for
creating Docker images using the build
command so let's go through and show on
the screen what that would look like and
this would be an opportunity where if
you're actually in a technical interview
you could potentially even ask hey can I
draw on a whiteboard and show you what
the architecture for using the build
command would look like and what the
process would look like again when
you're going through an interview
process as someone who interviews a lot
of people one of the things I really
like is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to to the
Whiteboard and actually show what can
actually be done through actually
creating images on the Whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as an architect this
should be something that you're
comfortable doing and by doing it in the
interview and suddenly you want to ask
permission before you actually do it but
doing this in the interview really helps
demonstrate your comfortable feelings of
working with these kind of architecture
drawings so back to the question of
creating a Docker file so we go through
and we have a Docker file that actually
then goes ahead and creates the docker
image which then in turns creates the
docker container and then we are able to
push that out up to a Docker Hub and
then share that Docker Hub with
everybody else as part of the docker
registry with the whole network so what
are the differences between Docker image
and Docker containers so let's go
through the docker image so the docker
images are templates of a Docker
container an image is built using a
Docker file and it stores that Docker
file in a Docker repository or a Docker
Hub and you can use Docker Hub as an
example and the image layer is a
read-only file system the docker
container is a collection of the runtime
instances of a Docker image and the
containers are created using Docker
images and they are stored in the docker
Daemon and every container is a layer is
a read write file system so you can't
replace the information you can only
append to it so while you can actually
use yaml for writing your so a question
you can be asked is instead of yaml what
can be an alternate file to build Docker
compose so yaml is the one that is the
default but you can also use Json so if
you are comfortable working with Json
and my that is something that should be
get comfortable with is you want to be
able to use that to name your files and
it's a frame reference Json is a logical
way of being able to do value paired
matching using a JavaScript like syntax
so you're going to be asked to how to
create a Docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a MySQL Docker
container so to do that you want to be
able to build a Docker image or pull
from an existing Docker image from a
Docker repository or Hub and then you
want to be able to then use Docker to
create a new container which has my SQL
from the existing Docker image
simultaneously the layer of read write
file system is also created on top of
that image now below at the bottom of
the screen we have what the command
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have Docker registry
Is An Open Source server-side service
used for hosting and distributing Docker
images whereas in contrast for
repositories collection of multiple
versions of a Docker image in a registry
a user can distinguish between Docker
images with their tag names and then
finally on the the registry Docker also
has its own default registry called
Docker hub for the repository it is a
collection of multiple versions of
Docker images it is stored in a Docker
registry and it has two tabs a public
and private registry so you can actually
create your own Enterprise registry so
you're going to be asked you know what
are the cloud platforms that support
Docker really you know lists them all
and we have listed here Amazon web
services Microsoft Azure Google Cloud
Rackspace but you could add in their IBM
bluemix you could put in Red Hat really
any of the cloud service providers out
there today do support Docker it's just
become an industry standard so what is
the purpose of Expose and publish
commands in Docker so if we go through
expose is an instruction used in Docker
file whereas publish is used in Docker
run command for expose it is used to
expose ports within a Docker Network
whereas with Publishers can be used as
side of a Docker environment for expose
it is a documenting instruction used at
the time of building an image and
running a container whereas with
published is used as to map a host port
to a running container port for expose
is the command used in Docker whereas
for publish we use the command Dash p
for when we're doing our command line
used in Docker and examples of these are
exposed 8080 or with Docker we would put
in all for publish we would do the
example Docker run Dash Dash p and then
0.0.0.80 a colon 80 as our command line
so let's look at continuous monitoring
so with continuous monitoring how does
nagos help in continuous monitoring of
systems applications and service and so
this is really just a high level
question of using nagas within your
environment and you should be able to
just come back very quickly and say
negus allows you to help manage the
servers and check if they've been
sufficiently utilized and if there are
any task failures that need to be
addressed and so there are three areas
of utilization and risk that you want to
be able to manage this is being able to
verify the status and services of the
entire network the health of your
infrastructure as a whole and if
applications are working properly
together with our web services and apis
that are reachable
so the second question you'll be asked
is how does negus help in continuous
monitoring of systems applications and
services so it's able to do this by
having the initial negative process and
scheduler and the additional plugins
that you would use for your network
connect with the remote resources and
the negative web interface to be able to
run status checks on a predefined
schedule so what do you mean by nagius
remote plugin executor or the mpre of
Nag years so mpre allows you to execute
plugins on links Unix machines that
allow you to do additional monitoring
and machine metrics such as disk usage
CPU load Etc what are the ports used by
nagios for monitoring purposes in this
example there are three and they're easy
to remember so I would memorize these
three but they're essentially ports five
six six five six six seven and five six
six eight so the there are two types of
checks within that year so you will be
asked for what is an active and passive
check in nagios so an active check and
is initiated by the nagios process and
is run on a regular schedule a passive
check is initiated and formed by an
external application or process so this
may be where a system is failing and
checks uh results are submitted to
netgear's for processing and to continue
with this for what is an active and
passive check active checks are
initiated by the check logic within the
90s Daemon and negus will execute a
plug-in and pass information about what
needs to be checked plugin will then
check the operational state of the host
or servers and then process the results
of the host or service check and send
out notifications in contrast with the
passive check it is an external
application that initiates the check it
writes the results of the check to an
external command line file and I guess
reads the external command file and
places the results of all passive checks
in a queue for later processing so you
can go back and revalidate and then
negos may send out notifications log
alerts Etc depending on the results that
they get from checking the information
so you're going to be asked to explain
the main configuration file and its
location in Nigeria so the main
configuration file consists of a number
of directives that affect how many years
operate so consider this as the
configuration file that it's read by
both now us processor and the cgis so
this will allow you to be able to manage
the main configuration file that's
placed into your settings directory so
what is the netgears network analyzer
and again hold out your four fingers
because there are four options here so
the nagis network analyzer R1 provides
an in-depth look at all your network
traffic source and security threats two
allows system admins together high level
information on the healthier Network
Three provides a central view of your
network traffic and bandwidth data and
then four allows you to proactive in
resolving outages abnormal behavior and
threats before they affect critical
business processes so what are the
benefits of HTTP and SSL certificate
monitorings with nag years so with HTTP
certificate monitoring it allows you to
have increased server and services and
application availability obviously very
important fast detection of network
outages and protocol failures and allows
web transaction and web service
performance monitoring the SSL
certificate monitoring allows you for
increased website availability frequent
application availability and provides
increased security so explain
virtualization with nagios so in
responses the first thing you should be
able to talk about is how nagios itself
can run on many different virtualization
platforms including Microsoft Visual PC
VMware Zen Amazon ec2 et cetera Etc so
just make sure you get that right off
the but I guess it was able to provide
capabilities tomorrow and so much of
metrics on different platforms it allows
for ensure for quick detection of
service and application failures and has
the ability to be able to monitor
against many metrics including CPU usage
memory networking and VM status so name
the three variables that affect
recursion inheritance in nagios and it
is name use and register so name is a
template name that can be referenced in
other object definitions use specifies
the name of the template object that you
want to inherit its properties and
variables from and register indicates
whether or not the object definition
should be registered to nagios and on
the right hand side of the screen we
have an example of what that script
would look like again you may want to be
able to memorize this as it's something
that you can achieve write down and show
someone if you're going through a
technical interview so why is nagyo said
to be object r oriented and
fundamentally comes down to the object
configuration format that you can use in
your object definitions it allows you to
inherit properties from other object
definitions and this is typical object
oriented development and is now applied
for the nagas environment so some of the
objects that you can inherit are
Services host commands and time periods
so finally explain what is State talking
in nagios and so there are really four
options here when you're talking about
State stalking so State stocking is used
for login purposes in nagios it allows
you to enable for a particular host or
service that nagos will watch over very
carefully it will log any changes it
sees in the output of the check results
and then finally it helps the analysis
of log files and this was all about for
this delps bootcamp hope you guys found
it informative and helpful if you like
this session then like share and
subscribe if you have any questions then
you can drop them in the comment section
below thanks for watching and stay tuned
for more from Simply learn
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
thank you