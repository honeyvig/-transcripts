hi everyone welcome to this fantastic
video on deep learning complete calls by
simply learn but before we begin if you
enjoy watching these videos and find
them interesting subscribe to our
Channel as we bring you the best videos
daily also hit the Bell icon to never
miss any updates from Simply learn let's
go through the agenda of this video real
quick
we will brief you with a detailed
introduction of deep learning covering
the data source and neural networks
after that we will see the difference
between AI ML and deep learning after
which we will take you through the road
map of deep learning followed by Deep
learning with python we will walk you
through some amazing Concepts like py
toch versus tens oflow versus gas the
difference between tensor flow 1.0 and
2.0 and the many Core Concepts after
that we will see what are Gans CNN and
RNN in deep learning and computational
graphs in deep learning finally we will
cover some essential deep learning
projects you can mention in your CV to
make it stand out from crowd in your
upcoming interviews speaking of
interviews we have covered you along
with the most frequently Asked deep
learning interview questions to help you
crack the most challenging interviews
but before we begin to understand what
deep learning is if you want to become
an AI expert and gain handsome salary
packages look at the wide range of IM
IML courses by simply learn in
collaboration with top universities
across the globe by enrolling in any of
these certification programs you will
gain expertise in the skills like
generative AI prompt engineering chat
GPT explainable AI machine learning
algorithms supervised and unsupervised
learning model training and optimization
and there is much more on the list with
hands on experience in the tools like
chat GPT di python open CV and
tensorflow you will catch the eye of top
recruiters so what are you waiting for
hurry up and enroll now an year of
experience is preferred to enroll in
these courses find the course Link in
the description box with that in mind
over to our tutors further Ado let's get
started so deep learning is considered
to be a part of machine learning so this
diagram very nicely depicts what deep
learning is at a very high level you
have the all-encompassing artificial
intelligence which is more a concept
rather than a technology or a technical
concept right so it is it is more of a
concept at a very high level artificial
intelligence under the herd is actually
machine learning and deep learning and
machine learning is a broader concept
you can say or a broader technology and
deep learning is a subset of machine
learning the primary difference between
machine learning and deep learning is
that deep learning uses neural networks
and it is suitable for handling large
amounts of unstructured data and the
last but not least one of the major
differences between machine learning and
deep learning is that in machine
learning the feature extraction or the
feature engineering is done by the data
scientists manually but in deep learning
since we use neural networks the feature
engineering happens automatically so
that's a little bit of a a quick
difference between machine learning Lear
and deep learning and this diagram very
nicely depicts the relation between
artificial intelligence machine learning
and deep learning now why do we need
deep learning machine learning was there
for quite some time and it can do a lot
of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amount of
terms of structured data but when it
comes to unstructured data it's a big
challenge so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's the
reason deep learning is required
compared to let's say traditional
machine learning it can also perform
complex uh algorithms more complex than
let's say what machine learning can do
and it can achieve best performance with
the large amounts of data so the more
you have the data let's say reference
data or label data the better the system
will do because the training process
will be that much better and last but
not least with deep learning you can
really avoid the manual process of
feature extraction those are some of the
reasons why we need deep learning some
of the applications of deep learning
deep learning has made major inroads and
it is a major area in which deep
learning is applied is healthare care
and within Health Care particularly
oncology which is uh basically cancer
related stuff one of the issues with
cancer is that a lot of cancers today
are curable they can be cured they are
detected early on and the challenge with
that is when a Diagnostics is performed
let's say an image has been taken of a
patient to detect whether there is
cancer or not you need a specialist to
look at the image and determine whether
it because the patient is fine or there
is any onset of cancer and the number of
Specialists are limited so if we use
deep learning if we use automation here
or if we use artificial intelligence
here then the system can with a certain
amount of the good amount of accuracy
determine whether a particular patient
is having cancer or not so the
prediction or the detection process of a
disease like cancer can be EXP expedited
the detection process can be expedited
can be faster without really waiting for
a specialist we can obviously then once
the application once the artificial
intelligence detects or predicts that
there is an onset of cancer this can be
cross checked by a doctor but at least
the initial screening process can be
automated and that is where the current
focus is with respect to deep learning
in healthcare what else robotics is
another area deep learning is is majorly
used in robotics and you must have seen
nowadays robots are everywhere humanoids
the industrial robots which are used for
manufacturing process you must have
heard about sopia who got citizenship
with Saudi Arabia and so on there are
multiple such robots which are knowledge
oriented but there are also industrial
robots are used in Industries in the
manufacturing process and increasingly
in security and also in defense for
example image processing video is fed to
them and they need to be able to detect
objects obstacles and so on and so forth
so that's where deep learning is used
they need to be able to hear and make
sense of the sounds that they are
hearing that needs deep learning as well
so robotics is a major area where deep
learning is applied then we have
self-driving cars or autonomous cars you
must have heard of Google's autonomous
car which has been tested for millions
of miles and pretty much incident free
there were of course a couple of
incidents here and there but it is uh
considered to be fairly safe and there
are today a lot of Automotive companies
in fact pretty much every automotive
company worth its name is investing in
self-driving cars or autonomous cars and
it is predicted that in the next
probably 10 to 15 years these will be in
production and they will be used
extensively in real life right now they
are all in R&D and in test phases but
pretty soon these will be on the road so
this is is another area where deep
learning is used and how is it used
where is it used within autonomous
driving the car actually is fed with
video of surroundings and it is supposed
to process that information process that
video and determine if there are any
obstacles it has to determine if there
are any cars in the side will detect
whether it is driving in the lane also
it has to determine whether the signal
is green or red so that accordingly it
can move forward or wait so for all
these video analysis deep learning is
used in addition to that the training
overall training to drive the car
happens in a deep learning environment
so again a lot of scope here to use deep
learning couple of other applications
are machine translation so today we have
a lot of information and very often this
information is in one particular
language and more specifically in
English and people need information in
in various parts of the world it is
pretty difficult for human beings to
translate each and every piece of
information or every document into all
possible languages there are probably at
least hundreds of languages or if not
more to translate each and every
document into every language is uh
pretty difficult therefore we can use
deep learning to do pretty much like a
realtime translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the Fly for
example you go to somewhere like China
and you want to know what is written on
a signboard now it is impossible for
somebody to translate that and put it on
the web or something like that so you
have an application which is trained to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate that
so therefore deep learning is used in
such situations as well and that is
again on demand so it is not like you
have to translate all these documents
from other languages into English in one
sh and keep it somewhere that is again
and pretty much an impossible task but
on a need basis so you have systems that
are trained to translate on the flag so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it it needs a lot
of effort to train such neural n it has
been proved that it is possible so this
is a relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black and white
image this was colored by a neural net
or a deep learning application as you
can see this done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning now one of the major secret
source of deep learning is neural
network deep learning works on neural
network or consists of neural network so
let us see what is neural network neural
network or artificial neural network is
designed or based on the human brain now
human brain consists of billions of
small cells that are known as neurons
artificial neural networks is in a way
trying to simulate the human brain so
this is a quick diagram of biological
Neuron a biological neuron consists of
the major part which is the cell nucleus
and then it has some tentacles kind of
stuff on the top called dendrite and
then there is like a long tail which is
known as the axon further again at the
end of this axon are what are known as
synapses these in turn are connected to
the dorids of the next neuron and all
these neurons are interconnected with
each other therefore there are like
billions of them sitting in our brain
and they're all active they're working
they based on the signals they receive
signals as inputs from other neurons or
maybe from other parts of the body and
based on certain criteria they send
signals to the neurons at the other end
so they they get either activated or
they don't get activated based on so it
is like a binary Gates so they get
activated or not activated based on the
inputs that they receive and so on so we
will see a little bit of those details
as we move forward in our artificial
neuron but this is a biological neuron
this is the structure of a biological
neuron and artificial neural network is
based on the human brain the smallest
component of artificial neural network
is an artificial neuron as shown here
sometimes is also referred to as a
perceptron now this is a very high level
diagram the artificial neuron has a
small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as X1 X2 and so on each of the
inputs are multiplied by what is known
as weights which are represented as W1
W2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like X1
into W1 + X2 into W2 and so on the
products are are then added and then
there is a bias that is added to that in
the next uh slide we will see that
passes through an activation function
and the output comes as a y which is the
output and based on certain criteria the
cell gets either activated or not
activated so this output would be like a
zero or a one binary format okay so we
will see that in a little bit more
detail but let's do a quick comparison
between biological and artificial neuron
just like a bi iCal neuron there are
dendrites and then there is a cell
nucleus and synapse and and an axon we
have in the artificial neuron as well
these inputs come like the dendrite if
you will act like the dendrites there is
a like a central unit which performs the
summation of these uh weighted inputs
which is basically W1 X1 W2 X2 and so on
and then a bias is added here and then
that passes through what is known as an
activation function okay so these are
known as the weights w W 1 W2 and then
there is a bias which will come out here
and that is added the bias is by the way
common for a particular neuron so there
won't be like B1 B2 B3 and so on only
weights will be one per input the bias
is common for the entire neuron it is
also common for or the value of the bias
Remains the Same for all the neurons in
a particular layer we will also see this
as we move forward and we see deep
neural network where there are multiple
neurons so that's the output now the
whole exercise of training a neuron is
about changing these weights and biases
as I mentioned artificial neural network
will consist of several such neurons and
as a part of the training process these
weights keep changing initially they are
assigned some random values through the
training process the weights the whole
process of training is to come up with
the optimum values of W1 W2 and WN and
then the B for or the bias for this
particular neuron on such that it gives
an accurate output as required so let's
see what exactly that means so the
training process this is how it happens
it takes the inputs each input is
multiplied by a weight and these weights
during training keep changing so
initially they're assigned some random
values and based on the output whether
it is correct or wrong there is a
feedback coming back and that will
basically change these weights until it
starts giving the right output that is
represented in here as Sigma I going
from 1 to n if there are n inputs uh wi
into x i so this is the product of W1 X1
W2 X2 and so on right and there is a
bias that gets added here and that
entire thing goes to what is known as an
activation function so essentially this
is Sigma of
wixi plus a value of bias which is a b
so that entire thing goes as an input to
an activation function now this
activation function takes this as an
input gives the output as a binary
output it could be a zero or a one there
are of course to start with let's assume
it's a binary output later we will see
that there are different types of
activation function so it need not
always be binary output but to start
with let's keep Simple so it decides
whether the neuron should be fired or
not so that is the output like a binary
output zero or one all right so again
let me summarize this so it takes the
inputs so if you're processing an image
for example the inputs are the pixel
values of the image X1 X2 up to xn there
could be hundreds of these so all of
those are fed as so these are some
values and these pixel values again can
be from 0 to to 56 each of those pixel
values are then multiplied with what is
known as a weight this is a numeric
value can be any value so this is a
number W1 similarly W2 is a number so
initially some random values will be
assigned and each of these weights are
multiplied with the input value and
their sum this is known as the weighted
sum so that is performed in this kind of
the central unit and then a bias is
added remember the bias is common for
each neuron so this is not the bias
value is not one bias value for per
input so just keep that in mind the bias
value there is one bias per neuron so it
is like this summation plus bias is the
output from this section this is not the
complete output of of the neuron but
this is a bias for output for step one
that goes as an input to what is known
as an activation function and that
activation function results in an output
usually a binary output like a zero or a
one which is known as the firing of the
neuron okay good so we talked about
activation function so what is an
activation function an activation
function basically takes the weighted
sum which is we saw W1 X1 W2 X2 the sum
of all that plus the bias so it takes
that as an input and it generates a
certain output now there are different
types of activation functions and the
output is different for different types
of activation functions moreover why is
an activation function required it is
basically required to bring in
nonlinearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today reu has kind of
taken over so reu is by far the most
popular activation function that is used
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0
and 1 threshold function is the value
will be zero up to a certain value and
beyond that this is also known as a step
function and beyond that it will be one
in case of sigmoid there is a gradual
increase but in case of threshold it's
like also known as a step function
there's a rapid or instantaneous change
from 0 to one whereas in sigmoid we will
see in the next slide there is a gradual
increase but the value in this case is
between 0 and one as well Now reu
function of on the other hand it is
equal to basically if the input is Zer
or less than zero then the output is0
whereas if the input is greater than
zero then the output is equal to the
input I know it's a little confusing but
in the next slides where we show the
relu function it will become clear
similarly hyperbolic tangent this is
similar to sigmoid in terms of the shape
of the function however while sigmoid
goes from 0 to one hyperbolic tangent
goes from Min -1 to 1 and here again the
increase or the change from Min -1 to 1
is gradual and not like threshold or
step function where it happens
instantaneously so let's take a little
detailed look at some of these functions
so let's start with the sigmoid function
so this is the equation of a sigmoid
function which is 1 by 1 + e the^ of - x
so X is the value that is the input it
goes from 0 to -1 so this is sigmoid
function the equation is 5X = 1 by 1 +
e^ - x and as you can see here this is
the input on the x- axis as X is the
value is coming from in fact it can also
go negative this is negative actually so
this is the zero so this is the negative
value of x so as X is coming from
negative value towards zero the value of
the output slowly as it is approaching
zero it it slowly and very gently
increases and actually at the point let
me just use a pen at the point here it
is it is 0.5 it is actually 0.5 okay and
slowly gradually it increases to one as
the value of X increases but then as the
value of X increases it tapers off it
doesn't go beyond one so that is the
speciality of sigmoid function so the
output value will remain between 0 and 1
it will never go below zero or above one
okay that so that is sigmoid function
now this is threshold function or this
is also referred to as a step function
and here we can also set the threshold
in this case it is that's why it's
called the threshold function normally
it is zero but you can also set a
different value for the threshold now
the difference between this and the
sigmoid is that here the change is Rapid
or instantaneous as the x value comes
from negative up to zero it remains zero
and at zero it pretty much immediately
increases to one okay so this is a
mathematical representation of threshold
function 5x is equal to 1 if x is
greater than equal to0 and 0 if x is
less than 0 so for all negative values
it is zero since we have set the
threshold to be zero so as soon as it
reaches zero it becomes one you see the
difference between this and the previous
one which is basically the sigmoid where
the increase from 0 to one is gradual
and here it is instantaneous and that's
why this is also known as a step
function threshold function or step
function this is a reu reu is one of the
most popular activation functions today
this is the definition of reu 5x is
equal to Max of X comma 0 what it says
is if the value of x is less than Zer
then 5x is um 0 the moment it increases
goes beyond zero the value of 5x is
equal to X so it doesn't stop at one
actually it goes all the way so as the
value of X increases the value of y will
also increase infinitely so there is no
limit here unlike your sigmoid or
threshold or the next one which is
basically hyperbolic tangent okay so in
case of reu remember there is no upper
limit the output is equal to either Zer
in case the value of x is negative or it
is equal to the value of x X so for
example here if the value of x is 10
then the value of y is also 10 right
okay so that is reu and there are
several advantages of reu and it is much
more efficient and uh provides much more
accuracy compared to other activation
functions like sigmoid and so on so
that's the reason it is very popular all
right so this is hyperbolic tangent
activation function the function looks
similar to sigmoid function the curve if
you see the shape it looks similar to
sigmoid function fun but the difference
between hyperbolic tangent and sigmoid
function is that in case of sigmoid the
output goes from 0 to 1 whereas in case
of hyperbolic tangent it goes from
minus1 to 1 so that is the difference
between hyperbolic tangent and sigmoid
function otherwise the shape looks very
similar there is a gradual increase
unlike the step function where there was
an instant increase or instant change
here again very similar to sigmoid
function the value changes gradually
from minus1 to 1 so this is the equation
of hyperbolic tangent activation
function yeah so then let's move on this
is a diagrammatic representation of the
activation function and how the overall
data or how the overall progression
happens from input to the output so if
we get the input from the input layer by
the way the neural network has three
layers typically there will be three
layers there is an input layer there is
is an output layer and then you have the
hidden layer so the inputs come from the
input layer and they get processed in
the hidden layer and then you get the
output in the output layer so let's take
a little bit of a detailed look into the
working of a neural network so let's say
we want to classify some images between
dogs and cats how do we do this this is
known as a classification process and we
are trying to use neural networks and
deep learning to implement this
classification so how how do we do that
so this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two hidden layers and what we
do is we provide labeled training data
which means these images are fed to the
network with the label saying that okay
this is a cat the neural network is
allowed to process it and come up with a
prediction saying whether it is a cat or
a dog dog and obviously in the beginning
there may be mistakes a cat may be
classified as a dog so we then say that
okay this is wrong this output is wrong
but every time it predicts correctly we
say yes this output is correct so that
learning process so it will go back make
some changes to its weights and biases
we again feed these inputs and it will
give us the output we will check whether
it is correct or not and so on so this
is a iterative process which is known as
the training process so we are training
the neural network and what happens in
the training process these weights and
biases you remember there were weights
like W1 W2 and so on so these weights
and biases keep changing every time you
feed these which is known as an Epoch so
there are multiple iterations every
iteration is known as an Epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so so once again
what is the input this input could be
like thousand images of cats and dogs
and they are labeled because we know
which is a cat and which is a dog and we
feed those thousand images the neural
network will initially assign some
weights and biases for each neuron and
it will try to process extract the
features from the images and it will try
to come up with the prediction for each
image and that prediction that is
calculated by the network is compared
with the actual value whether it is a
cat or a dog and that's how the error is
calculated so let's say there are 1,000
images and in the first run only 500 of
them have been correctly classified that
means we are getting only 50% accuracy
so we feed that information back to the
network further update these weights and
biases for each of the neurons and we
run this these inputs once again it will
try to calculate extract the features
and it will try to predict which of
these is cats and dogs and this time
let's say out of, 700 of them have have
been predicted correctly so that means
in the second iteration the accuracy has
increased from 50% to 70% all right then
we go back again we feed this maybe for
a third iteration fourth iteration and
so on and slowly and steadily the
accuracy of this network will keep
increasing and it may reach probably you
never know 90% 95% and there are several
parameters that are known as Hyper
parameters that need to be changed and
tweaked and that is the overall training
process and ultimately at some point we
say okay you will probably never reach
100% accuracy but then we set a limit
saying that okay if we receive 95%
accuracy that is good enough for our
application and then we say okay our
training process is done so that is the
way training happens and once the
training is done now with the training
data set the system has let's say seen
all these thousand images therefore what
we do is the next step like in any
normal machine learning process we do
the testing where we take a fresh set of
images and we feed it to the network the
fresh set which it has not seen before
as a part of the training process and
this is again nothing new in deep
learning this was there in machine
learning as well so you feed the test
images and then find out whether we are
getting similar accuracy or not so maybe
the accuracy May reduce a little bit
while training you may get 98% and then
for test you may get 95% but there
shouldn't be a drastic drop like for
example you get 98% in training and then
you get 50% or 40% with the test that
means your network has not learned you
may have to retrain your network so that
is the way neural network training works
and remember the whole process is about
changing these weights and biases and
coming up with the optimal values of
these weights and biases so that the
accuracy is the maximum possible all
right so little bit more detail about
how this whole thing works so this is
known as forward propagation which is
the data or the information is going in
the forward Direction the inputs are
taken weighted summation is done bias is
added here and then that is fed to the
activation function and then that is
that comes out as an output so that is
forward propagation and the output is
compared with the actual value and that
will give us the error the difference
between them is the error and in
technical terms the that is also known
as our cost function and this is what we
would like to minimize there are
different ways of defining the cost
function but one of the simplest ways is
mean square error so it is nothing but
the square of the difference of the
errors or the sum of the squares of the
difference of the errors and this is
also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
square error now there are different
ways of defining cost function it need
not always be the mean square error but
the most common one is this so you
define this cost function and you ask
the system to minimize this error so we
use what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known know as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate of the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate is very small
then it will probably take very long to
train whereas if the learning rate is
very high then it will appear to be
faster but then it will probably never
what is known as converge now what is
Convergence now we are talking about a
few terms here convergence is like this
this is a representation of convergence
so the whole idea of gradient descent is
to optimize the cost function or
minimize the cost function in order to
do that we need to represent the cost
function as this curve we need to come
to this minimum value that is what is
known as the minimization of the cost
function now what happens if we have the
learning rate very small is that it will
take very long to come to this point on
the other hand if you have large Higher
Learning rate what will happen is
instead of stopping here it will cross
over because the learning rate is high
and then it has to come back so it will
result in what is known as like an
oscillation so it will never come to
this point which is known as convergence
instead it will go back and forth so
these are known as Hyper parameters the
learning rate and so on and these have
to be those numbers or those values we
can determine typically using trial and
error out of experience we we try to
find out these values so that is the
gradient desent mechanism to optimize
the cost function and that is what is
used to train our neural network this is
another representation of how the
training process works and here in this
example we are trying to classify these
images whether they are cats or dogs and
as you can see actually each image is
fed in each time one image is fed rather
and these values of X1 X2 up to xn are
the pixel values within this image okay
so those values are then taken and for
each of those values a weight is
multiplied and then it goes to the next
layer and then to the next layer and so
on ultimately it comes as the output
layer and it gives an output as whether
it is a dog or a cat remember the output
will never be a named output so these
would be like a zero or a one and we say
Okay zero corresponds to dogs and one
corresponds to cats so that is the way
it typically happens this is a binary uh
classification we have similar
situations where there can be multiple
classes which which means that there
will be multiple more neurons in the
output layer okay so this is once again
a quick representation of how the
forward propagation and the backward
propagation works so the information is
going in this direction which is
basically the forward propagation and at
the output level we find out what is the
cost function the difference is
basically sent back as part of the
backward propagation and gradient desent
then adjust the weights and biases for
the next iteration this happens
iteratively till the cost function is
minimized and that is when we say the
whole the network has converged or the
training process has converged and there
can be situations where convergence may
not happen in rare cases but by and
large the network will converge and
after maybe a few iterations it could be
tens of iterations or hundreds of
iterations depending on what exactly the
number of iterations can vary and then
we say okay we are getting a certain
accuracy and we say that is our
threshold maybe 90% accuracy we we stop
at that and we say that the system is
trained the trained model is then
deployed for production and so on so
that is the way the neural network
training happens okay so that is the way
classification Works in deep learning
using neural network and this slide is
an animation of this whole process as
you can see the forward propagation the
data is is going forward from the input
layer to the output layer and there is
an output and the error is calculated
the cost function is calculated and that
is fed back as a part of backward
propagation and that whole process
repeats once again okay so remember in
neural networks the training process is
nothing but uh finding the best values
of the weights and biases for each and
every neuron in the network that's all
training of neural network consists of
finding the optimal values of the
weights and biases so that the accuracy
is
maximum human versus artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every niit and Nook we've gone
to the Moon uh we've got into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you're
a human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
tasks that require human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer where it's
identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give a bad
Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all autom
so as today's computers evolved it's
very quick and easy and affordable to do
this what is machine learning and deep
learning all about imagine this say you
had some time to waste not that any of
us really have a lot of time anymore to
just waste in today's world and you're
sitting by the road and you have a whole
lot of and a whole lot of time passes by
there's a few hours and suddenly you
wonder how many cars buses trucks and so
on passed by in the six hours now
chances are you're not going to sit by
the road for six hours and count buses
cars and trucks unless you're working
for the city and you're trying to do
City Planning and you want to know hey
do we need to add a new truck route
maybe we need a Bicycle Link we have a
lot of bicyclists here that kind of
thing so maybe City Planning would be
great for this machine learning well the
way machine Learning Works is we have
labeled data with
features okay so you have a truck or a
car a motorcycle a bus or a bicycle and
each one of those are labeled it comes
in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle let's look a little bit
more in depth on this in the model here
it actually the features we're looking
at would be like the tires someone sits
there and figures out what a tire looks
like takes a lot of work if you try to
try to figure the difference between a
car tire a bicycle tire a motorcycle
tire uh so in the M machine learning
field this could take a long time if
you're going to do each
individual aspect of a car and try to
get a result on there and that's what
they did do that was a a very this is
still used on smaller amounts of data
where you figure out what those features
are and then you label them deep
learning so with deep learning one of
our Solutions is to take a very large
unlabeled data set and we put that into
a training model using artificial neural
networks and then that goes into to the
neural network itself when we create a
neural network and you'll see um the
arrows are actually kind of backward but
uh which actually is a nice point
because when we train the neural network
we put the bicycle in and then it comes
back and says if it said truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns once
you've trained the neural network you
then put the new data in and they call
this test T in the model so you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28 by 28 pixels
that's a lot of information there um
could you imagine trying to guess that
this is the bicycle image by looking at
each one of those pixels and trying to
figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in is all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and outcomes and says okay this
is a
bicycle a lot of times they call this
the magic Black Box why CU as we watch
it go across here all these weights and
all the math behind this and it's not
it's a little complicated on the math
side you really don't need to know that
when you're programming or doing working
with the Deep learning but it's like
magic you you don't know you really
can't figure out what's going to come
out by looking what's in each one of
those dots and each one of those lines
are firing and what's going in between
them so we like to call it the magic box
uh so that's where deep learning comes
in and in the end it comes up and you
have this whole neural notwork it comes
up and it says okay we fire all these
different from pixels and we connects
all these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning machine learning
we're going to take a step into machine
learning here and you'll see how these
fit together in a minute the system is
able to make predictions or take
decisions based on past data that's very
important for machine learning is that
we're looking at stuff and based on
what's been there before we're creating
a decision on there we're creating
something out of there we're coloring a
beach ball we're telling you what the
weather is in Chicago what's nice about
machine learning is a very powerful
processing capability it's quick and
accurate outcomes so you get results
right away once you program the system
the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's it's like a
magic box performance improves with more
data so the more data the Deep learning
gets the more it gives you better
results it's scalability so you can
scale it up you can scale it down you
can increase what you're looking at
currently you know we're limited by the
amount of computer processing power as
to how big that can get but that
envelope continually gets pushed every
day on what it can do problem solved in
an end to end method so instead of
having to break it apart and you have
the first piece coming in and you
identify tires and the second piece is
identifying uh labeling handlebars and
then you bring that together that if it
has handlebars and tires it's a bicycle
and if it has something that looks like
a large Square it's probably a truck the
neural networks does this all in one
network you don't really know what's
going on in all those weights and all
those little bubbles uh but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming in into their own
best features are selected by the system
and it this is important they kind of
put it's on a bullet on the side here
it's a subset of machine learning this
is important when we talk about deep
learning it is a form of machine
learning there's lots of other forms of
machine learning data analysis but this
is the newest and biggest thing that
they apply to a lot of different
packages and they use all the other
machine learning tools available to work
with it and it's very fast to test um
you put in your information you then
have your group of uh test and then you
held some aside you see how does it do
it's very quick to test it and see
what's going on with your deep learning
and your neural network are they really
all that different H AI versus machine
learning versus deep learning concepts
of AI so we have concepts of AI you'll
see natural language processing uh
machine learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or AI in machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books that goes through some deep
learning the Digital Data is analyzed
text mining comes through the particular
book you're looking for for maybe it's a
genre books is identified and in this
case uh we have a robot that goes and
gives a book to the patron I have yet to
be at a library that has a robot bring
me a book but that will be cool when it
happens uh so we look at some of the
pieces here this information goes into
uh as far as this example the
translation of the handwritten printed
data to digital form that's pretty hard
to do that's pretty hard to go in there
and translate hundreds and hundreds of
books and understand what they're trying
to say if you've never read them so in
this case we use the Deep learning
because you can already use examples
where they've already classified a lot
of books and then they can compare those
texts and say oh okay this is a book on
automotive repair this is a book on
robotic building the Digital Data is in
analyzed then we have more text mining
using machine learning so maybe we'd use
a different program to do a basic
classify uh what you're looking for and
say oh you're looking for auto repair
and computers so you're looking for
automated cars once it's identified then
of course it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as the big
picture how does this compare to The
Human Experience in either uh doing the
same thing as a a human we do or it it
does it better than us and machine
learning which has a lot of tools uh is
something that learns from data past
experiences it's programmed it's uh
comes in there and it says hey we
already had these five things happen the
six one should be about the same and
then uh then there's a lot of tools in
machine learning but deep learning then
is a very specific tool in machine
learning it's the artificial neural
network which handles large amounts of
data and is able to take huge pools of
experiences pictures and ideas and bring
them together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the uh backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the processing
machine learning uh spam detection very
common machine learning doesn't really
have have the human interaction part so
this is the part where it goes and says
okay that's a Spam that's not a Spam and
it puts it in your spam
folder search engine result refining uh
another example of of machine learning
whereas it looks at your different
results and it Go and it uh is able to
categorize them as far as this had the
most hits this is the least viewed this
has five stars um you know however they
want to waight it uh all exam good
examples of machine learning and then
the Deep learning uh deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so it's able to
look at the exit in the middle and it
goes okay we want to know what that is
in French and it's able to push that out
in French French and learn how to do
that and then we have chatbots um I
remember when my Microsoft first had
their little paperclip um boy that was
like a long time ago that came up and
you would type in there and chat with it
these are growing you know it's nice to
just be able to ask a question and it
comes up and gives you the answer and
instead of it being where you just doing
a search on certain words it's now able
to start linking those words together
and form a sentence in that chat box
types of AI and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to reiter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at like a neural network
that's been programmed to identify cars
it doesn't remember all those pictures
it has no memory as far as the hundreds
of pictures you process through it all
it has is this is the pattern I used to
identify cars as the final output for
that neural network we looked at so when
they talk about limited memory this is
what they're talking about they're
talking about I've created this based on
all these things but I'm not going to
remember any one
specifically theory of Mind systems
being able to understand human emotions
and how they affect decision-making to
adjust their behaviors according to
their human understanding this is
important because this is our pagemark
this is how we know whether it is an
artificial intelligence or not is it
interacting with humans in a way that we
can understand uh without that
interaction is just an object uh so we
talk about theory of mind we really
understand how it interfaces that whole
if you're in web development user
experience would be the term I would put
in there so the theory of mind would be
user experience how is the whole UI
connected together and one of the final
things as we get into artificial
intelligence is system being aware of
themselves understanding their internal
States and predicting other people's
feelings ings and act appropriately so
as artificial intelligence continues to
progress uh we see ones are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people uh if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time uh to help your
life and help you grow that'd be the
self-awareness as being able to know all
those different things types of machine
learning and like I said on the last
slide this is very important this is
very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first one is supervised learning systems
are able to predict future outcome based
on past dat data requires both an input
and an output to be given to the model
for it to be trained so in this case
we're looking at anything where you have
100 images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them is
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees all we told it was hey separate
this picture by things that don't match
and things that do match and come
together and finally there's
reinforcement learning systems are given
no training it learns on the basis of
the reward punishment it received for
performing its Last Action it helps
increase the efficiency of a tool
function or a program reinforced
learning a reinforcement learning is
kind of you give it a yes or no yes you
gave me the right response no you didn't
and then it looks at that and says oh
okay so based on this data coming in uh
what I gave you was a wrong response so
next time I'll give you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the the many tools and so
they we're grouping a ton of machine
learning tools all together linear
regression K means clustering there's
all kinds of cool tools out there you
can use in machine learning enables
machines to take decisions to make
decisions on their own based on past
data enables machines to make decisions
with the help of artificial neural
networks so it's doing the same thing
but we're using an artificial neural
network as opposed to one of the more
traditional machine learning tools needs
only a small amount of training data
this is very important when you're
talking about machine learning they're
usually not talking about huge amounts
of data we're talking about maybe your
spreadsheet from your business and your
totals for the end of the year when
you're talking about neural networks you
usually need a large amount of data to
train the data so there's a lot of
training involved if you have under 500
points of data that's probably not going
to go into machine learning or maybe
have like the case of one of the things
500 points of data and 30 different
fields it starts getting really
confusing there in artificial
intelligence or machine learning and the
Deep learning aspect really shines when
you get to that larger data that's
really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop no
problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data
we're talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you the problem is divided into
parts and solved individually and then
combined so machine learning you usually
have all these different tools and use
different tools for different parts and
the problem is solved in an end to end
manner so you only have one neural
network or two neural networks that is
bringing the data in and putting it out
it's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem you might have
solving one piece of the puzzle with
regular machine learning and most of
machine learning tools out there they
take longer to test and understand how
they work and with the Deep learning
it's pretty quick once you build that
neural network you test and you know so
we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence be using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe you'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthc care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant I'm
excited about that first we'll
understand the basics and starting with
the basics we'll start with what is deep
learning so deep learning often touted
as an attempt to replicate human brain
learning operates through intricate
mathematical functions enabling
computers to perform tasks similar to
humans for instance it underpins
groundbreaking Technologies like
driverless cars as well as voice
activity assistance such as CI and
Amazon Alexa in practice deep learning
empowers computers to learn directly
from various data formats such as images
text or audio through this process
computer model achieve remarkable
accuracy occasionally surpassing human
level performance it's not just about
mimicking human cognition it's about
harnessing the immense computational
power to expect vast amounts of data and
extract meaningful patterns the backbone
of deep learning consist of neural
networks interconnected nodes that
process data through layers each layer
refining the representation of the data
training these networks involve
adjusting parameters to minimize errors
and improve accuracy a process
resembling the human learning experience
Albert own a vastly accelerated scale
while data and computational challenges
exist the field continues to evolve
research and innovation are driving deep
learning forward opening doors to
improved image recognition natural
language understanding medical
Diagnostics and more the promise of
unsupervised learning and the
integration of deep learning into
diverse Industries paints an exciting
future for this transformative
technology impacting the way we interact
with machines and pushing the boundaries
of what's possible so this was all about
the basics and about the Deep learning
now we'll see why we should learn deep
learning so it's a game changer in
various aspects offering state of the
art performance scalability with data
reduced feature engineering and
transferability across task firstly deep
learning shines where traditional
machine learning algorithms struggle
speech recognition image classification
and object detection are areas with deep
learning algorithms like CNN
convolutional neural networks and recent
neural networks RNN Excel due to their
specialized architectures pushing the
boundaries of what achievable moreover
as data becomes abundant deep learning's
performance continues to improve unlike
classical machine learning algorithms
which plate in performance with more
data deep learning algorithms thrive on
it offering a scalable solution for the
ever expanding data sets another
significant Advantage is the reduced
need for intricate feature engineering
traditional methods often require
complex manual feature extraction in
deep learning the emphasis is on
learning directly from the data reducing
the burden of feature engineering and
making it more accessible for beginners
additionally deep learning models are
transferable pre-rain networks such as
vgg16 rest Nets and mobile net can be
leveraged as feature extraction tools
for related task this accelerates model
training and enhances performance
especially when faced with limited data
and computational resources in a sense
deep learning andow you with cutting it
skills enabling you to tackle complex
problems scale with data and Achieve
impressive results without the need for
extens ensive feature engineering its
transferability across task provide a
strategic Advantage making it an
invaluable skill for modern Ai and
machine learning practitioners so these
were the points that show us why we
should learn deep learning and before
moving on to the railro and before
moving on to the road map to become a
deep learning engineer we have a course
offering for you that is if you are one
of the aspiring deep learning
enthusiasts looking for online training
or a professional who El assits to
switch careers in deep learning by
learning from the experence words then
try giving a short to celtech a ml boot
camp that is in collaboration with
simply learn and the link to this boot
camp is mentioned in the description box
below that will navigate you to the Boot
Camp Page where you can find a complete
overview of the program being offered
now moving on to the road map to become
an deep learning engineer so learning
deep learning as a beginner involves
several essential steps each building
upon the previous to develop a
comprehensive understanding and
practical skills in this field so the
first is learn and master python so
python is the foundation for many deep
learning libraries and Frameworks start
by mastering python syntax data
structures and basic programming
Concepts it's essential to be
comfortable with python as you will use
it extensively for coding deep learning
models and moving on to the next step
that is mathematics for deep learning
develop a solid understanding of the
mathematical Concepts behind deep
learning this includes linear algebra
that includes vectors matrices
operations then have calculus in that we
have differentiation integration and
then we have statistics that includes
probability and distributions these
concepts are vital to grasp the inner
workings of neural networks and then we
have neural networks the third step that
is dive into the core of deep learning
understanding neural networks explore
topics like loss functions that is how
the model measures its performance
activation functions that is which add
nonlinearity to the network then we have
weight initialization that is critical
for modal convergence and the vanishing
or exploding gradient problem that is a
challenge in deep networks then we have
the next step that is architectures so
familiarize yourself with various neural
network architectures start with feed
forward neural networks which form the
basis for many models progress to Auto
encoders convolutional neural networks
CNN for image task recent neural
networks RN for sequential data
Transformers for NLP task simes networks
for similarity comparison generative ADV
veral networks GN for generating data
and explore evolving architectures like
neat neuro evolution of augmenting
topologies and now moving on to the next
step that is learning tools and
Frameworks so learn to work with popular
deep learning tools and Frameworks
familiarize yourself with tensor flow P
torch caras and ml flow these tools
simplify model development and
management so after this step you should
learn the model optimization so
understand techniques for model
optimization this includes distillation
that is knowledge transfer from large
models to smaller ones quantization
reducing model size without significant
loss of performance and neural
architecture search that is automated
methods to find Optimal Network
architectures then you should have
Hands-On projects to practice Z that is
apply what you have learned by working
on Hands-On projects start with simple
task and gradually tackle more complex
problems implement the different
architectures you have learned use the
Frameworks and experiment with model
optimization techniques by following
this structured approach you will build
a strong foundation in deep learning
enabling you to create train and
optimize neural networks for various
tasks the combination of theoretical
understanding and practical experience
gain from Hands-On projects will equip
you to explore Advanced topics and
contribut to the exciting field of deep
learning and the field of deep learning
is rapidly evolving so stay updated with
the latest research papers new
techniques and emerging Technologies
engage with the AI community Through
forums blogs and conferences and
consider joining AI focused online
communities and attend conferences like
neural PS cvpr or icml and there you
have it the Deep learning engineer road
map remember this journey requires
dedication continuous learning and a
passion for solving complex problems
whether you are aiming to work in
Industry research or both this road map
will set you on the right path then
accelerate your career in Ai and ml with
a comprehensive post-graduate program in
artificial intelligence and machine
learning so enroll now and unlock
exciting Ai and machine learning
opportunity the link is mentioned in the
description box below deeper into
looking at the theory behind the neural
network and we kind of flip back and
forth between these because there's two
huge aspects of it one is from the
outside what are you is seeing and
what's going on from the inside so you
can find to do what you need to do and
give the best results you can and we
start off with uh what do we feed we
feed an unlabeled image to a machine
which identifies it without any human
intervention and so you can see here we
have a circle that comes in at 784
pixels and it comes in by 28 by 28 and
you can see how it colors in the um the
circle on there and we put a triangle in
the Triangle in also comes in as 28x 28
and it has 784 pixels so you'll see
between these two both of them are
784 pixels this machine is intelligent
enough to differentiate between the
various shapes so that's what we want to
use our neural network to do is to say
hey this is a circle this is a triangle
that's more of a categorical you can
also do a regression model where you're
actually putting out float value or a
numerical value we'll be looking at the
true false or the categorical model
mostly because that's where you usually
start at the different there is no real
difference when you as far as the way
the internal functioning goes when you
start flipping between them other than
well we'll talk about about that in just
a minute so you can actually go between
the two quite easily and the neural
network provides this capability so
we're going to use this capability to
look between those two one of the things
I want you to note in here is that we're
looking at 784 pixels we're looking at
784 inputs that's very different than
stock with a high low or last year's
cells based on date we're looking at
just a couple of numbers and they're
very clear they're numbers they're very
clear what they are which is something
you'd put into a machine learning linear
regression model this is a step up from
that in that we're looking at complex
patterns and how do you figure those
complex patterns out so a neural network
is a system modeled on the human brain
and we looked at that comparing the two
let's go ahead and look deeper into the
neural network itself we have our inputs
coming in so the inputs are fed to a
neuron that processes the data and gives
us an output input and output this is
the most basic structure of a neural
network known as a perceptron so if you
see the term perceptron that's what
we're talking about we're talking about
this single node mode that has inputs
and an output however neural networks
are usually much more complex let's
start with visualizing a neural network
as a black box and I always love that
symbol it's a black box it's kind of
magical we have our inputs coming in and
we want certain outputs the Box takes
inputs processes them and gives an
output let's have a look at what happens
within this box and you can see me there
in my uh secret agent getup I got my
hidden hood and everything I guess I'm
part of the black skull or something
like that group uh so let's take a look
at what happens within this magic box
and remember we're skipping back and
forth between the theory of what's going
on in the box which you have to know how
to fine-tune and how to build versus
looking at it from the outside we're
programming this box and we have an
input and an output to the box as a
whole within the Box exists a network
that is a core of deep learning and you
can see here we're showing one layer and
we have our grid coming in the network
consists of layers of neurons each
neuron is associated with a number
called the bias and you can think of the
bias uh if you overly simplify this and
we're doing a linear regression model
this is your Y intercept in your ukian
geometry you have to have something that
offsets it and so you always have a bias
in these cells neurons of each layer
transmit information to neurons of the
next layer over channels and so you can
see each of our layers going through
from left to right these channels are
associated with numbers called weights
these weights along with the biases
determine the information that is passed
over from the neuron to neuron so just
like the bias is your Y intercept in
ukian Geometry you could look at the an
one weight remember this is very
complicated so we're not looking at just
one weight you can look at the weight as
your slope of the line or if you're
doing xal uh my y + C it' be the M value
neurons of each layer transmit
information to neurons of the next layer
and you can see here as they light up
going across into the final layer and
then to the output and in this case the
output is going to be either uh a square
in this one or it might light up the
other one which is a circle the output
layer emits a predicted output so in
this case we're looking at a
classification uh true false is it a
circle is it a triangle is it a square
let's now go deeper what happens within
the neuron so we're going to dig deeper
and start getting a little bit closer to
some of the math don't worry you don't
have to be a calculus expert or know
your differential equations even though
this is one giant differential equation
you don't need to understand those to
understand what's going on within each
neuron the following operations are
performed the product of each input and
the weight of the channel it's passed
over is found this is simply addition
we're going to sum up the weight times
the output from the previous channel and
plus the bias some of the weighted
products is computed this is called the
weighted sum bias unique to the neuron
is added to the weighted sum the final
sum is then subjected to the particular
function and we'll discuss those that
particular function that part is really
important because those functions uh
have a huge impact on how well your
model performs under different
conditions the final sum is then
subjected to a particular function this
is the activation function so if you
ever hear the term activation function
that's what we're talking about what
activates this cell and what doesn't as
we dig deeper into activation function
an activation function takes the
weighted sum of the input as its input
adds a bias and provides an output and a
lot of times you'll actually see one
formula for the sum of the weight the
weighted sum and the bias you'll just
see that as a single line of everything
added together and here we broken it
apart because it makes it clear that
this bias is not computed the same as
the weighted sums here are the most
popular types of activation function and
I always find these interesting because
at one point I was sitting at a table
with a gentleman who was finishing his
PhD he was in his last year and he said
he went through all this stuff and he
ended up just trying the four different
activation functions on this particular
problem he was working on so knowing the
math behind it doesn't necessarily mean
you're going to know right away uh so
even somebody who might have a PhD and
be doing the calculations on this comes
back out of it and ends up just trying
the different uh activation functions to
see what's going to make a difference
and a lot of times that's a final step
that's the kind of thing where you built
your whole model you've come back and
you're like wait a minute can I do a
better deal with a sigmoid function or
the threshold or the rectifier knowing
what they're doing is important so you
can explain it to somebody else and
again you probably do this on a small
set of data if you're working with big
data uh you don't want to take down the
full server Farm just to test out your
three different series you take a small
portion of that data test it and then
you put it through to the big data so
let's take a look at this we have the
sigmoid function and it's used for
models where we have to predict the
probability as an output it exists
between 0er and one and you'll see
that's true of all of our um activation
functions we're working with either the
cell's on or off it's true or false and
there might be a little variation in
there which as an output could be used
to compute uncertainty in your solution
so if you're getting a 7 with this
activation function it might be well I'm
not sure if that's really a square or
I'm not sure that's really a triangle
and that might be a flag for it to be
looked at by a human Observer at least
in today's models where we're at right
now and you can see here we have the
formula simply equals 1 over 1 + e the
minus X where X is your value coming in
and it's going to give you a result that
looks very similar to the graph on there
which is somewhere between zero and one
um and right in the middle you can see
that there's a huge uh kind of you can
go through all the different values and
uncertainties involved so the sigmoid
function is probably the default of most
of them uh the next one is a threshold
function it is a threshold based
activation function if x value is
greater than a certain value the
function is activated and fired else not
pretty straightforward yes no true false
um I don't want to test for
improbabilities I just want a straight
answer I don't want to know if there's a
partial value on there it either is true
or it's false and the rectifier function
it is the most WID ly used activation
function I would debate that um
rectifier is pretty common one although
I see the the sigmoid function is used
to be the basic one but it's up there
the rectifier function is very commonly
used because the output of x if x is
positive and zero otherwise and you can
see here again just like um uh it's
either you kind of get a value going up
there so max of X of zero so it's it's
again it's like the threshold function
yes no true false uh it's either zero or
it's uh some kind of progressive value
and then we have have the rectifier
function I would argue with this because
the sigmoid function used to be the most
common one but with the rectifier
function uh it now says it is the most
commonly used or widely used activation
function and gives an output of x if x
is positive and zero otherwise this is
kind of nice because it now says
absolutely not or it gives you a value
of probability now when I say a value of
probability be very careful there I'm
not saying that it's going to tell you
this is 75% chance of being a circle I'm
going to tell you that it says hey if
this say says 0.1 it probably needs to
be looked at or0 2 or 3 it's going to
depend on your data as to what that
value means in general that just means
it's flagging it that if it's not a one
then Chances Are needs to be looked at
by a person and re-evaluated and there's
a hyperbolic tangent function this
function is similar to sigmoid function
is bound to a range of minus one to one
so you can see there's our 1 - eus 2x
and 1 plus over 1+ eus 2x again it's
very similar to the sigmoid function the
bonus of the hyperbolic function is you
have that variable coming through the
middle so again you can look at it and
you have a little bit more weight as far
as you can process that down the line
that's a little bit more advanced than
than what we're looking at right now and
a lot of times it's not even necessary
in a lot of our different uh uses for
these activation functions now we looked
at activation functions and I kind of
said those are a little bit like a black
box because even if you know all the
math a lot of times you end up just
playing with them to find out what works
and it also depends on what model you're
working with whether you need a flat yes
no true false or you need to have
something in the middle that says hey
this isn't quite a one you might need to
process this with the human intervention
and you can look at that one example
would be self-driving cars you don't
want a car to be yes no I'm going to go
through the the light you want it to be
like okay if it's uh almost yes maybe we
stop and have human intervention so we
don't get an accident cost function is
something you can really see and measure
and is very important the cost value is
the difference between the neural Nets
predicted output and the actual output
from a set of labeled training data so
we have our group of data that's a
square circle and since we're looking at
geometrical shapes we've had somebody
already labeled that data they've
already said this is a triangle this is
a square and so if this is coming up and
it's giving is it's saying a square is a
triangle and it's saying a triangle is a
circle the output is wrong and so that
output can then be measured in the
versus the actual output and that's the
cost uh you might also hear this as
error because that's the error value
value being returned how far off is it
and what we're looking for is the least
cost or the least error value and it's
obtained by making adjustments to the
weights and biases iteratively
throughout the training process and this
this is called back propagation and
we're going to look in that a little
deeper as we look into an example it's
really hard to see when you're just
looking at arrows without actual numbers
and where that flow is coming from but
you can look at this is here's our
inputs they put out a prediction the
prediction comes out and says hey we've
already labeled this data CU we're in
training mode and the training data is
off this is the cost can we send that
error or that cost back and adjust those
weights and we do it in very small
increments across large amounts of data
so that those weights minimize that cost
or that error but what happens Within
These neurons so let's look at a little
example of this kind of helps if you
have some kind of visual let's build a
neural network that predict bike prices
based on a few of its features and we'll
see here we have our CC our mileage and
our abs and these are our three input
layers and then we have the bike price
and the output layer now it doesn't do
us very good to just uh pump it in from
the beginning and pump it out and to be
honest I would use a machine learning
linear regression model on this since
these are just straight numbers but
because we want a simple example we're
going to put this through and show you
as a neural network what that looks like
and we got to put a hidden layer in
there the hidden layer helps in
improving the output accuracy and you
could look at this as a bunch of ores so
it might say hey when we compare these
three values on the first hidden layer
neuron we're looking at one set of
features and then we might weigh them in
the second one so these are a bunch of
different ores kind of how the math
comes out in behind the scenes and then
they go out of course to the bike prer
or the output layer and each of the
connections have a weight assigned with
it and you'll see here we have a mileage
CC with the weight one and weight two
going into our first neuron and you'd
also have your abs going in there and so
X1 * weight 1 + X2 time weight 2 plus
the bias of one and step two is our
activation the activation function
coming in there when does this fire and
the neuron takes a subset of the inputs
and processes it and then we go through
we do that with the um second hidden
layer neuron and the third one and so on
so you process each layer in order going
forward now when I told you this is in
its infant stage they now have neurons
that fire into the same layer or back a
layer so that you now have a Time series
and there's all kinds of wild things
that they experimenting with on these
layers this basic setup has been around
since the mid 90s it's only now because
of our technology that it's open to
almost everybody to play with it and
that's why I see this is in an infant
stage in development is this basic math
is here but what we can do with it is
amazing and what they're actually doing
with all these different things is
amazing and so we're just at the
beginning of how to use all these
different Tools in our deep learning in
our neural networks uh and so once we
have our hidden layer computed the
information reaching the neurons in the
hidden layer is subjected to to the
respective activation function and so
each one of these fires an activation
output uh and then those are each
weighted to the final output layer so
the processed information is now sent to
the output layer once again over
weighted channels and you could look at
this as each one of these is um I always
look at this as like a group of people
they're all looking at the bulletin
board and the first person says this is
what I project sales for the company the
second person and the third and so on
and then their perspectives are weighted
based on their expertise so your
accountant might have a very high weight
where the um maybe your janitor has a
very low weight cuz their expertise is
not in accounting and then that goes
into the output layer and once in the
output layer it goes uh the output which
is the predicted value is compared
against the original value so now we
have our output layer and since we have
like already a list of uh bikes with
their the different setups and what
their value is we can now generate an
error from this the cost function
determines the error and prediction and
reports it back to the neural network so
this is the cost this is how far off it
is this is your error coming back and as
you can see this is a back propagation
going on so now our error is going in
reverse because we know we're not
completely correct on this particular
channel the weights are adjusted in
order to reduce the error so each time
we go back we are changing those weights
to reduce that error and we change them
in small increments you don't want to
fit one input remember you might have a
data pool with a terabyte of data you
don't want to solve for the first set of
data that comes in and that be the main
solution because everything else will be
off this is going to confuse you that's
also called a bias so we have the bias
in the cell where we're adding a value
the kind of like the Y intercept and we
have a bias of the whole neural network
which means it is weighted towards one
set of answers so we want to make small
changes in these weights so we don't
create a bias and the weights are
adjusted in order to reduce the error or
the cost the network is now trained
using the new weights once again the
cost is determined and back propagation
is continued until the cost cannot be
reduced any further so let's go ahead
and plug in values and see how our
neural network works so here we come in
here and initially our channels are
assigned with random weights this is
important because if you assign them all
with the same weight you might be able
to reproduce it but it turns out that if
I put all my weights as one or all my
weights is zero it takes longer to train
where if you have random weights they
already have like a little bit of
adjustment in order is built in and that
will give us a better answer and train
faster our first neuron takes a value of
mileage and CC as inputs so here comes
our computation whatever those inputs
are and we do that again with the second
neuron with those values coming in you
can see here we have weight three and so
on and then our third neuron coming down
and of course our fourth neuron so we're
adding all these different values coming
in here in our hidden layer the process
value from each neuron is sent to the
output layer over weighted channels so
again here's our weights coming in and
we have N1 N2 N3 and N4 once again the
values are subjected to the activation
function and a single value is emitted
as the output on comparing the predicted
value to the actual value we clearly see
that our Network requires training so
here we have it that our bike price uh
we put out we thought it was worth 2,000
on our random weights and the bike
actually was 4,000 on there guessing
that's not US dollar cuz that'd be a
very expensive bike but maybe it is
there's some $2,000 $4,000 bikes out
there the cost function is calculated
and back propagation takes place and
this is pretty simple you can look at
that as our um we're subtracting one
value from the other we Square it and
then we take half of that and that is
propagated back up and each layer
generates its own errors let's go back
one cuz you have your predicted Y and
your actual y that goes back to the
first layer and then based on the value
of the cost function certain weights are
changed so when we look at the next
layer that error is not the original
4,000 - 2000 squ / two this error is
based on the error of each cell
generated how far off is that cell as
far as its weights we're not going to
show you it's actually a very
complicated differential equation and
you can probably write it out if you
wanted to you just write out each
formula that goes into the next level
and you add them all together and you
can write it out all the way through
computers make it so you don't have to
and our neural network is considered
trained when the value for the cost
function is minimum so when we get our
error way down as low as we can that's
when our neural network is TR Tred and
there I mean just recently they've come
up with all kinds of different means for
measuring that particular value a little
bit beyond the scope of today's neural
network but you can actually you can
actually see it you know how far do you
do this until the um neural network
doesn't need to be trained anymore and
you can overt trade a neural network now
the tools that we're looking at
automatically let you know when to stop
which is really nice and that is just
like I said we're at the beginning
stages in neural networks it's just
really cool what they can do now and how
much of it's automated and how much of
it is experimental right now let's take
a look at gradient descent but what
approach do we take to minimize the cost
function so here we have a nice error
thing coming in this is our cost or our
error uh let's start with plotting the
cost function against the predicted
value and so you can see they fed in
multiple y's and these are the errors
coming in and the cost of each of these
inputs and changes going on note we
start at a random point on the curve so
usually you put in you know you pick up
your data and you randomly pick where to
start in your data a lot of times you
just run it from the beginning because
you're going through so much data is not
that big of a deal but you start with
one point going in so your forward
propagation goes through you're going to
go ahead and find your cost or your
error it points it on the curve and you
can see how we're plotting it right here
since the gradient at this point is
positive we may move right so we're
going to move a little bit to the right
on here and this time the gradient is
negative we move a little bit to the
left eventually we try out the point
where the gradient is zero this is the
least value of cost function you have to
be a little careful with this because
this particular I mean they make it look
nice and simple in this graph sometimes
these curves look like stair steps and
so there is global minimums and then
there is local there might be a local
point where the gradient is zero but
it's not the global one uh so it might
be way off to the left where it just
happens to step down a little bit and
you think you're in the right gradient
and with that we have all the right
weights and we can say our network is
trained so here we have um just some
major these are some of the big names
out there right now in development for
deep learning platforms tensorflow which
we'll actually do an example in in a
minute deep learning for J which is in
the Java platform uh so if you're a Java
programmer uh by the way his tensor flow
is accessed most people are using python
to access it but it is a system that's
kind of separate from a lot of the
programming languages which makes it a
lot more um flexible as far as use deep
learning for J is Java based and then
cross is just exploding right now and
this is interesting cross is uh working
with tensor flow it actually can sit on
top of tensor flow and it can also do
its own thing uh so if you're studying
deep learning you're getting into it you
want to know the basics of tensor flow
but you also are going to want to know
the upper level of carass sitting on top
of tensor flow we're just looking at
tensor flow today though in our example
and there's also torch on there there's
a bunch more that we didn't list on here
um even sklearn or the Side Package in
Python has a neural network you can
program a very basic one and it is the
same basic one that you could do in
tensor flow if you stripped everything
out of it and then tensor flow has a lot
of tools they've added in and so has
carass but we're going to be looking
specifically at tensorflow in our
example and tensor flow is an open-
Source tool used to define and run
computations on what they call tensors
very common language now so more and
more we see the term tensor as being a
standard in the uh deep learning
language and this was originally
developed by Google so let's dig a
little bit big in there what are tensors
tensors are just another name for arrays
so a tensor of Dimension five you can
see here we have a b k m q whatever so
it's an array coming in and the tensor
of Dimension 54 more like a picture very
common to see that in a picture you can
also see a tensor even more detailed in
a picture as we go to the next one
tensor of Dimension 333 this is 3D space
you might have a picture that also has
colors that might be the third dimension
you might have four dimensions because
you have both your grid and your
different color channels and your zplot
you can see where you can now process a
very high level set of data coming in
whether as an image or features they
could be features that have nothing to
do with images so there's a lot of stuff
you can do now with the tensors coming
in thus is where the term tensor flow
comes from so we have um right now the
tensorflow is the most popular library
in deep learning and I did mention
carass now works with tensorflow so
there's a lot of stuff you can do
between the two uh it's an open-source
software Library developed by gole
Google uh so they hit a roadblock and
they realized hey this is an infant
stage technology you know we thought it
was going to be the next greatest thing
and we were going to have a hold on it
but it's really infant as far as how
it's applied and what we can do with it
let's open source it so everybody can
work on it uh let's take it to the next
level and that's really what open source
does to a lot of these uh packages when
they release them and you can run on
either a CPU or a GPU so when we look at
the details if you have your graphic
processing units um what's nice about
those is they run a lot faster the
downside is you have to play with them a
little bit to get them up and running
and it's a hardware upgrade when we run
it I'll be running it in the CPU mode I
have played with it in my GPU on my
personal computer you know it does
increase the processing uh but I did run
into some version problems with my
Python and stuff like that and when I
did finally work it out I went back to
the CPU because it didn't increase my
speed enough for what I was working on
but in a larger group you might be able
to put that on if you're working with a
larger stack of computers you might want
to run it in the GP puu you can create a
data flow graphs that have nodes and
edges so there's our edges coming in we
didn't talk about edges but that's very
up oncoming way of looking at your
analytical data as how do different
nodes connect what do those edges look
like in between them and it's used for
machine learning applications such as
neural networks it is mostly a neural
network but they have all kinds of tools
which sit on top of our basic neural
network they have new stuff evolving
into the tensor flow Library so it's
very much uh just exploding great time
to jump into tensor flow CU there's all
kinds of cool things are doing with it
and all kinds of cool applications you
can now use a tensorflow for so let's
take a look at uh implementation in
tensorflow and we're going to build a
neural network to identify handwritten
digits using the U uh mnist database or
the mnist database and that stands for
modified National Institute of Standards
and Technology database it is a
collection of 70,000 handwritten digits
and the digit labels identify each to
the digits from 0 to 9 this is a cool
example cuz it's simple enough that you
could actually run this through some
basic machine learning categorizing
algorithms and train them and you'll get
about the same answer because again it's
it's simple grid the digits on the grid
don't have a huge amount of variation
like you would say an automated driving
car looking at the environment so you
can still do this with a lot of your um
different linear models and stuff like
that you can solve this and you'll get
about the same answer when I run a
comparison between tins or flow and
between some basic uh regression models
or category models uh in machine
learning they came up pretty even as far
as their output uh so this is kind of
where we start to see the complexity of
something coming in uh in this case a
tensor you know or a grid of uh
information where the Deep learning
model does as good as the regular models
and when you get past this kind of
complexity and features suddenly the
neural networks come up with better
answers better Solutions and a better B
build and that's why there's such a move
into neural networks is we live in a
complicated world and it's just really
cool we can do with this so the
handwritten digits from the um nist
database they come in the data set is
used to train the machine a new image of
a digit is fed and the digit is
identified um and if you've looked at
any of our other machine learning tools
where we're doing training uh where we
train our uh model to fit and then you
test it out this will look pretty
familiar uh and there is some tools out
there for say untrained categorizing uh
where it's just looking for features
that fit together so there are tools
that don't need that training but this
is where uh when we talk about neural
networks we do need to train them and
this is what we're looking at so for
this I'm going to use the Anaconda
Navigator just because it's a very nice
visual tool you might be in py charm or
one of your other idees for editing
python because we are looking at Python
tensorflow and under Anaconda we have
the notebook which is something we use
pretty regularly and they have the
Jupiter lab the Jupiter lab is the
Jupiter notebook but with tabs and a few
new features so we'll be using the
Jupiter lab today and under the
environment you'll want to go ahead and
and uh if you haven't yet uh you'll see
that I have a number of different setups
in here right now I have the python
version 36 and the tensor flow in this
case I have tensor flow 1.12 if we
scroll down you can see that uh here we
go tensorflow and it's version 1.2 and
in here if you haven't yet you'll need
to install those go in and just open our
terminal and U if you've never used the
anaconda or if you're in your other
thing you might have something simple
like pip cond is what I use for my
install and you can simply do cond
install tensor flow that should bring in
the most current version now when I
installed this a few months ago python
version I'm not going to run this
because I already have installed on here
python version 3.7 the newest one out
still had a couple glitches with the
tensor flow I believe they fixed it as a
writing of this but um I'm going to
stick with 36 just so I don't get any
surprises on there so this is python
version 36 with in tensor flow 1.12 on
here and if you haven't installed it yet
you also want to install numpy for this
example that's numbers python or uh Nu
mpy you can just simply run install on
there keep in mind if you're in Anaconda
uh and you've created one of these
environment specific to this keep with
cond if you're going to use cond if
you're going to use pip keep with Pip
don't install one package with Pip and
one under cond because that's how they
track those version numbers and how they
fit together and you can end up with a
problem they don't pip doesn't see cond
and vice versa uh so just keep that in
mind when you're running your installs
we'll go ahead and open up Jupiter lab
and we're going to launch that so here's
my Jupiter lab one of the really cool
features of Jupiter lab is you have tabs
now so you can open up multiple uh
notebooks this is nice cuz I have my
notes I'm working on and then our actual
window we're looking in and we'll go
ahead and zoom in a little bit here
there here we go so you have a nice U
hopefully easy to see fonts and then
we'll go ahead and do a simple or get
our Imports out of the way um and so
we're going to import our ensor flow as
TF uh that's pretty much a standard for
tensor flow numpy our numbers python as
py and we'll import our Matt plot
Library as PLT again these are very
common so if you see TF or py or PLT
this is a standard that most people use
do you have to no you could just do
import numpy instead of doing as py and
then from tensorflow do
examples. tutorials this is always nice
because they actually include data set
we're going to play with so we're going
to import input data so there's our data
coming in that's all we're doing is
telling it this is where it's coming
from and if we're going to tell where
it's coming from we need to go ahead and
create a variable with that information
in it and we'll just call this uh MN
nist or minced you know I don't really
know how they pronounce that I should
probably look that up it's a very common
data set to use and there's our input
data and we're going to read data sets
and this is um if you look at this we
imported input data from our tensorflow
and so this is a tensor flow read
statement for their tutorials so this
isn't like some special python setup
this is just their setup makes it easy
to pull it in so once we get into their
data sets we need to go and tell out
what kind of data set and again this is
what we brought in but it's going to be
the nince data and this part is very
important one hot equals true this means
that instead of importing a value from 0
to nine we evaluate the data set it's
going to bring it in as one hot when you
see one hot encoder we're flattening
that out and we have true false for zero
true false for one true false for two so
our output if you remember from our
output from the slide we did earlier uh
in this case I grab the one for bike
price doesn't really matter which one we
use this has one output so we have a
bike price on this we're going to have
instead of one output we're going to
have 10 outputs representing each of the
digits in there and this code really
isn't going to show us anything it's
good to see what we're actually looking
at so um let's go ahead and do a figure
ax equals go into our plot Library
subplots 10 comma 10 and that is if you
remember we talked about tensor tensor
being data coming in this is a 10x10
Grid or 100 pixels on there and if we're
going to display it uh let's go do k0
for I in range 10 just a simple Loop
through on the data let's do what is it
uh for J in range 10 and I actually
misquoted that 10 uh 10 x 10 is not the
actual size of the pixels uh the actual
pixels are going to be um we look at the
shapes we'll get into that in just a
second here we'll take a quick look at
the shape on there uh turns out they're
uh what are they are I believe 28x 28 uh
so let's take a look at that and we're
just going to plot these what are we
looking at what are we working with as a
data scientist you should always be
looking back at your data and seeing
what it looks like and get that human
perspective because you just never know
you know the the computer might put
something out that looks makes no sense
and uh at that point you want to go back
and reevaluate what you did uh so we're
going to go ahead and plot we're going
to plot 10 digit you know 10 of the
digits by 10 of the digits and here's
our ax we'll create the igj on our
subplots and we're going to do an image
show we're going to look at the training
image for images uh K and then we want
to reshape this we're going to reshape
this and we're going to reshape this 28x
28 that's how I knew I had it wrong it's
because I looked down my notes I was
like oh no that says 28 it's not 10 x 10
and I should know that already because
I've done enough messing with this data
set that I should remember uh but it's
28 by28 and the aspect we're going to do
is Auto and this is all if you look at
this here's our variable nist the nist
is coming from data set uh so this is
all part of the TF tensor flow learning
or examples tutorial in there and then
we'll go go ahead and do uh K plus
equals 1 so we just keep paging through
our different um images and let's see
what that looks like let's go ahead and
do a plot show uh and we'll go ahead and
run this so we can take a look and see
what we have here and so we have a nice
plot here and you can just see that we
have uh some random numbers showing up
in each one of these little subplots if
you're wanting a copy of this code put a
note down in the YouTube video and let
us know or come visit us at www.s
simplylearn outcom and we'll send you
out a copy of what we're working on and
get a copy of that for your own setup uh
so now we taking a look we can just see
we have here's our pictures that are
coming on we've plotted them so we have
an idea of what we're looking at let's
go ahead and uh print let's look at the
shape of the features uh so when we have
this we have our nist train images and
we'll do the shape on there let's take a
look and just see what we're looking at
uh as far as uh our count and everything
and so you can see here we have 55,000
that's basically how many images we have
and this by 784 and in this data set
there's also our labels so let's take a
look at that we have our nit train
labels shape let's take a look and see
what that looks like uh and there we
have 10 because there's 10 digits so we
brought in that's our output we're
looking at and so we have there we go
55,000 they match they should match
because you should have equal numbers in
both of those you know here's our data
end and here's our answer if you
remember this is a bunch of zeros and
with one each each one will be 00001
would be what letter four or something
like that so let's take a look at what
our one hot encoding did for the first
observation and this is when we're
exploring data you really want to dig in
there and just see what the heck am I
looking at so we're going to look at the
labels and this would be the first label
that comes up and we'll go ahead and run
this and we look at that you can see
this is what I'm talking about 0 0 or 1
is zero two is z three is z four is z 5
is 0 6 is z 7 equals 1 so our very first
label is a seven but our very first
label comes up that it's a seven and so
we don't have like 0 through n we have a
bunch of zeros and just the one to mark
it as a seven on here so now we' kind of
Lo a quick look at the data and in here
you might ask some questions like what
is 784 24 * 24 remember that's the size
of our grid on there or our tents are
coming in so 784 is a setup on there and
we've gone through all this viewing the
data we'll go ahead and start looking at
our tenser flow so let's take our X
variable this is going to be our
training set we'll do a placeholder and
then we're going to have these come in
as float now if I remember correctly
they're actually you know zero or one
for the values because they're either
but we have them coming in as a float
value and we have a little bit of a
shape coming in here and there's our 784
uh so we let it know that this is what's
what our input is for our tensor flow
and this is our training set so we'll
just put a label on there to help us uh
track that train set and then W and with
W we'll go ahead and do TF variables and
we'll do this as uh zeros variables to
have zeros and we'll set this as as 784
by 10 10 being the output 784 being our
number of variables in and this is our
weights remember we have a bias in there
too and I'll go back over this in just a
second as we see how that fits together
in our tensor flow and we'll do this one
um with our variables again we have 10
so we're going to do the bias we do it
the same kind of format and setup on
here and so we'll do that as has TF Z of
10 so we just create an array of 10
there and this is our bias so with these
three lines um and there's actually
they're coming out with the eager
execution which would bypass some of
what we're doing but this is important
to understand is the first thing you
have to do with tensor flow is we have
to allocate a space for the variables
and our TF placeholder and our TF
variable with our weights and our biases
this actually hasn't done anything yet
so all it is is placeholders that's why
it's okay to use zeros um you could have
just as easily used ones or anything
else and it wouldn't matter the next
stage is to go ahead and set up some of
the functions going on but before we do
that just note that this hasn't done
anything even if I execute it all it's
done is created placeholders until we do
the final initialization and so we need
to go ahead and set up we'll do y tf.
nn. softmax and the code for this is tf.
mole xw + B and this is our uh sum let's
just put a note here so we can keep
check what's going on we're finding
weighted sum of inputs plus the bias uh
so there's our plus b the bias and then
we need to go ahead keep um let's do
ycore and again another placeholder and
this one will set um it's actually will
put in as TF placeholder on here TF
placeholder float none 10 there's our
one hot encoder going on there so our 10
values coming out and we'll do a cross
entropy on here and this is going to be
minus TF reduce sum and we'll do y
here's our ycore which is remember we
have your y output and your actual
output uh so this will be our ycore
times the TF log of Y and then finally
um before we do the actual
initialization of all our variables
we'll set up our train step this equals
our gradient descent Optimizer very
important remember we looked at that
chart on our um uh slides and so we've
set up all these formulas and here's our
gradient descent Optimizer and as it
keeps looking it keeps looking for that
zero value that's what we're doing with
that particular formula so let's take a
look and see what we're doing here we
just put together all of our pieces for
tensor flow and you know the devil's in
the um details we have here our training
set coming in we have to put a
placeholder on there we have our
variables with their weights we have our
biases coming out and then we put in our
uh the weighted sum so here's uh summize
our summation here then we have our y
variable output so there's our Y how it
works and then of course the actual
output on there and then we have our
cross entropy coming in and that's our
minus tf. reduce sum the y * the TF log
of Y and then the training step gradient
descent Optimizer and we're using a 0.01
in this and we're going to minimize
cross entropy so we're going to let it
do all the work so once we've set up all
of these different layers we've
allocated for them we need to go ahead
and initialize them so we're going to do
an nit TF initialize and it's going to
be all variables uh one of the cool
things is they're in the process of
doing away with this so all these steps
would be bundled into one instead of
having to had placeholders you
initialize them in the same process
going on and then finally everything in
tensor flows based on your session now
this is changing that there's other
options to be able to run this but we
want to go ahead and do uh session
there's our TF which is a TF session and
then we want to go ahead and do session
run and what are we going to run well we
did initi ization of all our variables
uh so this is what we're running and
this is where actually once we do this
we actually create our tensorflow object
so this whole piece of code right here
is our tensorflow object we have our
input coming in with our weighted
variables coming in our soft Max for our
metanol going out how does it added
together for our y value uh then we have
the actual uh float value coming out
checking on our all the way down so you
can see all the different stages going
through that we're setting up um and
this is one of the reasons that a lot of
people like tensor flow is because you
can designate all these different pieces
one step at a time this is also one of
the reasons people don't like tensor
flow is because you have to designate
all the different layers coming down and
there's a lot of steps being made right
now to minimize this to make it either
easier to automate it or to allow you to
do more complicated things and all those
steps are still at play so it's worth
looking into the more advanced version
of what's going on with carassa on top
of tensor flow it's also important to
understand what's going on in these
individual levels if you're going to
play with them it's important to
understand hey what's going on with the
um uh finding the weighted sum of the
inputs plus the bias because there's
other ways to do that there's all kinds
of other tools in there now but this is
the basic setup that you want to do on a
tensor flow coming in and we want to go
ahead and just run and a knit our tensor
flow so let's go ahead and do that let's
run this we do get a warning here
because uh there's a move to use Global
variables this is one of the changes
they're making but as far as this
example is not going to make a
difference because we're doing once we
initialize it this is initializing our
variables and again these are only
placeholders up here until we initialize
them and I would highly suggest put a
note down there or or go over to Simply
learn.com and let them know and have
them email you a copy of the code so you
can actually play with this code right
here because this is the body of what's
going on in tensorflow this is the build
in neural networks and then once we've
done that now comes kind of the fun part
is we need to go ahead and train it uh
so we've created our tensor flow we've
created our Network and now we need to
go ahead and train it uh so let's put
together that training code and let's
just do uh for I in range uh 1,000 0 to
1,000 so we're just going to look at uh
the first 1,000 in our training and the
way we pull that data from our mint's
train next batch of 100 uh so you look
at this we're going to be doing groups
of 100 and then there's going to go
through a thousand of them this is very
important that tensor flow builds this
in this is one of the downsides of doing
sklearn or one of the older packages is
they don't let you batch groups in uh
they wanted to have it all up front and
then you have to build your own batch
programs right now uh this lets us go
ahead and do that you can see here we
have batch X ofs batch y ofs so there's
our X and our y you can look at this as
our training of X and our train of Y or
the data n and the answer n uh and then
we simply do our session run uh so
here's our session that we've created
we're going to run it and we we want to
do the train step remember we
initialized our train step up here in
our TF and so there's our train step
feed it's a dictionary dictionary coming
in which we're going to create right
here uh is X is our batch X of our
sample comma and our y underscore is
going to be our batch of our y sample uh
and so this goes through and we've now
hit the Run button and we've trained our
session we've trained this setup on here
and once we've trained it then we need
to go ahead and find how good our
accuracy was and actually start running
some predictions through there uh so
we'll go ahead and create a a correct
prediction and this is where our tf.
equal we'll use our argmax y of one and
TF argmax of Y of underscore of one to
help us get the correct predictions on
there and then we want to use that to
feed into an accuracy and so our
accuracy is going to be TF reduce uh
mean and we'll take that and we'll do um
a cast and this is the correct
prediction that we're sending in there
and it is a a float value keep it simple
and let's go ahead and print this out so
we can see where we're looking at uh so
what are we printing out uh we need to
do a session run this session runs going
to be on the accuracy where did accuracy
comes from this is our we're casting our
TF on there with the correct predictions
on that so here's our accuracy feed in
so it needs a dictionary for the data
coming in we're going to create our
dictionary and X just going to be our
nist test images and Y underscore there
is going to be our n test. test labels
let me just double check and make sure I
have that typed in there correctly there
we go and let's go ahead and run that
and see what comes up and we end up with
a
9165 for our accuracy which means our
trained neural network does a pretty
good job letting us know what these
different symbols are and guessing that
a seven and a three and a four uh
something that as humans we kind of take
for granted I even have trouble reading
this so I don't know if I would be able
like that first one I would sit there
for for a long time figuring out that's
a seven versus a two that could have
easily been a two to me seeing see we do
a pretty good job analyzing this data
and this is used to analyze something
very complicated on these images very
different than uh just a straight value
of uh cost of sales and here's our
return and our marketing uh we can now
create this nice neural network that
does all kinds of cool things let's move
on to the differences between tensor
flow caras and pyo the first difference
that we'll be looking at is called level
of API that there are two main types of
apis a lowlevel API and a high level API
API stands for application programming
interface a lowlevel application
programming interface is generally more
detailed and allows you to have more
detailed control to manipulate functions
within them on how to use and Implement
them while a high level API is more
generic and simple and provides more
functionality with one command
statements than a lower level a API
highlevel interfaces are comparatively
easier to learn and to implement the
models using them they allow you to
write code in a shorter amount of time
and to be less involved with the details
in this case tensor flow is a high and
low-level API pure tensorflow is a
low-level API while tensorflow wrapped
in kasas is a high level
API Keras in itself is a highlevel API
which uses multiple low-level apis as a
back end and simplifies the operation of
these lowlevel apis py torch is a
lowlevel API the next criteria that
we'll be looking at is speed tensor flow
is very fast and is used for high
performances kasas is slower as it works
on top of tensor flow not only does it
have to wait for tensorflow to finish
implementation it then starts its own
implementation meanwhile py torch works
at the same speed as tensor flow as both
of them are both both lowlevel apis now
kasas is a rapper class for tensorflow
and has added abstraction
functionalities on top of tensor flow
which make it slower than tensorflow and
py toch in computation speed both tensor
flow and py toch are almost equal and in
development speed kasas is faster as it
has built-in functionalities which can
significantly reduce your development
time the next difference is on the
architecture tensor flow is not not very
easy to use and even though it provides
kasas as a framework that makes it work
easier tensorflow still has a very
complex architecture which is hard to
use meanwhile kasas has a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in Keras significantly easier pouch on
the other hand also has a complex
architecture and the readability is less
when compared to kasas tensorflow uses
computational graphs which makes it it
very complex and hard to interpret but
it has amazing computational ability
across platforms py to is a little hard
for beginners but is really good for
computer vision and deep learning
purposes data sets and debugging tensor
flow works with large data sets due to
its high execution speed and debugging
is really hard intensive flow due to its
complex nature meanwhile kasas only
works with very small data sets as its
speed of execution is low
programs do not require frequent
debugging in caras as they are
relatively simpler and py toch can
manage high level tasks in higher
Dimension data sets and is easier to
debug than both kiras and tens oflow
next we'll be looking at ease of
development as we said before tensorflow
works with many hard Concepts such as
computational graphs and tensors which
means that writing code in tensor flow
is very hard it is generally used by
people when they are doing research work
and really need very specific
functionalities kasas on the other hand
provides a high level of abstraction
which makes it very easy to use it is
best for people who are just starting
out with python and machine learning py
toch is easier than tens oflow but is
still comparatively hard than Kos it is
not very easy to learn for beginners but
is significantly more powerful than just
plain car
ease of deployment tensor flow is very
easy to deploy as it uses tensorflow
serving tensorflow serving is a flexible
high performance serving system for
machine learning models designed for
production environments tensorflow
serving makes it easy to deploy new
algorithms and experiments while keeping
the same server architecture and apis
tensorflow serving provides outof thee
boox integration with tensorflow models
but can be easily extended to serve
other types of models and data in kasas
model deployment can be done with either
tensorflow serving or flask which makes
it relatively easy but not as easy as
you as it would be with tensor flow and
py py uses py mobile which makes
deployment easy but again for tensorflow
deployment is way easier as tensorflow
serving can update your machine learning
back end on the fly without the user
even realizing there's a growing need to
execute ml models on edge devices to
reduce latency preserve privacy and
enable new interactive use cases in the
past Engineers used to train models
separately they would then go through a
multi-step error prone and often complex
process to train the models for
execution on a mobile device the mobile
runtime was often significantly
different from the operations available
during training leading to inconsistent
developer and eventually user experience
all of these frictions have been removed
by pyo Mobile by allowing a seamless
process to go from training to
deployment by staying entirely within
the py toch ecosystem it provides an
endtoend workflow that simplifies the
research to production environment for
mobile devices in addition it paves the
way for privacy preserving features via
Federated learning
techniques at the end of the day the
question that really matters is which
framework should you use
Keras tensor flow or
py now tensor flow has implemented
various levels of abstraction to make
implementation of deep learning and
neural networks easy this has also made
debugging easier caras is simple and
easy but not as fast as tensorflow it is
more user friendly than any other deep
learning API however and is easier to
learn for beginners py on the other hand
is the preferred deep learning API for
teach features but it is not as widely
used in production as tensor flow is it
is faster but it has lower GPU
utilization at the end of the day the
framework that we would suggest that you
use is tensor
flow why while pych may have been the
preferred deep learning library for
researchers tsor flow is much more
widely used in day-to-day production P
torch is ease of use combined with the
default eager execution mode for easier
debugging destines it to be used for
fast hacky Solutions and smaller scale
models but tensor flows extensions for
deployment on both servers and mobile
devices combined with the lack of python
overhead makes it the preferred option
for companies that work with deep
learning models in addition the tensor
flow board visualization features offers
a nice way of showing the inner workings
of your model to say your
customers meanwhile between tens of flow
and kasas the main difference isn't in
performance tensorflow is a bit faster
due to less overhead but also the level
of control you would like kasas is much
easier to start with than plain tensor
flow but if you want to do something
with kasas that doesn't come out of the
box it'll be harder to implement that
tensor flow on the other hand allows you
to create any arbitrary computational
graph providing much more flexibility so
if you're doing more research type of
work tensor flow is the short route to
go due to the flexibility that it
provides then accelerate your career in
Ai and ml with a comprehensive
post-graduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below so many things out there that the
1.0 really needed so when we start
talking about tensor flow 1.0 versus 2.0
um I guess you would need to know this
for um a legacy programming job if
you're pulling apart somebody else's
code the first thing is that tensorflow
2.0 supports eager execution by default
it allows you to build your models and
run them instantly and you can see here
from tensorflow one to tensorflow 2 uh
we have um almost double the code to do
the same thing so if I want to do um
with tf. session or tensorflow session
um as a session the session run you have
your variables your session run you have
your tables initializer and then you do
your model fit um X train y train and
then your validation data your value yv
value and your epics and your batch size
all that goes into the fit and you can
see here where that was all just
compressed to make it run easier you can
just create a model and do a fit on it
uh and you only have like that last set
of code on there so it's automatic
that's what they mean by the eager so if
you see the first part and you're like
what the heck is all this session thing
going on that's tensorflow 1.0 and then
when you get into 2.0 it's just nice and
clean if you remember from the beginning
I said coros uh on our list up there and
uh cross is the highlevel API in tensor
flow 2.0 cross is the official high
level API of tensorflow 2.0 it has
incorporated cross as tf. cross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose the right
level of abstraction for your project
and uh we hopefully touch base a little
bit more on this sequential being the
most common uh form that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional is where you can split the
layer so you might have your input
coming in one side it splits into two
completely Mo different models and then
they come back together um and one of
them might be doing classification the
other one might be doing just linear
regression kind of stuff or neural basic
uh reverse propagation neural network
and then those all come together into
another layer which is your your uh
neural network reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into carass
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow coros setup
a third big change we're going to look
at is that in tensorflow
1.0 uh in order to use TF layers as
variables uh you would have to write TF
variable block so you'd have to
predefine that in tensor flow 2 you just
add your layers in under the sequential
and it automatically defines them as
long as they're flat layers of course
this changes a little bit as a more
complicated um tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have um a little bit more
on the scope of this and you can see how
tensorflow one asks you to do um these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary in tensor 2.0
so you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to pre-create
the um uh variable scope so if you ever
see the variable scope you know that
came from an older version and then we
have the last two which is our API
cleanup and the autograph uh in the API
cleanup tensor flow one you could build
models using TF Gans TF app TF contrib
TF Flags Etc in tensor flow 2 uh a lot
of apis have been removed and this is
just they just cleaned them up CU people
weren't using them and they've
simplified them and that's your TF app
your TF Flags your TF logging are all
gone uh so there's those are three
Legacy features that are not in 2.0 and
then we have our TF function and
autograph feature in the old version uh
10 tensorflow 10 the python functions
were limited and could not be compiled
or exported reimported so you were
continually having to redo your code and
you couldn't very easily just um uh put
a pointer to it and say hey let's reuse
this in tensorflow 2 you can write a
python function using the TF function to
mark it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python
syntax uh now we just threw in a new
word in you graph uh graph is not a
picture of a person uh you'll hear graph
x and some other things graph is what
are all those lines that are connecting
different objects so if you remember
from before where we had uh the
different layers going through
sequentially each one of those wh lined
arrows would be a graph x that's where
that computation is taken care of and
that's what they're talking about and so
if you had your own special code or
python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensor flow
toolkits in their hierarchy at the high
level we have our object oriented API so
this is what you're working with you
have your TF carass you have your
estimators this sits on top of your TF
layers TF losses TF metrics so you have
your reusable libraries for model
building this is really where ensor flow
shines is between the carass uh running
your estimators and then being able to
swap in different layers you can your
losses your metrics all of that is so
built into tensorflow makes it really
easy to use and then you can get down to
your lowlevel TF API um you have
extensive control over this you can put
your own formulas in there your own
procedures or models in there uh you
could have it split we talked about that
earlier so with the 2.0 you can now have
it split One Direction where you do a
linear regression model and then go to
the other where it does a uh neural
network and maybe each neural network
has a different activation set on it and
then it comes together into another
layer which is another neural network so
you can build these really complicated
models and at the low level you can put
in your own apis you can move that stuff
around and most recently we have the TF
code can run on multiple platforms and
so you have your CPU which is uh
basically like on the computer I'm
running on I have uh eight cores and 16
dedicated threads I hear they now have
one out there that has over a 100 cores
uh so you have your CPU running and then
you have your GPU which is your graphics
card and most recently they also include
the TPU setup which is specifically for
tensorflow models uh neural network kind
of setup so now you can export the TF
code and it can run on all kinds of
different platforms for the most um
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture uh we
have uh you can see on the left this is
usually where you start out with and 80%
of your time in data science is spent
pre-processing data making sure it's
loaded correctly and everything looks
right uh so the first level in tensor
flow is going to be your read and
pre-processed data your TF data feature
columns this is going to feed into your
TF Cross or your pre-made
estimators and kind of you have your
tensorflow Hub that sits on top of there
so you can see what's going on uh once
you have all that set up you have your
distribution strategy where are you
going to run it are you going to be
running it on just your regular CPU are
you going to be running it uh with the
GPU added in uh um like I have a pretty
highend graphics card so it actually
grabs that GPU processor and uses it or
do you have a specialized TPU setup in
there that you paid extra money for uh
it could be if you're and later on when
you're Distributing the package you
might need to run this on some really
high processors CU you're processing at
a server level for uh let's say you
might be processing this at a um a
distribut you're Distributing it not the
distribution strategy but you're
Distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done every minute
um and so you need that higher speed to
give them a um to give them a
recommendation or a suggestion so they
can buy more stuff off your website or
maybe you're looking for uh data fraud
analysis working with the banks you want
to be able to run this at a high speed
so that when you have hundreds of people
sending their transactions in it says
hey this doesn't look right someone's
scamming this person probably has their
credit card so when we're talking about
all those fun things we're talking about
saved model this is we were talking
about that earlier where a used to be
when you did one of these models it
wouldn't truncate the float numbers the
same and so a model going from one you
build the model on your com machine in
the office and then you need to
distribute it and so we have our tensor
flow serving Cloud on premium that's
what I was talking about if you're like
a banking or something like that now
they have tensor flow light so you can
actually run a denser flow on an Android
or an iOS or Raspberry Pi little
breakout board there in fact they just
came out with a new one that has a
built-in this just a little mini TPU
with the camera on it so it can
pre-process a video so you can load your
tensorflow model onto that um talking
about an affordable way to beta test uh
a new product uh you have the tensorflow
JS which is for browser and node server
so you can get that out on the browser
for some simple computations that don't
require a lot of heavy lifting but you
want to distribute to a lot of end
points and now they also have other
language bindings so you can now create
your tensorflow back in save it and have
it accessed from C Java go C rust r or
from whatever package you're working on
so we kind of have an uh overview of the
architecture and what's going on behind
the scenes and in this case what's going
on as far as Distributing it let's go
ahead and take a look at uh three
specific pieces of tensor flow and those
are going to be constant variables and
sessions uh so very basic things you
need to know and understand when you're
working with the tensor flow uh setup so
constants in tensor flow in tensorflow
constants are created using the function
constant uh in other words they're going
to stay static the whole time whatever
you're working with the Syntax for
constant uh value dtype 9 shape equals
none name constant verify shape equals
false that's kind of the syntax you're
looking at and we'll explore this with
our hands on a little more in depth uh
and you can see here we do zal tf.
constant 5.2 name equals x uh D type is
a float that means that we're never
going to change that 5.2 it's going to
be a constant value and then we have our
variables in tensor flow uh variables in
tensor flow are in memory buffers that
store tensors and so we can declare a
2x3 tensor populated by ones you can
also do constants this way by the way so
you can create a um an array of ones for
your constants I'm not sure why you do
that but you know you might need that
for some reason um in here we have V
equals tf.
variables and then in tensor flow you
have tf. ons and you have the shape
which is 23 which is then going to
create a nice 2x3 um array that's filled
with ones and then of course you can go
in there and their variables so you can
change them it's a tensor so you have
full control over that and then you of
course have uh sessions in tensorflow a
session in tensorflow is used to run a
computation graph to evaluate the nodes
and remember when we're talking about
graph or graph x we're talking about all
that information then goes through all
those arrows and whatever computations
they have that take it to the next node
and you can see down here uh where we
have import tensor flow is TF if we do
xal a tf. constant of 10 we do yal a TF
constant of 2.0 or 20.0 and then you can
do zal tf. variable and it's a tf. addx
comma y uh and then once you have that
set up in there you go ahead andit your
TF Global variables initializer with TF
session as session you can do a session
run knit and then you print the session
run
y uh and so when you run this you're
going to end up with of course the uh 10
+ 20 is 30 and we'll be looking at this
a lot more closely as we actually roll
up our sleeves and put some code
together so let's go ahead and take a
look at that and for my coding today I'm
going to go ahead and go through
anaconda and then I'll use specifically
the Jupiter Notebook on there and of
course this code is going to work uh
whatever platform you choose whether
you're in a notebook um the Jupiter lab
which is just the Jupiter notebook but
with tabs for larger projects we're
going to stick with Jupiter notebook pie
charm uh whatever it is you're going to
use in here uh you know you have your
spider and your QT console for different
programs
environments the thing to note um it's
kind of hard to see but I have my main
Pi 36 right now when I was writing this
tensor flow Works in Python version 36
if you have python version 37 or 38
you're probably going to get some errors
in there uh might be that they've
already updated it and I don't know it
and I have an older version but you want
to make sure you're in Python version 36
in your environment and of course in
Anaconda I can easily set that
environment up make sure you go ahead
and and um pip in your tensor flow or if
you're in Anaconda you can do AA install
tensor flow to make sure it's in your
package so let's just go ahead and dive
in and bring that up this will open up a
nice browser window I just love the fact
I can zoom in and zoom out depending on
what I'm working on making it really
easy to adjust um a demo for the right
size go under new and let's go ahead and
create a new Python and once we're in
our new python window this is just going
to leave it Untitled uh let's go ahead
and import import tensor flow as TF uh
at this point we'll go ahe and just run
it real quick no errors yay no
errors I do that whenever I do my
imports because I I unbearably will have
opened up a new environment and
forgotten to install tensor flow into
that environment uh or something along
those lines so it's always good to
double
check uh and if we're going to double
check that we also it's also so good to
know uh what version we're working with
and we can do that simply by um using
the version command in
tensorflow which uh you should know is
is probably intuitively the TF _ uncore
version uncore
uncore and you know it always confuses
me because sometimes you do tf. version
for one thing you do tof doore version
uncore for another thing uh this is a
double underscore intenser flow for
pulling your version out and it's good
to know what you're working with we're
going to be working in tensorflow
version
2.1.0 and I did tell you that the um we
were going to dig a little deeper into
our constants and you can do an array of
constants and we'll just create this
nice array um aals tf. constant and
we're just going to put the array right
in there
4361 uh we can run this and now that is
what a is equal to and if we want to
just double check that uh remember we're
in Jupiter notebook where I can just put
the letter a and it knows that that's
going to be print um otherwise you you
surround it in print and you can see
it's a TF tensor it has the shape the
type and the and the array on here it's
a 2X two array and just like we can
create a constant we can go and create a
variable and this is also going to be a
2X two array and if we go ahead and
print the V out we'll run that uh and
sure enough there's our TF variable in
here uh then we can Also let's just go
back up here and add this in here um I
could create another tensor and we'll
make it a constant this
time and we'll go and put that in over
here uh we'll have B TF constant and if
we go and print out uh V and B let me go
and run that and this is an interesting
thing that always that happens in here
uh you'll see right here when I print
them both out what happens it only
prints the last one unless you use print
commands uh so important to remember
that in Jupiter notebooks we can easily
fix that by go ahead and print and
Surround V with brackets and now we can
see what the two different variables we
have uh we have the 3152 which is a
variable and this is just a flat a
constant so it comes up as a TF tensor
shape two kind of two and that's
interesting to note that this label is a
tf. tensor and this is a TF variable so
that's how it's looking in the back end
when you're talking about the difference
between a variable and a constant the
other thing I want you to notice is that
in variable we capitalize the V and with
the constant we have a lowercase C
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work uh so those are a couple little
things to note in here and just like any
other array in math uh we can do like a
concatenate or concatenate the different
values here uh and you can see we can
take um AB concatenated you just do a
tf. concat
values and there's our AB axis on one
hopefully you're familiar with axes and
how that works when you're dealing with
matrixes and if we go ahead and print
this out uh you'll see right here we end
up with a tensor so let's put it in as a
constant not as a variable and you have
your array 4378 and 6145 it's
concatenated the two together and again
I want to highlight a couple things on
this our axis equals 1 this means we're
doing the C columns um so if you had a
longer array like right now we have an
array that is like you know has a shape
one Whatever It Is 2 comma 2 um axis
zero is going to be your first one and
axis one is going to be your second one
and a translates as columns and rows if
we had a shape let me just put the word
shape here um so you know what I'm
talking about it's very clear and this
is I'll tell you what I spent a lot of
time looking at these shapes and trying
to figure out which direction I'm going
in and whether to flip it or whatever um
so you can get lost in which way your
Matrix is going and which is column
which is rows are you dealing with the
third Axis or the second axis um axis
one you know 0 one two that's going to
be our columns uh and if you can do
columns then we also can do rows and
that is simply just changing the
concatenate uh we'll just grab this one
here and copy it we'll do the whole
thing over um control C
copy contrl V and changes from axis one
to axis zero and if we run that uh
you'll see that now we can catenate by
row as opposed to column and you have 4
3 6 1 7 847 so it just brings it right
down turns it into rows versus columns
you can see the difference there your
output this really you want to look at
the output sometimes just to make sure
your eyes are looking at it correctly
and it's in the format um I find VIs
usually looking at it is almost more
important than understanding what's
going on uh cuz conceptually your mind
just just too many dimensions sometimes
the second thing I want you to notice is
it says a numpy array uh so tensor flow
is utilizing numpy as part of their
format as far as Python's concerned and
so you can treat you can treat this
output like a numpy array because it is
just that it's going to be a numpy array
another thing that comes up uh more than
you would think is filling U one of
these was zeros or ones and so you can
see here we just create a tensor tf. Zer
and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer and then if we um
print out our tensor again we're in
Jupiter so I can just type out tensor
and I run this you can see I have a nice
array of um with shape 3 comma four of
zeros one of the things I want to
highlight here is integer 32 if I go to
the um tensor flow data type I want you
to notice how we have float 16 float 32
float 64 uh complex if we scroll down
you'll see the integer down here of 32
the reason for this is that we want to
control how many bits are used in the
Precision this is for exporting it to
another platform uh so what would happen
is I might run it on this computer where
python goes does a float to indefinite
however long it wants to um and then we
can take it but we want to actually say
hey we don't want that high Precision we
want to be able to run this on any
computer and so we need to control
whether it's a TF float 16 in this case
we did an integer
32 we could also do this as a float so
if I run this as a float 32 uh that
means this has a 32bit Precision you'll
see 0 point whatever and then to go with
uh
zeros we have ones if we're going from
the opposite side and so we can easily
just create a tensor flow with ones you
might ask yourself why would I want
zeros and ones and your first thought
might be to initiate a new tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
a uniform uh set of ones or zeros you're
dealing with a lot of bias so be very
careful about starting a neural network
uh for one of your rows or something
like that with ones and zeros on the
other hand uh I use this for masking you
can do a lot of work with masking you
can also have uh it might be that one t
a row is massed um you know zero is is
false one is true or whatever you want
to do it um and so in that case you do
want to use the zeros and ones and there
are cases where you do want to
initialize it with all zeros or all ones
and then swap in different numbers as
the as the um tensor learns so it's
another form of control but in general
you see zeros and ones you usually are
talking about a mask over another array
and just like in uh numpy you can also
uh do reshapes so if we take our
remember this is shaped 3 comma 4 maybe
we want to swap that to 4 comma 3 and if
we print this out you will see let me
just go and do that contrl V let me run
that and you'll see that the the order
of these is now switched instead of uh
four across now we have three across and
four
down and just for fun let's go back up
here where we did the ones and I'm going
to change the ones to um tf. random
uniform uh and we'll go and just take
off well we'll go and leave that we'll
go and run this and you'll see now we
have uh
0441 and this way you can actually see
how the reshape looks a lot different uh
041 .15 71 and then instead of having
this one it rolls down here to the
0.14 and this is what I was talking
about sometimes you fill a lot of times
you fill these with random numbers and
so this is the random. uniform is one of
the ways to do that now I just talked a
little bit about this float 32 and all
these data types uh one of the things
that comes up of course is recasting
your
data um so if we have a dtype float 32
we might want to convert these two
integers because of the project we're
working on um I know one of the projects
I've worked on ended up wanting to do a
lot of roundoff so that it would take a
dollar amount or a float value and then
have to run it off to a dollar amount so
we only wanted two decimal points um and
in which case you have a lot of
different options you can multiply by
100 and then round it off or whatever
you want to do there's a lot of or then
convert it to an integer is one way to
round it off uh kind
of cheap and dirty
trick uh so we can take this and we can
take the same tensor and we'll go ahead
and create a um as an integer and so
we're going to take this tensor we're
going to tf. cast it and if we print
print tensor uh and then we're going to
go ahead and print our tensor let me
just do a quick copy and paste and when
I'm actually programming I usually type
out a lot of my stuff just to double
check it uh in doing a demo copy and
paste works fine but sometimes be aware
that uh copy and paste can copy the
wrong code over personal choice depends
on what I'm working on and you can see
here we took uh um a float 32 4.6 4.2
and so on and it just converts it right
down to a integer value uh sry integer
32 setup and uh remember we talked about
um a little bit about
reshape um as far as flipping it and I
just did uh 4 comma 3 on the reshape up
here and we talked about axis zero axis
one uh one of the things that is
important to be able to do is to take
one of these variables we'll just take
this last one tensor as
integer and I want to go ahead and
transpose it and so I can do um we'll do
a equals tf.
transpose and we'll do our tensor
integer in there and then if I print the
A out and we run this you'll see it's
the same array but we've fli it so that
our columns and rows are flipped this is
the same as reshaping uh so when you
transpose you're just doing a reshape
what's nice about this is that if you
look at the numbers The Columns when
when we went up here and we did the
reshape they kind of roll down to the
next row so you're not maintaining the
structure of your Matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a
transform uh so we come down here it's
all done for you and so there are so
many times you have to transpose your
digits that this is important to know
that you can just do that you can flip
your rows and columns rather quickly
here and just like numpy you can also do
multi your different math functions
we'll look at multiplication and so
we're going to take matrix
multiplication of tensors uh we'll go
aad and create a as a constant
5839 and we'll put in a vector v 4 comma
2 and we could have done this where they
matched where this was a 2X two array um
but instead we're going to do just a 2x1
array and the code for that is your tf.
matat mole uh so Matrix multiplier and
we have a * V and if we go ahead and run
this let's make sure we print out our av
on
there and if we go ahead and run this uh
you'll see that we end up with 36 by 30
and if it's been a while since you've
seen The Matrix math uh this is 5 * 4 +
8 * 2 um 3 * 4 + 9 *
2 and that's where we get the 36 and 30
now I know we're covering a lot really
quickly as far as the basic
functionality uh so the Matrix or your
Matrix multiplier is a very commonly
used backend tool as far as Computing um
uh different models or linear regression
stuff like that one of the things is to
note is that just like in um numpy you
have all of your different math so we
have our TF math and if we go in here we
have um functions we have our cosiness
absolute angle all of that's in here so
all of these are available for you to
use in the tensor flow model and if we
go back to our example and let's go
ahead and pull um oh let's do some
multiplication that's always good we'll
stick with our um AV our um constant a
and our vector
v and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a * V let's go and print that
out and you can see coming across here
uh we have the 42 and the
5839 and it produces uh 2032
68 and that's pretty straightforward if
you look at it you have four times uh 5
is 20 4 * 8 is uh 32 that's where those
numbers come
from uh we can also quickly create an
identity
Matrix which is basically um your main
values on the diagonal being ones and
zeros across the other side let's go
ahead and take a look and see what that
uh uh looks like and we can do let's do
this uh so we're going to get the shape
um this is a simple way very similar to
your numpy you can do a. shape and it's
going to return a tupal in this case our
rows and columns and so we can do a
quick uh print we'll do rows
oops and we'll do
columns
and if we run this uh you can see we
have three rows uh two
columns and then if we go ahead and
create an identity
Matrix the
oops the script for that hit a wrong
button there the script for that looks
like
this where we have the number of rows
equals rows the number of columns equals
columns and D type is a 32 and then if
we go ahead and just print out our
identity you can see we have a nice
identity column with our ones going
across here now clearly we're not going
to go through every math module um
available but we do want to start
looking at this as a prediction model
and seeing how it functions so we're
going to move on to a more of a um
direct setup where we can actually see
the full tensor flow in use for that
let's go back and create a uh new
setup and we'll go in here new Python 3
module there we go bring this out so it
takes up the whole window because I like
to do that hopefully you made it through
that first part and you have a basic
understanding of tensorflow as far as
being uh a series of numpy arrays you
got your math equations and different
things that go into them we're going to
start building a full um setup as far as
the numpy so you can see how uh K sits
on top of it and the different aspects
of how it works the first thing we want
to do is we're going to go and do a lot
of imports uh date times warning scipi
scipi is your um uh math so the backend
scientific math uh warnings because
whenever we do a lot of this you have
older versions newer versions um and so
sometimes when you get warnings you want
to go ahead and just suppress
we'll talk about that if it comes up on
this particular setup and of course date
time pandas again is your data frame
think rows and columns we import it as
PD numpy is your uh numbers array which
of course tensor flow is integrated
heavily with caborn for our graphics and
the caborn as SNS is going to be set on
top of our map plot Library which we
import as MPL and then of course we're
going to import our map plot Library pip
plot as PLT and right off the bat we're
going to set some graphic colors um
patch Force Edge color equals true the
style we're going to use the 538 style
you can look this all up there's when
you get into map plot Library into
Seaborn there are so many options in
here it's just kind of nice to make it
look pretty when we start the um when we
start up that way we don't have to think
about it later on uh and then we're
going to take we have our uh mlrc we're
going to put a patch color dim Gray Line
width again this is all part of our
Graphics here in our setup uh we'll go
ahead and do an interactive shell uh
node interactivity equals last
expression uh here we are PD for pandas
options display Max columns so we don't
want to display more than 50 um and then
our map plot library is going to be in
line This is a Jupiter notebook thing
the map plot Library inline uh then
warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
you don't have to worry about them not
really what you want to do when you're
working on a major project you want to
make sure you know those warnings and
then uh filter them out and ignore them
later on and if we run this it's just
going to be loading all that into the
background uh so that's a little backend
kind of stuff then what we want to go
ahead and do is we want to go ahead and
import our specific packages U that
we're going to be working with which is
under carass now remember carass kind of
sits on tensorflow so when we're
importing cross and the sequential model
we are in effect importing um tensor
flow underneath of it uh we just brought
in the math probably should have put
that up above and then we have our cross
models we're going to import sequential
now if you remember from our uh slide
there was three different options let me
just flip back over there so we can have
a quick uh recall on that and so in
coros uh we have sequential functional
and subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have uh sequential where it's going one
tensor flow layer at a time you go kind
of look at think of it as going from
left to right or top to bottom or
whatever Direction it's going in but it
goes in One Direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate um tensors
and then it comes back together into
another tensor um all those kinds of
things and then subclassing is really
the really complicated one where now
you're adding your own subclasses into
the tensor to do external computations
right in the middle of like a huge flow
of data uh but we're going to stick with
sequential it's not a big jump to go
from sequential to functional uh but
we're running a sequential tensor flow
and that's what this first import is
here we want to bring in our sequential
and then we have our layers and let's
talk a little bit about these layers
this is where cross in tensor flow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built uh so from carass
we have layers import DSE from carass uh
layers import
lstm when we talk about these layers uh
carass has so many built-in layers you
can do your own layers the dense layer
is your standard neural network U by
default it uses railu for its activation
and then the lstm is a long short-term
memory layer since we're going to be
looking probably at sequential data uh
we want to go ahead and do the lstm and
if we go
into um carass and we look at their
layers this is the Cross website you can
see as we scroll down for the cross
layers that are built in we can get down
here and we can look at let's see here
we have our layer activation our base
layers um activation layer weight layer
weight there's a lot of stuff in here we
had the railu which is the basic
activation that was listed up here for
layer activations you can change those
and here we have our core layers and our
dense layers you have an input layer a
dense layer um and then we've added more
customized one with the long-term
short-term memory layer and of course
you can even do your own custom layers
in carass there's a whole functionality
in there if you're doing your own thing
what's really nice about this is it's
all built in uh even the convolutional
layers this is for processing Graphics
there's a lot of cool things in here you
can do um this is why cross is so
popular it's open source and you have
all these tools right at your fingertips
so from Cross we're just going to import
a couple layers the dense layer um and
the long short-term memory layer and
then of course from uh sklearn our s kit
we want to go ahead and do our minmax
scaler standard scaler for pre-editing
our data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality. CSV let's go ahead and take a
quick look at that this is in Open
Office it's just a standard you know you
do excel whatever you're using for your
spreadsheet and you can see here we have
a number of columns uh number of rows it
actually goes down to like
8,000 the first thing we want to notice
is that the first row is kind of just a
random number put in going down probably
not something we're going to work with
the second row um is bandang I'm
guessing that's a reference for the
profile if we scroll to the bottom which
I'm not going to do because it takes
forever to get back up they're all the
same uh the same thing with the status
the status is the same we have a date so
we have a sequential order here um here
is the gem which I'm going to guess is
the time stamp on there so we have a
date and time we have our O3 coo NO2
reading s SO2 no CO2 VOC um and then
some other numbers here pm1 pm2.5 pm4
pm1 10 uh without actually looking
through the data um I mean some of this
I can guess is like temperature humidity
I'm not sure what the PMS are uh but we
have a whole slew of data here uh so
we're looking at air quality as far as
an area and a region
and what's going on with our date time
stamps on there and so codewise we're
going to read this into a pandas data
frame so our data frame uh DF is a nice
abbreviation commonly used for data
frames equals pd. read CSV and then our
the path to it just happens to be on my
D drive uh separated by spaces and so if
we go ahead and run this we'll print out
the head of our data and again this
looks very similar to what we were just
looking at um being in Jupiter I can
take this and go the other way uh make
it real small so you can see all the
columns going across and we get a full
view of it um or we can bring it back up
in size that's pretty small on there
overshot um but you can see it's the
same data we were just looking at uh
we're looking at the number we're
looking at the profile which is the
bendung the um date we have a timestamp
our 03 count Co and so forth on here uh
and this is just your basic pandas
printing out the top five rows we could
easily have done uh three rows uh five
rows 10 whatever you want to put in
there by default that's uh five for
pandas now I talk about this all the
time so I know I've already said it at
least once or twice during this video
most of our work is in Pre formatting
data what are we looking at how do we
bring it together uh so we want to go
ahead and start with our date time uh
it's come in in two columns we have our
date here and we have our time and we
want to go ahead and combine that and
then we have uh this is just a simple
script in there that says combine date
time that's our formula we're
building our we're going to submit our
um Panda's data frame and the tab name
when we go ahead and do this uh that's
all of our information that we want to
go ahead and create and then goes for
ION range DF uh shape zero so we're
going to go through um the whole setup
and we're going to list tab a pin DF
location I and here is is our date uh
going in there and then return the numpy
array list tab D types date time 64
that's all we're doing we're just
switching this to a date time stamp and
if we go ahead and do DF date time
equals combined date time and then I
always like to uh print we'll do DF head
just so we can see what that looks like
and so when we come out of this uh we
now have our setup on here and of course
it's added it on to the far right here's
our date time you can see the formats
change ched uh so there's our we've
added in the date time column and we've
brought the date over and we've taken
this format here and it's an actual
variable with a 0000 0 on here well that
doesn't look good so we need to also
include the time part of this and we
want to convert it into hourly data uh
so let's go ahead and do that uh to do
that uh to finish combining our date
time let's go ahead and create a uh a
little script here to combine the time
in there same thing we just did we're
just creating a numpy array returning a
numpy array and C forcing this into a
datetime format and we can actually
spend hours just going through uh these
conversions how do you pull it from the
Panda's data frame how do you set it up
um so I'm kind of skipping through it a
little fast because I want to stay
focused on tensor flow and carass keep
in mind this is like 80% of your coding
when you're doing a lot of this stuff is
going to be reformatting these things
resetting them back up uh so that it
looks right on here and uh you know it
just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are
for and we want to go ahead and uh a
couple things going on here is we're
going to go ahead and do our date time
we're going to reorganize some of our
setup in here convert into hourly data
me just put a pause in there um now
remember we can select from DF are
different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data uh so
there's nothing in there that exciting
and then we want to go ahead and we'll
create a uh second data frame here let
me just get rid of the DF head and df2
is we're going to group by date time and
we're looking at the mean value and then
we'll print that out so you can see what
we're talking about uh we have now
reorganized this so we put in date time
03 Co so now this is in the same order
um as it was before and you'll see the
date time now has our 0 0 uh same date 1
2 3 and so on so it's group the data
together so it's a lot more manageable
and in the format we want and in the
right sequential order and if we go back
to um there we go our air quality uh you
can see right here we're looking at um
these columns going across we really
don't need since we're going to create
our own date time column we can get rid
of those these are the different Columns
of information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data grouped it
together by date and then you can see
the different data coming out uh set up
on there uh and then as a data scientist
first thing I want to do is get a
description what am I looking at uh and
so we can go ahead and do the df2
describe and this gives us our um you
know describe gives us our basic uh data
analytics information we might be
looking for like what is the mean
standard
deviation uh minimum amount maximum
amount we have our first quarter second
quarter and third quarter um numbers
also in there uh so you can get a quick
look at a glance describing the data or
descriptive analysis and even though we
have our Quant information in here we're
going to dig a little deeper into that
uh we're going to calculate the quantile
for each variable uh we're going to look
at a number of things for each variable
we'll see right here q1 uh we can simply
do the quantile
.25% which should match um our 25% up
here and we're going to be looking at
the Min the max um and we're just going
to do this is basically we're breaking
this down for each uh different variable
in there one of the things is kind of
fun to do uh we're going to look at that
in just a second let me get put the next
piece of code in here um got clean out
some of our um we're going to drop a
couple thing our um last rows and first
row because those have usually have a
lot of null values and the first row is
just our titles uh so that's important
it's important to drop those rows in
here and so this right here as we look
at our different
quantiles again it's it's the same we're
still looking at the 25 quantile here
we're going to do a little bit more with
this um so now that we've cleared off
our first and last rows we're going to
go ahead and go through all of our
columns and this way we can look at each
uh column individually and so we'll just
create a q1 Q3 min max uh Min IQR Max
IQR and calculate the quantile of I of
df2 we're basically doing uh the math
that they did up here but we're
splitting it apart um that's all this is
and
this happens a lot because you might
want to look at individual uh if this
was my own project I would probably
spend days and days going through what
these different values mean one of the
biggest data science uh things we can
look at that's
important is uh use your use your common
sense you know if you're looking at this
data and it doesn't make sense and you
go back in there and you're like wait a
minute what the heck did I just do at
that point you probably go back and
double check what you have going on now
uh we're looking at this and you can see
right here here's our attribute for our
03 so we broken it down uh we have our
q1 5.88 Q3 10.37 if we go back up here
here's our 58 we've uh rounded it off
10.37 is in
there so we've basically done the same
math uh just split it up we have our
minimum and our Max IQR and that's
computed uh let's see where is it here
we we go uh q1 - 1.5 * IQR and the IQR
is your Q3 minus q1 so that's the
difference between our two different
quarters and this is all uh data science
um as far as a hard math we're really
not we're actually trying to focus on
carass and tensor flow you still got to
go through all this stuff I told you 80%
of your programming is going through and
understanding what the heck uh happened
here what's going on what does this data
mean
and so when we looking in that we're
going to go ahead and say hey
um we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your Min minimum
IQR uh that means something's going
wrong there and they usually in this
case is going to show us an outlier uh
so we want to go ahead and find the
minimum value if it's less than the
minimum minimum IQR it's an outlier and
if the max value is greater than in the
uh Max IQR uh we have an outlier and
that's all this is doing low outliers
found uh minimum value High outlier is
found uh really important actually
outliers are almost everything in data
sometimes sometimes you do this project
just to find the outliers cuz you want
to know uh crime detection what are we
looking for we're looking for the
outliers what doesn't fit a normal
business deal and then we'll go ahead
and throw in um just threw in a lot of
code oh my goodness uh so we have if
your max is greater than IQR print
outlier is found what we want to do is
we want to start cleaning up these
outliers and so we want to convert uh
we'll do create a convert Nan x max IQR
equals Max _ IQR Min IQR equal Min IQR
so this is just saying this is the data
we're going to send that's all that is
in Python and if x is greater than the
max IQR and X is less than the Min IQR x
equals uh null we're going to set it to
null
why because we want to clear these
outliers out of the data now again if
you're doing fraud detection you would
do the opposite you would be cleaning
everything else that's not in that
Series so that you can look at just the
outlier uh and then we're going to
convert the Nan hum again we have x uh
Max IQR is 100% Min IQR is min IQR if x
is greater than Max IQR and X is less
than Min IQR again we're going to return
null value otherwise it's going to
remain the same value x xals x and you
can see um as we go through the code if
I equals um our hm uh then we go ahead
and do that's the that's a column
specific to humidity that's your huum
column uh then we're going to go ahead
and convert do the run a map on there
and convert the non huum uh you can see
here is this is just cleanup uh we run
we found out that humidity probably has
some weird values in it uh we have our
outliers um that's all this is and so
when we go ahead and finish this and we
take a look at our outliers and we run
this code
here uh we have a low outlier 2.04 we
have a high outlier
99.06 outliers have been
interpolated that means we've given them
a new value uh chances are these days
when you're looking at something like um
these sensors coming in they probably
have a failed sensor in there something
went wrong um that's a kind of thing
that you really don't want to do your
data analysis on uh so that's what we're
doing is we're pulling that out and then
uh converting it over and setting it up
method linear so we interpolate method
linear it's going to fill that data in
based on a linear regression model of
similar data uh same thing with this up
here with the um df2 interpolate that's
what we're doing again this is all data
prep uh we're not actually talking about
tensor flow we're just trying to get all
our data set up correctly so that when
we run it it's not going to cause
problems or have a huge
bias so we've dealt with outliers
specifically in
humidity and again this is one of these
things where when we start running
um we run through this you can see down
here that we have our um outliers found
high low outliers um migrated them in we
also know there's other issues going on
with this data uh how do we know that
some of us just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and and use a logarithmic function
on the temp value and uh it's
interesting because it's like how do you
how do you he to even know to use
logarithmic on the temp value that's
domain specific we're talking about
being an expert in air care I'm not an
expert in Air Care um you know it's not
what I go look at I don't look at Air
Care data in fact this is probably the
first Air Care data setup I've looked at
but the experts come in there and they
come to you and say hey in data science
um this is a um exponentially VAR
variable on here so we need to go ahead
and do um transform it and use a
logarithmic scale on that
so at that point that would be coming
from your um uh data here we go data
science programmer overview does a lot
of stuff connecting the database and
connecting in with the experts data
analytics a lot of times you're talking
about somebody who is a data analysis
might be all the way usually a PhD level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it so when we're looking at
this we're looking at uh something
they've sent to me and they said hey
domain Air Care this needs to be this is
a skew because the data just goes up
exponentially and affects everything
else and we'll go ahead and take that
data um let me just go ahead and run
this just for another quick look at it
um we have our uh uh we'll do a
distribution DF we'll create another
data frame from the temp values and then
from a data set from the um log temp so
we can put them side by side and we'll
just go ahead and do a quick histogram
and this is kind of nice plot of figure
figure size here's our PLT from matplot
library uh and then we'll just do a
distribution umore DF there's our data
frames this is nice because it just
integrates the histogram right into
pandas love pandas and this is a chart
you would send back to your data
analysis and say hey is this what you
wanted this is how the data is
converting on here as a data science
scientist the first thing I note is
we've gone from a 10 20 30 scale a 2.5
3.0 3.5 scale um and the data itself has
kind of been um uh adjusted a little bit
based on some kind of a skew on there so
let's jump into uh we're getting a
little closer to actually doing our uh
um carass on here we'll go ahead and
split our data up um and this of course
is any good data scientists you want to
have a training set and a test Set uh
and we'll go ahead and do the train
size we're going to use 75% of the data
make sure it's an integer don't want to
take a slice as a float value give you a
nice error uh and we'll have our train
size is 75% and the test size is going
to be um of course the uh train size
minus the length of the data set and
then we can simply do train comma test
here's our data set which is going to be
the train size the test size uh and then
if we go and print this let me just go
aad and run this we can see how these
values um split it's a nice split of
1,98 and then 433 points of value that
are going to be for our um setup on here
and if you remember we're specifically
looking at the data set where did we
create that data set from um that was
from up here that's what we called the
uh logarithmic U value of the temp uh
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do uh convert an array of
values into a data set Matrix we're
going to create a little um setup in
here we're create our data set our data
set's going to come in we're going to do
a look back of one so we're going to
look back one piece of data going
backward and we have our data X and our
data y for ION range length of data set
look back minus one uh this is creating
let me just go ahead and run this
actually the best way to do this is to
go ahead and create this data
and take a look at the shape of it uh
let me go Ahad and just put that code in
here so we're going to do a look back
one here's our train X our train Y and
it's going to be adding the data on
there and then when we come up here and
we take a look at the shape there we go
um and we run this piece of code here we
look at the shape on this and we have uh
a new slightly different change on here
but we have a shape of X 1296 comma 1
shape of Y train y Test X test Y and so
what we're looking at is that um the X
comes in and we're only having a single
value out uh we want to predict what the
next one is that's what this little
piece of code is here for what are we
looking for well we want to look back
one that's the um what we're going to
train the data with is yesterday's data
yesterday says Hey the humidity was at
97% what should today's humidity be at
if it's 9 7% yesterday is it going to go
up or is it going to go down today if 97
does it go up to 100 what's going on
there uh and so our we're looking
forward to the next piece of data which
says Hey tomorrow's is going to you know
today's humidity is this this is what
tomorrow's humidity is going to be
that's all that is all that is is
stacking our data so that U our Y is
basically x + one or X could be Yus
one and then a couple things to know not
is our X data um we're only dealing with
the one column but you need to have it
in a shape that has it by the column so
you have the two different numbers and
since we're doing just a single point of
data we have and you'll see with the
train y we don't need to have the extra
shape on here now this is going to run
into a problem uh and the reason is is
that we have what they call a Time
step and the time step is that long-term
shortterm term memory layer uh so we're
going to add another reshape on here let
me just go down here and put it into the
next cell and so we want to reshape the
input uh array in the form of sample
time step features we're only looking at
one
feature and I mean this is one of those
things when you're playing with this
you're like why am I getting an error in
the numpy array why is this giving me
something weird going on uh so we're
going to do is we're going to add one
more uh level on here instead of being
12991 we want to go one
more and when they put the code together
in the back you can see we kept the same
shape the
1299 uh we added the one dimension and
then we have our trainx shape one um and
this could have depends again on how far
back in the long shortterm memory you
want to go that is what that piece of
code is for and that reshape is and you
can see the new shape is now one uh$
12991 1 uh versus the
12991 and then the other part of the
shape 4321 one again this is our TR our
xn and of course our test X and then our
Y is just a single column because we're
just doing one output that we're looking
for so now we've done our
80% um you know that's all the the
writing all the code reformatting our
data um bringing it in now we want to go
ahead and do the fun part which is we're
going to go ahead and create and fit the
lstm neural network uh and if we're
going to do that the first thing we need
is we're going to need to go ahead and
create model and we'll do the sequential
model and if you remember sequential
means it just goes in order uh that
means we have if you have two layers the
layers go from layer one to Layer Two or
layer zero to layer one this is
different than functional uh functional
allows you to split the data and run two
completely separate models and then
bring them back together we're doing
just sequential on here and then we
decided to do the long shortterm memory
uh and we have our input shape uh which
it comes in again this is what all this
switching was we could have easily made
this one two 3 or four going back as far
as the uh in number on there we just
stuck to going back one and it's always
a good idea when you get to this point
where the heck is this model coming from
um what kind of models do we have
available and uh there's Let Me Go a and
put the next model in there uh cuz we're
going to do two models and the next
model is going to go ahead and we're
going to do dent so we have model equals
sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our
code where we did the Imports oops here
we go our cross this is it right here
here's our importing a dense model and
here's our importing an
lstm now just about every tensor flow
model uses d uh your D model is your
basic forward propagation uh reverse
propagation error or it does reverse
propagation to program the model uh so
any of your neural networks you've
already looked at that uh looks and says
here's the error and sends the error
backwards that's what this is the long
short-term memory is a little different
the real question that we want to look
at right now is where do you find these
models what what kind of models do you
have available and so for that let's go
to the carass website uh which is the
cars.io if you go under API layers and I
always have to do a search just search
search for carass uh API layers it'll
open up and you can see we have um your
base layers right here class trainable
weights all kinds of stuff like there
your activation uh so a lot of your
layers you can switch how it activates
uh Ray which is like your smaller arrays
or if you're doing convolutional neural
networks the convolution usually uses a
Rilo um your sigmoid um all the way up
to soft Max soft plus all these
different choices as far as how those
are set up and and what we want to do is
we want to go ahead and if you scroll
down here you'll see your core layers
and here is your dense layer uh so you
have an input object your DSE layer your
activation layer embedding layer this is
your your kind of your one setup on
there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layers so as
you have the layers come together um
usually bring them down into uh single
layer although you can still do like
Global Max pulling 3D and there is just
I mean this list just goes on and on uh
there's all kinds of different things
hidden in here as far as what you can do
and it changes you know go in here and
you just have to do a search for what
you're looking for uh and figure out
what's going to work best for you as far
as what project you're working on uh
long shortterm memory is a big one cuz
this is when we start talking about tech
next uh what if someone saysthe what
comes after the uh the cat and the hat
little kids book there um starts
programming it and so you really want to
know not only um what's going on but
it's going to be something that has a
history the history behind it tells you
what the next one coming up is now once
we've built all our different you know
we built our model we've added our
different layers we went in there um
play with it remember if you're in
functional you can actually link these
layers together and they Branch out and
come back together if you do a uh um the
sub setup then you can create your own
different model you can embed a model in
there that might be comes linear
regression you can embed a linear
regression model uh as part of your
functional split and then have that come
back together with other things so we're
going to go ahead and compile your model
this brings everything together we're
going to put in with the losses which
we'll use the mean squared error uh and
we'll go a and use the atom Optimizer
clearly there's a lot of choices on here
depending on what you're doing and just
like uh any of these uh different
prediction models if you've been doing
any uh um s kit from python uh you'll
recognize that we have to then fit the
model uh so what are we doing in here
we're going to T send in our train X or
train y um we're going to decide how
many epics we're going to run it through
500 is probably a lot for this um I'm
guessing it probably be about two or 300
probably do just fine our batch
size so how many different uh when you
process it this is the math behind it if
you're in data analytics um you might
try to know what this number is as a
data scientist where I haven't had the
PHD level math that says this is why you
want to use this particular batch size
you kind of play with this number a
little bit um you can dig deeper into
the math see how it affects the results
uh depending on what you're doing and
there's a number of other settings on
here uh we did verbos 2 I'd have to
actually look that up to tell you what
verbos means I think that that's
actually the default on there if I
remember correctly uh there's a lot of
different settings when you go to fit it
the big ones are your epic and your
batch size those are what we're looking
for and so we're going to go ahead and
run
this and this is going to take a few
minutes to run because it's going
through um 500 times through all the
data so if you have a huge data set this
is the point where you're kind of
wondering oh my gosh is this going to
finish tomorrow um if I'm running this
on a single machine and I have a terab
terabyte of data uh going into
it if this is my personal computer and
I'm running a terabyte of data into this
um you know this is running rather
quickly through all 500 iterations uh
but you know at a terabyte of data we're
talking something closer to days week um
you know even with a uh um uh 3.5 ghah
machine in in eight cores it's still
going to take a long long time to go
through a full terabyte of data and then
we want to start looking at putting it
into some other framework like spark or
something that will pil to process on
there more across multiple um processors
and multiple
computers and if we scroll all the way
down to the bottom you're going to see
here's our square mean error of
0.0088 if we scroll way up you'll see it
kind of oscillates between 0888 and
0889 it's right around two 2 250 where
you start seeing that oscillation where
it's really not going anywhere so we
really didn't need to go through a full
500 epics uh you know if you're
retraining the stuff over and over again
it's kind of good to note where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model uh we want
to go ahead and run a prediction on it
so let's go ahead and make our
prediction and remember we have our
training test set and our test set or we
have the train X and the train y for
training it or train predict and then we
have our test X and our test y going in
there uh so we can test to see how good
it did uh and when we come in here we
have um uh you'll see right here we go
ahead and do our train predict equals uh
model predict train X and Test predict
model predict Test X why would we want
to run the prediction on trainex well
it's not 100% on its prediction we know
it has a certain amount of error and we
want to compare the error we have on
what we programmed it with with the
error we get when we run it on new data
that's never seen the model's never seen
before and one of the things we can do
uh we go ahead and invert the
predictions uh this helps us level it
off a little bit more um get rid of some
of our bias we have train predict equals
an NP um exponential M1 the train
predict and then train y equals the
exponential M1 for train y and then we
do the again that with train test
predict and test
y um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
uh which is your math square root times
the mean square root error train uh Y
and train predict and again we're just
um uh this is just feeding the data
through so we can compare it and the
same thing with the test and let's take
a look at that because really the code
makes sense if you're going through it
line by line you can see what we're
doing but the answer really helps to
zoom in uh so we have a train score
which is
2.40 of our root mean square error and
we have a test score of 3.16 of the root
mean square error if these were reversed
if our test score is better than our
training score then we've overtrained
something's really wrong at that point
you got to go back and figure out what
you did wrong
uh cuz you should never have a better
result on your test data than you do on
your training data and that's how we put
them both through that's why we look at
the error for both the training and the
testing when you're going out and
quoting your um U publishing this you're
saying hey how good is my model it's the
test score that you're showing people
this is what it did on my test data that
the model had never seen before this is
how good my model is and a lot of times
you actually want to put together like a
little formal code um where we actually
want to print that out and and if we
print that out you can see down here um
test prediction standard deviation of
data set 3.16 is less than 4 uh 40 i'
have to go back and we're up here if you
remember we did the square means error
this is standard deviation that's why
these numbers are different it's saying
the same thing that we just talked about
uh 3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
uh and along with putting a formal print
out of there um we want to go ahead and
plot what's going
on uh and this we just want a pretty
graft here so that people can see what's
going on when I walk into a meeting and
I'm dealing with a number of people they
really don't want these numbers they
don't want to say hey what's I mean
standard deviation unless you know what
statistics are um you might be dealing
with a number of different departments
head of celles might not work with
standard deviation or have any idea what
that really means number-wise and so at
this point we really want to put it in a
graph so we have something to display
and with displaying you got to remember
that we're looking at uh the data today
going into it and what's going to happen
tomorrow so let's take a quick look at
this uh we're going to go ahead and
shift the train predictions for plotting
uh we have our train predict plot uh NP
mty like data set uh train predict
plot uh set it up with uh null values
you know it's just kind of it's kind of
a weird thing where we we're creating
the um the data groups as we like them
and then putting the data in there is
what's going on here uh so we have our
train predict plot uh which is going to
be our look back or length plus look
back we're just is going to equal train
uh train predict so we're creating this
basically we're taking this and we're
dumping the train predict into it so now
we have our nice train predict
plot and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like I put it in here double no
it's just a yeah they put it in here
double um didn't mean to do that we
really only need to do it once oh here
we go um this is where the problem was
is because this is the test predict so
we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
uh data sets uh test predict plot length
prediction setup on there and then we're
going to go through the plotting the
original data set and the predictions uh
so we have a Time axis always nice to
have your time set on there um set that
to the time array time axis lap all this
is setting up the time variable for the
bottom and then we have a lot of stuff
going on here as far as setting up our
figure let's go ahead and run that and
then we'll break it
down we have on here uh our main plot we
have two different plots going on here U
the ispu going up and the data and the
ispu here with all these different
settings on
it and so when we look at this we have
our um ax1 that's the main plot I mean
our ax that's the main plot and we have
our ax1 which is the secondary plot over
here so we're doing a figure PLT our pt.
fig and we're going to dump those two
graphs on there um and so we take and if
you go through the code piece by piece
uh which we're not going to do we're
going to do the um the data set here um
exponential reverse exponential so it
looks correctly we're going to label it
the original data set um we're going to
plot the train predict plot that's what
we just created we're going to make that
orange and we'll label it train
prediction uh test predic plot we're
going to make that red and label it test
prediction and so forth um set our ticks
up this actually just put ticks um time
axis gets its ticks the little little
marks they going along the axes that
kind of thing and let's take a look and
see what these graphs look
like and these are just kind of fun you
know when you show up into a meeting and
this is the final output you say hey
this is what we're looking at um here's
our original data in blue here's our
training prediction um you can see that
it trains pretty close to what the data
is up there I would also probably put a
um like a little little time stamp and
do just right before and right after
where we go from uh train to test
prediction and you can see with the test
prediction the data comes in in
red um and then you can also see what
the original data set looked like behind
it and how it differs and then we can
just isolate that here on the right
that's all this is um is just the test
prediction on the right uh and it's you
know there's you'll see with the
original data set there's a lot of Peaks
for missing and a lot of lows are
missing but as far as the actual test
prediction it's pretty does pretty good
it's pretty right on you can get a good
idea what to expect for your ispu and so
from this we would probably publish it
and say hey this is um what you expect
and this is our area of this is a range
of error um that's the kind of thing i'
put out on a daily basis maybe we
predict the Sals are going to be this or
maybe weekly so you kind of get a nice
you kind of flatten the um data coming
out and you say hey this is what we're
looking at the big takeaway from this is
that we're working with let me go back
up here oops oh too far there we go um
is this model here this is what this is
all about we worked through all of those
pieces um all the tensor flows and that
is to build this sequential model and
we're only putting in the two layers
this can get pretty complicated if you
get too complicated it never um it it
never verges into a usable model uh so
if you have like 30 different layers in
here there's a good chance you might
crash it kind of thing um so don't go
too haywire on that and that you kind of
learn as you go again it's domain
knowledge um and also starting to
understand all these different layers
and what they
mean the data analytics behind those
layers um is something that you're data
analysis uh professional would come in
and say this is what we want to try but
I tell you as a data scientist um a lot
of these basic setups are common and I
don't know how many times uh working
with somebody and they're like oh my
gosh if I only did a tangent H instead
of a railu activation I worked for two
weeks to figure that out well as a data
science I can uh run it through the
model in you know 5 minutes instead of
spending two weeks doing the the math
behind it um so that's one of the
advantages of data scientist is we do it
from programming side and a data
analytics is going to look for it how
does it work in math and this is really
the core right here of tensor flow and
carass is being able to build your data
model quickly and efficiently and of
course uh with any data science putting
out a pretty graph so that your
shareholders um again we want to take
and um reduce the information down to
something people can look at and say oh
that's what's going on they can see
stuff what's going on is as far as the
dates and the change in the ispu now
let's talk a little bit about recurrent
neural networks so neural networks are
of different types we have CNN we have
RNN Ann so on now RNN is one type of
neural network RNN stands for recurrent
neural network the networks like CNN
andn are feed forward Network which
means that the information pretty much
only goes from left to right or from
front to back whichever you call it
whereas in case of recurrent neural
network there will be some information
traveling backwards as well so that is
why it is known as recurrent neural
network and each of these types have a
specific application so for example
convolutional neural networks or CNN are
very good for doing image processing and
U object detection using for video and
images whereas recurrent neural networks
are pretty good for doing NLP or speech
recognition and so on okay so for the
next few minutes minutes we will kind of
focus on recurrent neural networks and
we will see an example of how we can use
RNN to do a Time series analysis so in a
typical neural network this is how it
looks right so where you have a neuron
and then the inputs are coming to the
neuron and uh then you have an output
which goes to other neurons in the other
layers in case of recurrent neural
network what happens is you have the
inputs let's say at a given point in
time but then a part of the previous
output also gets fed in along with the
inputs for the given time now this can
be a little confusing so let's see if we
can take a little expanded view of this
so this is another view of one single
neuron which is what is known as in an
unfolded manner okay so if we are
getting inputs or data over a period of
time then this is how the neuron will
look remember these are not three
neurons this is one neuron and it is is
what is known as it is shown in a
unfolded way okay we are we have
unfolded this single neuron over a
period of time so at a given time let's
say t minus1 an input of XT minus one is
fed to the neuron and it gets a output
of YT minus one then the next instant
which is XT at a time T right there is
an input of XT and then there is an
output of YT so this is a single neuron
but is displayed in an unfolded way so
this is like expanding it over a period
of time so let's start with this part
here this particular neuron gets an
input at at an instant T minus1 let's
say time is equal to T minus1 it gets an
input of XT minus1 and it gives an
output of YT minus then when we go to
the instant T here it gets an input of
XT and it also additionally gets an
input from this previous time frame T
minus one so this YT minus 1 gets fed
here and that results in YT all right
then when we go to the time t + 1 there
is an input of XT minus one at that
given time plus the input from the
previous time frame which is a time
frame of T that also gets fed in here
and then we get an output of YT + 1 okay
so let me explain once again this is a
single neuron this these are not three
different neurons a single neuron seen
over a period of time from T minus1 to t
+ 1 and unlike regular feedforward
neuron which only gets X in this case
the input is X and also another input
which is coming from the previous time
frame and that is what this Loop is in
this on the left hand side diagram this
Loop is indicating that okay so on the
right hand side it is represented in an
unfolded way okay so the input if we
take a time frame T for example is not
only XT which is the input which is the
normal input at the time frame T but it
is also getting getting an input from
the previous time frame which is this YT
minus one is also being fed as an input
that is the key differentiator here okay
similarly at the instant t + 1 it is
getting a normal input of XT + 1 plus
this YT is actually being fed here and
then the output comes as YT + 1 okay so
this is the construct of a recurrent
neural network now recurrent neural
networks again can be of different
subtypes it can be one to one one to
many many to one and many to many
depending on what kind of application we
want to use one of the examples is like
the stock price so you're feeding so
there is only one input only thing is it
is spread over a period of time so
you're feeding the stock price input
that is what comes here as an input and
you get an output which is again the
stock price which is probably predicted
over the next uh 2 days or 3 days or 10
days or whatever so the number of
outputs and inputs is the same there is
only one input there is only one output
only thing is it is spread over time so
variables are not many then you have one
to many so there is one input but you're
expecting multiple outputs what is an
example of this let's say you want to
caption an image so how do you want to
do that what is the input that will go
here is just an image which is one input
but what is output you're looking for
you're looking for a phrase maybe right
not just a word but a phrase so it's
like cat is sitting on a mat right so
the images there is only one image which
consists of the cat sitting on a mat but
the out put are like maybe three or four
words which is saying the cat is sitting
on a mat so this is one to many right
then you can have many to one examples
of this are like you're feeding some
text and you want to know the output
whether the text is what kind of
sentiment is expressed by the text it
could be positive it could be negative
so that's the output you're looking for
so you feed a large number of words
maybe the text May consist of words or
lines or whatever so that is what is the
multiple inputs but the output all it
says is the sentiment is positive so it
is just one output or the sentiment is
negative just one output right so this
is many to one then we have many to many
so what is an example of this let's say
you want to do some translation so how
do you do a translation you feed in a
sentence maybe in a particular language
English and then you want another a
sentence actually in another language so
that is like there are multiple inputs
one sentence can consist of multiple
words and the output also is a sentence
consisting of multiple words words all
right so how do we Implement RNN this is
an example of implementing an RNN for a
particular use case so in our particular
example we have the data about milk
production over a few months and using
RNN we want to predict because this is
time series analysis so RNN is good to
do time series analysis so using RNN we
want to predict what will be the milk
production in the future so let us see
how we can do that I will first take you
through quickly through the slides and
then I will actually run this in in a
Jupiter notebook the code I will run it
in the Jupiter notebook so this is how
it looks the code looks so while this
extensive flow you still use the
standard python libraries like numai and
pandas and even matplot lib to do some
initial processing getting the data
processing it cleaning it whatever all
that can be still done in within the
same program so that's what we're doing
here we're importing some libraries like
numpy and P do and then we read the data
file and if we plot the data we see here
it is the data for for the years 1962 to
75 and we can see that there is a
certain Trend right so this is how a
typical time series data would look
again some of you if you're not familiar
with time series and uh time series
analysis and so on I would recommend you
to go through some videos around that
which will make it easy to understand
this so this is how typical time series
data would look in this case it is
nothing but there is only one variable
which is milk production and it is
spread across several years so it's
going from 1962 to 75 and that value has
been plotted so if you see there is a
certain kind of a trend here which is
basically going upwards and there will
be some seasonality so time series data
has these three components right so it
will have a trend it will have a
seasonality it will have seasonality and
then it will have some Randomness so
that's what this graph is uh showing and
um now if we want to perform analysis on
this time series analysis on this first
thing we need to do is split the data
into train and uh test and in this case
we will just use a straightforward
method which is uh taking the data for
the first 13 years so the idea here is
we need to train our model right this is
time series data so what and we want to
predict for the future so the way we
need to use this is we have to take the
data for a certain period of time and
train our models and then we use a part
of the known data so that we can then
ask our model to predict for that
duration and compare it with the known
information so what do we mean by that
so here if you see this data I will show
you in the notebook as well Jupiter
notebook as well this has 13 years of
data so what we are doing is we are
taking the first 12 years and using that
as our training set right so 12 years
we're doing and this has about I think
14 years of data so we are taking 13
years of data for training purpose and
then we are using the last one year
remaining one year of data for testing
purpose because here what we can do is
this onee data we know the values right
because if we want to compare the
accuracy or anything like that we need
to know the values so in this case we
know the values of this one particular
year so we will use that but at the same
time we train the model for 13 years of
data and for the 14th Year we will ask
our model to predict and then compare it
with this known value so that we know
how accurate our model is I hope that
makes sense okay so that's what we are
doing here 156 is nothing but 12 into 13
on a monthly basis we get the data the
next step is to scale the data this is
all regular data manipulation data
monging activity and then you split it
you are basically assigning the train
and test data to the to the scaled
variables and uh then you need to read
the data in batches so it is very
important as I mentioned earlier also
that instead of reading the entire data
in one shot you feed the data in batches
so in order to do that we are writing a
a function for that that's all we are
doing here so that is till here what we
have done is regular Python Programming
there is no tensor flow as of now here
okay so so far what we have done is
preparation data preparation data Ming
now what we are doing is actually
training our model so this is where
tensor flow now comes into picture so we
are importing first step is to import
the tensorflow library so this is how
you do it import tensorflow SDF and then
you can Define some variables or
constants whichever way now here of
course there are a couple of ways of
doing it you can create them as
tensorflow nodes but that is a little
bit of an overhead we will just use the
regular python variables or constants so
here I'm creating regular python
variables and I'm saying number of
inputs is one so instead of hard coding
I'm just storing them as variables so
this is number of inputs is one number
of time steps is 12 number of neurons is
100 and so on right and then learning
rate is 03 and number of iterations we
are seeing is 4,000 then we create
placeholders now we will be storing the
independent variables in X and the
dependent variables in y and this we
will read from an external file remember
I told you placeholders are used for
getting data from outside and then
feeding it to our model so that's the
reason we have two placeholders one for
reading the X values and another for
reading the Y values in this step we are
only just creating the nodes right so
this is just creating the graph and
similarly we are mentioning what is the
loss function and what is the optimizer
and uh how to run the optimizer and once
that is done you in initializing the
variables and then you're creating a sa
variable or a saver instance so sa is
basically nothing but in machine
learning you can train your model and
you can save it for later use so that's
where the saving comes into play we will
see how to use that as well and then the
last step is to create your session and
run this graph right so we are
initializing the variables remember this
we run in it so which is nothing but
this one so we are initializing the
variables whatever are there instead of
hard coding remember in earlier case we
were hardcoding how many iterations so
here we are saying for the given number
of iterations which are stored in this
maybe how many iterations We Said is
What is the value of iterations this is
training iterations is 4,000 so we
specify that based on the number of
training iterations next batch will
basically fetch the data and then we run
the session we are basically saying
train is the node which we will run in
the session and uh this will basically
train our model and this this is more
for printing every 100 times you print
it that's all this there's nothing more
to it so for example the output would
look somewhat like this this is for zero
this for 100 next for 200 and so on okay
and then you save the model you remember
sa we created so you save the model for
later use so this is our test data
remember we are doing this on training
data right so once the training data
training is done we then try to create
the inference on the test data so that
we can compare how accurate this is
right so this is the the test set would
look and uh then we will basically
restore this model okay because in the
previous slide we created the model and
we stored it so now we have to restore
the model and then run our test data
against it and see what are the values
that are predicted what are the Y values
and then compare with X to see the
accuracy so train seed is what we are
seeing here so that is what we'll have
the the predicted values and uh these
predicted values what we want to do is
we want to create an additional column
because remember we need to compare
because we want to find out the accuracy
of this model so we need to compare the
predicted values with the actual value
so what we are doing here is adding
another column called generated and uh
assign a value to that all right so this
is the result of the prediction and then
if we need to reshape because we have to
show this in the form of a monthly
results so that's what we doing here and
um once we generate the results and and
then display it we can actually see it
month-wise here and uh the actual and
the generated values okay so we create a
data frame which is a combination of the
actual values and the predicted values
from the test set and then we can plot
to see how the trend is as you can see
pretty much the actual value is in blue
color and the generated or predicted
value the curve is in yellow color the
trend is maintained so it will probably
it's not 100% accurate but the trend is
maintained all right so let's do one
thing let's go into the Jupiter notebook
and take a look at how this works in
tensorflow environment actually the code
I will walk you through the code this is
my Jupiter notebook and uh the data is
taken from this particular Link in case
you interested you can uh download it
from there and this is the data for the
years 1962 to 75 the first thing we do
is import these libraries because before
we start with the actual of flow
activity we need to prepare the data and
so on so for that you can use your
regular python libraries which are like
numai pandas and so on so that's what we
doing here and then we read the data
using pandas into a data frame so this
is a data frame milk is a data frame and
we will do some quick exploratory
analysis just to see how our data is
looking initial five records that we'll
get 1 2 3 4 5 that is head so it goes
from 1962 January to 1962 May once we
rechange or basically we need to split
this into month specifically separately
so that's what we are doing here date to
time so we kind of do a little bit of a
reindexing and then if we plot this this
is how the data look as you can see
there is a clear upward Trend and U
there is some seasonality and so on but
anyway we will not break that up into
those components we will just use RNN to
predict and test okay so the next thing
is to check some if we can do run like a
info it will tell us what is the
information about this data set so let
us just run that okay so it is just
telling us how many total columns and uh
what is the size of the file and so on
and so forth again in case you're
interested in doing some exploratory
analysis so what we'll do next is we
will take the 13 years of data for our
training data set so what are we going
to do now let me step back so what have
we seen here we have seen that there are
168 records so which is nothing but 14
years years of data we now have to split
this into our training and test data set
so how do we do that I think in case of
Time series we cannot do it like 8020
like we do in normal splitting in normal
machine learning process here it is time
series data so what we are going to do
is we will take out of this 14 years we
will take the first 13 years of data and
we will use that for training and then
we will test it with the last which is
the 14th year data we will use it for
testing okay so that's what we are going
to do here so let's uh split that so
training set will consist of my 156
records which is nothing but 12 13 into
12 right my 13 years of data I'm taking
for training and then my test set will
consist of the bottom 12 observations
which is last one year of data which is
the 14th year okay so that's now done
training and test splitting is done the
next step is to do some normalizing
which is basically we will use our
scaling rather we will use minmax scaler
and uh we will just scale the data and
now now we do it for both test as well
as uh train now we are ready to create
our RNN model but before that one quick
thing in order to fetch the data we have
to create a function so that's what we
are doing here we create a function
called Next batch and uh how you want to
fetch the steps they are all defined as
our constants if you remember and uh
that's what this uh function is all
about so we Define that function we will
be calling that in our Training Method
All right so from here onwards up to
there we are done with uh the
preparation of the data and whatever
functions or whatever are required from
here onwards is where the tensor flow
part starts so the first thing we do is
import the tensorflow library typically
this is a very standard way of importing
the library we say import tensorflow as
TF now TF is nothing but a name so you
can change it to tf1 or ABC or XY Z
whatever so this part you can change but
by and large everybody uses this so we I
would rather recommend you also use the
same syntax so you say import poens of
flow stf so it's very easy for others to
understand as well and then you declare
or Define a bunch of constants uh that's
what we are doing here like for example
the number of uh time steps in in this
case it is 12 number of neurons there
are 100 number of outputs it is only one
and so on right and then the learning
rate and all that we are declaring or
defining those variables in this
particular block next is to create
placeholders you remember the
placeholders are used for feeding the
data so we have X and Y X is for the
input which is the independent variable
and Y is the output which is the
dependent variable and in our case these
are not different characteristics or
features but it is the same one feature
or one variable but it is over a period
of time so that's the only difference so
that's what we are doing here and now we
need to create our Network right the
neural network so in our case we said we
will create a RNN layer now there are
different ways or different formats of
RNN which is is probably not in scope of
this module so for now we will just
assume we are creating one RNN layer and
one type of RNN is Gru cell so we will
use Gru cell and we use this apis for
creating our layer so each cell will be
with an output projection wrapper there
is a need for doing that and again the
details of this we will probably do in
another video where we talk in detail
about RNN so for now we are creating a
GRU cell with the wrapper and the the
Syntax for creating the gru cell is uh
like this the number of units which we
we said there should be 100 neurons what
will be the activation function in this
case it is reu and then number of
outputs in our case it is only one okay
so we create the cell here okay then we
got the gru cell and then we say what is
our output which is nothing but we get
the outputs and the states and their
states and which is uh the way we get
that is tf. nnn do dynamicore RNN so if
we call this and we pass the cell and
the data which is basically in
placeholder again remember all we are
doing here is we are creating just the
nodes for the graph so nothing is
actually getting executed from a
tensorflow perspective okay all right so
once that is done now we need to pass
this calculate the outputs and the
states using the dynamicore Arin method
and we pass the cell as a parameter and
then the X Val values as a parameter and
if we run that we will have the outputs
and the states and this is where we
actually run the Training Method so or
create not really run the Training
Method but we create the nodes for the
training and Optimizer then we have the
initialization of the variables and we
save this model we create a sa object
just to save the model because we will
then restore it and run it to do our
prediction so this is what we will do
here so that is another node and this is
where we actually create a session and
run the training okay so let's go ahead
and do that that will take a little
while because we said 4,000 iterations
so we will allow it to finish we will
probably come back once the training is
completed all right so the training is
done now let's go and U run this on the
test data so let's just quickly take a
look at the test data set so this is our
one-ear data for the year of 1975 so if
we see this this from January February
and so on and we will pass this to our
model so what are we doing here we are
restoring the model here for example
right this is the path that we have
given when we were saving the model
let's go back once and show you let me
show you where we did the saving of the
model yes so we saved the model here so
that again we will restore that and we
will use that to run it on our test data
and see how well it predicts so let's go
ahead and run this and this is just 12
records so this will not take time
remember I told you training is what
takes a lot of time the regular
inference this is called inference
doesn't take much time right so it just
depends on how much data there were only
12 records here so it was very quick but
in training what we do we pass this
multiple times there are iterations
4,000 iterations we did for example so
that takes longer and in general machine
learning deep learning training is what
takes the maximum amount of time all
right so let's go ahead and see the
results we will in order to plot it we
will have to kind of adjust the format
of the results otherwise we will not be
able to see it in a proper way and then
we will so this is our so what we have
done is we created a data frame which
consists of the predicted value and the
and the actual value so this is the data
frame so this is the actual value this
834 782 and so so on this is what our
model has predicted so it may not be so
easy to see in a table format so let's
go ahead and plot it so that we can
compare these two so when we plot it you
will see that the trend is more or less
maintained right so we go from the blue
color is the actual values and the
yellow color or the orange color
whatever is it is the one which is
generated by our model so it pretty much
is following the actual Trend so not bad
for such a quick iteration and uh
training this tutorial is about object
detection we will walk you through a
tensorflow code using which we will do
object detection in images we will tell
you what are the libraries that are
required a little bit about the Coco
data set and then we will show you the
implementation code itself a demo of the
code all right so let's get started so
what is the penslow object detection API
this is an open- source framework which
is actually provided by the tensorflow
team and there are train models
available and the sample code is also
available which we can use in order to
easily detect objects in images and
videos this is pretty robust and can
detect objects fairly quickly and this
is very easy for people to use as well
people with very less or no knowledge of
machine learning or deep learning can
also with a little bit of Python
Programming knowledge can actually use
this API this library to build object
detection applications this is a list of
libraries that are required and they
have been shown in the code as well the
exact purpose of each of this Library
why it is required is out of the scope
of this tutorial but we will see in the
code as we walk through some of these
libraries how and why they are used the
Coco data set Coco stands for common
objects in context so this data set
consists of 300,000 images of uh 90 most
commonly found objects like chairs and
tables and so on and so forth so this
model has been trained or in fact a set
of models have been trained on this data
set and this is pretty good to detect
the most common objects in the images
and videos so with that let's get into
the code all right so the first part is
to import all these libraries and this
we have shown you in uh the slides as
well again large part of this will be
for doing some helper functions and
maybe for visualizing the images and so
on and so forth so that's the reason
they are required as I said the exact
details of each and every Library
probably is out of scope but these are
needed so as a first step maybe you just
go ahead and include these libraries and
run the code and maybe at later point we
can discuss what each of these libraries
does now this will work with tensorflow
version higher than 1.4 so if you are
having tens flow version below 1.4 you
may have to upgrade to a higher version
so let me go ahead and execute the cell
and um we also need this line of code to
make sure that once we run this object
detection the labeled images are
displayed within this uh notebook and
many of you by now must be familiar with
this and we will import a few utility
libraries and you will see that we will
be using some of these for visualization
purpose so once the objects are detected
we need to display the information what
that object is and then what percentage
of confid idence the model has so all
these details that's the utility
functions that stored here and then the
next part is to prepare the model as I
mentioned we will be using an existing
trained model the tensorflow team has
actually provided these models the one
that we will be using is SSD with the
mobile net but you can actually use any
one of the ones that are listed in this
URL let me just quickly take you through
this URL these are a bunch of models
trained models that are readily
available for anybody to use it is open
source and let me scroll down the only
thing is that if there are some of them
with where the accuracy is much higher
but they take longer and there are some
where the accuracy is not so high and
they are much faster so they are faster
but the accuracy may not be that very
high so you can play around with some of
these and we in this particular exercise
or in this particular tutorial we are
using this SSD model which is SSD mobile
net vertion one so that's the model that
we'll be using so in this cell we are
primarily creating a bunch of variables
with the various for example the name of
the model the path and so on and so
forth so that we will be using these
names in The Next Step which is to
download this model and install it
locally these are also refer to as a
frozen model so once they are trained
and then you kind of extract or you you
freeze the model so that's the reason
they are called Frozen model so that
others can just use this without any
further training so this is where we
download and extract our model locally
so this will take a little while let me
see if I can wait or maybe pause the
video and come back once it is done
might take a little while let's see if
it uh completes I have a pretty
highspeed network but even then it takes
some time so that's good but this part
is over now let us see this part and yes
both are done so once that is done we
need to load some label mapping
basically what this will do is your
model as you may be aware by now if you
do some classification the model will
actually not give any output as a text
it will give some numbers so if there
are five classes it will say okay this
belongs to mod class one or two or three
or five and so on the numbers now each
of these will obviously the numbers will
not make any sense to the outside world
so we need to do some small mapping so
in this case let's say one may be a
chair two maybe a table three maybe a
balloon and so on so that kind of number
to text mapping we need to do and that
is what is being done in this particular
cell and then we have a helper code
which will load the image and convert it
into a numi array so that the num by
array is what gets processed and used by
the model to do the detection part of it
so that is what this uh method is all
about so we will be that later on we
will be calling that function and next
is preparation for detection so here we
are basically telling where the images
are stored and how many images or what
is the naming convention or format of
the images now if you want you can
modify this code for example currently I
have testore images as my folder so let
me go and show this to you so this is
under my object detection folder I have
another subfolder where I'm storing my
images which is textor images now you
can rename this folder and give some
other name and then in your code you can
probably give that particular name for
Thea similarly the format of these files
what is the name and format of this file
here it is in a very simple format which
is the names of the files are like beach
one beach 2 Beach 3 and so on so I have
taken Beach as the theme so I have
images which are related to beaches so
this is beach one beach 2 and then
beach3 a few others but we will use
these three for our demo and so that's
what I'm saying here the name of the
images will be Beach something. jpeg
which is U jpeg format and in this curly
braces basically we will will be filled
with either one 2 or three based on in
this particular for Loop okay so that is
what this is doing all right so the next
step is to run inference on these images
in a loop and what we are basically
doing here is um getting these images
one by one and then running through the
model to find out what are the objects
that can be detected and then against
each of the object a box will be be
drawn and it will be labeled with the
name and the percentage of accuracy or
confidence that the model detects these
objects okay so that is uh the function
here and so let me just run that piece
of code and here is basically where we
are calling this function so we are
loading this images and then we are
calling this function uh for each image
and then we are displaying this using
the Mac plot liet Library so let's run
this it will take one image at a time
and then detect the images now the
beauty is that the same format of the
code can be used for doing object
detection in a video so we have another
video for doing object deduction in a
video so most of the code out there will
be reused from here and only thing is
that instead of reading the images from
the local storage we read the phame
frames from the video and there is a
neat little video reader that is
available and it will be shown in the
other video and frame by frame we read
the video and then pass on to this
function and it will act as if each of
these frames is an image and then it
will do the same object detection on the
entire video so that's in a separate
video just uh look out for that and
actually the information about that is
uh provided in the cards the I symbol so
that's the the video object detection in
video that's the separate tutorial all
right so now that we have all the pieces
together this the last cell is where the
whole action takes place so let's run
this and see how it looks so it will
take probably a little while and there
about three images let's see what it
detects there we go so good so the first
one it has detected a person and that to
with 97% accuracy which is uh I think
pretty good okay and then the next image
it detects umbrella and chair there are
a few other objects but it's not able to
detect it has detector umbrella with 63%
accuracy or confidence rather and uh the
chair with 58% again not bad then let's
see the next image so here these are
actually balloons hot air balloons but
the model thinks it is a kite which is
uh probably not that bad it sees there's
something in the sky and therefore
probably it thinks it is a kite and it
detects that with 65% uh confidence okay
so that was pretty much all I wanted to
show you in this particular tutorial
about uh object detection in images and
in this video I will walk you through
the tensorflow code to perform object
detection in a video so let's get
started this part is basically you're
importing all the libraries we need a
lot of these libraries for example lpai
we need image IO datetime and pill and
so on and so forth and of course mat
plot lib so we import all these
libraries and then there are a bunch of
variables which have some parts for the
files and folders so this is ready reg
stuff let's keep moving then we import
the Mac plot lib and make it in line and
uh a few more Imports all right and then
these are some warnings we can just
ignore them so if I run this code once
again it will go away all right and then
here onwards we do the model preparation
and what we're going to do is we're
going to use an existing neural network
model so we are not going to train a new
one because that really will uh take a
long time and uh it needs a lot of uh
computation resources and so on and it
is really not required there are already
models that have been trained and in
this case it is the SSD with mobile net
that's the model that we are going to
use and uh this model is trained to
detect objects and uh it is readily
available as open source so we can
actually use this and if you want to use
other models there are a few more models
available so you can click on this link
here and uh let me just take there there
are a few more models but we have chosen
this particular one because this is
faster it may not be very accurate but
that is one of the faster models but on
this link you will see a lot of other
models that are readily available these
are trained models some of them would
take a little longer but they may be
more accurate and so on so you can
probably play around with these other
models okay so we will be using that
model so this piece of code this line is
basic basically importing that model and
this is also known as uh Frozen model
the term we use is frozen model so we
import download and import that and then
we will actually use that model in our
code all right so these two cells we
have downloaded and imported the model
and then once it is available locally we
will then load this into our program all
right so we are loading this into memory
and uh you need to perform a couple of
additional steps which is basically we
need to to map the numbers to text as
you may be aware when we actually build
the model and when we run predictions
the model will not give a text the
output of the model is usually a number
so we need to map that to a text so for
example if the network predicts that the
output is five we know that five means
means it is an airlane things like that
so this mapping is done in this next
cell all right so let's keep moving and
then we have a helper code which will
basically load the data or load the
images and transform into lumpi arrays
this is also used in doing object
detection in images so we are actually
going to reuse because video is nothing
but it consists of frames which in turn
are images so we are going to pretty
much use reuse the same code which we
used for doing object detection in
images so this is where the actual
detection starts so here this is the
path for where the images are stored so
this is here once again we are reusing
the code which we wrote for detecting
objects in an image so this is the path
where the images were stored and this is
the extension and this was done for
about two or three images so we will
continue to use this and uh we go down
I'll skip this
section so this is the cell where we are
actually loading the video and
converting it into frames and then using
frame by frame we are detecting the
objects in the image so in this code
what we are doing basically is there a
few lines of code what they do is
basically once they find an object a box
will be drawn around those uh each of
those objects and the input file the
name of the input video file is uh
traffic it is the extension is MP4 and
uh we have this video reader it's a
excellent object which is basically part
of this class called image iio so we can
read and write videos using that and uh
the video that we going to use is
traffic. MP4 you can use any mp4 file
but in our case I picked up video which
has uh like car so let me just show you
so this is in this object detection
folder I have this mp4 file I'll just
quickly play this video so a little slow
yeah okay so here we go this is the
video it's a short one relatively small
video so that for this particular demo
and what it will do is once we run our
code it will detect each of these cars
and it will annotate them as cars so in
this particular video we only have cars
we can later on see with another video I
think I have cat here so we can also try
with that but let's first check with
this uh traffic video so let me go back
so we will be reading this frame by
frame and um no actually we will be
reading the video file but then we will
be analyzing it frame by frame and we
will be reading them at 10 frames per
second that is the rate we are
mentioning here and analyzing it and
then annotating and then writing it back
so you will see that we will have a
video file named something like this
traffic annotated and um we will see the
annotated video so let's go back and and
run through this piece of code and then
we will come back and see the annotated
uh video this might take a little while
so I will pause the video after running
this particular cell and then come back
to show you the results all right so
let's go ahead and run it so it is
running now and it is also important
that at the end you close the video
writer so that it is similar to a file
pointer when you open a file you should
also make sure you close it so that it
doesn't hog the resources so it's very
similar at the end of it the last piece
or last line of code should be video
writer. close all right so I'll pause
and then I'll come back okay so I will
see you in a little bit all right so now
as you can see here the processing is
done the r Glass has disappeared that
means the video has been processed so
let's go back and check the annotated
video we'll go back to my file manager
so this was the original traffic. MP4
and now you have here traffic uncore
annotated MP4 so let's go and run this
and see how it
looks you see here it has got each of
these cars are getting detected let me
pause and show you so we pause here it
says car 70% let us allow it to go a
little further it detects something on
top what is that truck okay so I think
because of the board on top it somehow
thinks there is a truck
let's play some more and see if it
detects anything else so this is again a
car looks like so let us yeah so this is
a car and it has confidence level of 69%
okay this is again a car all right so
basically till the end it goes and
detects each and every car that is
passing by now we can quickly repeat
this process for another video let me
just show you the other video which is a
cat again there is uh this cat is not
really moving or anything but it is just
standing there staring and moving a
little slowly and uh our application
will our network will detect that this
is a cat and uh even when the cat moves
a little bit in the other direction
it'll continue to detect and show that
it is a cat Okay so yeah so this is how
the original video is let's go ahead and
change our code to analyze this one and
see if it detects our Network detects
this cat close this here we go and I'll
go back to my code all we need to do is
change this traffic to cat the extension
it will automatically pick up because it
is given here and then it will run
through so very quickly once again what
it is doing is this video reader video
uncore reader has a a neat little
feature or interface whereby you can say
for frame in video uncore reader so it
will basically provide frame by frame
frame so you are in a loop frame by
frame and then you take that each frame
that is given to you you take it and
analyze it as if it is an image
individual image so that's the way it
works so it is very easy to handle this
all right so now let's once again run
just this cell rest of the stuff Remains
the Same so I will run this cell again
it will take a little while so the r
classes come back I will pause and then
come back in a little while all right so
the processing is done let's go and
check the annotated video go here so we
have cat annotated MP4 let's play this
all right so you can see here it is
detecting the cat and in the beginning
you also saw it detected something else
here there looks like it detected one
more object so let's just go back and
see what it has detected here let's see
yes so what is it trying to show here
it's too small not able to see but uh it
is trying to detect something I think it
is saying it is a car bar I don't know
all right okay so in this video there's
only pretty much only one object which
is a cat and uh let's wait for some time
and see if it continues to detect it
when the cat turns around and moves as
well just in a little bit that's going
to happen and we will see there we go
and in spite of turning the other way I
think our network is able to detect that
it is a cat so let me freeze and then
show here it is actually still continues
to detect it as a cat all right so
that's pretty much it I think that's the
only object that it detects in this
particular video okay so close this so
that's pretty much it thank you very
much for watching this video hey there
learner simply learn brings you master's
program in artificial intelligence
created in collaboration with IBM to
learn more about this course you can
find the course Link in the description
box below in this tutorial we will take
the use case of recognizing handwritten
digits this is like a hollow world of
deep learning and this is a nice little
amness database is a nice little
database that has images of handwritten
digits nicely formatted because very
often in deep learning and neural
networks we end up spending a lot of
time in preparing the data for training
and with amness database we can avoid
that you already have the data in the
right format which can be directly used
for training and mnist also offers a
bunch of built-in utility functions that
we can straight away use and call those
functions without worrying about writing
our own functions and that's one of the
reasons why mes database is very popular
for training purposes initially when
people want to learn about deep learning
and tensor flow this is the database
that is used and and it has a collection
of 70,000 handwritten digits and a large
part of them are for training then you
have test just like in any machine
learning process and then you have
validation and all of them are labeled
so you have the images and their label
and these images they look somewhat like
this so they are handwritten images
collected from a lot of individuals
people have these are samples written by
human beings they have handwritten these
numbers these numbers going from 0 to 9
so people have written these numbers and
then the images of those have been taken
and formatted in such a way that it is
very easy to handle so that is mes
database and the way we are going to
implement this in our tensor flow is we
will feed this data especially the
training data along with the label
information and uh the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the previous slides all the
images are nothing but these are pixels
so an image is nothing but an
arrangement of pixels and the value of
the pixel either it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
how they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for Simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it is at
28 by 28 so there are 7484 pixels so
pixel number one starts here it goes all
the way up to 28 then 29 starts here and
goes up to 56 and so on and the pixel
number 784 is here so we take all these
pixels flatten them out and feed them
like one single line into our neural
network and this is a what is known as a
softmax layer what it does is once it is
trained it will be able to identify what
dig digit this is so there are in this
output layer there are 10 neurons each
signifying a digit and at any given
point of time when you feed an image
only one of these 10 neurons gets
activated so for example if this is
strained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use uh a pen or a laser to show you
here okay so you're feeding a number
nine let's say this has been trained and
now if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as softmax
here so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and the first step is
to import the tensorflow library so and
we do this by using this line of code
saying import tensor flow as TF DF is
just for convenience so you can name
give any name and once you do this TF is
tens flow is available as an object in
the name of TF and then you can run its
uh methods and accesses its attributes
and so on and so forth and M database is
actually an integral part of tensor flow
and that's again another reason why we
as a first step we always use this
example mnist database example so you
just simply import mnist database as
well using this line of code and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
uh let me just uh use the pen to show
what exactly it is so when you do this
one hot true what happens is each label
is stored in the form of an array of 10
digits and let's say the number is uh8
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like array at position zero this
is at position one position two and so
on and so so fourth let's say this is
position 7 then this is position 8 that
will be one because our input is eight
and again position 9 will be zero okay
so one hot encoding this one hot
encoding true will kind of load the data
in such a way that the labels are in
such a way that only one of the digits
has a value of one and that indicate So
based on which digit is one we know what
is the label so in this this case the
eighth position is one therefore we know
this sample data the value is eight
similarly if you have a two here let's
say then the labeled information will be
somewhat like this so you have your
labels so you have this as zero the
zeroth position the first position is
also zero the second position is one
because this indicates number two and
then you have third as zero and so on
okay so that is the significance of this
one hot true all right and then we can
check how the data is uh looking by
displaying the the data and as I
mentioned earlier this is pretty much in
the form of digital form like numbers so
all these are like pixel values so you
will not really see an image in this
format but there is a way to visualize
that image I will show you in a bit and
uh this tells you how many images are
there in each set so the training there
are 55,000 images in training and in the
test set there are 10,000 and then
validation there are 5,000 so altogether
there are 70,000 images all right so
let's uh move on and we can view the
actual image by uh using the matplot
flip library and this is how you can
view this is the code for viewing the
images and you can view them in color or
you can view them in Gray scale so the
cmap is what tells in what way we want
to view it and what are the maximum
values and the minimum values of the
pixel values so these are the Max and
minimum values so of the pixel values so
maximum is one because this is a scaled
value so one means it is uh White and
zero means it is black and in between is
it can be anywhere in between black and
white and the way to train the model
there is a certain way in which you
write your denor flow code and um the
first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and um
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your you remember these are your weights
and biases so for in our case there are
10 neurons and uh each neuron actually
has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is the
second neuron this also receives all the
78 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a a
Matrix of
784 values for each of the neurons and
uh so it is like a 10 by 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned bias is only one per neuron
so it is not one per input unlike the
weights so therefore there are only 10
biases because there are only 10 neurons
in this case so that is what we are
creating a variable for biases so this
is uh something little new in tensor
flow you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular
programming language you may have
everything as variables or at the most
variables and constants but in tens
oflow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute the whole
implementation so that the execution is
stored in the form of a graph and in
this this case what we are doing is we
are doing a multiplication TF you
remember this TF was created as a
tensorflow object here one more level
one more so TF is available here now
tensor flow has what is known as a
matrix multiplication or mmal function
so that is what is being used here in
this case so we are using the matrix
multiplication of tensor flow so that
you multiply your input values x with w
right this is what we were doing x w + B
you're just adding B and this is in very
similar to one of the earlier slides
where we saw Sigma XI wi so that's what
we are doing here matrix multiplication
is multiplying all the input values with
the corresponding weights and then
adding the bias so that is the graph we
created and then we need to Define what
is our loss function and what is our
Optimizer so in this case we again use
the tensor flows apis so tf. NN softmax
cross entropy with logits is the uh API
that we will use and reduce mean is what
is like the mechanism whereby which says
that you reduce the error and Optimizer
for doing deduction of the error what
Optimizer are we using so we are using
gradient descent Optimizer we discussed
about this in couple of slides uh
earlier and for that you need to specify
the learning rate you remember we saw
that there was a a slide somewhat like
this and then you define what should be
the learning rate how fast you need to
come down that is the learning rate and
this again needs to be tested and tried
and to find out the optimum level of
this learning rate it shouldn't be very
high in which case it will not converge
or shouldn't be very low because it will
in that case it will take very long so
you define the op Optimizer and then you
call the method minimize for that
Optimizer and that will Kickstart the
training process and so far we've been
creating the graph and in order to
actually execute that graph we create
what is known as a session and then we
run that session and once the training
is completed we specify how many times
how many iterations we want it to run so
for example in this case we are saying
Thousand Steps so that is a exit
strategy in a way so you specify the
exit condition so a training will run
for th000 iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install denslow
on your local machine and uh there is a
separate video on how to set up your Tor
flow environment you can watch that if
you want to install your local
environment or you can go for other any
cloud service like for example Google
Cloud Amazon or Cloud Labs any of these
you can use and U run and try the code
okay so it is got
started we will log in
all right so this is our deep learning
tutorial uh code and uh this is our
tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensorflow and then we will
import the data and we need to adjust
the data in such a way that the one hot
is encoding is set to True one hot
encoding right as I explained earlier so
in this case the label values will be
shown appropriately and if we just check
what is the type of the data so you can
see that this is a data sets python data
sets and if we check the number of
images the way it looks so this is how
it looks it is an array of type type
float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um matte
plot clip for this and um if we take a
look at the shape now shape gives us
like the dimension
of the tensors or or or the arrays if
you will so in this case the training
data set if we sees the size of the
training data set using the method shape
it says there are 55,000 and 55,000 by
784 so remember the 784 is nothing but
the 28 by 28 28 into 28 so that is equal
to 784 so that's what it is showing now
we can take just uh one image and just
see what is the the first image and see
what is the shape so again size
obviously it is only 784 similarly you
can look at the image itself the data of
the first image itself so this is how it
it shows so large part of it will
probably be zeros because as you can
imagine in the image only certain areas
are written rest is U blank so that's
why you will mostly see Zero either it
is black or white but then there are
these values are so the Valu are
actually they are scaled so the values
are between zero and one okay so this is
what you're seeing so certain locations
there are some values and then other
locations there are zeros so that is how
the data is stored and loaded if we want
to actually see what is the value of the
handwritten image if you want to view it
this is how you view it so you create
like do this reshape and um matplot lib
has has this um feature to show you
these images so we will actually use the
function called um IM am show and then
if you pass this parameters
appropriately you will be able to see
the different images now I can change
the values in this position so which
image we are looking at right so we can
say if I want to see what is there in
maybe
5,000 right
so 5,000 has three similarly you can
just say five what is in five five has
eight what is in
50 again H so basically by the way if
you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with Jupiter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all
so that is
how this code gets executed and um here
again we can check what is the maximum
value and what is the minimum value of
this pixel values as I mentioned this is
it is scaled so therefore it is between
the values lie between 1 and zero now
this is where we create our
model the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
values okay so one for this 10 is for
each neuron there are 10 neurons and 784
is for the pixel values inputs that are
given which is 28 into 28 and the biases
as I mentioned one for each neuron so
there will be 10 biases they are stored
in a variable by the name b and this is
the graph which is basically the
multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and Y uncore true as a second
placeholder and this will have values in
the form of uh 10 digigit 10 digigit uh
arrays and uh since we said one hot
encoded the position which has a one
value indicates what is the label for
that particular number all right then we
have cross ENT ropy which is nothing but
the loss loss function and we have the
optimizer we have chosen gradient
descent as our Optimizer then the
training process itself so the training
process is nothing but to minimize the
cross entropy which is again nothing but
the loss function so we Define all of
this in the form of a graph so the up to
here remember what we have done is we
have not exactly executed any tens oflow
code till now we are just preparing the
graph the execution plan that's how the
tensorflow code works so the whole
structure and format of this code will
be completely different from how we
normally do programming so even with
people with programming experience may
find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
uh video also maybe a couple of times to
understand this flow because the way
tensorflow programming is done is
slightly different from the normal
programming some of you who let's say
have done uh maybe spark programming to
some extent will be able to easily
understand this uh but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tens of flow so even
the code has to be written in a
completely different way so the code
doesn't get executed uh in the same way
as you have written so that that's
something you need to understand and a
little bit of practice is needed for
this so so far what we have done up to
here is creating the variables and
feeding the variables and um or rather
not feeding but setting up the variables
and uh the graph that's all defining
maybe the uh what kind of a network you
want to use for example we want to use
soft Max and so on so you have created
the variables how to load the data
loaded the data viewed the data and
prepared everything but you have not yet
executed anything in tens of flow now
the next step is the execution in tens
of flow so the first step for doing any
execution in tens of flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a a node for
initializing so this is a node you still
are not yet executing anything here you
just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
intensive flow and in order to execute
the code what you will need is a session
tensor flow session so so tf. session
will give you a session and there are a
couple of different ways in which you
can do this but one of the most common
methods of doing this is with what is
known as a width Loop so you have a
width tf. session as SS and with a uh
colon here and this is like a block
starting of the block and these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this with block this
is known as a withd block so with tf.
session as sess you say cs. run in it
now ss. run will execute a node that is
specified here so for example here we
are saying ss. run sess is basically an
instance of the session right so here we
are saying TF do session so an instance
of the session gets created and we are
calling that sess and then we run a node
within that one of the nodes in the
graph so one of the nodes here is in it
so we say run that particular node and
that is when the initialization of the
variables happens now what this does is
if you have any variables in your code
in our case we have W is a variable and
B is a variable so any variables that we
created you have to run this code you
have to run the initialization of these
variables otherwise you will get an
error okay so that is the that's what
this is doing then we within this width
block we specify a for Loop and we are
saying we want the system to iterate for
thousand steps and perform the training
that's what this for Loop does run
training for thousand
iterations and what it is doing
basically is it is fetching the data or
these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always do in batches so we
have maybe 50,000 images but you always
do it in batches of 100 or maybe 500
depending on the size of your system and
so on and so forth so in this case we
are saying okay get me 100 uh images at
a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you
are not in machine learning also not
this is not specific to deep learning
but in machine learning in general you
have what is known as training data set
and test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and You
observe here that we are actually
calling an mest function here so we are
saying Mist train. nextt batch right so
this is the advantage of using Mist
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
we use this for the initial learning
phase okay so when we say fetch what it
will do is is it will fetch the images
into X and the labels into Y and then
you use this batch of 100 images and you
run the training so cs. run basically
what we are doing here is we are running
the training mechanism which is nothing
but it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously the
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and Bs
get updated till it reaches th000
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
thousand iterations and typically by the
end of this thousand iterations the
model would have learned to recognize
these handwritten images obviously it
will not be 100% accurate okay so once
that is
done after so this happens for thousand
iterations once that is done you then
test the accuracy of these models by
using the test data set
right so this is what we are trying to
do here the code may appear a little
complicated because if you're seeing
this for the first time you need to
understand uh the various methods of
tensor flow and so on but it is
basically comparing the output with the
what has been what is actually there
that's all it is doing so you have your
test data and uh you're trying to find
out what is the actual value and what is
the predicted value and seeing whether
they are equal or not TF do equal right
and how many of them are correct and so
on and so forth and based on that the
accuracy is uh calculated as well so
this is the accuracy and uh that is what
we are trying to see how accurate the
model is in predicting these uh numbers
or these digits okay so let us run this
this entire thing is in one cell so we
will have to just run it in one shot it
may take a little while let us see and
uh not bad so it has finished the
thousand iterations and what we see here
as an output is the accuracy so we see
that the accuracy of this model is
around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and uh so that
this accuracy can be increased Beyond
90% hey there learner simply learn
brings you master's program in
artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
we're going to dive right into what is
carass we're also uh go all the way
through this into a couple of tutorials
because that's where you really learn a
lot is when you roll up your sleeves so
we talk about what is carass carass is a
highlevel deep learning API written in
Python for easy impl implementation of
neural networks uses deep learning
Frameworks such as tensor flow pie torch
Etc is backend to make computation
faster and this is really nice because
as a programmer there is so much stuff
out there and it's evolving so fast
it can get confusing and having some
kind of high level order in there we can
actually view it and easily program
these different neural networks uh is
really powerful it's really powerful to
to um uh have something out really quick
and also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensor flow
pytorch um ml play Etc as a backend for
fast computation while providing a user
friendly and easy to learn frontend and
you can see here we have the cross API
uh specifications and under that you'd
have like TF carass for tensor flow
thano carass and so on and then you have
your tensorflow workflow that this is
all sitting on top of and this is like I
said it organizes everything the heavy
lifting is still done by tensor flow or
whatever you know underlying package you
put in there and this is really nice
because you don't have to um dig as
deeply into the heavy-end stuff while
still having a very robust package you
can get up and running rather quickly
and it doesn't distract from the
processing time because all the heavy
lifting is done by packages like tensor
flow this is the organization on top of
it so the working principle of
carass uh the working principle of
carass is carass uses computational
graphs to express and evaluate
mathematical
Expressions you can see here we put them
in blue they have the expression um
expressing complex problems as a
combination of simple mathematical
operators uh where we have like the
percentage or in this case in Python
that's usually your uh left your um
remainder or multiplication uh you might
have the operator of x uh to the power
of3 and it uses useful for calculating
derivatives by using uh back propagation
so if we're doing with neural networks
when we send the error back up to figure
out how to change it uh this makes it
really easy to do that without really
having not banging your head and having
to hand write everything it's easier to
implement distributed computation and
for solving complex problems uh specify
input and outputs and make sure all
nodes are
connected and so this is really nice as
you come in through is that um as your
layers are going in there you can get
some very complicated uh different
setups nowadays which we'll look at in
just a second and this just makes it
really easy to start spinning this stuff
up in trying out the different models so
when we look at carass models uh carass
model we have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next
layer and this if you've done anything
else even like the sklearn with their
neural networks and propagation and any
of these setups this should look
familiar you should have your input
layer it goes into your layer one layer
two and then to the output layer and
it's useful for simple classifier decod
coder models and you can see down here
we have the model equals a coros
sequential and this is the actual code
you can see how easy it is uh we have a
layer that's dense your layer one has an
activation they're using the ru in this
particular example and then you have
your name layer one layer dense Rao name
Layer Two and so forth uh and they just
feed right into each other so it's
really easy just to stack them as you
can see here and it automatically takes
care of everything else for you and then
there's a functional model
and this is really where things are at
this is new make sure you update your
caros or you'll find yourself running
this um doing the functional model
you'll run into an error code because
this is a fairly new release and he uses
multi-input and multi-output model the
complex model which Forks into two or
more branches and you can see here we
have our image inputs equals your carass
input shape equals 32x 32x 3 you have
your dense layers dense 64 activation
railu this should look similar to what
you already saw before uh but if you
look at the graph on the right it's
going to be a lot easier to see what's
going on you have two different
inputs uh and one way you could think of
this is maybe one of those is a small
image and one of those is a full-sized
image and that feedback goes into you
might feed both of them into one node
because it's looking for one thing and
then only into one Noe for the other one
and so you can start to get kind of an
idea that there's a lot of use for this
kind of split and this kind of setup uh
where we have multiple information
coming in but the information's very
different even though it overlaps and
you don't want it to send it through the
same neural network um and they're
finding that this trains faster and is
also has a better result depending on
how you split the data up and and how
you Fork the models coming
down and so in here we do have the two
complex uh models coming in uh we have
our image inputs which is a 32x 32 by3
your three channels or four if you're
having an alpha channel uh you have your
dense your layers Den is 64 activation
using the railu very common uh x equals
dense inputs X layers dense x64
activation equals Ru X outputs equals
layers dense 10 X model equals coros
model inputs equals inputs outputs
equals outputs name equals nin
model uh so we add a little name on
there and again this is this kind of
split here this is setting us up to um
have the input go into different areas
so if you're already looking at corus
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve solve complex maical
problems and so just like the neuron one
neuron fires in and it fires out to all
these other neurons or nodes as we call
them and eventually they all come down
to your output layer and you can see
here we have the really standard graph
input layer a hidden layer and an output
layer one of the biggest parts of any
data processing is your data
pre-processing uh so we always have to
touch base on that with a neural network
like many of these mod models they're
kind of uh when you first start using
them they're like a black box you put
your data in you train it and you test
it and see how good it was and you have
to pre-process that data because bad
data in is uh bad outputs so in data
pre-processing we will create our own
data examples set with carass the data
consists of a clinical trial conducted
on 2100 patients ranging from ages 13 to
100 with a the the patients under 65 and
the other half over 65 years of age we
want to find the possibility of a
patient experiencing side effects due to
their age and you can think of this in
today's world with uh coid uh what's
going to happen on there and we're going
to go ahead and do an example of that in
our uh life Hands-On like I said most of
this you really need to have handson to
understand so let's go ahead and bring
up our anaconda and uh I'll open that up
and open up a Jupiter notebook for doing
the python code in now if you're not
familiar with those you can use pretty
much any of your uh setups I just like
those for doing demos and uh showing
people especially shareholders it really
helps it's a nice visual so let me go
and flip over to our anaconda and the
Anaconda has a lot of cool to tools they
just added datal lore and IBM Watson
Studio clad into the Anaconda framework
but we'll be in the Jupiter lab or
Jupiter notebook um I'm going to do
jupyter notebook for this because I use
the lab for like large projects with
multiple pieces because it has multiple
tabs where the notebook will work fine
for what we're doing and this opens up
in our browser window because that's how
Jupiter notebook soorry Jupiter notebook
is set to run and we'll go under new
create a new Python 3 and uh it creates
an Untitled python we'll go ahead and
give this a title and we'll just call
this uh
cross
tutorial
and let's change that to a capital there
we go we'll go and just rename that and
the first thing we want to go ahead and
do is uh get some pre-processing tools
involved and so we need to go ahead and
import some stuff for that like our
numpy do some random number
Generation Um I mentioned sklearn or
your s kit if you're installing sklearn
the SK learn stuff it's a s kit you want
to look
up that should be a tool of anybody who
is um doing data science if if you're
not if you're not familiar with the
sklearn
toolkit it's huge uh but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples uh
for training our
data and then just a note of what we're
we're actually doing in here uh let me
go ahead and change this this is kind of
a fun thing you can do we can change the
code to markdown
and then markdown code is nice for doing
examples once you've already built this
uh our example data we're going to do
experimental there we go experimental
drug was tested on 2100 individuals
between 13 to 100 years of age half the
participants are under 65 and 95% of
participants are under 65 experience no
side effects well 95% of participants
over 65 um experience side effects
so that's kind of where we're starting
at um and this is just a real quick
example because we're going to do
another one with a little bit more uh
complicated
information uh and so we want to go
ahead and
generate our setup uh so we want to do
for I in range and we want to go ahead
and create if you look here we have
random
integers train the labels a pin so we're
just creating some random
data uh let me go ahead and just run
that
and so once we've created our random
data and if you if I mean you can
certainly ask for a copy of the code
from Simply learn they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples a pin um and we're
just using this I do this kind of stuff
all the time I was running a thing on uh
that had to do with errors following a
bell shaped curve on uh a standard
distribution error and so what do I do I
generate the data on a standard
distribution error to see what it looks
like and how my code processes it since
that was the Baseline I was looking for
and this we're just doing uh uh
generating random data for our setup on
here and uh we could actually go in um
print some of the data up let's just do
this
print um we'll do
train
samples and we'll just do the first um
five pieces of data in there to see what
that looks like
and you can see the first five pieces of
data in our train samples is 49 85 41 68
19 just random numbers generated in
there that's all that is uh and we
generated significantly more than that
um let's see 50 up here 1,000 yeah so
there's 1,000 here 1,000 numbers we
generated and we could also if we wanted
to find that out we could do a quick uh
print the length of
it and so or you could do a shape kind
of thing and if you're using
numpy although the link for this is just
fine and there we go it's actually 2100
like we said in the demo setup in
there and then we want to go ahead and
take our labels oh that was our train
labels we also did samples didn't
we uh so we could also print do the same
thing oh
labels um and let's change this
to
labels and
[Music]
labels and run that just a double check
and sure enough we have 2100 and they're
labeled one0 one0 one0 I guess that's if
they have symptoms or not one symptoms
uh Zer none and so we want to go ahead
and take our train labels and we'll
convert it into a numpy array and the
same thing with our samples and let's go
ahead and run that and we also Shuffle
uh this is just a neat feature you can
do in uh numpy right here put my drawing
thing on which I didn't have on earlier
um I can take the data and I can Shuffle
it uh so we have our so it's it just
randomizes it that's all that's doing um
we've already randomized it so it's kind
of an Overkill it's not really
necessary but if you're doing uh a
larger package where the data is coming
coming in and a lot of times it's
organized somehow and you want to
randomize it just to make sure that that
you know the input doesn't follow a
certain pattern uh that might create a
bias in your model and we go ahead and
create a scaler uh the scaler range uh
minimum Max scaler feature range 0 to
one uh then we go ahead and scale the uh
scaled train samples we're going to go
ahead and fit and transform the data uh
so it's nice and scaled and that is the
age uh so you can see up here we have 49
85 41 we're just moving that so it's
going to be uh between zero and one and
so this is true with any of your neural
networks you really want to convert the
data uh to zero and one otherwise you
create a bias uh so if you have like a
100 creates a bias
versus the math behind it gets really
complicated um if you actually start
multiplying stuff there's a lot of
multiplication addition going on in
there that higher end value will
eventually multiply down and it will
have a huge bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in Jupiter notebook is that if you have
a variable and you're not doing anything
with it it's the last one on the line it
will automatically
print um and we're just going to look at
the first five samples on here and so
it's going to print the first five
samples and you can see here we go uh
995 791 so everything's between zero and
one and that just shows us that we
scaled it properly and it looks good uh
it really helps a lot to do these kind
of print UPS halfway through uh you
never know what's going to go on
there I don't know how many times I've
gotten down and found out that the data
sent to me that I thought was scaled was
not and then I have to go back and track
it down and figure it out on
there uh so let's go ahead and create
our artificial neural
network and for doing that this is where
we start diving into tensor flow and
carass uh tensor
flow if you don't know the history of
tensor
flow it helps to uh jump into we'll just
use
Wikipedia careful don't quote Wikipedia
on these things because you get in
trouble uh but it's a good place to
start uh back in 2011 Google brain built
disbelief as a proprietary stre learning
setup tensor flow became the open source
for it uh so tensorflow was a Google
product and then it became uh open
sourced and now it's just become
probably one of the defacto when it
comes for neural networks as far as
where we're at uh so when you see the
tensorflow
setup it it's got like a huge following
there are some other setups like um the
S kit under the sklearn has our own
little neural network uh but the
tensorflow is the most robust one out
there right now and carass sitting on
top of it makes it a very powerful tool
so we can leverage both the carass uh
easiness in which we can build a
sequential setup on top of tensor flow
and so in here we're going to go ahead
and do our input of tensor flow uh and
then we have the rest of this is all
coros here from number two down uh we're
going to import from tensor flow the
coros uh connection and then you have
your tensorflow cross models import
sequential it's a specific kind of model
we'll look at that in just a second if
you remember from the files that means
it goes from one layer to the next layer
to the next layer there's no funky
splits or anything like
that uh and then we have from tensorflow
Cross layers we're going to import our
activation and our dense
layer and we have our Optimizer atom um
this is a big thing to be aware of how
you optimize uh your data when you first
do it atom's as good as any atom is
usually uh there's a number of Optimizer
out there there's about uh there's a
couple main ons but atom is usually
assigned to bigger data uh it works fine
usually the lower data does it just fine
but adom is probably the mostly used but
there are some more out there and
depending on what you're doing with your
layers your different layers might have
different activations on them and then
finally down here you'll will'll see um
our setup where we want to go ahead and
use the
metrics and we're going to use the
tensorflow coros metrics um for
categorical cross centropy uh so we can
see how everything performs when we're
done that's all that is um a lot of
times you'll see us go back and forth
between tensor flow and then scikit has
a lot of really good metrics also for
measuring these things um again it's the
end of the you know at the end of the
story how good does your model do and
we'll go ahead and load all that that
and then comes the fun part um I
actually like to spend hours messing
with these
things and uh four lines of code you're
like ah you're G to spend hours on four
lines of code um no we don't spend hours
on four lines of code that's not what
we're talking about when I say spend
hours on four lines of code uh what we
have here I'm going explain that in just
a second we have a model and it's a
sequential model if you remember
correctly we mentioned the sequential up
here where it goes from one layer to the
next and our first layer is going to be
our
input it's going to be uh what they call
D which is um usually it's just D and
then you have your input and your
activation um how many units are coming
in we have 16 uh what's the shape What's
the activation and this is where it gets
interesting um because we have in here
uh
railu on two of these and softmax
activation on one of these there are so
many different options for what these
mean um and how they function how does
the ru how does the soft Max
function and they do a lot of different
things um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different
activations um and just some of it is
just uh um almost like you're playing
with it
like an artist you start getting a fill
for like a uh inverse tangent activation
or the tan
activation takes up a huge processing
amount uh so you don't see it a lot yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing Word documents and
you're tokenizing the words and so
you'll see the shift from one to the
other because you're both trying to
build a better model and if you're
working on a huge data set um it'll
crash the system it'll just take too
long to process um and then you see
things like soft Max uh soft Max
generates an interesting um
setup where a lot of these when you talk
about railu it oops let me do this uh
railu there we go Ru has um a setup
where if it's less than zero it's zero
and then it goes up um and then you
might have with they call lazy uh setup
where it has a slight negative to it so
that the errors can translate better
same thing with softmax it has a slight
laziness to it so that errors translate
better all these little
details make a huge different on your
model um so one of the really cool
things about data science that I like is
you build your uh what they call you
build def fail and it's an interesting
uh design setup oops I forgot the end of
my cover
here the concept of build a fail is you
want the model as a whole to work so you
can test your model
out so you can do um you can get to the
end and you can do your let's see where
was it overshot down here you can test
your test out the the quality of your
setup on there and see where did I do my
tensor flow oh here we go I it was right
above me here we go we start doing your
cross entropy and stuff like that is you
need a full functional set of code so
that when you run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
um and then you can start swapping in
these models and so when I say spend a
huge amount of time on pre-processing
data is probably 80% of your programming
time um well between those two it's like
8020 you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
uh set depending on your data your
models get more and more robust as you
start experimenting with different
inputs different data streams and all
kinds of things and we can do a simple
model summary
here uh here's our sequential here's our
layer output our
parameter this is one of the nice things
about coros is you just you can see
right here here's our sequential one
model boom boom boom boom everything's
set and clear and easy to read so once
we have our model built uh the next
thing we're going to want to do is we're
want to go ahead and train that
model and so the next step is of course
model training and when we come in here
this a lot of times is just paired with
the model because it's so
straightforward it's nice to print out
the model setup so you can have a
tracking but here's our model uh the key
word in carass is
compile Optimizer atom learning rate
another term right there that we're just
skipping right over that really becomes
the meat of um the setup is your
learning rate uh so whoops I forgot that
I had an arrow but I'll just underline
it a lot of times the learning rate set
to 0.0 uh set to 0.01 uh depending on
what you're doing this learning rate um
can overfit and underfit uh so you'd
want to look up I know we have a number
of tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss um
sparse categorical cross entropy so this
is going to tell carass how far to go
until it stops and then we're looking
for metrics of accuracy so we'll go
ahead and run that and now that we've
compiled our model we want to go ahead
and um run it fit it so here's our model
fit um we have our scaled train
samples our train labels our validation
split um in this case we're going to use
10% of the data for
validation uh batch size another number
you kind of play with not a huge
difference as far as how it works but it
does affect how long it takes to run and
it can also affect the bias a little bit
uh most of the time though a batch size
is between 10 to 100 um depending on
just how much data you're processing in
there we want to go ahead and Shuffle it
uh we're going to go through 30 epics
and uh put a verbose of two and let me
just go and run this and you can see
right here here's our epic here's our
training um here's our loss now if you
remember correctly up here we set the
loss let's see where was it um compiled
our
data there we go loss uh so it's looking
at the sparse categorical cross entropy
this tells us that as it goes how how
how much um how how much does the um
error go down uh is the best way to look
at that and you can see here the lower
the number the better it just keeps
going down and vice versa accuracy we
want let's see where's my
accuracy value accuracy at the end uh
and you can see 619 69. 74 it's going up
we want the accuracy would be ideal if
it made it all the way to one but we
also the loss is more important because
it's a balance um you can have 100%
accuracy and your model doesn't work
because it's overfitted uh again you
won't look up U overfitting and
underfitting
models and we went ahead and went
through uh 30 epics it's always fun to
kind of watch your code going um to be
honest I usually uh um the first time I
run it I'm like Ah that's cool I get to
see what it does and after the second
time of running it I'm like i' like to
just not see that and you can repress
those of course in your code uh repress
the warnings in the
printing and so the next step is going
to be building a test set and predicting
it now uh so here we go we want to go
ahead and build our test set and we have
uh just like we did our training
set a lot of times you just split your
your initial set up uh but we'll go
ahead and do a separate set on on here
and this is just what we did above uh
there's no difference as far as
um the randomness that we're using to
build this set on here uh the only
difference is
that we are already um did our scaler up
here well it doesn't matter because the
the data is going to be across the same
thing but this should just be just
transformed down here instead of fit
transform uh because you don't want to
refit your data um on your testing data
there we go and now we're just
transforming it because you never want
to transform the test data um easy
mistake to make especially on an example
like this where we're not doing um you
know we're randomizing the data anyway
so it doesn't matter too much because
we're not expecting something
weird and then we ahead and do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to and do here's
our xcal data batch size 10 verbose and
now we have our predictions in here and
we could go ahead and do a um oh we'll
print
predictions and then I guess I could
just put down predictions and five so we
can look at the first five of the
predictions and we have here is we have
our
age and uh the prediction on this versus
what what we think it's going to be what
what we think is going to going to have
uh symptoms or not and the first thing
we notice is that's hard to read because
we really want a yes no answer uh so
we'll go ahead and just uh round off the
predictions using the argmax um the
numpy argmax uh for prediction so it
just goes to
a01 and if you remember this is a
Jupiter notebook so I don't have to put
the print I can just put in uh rounded
predictions and we'll just do the first
five and you can see here 0 1 0 0 0 so
that's what the predictions are that we
have coming out of this um is no
symptoms symptoms no symptoms symptoms
no symptoms and just as uh we were
talking about at the beginning we want
to go ahead and um take a look at this
there we go confusion matrixes for
accuracy check um most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the pyit um the SK learn metrics uh pyit
being where that comes from import
confusion Matrix uh some iteration tools
and of course a nice map plot library
that makes a big difference so it's
always nice to
um have a nice graph to look at u
pictures worth a thousand
words um and then we'll go ahead and do
call it CM for confusion Matrix y true
equals test labels y predict rounded
predictions and we'll go ahead and load
in our
cm and I'm not going to spend too much
time on the plotting um going over the
different plotting
code um you can spend uh like whole we
have whole tutorials on how to do your
different plotting on there uh but we do
have here is we're going to do a plot
confusion Matrix there's our CM our
classes normalized false title confusion
Matrix cmap is going to be in
blues and you can see here we have uh to
the nearest cmap titles all the
different pieces whether you put tick
marks or not the marks the classes the
color bar um so a lot of different
information on here as far as how we're
doing the printing of the of the
confusion Matrix you can also just dump
the confusion Matrix um into a caborn
and real quick get an output put it's
worth knowing how to do all this uh when
you're doing a presentation to the
shareholders you don't want to do this
on the Fly you want to take the time to
make it look really nice uh like our
guys in the back did and uh let's go
ahead and do this forgot to put together
our CM plot labels we'll go and run
that and then we'll go ahead and call
the little the
definition for our
mapping and you can see here plot
confusion Matrix that's our the the
little script we just wrote and we're
going to dump our data into it um so our
confusion Matrix our classes um title
confusion Matrix and let's just go ahead
and run
that and you can see here we have our
basic setup uh no side effects
195 had side effects uh 200 no side
effects that had side effects so we
predicted the 10 of them who actually
had side effects and that's pretty good
I mean I I don't know about you but you
know that's 5% error on this and this is
because there's 200 here that's where I
get 5% is uh divide these both by by two
and you get five out of a 100 uh you can
do the same kind of math up here not as
quick on the Fly it's 15 and 195 not an
easily rounded number but you can see
here where they have 15
people who predicted to have no uh with
the no side effects but had side effects
kind of setup on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show uh whatever you're
working on comes up and you can actually
show them hey this is how good we are or
not how messed up it
is so this was a uh I spent a lot of
time on some of the parts uh but you can
see here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
uh here's our model
summary and we just have the layers on
here that we built with our model on
this and then we went ahead and trained
it and ran the prediction now we can get
a lot more complicated uh let me flip
back on over here because we're going to
do another uh demo so that was our basic
introduction to it we talked about the
uh oops here we go okay so implementing
a neural network with carass after
creating our samples and labels we need
to create our carass neural network
model we will be working with a
sequential model which has three layers
and this is what we we had our input
layer our hidden layers and our output
layers and you can see the input layer
uh coming in uh was the age Factor we
had our hidden layer and then we had the
output are you going to have symptoms or
not so we're going to go ahead and go
with something a little bit more
complicated um training our model is a
two-step process we first compile our
model and then we train it in our
training data set uh so we have
compiling compiling converts the code
into a form of understandable by Machine
we use used the atom in the last example
a gradient descent algorithm to optimize
a model and then we trained our model
which means it let it uh learn on
training data uh and I actually had a
little backwards there but this is what
we just did is we if you remember from
our code we just had let me go back here
um here's our model that we
created
summarized uh we come down here and we
compile it so it tells it hey we're
ready to build this model and use it uh
and then we train it and this is the
part where we go ahead and fit our model
and and put that information in here and
it goes through the training on there
and of course we scaled the data which
was really important to do and then you
saw we did the creating a confusion
Matrix with caras um as we are
performing classifications on our data
we need a confusion Matrix to check the
results a confusion Matrix breaks down
the various
misclassifications as well as correct
classifications to to get the
accuracy um and so you can see here this
is what we did with the true positive
false positive true negative false
negative and that is what we went over
let me just scroll down
here on the end we printed it out and
you can see we have a nice print out of
our confusion Matrix uh with the true
positive false positive false negative
true negative and so the blue ones uh we
want those to be the biggest numbers
because those are the better side and
then
uh we have our false predictions on here
uh as far as this one so it had no side
effects but we predicted let's see no
side effects predicting side effects and
vice versa if getting your learning
started is half the battle what if you
could do that for free visit skill up by
simply learn click on the link in the
description to know more now uh saving
and loading models with carass we're
going to dive into a more complicated
demo um and you're going to say oh that
was a lot of complication before well if
you broke it down we randomized some
data we created the um carass setup we
compiled it we trained it we predicted
and we ran our Matrix uh so we're going
to dive into something a lot little bit
more fun is we're going to do a face
mask detection with carass uh so we're
going to build a carass model to check
if a person is wearing a mask or not in
real time and this might be important if
you're at the front of a store this is
something today which is um might be
very use ful as far as some of our you
know making sure people are
safe uh and so we're going to look at
mask and no mask and let's start with a
little bit on the
data and so in my data I have with a
mask you can see they just have a number
of images showing the people in masks
and again if you want some of this
information uh contact simply learn and
they can send you some of the
information as far as people with and
without masks so you can try it on your
own and this is just such a wonderful
example of this setup on here so before
I dive into the mass detection uh
talking about being in the current with
uh coid and seeing that people are
wearing masks this particular example I
had to go ahead and update to a python
3.8 version uh it might run into a 37
I'm not sure I hav I kind of skipped 37
and installed
38 uh so I'll be running in a three
python 38 um and then you also want to
make sure your tensor flow is up to date
because the um they call
functional uh layers that's where they
split if you remember correctly from
back uh oh let's take a look at this
remember from here the functional model
and a functional layer allows us to feed
in the different layers into different
you know different nodes into different
layers and split them uh very powerful
tool very popular right now in the edge
of where things are with neural networks
and creating a better model
so I've upgraded to python 3.8 and let's
go ahead and open that up and go through
uh our next example which includes uh
multiple layers um programming it to
recognize whether someone wears a mask
or not and then uh saving that model so
we can use it in real time so we're
actually almost a full um end to end
development of a product here uh of
course this is a very simplified version
and there'd be a lot more to it you'd
also have to do like a recognizing
whether it's someone's face or not all
kinds of other things go into this so
let's go ahead and jump into that code
and we'll open up a new Python 3 oops
Python 3 it's working on it there we
go um and then we want to go ahead and
train our mask we'll just call this
train
mask and we want to go ahead and train
mask and save it uh so it's it's uh save
mask train mask detection not to be
confused with masking data a little bit
different we're actually talking about a
physical mask on your
face and then from the cross standpoint
we got a lot of imports to do here and
I'm not going to dig too deep on the
Imports uh we're just going to go ahead
and notice a few of them uh so we have
in here oops me go alt D there we go
have something to draw with a little bit
here we have our uh image
processing and the image processing
right here me underline that uh deals
with how do we bring images in because
most images are like a a square grid and
then each value in there has three
values for the three different colors uh
cross and tensorflow do a really good
job of uh working with that so you don't
have to do all the heavy listening and
figuring out what's going to go on uh
and we have the mobile net average
pooling 2D um this again is how do we
deal with the images and pulling them uh
dropout's a cool thing worth looking up
if you haven't when as you get more and
more into carass and tensor flow uh
it'll Auto drop out certain notes that
way you'll get a better um the notes
just kind of die uh they find that they
actually create more of a bias than a
help and they also add processing time
so they remove them um and then we have
our flatten that's where you take that
huge array with the three different
colors and you find a way to flatten it
so it's just a one-dimensional array
instead of a 2X two
by3 uh D input we did that in the other
one so that should look a little
familiar oops there we go our input um
our model again these are things we had
on the last one here's our Optimizer
with our
atom um we have some pre-processing on
the input that goes along with bringing
in the data in uh more pre-processing
with image to array loading the image um
this stuff is so nice it looks like a
lot of work you have to import all these
different modules in here but the truth
is is it does everything for you you're
not doing a lot of pre-processing you're
letting the software do the
pre-processing um and we're going to be
working with the setting something to
categorical again that's just a
conversion from a number to a category
uh 01 doesn't really mean it's like true
false um label binarized the same thing
uh we're changing our labels around and
then there's our train test split
classification report um our IM
utilities let me just go ahead and
scroll down here Notch for these this is
something a little different going on
down here this is not part of the uh
tensor flow or the SK learn this is the
S kit setup and tensor flow above uh the
path this is part part of um open CVV
and we'll actually have another tutorial
going out with the open CV so if you
want to know more about Open CV you'll
get a glance on it in uh this software
especially the ne the second piece when
we reload up the data and hook it up to
a video camera we're going to do that on
this round um but this is part of the
open CV thing and you'll see CV2 is
usually how that's referenced um but the
IM utilities has to do with how do you
rotate pictures around and stuff like
that uh and res size them and then the
map plot library for plotting because
it's nice to have a graph tells us how
good we're doing and then of course our
numpy numbers array and just a straight
OS access wow so that was a lot of
imports uh like I said I'm not going to
spend I spent a little time going
through them uh but we didn't want to go
too much into
them and then I'm going to create um
some variables that we need to go ahead
and initialize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we we talked about the learning rate uh
to the4
.001 um a lot of times it's 0.001 or
0.001 usually it's in that uh variation
depending on what you're doing and how
many epics and they kind of play with
the epics the Epic is how many times are
we going to go through all the
data now I have it as two um the actual
setup is for 20 and 20 works great the
reason I have it for two is it takes a
long time to proc process one of the
downsides of
Jupiter is that Jupiter isolates it to a
single kernel so even though I'm on an
eight core processor uh with 16
dedicated threads only one thread is
running on this no matter what so it
doesn't matter uh so it takes a lot
longer to run even though um tensor flow
really scales up nicely and the batch
size is how many pictures do we load at
once in process again those are numbers
you have to learn to play with depending
on your your data and what's coming in
and the last thing we want to go ahead
and do is there's a directory with a
data set we're going to
run uh and this just has images of MKS
and not
MKS and if we go in here you'll see data
set um and you have pictures with mass
they're just images of people with mass
on their face uh and then we have the
opposite let me go back up here without
masks so it's pretty straightforward
they look kind of a skew because they
tried to format them into very similar
uh setup on there so they're they're
mostly squares you'll see some that are
slightly different on here and that's
kind of important thing to do on a lot
of these data sets get them as close as
you can to each other and we'll we
actually will run in the in this
processing of images up here and the
cross uh layers and importing and and
dealing with images it does such a
wonderful job of converting these and a
lot of it we don't have to do a whole
lot with uh so you have a couple things
going on there and so uh we're now going
to be this is now loading the um images
and let me see and we'll go ahead and uh
create data and labels here's our um uh
here's the features going in which is
going to be our pictures and our labels
going out and then for categories in our
list directory directory and if you
remember I just flashed that at you it
had uh uh face mask or or no face mask
those are the two options and we're just
going to load in to that we're going to
pin the image itself and the labels so
we're just create a huge array uh and
you can see right now this could be an
issue if you had more data at some point
um thankfully I have a a 32 gig hard
drive or U
Ram even that does you could do with a
lot less of that probably under 16 or
even eight gigs would easily load all
this stuff um and there's a conversion
going on in here I told you about how we
are going to convert the size of the
image so res sizes all the images that
way our data is all identical the way it
comes
in and you can see here with our labeles
we have without mask without mask
without mask uh the other one would be
with mask those are the two that we have
going in
there uh and then we need to change it
to the one not hot
encoding and this is going to take our
um um up here we had what was it labels
and data uh we want the labels uh to be
kept categorical so we're going to take
labels and change it to categorical and
our labels then equal a categorical list
uh we'll run that and again if we do uh
labels and we just do the last or the
first 10 let's do the last 10 just
because um minus 10 to the end there we
go just so we can see what the other
side looks like we now have one that
means they have a mask one Z One Zer so
on uh one being they have a mask mask
and zero no mask and if we did this in
Reverse I just realized that this might
not make sense if you've never done this
before let me run this
01 so zero is uh do they have a mask on
zero do they not have a mask on one so
this is the same as what we saw up here
without mask one equals um the second
value is without mask so with mask
without mask uh and that's just a with
any of your data
processing we can't really zero if you
have a 01
output uh it causes issues as far as
training and setting it up so we always
want to use a one hot encoder if the
values are not actual uh linear Val or
regression values are not actual numbers
if they represent a
thing and so now we need to go ahead and
do our train X test X train y test y um
train split test data and we'll go ahead
and make sure it's going to be uh random
and we'll take 20% of it for testing and
the rest for um setting it up as far as
training their model this is something
that's become so cool when they're
training these Set uh they realize we
can augment the data what does augment
mean well if I rotate the data around
and I zoom in I zoom out I rotate it um
share it a little bit flip it
horizontally um fill mode as I do all
these different things to the data it um
is able to it's kind of like increasing
the number of samples I have uh so if I
have all these perfect samples what
happens when we only have part of the
face or the face is tilted sideways or
all those little shifts cause a problem
if you're doing just a standard set of
data so we're going to create an augment
and our image data generator
um which is going to rotate zoom and do
all kinds of cool thing and this is
worth looking up this image data
generator and all the different features
it has um a lot of times I'll the first
time through my models I'll leave that
out so I want to make sure there's a
thing we call build def fail which is
just cool to know you build the whole
process and then you start adding these
different things in uh so you can better
train your model and so we go and run
this and then we're going to load um and
then we need to go ahead and you
probably would have gotten an error if
you hadn't put this piece in right here
um I haven't run it myself cuz the guys
in the back did this uh we take our base
model and one of the things we want to
do is we want to do a mobile net
V2 um and this what we this is a big
thing right here include the top equals
false a lot of data comes in with a
label on the top row uh so we want to
make sure that that is not the case uh
and then the construction of the head of
the model that will be placed on the top
of the base model model uh we want to go
ahead and set that
up and you'll see a warning here I'm
kind of ignoring the warning because it
has to do with the uh size of the
pictures and the weights for input shape
um so they'll it'll switch things to
defaults to saying hey we're going to
Auto shape some of this stuff for you
you should be aware of that with this
kind of imagery we're already augmenting
it by moving it around and flipping it
and doing all kinds of things to it uh
so that's not a bad thing in there yes
but another data it might be if you're
working in a different
domain and so we're going to go back
here and we're going to have we have our
base model we're going to do our head
model equals our base model
output um and what we got here is we
have an average pooling 2D pool size 77
head model um head model flatten so
we're flattening the data uh so this is
all processing and flattening the images
and the pooling has to do with some of
the ways it can process some of the data
we'll look at that a little bit when we
get down to the lower level on this
processing it um and then we have our D
we've already talked a little bit about
a d just what you think about and then
the head model has a Dropout of
05 uh what we can do with a
Dropout the Dropout says that we're
going to drop out a certain amount of
nodes while training uh so when you
actually use the model you will use all
the nodes but this drops certain ones
out and it helps stop biases from up
forming uh so it's really a cool feature
in here they discovered this a while
back uh we have another dense mode and
this time we're using soft Max
activation lots of different activation
options here softmax is a real popular
one for a lot of things U and so is
Ru and you know there we could do a
whole talk on activation formulas uh and
why what their different uses are and
how they work when you first start out
you'll you'll use mostly the ru and the
softmax for a lot of them uh just
because they're they're some of the
basic setups it's a good place to
start uh and then we have our model
equals model inputs equals base model.
input outputs equals head model so again
we're still building our model here
we'll go ahead and run
that and then we're going to Loop over
all the layers in the base model and
freeze them so they will not be updated
during the first training process
uh so for layer and basee model layers
layers. trable equals False A lot of
times when you go through your data um
you want to kind of jump in partway
through um I I'm not sure why in the
back they did this for this particular
example um but I do this a lot when I'm
working with series and and specifically
in stock data I wanted to iterate
through the first set of 30 Data before
it does
anything um I would have to look deeper
to see why they froze it on this
particular one and then we're going to
compile our model uh so compiling the
model
atom and knit layer Decay um initial
learning rate over epics and we go ahead
and compile our loss is going to be the
binary cross entropy which will have
that print out Optimizer for opt
metric's accuracy same thing we had
before not a huge jump as far as um the
previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model uh so train the
head of the network print info training
head
run now I skipped a little time because
it you'll see the run time here is um at
80 seconds per epic takes a couple
minutes for it to get through on a
single
kernel one of the things I want you to
notice on here while we're while it's
finishing the proc
processing is that we have up here our
augment going on so anytime the train X
and trading y go in there's some
Randomness going on there and is
jiggling it around what's going into our
setup uh of course we're batch sizing it
uh so it's going through whatever we set
for the batch values how many we process
at a time and then we have the steps per
epic uh the train X the batch size
validation data here's our test X and
Test Y where we're sending that
in uh and that's again it's validation
one of the important things to know
about validation is our um when both our
training data and our test data have
about the same accuracy that's when you
want to stop that means that our model
isn't biased if you have a higher
accuracy on your uh testing you know
you've trained it and your accuracy is
higher on your actual test data then
something in there is probably uh has a
bias and it's overfitted uh so that's
what this is really about right here
with the validation data and validation
steps so it looks like it's let me go
ahead and see if it's done processing
looks like we've gone ahead and gone
through two epics again you could run
this through about 20 with this amount
of data and it would give you a nice
refined uh model at the end we're going
to stop at 2 because I really don't want
to sit around all afternoon and I'm
running this on a single thread so now
that we've done this we're going to need
to evaluate our model and see how good
it is and to do that we need to go ahead
and make our
predictions um these are predictions on
our test X to see what it thinks are
going to be uh so now it's going to be
evaluating the network and then we'll go
ahead and go down here and we will need
to uh turn the index in because remember
it's it's either zero or one it's uh 0
one 0 one so you have two outputs uh not
wearing uh wearing a mask not wearing a
mask and so we need to go ahead and take
that argument at the end and change
those predictions to a zero or one
coming out uh and then to finish that
off we want to go ahead and let me just
put this right in here and do it all in
one shot we want to show a nicely
formatted classification report so we
can see what that looks like on here and
there we have it we have our Precision
uh it's 97% with the mask there's our f
one score support without a mask
97% um so that's pretty high high uh
setup on there you know three people are
going to sneak into the store who are
without a mask and that thinks they have
a mask and there's going to be three
people with a mask that's going to flag
the person at the front to go oh hey
look at this person you might not have a
mask um if I guess it's a set up in
front of a store um so there there you
have it and of course one of the other
cool things about this is if someone's
walking into the store and you take
multiple pictures of them um you know
this is just an it it would be a way of
flagging and then you can take that
average of those pictures and make sure
they match or don't match if you're on
the back end and this is an important
step because we're going to this is just
cool I love doing this stuff uh so we're
going to go ahead and take our model and
we're going to save it uh so model save
Mass detector. model we're going to give
it a name uh we're going to save the
format um in this case we're going to
use the H5 format and so this model we
just programmed has just been saved uh
so now I can load it up into say another
program what's cool about this is let's
say I want to have somebody work on the
other part of the program well I just
saved the model they upload the model
and now they can use it for whatever and
then if I get more information uh and we
start working with that at some
point I might want to update this model
um make a better model and this is true
of so many things where I take this
model and maybe I'm uh running a
prediction on uh making money for a
company and as my model gets
better I want to keep updating it and
then it's really easy just to push that
out to the actual end user uh and here
we have a nice graph you can see the
training loss and accuracy as we go
through the epics uh we only did the you
know only shows just the one Epic coming
in here but you can see right here as
the uh um value loss train accuracy and
value accuracy see starts switching and
they start converging and you'll hear
converging this is a convergence they're
talking about when they say you're
you're um I know when I work in the S
kit with sklearn neural networks this is
what they're talking about a convergence
is our loss and our accuracy come
together and also up here and this is
why I'd run it more than just two epics
as you can see they still haven't
converged all the way uh so that would
be queue for me to keep
going but what we want to do is we want
to go ahead and create a new Python
3
program and we just did our train mask
so now we're going to go ahead and
import that and use it and show you in a
live action um get a view of uh both
myself in the afternoon along with my
background of an office which is in the
middle still of reconstruction for
another
month
and we'll call this uh
mask
detector and then we're going to grab a
bunch of um a few items coming
in uh we have our um mobilet V2 import
pre-processing input so we're still
going to need that um we still have our
tensor floor image to array we have our
load model that's where most of the
stuff's going on this is our CV2 or open
CV again I'm not going to dig too deep
into that we're going to flash a little
open CV code at you uh and we actually
have a tutorial on that coming out um
our numpy array our IM utilities which
is part of the open CV or CV2
setup uh and then we have of course time
and just our operating system so those
are the things we're going to go ahead
and set up on here and then we're going
to create this takes just a
moment
our module here which is going to do all
the heavy
lifting uh so we're going to detect and
predict a mask we have frame face net
Mass net these are going to be generated
by our open CV we have our frame coming
in and then we want to go ahead and and
create a mask around the face that's
going to try to detect the face and then
set that up so we know what we're going
to be processing through our model
model um and then there's a frame shape
here this is just our height versus
width that's all HW stands for um
they've called it blob which is a CV2
DNN blob form image frame so this is
reformatting this Frame that's going to
be coming in literally from my camera
and we'll show you that in a minute that
little piece of code that shoots that in
here uh and we're going to pass the blob
through the network and obtain the face
detections uh so faet do set inport blob
detections faet forward print detections
shape uh so these is this is what's
going on here this is that model we just
created we're going to send that in
there and I'll show you in a second
where that is but it's going to be under
face
net uh and then we go ahead and
initialize our list of faces their
corresponding locations and the list of
predictions from our face mask
Network we're going to Loop over the
detection
and this is a little bit more work than
you think um as far as looking for
different faces what happens if you have
a fa a crowd of faces um so We're
looping through the detections and the
shapes going through here and
probability associated with the
detection uh here's our confidence of
detections we're going to filter out
weak detection by ensuring the
confidence is greater than the minimum
confidence uh so we've said it remember
0o to one so 0 five would be our minimal
confidence probably is pretty good um
and then we're going to put in compute
bounding boxes for the object if I'm
zipping through this it's because we're
going to do an open CV and I really want
to stick to just the carass
part and so I'm I'm just going of
jumping through all this code you can
get a copy of this code from Simply
learn and take it apart or look for the
open CV coming
out and we'll create a box uh the box
sets it around the image
ensure the bounding boxes fall within
the dimensions of the
frame uh so we create a box around
what's going to what we hope it's going
to be the face extract the face Roi
convert it from BGR to RGB
Channel again this is an open CV issue
not really an issue but it has to do
with the order um I don't know how many
times I've forgotten to check the order
colors working with open CV because it's
all kinds of fun things when red becomes
blue and blue becomes red uh then we're
going to go ahead and resize it process
it frame it uh face frame setup again
the face the CBT color we're going to
convert it uh we're going to resize it
image to array pre-process the input uh
pin the face locate face. x. y and x boy
that was just a huge amount and I
skipped over a ton of it but the bottom
line is we're building a box around the
face and that box cuz the open CV does a
decent job of finding the the face and
that box is going to go in there and see
hey does this person have a mask on
it uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals mass net. predict
faces batch size
32 uh so these different images of where
we're guessing where the face is are
then going to go through an generate an
array of faces if you will and we're
going to look through and say does this
face have a mask on it and that's what's
going right here is our prediction
that's the big thing that we're working
for and then we return the locations and
the predictions the location just tells
where on the picture it is and then the
um prediction tells us what it is is it
a mask or is it not a mask all right so
we've loaded that all up so we're going
to load our serialized face detector
model from disc um and we have our the
path that it was saved in obviously
you're going to put it in a different
path depending on where you have it or
however you want to do it and how you
saved it on the last one where we
trained it uh then we have our weights
path um and so finally our face net here
it is equals CB2 DNN do read net U
protot text path weights path and we're
going to load that up on here so let me
go ahead and run
that and then we also need to I'll just
put it right down here I always hate
separating these things in there um and
then we're going to load the actual mass
detector model from dis this is the the
the model that we saved so let's go
ahead and run that on there also so this
is pulling in all the different pieces
we need for our model and then the next
part is we're going to create open up
our video uh and this is just kind of
fun because it's all part of the open
CV video
setup and me just put this all in as one
there we go uh so we're going to go
ahead and open up our video we're going
to start it and we're going to run it
until we're
done and this is where we get some real
like kind of live action stuff which is
fun this is what I like working about
with images and videos is that when you
start working with images and videos
it's all like right there in front of
you it's Visual and you can see what's
going on uh so we're going to start our
video streaming this is grabbing our
video stream Source zero start uh that
means it's C grabbing my main camera I
have hooked up um and then you know
starting video you're going to PR it out
here's our video Source equals zero
start Loop over the frames from the
video
stream oops a little redundancy there um
let me
close I'll just leave it that's how they
had it in the code so uh so while true
we're going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
uh from our visual uh stream we're going
to resize
it and then we have a returning remember
we returned from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
the mass net so we're sending all the
different pieces that say this is what's
going through on
here and then it returns our location
and predictions and then for our box and
predictions in the location and
predictions um and the box is is again
this is an open CV set that says hey
this is a box coming in from the
location um because you have the two
different points on there and then we're
going to unpack the box and predictions
and we're going to go ahead and do mask
without a mask equals
prediction we're going to create our
label no mask we create color if the
label equals mask L
00225 and you know this is going to make
a lot more sense when I hit the Run
button here uh but we have the
probability of the label
we're going to display the label and
bounding box rectangle on the output
frame uh and then we're going to go
ahead and show the output from the frame
CV2 IM am show frame frame and then the
key equals CV2 weit key one we're just
going to wait till the next one comes
through from our
feed and we're going to do this until we
hit the stop button pretty much so are
you ready for this let's see if it works
we've distributed our uh our model we've
loaded it up into our distributed uh
code here we've got it hooked into our
camera and we're going to go ahead and
run
it and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
popup and there I am in my office with
my funky headset
on uh and you can see in the background
my unfinished wall and it says up here
no mask oh no I don't have a mask on uh
I wonder if I cover my mouth what would
happen uh you can see my no mask
goes down a little bit I wish I'd
brought a mask into my office it's up at
the house but you can see here that this
says you know there's a 95 98% chance
that I don't have a mask on and it's
true I don't have a mask on right now
and this could be distributed this is
actually an excellent little piece of
script that you could start you know you
install somewhere on a a video feed on a
on a security camera or something and
then you'd have this really neat uh
setup saying hey do you have a mask on
when you enter a store or a public
transportation or whatever it is or
they're required to wear a mask uh let
me goe and stop
that now if you want a copy of this uh
code definitely give us a hauler we will
be going into open CV in another one so
I skipped a lot of the open CV um code
in here as far as going into detail
really focusing on the carass uh saving
the model uploading the model and then
processing a streaming video through it
so you can see that the model model
works we actually have this working
model that hooks into the video
camera which is just pretty cool and a
lot of
fun so I told you we're going to dive in
and really Roll Up Our Sleeve and do a
lot of coating today uh we did the basic
uh demo up above for just pulling in a
carass and then we went into a carass
model uh where we pulled in data to see
whether someone was wearing a mask or
not so very useful in today's world as
far as a fully running application my
name is Richard Curry ER I'm with the
simply learn team what's in it for you
well today we're going to cover what is
a neural network what can neural
networks do how does a neural network
work types of neural networks and then
we're going to jump into a use case to
classify between the photos of dogs and
cats and we'll do that on the carass
with the tensor flow in the back but
it's a python script so that's always my
favorite part as when we dive into the
actual script so what is a neural
network so hi guys I heard you want to
know what a neural network is here we
have uh looks like he just went shopping
at a red tag sale my robot's back so as
a matter of fact you have been using
neural network on a daily basis in
today's world is just amazing how much
we use our new technology we're not even
aware of it when you ask your Mobile
Assistant to perform a search for you
you know like saying uh your Google or
Siri or whoever you use Amazon web
self-driving cars so that's the newest
thing coming out they're just now trying
to make those legal in different states
in the US and around the world even in
the UK they know have a self-driving car
is going up and down the street it's
pretty amazing these are all neural
network driven computer games use it so
a lot of computer games are driven by
neural networks in the back end as part
of the game system and how it adjusts to
the players and it's also used in
processing the map images on your phone
so every time you do a navigation
someplace and it opens it up they now
use neural networks to help you find the
quickest way to get there neural network
a neural network is a system or Hardware
that is designed to operate like a human
brain in today's development this is so
important to understand because we don't
have anything else to compare it to I'm
sure someday in the future the computer
will redefine or the neural network or
the AI artificial intelligence will
redefine what these mean but as far as
we can today's world and today's
commercial development we have to
compare it to what humans do so it's we
want to compare and how it operates to a
human brain and how it solves problems
like a human does what can a neural net
do and really we're just going to dive
in deeper to what we just covered and
look at other examples so what can a
neural network do well let's list out
the things neural networks can do for
you translate text boy we got Google
translate and Microsoft has their own
translate they have some really cool
they actually have an earpiece it's
supposed to start translating as you
talk what a cool technology what a cool
time to live identify faces can you
imagine all the uses for facial
identification in the case of our sample
or our code that we're going to look at
later we'll be identifying dogs and cats
so not quite as detailed as
understanding whose face belongs to who
I I'm waiting for the Google Glasses to
come out so I can see who's Who and the
identify faces as I'm walking around
have a little name tag over them not out
there yet but boy we are close we can
identify the faces and they have all
kinds of Technologies to bring that
information back to us recognize speech
goes along with the translate text so
now as you're talking into your
assistant it can use that to do commands
turn lights on all kinds of things you
can do with recognizing speech read had
written text they're starting to
translate all these old text documents
that they've had in storage instead of
doing it individually where somebody's
going through each text by thems in a
room picture like an old Raiders of the
Lost Arc theme where he's in the back
you know archaeologist studying the text
now it's fed into a computer they take a
picture they even use neural networks to
take a scroll that is so messed up that
they can't undo the scroll and they
x-ray it and then they use that x-ray to
translate the text off of it without
ever opening the scroll I mean just way
cool stuff they're starting to do with
all this and of course control robots
what would be a neural network without
bringing in the robots and we have our
own favorite robot in the middle who
goes to our red tag cell and go shopping
for us so you know these are just a few
of the wonderful things that neural
networks are being applied to it's such
an infant stage technology what a
wonderful time to jump in and there are
a lot of other things that goes into I
mean we could spend just forever talking
about all the different applications
from business to whatever you could even
imagine they're now applying neural
networks to help us understand so now we
talked a little bit about all the cool
things you can do with a neural network
let's dive in and say how does a neural
network work so now we've come far
enough to understand how neural network
works let's go ahead and walk through
this in a nice graphical representation
they usually describe a neural network
as having different layers and you'll
see that we've identified a Green Layer
an orange layer and a red layer the
green Green Layer is the input so you
have your data coming in it picks up the
input signals and passes them to the
next layer the next layer does all kinds
of calculations and feature extraction
it's called The Hidden layer a lot of
times there's more than one hidden layer
we're only showing one in this uh
picture but we'll show you how it looks
like in a more detail in a little bit
and then finally we have an output layer
this layer delivers the final result so
the only two things we see is the input
layer and the output layer now let's
make use of this neural Network and see
how it works wonder how traffic cameras
identify Vehicles registration plate on
the road to detect speeding vehicles and
those breaking the law they got me going
through a red light the other day well
last month that's like the horrible
thing they send you this picture of you
and all your information cuz they pulled
it up off of your license plate and your
picture that shouldn't have gone through
the red light so here we are and we have
an image of a car and you can see the
license plates on there so let's
consider the image of this vehicle and
find out what's on the number plate the
picture itself is 28x 28 pixels and the
image is fed as an input to identify the
registration plate each neuron has a
number called activation that represents
the grayscale value of the corresponding
pixel range and we range it from 0o to
one one for a white pixel and zero for a
black pixel and you can see down here we
have an example where one of the pixels
is registered as like 082 meaning it's
probably pretty dark each neuron is lit
up when its activation is close to one
so as we get closer to black on white we
can really start seeing the details in
there and you can see again the pixel
shows this one up there it's like part
of the car and so it lights up so pixels
in the form of arrays are fed to the
input layer and so we see here the pixel
of a car image fed as an input and
you're going to see that the input layer
which is green is on Dimension while our
image is twood dimension now when we
look at our setup that we're programming
in Python it has a cool feature that
automatically does the work for us if
you're working with an older neural
network pattern package you then convert
each one of those rows so it's all one
array so you'd have like Row one and
then just tack row two onto the end you
can almost feed the image directly into
some of these neural networks the key is
though is that if you're using a 28x 28
and you get a picture of this 30x30
shrink the 30x30 down to fit the 28 by
28 so you can't increase the number of
input in this case Green Dots it's very
important to remember when you're
working on neural networks and let's
name the inputs X1 X2 X3 respectively so
each one of those represents one of the
pixels coming in and the input layer
passes it to the hidden layer and you
can see here we now have two hidden
layers in this image in the orange and
each one of those pixels connects to
each one of those hidden layers and the
interconnections are assigned weights at
random so they gets these random weights
that come through if X1 lights up then
it's going to be X1 * this weight going
into the hidden layer and we sum those
weights the weights are multiplied with
the input signal and a bias is added to
all of them so as you can see here we
have X1 comes in and it actually goes to
all the different hidden layer nodes or
in this case uh whatever you want to
call them network setup the orange dots
and so you take the value of X1 you
multiply it by the weight for the next
hidden layer so X1 goes to Hidden layer
one X1 goes to Hidden layer 2 X1 goes
hidden layer one node two hidden layer
one node three and so on and the bias a
lot of times they just put the bias in
as like another Green Dot or another
orange Dot and they give the bias of
value one and then all the weights go in
from the bias into the next node so the
bias can change we always just remember
that you need to have that bias in there
there's things that can be done with it
generally most the packages out there
control that for you so you don't have
to worry about figuring out what the
bias is but if you ever dive deep into
neural networks you got to remember
there's a bias or the answer won't come
out correctly the weighted sum of the
input is fed as an input to the
activation function to decide which
nodes to Fire and for feature extraction
as a signal flows within the hidden
layers the weighted sum of inputs is
calculated and is fed to the activation
function in each layer to decide which
nodes to fire so here's our feature
extraction of the number plate and you
can see these are still hidden nodes in
the middle and this becomes important
we're going to take a little detour here
and look at the activation function so
we're going to dive just a little bit
into the math so you can start to
understand where some of the games go on
when you're playing with neural networks
in your programming so let's look at the
different activation functions before we
move ahead here's our friend red tag
shopping robot and so one is a sigmoid
function and the sigmoid function which
is 1/ 1 + e to the minus X takes the x
value and you can see where it generates
almost a zero and almost a one with a
very small area in the Middle where it
crosses over and we can use that value
to feed into another function so if it's
really uncertain it might have a 0.1
or02 or3 but for the most part it's
going to be really close to one and
really close to this case 0o 0 to one
the thresh threshold function so if you
don't want to worry about the
uncertainty in the middle you just say
oh if x is greater than or equal to zero
if not then uh X is zero so it's either
zero or one really straightforward
there's no in between in the middle and
then you have the what they call the ru
relo function and you can see here where
it puts out the value but then it says
well if it's over one it's going to be
one and if it's uh less than zero it's
zero so it kind of just dead ends it on
those two ends but allows all the values
in the middle and again this like the
sigmoid function allows that information
to go to the next level so it might be
important to know if it's a 0.1 or a
minus point one the next hidden layer
might pick that up and say oh this piece
of information is uncertain or this
value has a very low certainty to it and
then the hyperbolic tangent function and
you can see here it's a 1 - e - 2x over
1 + e - 2x and it's very much along the
same theme a little bit different in
here in that it goes between minus1 and
one so you you'll see some of these they
go 0 to one but this one goes minus one
to one and if it's less than zero it's
you know it doesn't fire and if it's
over zero it fires and it also still
puts out a value so you still have a
value you can get off of that just like
you can with the sigmoid function and
the relo function very similar in use
and I believe the originally used to be
everything was done in the sigmoid
function that was the most uh commonly
used and now they just kind of use more
the relo function the reason is one it
processes faster because you already
have the value and you don't have to add
another compute the 1 over 1 plus e
Theus X for each hidden node and the
data coming off works pretty good as far
as putting it into the next level if you
want to know just how close it is to
zero how close is it not to functioning
you know is it minus .1 minus point2
usually they're float values you get
like minus Point minus 00138 or
something so you know important
information but the relu is most
commonly used these days as far as the
setup we're using but you'll also see
the sigmoid function very commonly used
also now now that you know what an
activation function is let's get back to
the neural network so finally the model
would predict the outcome of applying a
suitable activation function to the
output layer so we go in here we look at
this we have the optical character
recognition OCR is used on the images to
convert it into a text in order to
identify what's written on the plate and
as it comes out you'll see the red node
and the red node might actually
represent just a letter A so there's
usually a lot of outputs when you're
doing text identification we're not
going to show that on here but you might
have it even in the order it might be um
what order the license plates in so you
might have AB bcde e FG you know the
alphabet plus the numbers and you might
have the 1 2 3 4 5 6 7 8 nine 10 places
so it's a very large array that comes
out it's not a small amount of u you
know we show three dots coming in eight
hidden layer nodes you know two sets of
four we just show one red coming out a
lot of times this is uh you know 28 * 28
if you did 30 * 30 that's you know 900
notes so 28 is a little bit less than
that uh just on the input and so you can
imagine the hidden layer is just as big
each hidden layer is just as big if not
bigger then the output is going to be
there's so many digits you know it's a
lot there it's a huge amount of input
and output but we're only showing you
just you know be hard to show in one
picture and so it comes up and this is
what it finally gets out on the output
is it identifies a number on the plate
and in this case we have 08- D-
03858 error in the output is back
propagated through the network and
weights are adjusted to minimize the
error rate this is calculated by a cost
function when we're training our data
this is what's used and we'll look at
that in the code when we do the data
training so we have stuff we know the
answer to and then we put the
information through and it says yes that
was correct or no because remember we
randomly set all the weights to begin
with and if it's wrong we take that
error how far off are you you know are
you off by is it if it was like minus
one you're just a little bit off if it's
like minus 300 was your output remember
when we're looking at those different
options you know hyperbolic or whatever
and we looking at the re the RL could
doesn't have an limit on top or bottom
it actually just generates a number so
if it's way off you going have to adjust
those weights a lot but if it's pretty
close you might adjust the weights just
a little bit and you keep adjusting the
weights until they fit all the different
training models you put in so you might
have 500 training models and those
weights will adjust using the back
propagation it sends the error backward
the output is compared with the original
result and multiple iterations are done
to get the maximum accuracy so not only
does it look at each one but it goes
through it and just keeps cycling
through these the data making small
changes in the network until it gets the
right answers with every iteration the
weights at every interconnection are
adjusted based on the error we're not
going to dive into that math because it
is a differential equation and it gets a
little complicated but I will talk a
little bit about some of the different
options they have when we look at the
code so we've explored a neural network
let's look at the different types of
artificial neural networks and this is
like the biggest area growing is how
these all come together let's see the
different types of neural network and
again we're comparing this to human
learning so here's a human brain I feel
sorry for that poor guy so we have a
feed for forward neural network simplest
form of a they call it an Ann a neural
network data travels only in One
Direction input to Output
this is what we just looked at so as the
data comes in all the weights are added
it goes to the hidden layer all the
weights are added it goes to the next
hidden layer all the weights are added
and it goes to the output the only time
you use a reverse propagation is to
train it so when you actually use it
it's very fast when you're training it
it takes a while because it has to
iterate through all your training data
and you start getting into Big Data
because you can train these with a huge
amount of data the more data you put in
the better train they get the
applications vision and speech
recognition actually they prettyy much
everything we talked about a lot of
almost all of them use this form of
neural network at some level radial
basis function neural network this model
classifies the data point based on its
distance from a Center Point what that
means is that you might not have
training data so you want to group
things together and you create Central
points and it looks for all the things
you know some of these things are just
like the other if you've ever watched
the Sesame treat as a kid that dates me
so it brings things together and this is
a great way if you don't have the right
training model you can start finding
things that are connected you might not
have noticed before applications power
restoration systems they try to figure
out what's connected and then based on
that they can fix the problem if you
have a huge power system and
self-organizing neural network vectors
of random dimensions are input to
discrete map comprised of neurons so
they basically find a way to draw they
call them they say Dimensions or vectors
or planes because they actually chop the
data in one dimension two Dimension
three dimension four five six they keep
adding dimensions and finding ways to
separate the data and connect different
data pieces together applications used
to recognize patterns in data like in
medical analysis the hidden layer saves
its output to be used for future
prediction recurrent neural networks so
the hidden layers remember its output
from last time and that becomes part of
its new input uh you might use that
especially in robotics or flying a drone
Dr you want to know what your last
change was and how fast it was going to
help predict what your next change you
need to make is to get to where the
Drone wants to go applications text to
speech conversation model so you know I
talked about drones but you know just
identifying on Lexus or Google assistant
or any of these they're starting to add
in I'd like to play a song on my Pandora
and I'd like it to be at volume 90% so
you now can add different things in
there and it connects them together the
input features are taken in batches like
a filter this allows a network to
remember an image in Parts convolution
neural network today's world in photo
identification and taking apart photos
and trying to you know have you ever
seen that on Google where you have five
people together this is the kind of
thing separates all those people so then
it can do a face recognition on each
person applications used in signal and
image processing in this case I use
facial images or Google picture images
as one of the options modular neural
network it has a collection of different
neural networks working together to get
the output so wow we just went through
all these different types of neural
networks and the final one is to put
multiple neural networks together and I
mentioned that a little bit when we
separated people in a larger photo in
individuals in the photo and then do the
facial recognition on each person so one
network is used to separate them and the
next network is then used to figure out
who they are and do the facial
recognition applications still
undergoing research this is a cutting
Edge you hear the term Pipeline and
there's actual in Python code and in
almost all the different neural network
setups out there they now have a
pipeline feature usually and it just
means you take the data from one neural
network and maybe another neural network
or you put it into the next neural
network and then you take three or four
other neural networks and feed them into
another one so how we connect the neural
networks is really just Cutting Edge and
it's so experimental I mean it's almost
creative in its nature there's not
really a science to it because each
specific domain has different things
it's looking at so if you're in the
banking domain it's going to be
different than the medical domain than
the automatic car domain and suddenly
figuring out how those all fit together
is just a lot of fun and really cool so
we have our types of artificial neural
network we have our feed forward neural
network we have a radial basis function
neural network we have our cohenan
self-organizing neural network recurrent
neural network convolution neural
network and modular neural network
Network where it brings them all
together and uh know the colors on the
brain do not match what your brain
actually does but they do bring it out
that most of these were developed by
understanding how humans learn and as we
understand more and more of how humans
learn we can build something in the
computer industry to mimic that to
reflect that and that's how these were
developed so exciting part use case
problem statement so this is where we
jump in this is my favorite part let's
use the system to identify between a cat
and a dog if you remember correctly I
said we're going to do some python code
and you can see over here my hair is
kind of sticking up over the computer
cup of coffee on one side and a little
bit of old school a pencil and a pen on
the other side yeah most people Now take
notes I love the stickies on the
computer that's great that's that is my
computer I have sticky notes on my
computer in different colors so not too
far from uh today's programmer so the
problem is is where we want to classify
photos of cats and dogs using a neural
network and you can see over here we
have have quite a variety of dogs in the
pictures and cats and you know just
sorting out it is a cat is pretty
amazing and why would anybody want to
even know the difference between a cat
and a dog okay you know why well I have
a cat door it'd be kind of fun that
instead of it identifying instead of
having like a little collar with a
magnet on it which is what my cat has
the door would be able to see oh that's
the cat that's our cat coming in oh
that's the dog we have a dog too that's
the dog I want to let in maybe I don't
want to let this other animal in cuz
it's a raccoon so you can see where you
could take this one step further and
actually apply this you could actually
start a little startup company idea
self-identifying door so this use case
will be implemented on python I am
actually in Python 3.6 it's always nice
to tell people the version of python
because that does affect sometimes which
modules you load and everything and
we're going to start by importing the
required packages I told you we're going
to do this in carass so we're going to
import from carass Models a sequential
from the carass layers conversion 2D or
Co NV 2D Max pooling 2D flatten and
dents and we'll talk about what each one
of these do in just a second but before
we do that let's talk a little bit about
the environment we're going to work in
and uh you know in fact let me go ahead
and open a uh the website kass's website
so we can learn a little bit more about
carass so here we are on the carass
website and it's a k
r.i that's the official website for
carass and the first thing you'll notice
is that carass runs on top of either
tensorflow cntk and I think it's
pronounced thano or theano what's
important on here is that tensorflow and
the same is true for all these but
tensorflow is probably one of the most
widely used currently packages out there
with the carass and of course you know
tomorrow this is all going to change
it's all going to disappear and they'll
have something new out there so make
sure when you're learning this code that
you understand what's going on and also
know the code I mean look when you look
at the code it's not as complicated once
you understand what's going on the code
itself is pretty straightforward and the
reason we like carass and the reason
that people are jumping on it right now
it's such a big deal is if we come down
here let me just scroll down a little
bit they talk about user friendliness
modularity easy extensibility work with
python Python's a big one because a lot
of people in data science now use Python
although you can actually access carass
other ways is if we continue down here
is layers and this is where it gets
really cool when we're working with
carass you just add layers on remember
those hidden layers we were talking
about we talked about the
ru activation you can see right here let
me just up that a little bit in size
there we go that's big I can add in an
Ru layer and then I can add in a softmax
layer the next instance we didn't talk
about softmax so you can do each layer
separate now if I'm working in some of
the other kits I use I take that and I
have one setup and then I feed the
output into the next one this one I can
just add hidden layer after hidden layer
with a different information in it which
makes it very powerful and very fast to
spin up and try different setups and see
how they work with the data you're
working on and we'll dig a little bit
deeper in here and a lot of this is very
much the same so when we get to that
part I'll point that out to you also now
just a quick side note I'm using
Anaconda with python in it and I went
ahead and created my own package and I
called it the carass python 36 cuz I'm
in Python 36 Anaconda is cool that way
you can create different environments
really easily if you're doing a lot of
different experimenting with these
different packages probably want to
create your own Environ in there and the
first thing as you can see right here
there's a lot of dependencies a lot of
these you should recognize by now if
you've done any of these videos if not
kudos for you for jumping in today pip
install numpy scipi the S kit learn
pillow and
h5py are both needed for the tensor flow
and then putting the carass on there and
then you'll see here uh and pip is just
a standard installer that you use with
python you'll see here that we did pip
install tensorflow since we're going to
do carass on top of tensor flow and then
pip install and I went ahead and used
the GitHub so git plus git and you'll
see here github.com this is one of their
releases one of the the most current
release on there that goes on top of
tensor flow and you can look up these
instructions pretty much anywhere this
is for doing it on Anaconda certainly
you'd want to install these if you're
doing it in auntu server setup you you'd
want to get I don't think you need the
h5py and auntu but you do need the rest
in there because they are dependencies
in there and it's pretty straightforward
and that's actually in some of the
instructions they have on their website
so you don't have to necessarily go
through this just remember their website
on there and then when I'm under my uh
Anaconda Navigator which I like you'll
see where I have environments and on the
bottom I created a new environment and I
called it carass python 36 just to
separate everything you can say I have
Python 30.5 and python 36 I used to have
a bunch of other ones but I kind of
cleaned house recently and of course
once I go in here I can launch my
Jupiter notebook making sure I'm using
the right environment that I just set up
this of course opens up my um in this
case I'm using Google Chrome and in here
I could go and just create a new
document in here and this is all in your
um browser when when you use the
Anaconda do you have to use anaconda and
Jupiter notebook no you can use any kind
of python editor whatever setup you're
comfortable with and whatever you're
doing in there so let's go ahead and go
in here and paste the code in and we're
importing a number of different settings
in here we have import sequential it's
under the models cuz that's the model
we're going to use as far as our neural
network and then we have layers and we
have conversion 2D Max pooling 2D
flatten dense and you can actually just
kind of guess at what these do we're
talking we're working in a 2d photograph
and if you remember correctly I talked
about how the actual input layer is a
single array it's not in two Dimensions
it's one dimension all these do is these
are tools to help flatten the image so
it takes a two-dimensional image and
then it creates its own proper setup you
don't have to worry about any of that
you don't have to do anything special
with the photograph you let the carass
do it we're going to run this and you'll
see right here they have some stuff that
is going to be depreciated and changed
cuz that's what it does everything's
being changed as we go you don't have to
worry about that too much if you have
warnings if you run it a second time the
warning will disappear and this is just
imported these packages for us to use
Jupiter's nice about this that you can
do each thing step by step and I'll go
ahead and also zoom in there a little
control plus that's one of the nice
things about being in a browser
environment so here we are back another
sip of coffee if you're familiar with my
other videos you notice I'm always
sipping coffee I always have a my case
latte next to me an espresso so the next
step is to go ahead and initialize we're
going to call it the CNN or classifier
neural network and the reason we call it
a classifier is because it's going to
classify it between two things it's
going to be cat or dog so when you're
doing classification you're picking
specific objects you're specific it's a
true or false yes no it is something or
it's not so first thing we're going to
create our classifier and it's going to
equal sequential so their sequential
setup is the classifier that's the
actual model we're using that's the
neural network so we call it a
classifier and uh the next step is to
add in our convolution and let me just
do a uh let me shrink that down inside
so you can see the whole line and let's
talk a little bit about what's going on
here I have my classifier and I add
something what am I adding well I'm
adding my first layer this first layer
we're adding in is probably the one that
takes the most work to make sure you
have it set correct and the reason I say
that is this is your actual input and
we're going to jump here to the part
that says input shape equals 64x 64 by 3
what does that mean well that means that
our pictures coming in and there's these
pictures remember we had like the
picture of the car was 128 by 128 pixels
well this one is 64 by 64 pixels and
each pixel has three values that's where
these numbers come from and it is so
important that this matches I mentioned
a little bit that if you have like a
larger picture you have to reformat it
to fit this shape if it comes in as
something larger there's no input notes
there's no input neural network there
that will handle that extra space so you
have to reshape your data to fit in here
now the first layer is the most
important because after that carass
knows what your shape is coming in here
and it knows what's coming out and so
that really sets the stage most
important thing is that input shape
matches your data coming in and you'll
get a lot of Errors if it doesn't you'll
go through there and picture number 55
doesn't match it correctly and guess
what it does it usually gives you an
error and then the activation if you
remember we talked about the different
activations on here we're using the ru
model like I said that is the most
commonly used now because one it's fast
doesn't have the added calculations in
it it just says here's a value coming
out based on the weights and the value
going in and um from there you know it's
uh it's if it's over one then it's good
or over zero it's good if it's under
zero then it's considered not active and
then we have this conversion 2D what the
heck is conversion 2D I'm not going to
go into too much detail in this because
this has a couple things it's doing in
here a little bit more in-depth than
we're ready to cover in this tutorial
but this is used to convert from the
photo because we have 64x 64x 3 and
we're just converting it to
two-dimensional kind of setup so it's
very aware that this this is a
photograph and that different pieces are
next to each other and then we're going
to add in uh a second convolutional
layer that's what the co andv stands for
2D so it's these are hidden layers so we
have our input layer and our two hidden
layers and they are two-dimensional
because we're doing with a
two-dimensional photograph and you'll
see down here that on the last one we
add a Max pooling 2D and we put a pool
size equals 22 and so what this is is
that as you get to the end of these
layers one of the things you always want
to think of of is what they call mapping
and then reducing wonderful terminology
from the Big Data we're mapping this
data through all these layers and now we
want to reduce it to only two sets in
this case it's already in two sets cuz
it's a 2d photograph but we had you know
two Dimensions by we actually have 64x
64x 3 so now we're just getting it down
to a 2X two just the two-dimension
two-dimensional instead of having the
third dimension of colors and we'll go
ahead and run these we're not really
seeing anything on our run script
because we're just setting up this is
all set up and this is where you start
playing cuz maybe you'll add a different
layer in here to do something else to
see how it works and see what your
output is that's what makes carass so
nice is I can with just a couple flips
of code put in a whole new layer that
does a whole new processing and see
whether that improves my run or makes it
worse and finally we're going to do the
final setup which is to flatten
classifier add a flatten setup and then
we're going to also add a layer a dense
layer and then we're going to add in
another dense layer and then we're going
to build it we're going to compile this
whole thing together so let's flip over
and see what that looks like and we've
even numbered them for you so we're
going to do the flattening and flatten
is exactly what it sounds like we've
been working in a two-dimensional array
of picture which actually is in three
dimensions because of the pixels the
pixels have a whole another dimension to
it of three different values and we'
kind of resize those down to 2 by2 but
now we're just going to flatten it I
don't want to have multiple Dimensions
being worked on by 10 suround by carass
I want just a single array so it's
flattened out and then step four full
connection so we add in our final two
layers and you could actually do all
kinds of things with this you could
actually leave out this um some of these
layers and play with them you do need to
flatten it that's very important then we
want to use the dents again we're taking
this and we're taking whatever came into
it so once we take all those different
the two Dimensions or three dimensions
as they are and we flatten it to one
dimension we want to take take that and
we're going to pull it into units of 128
they got that you say where did they get
128 from you could actually play with
that number and get all kinds of weird
results but in this case we took the 64
+ 64 is 128 you could probably even do
this with 64 or 32 usually you want to
keep it in the same multiple whatever
the data shape you're already using is
in and we're using the activation the ru
just like we did before and then we
finally filter all that into a single
output and it has how many units one why
cuz we want to know whether true or
false it's either a dog or a cat you
could say one is dog zero is cat or
maybe you're a cat lover and it's one is
cat and zero is dog and if you love both
dogs and cats you're going to have to
choose and then we use the sigmoid
activation if you remember from before
we had the reu and there's also the
sigmoid the sigmoid just makes it clear
it's yes or no we don't want to any kind
of in between number coming out and
we'll go ahead and run this and you'll
see it's still all and setup and then
finally we want to go ahead and compile
and let's put the compiling our um
classifier neural network and we're
going to use the optimizer atom and I
hinted at this just a little bit before
where does atom come in where does an
Optimizer come in well the optimizer is
the reverse propagation when we're
training it it goes all the way through
and says error and then how does it
readjust those weights there are a
number of them atom is the most commonly
used and it works best on large large
data most people stick with the atom
because when they're testing on smaller
data see if their model is going to go
through and get all their errors out
before they run it on larger data sets
they're going to run it on atom anyway
so they just leave it on atom most
commonly used but there are some other
ones out there you should be aware of
that that you might try them if you're
stuck in a bind or you might blore that
in the future but usually Adam is just
fine on there and then you have two more
settings you have loss and metrics we're
not going to dig too much into loss or
metrics these are things you really have
to expl explore carass cuz there are so
many choices this is how it computes the
error there's so many different ways to
on your back propagation and your
training so we're using the atom model
but you can compute the error by um
standard deviation standard deviation
squared they use binary crossentropy I'd
have to look that up to even know what
that is there's so many of these a lot
of times you just start with the ones
that look correct that are most commonly
used and then you have to go read the
carass site and actually see what these
different losses and metrics and what
different options they have so we're not
going to get too much into them other
than to reference you over to the carass
website to explore them deeper but we
are going to go ahead and run them and
now we've set up our classifier so we
have an object classifier and if you go
back up here you'll see that we've added
in step one we added in our layer for
the input we added a layer that comes in
there and uses the ru for Activation and
then it pulls the data so this is even
though these are two layers the actual
neural network layer is up here and then
it uses this to pull the data and into a
2X two so into a two-dimensional array
from a threedimensional array with the
colors then we flatten it so there's our
add our flatten and then we add another
dense what they call dense layer this
dense layer goes in there and it
downsizes it to 128 it reduces it so you
can look at this as uh we're mapping all
this data down the two-dimensional setup
and then we flatten it so we map it to a
flatten map and then we take it and
reduce it down to 128 and we use the
Reel again and then finally we reduce
that down to just a single output and we
use a sigmoid to do that to figure out
whether it's yes no true false in this
case cat or dog and then finally once we
put all these layers together we compile
them that's what we've done here and
we've compiled them as far as how it
trains to use these settings for the
training back propagation so if you
remember we talked about training our
setup and when we go into this you'll
see that we have two data sets we have
one called the training set and the
testing set and that's very standard in
any data processing is you need to have
that's pretty common in any data
processing is you need to have a certain
amount of data to train it and then you
got to know whether it works or not is
it any good and that's why you have a
separate set of data for testing it
where you already know the answer but
you don't want to use that as part of
the training set so in here we jump into
part two fitting the classifier neuron
Network to the images and then from
carass let me just zoom in there I
always love that about working with
Jupiter notebooks you can really see
we're going to come in here we do the
cross pre-processing an image and we
import image data generator so nice of
carass it's such a high-end product
right now going out and since images are
so common they already have all this
stuff to help us process the data which
is great and so we come in here we do
train data gen and we're going to create
our object for helping us train for
reshaping the data so that it's going to
work with our setup and we use an image
data generator and we're going to
rescale it and you'll see here we have 1
point which tells us it's a float value
on the rescale over 255 where does 255
come from well that's the scale in the
colors of the pictures we're using they
value from 0 to 255 so we want to divide
it by 255 and it'll generate a number
between zero and one they have Shear
range and zoom range horizontal flip
equals true and this of course has to do
with if the photos are different shapes
and sizes like I said it's a wonderful
package you really need to dig in deep
to see all the different options you
have for setting up your images for
right now though we're going to just
stick with some basic stuff here and let
me go ahead and run this code and again
it doesn't really do anything CU we're
still setting up the pre-processing
let's take a look at this next set of
code and this one is just huge we're
creating the training set so the
training set is going to go in here and
it's going to use our train data gen we
just created do flow from directory it's
going to access in this case the path
data set training set that's a folder so
it's going to pull all the images out of
that folder now I'm actually running
this in the folder that the data sets in
so if you're doing the same setup and
you load your data in there and you're
doing this make sure wherever your
Jupiter notebook is saving things to
that you create this path or you can do
the complete path if you need to you
know C colon slash Etc and the target
size the batch size and class mode is
binary so the class is we're switching
everything to a binary value batch size
what the heck is batch size well that's
how many pictures we're going to batch
through the training each time and the
target size 64x 64 little confusing but
you can see right here that this is just
a general training and you can go in
there and look at all the different
settings for your training set and of
course with different data we're doing
pictures there's all kinds of different
settings depending on what you're
working with let's go ahead and run that
and see what happens and you'll see that
it found 800 images belonging to one
classes so we have 800 images in the
training set and if we're going to do
this with uh the training set we also
have to format the pictures in the test
set now we're not actually doing any
predictions we're not actually
programming the model yet all we're
doing is preparing the data so we're
going to prepare a training set and the
test set so any changes we make to the
training set at this point also have to
be made to the test set so we've done
this thing we've done a train data
generator we've done our training set
and then we also have remember our test
set of data so I'm going to do the same
thing with that I'm going to create a
test data gen and we're going to do this
image data generator we're going to
rescale one over 255 we don't need the
other settings just the single setting
for the test data gen and we're going to
create our test set we're going to do
the same thing we did with the test set
except that we're pulling it from the
test set folder and we'll run that and
you'll see in our test set we found
2,000 images that's about right we're
using 20% of the images as test and 80%
to train it and then finally we've set
up all our data we've set up all our
layers which is where all the work is is
cleaning up that data making sure it's
going in there correctly and we're
actually going to fit it we're going to
train our data set and let's see what
that looks like and here we go let's put
the information in here and let's just
take a quick look at what we're looking
at with our fit generator we have our
classifier do fit generator that's our
back propagation so the information goes
through forward with a picture and it
says oh you're either right or you're
wrong and then the air goes backward and
reprograms all those weights so we're
training our neural network and of
course we're using the training set
remember we created the training set up
here and then we're going steps per epic
so it's 8,000 steps epic means that
that's how many times we go through all
the pictures so we're going to rerun
each of the pictures and we're going to
go through the whole data set 25 times
but we're going to look at each picture
during each epic 8,000 times so we're
really programming the heck out of this
and going back over it and then they
have validation data equals test set so
we have our um training set and then
we're going to have our test set to
validate it so we're going to do this
all in one shot and we're going to look
at that and they're going to do 200
steps for each validation and we'll see
what that looks like in just a minute
let's go ahead and run our training here
and we're going to fit our data and as
it goes it says epic one of 25 you start
realizing that this is going to take a
while on my older computer it takes
about 45 minutes I have a dual processor
you we're processing uh 10,000 photos
that's not a small amount of photographs
to process so if you're on your laptop
you which I am it's going to take a
while so let's go ahead and uh go get
our cup of coffee and a sip and come
back and see what this looks like so I'm
back you didn't know I was gone that was
actually a lengthy pause there I made a
couple changes and let's discuss those
changes real quick and why I made them
so the first thing I'm going to do is
I'm going to go up here and insert a
cell Buton and let's paste the original
code back in there and you'll see that
the original thing was steps per epic
8,000 25 epics and validation steps
2,000 and I changed these to 4,000 epics
or 4,000 steps per epic 10 epics and
just 10 validation steps and this will
cause problems if you're doing this as a
commercial release but for demo purposes
this should work and if you remember our
steps per epic that's how many photos
we're going to process in fact let me go
ahead and get my drawing panel out and
uh let's just highlight that right here
well we have 8,000 pictures we're going
through so for each epic I'm going to
change this to 4,000 we going to cut
that in half so it's going to randomly
pick 4,000 pictures each time it goes
through an epic and the Epic is how many
processes so this is 25 and I'm just
going to cut that to 10 so instead of
doing 25 runs through 8,000 photos each
which you can do the math of 25 * 8,000
I'm only going to do 10 through 4,000 so
I'm going to run this 40,000 times
through the the processes and the next
thing I not you'll you'll want to notice
is that I also changed the validation
step and this would cause some major
problems in releasing because I dropped
it all the way down to 10 what the
validation step does is it says we have
2,000 photos in our training or in our
testing set and we're going to use that
for validation well I'm only going to
use a random 10 of those to validate so
not really the best settings but let me
show you why we did that let's scroll
down here just a little bit and let's
look at the output here and see what
that what's going on there so I've got
my drawing tool back on and you'll see
here it lists a run so each time it goes
through an epic it's going to do 4,000
steps and this is where the 4,000 comes
in so that's where we have we have epic
one of 10 4,000 steps it's randomly
picking half the pictures in the file
and going through them and then we're
going to look at this number right here
that is for the whole epic and that's
2,411 seconds and if you remember
correctly you divide that by 60 you get
minutes if you divide that by 60 you get
hours or you can just divide the whole
thing by 60 * 60 which is 3600 if 3600
is an hour this is roughly 45 minutes
right here and that's 45 minutes to
process half the pictures so if I was
doing all the pictures we're talking an
hour and a half per epic times 36 or no
25 they had 25 up above 25 so that's
roughly a couple days a couple days of
processing well for this demo we don't
want to do that I don't want to come
back the next day plus my computer did a
reboot in the middle of the night so we
look at this and we say okay let's we're
just testing this out my computer that
I'm running this on is a dual core
processor uh runs 0.9 GHz per second for
a laptop you know was good about four
years ago but for running something like
this it's probably a little slow so we
cut the times down and the last one was
validation we're only validating it on a
random 10 photos and this comes into
effect CU you're going to see down here
where we have accuracy value loss value
accuracy and loss those are very
important numbers to look at so the 10
means I'm only validating across 10
pictures that is where here we have
value this is ACC is for accuracy value
loss we're not going to worry about that
too much and accuracy now accuracy is
while it's running it's putting these
two numbers together that's what
accuracy is and value accuracy is at the
end of the Epic what's our accuracy into
the Epic what is it looking at in this
tutorial we're not going to go so deep
but these numbers are really important
when you start talking about these two
numbers reflect bias that is really
important I just put that up there and
bias is a little bit beyond this
tutorial but the short of it is is if
this accuracy which is being our
validation per step is going down and
the value accuracy continues to go up
that means there's a bias that means I'm
memorizing the photos I'm looking at I'm
not actually looking for what makes a
dog a dog what makes a cat a cat I'm
just memorizing them and so the more
this discrepancy grows the bigger the
bias is and that is really the beauty of
the carass neural network it is a lot of
built-in features like this that make
that really easy to track so let's go
ahead and take a look at the next set of
code so here we are into part three
we're going to make a new prediction and
so we're going to bring in a couple
tools for that and then we have to
process the image coming in and find out
whether it's an actual dog or cat if we
can actually use this to identify it and
of course the final step of part three
is to print prediction we'll go ahead
and combine these and of course you can
see me there adding more sticky notes to
my computer screen hidden behind the
screen and you know last one was don't
forget to feed the cat and the
dog so let's go a and take a look at
that and see what that looks like in
code and put that in our Jupiter
notebook all right and let's paste that
in here and we'll start by importing
numpy as NP numpy is a very common
package I pretty much import it on any
python project I'm working on another
one I use regularly is pandas they're
just ways of organizing the data and
then NP is usually the standard in most
machine learning tools as the return for
the data array although you know you can
use the standard data array from Python
and we have coros pre-pressing import
image this that all look familiar
because we're going to take a test image
and we're going to set that equal to in
this case cat or dog one as you can see
over here and you know what let me get
my drawing tool back on so let's take a
look at this we have our test image
we're loading and here we have test
image one and this one hasn't data
hasn't seen this one at all so this is
all new oh let me shrink the screen down
let me start that over so here we have
my test image and we went ahead and the
coros processing has this nice image
setup so we're going to load the image
and we're going to alter it to a 64x 64
print so right off the bat we're going
to cross this nice that way it
automatically sets it up for us so we
don't have to redo all our images and
find a way to reset those and then we
use all to set the image to an array so
again we're all and pre-processing the
data just like we pre-processed before
with our test information and our
training data and then we use the numpy
here's our numpy that's uh from our
right up here import numpy as NP expand
the dimensions test image axis equal
zero so it puts it into a single array
and then finally all that work all that
pre-processing and all we do is we run
the result we click on here we go result
equals classifier predict test image and
then we find out well what is the test
image and let's just take a quick look
and just see what that is and you can
see when I ran it it comes up dog and if
we look at those images there it is cat
or dog image number one that looks like
a nice floppy eared lab friendly with
his tongue hanging out it's either that
or a very floppy eared cat I'm not sure
which but according to our software it
says it's a dog and uh we have a second
picture over here let's just see what
happens when we run the second picture
we can go up here and change this uh
from dog image one to two we'll run that
and it comes down here and says cat you
can see me highlighting it down there as
cat so our process works you're able to
label a dog a dog and a cat a cat just
from the pictures there we go cleared my
drawing tool and the last thing I want
you to notice when we come back up here
to when I ran it you'll see it has an
accuracy of one and the value accuracy
of one well the value accuracy is the
important one because the value accuracy
is what it actually runs on the test
data remember I'm only testing it on I'm
only validating it on random 10 photos
and those 10 photos just happen to come
up one now when they ran this on the
server it actually came up about
86% this is why cutting these numbers
down so far for a commercial release is
bad so you want to make sure you're a
little careful of that when you're
testing your stuff that you change these
numbers back when you rent it on a more
Enterprise computer other than your old
laptop that you're just practicing on or
messing with and we come down here and
again you know we had the validation of
cat and so we have successfully built a
neural network that could distinguish
between photos of a cat and a dog
imagine all the other things you could
distinguish imagine all the different
Industries you could dive into with that
just being able to understand those two
difference of pictures what about
mosquitos could you find the mosquitoes
that bite versus the mosquitoes that are
friendly it turns out the mosquitoes
that bite us are only 4% of the mosquito
population if even that maybe 2% there's
all kinds of industries that use this
and there's so many industries that are
just now realizing how powerful these
tools are just in the photos alone there
is a myriad of Industry sprouting up and
I said before I'll say it again what an
exciting time to live in with these
tools and that we get to play with so
key takeaways well we covered what is a
neural network we use all kinds of
processing the map images on your phone
we talked about things that a neural
network can do translate text identify
faces all the way to control robots you
know lots of exciting things how does a
neural network work so we discussed that
with the different layers going from the
picture to the um input layer to the
hidden layers and their weights to the
final output layer we also talked about
how it does a math in Computing the
output as yes or no categorically true
false we discussed types of artificial
neural networks a lot of vocabulary
there from the feed forward neural
network which is the most commonly used
that's one the neural network we used is
a feed forward neural network that does
backward propagation to train and
there's a lot of other ones out there
there's the radial biases the cohenon
self-organizing recurrent neural network
convolution neural network netw work
modular neural network the big one was
modular cuz it incorporates pieces of
all the other ones so that whatever
you're working on now is a huge
conglomerate of multiple networks just
all Cutting Edge all of it's new people
even working on it don't even know where
it's going again very exciting times and
finally we dug through my favorite part
you can see with my uh latte on one side
my old school pens and pencil and all my
sticky notes working away that's not
actually me by the way you probably
guessed that and we walked through and
actually did a cat and dog photo a
simple cat and dog photo and you could
see where some of the problems are in
processing large amounts of photographs
and data where that starts to become
going from a single machine on my laptop
with its you know lower amount of
resources all the way to Big Data how if
you're processing hundreds and thousands
of these photos this now needs to be set
up on an Enterprise machine or even on a
cluster of computers again significantly
past the scope of this the neat part
about it though is once you write this
code most of those code they now have
tools that you can almost take the same
ideas if not the actual code and push it
right onto a cluster computation so
really cool times for this generative
adversarial networks or Gans introduced
in 2014 by ianj good fellow and
co-authors became very popular in the
field of machine learning Gan is an
unsupervised learning task in machine
learning it consists of two models that
automatically discover and learn the
patterns in input data the two models
called generator and discriminator
compete with each other to analyze
capture and copy the variations within a
data set Gans can be used to generate
new examples that possibly could have
been drawn from the original data
set in the image below you can see that
there is a database that has real 100
rupee notes the generator which is
basically a neural network generates
fake 100 rupees notes the discriminator
network will identify if the notes are
real or fake let us now understand in
brief about what is a generator a
generator in Gans is a neural network
that creates fake data to be trained on
the discriminator it learns to generate
plausible data the generated instances
become negative training examples for
the discriminator it takes a fixed
length random Vector carrying noise as
input and generates a sample now the
main aim of the generator is to make the
discriminator classify its output as
real the portion of the Gan that trains
a generator
includes a noisy input vector
the generator Network which transforms
the random input into a data instance a
discriminator network which classifies
the generator data and a generator loss
which penalizes the generator for
failing to do the discriminator the back
propagation method is used to adjust
each weight in the right direction by
calculating the weight's impact on the
output the back propagation method is
used to obtain gradients and these
gradients can help change the generator
weights now let us understand in brief
what a discriminator is a discriminator
is a neural network model that
identifies real data from the fake data
generated by the generator the
discriminator training data comes from
two
sources the real data instances such as
real pictures of birds humans currency
notes Etc are used by the discriminator
as positive samples during the training
the fake data instances created by the
generator are used as negative examples
during the training process while
training the discriminator it connects
with two loss functions during
discriminator training the discriminator
ignores the generator loss and just uses
the discriminator loss in the process of
training the discriminator the
discriminator classifies both real data
and fake data from the generator the
discriminator law penalizes the
discriminator from misclassifying a real
data instance as fake or a fake data
instance as real now moving ahead let's
understand how Gans work now Gans
consists of two networks a generator
which is repres represented as G ofx and
a discriminator which is represented as
D ofx they both play an adversarial game
where the generator tries to fool the
discriminator by generating data similar
to those in the training set the
discriminator tries not to be fooled by
identifying fake data from the real data
they both work simultaneously to learn
and train complex data like audio video
or image files now you are aware that
Gans consists of two networks a
generator G ofx and discriminator D ofx
now the generator Network takes a sample
and generates a fake sample of data the
generator is trained to increase the
probability of the discriminator network
to make mistakes on the other hand the
discriminator network decides whether
the data is generated or taken from the
real sample using a binary
classification problem with the help of
a sigmoid function that gives the output
in the range 0o and 1 here is an example
of a generative adversarial Network
trying to identify if the 100 rupee
notes are real or fake so the first a
noise vector or the input Vector is fed
to the generator Network the generator
creates fake 100 rupe nodes the real
images of 100 rupee notes stored in a
database are passed to the discriminator
along with the fake notes the
discriminator then identifies the notes
and classifies them as real or fake we
train the model calculate the loss
function at the end of the discriminator
network and back propagate the loss into
both discriminator and Generator now the
mathematical equation of training again
can be represented as you can see here
now this is the
equation and these are the parameters
here G represents generator D represents
the discriminator now P data of X is the
probability distribution of real data P
of Z is the distribution of Generator X
is the sample of probability data of X
Zed is the sample size from P of z d of
X is the discriminator network and G of
Z is the generator Network now the
discriminator focuses to maximize the
objective function such that D of X is
close to 1 and Z of Z is close to zero
it simply means that the discriminator
should identify all the images from the
training set as real that is one and all
the generated images as fake that is
zero the generator wants to minimize the
objective function such that D of Z of Z
is 1 this means that the generator tries
to generate images that are classified
as real that is one by the discriminator
network next let's see the steps for
training a neural network so we have to
first Define the problem and collect the
data then we'll choose the architecture
of Gan now depending on your problem
choose how your Gan should look like
then we need to train the discriminator
in real data that will help us predict
them as real for n number of times next
you need to generate fake inputs for the
generator after that you need to train
the discriminator on fake data to
predict the generator data is fake
finally train the generator on the
output of
discriminator with the discriminator
predictions available train the
generator to fool the discriminator let
us now look at the different types of
Gans so first we have vanilla Gans now
vanilla Gans have minmax optimization
formula that we saw earlier where the
discriminator is a binary classifier and
is using sigmoid cross entropy loss
during
optimization in vanilla Gans the
generator and the discriminator are
simple multi-layer percept droms the
algorithm tries to optimize the
mathematical equation using stochastic
gradient decent up next we have deep
convolutional Gans or DC Gans now DC
Gans support convolutional neural
networks instead of vanilla neural
networks at both discriminator and
Generator they are more stable and
generate higher quality images the
generator is a set of convolutional
layers with fractional strided
convolutions or transpose convolution so
it unsampled the input image at every
convolutional layer the discriminator is
a set of convolutional layers with
strided convolutions so it down samples
the input image at every convolutional
layer moving ahead the third type you
have is conditional Gans or C Gans
vanilla Gans can be extended into
conditional models by using an extra
label information to generate better
results in C Gan an additional parameter
called Y is added to the generator for
generating the corresponding data labels
are fed as input put to the
discriminator to help distinguish the
real data from fake data generated
finally we have super resolution Gans
now Sr Gans use deep neural networks
along with adversarial neural network to
produce higher resolution images super
resolution Gans generate a
photorealistic high resolution image
when given a low resolution image let's
look at some of the important
applications of
Gans so with the help of DC Gans you can
train images of cartoon characters for
generating faces of Anime characters and
Pokmon characters as well next Gans can
be used on the images of humans to
generate realistic faces the faces that
you see on your screens have been
generated using Gans and do not exist in
reality third application we have is
Gans can be used to build realistic
images from textual descriptions of
objects like birds humans and other
animals we input a sentence and generate
multiple images fitting the description
here is an example of a text to image
translation using Gans for a bird with a
black head yellow body and a short beak
the final application we have is
creating 3D objects so Gans can generate
3D models using 2D pictures of objects
from multiple perspectives Gans are very
popular in the gaming industry Gans can
help automate the task of creating 3D
characters and backgrounds to give them
a realistic field do you know how deep
learning recognizes the objects in an
image and really this particular neural
network is how image recognition works
it's very Central one of the biggest
building blocks for image recognition it
does it using convolution neural network
and we over here we have the basic
picture of a u hummingbird pixels of an
image fed as input you have your input
layer coming in so it takes that graphic
and puts it into the input layer you
have all your hidden layers and then you
have your output layer and your output
layer one of those is going to light up
and say oh it's a bird we're going to go
into depth we're going to actually go
back and forth on this a number of times
times today so if you're not catching
all the image um don't worry we're going
to get into the details so we have our
input layer accepts the pixels of the
image as input in the form of arrays and
you can see up here where they've
actually um labeled each block of the
bird in different arrays so we'll dive
into deep as to how that looks like and
how those matrixes are set up your
hidden layer carry out feature
extraction by performing certain
calculations and manipulation so this is
the part that kind of reorganizes that
picture multiple ways until we get some
data that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around and alter it and use that
operation to dedi a new pattern there
are multiple hidden layers like
convolution layer real U is how that is
pronounced and that's the rectified
linear unit that has to do with the
activation function that that's used
pooling layer also uses multiple filters
to detect edges corners eyes feathers
beak Etc and just like the term says
pooling is pulling information together
and we'll look into that a lot closer
here so if you're if it's a little
confusing now we'll dig in deep and try
to get you uh squared away with that and
then finally there is a fully connected
layer that identifies the object in the
image so we have these different layers
coming through in the hidden layers and
they come into the final area and that's
where we have say one node or one neural
network entity that lights up that says
it's a bird what's in it for you we're
going to cover an introduction to the
CNN what is convolution neural network
how CNN recognizes images we're going to
dig deeper into that and really look at
the individual layers in the
convolutional neural network and finally
we do a use case implementation using
the CNN we'll begin our introduction to
the CNN by introducing the pioneer of
convolutional neural network Yan leun he
was the director of Facebook AI research
group built the first convolutional
neural network called lenette in
1988 so these have been around for a
while and have had a chance to mature
over the years it was used for character
recognition tasks like reading zip code
digits imagine processing mail and
automating that process CNN is a feed
forward neural network that is generally
used to analyze visual images by
producing data with a grid-like topology
a CNN is also known as a convet and very
key to this is we are looking at images
that was what this was designed for and
you'll see the different layers as we
dig in Mirror some of the other some of
them are actually now used since we're
using uh tensorflow and carass in our
code later on you'll see that some of
those layers appear in a lot of your
other neural network Frameworks uh but
in this case this is very Central to
processing images and doing so in a
variety that captures multiple images
and really drills down into their
different features in this example here
you see flowers are two varieties Orchid
and a rose I think the Orchid is much
more dainty and beautiful and the rose
smells quite beautiful I have a couple
rose bushes in my yard uh they go into
the input layer that data is then sent
to all the different nodes in the next
layer one of the Hidden layers based on
its different weights and its setup it
then comes out and gives those a new
value those values then are multiplied
by their weights and go to the next
hidden layer and so on and then you have
the output layer and one of those notes
comes out and says it's an orchid and
the other one comes out and says it's a
rose depending on how was well it was
trained what separates the CNN or the
convolutional neural network from other
neural networks is a convolutional
operation forms the basis of any
convolutional neural network in a CNN
every image is represented in the form
of arrays of pixel values so here we
have a real image of the digit 8 uh that
then gets put onto its pixel values
represented in the form of an array in
this case you have a two-dimensional
array and then you can see in the Final
End form we transform the digit 8 into
its representational form of pixels of
zeros and on where the ones represent in
this case the black part of the eight
and the zeros represent the white
background to understand the convolution
neural network or how that convolutional
operation Works we're going to take a
side step and look at Matrix in this
case we're going to simplify it we're
going to take two matrices A and B of
one dimension now kind of separate this
from your thinking as we learned that
you want to focus just on the Matrix
aspect of this and then we'll bring that
back together and see what that looks
like when we put the pieces for the
convolutional operation here we've set
up two arrays we have uh in this case
there a single Dimension Matrix and we
have a = 5
37597 and we have b equal 1 2 3 so in
the convolution as it comes in there
it's going to look at these two and
we're going to start by doing
multiplying them a * B and so we
multiply the arrays element wise and we
get 5 66 where five is the 5 * 1 6 is 3
* 2 and then the other six is 2 * 3 and
since the two arrays aren't the same
size they're not the same setup we're
going to just truncate the first one and
we're going to look at the second array
multiplied just by the first three
elements of the first array now that's
going to be a little confusing remember
a computer gets to repeat these
processes hundreds of times so we're not
going to just forget those other numbers
later on we'll see we'll bring those
back in and then we have the sum of the
product in this case 5 + 6 + 6 = 17 so
in our a * B our very first digit in
that Matrix of a * B is 17 and if you
remember I said we're not going to
forget the other digits so we now have
325 we move one set over and we take 325
and we multiply that times B and you'll
see that 3 * 1 is 3 2 * 2 is 4 and so on
and so on WE sum it up so now we have
the second digit of our a * B product in
The Matrix and and we continue on with
that same thing so on and so on so then
we would go from uh 375 to 759 to 597
this short Matrix that we have for a
we've now covered all the different
entities in a that match three different
levels of B now in a little bit we're
going to cover where we use this math at
this multiplying of matrixes and how
that works uh but it's important to
understand that we're going through the
Matrix and multiplying the different
parts to it to match the smaller Matrix
with the larger Matrix I know a lot of
people get lost at is you know what's
going on here with these matrixes uh H
scary math not really that scary when
you break it down we're looking at a
section of a and we're comparing it to B
so when you break that down your mind
like that you realize okay so I'm I'm
just taking these two matrixes and
comparing them and I'm bringing the
value down into one Matrix a * B we're
reducing that information in a way that
will help the computer see different
aspects let's go ahead and flip over
again back to our images here we are
back to our images talking about going
to the most basic two-dimensional image
you can get to consider the following
two images the image for the symbol
backslash when you press the backs slash
the above image is processed and you can
see there for the image for the forward
slash is the opposite so we click the
forward slash button that flips uh very
basic we have four pixels going in can't
get any more basic than that here we
have a little bit more complicated
picture we take a real image of a smiley
face
um then we represent that in the form of
black and white pixels so if this was an
image in the computer it's black and
white and like we saw before we convert
this into the zeros and one so where the
other one would have just been a matrix
of just four dots now we have a
significantly larger image coming in so
don't worry we're going to bring this
all together here in just a little bit
layers in convolutional neural network
when we're looking at this we have our
convolution layer and that really is the
central aspect of processing images in
the convolutional neural network that's
why we have it and then that's going to
be feeding in and you have your relu
layer which is you know as we talked
about the rectified linear unit we'll
talk about that a little bit later the
reu isn't how it Act is how that layer
is activated is the math behind it what
makes the neurons fire you'll see that
in a lot of other neural networks when
you're using it just by itself is for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the relu layer works great you
have your pooling layer that's where
you're pulling the data together pooling
is a neural network term it's very
commonly used I like to use a term
reduce so if you're coming from the map
and reduce side you'll see that we're
mapping all this data through all these
networks and then we're going to reduce
it we're going to pull it together and
then finally we have the fully connected
layer that's where our output's going to
come out so we have started looking at
matrixes we've started to look at the
convolutional layer and where it fits in
and everything we've taken a look at
images so we're going to focus more on
the convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following 5x5 image whose pixel values
are only zero and one now obviously when
we're dealing with color there's all
kinds of things that come in on color
processing but we want to keep it simple
and just keep it black and white and so
we have our image pixels uh so we're
sliding the filter Matrix over the image
and Computing the dot product to detect
the patterns and right here you're going
to ask where does this filter come from
this is a bit confusing because the
filter is going to be derived uh later
on we build the filters when we program
or train our model so you don't need to
worry what the filter actually is what
you do need to understand how a
convolution layer works is what is the
filter doing filter and you'll have many
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the filter works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two-dimensional Matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here 1 * 1 1 * 0 0 1 *
1 we multiply those all together then
sum them and we end up with a convolved
feature of four we're going to take that
and sliding the filter Matrix over the
image and Computing the dot product to
detect patterns so we're just going to
slide this over we're going to predict
the first one and slide it over one
notch predict the second one and so on
and so on all the way through until we
have a new Matrix and this Matrix which
is the same sizes a filter has reduced
the image and whatever filter whatever
that's filtering out is going to be
looking at just those features reduce
down to a smaller uh Matrix so once the
feature maps are extracted the next step
is to move them to the reu layer so the
realu layer The Next Step first is going
to perform an element wise operation so
each of those Maps coming in if there's
negative pixels so it sets all the
negative pixels to zero um and you can
see this nice graph where it just zeros
out the negatives and then you have a
value that goes from zero up to whatever
value is um coming out of the Matrix
this introduces nonlinearity to the
network uh so up until now we have a we
say linearity we're talking about the
fact that the feature has a value so
it's a linear feature this feature um
came up and has let's say the feature is
the edge of the beak you know it's like
or the backslash that we saw um you'll
look at that and say okay this feature
has a value from -10 to 10 in this case
um if it was one and' say yeah this
might be a beak it might not might be an
edge right there a minus 5 means no
we're not even going to look at it a
zero and so we end up with an output and
the output takes all these feature all
these filtered features remember we're
not just running one filter on this
we're running a number of filters on
this image and so we end up with an
rectified feature map that is looking at
just the features coming through and how
they weigh in from our filters so here
we have an input of a looks like a
twocan
bird very exotic looking real image is
scanned in multiple convolution and the
relu layers for locating features and
you can see up here is turned it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
Skip by two or three or four pixels uh
to speed up the process that's one of
the ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
theed image uh you have multiple filters
that been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new Matrix one more aspect to
note about the reu layer is we're not
just having one reu coming in uh so not
only do we have multiple features going
through but we're generating multiple
reu layers for locating the features
that's very important to note you know
so we have a quite a bundle we have
multiple filters multiple
uh which brings us to the next step
forward propagation now we're going to
look at the pooling layer the rectified
feature map now goes through a pooling
layer pooling is a down sampling
operation that reduces the
dimensionality of the feature map that's
all we're trying to do we're trying to
take a huge amount of information and
reduce it down to a single answer this
is a specific kind of bird this is an
iris this is a rose so you have a
rectified feature map and you see here
we have a rectified feature map coming
in um we set the max pooling with a 2X
two filters and a stride of Two And if
you remember correctly I talked about
not going one pixel at a time uh well
that's where the stride comes in we end
up with a 2X two pulled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a we skip a few there we go by two
we skip every other pixel and we just do
every other one um and this reduces our
rectified feature map which as you can
see over here 16x 16 to a 4x4 so we're
continually trying to filter and reduce
our data so that we can get to something
we can manage and over here you see that
we have the Max uh 3 4 1 and two and in
the max pooling we're looking for the
max value little bit different than what
we were looking at before so coming from
the rectified feature we're now finding
the max value and then we're pulling
those features together so instead of
think of this as image of the map think
of this as how valuable is a feature in
that area how much of a feature value do
we have and we just want to find the
best or the maximum feature for that
area they might have that one piece of
the filter of the beak said oh I see a
one in this beak in this image and then
it skips over and says I see a three in
this image and says oh this one is rated
as a four we don't want to sum it
together because then you know you might
have like five ones and I'll say ah five
but you might have uh four zeros and one
10 and that 10 says well this is
definitely a beak where the ones will
say probably not a beak little strange
analogy since we're looking at a bird
but you can see how that pulled feature
map comes down and we're just looking
for the max value in each one of those
matrixes pooling layer uses different
filters to identify different parts of
the image like edges corners body
feathers eyes beak Etc um I know I focus
mainly on the beak but obviously uh each
feature could be a different part of the
bird coming in so let's take a look at
what that looks like structure of a
convolution neural network so far this
is where we're at right right now we
have our input image coming in and then
we use our filters and there's multiple
filters on there that are being
developed to kind of twist and change
that data and so we multiply the
matrixes we take that little filter
maybe it's a 2 by two we multiply it by
each piece of the image and if we step
two then it's every other piece of the
image that generates multiple
convolution layers so we have a number
of convolution layers we have um set up
in there just looking at that data we
then take those convolution layers we
run them through the reu setup and then
once we've done through the reu setup
and we have multiple reu going on
multiple layers that are reu then we're
going to take those multiple layers and
we're going to be pooling them so now we
have the pooling layers or multiple
poolings going on up until this point
we're dealing with u sometimes it's
multiple Dimensions you can have three
dimensions some strange data setups that
aren't doing images but looking at other
things they can have four five six seven
dimensions uh so right now we're looking
at 2D image Dimensions coming in into
the pooling layer so the next step is we
want to reduce those Dimensions or
flatten them so flattening flattening is
a process of converting all of the
resultant two-dimensional arrays from
pulled feature map into a single long
continuous linear Vector so over here
you see where we have a pulled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 6847
or a single linear vector and we find
out that not only do we do each of the
pulled feature Maps we do all of them
into one long linear Vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we flatten them out and combined
them into a single linear Vector going
in so after we've done the flattening we
have H just a quick recap because we've
covered so much so it's important to go
back and take a look at each of the
steps we've gone through the structure
of the network so far is we have our
convolution where we twist it and we
filter it and multiply the matrixes we
end up with our convolutional layer
which uses the reu to figure out the
values going out into the pooling and
you have numerous convolution layers
that then create numerous pooling layers
pulling that data together which is the
max value which one we want to send
forward we want to send the best value
and then we're going to take all of that
from each of the pooling layers and
we're going to flatten it and we're
going to combine them into a single
input going into the final layer once
you get to that step might be looking at
that going boy that looks like the
normal into it to most neural network
and you're correct it is so once we have
the flatten Matrix from the pooling
layer that becomes our input so the
pooling layer is fed as an input to the
fully connected layer to classify the
image and so you can see as our flatten
Matrix comes in in this case we have the
pixels from the flatten Matrix fed as an
input back to our toucan or whatever
that kind of bird that is um I need one
of these to identify what kind of bird
that is it comes into our Ford
propagation network uh and that will
then have the different weights coming
down across and then finally it selects
that that's a bird and that it's not a
dog or a cat in this case even though
it's not labeled the final layer there
in red is our output layer our final
output layer that says bird cat or dog
so quick recap of everything we've
covered so far we have our input image
which is twisted and multiply the
filters are multiplied times the uh
matri the two matrixes multiplied all
the filters to create our convolution
layer our convolution layers there's
multiple layers in there because it's
all building multiple layers off the
different filters then goes through the
reu as say activation and that creates
our pooling and so once we get into the
pooling layer we then on the pooling
look for who's the best what's the max
value coming in from our convolution and
then we take that layer and we flatten
it and then it goes into a fully
connected layer our fully connected
neural network and then to the output
and here we can see the entire process
how the CNN recognizes a bird this is
kind of nice cuz it's showing the little
pixels and where they're going you can
see the filter is generating this
convolution network and that filter
shows up in the bottom part of the
convolution network and then based on
that it uses the relo for the pooling
the pooling then find out which one's
the best and so on all the way to the
fully connected layer at the end or the
classification and the output layer so
that'd be a classification neural
network at the end so we covered a lot
of theory up till now and you can
imagine each one of these steps has to
be broken down in code so putting that
together can be a little complicated not
that each step of the process is overly
complicated but because we have so many
steps uh we have 1 2 3 four five
different steps going on here with
substeps in there we're going to break
that down and walk through that in code
so in our use case implementation using
the CNN we'll be using the cfr1 data set
from Canadian Institute for advanced
research for classifying images is
across 10 categories Unfortunately they
don't let me know whether it's going to
be a twocan or some other kind of bird
but we do get to find out whether it can
categorize between a ship a frog deer
bird airplane automobile cat dog horse
truck so that's a lot of fun and if
you're looking anything in the news at
all of our automated cars and everything
else you can see where this kind of
processing is so important in today's
world and very Cutting Edge as far as
what's coming out in the commercial
deployment I mean this is really cool
stuff we're starting to see this just
about everywhere in Industry uh so great
time to be playing with this and
figuring it all out let's go ahead and
dive into the code and see what that
looks like when we're actually writing
our script before we go on let's do uh
one more quick look at what we have here
let's just take a look at data batch one
keys and remember in Jupiter notebook I
can get by with not doing the print
statement if I put a variable down there
it'll just display the variable and you
can see under data batch one for the
keys since this is a dictionary we have
the batch one label data and file names
uh so you can actually see how it's
broken up in our data set so for the
next step or step four as we're calling
it uh we want to display the image using
matte plot Library there's many ways to
display the images you could even uh
well those other ways to drill into it
but map plot library is really good for
this and we'll also look at our first
reshape uh setup or shaping the data so
you can have a little glimpse into what
that means uh so we're going to start by
importing our map plot and of course
since I am doing Jupiter notebook I need
to do the map plot inline command so it
shows up on my page so here we go we're
going to import matplot library. pyplot
is PLT and if you remember matplot
Library the PIP plot is like a canvas
that we paint stuff onto and there's my
percentage sign map plot library in line
so it's going to show up in my notebook
and then of course we're going to import
numpy as NP for our numbers python array
setup and let's go ahead and set um x
equals to data batch one so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data uh we need to
go a little bit of reshaping so in here
we have to go ahead and reshape the data
we have 10,000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to I had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32x32
picture and let me do this let me go
ahead and do a drawing pad on here uh so
we have 32 bits by 32 bits and it's in
color so there's three bits of color now
I don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're going to take uh the shape we're
going to take the data which is just a
long stream of information and we're
going to break it up into 10 10,000
pieces and those 10,000 pieces then are
broken into three pieces each and those
three pieces then are 32 by 32 you could
look at this like an oldfashioned
projector where they have the red screen
or the red projector the blue projector
and the green projector and they add
them all together and each one of those
is a 32x 32 bit so that's probably how
this was originally formatted with in
that kind of Ideal things have changed
so we're going to transpose it and we're
going to take the three which was here
and we're going to put it at the end so
the first part is is reshaping the data
from a single line of bit data or
whatever format it is into 10,000 x 3X
32x 32 and then we're going to transpose
the color factor to the last place so
it's the image then the 32x 32 in the
middle that's this part right here and
then finally we're going to take this uh
which is three bits of data and put it
at the end so it's more like we do we
process images now and then as type this
is really important that we're going to
use an integer 8 you can come in here
and you'll see a lot of these they'll
try to do this with a float or a float
64 what you got to remember though is a
float uses a lot of memory so once you
switch this into uh something that's not
integer 8 which is goes up to 128 you
are just going to the the amount of ram
I just put that in here is going to go
way up the amount of ram that it loads
uh so you want to go ahead and use this
you can try the other ones and see what
happens if you have a lot of RAM on your
computer but for this exercise this will
work just fine and let's go ahead and
take that and run this so now our X
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we are
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops I
said 128 it's 255 uh you'll see it
doesn't go over 255 because it's an
basically an asky character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus a float value which would
bring this up um exponentially in size
and since we're using the map plot
Library we can do um oops that's not
what I wanted since we're using the map
plot Library we can take our canvas and
just do a PLT do IM for image show and
let's just take a look at what x0 looks
like and it comes in I'm not sure what
that is but you can see it's a very low
grade image uh broken down to the
minimal pixels on there and if we did
the same thing oh let's do uh let's see
what one looks like hopefully it's a
little easier to see run on there not
enter let's hit the run on that uh and
we can see this is probably a semi
that's a good guess on there and I can
just go back up here instead of typing
the same line in over and over and we'll
look at three uh that looks like a dump
truck unloading uh and so on you can do
any of the 10,000 images we can just
jump to 55 uh looks like some kind of
animal looking at us there probably a
dog and just for fun let's do just one
more uh run on there and we can see a
nice car for image number four uh so you
can see we past through all the
different images and it's very easy to
look at them and they've been reshaped
to fit our view and what the uh map plot
Library uses for its format so the next
step is we're going to start creating
some helper functions we'll start by a
one hot encoder to help us we're
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
encoder to do that and then we'll also
create a uh class uh cfar helper so it's
going to have a knit and a setup for the
images and then finally we'll go ahead
and run that code so you can see what
that looks like and then we get into the
fun part where we're actually going to
start creating our model our actual
neural network model so let's start by
creating our one hot encoder we're going
to create our own here uh and it's going
to return an out we'll have our Vector
coming in and our values equal 10 what
this means is that we have the 10 values
the 10 possible labels and remember we
don't look at the labels as a number
because a car isn't one more than a
horse and that'd be just kind of bizarre
to have horse equals zero car equal 1
plane equals 2 cat equals 3 so a cat
plus a car equals what uh so instead we
create a numpy array of zeros and
there's going to be 10 values so we have
10 different values in there so have uh
zero or one one means it's a cat zero
means it's not a cat um in the next line
it might be that uh one means it's a car
zero means it's not a car so instead of
having one output with a value of 0 to
10 you have 10 outputs with the values
of 0 to one that's what the one hot
encoder is doing here and we're going to
utilize this en code in just a minute so
let's go ahead and take a look at the
next helpers we have a few of these
helper functions we're going to build
and when you're working with a very
complic ated python project dividing it
up into separate definitions and classes
is very important otherwise it just
becomes really ungainly to work with so
let's go ahead and put in our next
helper uh which is a class and this is a
lot in this class so we we'll break it
down here let's just start uh oops we
put a space right in there there we go
now this a little bit more readable add
a second space so we're going to create
our class the cipher Helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the uh a nit part uh self. I equals 0
that'll come in in a little bit we'll
come back to that in the lower part we
want to initialize our training batches
so when we went through this there was
like a meta batch we don't need the meta
batch but we do need the data batch one
2 3 4 five and we do not want the
testing batch in here this is just the
self all train batches so we're going to
come make an array of of all those
different images and then of course we
left the test batch out so we have our
self. test batch uh we're going to
initialize the training images and the
training labels and also the test images
and the test labels so these are just
this is just to initialize these
variables in here then we create another
definition down here and this is going
to set up the images let's just take a
look and see what's going on in there
now we could have all just put this as
part of the uh init part uh since this
is all just helper stuff but breaking it
up again makes it easier to read it also
makes it easier when we start exec in
the different pieces to see what's going
on so that way we have a nice print
statement to say hey we're now running
this and this is what's going on in here
we're going to set up these self trining
images at this point and that's going to
go to a numpy array vstack and in there
we're going to load up uh in this case
the data for D and S all trained batches
again that points right up to here so
we're going to go through each one of
these uh files or each one of these data
sets because they're not a file anymore
we've brought them in data batch one
points to the actual data and so our
self-training images is going to stack
them all into our into a numpy array and
then it's always nice to get the
training length and that's just a total
number of uh self-training images in
there and then we're going to take the
selft trining images and let me switch
marker colors because I am getting a
little too much on the markers up here
oops there we go bring down our marker
change so we can see it a little better
and at this point this should look
familiar where did we see this well we
wanted to uh uh look at this above and
we wanted to look at the images in the
matplot library we had to reshape it so
we're doing the same thing here we're
taking our self-training images and uh
based on the training length total
number of images because we stacked them
all together so now it's just one large
file of images we're going to take and
look at it as our our three video
cameras that are each displaying uh 32x
32 we're going to switch that around so
that now we have um each of our images
that's stays the same place and then we
have our 32 by 32 and then by our three
our last our three different values for
the color and of course we want to go
ahead and uh they run this where we say
divide by 255 that was from earlier it
just brings all the data into 0 to one
that's what this is doing so we're
turning this into a 0: one array which
is uh all the pictures 32x 32 by 3 and
then we're going to take the
self-training labels and we're going to
pump those through our one hot encoder
we just made and we're going to stack
them together and uh again we're
converting this into an array that goes
from uh instead of having horse equals 1
dog equals 2 and then horse plus dog
would equal three which would be cat no
it's going to be uh you know an array of
10 where each one is zero to one then we
want to go ahead and set up our test
images and labels and uh when we're
doing this you're going to see it's the
same thing we just did with the rest let
me just change colors right here this is
no different than what we were doing up
here with our training Set uh we're
going to stack the different uh images
uh we're going to get the length of them
so we know how many images are in there
uh you certainly could add them by hand
but it's nice to let the computer do it
especially if it ever changes on the
other end and you're using other data
and again we reshape them and transpose
them and we also do the one hot encoder
same thing we just did on our training
images so now our test images are in the
same format so now we have have a
definition which sets up all our images
in there and then the next step is to go
ahead and batch them or next batch and
let's do another breakout here for
batches because this is really important
to understand TS to throw me for a
little Loop when I'm working with tensor
flow or carass or a lot of these we have
our data coming in if you remember we
had like 10,000 photos let me just put
10,000 down here we don't want to run
all 10,000 at once so we want to break
this up into batch sizes and you also
remember that we had the number of
photos in this case uh length of test or
whatever number is in there uh we also
have 32 by 32 by 3 so when we're looking
at the batch size we want to change this
from 10,000 to um a batch of in this
case I think we're going to do batches
of a 100 so we want to look at just 100
the first 100 of the photos and if you
remember we set selfi equal to zero uh
so what we're looking at here is we're
going to create X we're going to get the
next batch from the very initialized
we've already initialized it for zero so
we're going to look at X from zero to
batch size which we set to 100 so just
the first 100 images and then we're
going to reshape that into uh and this
is important to let the data know that
we're looking at 100x 32x 32x 3 now
we've already formatted it to the 32x 32
by3 this just sets everything up
correctly so that X has the data in
there in the correct order in the
correct shape and then the Y just like
the X uh is our labels so our training
labels again they go from zero to batch
size in this case they do selfi plus
batch size because the selfi is going to
keep changing and then finally we
increment the selfi because we have zero
so we so the next time we call it we're
going to get the next batch size and so
basically we have X and Y X being the
photograph data coming in and Y being
the label and that of course is labeled
through one hot encoder so if you
remember correctly if it was say a horse
is equal to zero it would be um one for
the zero position since this is the
horse and then everything else would be
zero in here let me just put lines
through there there we go there's our
array hard to see that array so let's go
ahead and take that and uh we're going
to finish loading it since this is our
class and now we're armed with all this
um uh our setup over here let's go ahead
and load that up and so we're going to
create a variable CH with the CFR helper
in it and then we're going to do ch.
setup images uh now we could have just
put all the setup images under the init
but by breaking this up into two parts
it makes it much more readable and um
also if you're doing other work there's
reasons to do that as far as the setup
let's go ahead and run that and you can
see where it says uh setting up training
images and labels setting up test images
and that's one of the reasons we broke
it up is so that if you're testing this
out you can actually have print
statements in there telling you what's
going on which is really nice uh they
did a good job with this setup I like
the way that it was broken up in the
back and then one quick note you want to
remember that batch to set up the next
batch say we have to run uh batch equals
CH next batch of 100 because we're going
to use the 100 size uh but we'll come
back to that we're going to use that
just remember that that's part of our
code we're going to be using in a minute
from the definition we just made so now
we're ready to create our model first
thing we want to do is we want to import
our tensor flow as TF I'll just go ahead
and run that so it's loaded up and you
can see we got a a warning here here uh
that's because they're making some
changes it's always growing and they're
going to be depreciating one of the uh
values from float 64 to float type or
it's treated as an NP float 64 uh
nothing to really worry about CU this
doesn't even affect what we're working
on because we've set all of our stuff to
a 255 value or 0 to one and do keep in
mind that 0 to one value that we
converted to 255 is still a float value
uh but it'll easily work with either the
uh numpy float 64 or the numpy dtype
float it doesn't matter which one it
goes through so the depreciation would
not affect our code as we have it and in
our tensor flow uh we'll go ahead let me
just increase the size in there just a
moment so you can get better view of the
um what we're typing in uh we're going
to set a couple placeholders here and so
we have we're going to set x equals TF
placeholder TF float 32 we just talked
about the float 64 versus the numpy
float we're actually just going to keep
this at float 32 more than uh
significant number decimals for what
we're working with and since it's a
placeholder we're going to set the shape
equal to and we've set it equal to none
cuz at this point we're just hold
holding the place on there we'll be
setting up as we run the batches that's
what the first value is and then 32x 32x
3 that's what we reshaped our data to
fit in and then we have our y true
equals placeholder TF float 32 and the
shape equals none comma 10 10 is the 10
different labels we have so it's an
array of 10 and then let's create one
more placeholder we'll call this a hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we call Dropout
if you remember from our Theory before
we drop out so many nodes is looking at
or the different values going through
which helps decrease bias so we need to
go ahead and put a a placeholder for
that also and we'll run this so that's
all loaded up in there so we have our
three different placeholders and since
we're in tensor flow when you use carass
it does some of this automatically but
we're in tensor flow direct sits on
tensor flow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each uh layer
has to have a bias going in we're going
to go ahead and work on our our
conversional 2D our Max pool so we have
our pooling layer our convolutional
layer and then our normal full layer so
we're going to go ahead and put those
all into definitions and let's see what
that looks like in code and you can also
grab some of these helper functions from
the MN the uh nist setup let me just put
that in there if you're under the tensor
flow so a lot of these are already in
there but we're going to go ahead and do
our own and we're going to create our uh
a knit weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here uh now we're not going to
be doing that we're just going to
recreate them for our code and let's
take a look at this we have our weights
and and so what comes in is going to be
the shape and what comes out is going to
be uh random numbers so we're going to
go ahead and just knit some random
numbers based on the shape with a
standard deviation of 0.1 kind of a fun
way to do that and then the TF variable
uh in nit random distribution so we're
just creating a random distribution on
there that's all that is for the weights
now you might change that you might have
a a higher standard deviation in some
cases you actually load preset weights
that's pretty rare usually you're
testing that against another model or
something like that and you want to see
how those weights configure with each
other uh now remember we have our bias
so we need to go ahead and initialize
the bias with the constant uh in this
case we're using 0.1 a lot of times the
bias is just put in as one and then you
have your weights to add on to that uh
but we're going to set this as 0.1 uh so
we want to return a convolutional 2d in
this case a neural network this is uh
would be a layer on here what's going on
with the con 2D is we're taking our data
coming in uh we're going going to filter
it strides if you remember correctly
strides came from here's our image and
then we only look at this picture here
and then maybe we have a stride of one
so we look at this picture here and we
continue to look at the different
filters going on there the other thing
this does is that we have our data
coming in as
32 by
32 by three and we want to change this
so that it's just this is three
dimensions and it's going to format this
as just two Dimensions so it's going to
take this number here and combine it
with the 32x 32 so this is a very
important layer here it's reducing our
data down using different means and it
connects down I'm just going to jump
down one here uh it goes with the
convolutional layer so you have your
your kind of your pre- formatting and
the setup and then you have your actual
convolution layer that goes through on
there and you can see here we have a
knit weights by the shape a knit bias
shape of three because we have the three
different uh here's our three again and
then we return the tfnn relu with the
convention 2D so this convolutional uh
has this feeding into it right there
it's using that as part of it and of
course the input is the XY plus b the
bias so that's quite a mouthful but
these two are the are the keys here to
creating the convolutional layers there
the convolutional 2D coming in and then
the convolutional layer which then steps
through and creates all those filters we
saw then of course we have our pooling
uh so after each time we run it through
the convectional l we want to pull the
data uh if you remember correctly on the
on the pool side and let me just get rid
of all my marks it's getting a little
crazy there and in fact let's go ahead
and jump back to that slide let's just
take a look at that slide over here uh
so we have our image coming in we create
our convolutional layer with all the
filters remember the filters go um you
know the filters coming in here and it
looks at these four boxes and then if
it's a step let's say step two and then
goes to these four boxes and then the
next step and so on uh so we have our
convolutional layer that we generate or
convolutional layers they use the uh
realu function um there's other
functions out there for this though the
reu is the uh most the one that works
the best at least so far I'm sure that
will change then we have our pooling now
if you remember correctly the pooling
was Max uh so if we had the filter
coming in and they did the
multiplication on there and we have a
one and maybe a two here and another one
here and a three here three is the max
and so out of all of these you then
create an array that would be three and
if the max is over here two or whatever
it is that's what goes into the pooling
of what's going on in our pooling uh so
again we're reducing that data down
we're reducing it down as small as we
can and then finally we're going to
flatten it out into a single array and
that goes into our fully connected layer
and you can see that here in the code
right here we're going to create our
normal full layer um so at some point
we're going to take from our pooling
layer this will go into some kind of
flattening process and then that will be
fed into the full the different layers
going in down here um and so we have our
input size you'll see our input layer
get shape which is just going to get the
shape for whatever is coming in uh and
then input size initial weights is also
based on uh the input layer coming in
and the input size down here is based on
the input layer shape so we're just
going to already use the shape and
already have our siid coming in and of
course uh you have to make sure you init
the bias always put your bias on there
and we'll do that based on the size so
this will return tf. matmo input layer
w+b this is just a normal full layer
that's what this means right down here
that's what we're going to return so
that was a lot of steps we went through
let's go ahead and run that so those are
all loaded in there and let's go ahead
and uh create the layers let's see what
that looks like now that we've done all
the heavy lifting and everything uh we
get to do all the easy part let's go
ahead and create our layers we'll create
a convolution layer one and two two
different convolutional layers and then
we'll take that and we'll flatten that
out create a a reshaped pooling in there
for our reshape and then we'll have our
full uh layer at the end so let's start
by creating our first uh convolutional
layer then we come in here and let me
just run that real quick and I want you
to notice on here the three and the 32
this is important because coming into
this convolutional layer we have three
different channels and 32 pixels each uh
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 44 is just
fine that should work pretty good for
what we're doing and for the size of the
image and then of course at the end once
you have your convolutional layer set up
you also need to pull it and you'll see
that the pooling is automatically set up
so that it would see the different shape
based on what's coming in so here we
have Max two 2 by two and we put in the
convolutional one that we just created
the convolutional layer we just created
goes right back into it and that right
up here as you can see is the X it's
coming in from here so it knows to look
at the first model and set the the data
accordingly set that up so it matches
and we went ahead and ran this already I
think I ran let me go and run it again
and if we're going to do one layer let's
go aad do a second layer down here and
it's we'll call it convo 2 it's also a
convolutional layer on this and you'll
see that we're feeding convolutional one
in the pooling so it goes from
convolutional one into convolutional one
pooling from convolutional one pooling
into convolutional two and then from
convolutional two into convolutional two
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flatten
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have uh 4x4 going in let's do 8X 8 by
64 so let's do
4,096 this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our convo
2 pooling and that will feed into here
the convo 2 pooling and then we're going
to set it up as a single layer that's
4,096 in size that's what that means
there we'll go ahead and run this so
we've now created this variable the
convo two flat and then we have our
first full layer this is the final uh
neural network where we have the flat
layer going in and we're going to again
use the uh reu for our uh setup on there
on a neural network for evaluation and
you'll notice that we're going to create
our first full layer our normal full
layer that's our definition so we
created that that's creating the normal
full layer and our input for the data
comes right here from the this goes
right into it uh the convo to flat so
this tells it how big the data is and
we're going to have it come out it's
going to have uh 1024 that's how big the
layer is coming out we'll go ahead and
run this so now we have our full layer
one and with the full layer one we want
to also Define the full one Dropout to
go with that so our full layer one comes
in uh keep probability equals hold
probability remember we created that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and uh finally
we'll go ahead and create a y predict
which is going to equal the normal full
one Dropout and 10 because we have 10
labels in there now in this neural
network we could have added additional
layers that would be another option to
play with you can also play with instead
of 1024 you can use other numbers for
the way that sets up and what's coming
out going into the next one we're only
going to do just the one layer and the
one layer layer Dropout and you can see
if we did another layer it'd be really
easy just to feed in the full one
Dropout into full layer two and then
full Layer Two Dropout would have full
Layer Two feed into it and then you'd
switch that here for the Y prediction
for right now this is great this
particular data set is tried and true
and we know that this will work on it
and if we just type in y predict and we
run that uh we'll see that this is a
tensor object uh shaped question mark 10
dtype 32 quick way to double check what
we're working on so now we've got all of
our uh we've done a setup all the way to
the Y predict which we just did uh we
want to go ahead and apply the loss
function and make sure that's set up in
there uh create the optimizer and then
uh trainer Optimizer and create a
variable to initialize all the global TF
variables so before we dive into the um
loss function let me point out one quick
thing or just kind of a rehap over a
couple things and that is when we're
playing with this these setups um we
pointed out up here we can change the 44
and use different numbers there they
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here of the 1024 also
this is also another number that if you
continue to raise that number you'll get
um possibly a better fit you might
overfit and if you lower that number
you'll use less resources and generally
you want to use this in um the
exponential growth an exponential being
2 4 8 16 and in this case the next one
down would be 512 you can use any number
there but those would be the ideal
numbers uh when you look at this data so
the next step in all this is we need to
also create uh a way of tracking how
good our model is and we're going to
call this a loss function and so we're
going to create a cross entropy loss
function and so before we discuss
exactly what that is let's take a look
and see what we're feeding it uh we're
going to feed it our labels and we have
our true labels and our prediction
labels uh so coming in here is we're the
two different uh variables we're sending
in or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy uh in
information Theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this and so the next step is
we're going to take the error we want to
know uh our cross entropy or our loss
function how much loss we have that's
going to be part of how we train the
model so when you know what the loss is
and we're training it you feed that back
into the back propagation setup and so
we want to go ahead and optimize that
here's our Optimizer we're going to
create the optimizer using an atom
Optimizer remember there's a lot of
different ways of optimizing the data
atom is the most popular used uh so our
Optimizer is going to equal the TF train
atom Optimizer if you don't remember
what the learning rate is let me just
pop this back into here here's our
learning rate when you have your weights
you have all your weights in your
different nodes that are coming out
here's our node coming out um and it has
all its weights and then the error is
being prop sent back through in reverse
on our neural network so we take this
error and we adjust these weights based
on the different formulas in this case
the atom formula is what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be bias
to whatever the last data we sent
through is instead we're going to
multiply that by 0.001 make a very small
shift in this weight so our Delta W is
only 0.001 of the actual Delta W of the
full change we're going to compute from
the atom and then we want to go ahead
and train it so our training or set up a
training uh uh variable or function and
this is going to equal our Optimizer
minimize cross entropy and we make sure
we go ahead and run this so it's loaded
in there and then we're almost ready to
train our model but before we do that we
need to create one more um variable in
here and we're going to create a
variable to initialize all the global TF
variables and when we look at this um
the TF Global variable initializer this
is a tensor flow um object it goes
through there and it looks at all our
different setup that we have going under
our tensor flow and then initializes
those variables uh so it's kind of like
a magic one because it's all hidden in
the back end of tensor flow all you need
to know about this is that you have to
have the initialization on there which
is an operation um and you have to run
that one once you have your setup going
so we'll go ahead and run this piece of
code and then we're going to go ahead
and train our data so let me run this so
it's loaded up there and so now we're
going to go ahead and run the model by
creating a graph session graph session
is a tensorflow term so you'll see that
coming up it's one of the things that
throws me because I always think of
graphx and Spark and graph as just
general graphing uh but they talk about
a graph session so we're going to go
ahead and run the model and let's go
ahead and walk through this uh what's
going on here and let's paste this data
in here and here we go so we're going to
start off with the with the TF session
as cess so that's our actual TF session
we've created uh so we're right here
with the TF uh session our session we're
creating we're going to run TF Global
variable initializer so right off the
bat we're initializing our variables
here uh and then we have for I in range
500 so what's going on here remember 500
we're going to break the data up and
we're going to batch it in at 500 points
each we've created our session run so
we're going to do with TF session as
session right here we've created our
variable session uh and then we're going
to run we're going to go ahead and
initialize it so we have our TF Global
variables initializer that we created um
that initializes our our session in here
the next thing we're going to do is
we're going to go for I in range of 500
batch equals ch. nextt batch so if you
remember correctly this is loading up um
100 pictures at a time and uh this is
going to Loop through that 500 times so
we are literally doing uh what is that
uh 500 time 100 is uh 50,000 so that's
50,000 pictures we're going to process
right there in the first process is
we're going to do a session run we're
going to take our train we created our
train variable or Optimizer in there
we're going to feed it the dictionary uh
we had our feed dictionary that created
and we have x equals batch 0 coming in y
true batch one hold the probability
0.5 and then just so that we can keep
track of what's going on we're going to
every uh 100 steps we're going to run a
print So currently on step format C
accuracy is um and we're going to look
at matches equals tf. equal TF argument
y prediction one tf. AR Max y true comma
1 so we're going to look at this is how
many matches it has and here our ACC uh
all we're doing here is we're going to
take the matches how many matches they
have it creates it generates a chart
we're going to convert that to float
that's what the TF cast does and then we
just want to know the average we just
want to know the average of the um
accuracy and then we'll go ahead and
print that out uh print session run
accuracy feed dictionary so it takes all
this and it prints out our accuracy on
there so let's go ahead and take this
oops screens there let's go ahead and
take this and let's run it and this is
going to take a little bit to run uh so
let's see what happens on my old laptop
and we'll see here that we have our
current uh we're currently on Step Zero
it takes a little bit to get through the
accuracy and this will take just a
moment to run we can see that on our
Step Zero it has an accuracy of 0.1 or
0128 um and as this running we'll go
ahead you don't need to watch it run all
the way but uh this accuracy is going to
change a little bit up and down so we've
actually lost some accuracy during our
step two um but we'll see how that comes
out let's come back after we run it all
the way through and see how the
different steps come come out I was
actually reading that backwards uh the
way this works is the closer we get to
one the more accuracy we have uh so you
can see here we've gone from a 0.1 to a
39 um and we'll go ahead and pause this
and come back and see what happens when
we're done with the full run all right
now that we've uh prepared the meal got
it in the oven and pulled out my
finished dish here if you've ever
watched uh any of the old cooking shows
let's discuss a little bit about this
accuracy going on here and how do you
interpret that that we've done a couple
things first we've defined accuracy um
the reason I got it backwards before is
you have uh loss or accuracy and with
loss you'll get a graph that looks like
this it goes oops that's an S by the way
there we go you get a graph that curves
down like this and with accuracy you get
a graph that curves up this is how good
it's doing now in this case uh one is
supposed to be really good accuracy that
mean it gets close to one but it never
crosses one so if you have accuracy of
one that is phenomenal um in fact that's
pretty much un you know unheard of and
the same thing with loss if you have a
loss of zero that's also unheard of the
zero is actually on this this axis right
here as we go in there so how do we
interpret that because you know if I was
looking at this and I go oh 0.51 that's
uh 51% you're doing 50/50 no this is not
percentage let me just put that in there
it is not percentage uh this is
logarithmic what that means is that 0. 2
is twice as good as 0.1 and uh when we
see 0. 4 that's twice as good as 0. 2
real way to convert this into a
percentage you really can't say this is
is a direct percentage conversion what
you can do though is in your head if we
were to give this a percentage uh we
might look at this as uh 50% we're just
guessing equals 0.1 and if 50% roughly
equals 0.1 that's where we started up
here at the top remember at the top here
here's our 0128 the accuracy of 50% then
75% is
about2 and so on and so on don't quote
those numbers because that doesn't work
that way they say that if you have
.95 that's pretty much saying 100% And
if you have uh anywhere between you'd
have to go look this up let me go and
remove all my drawings there uh so the
magic number is 0.5 we really want to be
over a 0.5 in this whole thing and we
have uh both 0504 remember this is
accuracy if we were looking at loss then
we'd be looking the other way but 0.0
you know instead of how high it is we
want how low it is uh but with accuracy
being over a 0. five is pretty valid
that means this is pretty solid and if
you get to a 0.95 then it's a direct
correlation that's what we're looking
for here in these numbers you can see we
finished with this model at0 5135 so
still good um and if we look at uh when
they ran this in the other end remember
there's a lot of Randomness that goes
into it when we see the weights uh they
got 0. 5251 so a little better than ours
but that's fine you'll find your own uh
comes up a little bit better or worse
depending on uh just that Randomness and
so we've gone through the whole model
we've created we trained the model and
we've also gone through on every uh
100th run to test the model to see how
accurate it
is what's in it for you we will start
with of course the fundamentals what is
is a neural network and popular neural
networks it's important to know the
framework we're in and what we're going
to be looking at specifically then we'll
touch on why a recurrent neural network
what is a recurrent neural network and
how does an RNN work uh one of the big
things about rnns is what they call the
vanishing and exploding gradient problem
so we'll look at that and then we're
going to be using a used case uh study
is going to be in carass on tensor flow
carass is a python model module for
doing neural networks in deep learning
and in there there's the what they call
long shortterm memory lstm and then
we'll use the use case to implement our
lstm on the carass so when you see that
lstm that is basically the RNN Network
and we'll get into that the use case is
always my favorite part before we dive
into any of this we're going to take a
look at what is an RNN or an
introduction to the RNN do you know how
Google's autocomplete feature predicts
the rest of the words a user is typing I
love that autocomplete feature as I'm
typing away saves me a lot of time I can
just kind of Hit the enter key and it
autofills everything and I don't have to
type as much well first there's a
collection of large volumes of most
frequently occurring consecutive words
uh this is fed into a recurrent neural
network analyses the data by finding the
sequence of words occurring frequently
and builds a model to predict the next
word in the sentence and then Google
what is the best food to eat in loss I'm
guessing you're going to say Los Mexico
no it's going to be Las Vegas uh so the
Google search will take a look at that
and say hey the most common auto
complete is going to be Vegas in there
and usually gives you three or four
different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a Google
search or even in Microsoft Words has a
some people get very mad at it it autof
fills with the wrong stuff uh but you
know you're typing away and it helps the
autofill I have that in a lot of my
different packages it's just a standard
feature that we are all used to now so
so before we dive into the RNN and
getting Into the Depths let's go ahead
and talk about what is a neural network
neural networks used in deep learning
consist of different layers connected to
each other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking throughout deep
learning the only way we can evaluate an
artificial intelligence or anything like
that is to compare it to human function
very important note on there and it
learns from a huge volumes of data and
it uses complex algorithm to train a
neural net so in here we have image
pixels of two different breeds of dog uh
one looks like a nice floppy eared lab
and one a German Shepherd you know both
wonderful breeds of animals that image
then goes into an input layer uh that
input layer might be formatted at some
point because you have to let it know
like you know different pictures are
going to be different sizes and
different color content then it'll feed
into hidden layers so each of those
pixels or each point a data goes in and
then um splits into the hidden layer
which then goes into another hidden
layer which then goes to an output layer
RNN there's some changes in there which
we're going to get into so it's not just
a straightforward propagation of data
like we've covered in many other
tutorials and finally you have an output
layer and the output layer has two
outputs it has one that lights up if
it's a German shepherd and another that
lights up it's if it's a labador so
identifies the dog's breed such networks
do not require memorizing the past
output so our forward propagation is
just that it goes forward it doesn't
have to rememorize stuff and you can see
there that's not actually me in the
picture uh dressed up in my
suit I haven't worn a suit in years so
as we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural networks first there's the feed
forward neural network used in general
regression and classification problems
and we have the convolution neural
network used for image recognition deep
neural network used for acoustic
modeling deep belief Network used for
cancer detection and recurrent neural
network used for speech recognition now
taking a lot of these and mixed them
around a little bit so just because it's
used for one thing doesn't mean it can't
be used for other modeling but generally
this is where the field is and this is
how those models are generally being
used right now so we talk about a feed
forward neural network in a feed forward
neural network information flows only in
the forward direction from the input
nodes through the hidden layers if any
into the output nodes there are no
Cycles or Loops in the network and so
you can see here we have our input layer
I was talking about how it just goes
straight forward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output and they
refer to the input is X a lot of times
and the output as y decisions are based
on current input no memory about the
past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
isssues is because it doesn't have a
scope of memory or time a feed forward
neural network doesn't know how to
handle sequential data uh it only
considers only the current input so if
you have a series of things and because
three points back affects what's
happening now and what your output
affects what's happening that's very
important so whatever I put as an output
is going to affect the next one um a
feed forward doesn't look at any of that
it just looks at this is what's coming
in and it cannot memorize previous
inputs so it doesn't have that list of
inputs coming in solution to feed
forward NE Network you'll see here where
it says recurrent neural network and we
have our X on the bottom going to H
going to Y that's your feed forward uh
but right in the middle it has a value C
so it's a whole another process it's
memorizing what's going on in the hidden
layers and the hidden layers as they
produce data feed into the next one so
your hidden layer might have an output
that goes off to Y uh but that output
goes back into the next prediction
coming in what this does is this allows
it to handle sequential data it
considers the current input and also the
previously received inputs and if we're
going to look at General drawings and um
Solutions we should also look at
applications of the RNN image captioning
RNN is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough I mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
Time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using RNN and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the New York Stock
Exchange in the US produces somewhere in
the neighborhood if you count all the
individual trades and fluctuations by
the second um it's like three terabytes
a day of data so we're only to look at
one stock just analyzing One stock is
really tricky in here we'll give you a
little jump on that so that's exciting
but don't expect to get rich off of it
immediately another application of the
RNN is natural language processing text
Mining and sentiment analysis can be
carried out using RNN for natural
language processing and you can see
right here the term natural language
processing when you stream those three
words together is very different than I
if I said processing language natural
leave so the time series is very
important when we're analyzing
sentiments it can change the whole value
of a sentence just by switching the
words around or if you're just counting
the words you might get one sentiment
where if you actually look at the order
they're in you get a completely
different sentiment when it rains look
for rainbows when it's dark look for
stars both of these are positive
sentiments and they're based upon the
order of which the sentence is going in
machine translation given an input in
one language RNN can be used to
translate the input into a different
languages as output I myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking English you would say
big cat and if you're speaking Spanish
you would say cat big so that
translation is really important to get
the right order to get uh there's all
kinds of parts of speech that are
important to know by the order of the
words here this person is speaking in
English and getting translated and you
can see here a person is speaking in
English in this little diagram I guess
that's denoted by the flags I have a
flag I own it no um but they're speaking
in English and it's getting translated
into Chinese Italian French German and
Spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
ched I can now travel into Worlds I
would never think of because I can have
something translate my English back and
forth readily and I'm not stuck with a
communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recurrent neural network
we turn that on its side so here it is
and now our X comes up from the bottom
into the hidden layers into Y and they
usually draw very simplified X to H with
c as a loop a to Y where a a B and C are
the perimeters a lot of times you'll see
this kind of drawing in here digging
closer and closer into the H and how it
works going from left to right you'll
see that the C goes in and then the X
goes in so the x is going Upward Bound
and C is going to the right a is going
out and C is also going out that's where
it gets a little confusing so here we
have xn uh CN and then we have y out and
C out and C is based on HT minus one so
our value is based on the Y and the H
value are connected to each other
they're not necessarily the same value
because H can be its own thing and
usually we draw this or we represent it
as a function h of T equals a function
of C where H of T minus one that's the
last H output and x a t going in so it's
the last output of H combined with the
new input of x uh where HT is the new
state FC is a function with the
parameter C that's a common way of
denoting it uh HT minus one is the Old
State coming out and then xit T is an
input Vector at time of Step T well we
need to cover types of recurrent neural
networks and so the first one is the
most common one which is a one toone
single output one toone neural network
is usually known as a vanilla neural
network used for regular machine
learning problems why because vanilla is
usually considered kind of a just a real
basic flavor but because it's very basic
lot of times they'll call it the vanilla
neural network uh which is not the
common term but it is you know kind of a
slang term people will know what you're
talking about usually if you say that
then we run one to mini so you have a
single input and you might have a
multiple outputs in this case uh image
captioning as we looked at earlier where
we have not just looking at it as a dog
but a dog catching a ball in the air and
then you have many to1 Network takes in
a sequence of inputs examples cinamon
analysis where a given sentence can be
classified as expressing positive or
negative sentiments and we looked at
that is we were discussing if it rains
look for a rainbow so positive sentiment
where rain might be a negative sentiment
if you were just adding up the words in
there and then of course if you're going
to do a one to one many to one one to
many there's many to many networks takes
in a sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in English and then
going out in all the different languages
uh you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize this
how difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called The Vanishing gradient
problem while training an RNN your slope
can be either too small or very large
and this makes training difficult when
the slope is too small the problem is
known as Vanishing gradient and you'll
see here they have a nice U image loss
of information through time so if you're
pushing not enough information forward
that information is lost and then when
you go to train it you start losing the
third word in the sentence or something
like that or it doesn't quite follow the
full logic of what you're working on
exploding gradient problem Oh this is
one that runs into everybody when you're
working with this particular neural
network when the slope tends to grow
exponentially instead of decaying this
problem is called exploding gradient
issues in gradient problem long tring
time poor performance bad accuracy and
I'll add one more in there uh your
computer if you're on on a a lower-end
computer testing out a model will lock
up and give you the memory error
explaining gradient problem consider the
following two examples to understand
what should be the next word in the
sequence the person who took my bike and
blank a thief the students who got into
engineering with blank from Asia and you
can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word Maybe Will C the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from Asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the RNN must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the SE sequence the RNN must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
Asia it might be sometimes difficult for
the era to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having uh whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular uh set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip with that that
gradient looks like and narrow the
training model that we're using when you
have a Vanishing gradient the OPA
problem uh we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talk too
much about activation functions so we'll
look at that just minimally uh there's a
lot of choices out there and then
finally there's long short-term memory
networks the
lstms and we can make adjustments to
that so just like we can clip the
gradient as it comes out we can also um
expand on that we can increase the
memory Network the size of it so it
handles more information and one of the
most common problems in today's uh setup
is what they call longterm dependencies
suppose we try to predict the last word
in the text the clouds are in the and
you probably said sky here we do not
need any further context it's pretty
clear that the last word is going to be
Sky suppose we try to predict the last
word in the text I have been staying in
Spain for the last 10 years I can speak
fluent maybe you said Portuguese or
French no you probably said Spanish the
word we predict will depend on the
previous few words in context here we
need the context of Spain to predict the
last word in the text it's possible that
the gap between the relevant information
and the point where it is needed to
become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior All recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent H
layer lstm s's also have a chainlike
structure but the repat competing module
has a different structure instead of
having a single neural network layer
there are four interacting layers
communicating in a very special way
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior LST tms's also
have a chain-like structure but the
repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs kit in here I
want you to note that you have X of T
minus one coming in you have X of T
coming in and you have x a t + one and
you have H of T minus one and H of T
coming in and H of t + one going out and
of course uh on the other side is the
output a um in the middle we have our
tangent H but it occurs in two different
places so not only when we're Computing
the x of t + one are we getting the
tangent H from X of T but we're also
getting that value coming in from the X
of T minus one so the short of it is as
you look at these layers not only does
it does the propagate through the first
layer goes into the second layer back
into itself but it's also going into the
third layer so now we're kind of
stacking those up and this can get very
complicated as you grow that in size it
also grows in memory too and in the
amount of resources it takes uh but it's
a very powerful tool to help us address
the problem of complicated long
sequential information coming in like we
were just looking at in the sentence and
when we're looking at our long shortterm
memory network uh there's three steps of
processing assessing in the lstms that
we look at the first one is we want to
forget irrelevant parts of the previous
state you know a lot of times like you
know is as in a unless we're trying to
look at whether it's a plural noun or
not they don't really play a huge part
in the language so we want to get rid of
them then selectively update cell State
value so we only want to update the cell
State values that reflect what we're
working on and finally we want to put
only output certain parts of the cell
state so whatever is coming out we want
to limit what's going out too and let's
dig a little deeper into this let's just
see what this really looks like uh so
step one decides how much of the past it
should remember first step in the lstm
is to decide which information to be
omitted in from the cell in that
particular time step it is decided by
the sigmoid function it looks at the
previous state h of T minus one and the
current input X of T and computes the
function so you can see over here we
have a function of T equals the sigmoid
function of the weight of f the H at T
minus one and then X at t plus of course
you have a bias in there with any of our
neural networks so we have a bias
function so F of T equals forget gate
decides which information to delete that
is not important from the previous time
step considering an L STM is fed with a
following inputs from the previous and
present time step Alice is good in
physics John on the other hand is good
in chemistry so previous output John
plays football well he told me yesterday
over the phone that he had served as a
captain of his college football team
that's our current input so as we look
at this the first step is the forget
gate realizes there might be a change in
context after en counting the First full
stop Compares with the current input
sentence of exit so we're looking at
that full stop and then Compares it with
the input of the new sentence the next
sentence talks about John so the
information on Alice is deleted okay
that's important to know so we have this
input coming in and if we're going to
continue on with John then that's going
to be the primary information we're
looking at the position of the subject
is vacated and is assigned to John and
so in this one we've seen that we've
weeded out a whole bunch of information
and we're only passing information on
John since that's now the new topic so
step two is in to decide how much should
this unit add to the current state state
in the second layer there are two parts
one is a sigmoid function and the other
is a tangent H in the sigmoid function
it decides which values to let through
zero or one tangent H function gives the
weightage to the values which are passed
deciding their level of importance minus
one to one and you can see the two
formulas that come up uh the I of T
equals the sigmoid of the weight of I a
to T minus 1 x of t plus the bias of I
and the C of T equals the tangent of H
of the wave of C of H of t minus1 x of t
plus the bias of C so our I of T equals
the input gate determines which
information to let through based on its
significance in the current time step if
this seems a little complicated don't
worry because a lot of the programming
is already done when we get to the case
study understanding though that this is
part of the program is important when
you're trying to figure out these what
to set your settings at you should also
note when you're looking at this it
should have some semblance to your
forward propagation neural networks
where we have a a value assigned to a
weight plus a bias very important steps
than any of the neural network layers
whether we're propagating into them the
information from one to the next or
we're just doing a straightforward
neural network propagation let's take a
quick look at this what it looks like
from the human standpoint um as I step
out in my suit again consider the
current input at xft John plays football
well he told me yesterday over the phone
that he had served as a captain of his
college football team that's our input
input gate analysis the important
information John plays football and he
was a captain of his college team is
important he told me over the phone
yesterday is less important hence it is
forgotten this process of adding some
new information can be done via the
input gate now this example is as a
human form and we'll look at training
this stuff in just a minute uh but as a
human being if I wanted to get this
information from a conversation maybe
it's a Google Voice listening in on you
or something like that um how do we weed
out the information that he was talking
to me on the phone yesterday well I
don't want to memorize that he talked to
me on the phone yesterday or maybe that
is important but in this case it's not I
want to know that he was the captain of
the football team I want to know that he
served I want to know that John plays
football and he was a captain of the
college football team those are the two
things that I want to take away as a
human being again we measure a lot of
this from the human Viewpoint and that's
also how we try to train them so we can
understand these neural networks finally
we get to step three decides what part
of the current cell State makes it to
the output the third step is to decide
what will be our output first we run a
sigmoid layer which decides what parts
of the cell State make it to the output
then we put the cell State through the
tangent H to push the values to be
between minus one and one and multiply
it by the output of the sigmoid gate so
when we talk about the output of T we
set that equal to the sigmoid of the
weight of zero of the H of T minus one
you back One Step in Time by the x of t
plus of course the bias the h t equals
the out of T times the tangent of the
tangent h of c t so our o equals the
output gate allows the past in
information to impact the output in the
current time step let's consider the
example to predicting the next word in
the sentence Jon played tremendously
well against the opponent and won for
his team for his contributions Brave
blank was awarded player of the match
there could be a lot of choices for the
empty space current input Brave is an
adjective adjectives describe a noun
John could be the best output after
Brave thumbs up for John awarded player
of the match and if you were to pull
just the nouns out of the sentence team
doesn't look right because that's not
really the subject we're talking about
contributions you know Brave
contributions or Brave team Brave player
Brave match um so you look at this and
you can start to train this these this
neural network so starts looking at and
goes oh no JN is what we're talking
about so brave is an adjective
John's going to be the best output and
we give John a big thumbs up and then of
course we jump into my favorite part the
case study use case implementation of
lstm let's predict the prices of stocks
using the lstm network based on the
stock price data between 2012 2016 we're
going to try to predict the stock prices
of 2017 and this will be a narrow set of
data we're not going to do the whole
stock market it turns out that the New
York Stock Exchange generates roughly
three terabytes of data per day that's
all the different trades up and down of
all the different stocks going on and
each individual one uh second to second
or nanc to nanc uh but we're going to
limit that to just some very basic
fundamental information so don't think
you're going to get rich off this today
but at least you can give an a you can
give a step forward in how to start
processing something like stock prices a
very valid use for machine learning in
today's markets use case implementation
of
lstm let's dive in we're going to import
our libraries we're going to import the
training set and uh get the scaling
going U now if you watch any of our
other tutorials a lot of these pieces
just start to look very familiar CU it's
very similar setup uh but let's take a
look at that and um just a reminder
we're going to be using Anaconda the
Jupiter notebook so here I have my
anaconda Navigator when we go under
environments I've actually set up a
carass python through 36 I'm in Python
36 and U nice thing about Anaconda
especially the newer version remember a
year ago messing with Anaconda different
versions of python and different
environments um Anaconda now has a nice
interface um and I have this installed
both on a Ubuntu Linux machine and on
windows so it works fine on there you
can go in here and open a terminal
window and then in here once you're in
the terminal window this is where you're
going to start uh installing using pip
to install your different modules and
everything now we've already
pre-installed them so we don't need to
do that in here uh but if you don't have
them installed on your particular
environment you'll need to do that and
of course you don't need to use the
anaconda or the Jupiter you can use
whatever favorite python ID you like I'm
just a big fan of this because it keeps
all my stuff separate you can see on
this machine I have specifically
installed one for carass since we're
going to be working with carass under
tensor flow when we go back to home I've
gone up here to application and that's
the environment I've loaded on here and
then we'll click on the launch Jupiter
notebook
now I've already in my Jupiter notebook
um have set up a lot of stuff so that
we're ready to go kind of like uh Martha
Stewarts in the old cooking shows we
want to make sure we have all our tools
for you so you're not waiting for them
to load and uh if we go up here to where
it says new you can see where you can um
create a new Python 3 that's what we did
here underneath the setup so it already
has all the modules installed on it and
I'm actually renamed this if you go
under file you can rename it we I'm
calling it RNN stock and let's just take
a look at start diving into the code
let's get into the exciting part now
we've looked at the tool and of course
you might be using a different tool
which is fine uh let's start putting
that code in there and seeing what those
Imports and uploading everything looks
like now first half is kind of boring
when we hit the rum button because we're
going to be importing numpy as NP that's
uh uh the number python which is your
numpy array and the mat plot library
because we're going to do some plotting
at the end and our pandas for our data
set our pandas is PD and when I hit run
uh it really doesn't do anything except
for load those modules just a quick note
let me just do a quick uh draw here oops
shift alt there we go you'll notice when
we're doing this setup if I was to
divide this up oops I'm going to
actually U let overlap these here we
go uh this first part that we're going
to do
is our
data prep a lot of prepping involved um
in fact depending on what your system
and since we're using carass I put an
overlap here uh but you'll find that
almost maybe even half of the code we do
is all about the data prep and the
reason I overlap this with uh carass me
just put that down because that's what
we're working in uh is because carass
has like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple Steps A lot of
times that are in the car setup uh we'll
take a look at that to see what comes up
in our code as we go through and look at
stock and then the last part is to
evaluate and if you're working with um
shareholders or uh you know classroom
whatever it is you're working with uh
the evaluate is the next biggest piece
um so the actual code here cross is a
little bit more but when you're working
with uh some of the other packages you
might have like three lines that might
be it all your stuff is in your
pre-processing in your data since carass
has is is Cutting Edge and you load the
individual layers you'll see that
there's a few more lines here in Cross
is a little bit more robust and then you
spend a lot of times uh like I said with
the evaluate you want to have something
you present to everybody else to say hey
this is what I did this is what it looks
like so let's go through those steps
this is like a kind of just general
overview and let's just take a look and
see what the next set of code looks like
and in here we have a data set train and
it's going to be read using the PD or
pandas read CSV and it's the Google
stock pric train.csv and so under this
we have training set equals data set
train. iocation and we've kind of sorted
out part of that so what's going on here
let's just take a look at let's let's
look at the actual file and see what's
going on there now if we look at this uh
ignore all the extra files on this um I
already have a train and a test set
where it's sorted out this is important
to notice because a lot of times we do
that as part of the pre-processing of
the data we take 20% of the data out so
we can test it and then we train the
rest of it that's what we use to create
our neural network that way we can find
out how good it is uh but let's go ahead
and just take a look and see what that
looks like as far as the file itself and
I went ahead and just opened this up in
a basic word pad text editor just so we
can take a look at it certainly you can
open up an Excel or any other kind of
spreadsheet um and we note that this is
a comma separated variables we have a
date uh open high low close volume this
is the standard stuff that we import
into our stock one the most basic set of
information you can look at in stock
it's all free to download um and this
case we downloaded it from uh Google
that's why we call it the Google stock
price um and it specifically is Google
this is the Google stock values from uh
as you can see here we started off at 13
20102 so when we look at this first
setup up here uh we have a data set
train equals pdor CSV and if you noticed
on the original frame um let me just go
back there they had it set to home
Ubuntu downloads Google stock price
train I went ahead and change that
because we're in the same file where I'm
running the code so I've saved this
particular python code and I don't need
to go through any special paths or have
the full path on there and then of
course we want to take out um certain
values in here and you're going to
notice that we're using um our data set
and we're now in pandas uh so pandas
basically it looks like a spreadsheet um
and in this case we're going to do
iocation which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we always start with zero which is the
date and we're going to be looking at
open and high which would be one and
two we'll just label that right there so
you can see now when you go back and do
this you certainly can extract culate
and do this on all the columns um but
for the example let's just limit a
little bit here so that we can focus on
just some key aspects of
stock and then we'll go up here and run
the code and uh again I said the first
half is very boring whenever you hit the
Run button it doesn't do anything CU
we're still just loading the data and
setting it up now that we've loaded our
data we want to go ahead and scale it we
want to do what they call feature
scaling and in here we're going to pull
it up from from the SK learn or the SK
kit pre-processing import min max scaler
and when you look at this you got to
remember that um biases in our data we
want to get rid of that so if you have
something that's like a really high
value U let's just draw a quick graph
and I have something here like the maybe
the stock has a value One stock has a
value of a 100 and another stock has a
value of
five um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100's going to
be the Max and five is going to be the
Min and then everything else goes and
then we change this so we just squish it
down I like the word squish so it's
between one and zero so 100 = 1 or 1
equal 100 and 0 equal 5 and you can just
multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus5 and then 100 divided or 95
divided by one so or whatever value is
is divided by
95 and uh once we've actually created
our scale we've telling it's going to be
from 0er to one we want to take our
training set and we're going to create a
training set scaled and we're going to
use our scaler SC we're going to fit
we're going to fit and transform the
training Set uh so we can now use the SC
this this particular object we'll use it
later on our testing set because
remember we have to also scale that when
we go to test our uh model and see how
it works and we'll go ahead and click on
the run again uh it's not going to have
any output yet because we're just
setting up all the
variables okay so we pasted the data in
here and we're going to create the data
structure with the 60 time steps and
output first note we're running 60 time
steps and that is where this value here
also comes in so the first thing we do
is we create our X train and Y train
variables and we set them to an empty
python array very important to remember
what kind of array we're in and what
we're working with and then we're going
to come in here we're going to go for I
in range 60 to 1258 there's our 60 60
time steps and the reason we want to do
this is as we're adding the data in
there there's nothing below the 60 so if
we're going to use 60 time steps uh we
have to start at 60 because it includes
everything underneath of it otherwise
you'll get a pointer error and then
we're going to take our X train and
we're going to append training set
scaled this is a scaled value between
zero and one and then as I is equal to
60 this value is going to be um 60 - 60
is 0 so this actually is0 to I so it's
going to be 0 to 60 1 to 61 let me just
circle this part right here 1 to 61 uh 2
to 62 and so on and so on and if you
remember I said 0 to 60 that's incorrect
because it does not count remember it
starts at zero so this is a count of 60
so it's actually 59 important to
remember that as we're looking at this
this and then the second part of this
that we're looking at so if you remember
correctly here we go we go from uh 0 to
59 of I and then we have a comma a zero
right here and so finally we're just
going to look at the open value now I
know we did put it in there for one to
two um if you remember correctly it
doesn't count the second one so it's
just the open value we're looking at
just open um and then finally we have y
train. append training set I to zero and
if you remember correctly I2 or I comma
0 if you remember correctly this is 0 to
59 so there's 60 values in it uh so when
we do I down here this is number 60 so
we're going to do this is we're creating
an array and we have 0 to
59 and over here we have number 60 which
is going into the Y train it's being
appended on there and then this just
goes all the way up so this is down here
is a a 0 to 59 and we'll call it 60
since that's the value over here and it
goes all the way up to 12 58 that's
where this value here comes in that's
the length of the data we're loading so
we've loaded two arrays we've loaded one
array that has uh which is filled with
arrays from 0 to 59 and we loaded one
array which is just the value and what
we're looking at you want to think about
this as a Time sequence uh here's my
open open open open open open what's the
next one in the series so we're looking
at the Google stock and each time it
opens we want to know what the next one
uh 0 through 59 what's 60 1 through 60
what's 61 2 through 62 what's 62 and so
on and so on going up and then once
we've loaded those in our for Loop we go
ahead and take XT train and Y train
equals np. array XT tr. NP array ytrain
we're just converting this back into a
numpy array that way we can use all the
cool tools that we get with numpy array
including reshaping so if we take a look
and see what's going on here we're going
to take our X
train we're going to reshape it wow what
the heck does reshape mean uh that means
we have an array if you remember
correctly um so many numbers by
60 that's how wide it is and so we're
when you when you do xtrain do shape
that gets one of the shapes and you get
um xtrain do shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um in this case where's that value 60
by 11 99 1258 minus
60199 and we're making sure that that is
shaped correctly so the data is grouped
into uh
11.99 by 60 different arrays and then
the one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the in layer needs to be
one value
that's like the leaf of a tree where
this is the branch and then it branches
out some more um and then you get the
Leaf in p. reshape comes from and using
the existing shapes to form it we'll go
ahead and run this piece of code again
there's no real output and then we'll
import our different carass modules that
we need so from carass Models we're
going to import the sequential model
we're dealing with sequential data we
have our dense layers we have actually
three layers we're going to bring in our
dense our LS TM which is what we're
focusing on and our Dropout and we'll
discuss these three layers more in just
a moment but you do need the with the
lstm you do need the Dropout and then
the final layer will be the dents but
let's go ahead and run this and that'll
bring Port our modules and you'll see we
get an error on here and if you read it
closer it's not actually an error it's a
warning what does this warning mean
these things come up all the time when
you're working with such Cutting Edge
modules that are completely being
updated all the time we're not going to
worry too much about the warning all
it's saying is that the h 5 py module
which is part of carass is going to be
updated at some point and uh if you're
running new stuff on carass and you
start updating your carass system you
better make sure that your H5 Pi is
updated too otherwise you're going to
have an error later on and you can
actually just run an update on the H5 Pi
now if you wanted to not a big deal
we're not going to worry about that
today and I said we were going to jump
in and start looking at what those
layers mean I meant that and uh we're
going to start off with initializing the
RNN and then we'll start adding those
layers in and you'll see that we have
the lstm and then the Dropout lstm then
Dropout lstm then Dropout what the heck
is that doing so let's explore that
we'll start by initializing the RNN
regressor equals sequential because
we're using the sequential model and
we'll run that and load that up and then
we're going to start adding our lstm
layer and some Dropout regularization
and right there should be the Q Dropout
regularization and if we go back here
and remember our exploding gradient well
that's what we're talking about the
Dropout drops out unnecessary data so
we're not just shifting huge amount of
data through um the network so and so we
go in here let's just go ahead and uh
add this in I'll go ahead and run this
and we had three of them so let me go
and put all three of them in and then we
can go back over them there's the second
one and let's put one more in let let's
put that in and we'll go and put two
more in I meant to put I said one more
in but it's actually two more in and
then let's add one more after that and
as you can see each time I run these
they don't actually have an output so
let's take a closer look and see what's
going on here so we're going to add our
first lstm layer in here we're going to
have units 50 the units is the positive
integer and it's the dimensionality of
the output space this is what's going
out into the next layer so we might have
60 coming in but we have 50 going out we
have a return sequence because it is a
sequence data so we want to keep that
true and then you have to tell it what
shape it's in well we already know that
shape by just going in here and looking
at xtrain shape so input shape equals
the xtrain shape of one comma 1 makes it
really easy you don't have to remember
all the numbers that put in 60 or
whatever else is in there you just let
it tell the regressor what model to use
and so we follow our STM with a Dropout
layer now understanding the Dropout
layer is kind of exciting because one of
the things that happens is we can
overtrain our Network that means that
our neural network will memorize such
specific data that it has trouble
predicting anything that's not in that
specific realm to fix for that each time
we run through the training mode we're
going to take 02 or 20% of our neurons
and just turn them off so we're only
going to train on the other ones and
it's going to be random that way each
time we pass through this we don't
overtrain these nodes come back in in
the next training cycle we randomly pick
a different 20 and finally I see a big
difference as we go from the first to
the second and third and fourth the
first thing is we don't have to input
the shape because the shape's already
the output units is 50 here this Auto
The Next Step automatically knows this
layer is putting out 50 and because it's
the next layer it automatically sets
that and says oh 50 is coming out from
our last layer it's coming out you know
goes into the regressor and of course we
have our Dropout and that's what's
coming into this one and so on and so on
and so the next three layers we don't
have to let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we're
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
Dropout and dense uh D just says we're
going to bring this all down into one
output instead of putting out a sequence
we just know want to know the answer at
this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so so far we've brought in our uh
way up here we brought in our data we
brought in our different modules we'
formatted the data for training it we've
set it up you know we have our y x train
and our y train we have our source of
data and the answers we're we know so
far that we're going to put in there
we've reshaped that we've come in and
built our carass we've imported our
different layers and we have in here if
you look we have what uh five total
layers now carass is a little different
than a lot of other systems because a
lot of other systems put this all in one
L line and do it automatic but they
don't give you the options of how those
layers interface and they don't give you
the options of how the data comes in
carass is Cutting Edge for this reason
so even though there's a lot of extra
steps in building the model this has a
huge impact on the output and what we
can do with this these new models from
carass so we brought in our dense we
have our full model put together our
regressor so we need to go ahead and
compile it and then we're going to go
ahead and fit the data we're going to
compile the pieces so they all come
together and then we're going to run our
training data on there and actually
recreate our regressor so it's ready to
be used so let's go ahead and compile
that and I go ahe and run that and uh if
you've been looking at any of our other
tutorials on neural networks you'll see
we're going to use the optimizer atom
adom is optimized for Big Data there's a
couple other optimizers out there uh
beyond the scope of this tutorial but
certainly Adam will work pretty good for
this and loss equals mean squared value
so when we're training it this is where
we want to base the loss on how bad is
our error well we're going to use the
mean squared value for our error and the
atom Optimizer for its differential
equations you don't have to know the
math behind them but certainly it helps
to know what they're doing and where
they fit into the bigger models and then
finally we're going to do our fit
fitting the RN into the training set we
have the regressor do fit xtrain y train
epics and batch size so we know where
this is this is our data coming in for
the X train our y train is the answer
we're looking for of our data our
sequential input appex is how many times
we're going to go over the whole data
set we created a whole data set of
xtrain so this is each each of those
rows which includes a Time sequence of
60 and badge size another one of those
things where carass really shines is if
you were pulling the save from a large
file instead of trying to load it all
into RAM it can now pick smaller batches
up and load those indirectly we're not
worried about pulling them off a file
today this isn't big enough to uh cause
the computer too much of a problem to
run not too straining on the resources
but as we run this you can imagine what
would happen if I was doing a lot more
than just one column in one set of stock
in this case Google stock imagine if I
was doing this across all the stocks and
I had instead of just the open I had
open close high low and you can actually
find yourself with about 13 different
variables times 60 because it's a Time
sequence suddenly you find yourself with
a gig of memory you're loading into your
RAM which will just completely you know
if it's just if you're not on Multi
computers or cluster you're going to
start running into resource problems but
for this we don't have to worry about
that so let's go ahead and run this and
this will actually take a little bit on
my computer it's an older laptop and
give it a second to kick in there there
we go all right so we have epic so this
is going to tell me it's running the
first run through all the data and as
it's going through it's batching them in
32 pieces so 32 uh lines each time and
there's 1198 I think I said $199 earlier
but it's $ 1198 I was off by one and
each one of these is 13 seconds so you
can imagine this is roughly 20 to 30
minutes run time on this computer like I
said it's an older laptop running at09
GHz on a dual processor and that's fine
what we'll do is I'll go ahead and stop
go get a drink of coffee and come back
and let's see what happens at the end
and where this takes us and like any
good cooking show I've kind of gotten my
latte I also had some other stuff
running in the background so you'll see
these numbers jumped up to like 19
seconds 15 seconds which you can scroll
through and you can see we've run it
through 100 steps or 100 Epic
so the question is what does all this
mean one of the first things you'll
notice is that our loss can is over here
kind of stopped at 0.0014 you can see it
kind of goes down until we hit about
0.0014 three times in a row so we
guessed our epic pretty close since our
loss has remain the same on there so to
find out what we're looking at we're
going to go ahead and load up our test
data the test data that we didn't
process yet and uh real stock price data
set test is location this is the same
thing we did when we prepped the data in
the first place so let's go ahead and go
through this code and we can see we've
labeled it part three making the
predictions and visualizing the results
so the first thing we need to do is go
ahead and read the data in from our test
CSV you see I've changed the path on it
for my computer and uh then we'll call
it the real stock price and again we're
doing just the one column here and the
values from ication so it's all the rows
and just the values from these that one
location that's the open Stock open
let's go ahead and run that so that's
loaded in there and then let's go ahead
and uh create we have our inputs we're
going to create inputs here and this
should all look familiar this is the
same thing we did before we're going to
take our data set total we're going to
do a little Panda concap from the data
State train now remember the end of the
data set train is part of the data going
in and let's just visualize that just a
little bit here's our train data let me
just put TR for train and it went up to
this value here but each one of these
values generated a bunch of columns so
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data so to find out what we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and real stock
price data set test iocation this is the
same thing we did when we prepped the
data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test CSV you see I've changed
the path on it for my computer and uh
then we'll call it the real stock price
and again we're doing just the one
column here and the values from ication
so it's all the rows and just the values
from these that one location that's the
open Stock open and let's go ahead and
run that so that's loaded in there and
then let's go ahead and uh create we
have our our inputs we're going to
create inputs here and this should all
look familiar this is the same thing we
did before we're going to take our data
set total we're going to do a little
Panda concap from the data set train now
remember the end of the data set train
is part of the data going in and let's
just visualize that just a little bit
here's our train data let me just put TR
for train and it went up to this value
here but each one of these values
generated a bunch of columns it was 60
across and this value here here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data cuz that's part of the next
data or it's actually the top 59 so
that's what this first setup is over
here is we're going in we're doing the
real stock price and we're going to just
take the data set test and we're going
to load that in and then the real stock
price is our data test. test location so
we're just looking at that first uh
column the open price and then our data
set total we're going to take pandas and
we're going to concat and we're going to
take our data set train for the open and
our data set test open and this is one
way you can reference these columns
we've referenced them a couple different
ways we've referenced them up here with
the one two but we know it's labeled as
a panda set is open so pandas is great
that way lots of Versatility there and
we'll go ahead and go back up here and
run this there we go and uh you'll
notice this is the same as what we did
before we have our open data set we
pended our two different or concatenated
our two data sets together we have our
inputs equals data set total length data
set total minus length of data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works cuz normally when you're
running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that uh we didn't
train it with to see how well it graphs
and we have our inputs equals inputs.
reshapes or reshaping like we did before
we're Transforming Our inputs so if you
remember from the transform between zero
and one and finally want to go ahead and
take our X test and we're going to
create that X test and for I in range 60
to 80 so here's our X test and we're
appending our inputs I to 60 which
remember is 0 to 59 and I comma 0 on the
other side so that's just the First
Column which is our open column and uh
once again we take our X test we convert
it to a numpy array we do the same
reshape we did before and uh then we get
down to the final two lines and here we
have something new right here on these
last two lines let me just highlight
those or or Mark predicted stock price
equals regressor do predicts X test so
we're predicting all the stock including
both the training and the testing model
here and then we want to take this
prediction and we want to inverse the
transform so remember we put them
between zero and one well that's not
going to mean very much to me to look at
a at a float number between zero one I
want the dollar amounts I want to know
what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together takes just a second to run the
same neural network that took us what a
half hour to train ahead and plot the
data we're going to plot what we think
it's going to be and we're going to plot
it against the real data what what the
Google stock actually did so let's go
ahead and take a look at that in code
and let's uh pull this code up so we
have our PLT that's our uh oh if you
remember from the very beginning let me
just go back up to the top we have our
matplot library. pyplot as PLT that's
where that comes in and we come down
here we're going to plot let me get my
drawing thing out again in we're going
to go ahead and PLT is basically kind of
like an object it's one of the things
that always threw me when I'm doing
graphs in Python cuz I always think you
have to create an object and then it
loads that class in there well in this
case PLT is like a canvas you're putting
stuff on so if you've done HTML 5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real Google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your graph especially if you're going
to present this to somebody you know to
your shareholders in the office and uh
the xlabel is going to be time because
it's a Time series and we didn't
actually put the actual date and times
on here but that's fine we just know
they're incremented by time and then of
course the Y label is the actual stock
price pt. Legend tells us to build the
legend on here so that the color red and
and real Google stock price show up on
there and then the plot shows us that
actual graph so let's go ahead and run
this and see what that looks like and
you can see here we have a nice graph
and let's talk just a little bit about
this graph before we wrap it up here's
our Legend I was telling you about
that's why we have the legend to showed
the prices we have our title and
everything and you'll notice on the
bottom we have a Time sequence we didn't
put the actual time in here now we could
have we could have gone ahead and um
plotted the X since we know what the the
dates are are and plotted this to dates
but we also know it's only the last
piece of data that we're looking at so
last piece of data which ends somewhere
probably around here on the graph I
think it's like about 20% of the data
probably less than that we have the
Google price and the Google price has
this little up jump and then down and
you'll see that the actual Google
instead of a a turn down here just
didn't go up as high and didn't low go
uh down so our prediction has the same
pattern but the overall value is pretty
far off as far as um stock but then
again we're only looking at one column
we're only looking at the open price
we're not looking at how many volumes
were traded like I was pointing out
earlier we talk about stock just right
off the bat there's six columns there's
open high low close volume then there's
weather uh I mean volume shares then
there's the adjusted open adjusted High
adjusted low adjusted close they have a
special formula to predict exactly what
it would really be worth based on the
value of the stock and then from there
there's all all kinds of other stuff you
can put in here so we're only looking at
one small aspect the opening price of
the stock and as you can see here we did
a pretty good job this curve follows the
curve pretty well it has like a you know
little jumps on it bends they don't
quite match up so this Bend here does
not quite match up with that bin there
but it's pretty darn close we have the
basic shape of it and the prediction
isn't too far off and you can imagine
that as we add more data in and look at
different aspects in the specific domain
of stock we should be able to get a
better representation each time we drill
in deeper of course this took a half
hour for my program my computer to train
so you can imagine that if I was running
it across all those different variables
might take a little bit longer to train
the data not so good for doing a quick
tutorial like this then accelerate your
career in Ai and ml with a comprehensive
post-graduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below graphs so computational graphs are
really the heart and soul of neural
networks uh we talk about a
computational graph they are a visual
representation of expressing and
evaluating mathematical equations the
nodes and data flow in a graph
correspond to mathematical operations
and variables you'll hear a lot uh some
of the terms you might hear are node and
Edge The Edge being the data flow in
this case um it could also represent an
actual value
they have um oh I think in spark they
have a graph x which works just on
Computing edges there's all kinds of
stuff that has evolved from
computational graphs we're focusing just
on carass and on uh neural networks so
we're not going to go into great detail
on everything a computational graph does
it is a core component of a neural
network is what's important to know on
this so carass offers a python
userfriendly front- end while
maintaining a strong computation power
by using a low-level API like tensor
flow pie torch Etc which use
computational grass as a back end so one
this allows for abstraction of complex
problems while specifying control flow
if you've ever looked at some of the
backin or the original versions of
tensorflow uh it's really a nightmare
you have all these different settings
you have to put in there and create uh
it's a lot of a lot of back-end
programming this is like the old
computers when you had to uh tell it how
to dispose of a variable and how to
properly reallocate the memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with carass is it covers
a lot of this stuff and does things for
you that you would could spend hours on
just trying to figure
out it's useful for calculating
derivatives by using back propagation
we're definitely not going to teach a
class on derivatives uh in this little
video but understanding uh a derivative
is the rate of change so if you have a
particular function you're using in your
neural network a lot of them is just
simple uh uh yal MX plus b um your ukian
geometry where you just have a simple
slope times the intercept and they get
very complicated they have the inverse
tangent function for Activation as
opposed to just a linear ukian model and
you can think about this as you have
your data coming in and you have to
alter it somehow well you alter it going
down to get an answer you end up with an
error and that error goes back up and
you have to have that back propagation
with the derivative you want to know how
it changed so that you can figure out
how to adjust it for the
error a lot of that's hidden so you
don't even have to worry about it with
carass and in today's coros it'll even
if you create your own um uh formula for
computing an answer it will
automatically compute the back prop the
the derivative for you in a lot of
cases it's easier to implement
distributed computation
uh so cross is really nice way to
package it and get it off on different
computers and share it and it allows
parallelism which means that two
operations can run
simultaneously so as we start developing
these backends it can do all kinds of
cool things and utilize multiple cores
gpus on a computer uh to get that
parallel processing
up what are neural networks well like I
said there already uh we talked about in
computational edges you have a node and
you have a connection or your Edge so
neural networks are algorithms fashioned
after the human brain which contain
multiple layers each layer contains a
node called a neuron which performs a
mathematical operation they break down
complex problems into simple
operations so one an input layer takes
in our data and pre-processes it when we
talk about pre-processing when you're
dealing with neural networks uh you
usually have to pre-process your data so
that it's between minus one and one or
zero and one um into some kind of value
that's usable that occurs before it gets
to the neural network in fact 80% of
data science is usually impr prepping
that data and getting it ready for your
different
models two you have hidden layer
performs a nonlinear transformation of
input now it can do a hidden a linear
transformation it can use just a basic
um ukian geometry and you could think of
a node adding all the different
connections coming in uh so each
connection would have a weight and then
it would add to that weight plus an
intercept um in the Noe itself so you
can actually use ukian geometry but a
lot of these get really complicated they
have all these different formulas and
they're really cool to look at but when
you start looking at them look at how
they work uh you really don't need to
know the high math behind it um to
figure them out and figure out what
they're doing which is really cool that
means a lot of people can use this
without having to go get a p PhD in
mathematics number three the output
layer takes the results from hidden
layer transform them and gives a final
output so sequential models uh so what
makes this a sequential model sequential
models are linear stacks of layers where
one layer leads to the next it is simple
and easy to implement and you just have
to make sure that the previous layer is
the input to the next layer so uh you
have used for plain stack of layers
where each layer has one input and one
output tensor and this is what tensor
flow is named after is um each one of
these layers is like a tensor each each
node is a tensor and then the layer is
also considered a tensor of
values and it's used for simple
classifier declassify models you can
it's also used for regression models too
so it's not just about uh this is
something this is a teapot this is a cat
this is a dog um it's also used for
generating um uh regret the actual
values you know this is worth $10 that's
worth $30 uh the weather's going to be
90 degrees out or whatever it is so you
can use it for both classifier and
declassify uh
models and one more note when we talk
about sequential models the term
sequential is used a lot and it's used
in different areas and different
notations when you're in data science so
when we talk about time series we'll
talk about sequential that is something
very different uh sequential in this
case means it goes from the input to
layer one to Layer Two to the output so
it's very directional it's important to
note this because if you have a
sequential model can you have a
non-sequential model and the answer is
yes uh if you master the basics of a
sequential model you can just as easily
have another model that shares layers um
you can have another model where they
you have an input coming in and it
splits and then you have one set that's
doing one set of uh nodes maybe they're
doing a yes no kind of node where it's
either putting out a zero or a one a
classifier and the other one might be
regression it's just processing numbers
and then you recombine them for the
output um that's what they call a cross
uh the cross
API so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that we're just focusing on
the basic cross model with the
sequential
model
so let's dive into the meat of the
matter let's do a and do a demo on here
uh today's demo in this demo we will be
performing flower classification using
sequential model and carass and we'll
use our model to classify between five
different types of
flowers now for this demo and you can do
this demo on whatever platform you want
or whatever um user interface for
developing python um I'm actually using
anaconda and then I'm using Jupiter
notebooks to develop in and if you're
not familiar with this um you can go
under environment once you've created
environment you can come in here to open
a terminal window and if you don't have
the different modules in here you can do
youra install or whatever module it is
um just happened that this particular
setup didn't have a caborn in it which I
already installed uh so here's our an
Conta and then I'm going to go
back and start up my Jupiter
notebook where I already created a uh
new uh python project Python 3 I'm in
Python 3.8 on this particular one um
sequential model for flowers so lots of
fun there uh so we're going to jump
right into this the first thing is to
make sure you have all your modules
installed so if you don't have a numpy
panda Windows Mt plot library and
Seaborn and the carass um and sklearn or
S kit it's not actually sklearn you'll
need to go ahead and install all of
those now having done this for years and
having switched environments and doing
different things um I get all my imports
done and then we just run it and if we
get an error we know we have to go back
and install something um right off the
bat though we have numpy pandas matplot
Library Seaborn these are built on top
of each other pandas data frame and
built on top of numpy the uh um data
array and then we bring in our SK learn
or S kit this is the scikit setup SCI uh
kit even though you use sklearn to bring
it in it's a scit and then our carass we
have our pre-processing the images image
data generator um our model this is our
basic model or sequential
model uh and then we bring in from coros
layers uh import Port dents um
optimizers these optimizers a lot of
them already come in these are your
different optimizers and it's almost a
lot of this is so automatic now um atom
is the a lot of times the default
because you're dealing with a large data
uh and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models um you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out and we're doing just the sequential
you'll see here layers
dense and then if we come down a little
bit more uh when they put this together
and they're running the dense layers
you'll also see they have Dropout they
have flatten they have activation uh
they have the uh convolutional layer 2D
Max pooling 2D batch
normalization what are all these layers
uh and when we get to the model we'll
going to talk about them uh a lot of
times when you're just starting you can
just uh uh import cross. layers and then
you have your drop out your flatten uh
your convolutional uh neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there uh what I want you take from here
though is you need to run your Imports
uh and load your different aspects of
this and of course your tensor flow TF
because this is all built on tensor flow
and then finally uh import random is RN
just for random
generation and then we get down here we
have our uh
CV2 that is your um open image or your
open CV they call it for processing
images that's what the CV2
is uh we have our tqdm
the tqdm is for um is a progress bar
just a fancy way um of adding when
you're running a process you can view
the bar going across in the Jupiter uh
setup not really necessary but it's kind
of fun to have um we want to be able to
shuffle some files uh again these are
all different things pill is another um
image processor it goes with the CV2 a
lot of times you'll see both of those
and so we run the we got to bring them
all
in and the next thing is to set up our
directories and so when we come into the
directories there's an important thing
to note on here other than we're looking
at a lot of flowers which is fun uh as
we get down here we have our directory
archive flowers that just happens to be
where the different uh files for
different flowers are put in we're
denoting an X and a z and the x is the
data of the image and the Z is the tag
for it what kind of flower is this uh
and the image size is really important
because we have to resize everything if
you have a neural network and if you
remember from our neural networks uh let
me flip back to that
slide when we look at this slide we have
two input nodes here uh with an image
you have an input node depending on how
you set it up for each pixel and that
pixel has three different color schemes
usually in it
sometimes four so if you have a picture
that's 150 by
150 uh you multiply 150 * 150 * three
that's how many nodes input layers
coming in I mean so this is a massive
input a lot of times you think oh yeah
it's just a a small amount of data or
something like that uh no it's a full
image coming in then you have your
hidden layers A lot of times they match
what the image size is coming in so each
one of those is also just as big and
then we get down to just a single output
so that's kind of a a thing to note in
here what's going on behind the scenes
and of course each one of these layers
has a lot of processes and stuff going
on and then we have our our different uh
directories on here let me go and run
that so I'm just setting the directories
that's all this is um archive flowers
Daisy sunflower tulip dandelion Rose uh
just our different directories that
we're going to be looking
at
uh and then we want to go ahead and we
need to assign labels remember we
defined x and
z so we're just going to create a uh a
definition here um and the first thing
is uh return flower type
okay just returns it what kind of flower
it is I guess assign label to it uh but
we're going to go ahead and make our
train data and when you look at this
there's a couple things to take away
from here uh the first one is we're just
appending right onto our numpy array the
image we're going to let numpy handle
all that different aspects as far as 150
by 150 by 3 uh we just dump it right
into the numpy which makes it really
easy we don't have to do anything funky
on the processing and we want to leave
it like that and I'm going to talk about
that in a minute uh and then of course
we have to have the string a pin the
label on there and I want you to notice
right here uh we're going to read the
image in
and then we're going to size it and this
is important because we're just changing
this to 150 by 150 we're resizing the
image so it's uniform every image comes
in identical to the other ones uh this
is something that's so important is um
when you're resizing or reformatting
your data you really have to be aware of
what's going on with images it's not a
big deal because with an image you just
resize it so it looks squishy or spread
out or stretched um the neural Network
picks up on that and it doesn't really
change how it processes
it so let's go ahead and run that uh and
now we've got our definition set up on
there and then we want to go ahead and
make our
uh training data uh so make the train
data uh daisy flower daisy directory uh
print length of X so here we go let's go
and run that and we're just loading up
the flower daisy
uh so this is going all in there and
it's setting um it's adding it in to the
our setup on there to our X andz setup
and we see we have
769 um and then of course you can see
this nice bar here this is the bar going
across is that little added uh code in
there that just makes it really cool for
doing demos uh not necessarily when
you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called um
tqdm I can never remember that uh but
the tqdm module in there is really nice
and we'll go ahead and do sunflowers and
of course you could have just uh created
an array of these um but this has an
interesting problem that's going to come
up and I want to show you something it
doesn't matter how good the people in
the back are or how good you are at
programming errors are going to come up
and you got to figure out how to handle
them uh and so when we get all the way
down
to the um where is it dandelion here's
our dandelion directory we're going to
build
um Jupiter has some cool things it does
which makes this really easy to deal
with but at the same time you would want
to go back in there depending on how
many times you rerun this how many times
you pull this so when you're finding
errors uh going in here there's a couple
things you can do and we're just going
to um oh it wasn't there it is there's
our error I knew there was an error this
processed 1,6 62 out of
1065 now I can do a couple things one I
can go back into our definition and I
can just put in here try and so if it
has a bad conversion because this is
where the errors is coming from uh just
skip it that's one way to do it um when
you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
uh date at the end you just say okay I
lost three points who cares um or you
can go in there and try to delete it um
it really doesn't matter for this
particular demo and so we're just going
to leave that eror right alone and skip
over because it's already added all the
other files in there and this is
wonderful thing about Jupiter notebook
is that I can just continue on there and
the x and z which we're creating is
still uh running and we'll just go right
into the next flower row so all these
flowers are in there um that's just a
cool thing about Jupiter
notebook
uh and then we can go ahead and just
take a quick look and
see what we're dealing with and this is
of course really when you're dealing
with a other people and showing them
stuff this is just kind of fun where we
can display it on the plot Library here
and we're just going to go through and
um let's see what we got here uh looks
like we're going to do like five of each
of them I think is that how they set
this up um plot Library 5x two okay oh I
see how they did it okay so two each so
we have 5x two set up on our axes and
we're just going to go in and look at a
couple of these
flowers it's always a good thing to look
at some of your data uh no matter what
you're doing we've reformatted this to
150 by 150 you can see how it really
blurs this one up here on the Tulip that
is that resize to 150 by 150 um and
these are what's actually going in these
are all 150x 150 images you can check
the dimensions on the side and you can
see uh just a quick sampling of the
flowers we're actually going to process
on here and again like I said at the
beginning most of your work in data
science is
reprocessing this different uh
information so we need to go ahead and
take our
labels uh and run a label encoder on
there and then we're just going to Le is
a label encoder one of the things we
imported and then we always use the fit
um to categorical y comma 5 uh X here's
our array um X so if you look at this
here's our fit we're going to transform
Z that's our Z array we
created um and then we have Y which
equals that and then we go ahead and do
uh to categorical we want five different
categories and then we create our x uh
inpay of x x = x over
255 so what's going on here there's two
different Transformations one we've
turned our categories into 0 1 2 3 4 5
as the output and we have taken our X
array and remember the X array is three
values of your different
colors this is so important to
understand when we do this across a
numpy array this takes every one of
those three colors so we have 150 by 150
pixels out of those 150 by 150 pixels
they each have three um color arrays and
those clay arays ra range from 0 to 250
so when we take the xal X over
255 I'm sorry range from 0 to 255 this
converts all those pixels to a number
between 0o and one and you really want
to do that when you're working with
neural networks uh now if you do a
linear regression model um it doesn't
affect it as much and so you don't have
to do that conversion if you're doing
straight numbers but when you're running
neural networks if you don't do this
you're going to create a huge bi
and that means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions so now we have our um X and
Y values uh X being the data n y being
our no one
output and with any good setup we want
to divide this data into our training so
we have X train uh we have our X test
this is the data we're not going to
program the model with and of course
your y train corresponds to your X train
and your y test corresponds to your X
test the outputs and this is uh when we
do the train test split this was from
the S kit sklearn we imported train test
split and we're just going to go ahead
and do the test size at about a quarter
of the data 0.25 and of course random is
always good this is such a good tool I
mean certainly you can do your own
division um you know you could just take
the first you know 0.25 of the data or
whatever do the length of the data not
real hard to do but this is randomized
so that if you're running this test a
few times you can kind of get an idea
whether it's going to work or
not sometimes what I will do is um I'll
just split the data into three parts and
then I'll test it on two with one being
the U or train it on two of those parts
with one being the test and I rotate it
so I come up with three different
answers which is a good way of finding
out just how good your model is uh but
for setting up let's stick with the XT
train X test and the SK learn package
and then we're going to go ahead and uh
do a random
seed uh now a lot of times the cross
actually does this automatically but
we're going to go ahead and set it up on
here and you can see we did an NP random
seed um from 42 and we get a nice RN
number um and then we do TF random we
set the seed so you can set your
Randomness at the beginning of your
tensor flow and that's what the tf.
random. set
is so that's a lot of prep um all this
prep and then we finally get to the
exciting part um this is where you
probably spend once you have the data
prepped and you have your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these
models and so we look at this model one
we're going to designate it sequential
um they have the API which is a cross
the cross tensorflow API versus
sequential sequential means we're going
one layer to the next so we're not going
to split the layer and bring it back
together it looks almost the same with
the exception of um bringing it back
together so it's not a huge step to go
from this to an
API and the first thing we're going to
look at is um our convolutional neural
network in 2D uh so what's going on here
there's a lot of stuff that's going on
here um the default for well let's start
with the beginning what is a
convolutional 2d Network
networ well convolutional 2D Network
creates a number of small windows and
those small Windows float over the
picture and each one of them is their
own neural network and it's basically U
becomes like a um a categorization and
then it looks at that and it says oh if
we add these numbers up a certain way uh
we can find out whether this is the
right flower based on this this little
window floating around which looks at
different things and we have fil fter is
32 so this is actually creating 32
Windows is what that's
doing and the kernel size is 5x5 so
we're looking at a 5x5 Square remember
it's 150 by 150 so this narrows it down
to a 5x5 it's a 2d so it has your XY
coordinates um and when we look at this
5x5 remember each one of these is is
actually looking at 5x 5
by3 uh so we're actually looking at 15x
15 different um
pixels and padding is just um H usually
I just ignore that activation by default
is railu we went ahead and put the rilu
in
there there's a lot of different
activations Ru is for your smaller uh
when you remember I mentioned atom when
you have a lot of data data use an atom
kind of activation or use an atom
processing we're using the railu here uh
it kind of gives you a yes or no
but it it doesn't give you a full yes or
no it has a um a zero and then it kind
of shoots off at an angle very common
this is the most common wand and then of
course here's our input shape 150 by 150
by 3
pixels and then we have to pull it so
whenever you have a two convolutional 2D
um uh layer we have to bring this back
together and pull this into uh neural
network and then we're going to go ahead
and repeat this
uh so we're going to add another Network
here one of the cool things if you look
at this is that it as it comes in it
just kind of automatically assumes
you're going down to the next layer and
so we have another convolutional null
network uh 2D here's our Max pooling
again we're going to do that again Max
pooling uh and we're just going to
filter on down now one of the things
they did on this one is they changed the
kernel size they changed the number of
filters and so each one of these steps
kind of looks at the data a little bit
differently and that's kind of cool
because then you get a little added
filtering on there this is where you
start playing with the model you might
be looking at a convolutional no network
which is great for image
classifications um we get down to here
one of the things we see is flatten so
we add a we just flatten it remember
this is 150 by 150 by 3 well and
actually the pool size changes so it's
actually smaller than that flatten just
puts that into a 1D array uh so instead
of being you know a tensor of this
really complexity with the the pixels
and everything it's just flat and then
the
DSE is just another activation on there
um by default it is probably railu as
far as this
activation and then oh yeah here we go
in sequential they actually added the
activation as railu so this just because
this is sequential this activation is
attached to the dents uh and there's a
lot of different activation but Ru is
the most common one and then we also see
a soft Max uh soft Max is similar but it
has its own kind of variation and one of
the cool things you know what let me
bring this up because if we if you don't
know about these activations this
doesn't make
sense and I just did a quick Google
search on images of tensorflow
activations um I should probably look at
which website this is but this is the
output of the values uh so as your X as
it adds in all those uh weighted X
values going into the node it's going to
activate it a certain way and that's a
sigmoid activation and you can see it
goes between zero and one and has a nice
curve there this also shows the
derivatives um and if we come down the
seven popular activation functions
nonlinear activations there's a lot of
different options on this let me see if
I can find
the oop let me see we can find the
specific to
Ru so this is a leaky
railu and you can see instead of it just
being zero and then a value between uh
going up it has a little leaky there
otherwise your railu loses some noes
they just become inactive um but you can
see there's a lot of different options
here here's a good one right here with
the raayo you can see the railo function
on the upper on the upper left here and
then the Leaky rayu over here on the
right which is very commonly used
also one of the things I use with
processing um language is the S is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice uh funky curve that comes in
that um has a whole different meaning
and captures word use better again these
are very specific to domain and you can
spend a lot of time playing with
different models for our basic model uh
we'll stick to the ru and the softmax on
here and we'll go ah and run and build
this
model so now that we've had playing with
all these different models that we can
add in there uh we need to go ahead and
have a batch size on here uh
128 epics
10 this means that we're going to send
128 uh rows of data or flowers at a time
to be processed and the Epic 10 that's
how many times we're going to Loop
through all the data reduce um the
values and verbose verbose equals one
means that we're going to show what's
going on um value monitor what we're
monitoring we'll see that as we actually
train the model this is what's what's
going to come out of there if you set
the verbos equal to zero um you don't
have to watch it train the model
although it is kind of nice to actually
know what's going on
sometimes and since we're still working
on uh bringing the data in here's our
bat side here's our epics we need to go
ahead and create a data generator uh
this is our image data generator
and it has all the different settings in
here almost all of these are defaults uh
so if you're looking at this going oh my
gosh this is confusing most of the time
you can actually just ignore most of
this um vertical flip so you can
randomly flip pictures you can randomly
horizontally flip them um you can shift
the picture around this kind of helps
gives you multiple data off of them uh
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um um setup because we already have
a huge amount of data if you're short
data you can start flipping like a
horizontal picture and it will generate
it's like doubling your data almost um
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have um
5,000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias uh that is the downside
of that
and so we have our model comp model
compile and this you're going to see in
all the carass we're going to take this
model here we're going to take all this
information as far as how we want it to
go and we're going to compile it this
actually builds the model and so we're
going to run that and I want you to
notice uh learning
rate very important this is the default
001 um there's there you really don't
this is how slowly adjust to find the
right answer and the more data you have
you might actually make this a smaller
number um with larger with you have a
very small sample of data you might go
even larger than that and then we're
going to look at the loss categorically
categorical cross entropy most commonly
used and this is uh how how much it
improves the model is improving is what
this number means or yeah that's that's
important on there and then the accuracy
we want to know just how good our model
is on the
accuracy and then uh one of the cool
things to do is if you're in a group of
people who are studying the model if
you're in shareholders you don't want to
do this is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
um we'll call it history but we want to
do a model fit
generator
and so what this history is doing is
this is tracking what's going on as
while it fits the
model now there's a lot of new setups in
here where they just use fit and then
you put the generator in here um we're
going to leave it like this even though
the new default um is a little different
on that doesn't really matter it does
the same thing and we'll go ahead and
just run
that and you can see while it's running
right here uh we're going through the
epics we have one of 10 now we're going
through 6 of 25 here's our loss we're
printing that out so you can see how
it's improving and our accuracy the
accuracy gets better and better and this
is 6 out of 25 this is going to take a
couple minutes to process uh because we
are training 150 by 150 by 3 pixels
across uh six layers or eight layers
whatever it was that is a huge amount of
processing so this will take a few
minutes to process
this is when we talk about the hardware
and the problems that come up in data
science and why it's only now just
exploding being able to do neural
networks this is why this process takes
a long
time now you should have seen a jump on
the screen here because I did uh uh
pause the recorder to let this go ahead
and run all the way through its epics
let's go ahead and take a look and see
what these epics are and um if you set
the verbos to zero instead of one it
won't show what's going on in the behind
the scenes that is training it so we
look at this epic 10 epics we went
through all the data 10 times uh if I
remember correctly there's roughly a gig
of data there so that's a lot of data
the first thing you're going to notice
is the 270 seconds um that's how much
each of those epics took to run and so
if you divide 60 in there you roughly
get about 5 minutes worth of each epic
so if I have 10 epics that's 50 minutes
almost an hour of run
time that's a big deal when we talk
about processing uh in on this
particular computer um I actually have
what is it uh uh eight cores with 16
dedicated threads so it runs like a 16
core computer it alternates the threads
going in and it still takes it five
minutes for each one of these epics so
you start to see that if you have a lot
of data this is going to be a problem if
you have a number of models you want to
Fe find out how good the models are
doing and what model to use and so each
of those models could take all night to
run in fact I have a model I'm running
now that takes over uh takes about a day
and a half to test each model um it
takes four days to do with the whole
data uh so what I do is I actually take
a small piece of the
data test it out to find out uh get an
idea of of how the different setups are
going to do and then I increase that
size of the data and then increase it
again and I can just take that that
curve and kind of say okay if uh the
data is doing this then I need to add in
more dense layers or whatever uh so you
can do a small chunks of data then
figure out what it cost to do a large
set of data and what kind of model you
want the loss as we see here continues
to go down uh this is the error this is
how much error is in there it really
isn't a um userfriendly number other
than the more it Trends down the better
and so if you continue to see the loss
going down eventually get to the point
where it stops going down and it goes up
and down and kind of waivers a little
bit that point you know you've run too
many epics you're you're starting to get
a bias in there and it's not going to
give you a good model fit the accuracy
just turns this into something that uh
we can use and so the accuracy is what
percentage of guesses in this case is
categorical so this is the percentage of
guesses are correct um value loss is
similar you know it's a minus a value
loss and then you have the value
accuracy and you'll see the value accur
accuracy is pretty similar to the
accuracy um just rounds it off basically
and so a lot of times you come down here
and you go okay we're doing 0.
[Music]
5.67 and that is 70% accuracy or in this
case 68 Point uh 59% accuracy that's a
very usable number and it's very
important to have if you're identifying
uh flowers that's probably good enough
if you can get within a close distance
and knowing what flower you're you're
identifying uh if you're trying to
figure out whether someone's going to
die from a heart attack or not might
want to rethink it a little bit or rekey
how you're building your model so if I'm
working with a uh uh a group of um
clients um shareholders in a company or
something like that you don't really
want to show them this um you don't want
to show them hey you know this is what's
going on with the accuracy these are
just numbers and so we want to go and
put the finishing touches just like when
you are building a house and you put in
the frame and the trim on the house just
it's nice to have something a nice view
of what's going on and so we'll go ahead
and do a uh pie plot and we'll just plot
the history of the loss uh the history
of the value
loss over here um epics train and test
and so we're just going to compute these
this is really important uh and what I
want you to notice right here is when we
get to about oh five epics a little more
than five six epics you see a crossover
here and it start Crossing as far as the
um uh value loss and what's going on
here is you have the loss in your actual
model in your actual data and you had
the value loss where it's testing it
against the the test data the the data
wasn't used to program your model wasn't
used to train your model on and so when
we see this crossing over this is where
the bias is coming in this is becoming
overfitted and so when you put these two
together uh right around five and six
you start to see how it does this this
switch over here and that's really where
you need to stop right around five yeah
six um it's always hard to guess because
at this point the model is kind of a
black box uh see but you know that right
around here if you're saving your model
after each run you want to use the one
that's right around five epics because
that's the one that's going to have the
least amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here um I show somebody this kind of
model and I say this is where the
training and the testing comes in on
this model uh it just makes it easier to
see and people can understand what's
going
on so that completes our demo and you
can see we did what we were set out to
do we took our flowers and we're able to
classify them uh within about you 68 70%
accuracy whether it's going to be a
dollia sunflower Cherry block awesome
rows um a lot of other things you can do
with your output as far as a different
tables to see where the errors are
coming from and what problems are coming
up and we're going to take a look at
image classification using carass and
the basic setup we'll actually look at
two different demos on
here uh what's in it for you today what
is image
classification Intel image
classification data creating neural
networks with carass and the vgg16
model what is image
classification the process of image
classification refers to assigning
classes to an entire image images can be
classified based on different categories
like we it is a nighttime or daytime
shot what the image represents Etc you
can see here we have uh mountains
looking for mountains we'll actually be
doing some uh uh pictures of scenery and
stuff like that in deep learning we
perform image classification by using
neural networks to extract features from
images and classifies them based on
these
features and you can see here where it
says like what computer sees and this
says oh yeah we see mostly Forest maybe
a little bit of mountains because the
way the image is um and this is really
where one of the areas that neural
networks really shines um if you try to
run this stuff through uh more like a
linear regression model you'll still get
results uh but the results kind of miss
a lot of things as they as the neural
networks get better and better at what
they do with different tools we have out
there uh so Intel image classification
data the data being used is the Intel
image classification data set which
consists of images of six types of land
areas and so we have Forest building
glaciers and mountains sea and Street uh
and you can see here there's a couple of
the images out of there there as a setup
in the in the um uh Intel image
classification data that they
use and then we're going to go into
creating a neural networks with
carass the convolutional neural network
that we are creating from scratch looks
uh as showing
below you see here we have our input
layer
um they have a listed Max pulling uh so
you have as you're coming in with with
the input layer and this the input layer
is actually um before this but the first
layer that it's going to go into is
going to be a convolutional neural
network uh then you have a Max pooling
that pulls those the the convolutional
neural Network's returns uh in this case
they have two of those that is very
standard with convolutional neural
networks uh one of the ones that I was
looking at earlier that was standard
being used by um I want one of the
larger companies I can't remember which
one for doing a large amount of
identific ification had two
convolutional neural networks each with
their Max pooling and then about 17
dense layers after it we're not going to
do that heavy duty of a of a code but
we'll get you head in the right
direction and that gives you an idea of
what you're actually going to be looking
at when you look at the flattened part
and then the dense we're talking like 17
dense layers
afterwards uh I find that a lot of the
stuff I've been working on I end up
maxing it out right around nine dense
layers it really depends on what you
have going in and what you're working
with and the vgg16
model uh vgg16 is a pre-trained CNN
model which is used for image
classification it is trained on a large
varied data set and fine-tune to fit
image classification data sets with
ease and you can see down here we have
the input coming in uh the convolutional
neural network 1:1 1:2 and then pooling
and then we do 2: 1 2:2 two
convolutional neural network then
pooling 3 to2 and you can see there's
just this huge layering of convolutional
neural networks and in this case they
have five such layers going in and then
three dents going out or uh more now
when they took this setup this actually
won an award uh back in 2019 for this
particular
setup uh and it does it does really good
except that again um we only show the
three dense layers here and as you find
out depending on your data going in and
what you have set up uh that really
isn't enough on one of these setups and
I'm going to show you why we restricted
it because it does take up a lot of
processing power in some of these things
so let's go ahead and roll up our
sleeves and we're going to look at both
the setups we're going to start with the
um uh the first
classification um and then we'll go into
the vgg 16 and show you how that's set
up now I'm going to be using anaconda
and let me flip over to my an anonda so
you can see what that looks like now I'm
running in the Anaconda here uh you'll
see that I've set up a main python uh 38
I always put that in there this is where
I'm doing like most of my kind of
playing around uh this is done in Python
version 3.8 we're not going to dig too
much into versions uh at this point you
should already have carass installed on
there usually carass takes a number of
extra
steps and then our usual um uh setup is
the numpy the pandas uh your SK your s
kit which is going to be the sklearn
your caborn and I'll I'll show you those
in just a minute um and then I'm just
going to be in the Jupiter lab where
I've created a new um notebook in here
and let's flip on over there to my blank
notebook now I'm there's a couple cool
things to note in here is that um one I
use the the um Anaconda Jupiter notebook
setup because it keeps everything
separate uh except for carass uh carass
is actually running separately in the
back I believe it's a a c program uh
what's nice about that is that it
utilizes the multiprocessors on the
computer and I'll mention that just in a
little bit when we actually get down to
running the
code and when we look in here uh a
couple things to note is here's our uh
um oops I thought I grabbed the other
drawing thing uh but here's our numpy
and our pandas right here and our
operating system this is our s kit you
always import it as sklearn for the
classification report uh we're going to
be using well usually import like
Seaborn brings in all of your pip plot
Library
also kind of nice to throw that in there
I can't remember if we're actually using
caborn if they just the people in the
back just threw that together um and
then we have the sklearn shuffle for
shuffling data here's our map plot
library that the caborn is pretty much
built on um CV2 if you're not familiar
with that that is our image um module
for importing the image and then of
course we have our ttin or flow down
here which is what we're really working
with and then the last thing is just for
visual effect while we're running this
um if you're doing a demo and you're
working with uh the partners or the
shareholders uh this tqdm is really kind
of cool it's an extensible progress bar
for Python and I I'll show you that too
remember data science is not I mean you
know most of this code when I'm looking
through this code I'm not going to show
half of this stuff to the shareholders
or anybody I'm working with they don't
really care about pandas and all that we
do because we want to understand how it
works uh so we need to go ahead and
import those different um setup on there
and then the next thing is we're going
to go ahead and set up our
classes uh now we remember if we had
Mountain Street Glacier building sea and
Forest those were the different images
that we have coming
in and we're going to go ahead and just
do class name labels and we're going to
kind of match that class name of I for I
class name uh equals the class names so
our labels are going to match the names
up
here uh and then we have the number of
uh
classes and print the class names and
the labels and we'll go ahead and set
the image size this is important that we
resize everything because if you
remember with neural
networks they take one siiz data coming
in and so when you're working with
images you really want to make sure
they're all resized to the same uh setup
it might squish them it might stretch
them that generally does not cause a
problem in these uh and some of the
other tricks you can do with if you if
you need more data um and this is one
that's used regularly we're not going to
do it in here is you can also take these
images and not only resize them but you
can tilt them one way or the other crop
parts of them um so they process
slightly differently and it'll actually
increase your accuracy of some of these
predictions uh and so you can see here
we have Mountain equals Zer that's what
this class name label is Street equals 1
Glacier equals 2 buildings equals 3 C4
Forest equals
five now we did this as an enumerator so
each one is 0 through five uh a lot of
times we do this instead as um uh uh 0 1
010 1 so you have five outputs and each
one's a zero or a one coming out so the
next thing we really want to do is we
want to go ahead and load the data up
and just put a label in there loading
data just just so you know what we're
doing we going to put in the loading
data down here uh make sure it's well
labeled uh and we'll create a definition
for this and this this is all part of
your
preprocessing at this point you could
replace this with all kinds of different
things depending on what you're working
on and if you once you download you can
go download this data set uh send a note
to the simply learn team here in YouTube
um and they'll be happy to direct you in
the right direction and make sure you
get this path here um so you have the
right whatever wherever you saved it a
lot of times I'll just abbreviate the
path or put it as a sub thing and just
get rid of the directory um but again
double check your paths um we're going
to separate this into a segment for
training and a segment for testing and
that's actually how it is in the folder
let me just show you what that looks
like so when I have my uh lengthy path
here where I keep all my programming
simply learned this particular setup
we're working on image classification
and image classification clearly you
probably wouldn't have that lengthy of a
list and when we go in here uh you'll
see see sequence train sequence test
they've already split this up this is
what we're going to train the data in
and again you can see buildings Forest
Glacier Mountain SE Street uh and if we
double click let's go under Forest you
can see all these different Forest uh
images and and there's a lot of variety
here I mean we have wintertime we have
summertime um so it's kind of
interesting you know here's like a
Fallen Tree versus um a road going down
the middle that's really hard to train
and if you look at the
buildings A lot of these buildings
you're looking up a skyscraper we're
looking down the
setup here's some trees with one I want
to highlight this one it has trees in it
uh let me just open that up so you can
see it a little
closer the reason I want to highlight
this is I want you to think about this
we have trees growing is this the city
or a
forest um so this kind of imagery makes
it really hard for a class of fire and
if you start looking at these you'll see
a lot of these images do have trees and
other things in the foreground weird
angles really a hard thing for a
computer to sort out and figure out
whether it's going to be a forest or a
um
city and so in our loading of data uh
one we have to have the path of
directory we're going to come in here we
have our images and our labels so we're
going to load the images in one section
labels in another
um and if you look through here it just
goes through the different folders uh in
fact let me do this let
me there we go uh as we look at this
we're just going to Loop through the
three the six different folders that
have the different Landscapes and then
we're going to go through and pull each
file
out and each label uh so we set the
label we set the folder for file and
list uh here's our image path join the
paths this is all kind of n General
stuff um so I'm kind of skipping through
it really
quick and here's our image setup uh if
you remember we're talking about the
images we have our CV2 reader so it
reads the the image in uh it's going to
go ahead and take the image and convert
it to uh from blue green red to red
green green blue this is a CV2 thing um
almost all the time it Imports it and
instead of importing it as a standard
that's used just about everywhere it
Imports it with the BGR versus RGB um
RGB is pretty much a standard in here
you have to remember that was CV2 uh and
then we're going to go ahead and resize
it this is the important part right here
we've set a we've decided what the size
is and we want to make sure all the
images have the same size on
them and then we just take our images
and we're just going to impin the image
pin the label um and then the images
it's going to turn into a numpy array
this just makes it easier to process and
manipulate and then the labels is also a
numpy array and then we just return the
output append images and labels and we
return the output down
here so we've loaded these all into
memory uh we haven't talked to much
there'd be a different setup in there
because there is ways to feed the files
directly into your cross model um but we
want to go ahead and just load them all
it's
really for today's processing and that
what our computers can handle that's not
a big deal and then we go ahead and set
the uh train images train labels test
images test labels and that's going to
be returned in our output a pin and you
can see here we did um uh images and
labels set up in there and it just loads
them in there so we'll have these four
different categories let me just go
ahead and run
that uh so now we've gone ahead and
loaded everything on
there and then if you remember from
before uh we imported just go back up
there Shuffle here we go here's our SK
learn utilities import Shuffle and so we
want to take these labels and shuffle
them around a little bit um just mix
them up so it's not having the same if
you run the same process over and over
uh then you might run into some problems
on there
and just real quick let's go ahead and
do uh um a plot so we can just you know
we we've looked at them as far as from
outside of our code we pulled up the
files and I showed you what that was
going on we can goe and just display
them here too and I tell you when you're
working with different
people this should be highlighted right
here um this thing is like when I'm
working on code and I'm looking at this
data and I'm trying to figure out what
I'm doing I skip this process
the second I get into a meeting and I'm
showing what's going on to other people
I skip everything we just did so and go
right to here where we want to go ahead
and display some images and take a look
at
it and in this display um I've taken
them and I've resized the images to 20
by
20 that's pretty small uh so we're going
to lose just a massive amount of detail
and you can see here these nice
pixelated images um I might even just
stick with the folder showing them what
images we're
processing uh again this is you got to
be a little careful this maybe resizing
it was a bad idea um in fact let me try
it without resizing it and see what
happens oops so I took out the image
size and then we put this straight in
here one of the things again this is
um put the D there we go one of the
things again that we want to
know whenever we're working on these
things uh is the
CV2 there are so many different uh image
classification setups it's really a
powerful package when you're doing
images but you do need to switch it
around so that it works with the pi plot
and so make sure you take your numpy
array and change it to a u integer 8
format uh because it comes in as a float
otherwise you'll get some weird images
down there uh and so this is just
basically we split up our we've created
a plot we went ahead and did the plot 20
by 20 um or the plot figure size is 20
by 20 um and then we're doing 25 so a
5x5 subplot um nothing really going on
here too exciting but you can see here
where we get the images and really when
you're showing people what's going on
this is what they want to see uh so you
skip over all the code and you have your
meeting you say okay here's our images
of the
building um don't get caught up in how
much work you do get caught up in what
they want to see so if you want to work
in data science that's really important
to
know and this is where we're going to
start uh having fun uh here's our model
this is where it gets exciting when
you're digging into these models and you
have here uh let me
get there we
go when you have here if you look here
here's our convolutional neural network
uh
2D and uh 2D is an image you have two
different dimensions x y and even though
there's three colors it's still
considered 2D if you're running a video
you'd be convolutional neural network 3D
if you're doing a series going across um
a Time series it might be
1D and on these you need to go ahead and
have your convolutional n network if you
look here there's a lot of really cool
settings going on to dig into um we have
our input shape so everything's been set
to 150 by 150 uh and it has of course
three different color schemes in it
that's important to notice um
activation default is railu uh this is
small amounts of data being processed on
a bunch of little um neural
networks and right here is the 32 that's
how many many of these convolutional
null networks are being strung up on
here and then the
3X3 uh when it's doing its steps it's
actually looking at uh a little 3x3
Square on each image and so that's
what's going on here and and with
convolutional noral networks the window
floats across and adds up all these
numbers going across on this data and
then eventually it comes up with 30 in
this case 32 different feature options
uh that it's looking for and of course
you can change that 32 you can change
the 3X3 so you might have a larger setup
you know if you're going across
150 um by 150 that's a lot of steps so
we might run this as 15 by 15 uh there's
all kinds of different things you can do
here we're just putting this together
again that would be something you would
play with to find out which ones are
going to work better on this setup um
and there's a lot of play
involved that's really where it becomes
an art form is guessing at what that's
going to be the second part I mentioned
earlier and I I can only begin to
highlight this um when you get to these
dense layers one is the activation is a
Ru they use a railu and a softmax here
um it's a whole whole uh setup just
explaining why these are different um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these you can build your own custom
activations depending on what you're
doing a lot of different things go into
these activations uh there are two or
three major thoughts on these
activations and Ru and softmax are uh
well Ru uh you're really looking at just
the number you're adding all the numbers
together and you're looking at ukian
geometry um ax plus
bx2 plus
cx3 plus a bias
with soft Max this belongs to the party
of um it's activated or it's not except
it's they call it softmax because when
you get to the to zero instead of it
just being zero uh it's actually
slightly a little bit less than zero so
that when it trains it doesn't get lost
um there's a whole series of these
activations another activation is the
tangent um where it just drops off and
you have like a very narrow area where
you have from minus1 to one or
exponential which is 0 to one so there's
a lot of different ways to do the
activation again we can do that'd be a
whole separate lesson on here we're
looking at the convolutional neural
network um and we're doing the two pools
this is so common you'll see two two
convolutional n networks stacked on top
of each other each with its own Max pull
underneath and let's go ahead and run
that so we built our model there and
then we need to go ahead and
um compile the model so let's go ahead
and do
that uh we going to use the atom uh
Optimizer the bigger the data the atom
fits better on there there's some other
Optimizer but I think Adam is a default
um I don't really play with the
optimizer too much that's like the if
once you get a model that works really
good you might try some different
optimizers uh but adom is usually the
most and then we're looking at a loss
pretty standard we want to minimize our
Lo we want
to maximize the loss of error and then
we're going to look at accuracy um
everybody likes say accuracy I'm going
to tell you right
now I start talking to people like okay
what's what's the loss on this and that
and as a data science yeah I want to
know how the LW what what's going on
with that and we'll show you why in a
minute but everybody wants to see
accuracy we want to know how accurate
this is uh and then we're going to run
the fit and I wanted to do this just so
I can show you even though we're in uh
python setup in here where Jupiter
notebook is using only a single
processor I'm going to bring over my
little CPU Tool uh this is eight cores
on 16 dedicated threats so it shows up
as 16
processors and actually I got to run
this and then move it over so we're
going to run this and hopefully it
doesn't destroy my mic
uh and as it comes in you can see it's
starting to do go through the epics we
said I set it for five epics and then
this is really nice because carass uses
all the different uh threads available
so it does a really good job of doing
that uh this is going to take a while if
you look at here it's um ETA 2 minutes
and 25 seconds 24 seconds so this is
roughly two and a half minutes per epic
uh and we're doing five epics so this is
going to be done in roughly 15 minutes I
don't know about you but I don't think
you want to sit here for 15 minutes
watching The Green bars go across so
we'll go ahead and let that run and
there we go uh there was our 15 minutes
it's actually less than that uh because
I did when I went in here realized that
uh uh where was
it here we go here's our model compile
here's our model FL uh fit
and here's our epics uh so I did four
epics so a little bit better more like
10 to 11 minutes instead of uh uh doing
the full uh 15 and when we look at this
here's our model we did we talked about
the compiler uh here's our history we're
going to um history equals the model fit
we'll go into that in just a
minute and we're looking at is we have
our epics um here's our validation split
so as we train it
uh we're weighing the accuracy versus
you kind of pull some data off to the
side uh while you're training it and the
reason we do that is that um you don't
want to overfit and we'll look at that
chart in just a
minute uh here's batch
size this is just how many images you're
sending through at a time the larger the
batch it actually increases the
processing speed um and there's reasons
to go up or down on the batch size
because of the U the the smaller the
batch there's a certain point where um
you get too large of a batch and it's
trying to fit everything at once uh so I
128 is kind of big U depends on the
computer you're on what it can
handle and then of course we have our
train images and our train labels going
in telling it what we're going to train
on and then we look at our four epics
here uh here's our accuracy we want the
accuracy to go up and we get all the way
up to 83 or
83% now this is actual percentage based
pretty much and we can see over here our
loss we want our loss to go down really
fluctuates uh 55 1.2
7748 uh so we have a lot of things going
on there let's go ahead and graph
those
turn that off and our our team in the
back did a wonderful job of putting
together um this basic plot setup um
here's our subplot coming in we're going
to be looking at um uh from the history
we're going to send at the accuracy and
the value accuracy labels and setup on
there um and we're going to also look at
loss and value loss so you can see what
this looks like what's really
interesting about this setup and let me
just go ahead and show you because uh
without actually seeing the plots it
doesn't make a whole lot of sense uh
it's just basic plotting of uh of the
data using the piip plot library and I
want you to look at this this is really
interesting um when I ran this the first
time I had very different
results um and they they Vari greatly
and you can see here our accuracy
continues to
climb um and there's a crossover
here put it in here right here is our
crossover and I point that out because
as we get to the right of that crossover
where our accuracy U and we're like oh
yeah I got 8% we're starting to get an
overfit here that's what this this
switch over means um as our value um as
a training set versus a value um
accuracy stays the same and so that this
is the one we're actually really want to
be a Ware of and where it
crosses is kind of where you want to
stop at um and we can see that also with
the train loss versus the value loss
right here we did one Epic and look how
it just flat lines right there with our
loss so really one Epic is probably
enough and you're going to say wow okay
0.8% um certainly if I was working with
the shareholders um telling them that it
has an 80% accuracy isn't quite true and
and we'll look at that a little deeper
it really comes out here that the
accuracy of our actual values is closer
to 0 41% right here um even after
running it this number of times and so
you really want to stop right here at
that crossover one Epic would have been
enough um so the data is a little
overfitted on this when we do four
epics and uh oops there we are
okay my drawing won't go away um let me
see if I can get there we
go uh for some reason I've killed my
drawing ability on my
recorder all right took a couple extra
clicks uh so let's go ahead and take a
look at our actual test loss um so you
see where a cross is over that's where
I'm looking at that's where we start
overfitting the model
and this is where if uh we were going to
go back and continually upgrade the
model we would start taking a look at
the images and start rotating them uh we
might start playing with the
convolutional neural network instead of
doing the 3X3 window um we might expand
that or you know find different things
that might make a big difference as far
as the way it processes these things um
so let's go ahead and take a click at
our uh our test loss now remember we had
our training data now we're going to
look at our test images and our test
labels for our test loss here and this
is just model evaluate uh just like we
did fit up here where was it um one more
model fit with our training data going
in now we're going to evaluate it on the
and and this data has not been touched
yet so this model's never seen this data
this is on uh completely new information
as far as the model is concerned of
course we already know what it is from
the labels we
have
and this is what I was talking about
here's the actual accuracy right here
048 uh or
4847 so this 49% of the Time guesses
what the image
is uh and I mean really that's a bottom
dollar uh does this work for what you're
needing does 49% work do we need to
upgrade the model more um in some cases
this might be uh
oh what was I doing I was working on uh
stock
evaluations and we were looking at what
stocks were the top
performers well if I get that 50%
correct on top
performers uh I'm good with that um
that's actually pretty good for stock
evaluation in fact the number I had for
stock was more like uh um 30 something
percent as far as being a top performer
stock much harder to predict uh but at
that point you're like well I'm you'll
make money off of that so again this
number right here depends a lot on the
domain you're working
on then we want to go ahead and bring
this home a little bit more uh as far as
looking at the different setup in here
and one of the uh from sklearn if you
remember actually let's go back to the
top uh we had the classification report
and this came in from our sklearn or py
hit setup and that's right here you can
see it right here on the
um see there we go uh classification
report right here uh sklearn metrics
import classification report that's what
we're going to look at
next a lot of this stuff uh depends on
who you're working with so when we start
looking at um
Precision you know this is like for each
value I can't remember what 111 was
probably mountains so if 44% is not good
enough if if you're doing like um you're
in the medical department and you're
doing cancer is it is this cancerous or
not and I'm only 44% accurate not a good
deal you know I would not go with that
um so it depends on what you're working
with on the different labels and what
they're used for Facebook you know 44%
I'm guessing the right person I would
hope it does a little bit better than
that um but here's our main accuracy
this is what almost everybody looks at
they say oh 48% that's what's important
um again it depends on what domain
you're in and what you're working
with and now we're going to do the same
model oh somehow I got my there it goes
I thought I was going to get stuck in
there again uh this time we're going to
be using the vgg 16 and remember this
one is uh all those layers going into it
so it's basically a bunch of
convolutional n networks getting smaller
and smaller on here uh and so we need to
go ahead and um import all our different
stuff from carass uh we're importing the
main one is the V g16 setup on there
just aim that there we go
um there's kind of a pre-processing
images um applications pre-process input
this is all part of the VG g16 setup on
there uh and once we have all those we
need to go ahead and create our
model and we're just going to create a
vgg16 model in here um inputs model
inputs outputs model inputs I'm not
going to spend as much time as I did on
the other one uh we're going to go
through it really quickly one of the
first things I would do is if you
remember in carass you can trate treat a
model like you would a
layer um and so at this point I would
probably add a lot lot of dense layers
on after the vgg16 model and create a
new model with all those things in there
and we'll go ahead and uh run this uh
because here's our model coming in and
our setup and it'll take it just a
moment to compile that hey what's funny
about this is I'm I'm waiting for it to
download the um package since I prean
this um it takes it a couple minutes to
download the vgg16 model into here um
and so we want to go ahead and train
features for the model we're going to
predict the we're going to predict the
train images and we're going to test
features on the predict test images on
here and then I told you I was going to
create another model too and the people
in the back uh did not disappoint me
they went ahead and did just that and
this is really an important part um this
is worth stopping for I told you I was
going to go through this really quick so
here's our
uh we H we have our model
2 um coming in and we we've created a
model up here with the vgg16 model
equals model inputs model inputs and so
we have our
vgg16 this has already been
pre-programmed uh and then we come down
here and I want you to notice on this um
right here layer model two layers minus
4 to One X layer X um we're basically
taking this model and we're adding stuff
on to it and so uh we've taken we've
just basically duplicated this model we
could have done the same thing by using
model up here as a layer um we could
have had the input go to this model and
then have that go down here so we've
added on this whole setup this whole
block of code from 13 to 17 has been
added on to our vgg16 model and we have
a new model uh with the lay layer input
and X down here let's go ahead and run
that and compile it and that was a lot
to go through right there uh when you're
building these models this is the part
that gets so
complicated that you get stuck playing
in and yet it's so fun uh it's like a
puzzle how can I Loop these models
together and in this case you can see
right here that the layers uh we're just
copying layers over and adding each
layer in um this is one way to build a
new model and we'll go ahead and run
that like I said the other way is you
can actually use the model as a layer I
had a little trouble playing with it uh
sometimes when you're using the Straight
model
over you run into issues
um it seems like it's going to work and
then you mess up on uh the input and the
output layers there's all kinds of
things that come
up let's go ahead and do the new model
we're going to compile it uh again
here's our metrics accuracy sparse
categorical loss uh pretty
straightforward just like we did before
you got to compile the
model and just like before we're going
to take our create a history uh the
history is going to be uh new model fit
train
128 and uh just like before if you
remember when we started running this
stuff we're going to have to go ahead
and it's going to light up our uh setup
on here and this is going to take a
little bit to get us all set up uh it's
not going to just happen in in a couple
minutes so let me go aad and pause the
video and run it and then we'll talk
about what
happened okay now when I ran that these
actually took about six minutes each um
so it's a good thing I put it on hold we
did four epics uh actually had to stopic
is at 10 and switch it to four because I
didn't want to wait an
hour and you can see here our
accuracy um and our loss numbers going
down and just at a
glance it actually performed if you if
you look at the
accuracy
2658 um so our accuracy is going down or
you know
26% um 34% 35% and you can see here at
some point it just kind of kicks the
bucket again this is
overfitting that's always an issue when
you're running on uh programming these
different neural
networks and then we're going to go
ahead and plot the accuracy um history
we built that nice little sub routine up
above so we might as well use it and you
can see it right
here um there's that cross over
again and if you look at this look how
the how the um uh the red shifts up how
the uh our loss functions and everything
crosses over we're overfitting after one
Epic um we're
clearly not helping the problem we're
doing better um we're just going to it's
just going to Baseline this one actually
shows what the training versus the
loss um value loss maybe second epic so
on here we're now talking more between
the first and the SE second epic and and
that also shows kind of here so
somewhere in here it starts
overfitting and right about now you
should be saying uhoh uh something went
wrong there I thought that um when we
went up here and ran this look at this
we have the accuracy up here is hitting
that
48% and we're down here
um you look at the score down here that
looks closer to 20% not nearly anywhere
in the ballpark of what we're looking
for and we'll go ahead and run it
through the uh the actual test features
here and and there it is um we actually
run this on the Unseen data and
everything uh8 or
18% um I don't know about you but I I
wouldn't want you know at 18% this did a
lot worse than the other one I thought
this is supposed to be the super model
the model that beats all models the
vgg16 that won the awards and everything
well the reason is is that um one we're
not pre-processing the data uh so it
needs to be more there needs to be more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it so you can create a lot more data
to go through here um and that would
actually greatly change the outcome on
here and then we went up here we only
added a couple dense layers uh we added
a couple convolutional neural
networks this huge pre-trained setup is
looking for a lot more information
coming in as far as how it's going to
train and so uh this is one of those
things where I thought it would have
done better and I had to go back and
research it and look at it and say why
didn't this work why am I getting only
uh 18% here instead of 44% or better and
that would be wise it doesn't have
enough training data coming in uh and
again you can make your own training
data so it's not that we'll have a
shortage of data it's that that some of
that has to be switched around and moved
around a little bit and this is
interesting right here too if you look
at the
Precision we're getting it on number two
and yet we had zero on everything else
so for some reason is not seeing
uh the different variables in here so
that'd be something else to look in and
try to find track down um and that
probably has to do with the input but
you can see right here we have a really
good solid Point 48 up here uh and
that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in um where's the setup
on there and that is the art of data
science right there is finding out which
models work better and why now let's
talk about our first deep learning
project we have image classification
using the cifar 10 data set now this
data set was created by the Canadian
Institute for advanced research the
cifar 10 data set contains 6,32 cross 32
color images in 10 different classes the
10 different classes are airplanes cars
Birds cats deer and others which you can
see on the screen there are 6,000 images
of each class now there are 50,000
training images and 10,000 test images
you can build a convolutional neural
network model using the Caris library to
classify each image into a category so
it is more of an image recognition kind
of a project that is recommended for
beginners who are new to deep learning
the next project in our list is brain
tumor detection such projects are
heavily used in the healthcare
Industries for detecting diseases before
they occur brain tumors are the
consequence of abnormal growths and
uncontrolled cell division in the brain
they can lead to death if they are not
detected early and accurately brain
tumors can be classified into two types
benign and
malignant deep learning can help
Radiologists in tumor Diagnostics
without invasive measures a deep
learning algorithm that has achieved
significant results in image
segmentation and classification is the
convolutional neural network it can be
used to detect a tumor through brain
using magnetic resonance imaging or MRI
image pixels are First Fed as input to
the CNS
soft Max's fully connected layers are
used to classify the images the accuracy
of the convolutional neural network can
be obtained with the radial basis
function classifier next we have Anna
chatbot now this chatbot is the world's
first open source conversation platform
which comes with a graphical chart flow
designer and chat simulator it's
supported channels of web chat Android
iOS and Facebook Messenger the Ana chat
board is available to answer your
questions 24 hours a day 365 5 days a
year Anna is free for personal and
commercial use with the Anna Studio
server simulator and SDK which is
software development kit your
development time is cut from days to
hours using Anna you can create chat
Bots to suit your needs chat Bots play
an important role in customer support
for an e-commerce website you can see a
realtime order and sipping updates
personalized product recommendations and
targeted offers within the conversation
similarly automobile Brands showrooms
and service centers can use a chatboard
for lead generation scheduling test
drives and roadside
assistance you can check the GitHub
repository that's on the screen to know
more about working on this project the
next project we have in deep learning is
image captioning now image captioning is
the process of generating textual
description of an image it uses both
methods from computer vision to
understand the content of the image and
a language model from the field of
natural language processing to turn the
understanding of the image into words in
the right order Microsoft has built its
own caption bot where you can upload an
image or the URL of an image and it will
display the textual description of the
image on the screen you can see we have
the picture of Alon musk and the caption
bot has generated a description that
says I think it's Alon musk wearing a
shoot and a tie and he seems it ends
with an
emoticon another such application that
suggests a perfect caption and best ha
tags for a picture is caption AI
automatic image caption generation
software can be built using recurring
neural networks and long shortterm
memory networks or
lstms now we have image colorization as
our next project image colorization has
seen significant advancements using deep
learning image colorization is the
process of taking an input of a
grayscale image and then producing an
output of a colored image or a colorized
image colorization is a highly
undetermined problem that that requires
mapping a real valued luminance image to
a three-dimensional colored value one
that has not a unique solution chromag
Gan is an example of a picture
colorization Model A generative network
is framed in an adversarial model that
learns to colorize by incorporating
perceptual and semantic understanding of
color and class distributions the model
is trained using a fully supervised
strategy if you want to implement
chroman check the GitHub link that's on
the screen below next next in our list
of projects we have open nmt machine
translation open nmt is an open source
ecosystem for neural machine translation
and neural sequence learning it started
in December 2016 by the Harvard NLP
group anran the project has since been
used in several research and Industry
applications neural machine translation
or nmt is a new methodology for machine
translation that has led to significant
improvements particularly in terms of
human evaluation compared to rule-based
and statistical machine translation
systems open and empty provides
implementations in two popular deep
learning Frameworks P torch and
tensorflow please feel free to refer to
the below GitHub link to learn more
about neural machine
translation now we have music generation
using deep learning it is possible for a
machine to learn the notes structures
and patterns of Music and start
producing music automatically music 21
is a python toolkit used for computer
rated musicology
it allows us to teach the fundamentals
of Music Theory generate music examples
and study music the toolkit provides a
simple interface to acquire the music
notation of mid files which stands for
musical instrument digital interface
using webet architecture and long
shortterm memory networks you can
generate music without human
intervention Amper music mubert and Juke
deck produce smart music powered by Deep
learning
algorithms then we have Alpha go
now Alpha go was created by Google Deep
Mind and is the first computer program
to defeat a professional human go player
Alpha go algorithm uses a combination of
machine learning and Tre search
techniques combined with extensive
training both from human and computer
play it uses Monte carot tree search
Guided by a value Network and a policy
Network both implemented using deep
neural network technology alphao was
initially trained to mimic human play by
attempting to match the moves of expert
players from recorded historical games
it was done using a database of around
30 million moves once it had reached a
certain degree of proficiency by being
set to play large number of games
against other instances of itself using
reinforcement learning to improve its
play next in our list of projects we
have deep dream deep dream is a computer
vision program which uses a
convolutional neural network to find and
enhance patterns in images via
algorithmic paradia it then creates a
dream like helicogenic
appearance in the overprocessed images
deep dream is an experiment that
visualizes the patterns learned by a
neural
network similar to when a child watches
clouds and interprets and tries to
interpret random shapes deep dream over
interprets and enhances the patterns it
sees in an image do check the given
GitHub link to install dependencies
listed in the notebook and play with the
code locally then we have deep voice
deep voice was developed by BYU which is
an AI system that can clone an
individual's voice the train model takes
just 3 seconds to replicate the output
of a person's voice deep voice 3 is a
fully convolutional attention based
neural text to speech system that
converts text to spectrograms or other
acostic parameters to be used with an
audio waveform synthesis method below
you can find the GitHub link for py
torch implementation of convolutional
networks based text to speech synthesis
models
now we have IBM Watson IBM Watson helps
run machine learning models anywhere
across any Cloud using IBM Watson
machine learning you can build
analytical models and neural networks
trained with your own data that you can
deploy for use in
applications with its open extensible
model operation Watson machine learning
helps businesses simplify and harness AI
at scale across any Cloud it is used in
healthcare teaching assistant chatbot as
well as CL for weather forecasting and
finally in our list of deep learning
projects we have the YOLO realtime
object detection you only look once or
YOLO is a state-of-the-art real-time
object detection system it frames object
detection in images as a regression
problem to spatially separated bounding
boxes and Associated class
probabilities using this approach a
single neural network divides the image
into regions and predicts bounding boxes
and probabilities for each region the
new neural network predicts bounding
boxes and class probabilities directly
from full images in one evaluation the
base YOLO model processes images in real
time at 45 frames per second you can use
the Coco data set and tensorflow library
to train and test the model to learn
more about YOLO object detection check
the GitHub link that's shown on the
screen below then accelerate your career
in Ai and ml with a comprehensive
post-graduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below keep learning interview questions
and we're going to go from the very
basics of neural networks and deep
learning into some of the more commonly
used models so you can have an
understanding of what kind of questions
are going to come up and what you need
to know in interview questions we'll
start with a very general concept of
what is deep learning this is where we
take large volumes of data in this case
on cats and dogs or whatever allow lot
of times you use um a training setup to
train your model remember it's kind of
like a magic Black Box going on there
then we use that to extract features or
extract information and in this case
classify the image of a cat and a dog so
the primary takeaway when we're talking
about deep learning is it learns from
large volumes of structured and even
unstructured data and uses complex
algorithms to train neural network it
also performs complex operations to
extract hidden patterns and features and
if we're going to discuss deep learning
in this very
simplified overview and we also have to
go over what is a neural network this is
a common image you'll see of a drawing
of a forward propagation neural network
and it's it's a human brain inspired
system which replicate the way humans
learn so this is inspired how our own
neurons in our brain fire but at a much
simplified level obviously it's not
ready to take over the human uh
population and and be our leader yet not
for many years it's very much in its
infant stage but it's inspired by how
our brains work um and they use a lot of
other inspirations you can study brains
of moths and other animals that they've
used to figure out how to improve these
neural networks the most common one
consists of three layers of network and
this is generally how you view these
networks is you have an input you have a
hidden layer and an output and the
neural network is uh broken up into many
pieces but when we focus just on the
neural network it's always on the hidden
layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN G Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we studi the Deep learning and the
neural network framework let's start
with what is a a multi-layer patron or
MLP a lot of time is it referred to and
you'll see these abbreviations I'll be
honest I have to write them down on a
piece of paper and go through them cuz I
never remember what they all mean even
though I play with them all the time
what is a multi-layer patron well if you
look at the image on the right it's very
similar to what we just looked at you
have your input layer your hidden layer
and your output layer and that's exactly
what this is it has the same structure
of a single layer Perron with one or
more hidden layers except the input
layer each node in the other layers uses
a nonlinear activation function what
that means is your input layers your
data coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer Perron can classify only
linear separable classes with binary
output 01 but the MLP can classify
nonlinear classes so let's break this
down just a little bit the multi-layer
Perron with an input layer and a hid
layer and an output layer as you see
that it comes in there it has adds up
all the numbers and weights depending on
how your setup is that then goes to the
next layer that then goes to the next
hidden layer if you have multiple hidden
layers and finally to the output layer
the back propagation takes the error
that it sees so whatever the output is
it says hey this has an error to it it's
wrong and then sends that error
backwards from where it came from and
there's a lot of different functions
used to uh train this based on that
error and how that error goes backwards
in the notes uh so forward is you get
your answers back where it is for
training you see this every day even my
uh Google pixel phone has this it they
train the neural network which takes a
lot more data to train than it does to
use and then they load up that neural
network into in this case I have a pixel
2 which actually has a built-in neural
network for processing pictures and so
it's just the forward propagation I use
when it processes my phone photos but
when they were training it you use the
back propagation to train it with the
errors they had we'll be coming back to
different models that are used for right
now though multi-layer Perron MLP put
that down as your vocabulary word and of
course back propagation what is data
normalization and why do we need it this
is so important we spend so much time in
normalizing our data and getting our
data clean and setting it up uh so we
talk about data there's a pre-processing
step to standardize the data so whatever
we have coming in we don't want it to be
a uh you know 1 gbyte file here a 2
gbyte picture here and a 3 kilobyte text
there even as a human I can't process
those all in the same group I have to
reformat them in some way that Loops
them together so that a standardized
format we use this uh data normalization
in pre-processing to reduce and
eliminate data redundancy a lot of times
the data comes in and you end up with
two of the same images or um uh the same
information in different formats then we
want to rescale values to fit into a
particular range for achieving better
convergence what this means is with most
neural networks they form a bias we've
seen this in recently in attacks on
neural networks where they light up one
pixel or one piece of the view and it
skews the whole answer so suddenly um
because one pixel is really bright uh it
doesn't know what to do well when we
start rescaling it we put all the values
between say minus one and one and we
change them and refit them to those
values it helps get rid of that bias
helps fix for some of those problems and
then finally we restructure the data and
improve the Integrity we want to make
sure that we're not missing values um or
we don't have partial data coming in one
way to look at this is uh bad data in
bad data out so you want clean data in
and you want good answers coming out one
of the most basic models used is a
boltzman machine so let's address what
is a boltzman machine and if you know we
just did the MLP multi-layer Petron so
now we're going to come into almost a
simplified version of that and in this
we have our visible input layer and we
have our hidden layer the boltman
machines are almost always shallow
they're usually just two layer neural
Nets that make stochastic decisions
whether a neuron should be on or off
true false yes no first layer is the
visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes of
the same layer are connected hence it is
also known as restricted boltzman
machine now that we've covered a basic
MLP or multi-layer Perron and we've gone
over the boltzman machine also known as
the restricted boltzman machine let's
talk a little bit about activation
formulas and this is a huge topic that
can get really complicated but it also
is automated so it's very simple so you
have both a complicated and a simple at
the same time so what is the role of
activation functions in a neural network
activation function decides whether a
neuron should be fired or not that's the
most basic one and that actually changes
a little bit because it's either whether
fired or not in this case activation
function or what value should come out
when it's fired but in these models
we're looking at just the bolts men
restricted layers so this is what causes
them to fire either they don't or they
do it's a yes or no true false All or
Nothing it accepts the weighted sum of
the inputs the bias as input to any
activation function so whatever our
activation function is it needs to have
the sum of the weights times the input
so each put if you remember on that
model and let's just go back to that
model real quick and then you always
have to add a bias and you can look at
the bias if you remember from your ukian
geometry you draw a straight line
formula for that line has a y-coordinate
at the end it's always um CX plus M or
something like that where m is where it
crosses the Y coordinates if you're
doing a straight line with these weights
it's very similar but a lot of times we
just add it in as its own weight we take
it as a node of a one value coming in in
and then we compute its new weight and
that's how we compute that bias just
like we compute all the other weights
coming in the node which gets fires
depends on the Y value and then we have
a step function and the step function
this is where remember I said it's going
to get complicated and simple all at the
same time we have a lot of different
step functions we have the sigmoid
function we have just a standard step
function we have the um Ru it's
pronounced like Ray the ray of from the
Sun and L like a name so Rayo function
and we have the tangent H fun function
and if you look at these they all have
something similar they all either force
it to be um one value or the other they
force it to be in the case of the first
three is zero or one and in the last one
it's either a minus one or one and you
can easily convert that to a z one yes
no true false and on this one of the
most common ones is the step function
itself because there is no middle value
there is no um uh discrepancy that says
well I'm not quite sure but as you get
into different models probably the most
commonly used used to be the sigmoid was
most commonly used but I see the ru used
more often really depending on what
you're doing you just have to play with
these and find out which one works best
depending on the data in your output the
reason to have a non Z1 answer or
something kind of in the middle is when
you're looking at this and it's coming
out you can actually process that middle
ground as part of the answer into
another neural network so it might be
that the railu function says hey this is
only a 6 not a one and and uh even
though the one is what's going into the
next neural network or the next hidden
layer as an input the 6 value might also
be going in there to let you know hey
this is not a straight up one or
straight up zero someplace in the middle
this is a little uncertain what's coming
out here so it's a very powerful tool
but in the basic neural network you
usually just use the step function it's
yes or no let's take a um a big step
back and take a kind of an overview the
next function is what is a cost function
that we're going to cover this is so
important because this is your end
result that you're going to do over and
over again and use to decide whether the
model is working or not whether you need
to try a different step function whether
you need to try a different activation
whether you need to try a fully
different model used uh so what is the
cost function cost function is a measure
to evaluate how good your model's
performance is it is also referred as
loss or error used to compute the error
of the output layer during back
propagation there's our back propagation
where we're training our model that's
one of our key words mean squared error
is an example of a popular cost function
and so here we have the cost function
Cal half of y- Y predicted um and then
you square that so the first thing is um
you know real quick if you haven't done
statistics this is not a percentage it's
not a percentage of how accurate it is
it's just a measurement of the error and
we take that error for training it and
we push that error backwards through the
neural network and we use that through
the different training functions
depending on what model you're using to
train the neural network so when you
deploy the network you're usually done
training it because it takes a lot of
computational force to train it um this
is a very simple model and so you deploy
the trained one uh but we want to know
how your error is and so how do we do
that well you split your data part of
your data is for training and part of
your data is for testing and then we can
also test the error on there so it's
very important and then we're going to
go one more step on this we got to look
at both the local and the global setup
it might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests and the error test as
far as your loss you don't you want to
make sure that you're in a closed
environment when you do initial testing
but you also want to open that up and
make sure you follow up with the testing
on the larger scale of data because it
will change it might not fit the larger
scale there might be something in there
in the way you brought the data in
specifically or the data group you used
or um any of those could cause an error
so it's very important to remember that
we're looking at both the local and the
global context of our error and just one
other side note on a lot of the newer
models of neural networks by comparing
the error we get on the data our
training data with a portion of the test
data we can actually figure out how good
the model is whether it's overfitted or
not we'll go into that a little bit more
as we go into some of the different
models so we have our output we're able
to um figure out the error on it based
on the Square means usually although
there's other uh functions used so we
want to talk about what is gradient
descent another vocabulary word gradient
descent is an optimation algorithm to
minimize the cost function uh or to
minimize the error aim is to find the
local or Global Minima of a function
determine the direction the model should
take to reduce the error so as we're
looking at this we have our uh squared
error that we just figured out the co
based on the cost function it says how
bad is my model fitting the data I just
put through it and then we want to
reduce that error so how how do you
figure out what direction to do that in
well you could be that you're looking at
just that line of that line of data
coming in so that would be a local
Minima we want to know the error of that
particular setup coming in and then you
have your Global your Global Minima we
want to minimize it based on the overall
data we're putting through it and with
this we can figure out the global
minimum cost we want to take all those
local minimum cost of each piece of data
coming in and figure out the global one
how are we going to adjust this model to
fit all the data we don't want it to be
biased just on three or four lines of
data coming in we want it to kind of
extrapolate a general answer for all the
data coming in at this of course uh we
mentioned it briefly about back
propagation this is where really comes
in handy is training our model neural
network technique to minimize the cost
function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our uh
predicted y coming out and then we have
since it's a training set we already
know the answer and the answer comes
back and based on case of the square
means was one of the functions we looked
at uh one of the activation functions
based on cost function that cost
function then depending on what you
choose for your back propagation method
and there's a number of them will change
the weights it will change the weight
going to each of one of those nodes in
the hidden layer and then based upon the
error that's still being carried back
it'll change the weights going to the
next hidden layer and then it computes
an error level on that and sends that
back up and you're going to say well if
it computes the error into the first
hidden layer and fixes it why would it
stop there well remember we don't want
to create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation naral
networks everything goes forwards goes
left to right uh but this let's take a
little deter and let's see what is the
difference between a feed forward neural
network and a recurrent neural network
now this is in the function not when
we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new uh vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones this has a big highlight nowadays
is the CNN a convolutional neural
network tensor flow the one put out by
good Google is probably most known for
their CNN where the information goes
forward it uh first takes a picture
splits it apart goes through the
individual pixels on the picture so it
picks up a different reading then
calculates based on that goes into a
regular feedforward neural network and
then gives your categorization on there
now we're not covering the CNN today but
we do have a video out that you can look
up on YouTube put out by simply learn
the convolutional neural network
wonderful tutorial check that out and
learn a lot more about the conv ution
neural network but you do need to know
that the CNN is a forward propagation
neural network only so it's only moving
in One Direction so we will look at a
recurrent neural network signals travel
in both directions making it a looped
Network consid the current input along
with the previous received inputs for
generating the output of a layer has the
ability to memorize past data due to its
internal memory and you can see they
have a nice uh image here we have our um
input and for some reason they always do
the recurrent neural network work um in
Reverse from bottom up in the images
kind of a standard although I'm not sure
why your X goes into your hidden layer
and your hidden layer the answer for
part of the answer from that it
generates feeds back into the hidden
layer so now you have an input of both X
and part of the Hidden layer and then
that feeds into your output now if we go
back to the forward let me just go back
a slide and we're looking at uh our
forward propagation Network one of the
tricks you can do to use just a forward
propagation network is if you're a what
they call a Time sequence that's a good
uh term to remember or a Time series
meaning that it's sequential data each
term comes after the other you can trick
this by creating your input nodes as
with the history so if you know that uh
you have values one five and seven going
in and you know what the output is from
one what those outputs are you can
expand the input to include the history
input that's one of the ways to trick a
forward propagation Network into looking
at that but when you deal with a
recurrent neural Network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so it no
longer be positive so when you're
looking at a sentence knowing the order
of the word is as important as the
meaning of the words you can't just
count how many good words there are
versus bad words to get positive
sentiment you know have to know what
they're addressing and there's lots of
other different uses uh kids are playing
football or soccer as we call it in the
US RN can help you cap an image So based
on previous information coming in it
refeeds that back in and you have a
image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or sale of
products can be solved using an RNN and
this is a really good example you have
whatever your stocks were doing earlier
this month will have a huge effect on
what they're doing today if you're
investing so having an RNN Model A
recurrent neural network feeding into
itself what was happening previously
allows it to take that model and program
in that whole series without having to
put in the whole a month at a time of
data you can only put in one day at a
time but if you keep them in order it
will look back and say oh this because
of what happened yesterday I need some
information from that and I'm going to
use that to help predict today and so on
and so on we're going to go back to our
activation functions remember I told you
railu is one of the most common
functions used uh so let's talk a little
bit more about Ru and also softmax
softmax is an activation function that
generates the output between 0er and one
it divides each output such that the
total sum of the outputs is equal to one
it is often used in the output layers
softmax L of the N equals e to L on the
N over the absolute value of e to the L
so what does this function mean I mean
what is actually going on here so we
have our output nodes and our output
nodes are giving us uh let's say they
gave us
1.2.9 and point4 as a human being I look
at that and I say well the greatest
value is 1.2 so whatever category that
is if you have three different
categories maybe you're not just doing
if it's a cat or it's a dog or um oh
let's say it's a cow we had cats and
dogs earlier why the cats and dogs are
hanging out with a cow I don't know but
we have a value and it might say 1.2 is
a cat 0.9 is the dog and point4 is a cow
uh for some reason it SS that there's a
chance of it being any one of these
three items and that's how it comes out
of the output layer well as a human I
can look at 1.2 and say this is
definitely what it is it's definitely a
cat or whatever it is uh maybe it's
looking at different kinds of cars might
be a better whether it's a car truck or
motorcycle maybe that'd be a better
example well from a computer standpoint
that may be a little confusing because
they're just numbers waving at us and so
with the soft Max we want all those
numbers to always add up to one so when
I add three numbers together I want the
final output to be one on there and so
it goes through this formula changes
each of these numbers in this case it
changes them to0 46 34 and .20 they all
add up to one and that's a lot easier to
register cuz it's very set it's a set
output it's never going to be more than
one it's never going to be less than
zero and so you can see here that
there's probably a pretty high chance
that it's the first one so you're a
human being we have no problem knowing
that but this output can then also go
into say another input so be an
automated car that's picking up images
and it says that image in front of us is
probably a big truck we should deal with
it like it's a big truck it's probably
not a motorcycle um or whatever those
categories are that's the softmax part
of it but now we have the railu well
what where's the railu coming from well
the railu is what's generating the 1.2
and the 0.9 and the 04 and so if you
remember our railu stands for rectified
linear unit and is the most widely used
activation function we looked at a
number of different activation functions
including tangent H the step function I
remember I said the step function is
really used if that's what your actual
output is because then you know it's a
zero or one but the railu if you have
that as your output you now have a
discrepancy in there and if that's going
into another neural network or another
process having that discrepancy is
really important and it gives an output
of x if x is positive and zero otherwise
so it says my x value is going to be
somewhere between zero or one and then
the uh usually unless it's really
uncertain the output's usually a one or
zero and then you have that little piece
of uncertainty there that you can send
forward to another Network or you can
look at to know that there's uncertainty
involved and is often used in the hidden
layers this is what's coming out of the
Hidden layers into the output layer
usually or as we reference the
convolution neural network the CNN you'd
have to go to another video to review
the railu is the most common used for
convolutional part of that Network it
has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the ru works really good
for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layer is going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have uh numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between zero one so it's
very uh standardized so we have our soft
Max we looked at that let's go back one
we looked at that here where it
transforms in numbers and then we have
our Ru function which takes the
information and the summation and puts
it between a zero and a one where it's
either clearly a zero or depending on
how confident our model is it'll go
between the zero and one value what are
hyperparameters oh this is a great
interview question hyperparameters when
you are doing neural networks this is
what you're playing with most of the
time once You' gotten the data formatted
correctly a hyperparameter is a
parameter whose value is set before the
learning process begins determines how a
network is trained and the structure of
the network this includes things like
the number of hidden units how many
hidden layers are you going to have and
how many nodes in each layer learning
rate learning rate is usually multiplied
once you figured out the error and how
much you want to change the weights we
talked about or I mentioned it early
just briefly you don't want to just make
a huge change otherwise you're going to
have a biased model so you only take
little incremental changes and that's
what the learning rate is is a small
incremental changes epics how many times
are you going to go through all the data
in your training set so one Epic is one
trip through all the data and there's a
lot of other things depending on which
model you're working with and which
programming script you're working with
like the python SK learn package will
have it slightly different than say
Google's tensorflow package which will
be a little bit different than the spark
machine learning package so these are
just some examples of the
hyperparameters and so you see in here
we have a nice image of our data coming
in and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so then we send it back and we
change our hyperparameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set too low or too high
we have a nice couple graphs here we
have one over here says the learning
rate set too low you can see that it
slowly Works its way down the curve and
on the right you can see a learning rate
set too high it's just bouncing back and
forth when your learning rate is too low
that's what we studied at two slides ear
asked what the learning rate was
training of the model will progress very
slowly as we are making very tiny
updates to the weights we'll take many
updates before reaching the minimum
point so I just mentioned epic going
through all the data you might have to
go through all the data a thousand times
instead of 500 times for it to train
learning rate too high causes
undesirable Divergent Behavior to the
loss function due to drastic updates and
weights at times it may fail to converge
or even diverge so if you have your
learning rate set too high and it's
training too quickly maybe you'll get
lucky and it trains after one epic run
but a lot of times it might never be
able to train because the weights are
changing too fast they they flip back
and forth too easy and you see down here
we've introduced uh two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjust Ed and it's
minimized the error diverg means that
the data is so chaotic that it can never
manage to to train to that data the data
is just too chaotic for it to train so
we have two new words there converge and
diverge are important to know also what
is Dropout and batch normalization
Dropout is a technique of dropping out
hidden and visible units of a network
randomly to prevent overfitting of data
it doubles the number of iterations
needed to converge the network so here
we have our standard neural network and
then after applying Dropout now it
doesn't mean we actually delete the node
the node is still there and we're still
going to use that node what it means is
that we're only going to work with a few
of the notes um a lot of times I think
the most common one right now used is
20% uh so you'll drop out 20% of the
nodes when you do your training you
reverse propagate your data and then
you'll randomly pick another 20 nodes
the next time you go through an epic
data training so each time you go
through one Epic you will randomly pick
20 of those nodes not to not to mess
with and this allows for less
overfitting of the data so by randomly
doing this you create some I guess it
just kind of pull some nodes off to the
side and says we're going to handle the
data later on so we don't overfit batch
normalization is a technique to improve
the performance and stability of neural
network the idea is to normalize the
inputs in every layer so that they have
mean output and activation of zero and
standard deviation of one this question
covers a lot of different things which
is great it's a great uh interview
question because it pulls in that you
have to understand what the mean value
is so a mean output activation of zero
that means our average activation is
zero so when you normalize it remember
usually we're going between minus one
and one on a lot of these it's a very
standard setup so you have to be very
aware that this is your mean output
activation of zero and then we have our
standard deviation of one so we want to
keep our error down to just a one value
the benefits of this doing a batch
normalization is it provides
regularization it trains fast F Higher
Learning rates and weights are easier to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the entire data set
it takes time to converge because the
volume of data is huge and weights
update slowly so you can look at the
batches a lot of times if you're using
big data batch the data in but you still
go through a full epic you still go
through all the data on there so bash
gradient descent means you're going to
use it to fit all the data and look for
a convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise in the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with nonlinear models that have
more flexibility when learning a Target
function an example of this would be um
if you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box- like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to use it in the
real world it leaves out a lot in start
start is not very functional outside of
your small area your small laboratory
data coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model has a
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you because it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy kfold cross
validation having a validation data set
to evaluate the model so we do the
resampling we're randomly going to be
picking out data and we'll run it a few
times to see how that works depending on
our random data and how we U sample the
data to generate our model and then we
want to go ahead and validate the data
set by having our training data and then
keeping some data on the side uh testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other uh
machine learning tools initializing all
weights randomly here the weights are
assigned randomly by initializing them
very close to zero it gives better
accuracy to the model since every neuron
performs different computations and here
we have the weights are set randomly we
have our input layer the hidden layers
and the output layer and W equals NP
random random n layer size L layer size
L minus one this is the most commonly
used is to randomly generate your
weights what are the different layers in
CNN convolutional neural network first
is the convolutional layer that performs
a convolutional operation we have our
other video out if you want to explore
that more so go into detail exactly how
the C the convolutional layer works in
the CNN as far as creating a number of
smaller uh picture windows that go over
the data uh the second step is has a
railu layer railu brings nonlinearity to
the network and converts all the
negative pixels to zero output is
rectified feature map so it goes into a
mapping feature there pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map so we have all our railu
layer which is pulling all these little
Maps out of our convolutional layer is
taking that picture and little creating
little tiny neural networks to look at
different parts of the picture uh then
we need to pull it together and then
finally the fully connected layer so we
flatten our pooling layer out and we
have a fully connected layer recognizes
and classifies the objects in the image
and that's actually your forward
propagation reverse propagation training
model usually I mean there's number
different models out there of course
what is pooling in CNN and how does it
work pooling used to reduce the spatial
dimensions of a CNN performs down
sampling operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide um it's important to
know that you have if you can see here
they have a rectified feature map and so
each one of those colors like the yellow
color that might be one of the a smaller
little neural net work using the ru
you'll look at it'll just kind of um go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces um and then you have another one
that's also looking at features and it
has a 2785 each one of those is a map so
it might be the first one might be a map
looking for cat ears and the second one
looking for human eyes when it does this
you then have this rectified feature map
looking at these different features and
the max pooling with a 2X two filters
and a stride of two stride means instead
of skipping every pixel you're going to
go every two pixels you take the maximum
values and you can see over here when we
look at a pulled feature map one of the
feature says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pulled
feature map you can see the process here
is we have our stepping we look for the
max value and then we create a pulled
feature map of the maxed values how does
a lstm network work that's long
shortterm memory so the first thing to
know is that an lstms are a special kind
of recurrent neural network capable of
learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm as a much more complicated
feedback and you can see here we have um
the hidden layer of T minus one and
hidden layer that's what the H stands
for hidden layer of T and the formula is
going in as we can see here we have the
hidden layers we have T minus one and
then h of T where T stands for time so
this is a series remember working with
series and we want to remember the past
and you can see you have your your input
of T and that might be a frame in a
video as a frame comes in they usually
use in this one the tangent H activation
formula but you also see that it goes
through a couple other formulas the
Omega formula and so when it combines
these that thing goes into the next
layer your next hidden layer that then
goes into the data that's submitted to
the next input so you have your X of t +
one so when you have that coming in then
you have your H value that's coming
forward from the last process and
depending on how many of these um Omega
structures you put in there depends on
how long-term the memory gets so it's
important to remember this is more for
your long-term recurrent neural networks
the three steps in an lstm step one
decides what to forget and what to
remember step two selectively update
cell State values So based on what we
want to remember and forget we want to
update those cell values and then
decides what part of the current state
make it to the output so now we have to
also have an output on there what are
Vanishing and exploding gradients this
is a great question that affects all our
neural networks while training an RNN
your slope can become either too small
or too large and and this makes the
training difficult when the slope is too
small the problem is known as Vanishing
gradient so our slope we have our change
in X and our change in y when the slope
decreases gradually to a very small
value sometimes negative and makes
training difficult when the slope tends
to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you can see a nice graph
of that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between Epic bat and iteration and deep
learning epic an epic represents one
iteration over the entire data set so
it's everything you're going to go ahead
and put into that training model batch
we cannot pass the entire data set into
the neural network at once so we divide
the data set into a number of batches
and then iteration if we have 10,000
images as data and a batch size of 200
then the Epic should run 10,000 times
over 200 so that means we have our total
number over the 200 equals 50 iteration
so in each epic we're running over all
the data set we're going to have 50
iterations and each of those iterations
includes a batch of 200 images in this
case why tensorflow is the most
preferred library in deep learning uh
well first tensorflow provides both C++
and python apis that makes it easier to
work on has a faster compilation time
than other deep learning libraries like
carass and torch tensorflow supports
both CPUs and gpus Computing devices so
right now tensor flow is at the top of
the market because it's so easy to use
for both programmer side and for
Hardware side and for the speed of
getting something up and running what do
you mean by tensor and tensor flow
tensor is a mathematical object
represented as arrays of higher
Dimension these arrays of data with
different dimensions and ranks that are
fed as input to the neural network are
called tensors and you can see here we
have a tensor of Dimensions five comma
four so it's a two-dimensional tensor
coming in um you can look at an image
like this that each one of those pi
is a different value if it's a black and
white so it might be zero and ones and
then each one represents a black and
white image in a color photo you might
um either find a different value system
or you might have a tensor value that
has the XY coordinates as we see here
plus the colors so you might have three
more different dimensions for the three
different images the red the blue and
the yellow coming in and even as you go
from one layer or one tensor to the next
these layers might change we might
flatten them might bring in numerous in
the case P of the convergence neural
network we have all those smaller
different mappings of features that come
in so each one of those layers coming
through is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensor flow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf. constant command
example a equal tf. constant 2.0 TF
float 32 so it's a tensor float value of
32 b equals TF constant 3.0 print AB if
we did a print of ab we'd have um tf.
constant and then of course uh B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use tf.
variable command and initialize them
before running the graph in session
example wal TF variable. 3 dtype TF
float 32 or Bal a TF variable minus 3
comma dtype float 32 placeholders
placeholders allow us to feed data to a
tensorflow model from outside model it
permits a value to be assigned later to
define a placeholder we use TF
placeholder command example AAL TF
placeholder b equal a * 2 with the TF
session as sess result equals session
run B comma feed dictionary equals a 3.0
print result uh so we have a nice
example there of a placeholder session a
session is run to evaluate the nodes
this is called as the tensor flow
runtime so for example you have AAL TF
constant 2.0 Bal TF constant 4.0 Cal a
plus b and at this point you'd go ahead
and create a session equals TF session
and then you could evaluate the tensor C
print session run C that would input c
as an input into your session what do
you understand by a computational graph
everything in tenser flow is based on
creating a computational graph it has a
network of nodes where each node
performs an operation nodes represent
mathematical operation and edges
represent tensors since data flows in a
form of a graph it is also called a data
flow graph and we have a nice visual of
this graph or graphic image of a
computational graph and you can see here
we have our input nodes our add multiply
nodes and our multiply node at the end
and then we have the edges where the
data flows so we have from a going to C
A going to D you can see we have a two
flowing a four flowing explain
generative adversarial Network along
with an example suppose there's a wine
shop that purchases wine from dealers
which they will resell later so we have
dealer going to the wine or shop owner
that then sells it for a profit but
there are some malactor dealers who sell
fake wine in this case the shop owner
should be able to distinguish between
fake and authentic wine the forger will
try to different techniques to sell fake
wine and make sure certain techniques go
past the shop owner's check so here's
our forger fake wine shop owner the shop
owner would probably get some feedback
from the wine experts that some of the
wine is not original the owner would
have to improve how he determines
whether a wine is fake or authentic goal
of Forger create wines that are
indistinguishable from the authentic
ones goal of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we'll refer it
to as a noise Vector coming in where we
have our forger who's going to generate
fake wine and then we have our real
authentic wine and of course our shop
owner has to figure out whether it's
real or fake the generator is a CNN that
keeps producing images that are closer
in appearance to the real images while
the discriminator tries to determine the
difference between real and fake images
the ultimate aim is to make the
discriminator learn to identify real and
fake images what is an autoencoder the
network is trained to reconstruct its
inputs it is a neural network that has
three layers here the input neurons are
equal to the output neuron the Network's
Target outside is same as the input it
uses dimensionality reduction to
restructure the input input image comes
in we have our Latin space
representation and then it goes back out
reconstru constructing the image it
works by compressing the input to a
Lattin Spas representation and then
reconstructing the output from this
representation what is bagging and
boosting bagging and boosting are
Ensemble techniques where the idea is to
train multiple models using the same
learning algorithm and then take a call
so we have in here where we're bagging
we take a data set and we split it we're
going to have our training data and our
test data very standard thing to do then
we're going to randomly select data into
the bags and train your model separately
so we might have bag one model one bag
two model two bag three model 3 and so
on in boosting the infanist is to select
the data points which give wrong output
in order to improve the accuracy so in
boosting we have our data set again we
split it to test data and train data and
we'll take a bag one and we'll train the
model data points with wrong predictions
then go into Bag two and we then train
that model and repeat and with that we
have come to the end of deep learning
complete course I hope you found it
valuable and entertaining please ask any
questions about the top topics covered
in this video in the comment box our
experts will assist you in addressing
your problems thanks for watching this
video stay safe and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video
video subscribe to the simply learn
YouTube channel and click here to watch
similar videos to nerd up and get
certified click
here