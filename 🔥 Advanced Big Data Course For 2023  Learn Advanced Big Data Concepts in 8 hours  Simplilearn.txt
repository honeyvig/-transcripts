hello everyone and welcome to the
advanced Big Data full course by simply
learn in this comprehensive program we
will dive deep into the concepts of Big
Data equipping you with the Knowledge
and Skills to harass the immense
potential hidden within vast data sets
in today's interconnected World
organizations generate extraordinary
data every second from social media
interactions to online transactions to
sensor readings and machine logs the
sheer volume velocity and variety of
data have created New Opportunities and
challenges for businesses across various
Industries Big Data offers many career
prospects including roles like data
analyst data scientists data engineer bi
analyst database administrator and
architect now these positions require
diverse skills including data analysis
machine learning programming and
database management according to
Glassdoor the average yearly salary for
a big data engineer in the United States
is around 120 000 per year in India the
average annual survey for this role is
around 80 lakhs now whether you are an
aspect data analyst a seasoned ID
professional or a business leader
looking to Leverage The Power of Big
Data the data engineering postgraduate
program by simply learn in collaboration
with Purdue University and IBM please
tell her to meet your needs this program
offers an exceptional opportunity for
professional growth and exposure with
the focus on practical learning and in
alignment with industry leading
certifications from AWS and Azure now
this program equips participants with
essential skills required to excel in
this field of data engineering by
enrolling in this applied learning
program individuals can gain master or
crucial data engineering techniques and
Technologies enroll today and take the
next step towards becoming a proficient
Big Data professional check out the
course Link in the description box below
for more details this course will
explore big data and its challenges
exploring use cases and tools for
processing and analyzing massive data
sets the course covers the hardware
ecosystem big data analytics using
python data science Concepts processing
analytics and three V's of Big Data
additionally we will provide Hadoop
interview questions by end of this
course but before we begin let's take a
minute to hear from our Learners who
have experienced huge success in their
careers the learning from Simply learn
has been really amazing and extremely
helpful
whoever is watching this video I
recommend you go out to Simply learns
website and check out their courses
they have brilliant instructors who are
industry-wide solution Architects and
directors working at the biggest firms
right now
hi my name is Aditya and I'm working as
a consultant in atos in the field of AI
machine learning and RPA I did my
engineering from Maharaja surajmal
Institute of Technology I majored in
electronics and communication
engineering I passed out in 2013. and on
campus I got placed into a company
called nucleus software so I started
working in software engineering team the
team was involved into application
development using Oracle forms SQL and
database languages I wanted to explore
better Technologies which were there in
the Market at that time I got to know
that machine learning and Big Data they
revolve around SQL so if you are a SQL
person you are very much comfortable
with learning new machine learning
techniques and database Technologies
like big data Hadoop so that's when I
started looking out for the training
institutes and then my relative referred
me once I decided to go for simply learn
I had one course in my mind which was
Big Data only but when I approach simply
learn they offered me a bundle of
courses in one master program which
included not only big data but also
machine learning and python Basics are
Basics data science with python and some
Java fundamentals as well so that was
really good to have what I liked most
about the training was the context of
the training as well as the instructor
to student engagement
after the training I got an opportunity
to learn more about these Technologies
and once I was done learning I attended
a lot of interviews so they were in some
big mnc's and the interviewers were real
life big data Architects and data
science experts and they were really
happy to see the knowledge which I have
so simply learn definitely set up a base
for me on that and once I started giving
interviews eventually I landed up in MNC
in Chennai where I'm working as a data
science expert and a consultant simply
learns Big Data master program helped me
to get a new job with a very good salary
hike a very big thanks to Simply learn
for making me who I am I feel really
fortunate to be associated with these
Technologies now I can lead different
teams and drive different business
transformation programs using my skill
sector I really wanted to upskill
because in the market and in the IIT
industry upscaling is really important
what is Big Data big data is extremely
large or complex set of data and it's so
large that it's difficult to process it
using traditional database and software
techniques
every day we are creating approximately
2.5 quintillion bytes of data
so where is this huge amount of data
getting generated from
earlier we had mobile phones with the
functionality of calling and text
messages or clicking some pictures maybe
but with the new technologies like
smartphones we have a lot of
applications for music Sports social
media like Facebook Twitter LinkedIn and
many more
also data is getting generated when we
shop online
so why does it need attention
as the data is growing companies are
capturing the data that streams into
their businesses
they can apply analytics and get
significant value from it with better
speed and efficiency
companies are leveraging the benefits of
big data by analyzing the patterns and
Trends and predicting something useful
out of it
for example companies like Amazon and
Netflix use big data to improve customer
experience
as we see here from the statistics shown
by 2020 1.7 megabytes of data will be
created every second for each human
this needs immediate attention because
this data can't be just thrown away
it's going to give profit to the
businesses Big Data challenges
data is not just about the volume of
data it poses other challenges as well
like velocity and variety
as a volume 40 zettabytes of data will
be created by 2020. this huge volume of
data is either human generated like from
social media YouTube or can be machine
generated like through sensors and
personal health trackers
and can also be generated with
organizations like credit card details
commercial transactions and medical
records
another challenge is velocity
which data is coming into the system the
data needs to be processed with faster
speed
and then there is variety of data
data is not only structured but
unstructured and semi-structured data
like images videos and tweets
so how are Enterprises using this big
data today
let us see big data popular use cases
Internet of Things
these are numerous ways in which
analytics can be applied to Internet of
Things
for example sensors are used to collect
data that can be analyzed to achieve
actionable insights tracking customer or
product movement Etc
many Enterprises are creating a
dashboard application that provides a
360 degree view of the customers that
pulls data from a variety of sources
analyzes it and presents it to customer
service
so that allows them to gather the rich
insights about businesses
Big Data popular use cases are related
information security and data warehouse
optimizations
Big Data tools are being used to remove
some of the burdens from the data
warehouses
even the healthcare industry is looking
for patterns and treatment that lead to
the best outcomes for patients
the main challenge of big data is
storing and processing the data at a
specified time span
the traditional approach is not
efficient in doing that
so Hadoop technology and various Big
Data tools have emerged to solve the
challenges faced in the Big Data
environment
so there are a lot of Big Data tools and
all of them help the user in some or
another way in Saving Time money and
uncovering business insights
these can be divided into the following
categories like data storage and
management for example the nosql
databases such as mongodb Cassandra
neo4j and hbase or popular nice nosql
databases
the talent Hadoop Microsoft HD insight
and zookeeper are popular for data
storage and management tools
next broad category is data cleaning
data needs to be cleaned up and well
structured examples of such tools which
help in defining and reshaping the data
into usable data sets are Microsoft
Excel and open refine
data mining is a process of discovery
insights within a database
some of the popular tools used for data
mining are teradata and rapidminer
data visualization tools are a useful
way of conveying the complex Data
Insights in a pictorial way that is easy
to understand
for example Tableau and IBM Watson
analytics and plotly are the common
tools for data reporting power bi tools
are used data ingestion is the process
of getting the data into Hadoop forward
which can be done using scoop Flume or
storm
data analysis requires asking questions
and finding the answers and data
the popular tools used for data analysis
are Hive Pig mapreduce and Spark
data acquisition is also used for
acquiring the data for which scoop Flume
or storm tools are quite popular
the popular Big Data tools offer a lot
of advantages which can be summarized as
follows
they provide the analysts with Advanced
analytics algorithm and models they help
the user to run on Big Data platforms
such as Hadoop or any high performance
analytics systems
they help the user to work not only with
structured data but unstructured and
semi-structured data coming from
multiple sources
and it's quite easy to visualize the
analyze data in a form that helps in
conveying the complex Data Insights in a
pictorial way which is easy to
understand by users
Big Data tools help you to integrate
with other Technologies very easily
thank you so much for listening to the
video
now whether you are an aspen data
analyst a seasoned ID professional or a
business leader looking to Leverage The
Power of Big Data the data engineering
postgraduate program by simply learn in
collaboration with Purdue University and
IBM please tell her to meet your needs
this program offers an exceptional
opportunity for professional growth and
exposure with the focus on practical
learning and in alignment with industry
leading certifications from AWS and
Azure now this program equips
participants with essential skills
required to excel in this field of data
engineering by enrolling in the supplied
learning program individuals can gain
Master crucial data engineering
techniques and Technologies enroll today
and take the next step towards becoming
a proficient Big Data professional check
out the course Link in the description
box below for more details so let's we
took a look at the the master node and
the you know the name node and the
secondary name node let's take a look at
the cluster architecture of our Hadoop
file system the hdfs cluster
architecture so we have our name node
and it stores some metadata and we have
our block location station so we have
our FS image cluster edit log and then
we have the backup FS image and edit log
and then we have so you have your rack
we have our switch on top remember I was
talking about the switch that's the most
common thing to go in the rack is the
switches and underneath the rack you
have your different data nodes you have
your data node one two three four five
maybe you have 10 15 on this rack you
can stack them pretty high nowadays uh
used to be you only get about 10 servers
on there but now you see racks that
contain a lot more and then you have
multiple racks so we're not talking
about just one rack we also have you
know rack two rack three four five six
and so on until you have Rack in so if
you had a hundred data nodes we would be
looking at 10 racks of 10 data nodes
each and that is literally 10 commodity
server computers hardware and we have a
core switch which maintains the network
bandwidth and connects the name node to
the data nodes so just like each rack
has a switch that connects all your
nodes on the rack you now have core
switches that connect all the racks
together and these also Connect into our
name node setup so now we can look up
your FS image and your edit log and pull
that information your metadata out so
we've looked at the architecture from
the name node coming down and you have
your metadata your Block locations this
then sorts it out you have your core
switches which connect everything all
your different racks and then each
individual rack has their own switches
which connect all the different nodes
and to the core switches so now let's
talk about the actual data blocks what's
actually sitting on those commodity
machines and so the Hadoop file system
splits massive files into small chunks
these chunks are known as data blocks
each file in the Hadoop file system is
stored as a data block and we have a
nice picture here where it looks like a
Lego if you ever played with the Legos
as a kid it's a good example we just
stack that data right on top of each
other but each block has to be the same
symmetry has to be the same size so that
it can track it easily and the default
size of one data block is usually 128
megabytes now you can go in and change
that this standard is pretty solid as
far as most data is concerned when we're
loading up huge amounts of data and
there's certainly reasons to change it
128 megabytes is pretty standard block
so why 128 megabytes if the block size
is smaller then there will be too many
data blocks along with lots of metadata
which will create overhead so that's why
you don't really want to go smaller on
these data blocks unless you have a very
certain kind of data similarly if the
block size is very large then the
processing time for each block increases
then as I pointed out earlier each block
is the same size just like your Lego
blocks are all the same but the last
block can be the same size or less so
you might only be storing 100 megabytes
in the last block and you can think of
this as if you had a terabyte of data
that you're storing on here it's not
going to be exactly divided into 128
megabytes we just store all of it 128
megabytes except for the last one which
could have anywhere between 1 and 128
megabytes depending on how evenly your
data is divided now let's look into how
files are stored in the Hadoop file
system so we have a file
let's see it's 520 megabytes we have
block a so we take 128 megabytes out of
the 520 and we store it in block a and
then we have Block B again we're taking
128 megabytes out of our 520 storing it
there and so on Block C block D and then
block e we only have eight megabytes
left so when you add up 128 plus 128
plus 128 plus 128. you only get 512 and
so at last eight megabytes goes into its
own block the final block uses only the
remaining space for storage data node
failure and replication and this is
really where Hadoop shines this is what
makes it this is why you can use it with
commodity computers this is why you can
have multiple racks and have something
go down all the data blocks are stored
in various data nodes you take each
block you store it at 128 megabytes and
then we're going to put it on different
nodes so here's our block a Block B
Block C from our last example and we
have node one node 2 node three node
four node 5 node six and so each one of
these represents a a different computer
it literally splits the data up into
different machines so what happens if
node 5 crashes well that's a big deal I
mean we might not even have just node
five you might have a whole rack go down
and if you're a company that's building
your whole business off of that you're
going to lose a lot of money so what
does happen when node 5 crashes or the
first rack goes down the data stored in
node 5 will be unavailable as there is
no copy stored elsewhere in this
particular image so the Hadoop file
system overcomes the issue of data node
failure by creating copies of the data
this is known as the replication method
and you can see here we have our six
nodes here's our block a but instead of
storing it just on the first machine
we're actually going to store it on the
second and fourth nodes so now it's
spread across three different computers
and in this if these are on a rack one
of these is always on a different rack
so you might have two copies on the same
rack but you never have all three on the
same rack and you always have you never
have more than two copies on one node
there's no reason to have more than one
copy per node and you can see we do the
same thing with Block B Block C is then
also spread out across the different
machines and same with block D and
blocky node 5 crashes will the data
blocks b d and e be lost well in this
example no because we have backups of
all three of those on different machines
the blocks have their copies and the
other nodes due to which the data is not
lost even if the node 5 crashes and
again because they're also stored on
different racks even if the whole rack
goes down you are still up and live with
your Hadoop file system the default
replication factor is three and total
we'll have three copies of each data
block now that can be changed for
different reasons or purposes but you
got to remember when you're looking at a
data center this is all in one huge room
these switches are connecting all these
servers so they can Shuffle the data
back and forth really quick and that is
very important when you're dealing with
big data and you can see here each block
is by default replicated three times
that's the standard there is very rare
occasions to do four and there there's
even fewer reasons to do two blocks I've
only seen four used once and it was
because they had two data centers and so
each data center kept two different
copies rack awareness in the Hadoop file
system rack is a collection of 30 to 40
data nodes rack awareness is a concept
that helps to decide where the replica
of the data block should be stored so
here we have rack one we have our data
node one to four remember I was saying
they used to be you only put 10 machines
and now then it went to 20 now it's 30
to 40. so you can have a rack with 40
servers on it then we have rack 2 and
rack 3 and we put block one on there
replicas a block a cannot be in the same
rack and so it'll put the replicas onto
a different rack and notice that these
are actually these two are on the same
rack but you'll never have all three
stored on the same Rack in case the
whole rack goes down and replicas of
block a are created in rack two and they
actually do they do by default create
the replicas onto the same rack and that
has to do with the data exchange and
maximizing in your processing time and
then we have of course our Block B and
it's replicated onto rack 3 and Block C
which will then replicate on Direct one
and so on for all of your data all the
way up to block D or whatever how much
every data you have on there hdfs
architecture so let's look at where the
architecture is a bigger picture we
looked at the name node and we store
some metadata names replicas home food
data three so it has all your different
info your metadata stored on there and
then we have our data nodes and you can
see our data nodes are each on different
racks with our different machines and we
have our name node and you're going to
see we have a heartbeat or pulse here
and a lot of times one of the things
that confuses people sometimes in class
is they talk about nodes versus machines
so you could have a data node that's a
Hadoop data node and you could also have
a spark node on there Sparks a different
architecture and these are each Damons
that are running on these computers
that's why you refer to them as nodes
and not just always as servers and
machines so even though I use them
interchangeable be aware that these
nodes you can even have virtual machines
if you're testing something out it
doesn't make sense to have 10 virtual
nodes on one machine and deploy it
because you might as well just run your
code on the machine and so we have our
heartbeat going on here and the
heartbeat is a signal that data nodes
continuously send to the name nodes this
signal shows the status of the data Note
so there's a continual pulse going up
and saying hey I'm here I'm ready for
whatever instructions or data you want
to send me and you can see here we've
divided up into rack one and rack two
and our different data nodes and it also
have the replications we talked about
how to replicate data and replicates it
in three different locations and then we
have a client machine and the client
first requests the name node to read the
data now if you're not familiar with the
client machine the client is you the
programmer the client is you've logged
in external to this Hadoop file system
and you're sending it instructions and
so the client whatever instructions or
script you're sending is first request a
name node read the data the name node
allows a client to read the requested
data from the data nodes the data is
read from the data nodes and sent to the
client and so you can see here that
basically the the name node connects the
client up and says here here's a data
stream and now you have the query that
you've sent out returning the data you
asked for and then of course it goes and
finishes it and says oh metadata
operations and it goes in there and
finalizes your request the other thing
the name node does is you're sending
your information because the client
sends the information in there is your
block operations so your block
operations are performs creation of data
so you're going to create new files and
folders you're going to delete the
folders and also it covers the
replication of the folders which goes on
in the background and so we can see here
that we have a nice full picture you can
see where the client machine comes in it
cues and metadata the metadata goes into
it stores the metadata and then it goes
into block operations and maybe you're
sending the data to the Hadoop file
system maybe you're querying maybe
you're using it to delete if you're
sending data in there it then does the
replication on there it goes back so you
have your data client which is writing
the data into the data node and of
course replicating it and that's all
part of the block operations and so
let's talk a little bit about read
mechanisms in the Hadoop file system the
Hadoop file system read mechanism we
have our Hadoop file system client
that's you on your computer and the
client jvm on the client node and so we
have our client jvm or the Java virtual
machine that is going through and then
your client node and we're zooming in on
the read mechanism so we're looking at
this picture here as you can guess your
client is reading the data and I also
have another client down here writing
data we're going to look a little closer
at that and so we have our name node up
here we have our racks of your data
nodes and your racks of computers down
here and so our client the first thing
it does is it opens a connection up with
the distributed file system to the hdfs
and it goes hey can I get the Block
locations and so it goes by using the
RPC remote procedure call it gives it's
those locations and the name node first
checks if the client is authorized to
access the requested file and if yes it
then provides a block location and a
token to the client which is shown to
the slave for authentication so here's
the name node it tells a client hey
here's the token the client's going to
come in and get this information from
you and it tells the client oh hey
here's where the information is so this
is what it's telling your script you
sent to query your data whether you're
writing your script in one of the many
setups that you have available through
the Hadoop file system or a connection
through your code and so you have your
client machine at this point then reads
so your FS data input stream comes
through and you have and you can see
right here we did one and two which is
verify who you are and give you all the
information you need then three you're
going to read it from the input stream
and then the input stream is going to
grab it from the different nodes where
it's at and it'll Supply the tokens to
those machines saying hey this client
needs this data here's a token for that
we're good to go let me have the data
the client will show the authentication
token to the data nodes for the read
process to begin so after reaching the
end of the data block the connection is
closed and we can see here where we've
gone through the different steps get
Block locations we have step one you
open up your connection you get the
Block locations by using the RPC then
you actively go through the fs data
input stream to grab all those different
data brings it back into the client and
then once it's done it closes down that
connection and then once the client or
in this case the programmer you know
manager's gone in and pick script you
can do that there's an actual coding in
each in Hadoop or pulling data called
pig or Hive once you get that data back
we close the connection delete all those
randomly huge series of tokens so they
can't be used anymore and then it's done
with that query and we'll go ahead and
zoom in just a little bit more here and
let's look at this even a little closer
here's our Hadoop file system client and
our client jvm our Java virtual machine
on the client node and we have the data
to be read block a Block B and so we
request to read block A and B and it
goes into the name node two then it
sends a location in this case IP
addresses of the blocks for the dn1 and
dn2 where those blocks are stored then
the client interacts with the data nodes
through the switches and so you have
here the core switch so your client node
comes in three and it goes to the core
switch and that then goes to rec switch
one rack switch 2 and rack switch 3. now
if you're looking at this you'll
automatically see a point of failure in
the core switch and certainly you want a
high-end switch mechanism for your core
switch you want to use Enterprise
hardware for that and then when you get
to the racks that's all commodity all
your rack switches so one of those goes
down you don't care as much you just
have to get in there and swap it in and
out really quick you can see here we
have block a which is replicated three
times and so is Block B and it'll pull
from there so we come in here and here
the data is read from the dn1 and dn2 as
they are the closest to each other and
so you can see here that it's not going
to read from two different racks it's
going to read from one rack whatever the
closest setup is for that query the
reason for this is if you have 10 other
queries is going on you want this one to
pull all the data through one setup and
it minimizes that traffic so the
response from the data nodes to the
client that read the operation was
successful it says Ah we've read the
data we're successful which is always
good we like to be successful I don't
know about you I like to be successful
so if we're looking at the read
mechanism let's go ahead and zoom in and
look at the right mechanism for the
Hadoop file system so our hdfs rate
mechanism and so when we have the hdfs
write mechanism here's our client
machine this is again the programmer on
their in-computer and it's going through
the client Java machine or Java virtual
machine the jvm this is all occurring on
the client node somebody's office or
maybe it's on the local server for the
office so we have our name node we have
data nodes and we have the distributed
file system so the client first executes
create file on distributed file system
says hey I'm going to create this file
over here and then it goes through the
RPC call just like our read did The
Client First executes create file in the
distributed file system then the DFS
interacts with the name node to create a
file name node then provides a location
to write the data and so here we have
our hdfs client and the fs data output
Stream So this time instead of the data
going to the client it's coming from the
client and so here the client writes the
data through the fsdata output stream
keep in mind that this output stream the
client could be a streaming code it
could be I mean you know I always refer
to the client as just being this
computer that your programmer is writing
on it could be you have your SQL Server
there where your data is it's current
with all your current sales and it's
archiving all that information through
scoop one of the tools in the Hadoop
file system it could be streaming data
it could be a connection to the stock
servers and you're pulling stock data
down from those servers at a regular
time and that's all controlled you can
actually set that code up to be
controlled in many of the different
features in the Hadoop file system and
some of the different resources you have
that sit on top of it so here the client
writes the data through the fs data
output Street and the fs data output
stream as you can see goes into right
packet so it divides it up into packets
128 megabytes and the data is written
and the slave further replicates it so
here's our data coming in and then if
you remember correctly that's part of
the fs data setup is it tells it where
to replicate it out but the data node
itself is like oh hey okay I've got the
data coming in and it's also given the
tokens of where to send the replications
to and then acknowledgment is sent after
the required replicas are made so then
that goes back up saying hey successful
a written data made three replications
on this as far as our you know going
through the pipeline of the data nodes
and then that goes back and says after
the data is written the client performs
the close method so the client's done
and says okay I'm done here's the end of
the data we're finished and after the
data is written the client performs a
close method and we can see here just a
quick reshape we go in there just like
we did with the read we create the
connection this creates a name node
which lets it know what's going on step
two step three that also includes the
tokens and everything and then we go
into step three where we're now writing
through the fs data output stream and
that sorts it out into uh whatever data
node it's going to go to and which also
tells it how to replicate it so that
data node then sends it to other data
nodes so we have a replication and of
course you finalize it and close
everything up marks it complete to the
name node and it deletes all those
magical tokens in the background so that
they can't be reused and we can go ahead
and just do this with an example the
same setups and whether you're doing a
read or write they're very similar as we
come in here from our client and our
name node and you can see right here we
actually depicting these as the actual
rack and the switch is going on and so
as the data comes in you have your
request like we saw earlier it sends the
location of the data nodes and this
actually turns your your IP addresses
your Dynamic node connections they come
back to your client your hdfs client and
at that point with the tokens it then
goes into the core switch and the core
switch says Hey here it goes the client
interacts with the data nodes through
the switches and you can see here where
you're writing in block a replication of
block a and a second replication of
block a on the second server so blocking
is replicated on the second server and
the third server at this point you now
have three replications of block a and
it comes back and says it acknowledges
it says hey we're done and that goes
back into your Hadoop file system to the
client it says okay we're done and
finally the success written to the name
node and it just closes everything down
a quick recap of the Hadoop file system
and the advantages and so the first one
is probably one of the all of these are
huge one you have multiple data copies
are available so it's very fault
tolerant whole racks can go down
switches can go down even your main name
node could go down if you have a
secondary name node something we didn't
talk too much about is how scalable it
is because it uses distributed storage
you run into their oh my gosh I'm out of
space or I need to do some heavier
processing let me just add another rack
of computers so you can scale it up very
quickly it's a linear scalability where
it used to be if you bought a server you
would have to pay a lot of money to get
that the Craig computer remember the big
Craig computers coming out the Craig
computer runs 2 million a year just the
maintenance to liquid cool it that's
very expensive compared to just adding
more racks of computer and extending
your data center so it's very cost
effective since commodity Hardware is
used we're talking cheap knockoff
computers we still need your high-end
Enterprise for the name node but the
rest of them literally it is a tenth of
the cost of storing data on more
traditional high-end computers and then
the data is secure so it has a very
high-end data security and provides data
security for your data hello Learners
simply and brings you a postgraduate
program in data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below so all Theory and
no play doesn't make much fun so let's
go ahead and show you what it looks like
as far as some of the dimensions when
you're getting into pulling data or
putting data into the Hadoop file system
and you can see here I have Oracle
virtual machine virtual box manager I
have a couple different things loaded on
there Cloudera is one of them so we
should probably explain some of these
things if you're new to Virtual machines
the Oracle virtual box allows you to
spin up a machine as if it is a separate
computer so in this case this is running
I believe Centos a Linux and it creates
like a box on my computer so the
symptoms is running on my machine while
I'm running my Windows 10 it happens to
be a Windows 10 computer and then
underneath here I can actually go under
let me just open up the general might be
hard to see there and you can see I can
actually go down to system processor I
happen to be on an 8 core it has 16
dedicated threads registers of 16 CPUs
but eight cores and I've only designated
this for one CPU so it's only going to
use one of my dedicated threads on my
computer and this the Oracle virtual
machine is open source you can see right
here we're on the Oracle
www.oracle.com I usually just do a
search for downloading virtualbox if you
search for virtualbox all one word it
will come up with this page and then you
can download it for whatever operating
system you're working with there
certainly are a number of different
options and let me go and point those
out if you're setting this up as a for
demoing for yourself the first thing to
note for doing a virtual box and doing a
Cloudera or Horton setup on that virtual
box for doing the Hadoop system to try
it out you need a minimum of 12
gigabytes it cannot be a Windows 10 home
edition because you'll have problems
with your virtual setup and sometimes
you have to go turn on the virtual
settings so it knows it's in there so if
you're on a home setup there are other
sources there's Cloudera and we'll talk
a little bit about Cloudera here in just
a second but they have the Cloudera
online live we can go try the Cloudera
setup I've never used it but Cloudera is
a pretty good company Cloudera and
hortonworks are two of the common ones
out there and we'll actually be running
a Cloudera Hadoop cluster on on our demo
here so you have Oracle virtual machine
you also have the option of doing it on
different VMware that's another one like
virtual machine this is more of a paid
service there is a free setup for just
doing it for yourself which will work
fine for this and then in the Cloudera
like again they have the new online
setup where you can go in there to the
online and for Cloudera you want to go
underneath the Cloudera quick start if
you type in a search for Cloudera quick
start it'll bring you to this website
and then you can select your platform in
this case I did virtual box there's
VMware we just talked about Docker
Docker is a very high-end virtual setup
unless you already know it you really
don't want to mess with it then your KVM
is if you're on a Linux computer that
sets up multiple systems on that
computer so the two you really want to
use are usually the virtual box or do an
online setup and you can see here with
the download if you're going into the
important versions called hortonworks
and they call it sandbox so you'll see
the term hortonworks sandbox and these
are all test demos you're not going to
deploy a single node Hadoop systems that
would just be kind of ridiculous and
defeat the whole purpose of having a
Horton or having a Hadoop system if it's
only installed on one computer in a
virtual node so a lot of different
options if you're not on a professional
Windows version or you don't have at
least 12 gigabytes Ram to run this
you'll want to try and see if you can
find an online version and of course
simply learn has our own Labs if you
sign up for our classes we set you up I
don't know what it is now but last time
I was in there was a five node setup so
you could get around and see what's
going on whether you're studying for the
admin side or for the programming side
in script writing and if I go into my
Oracle virtual box and I go under my
Cloudera and I start this up and each
one has their own flavors Horton uses
just a login so you log everything in
through a local host through your
Internet Explorer or might use Chrome
Cloudera actually opens up a full
interface you actually are in that setup
and you can see when I've started it let
me go back here once I downloaded this
this is a big download by the ways I had
to import The Appliance in virtualbox
the first time I ran it it takes a long
time to configure the setup and the
second time it comes up pretty quick and
with the Cloudera quick start again this
is a pretend single node it opens up and
you'll see that it actually has Firefox
here so here's my web browser I don't
have to go to a local host I'm actually
already in the quick start for Cloudera
and if we come down here you can see
getting started I have some information
analyze your data manage your cluster
your general information on there and
what I always want to start to do is to
go ahead and open up a terminal window
so open up a terminal widen this a
little bit let me just maximize this out
here so you can see so we are now in a
virtual machine this virtual machine is
Centos Linux so I'm on a Linux computer
on my Windows computer and so when I'm
on this terminal window this is your
basic terminal if I do list you'll see
documents Eclipse these are the
different things that are installed with
the quick start guide on the Linux
system so this is a Linux computer and
then Hadoop is running on here so now I
have Hadoop single node so it has both
the name node and the data node and
everything squished together in one
virtual machine we can then do let's do
hdfs telling it that it's a Hadoop file
system DFS minus LS now notice the ls is
the same I have LS for list and LS for
list and I click on here and I'll take
you just a second reading the Hadoop
file system and it comes up with nothing
so a quick recap let's go back over this
three different environments I have this
one out here let's just put this in a
bright red so you can actually see it I
have this environment out here which is
my slides I have this environment here
where I did a list that's looking at the
files on the Linux Centos computer and
then we have this system here which is
looking at the files on the Hadoop file
system so three completely separate
environments and then we connect them
and so right now we have I have whatever
files I have my personal files and the
um of course we're also looking at the
screen for my Windows 10 and then we're
looking at the screen here here's our
list there that's looking at the files
and this is the screen for Centos Linux
and then this is looking at the files
right here for the Hadoop file system so
three completely separate files this one
here which is the Linux is running in a
virtual box so this is a virtual box I'm
using one core to run it or one CPU and
everything in there is it has its own
file system you can see we have our
desktop and documents and whatever in
there and then you can see here we right
now have no files in our Hadoop file
system and this Hadoop file system
currently is stored on the Linux machine
but it could be stored across 10 Linux
machines 20 a hundred this could be
stored across in petabytes I mean it
could be really huge or it could just be
in this case just a demo where we're
putting it on just one computer and then
once we're in here let me just see real
quick if I can go under view zoom in
view zoom in this is just a standard
browser so I could use any like the
Control Plus and stuff like that to zoom
in and this is very common to be in a
browser window with the Hadoop file
system so right now I'm in a Linux and
I'm going to do oh let's just create a
file go file my new file and I'm going
to use VI and this is a VI editor just a
basic editor and we go ahead and type
something in here one two three four
maybe it's columns 44 66 77 of course I
do file system just like your regular
computer can also so in our value editor
you hit your colon I actually work with
a lot of different other editors and
we'll write quit VI so let's take a look
and see what happened here I'm in my
Linux system I type in LS for list and
we should see my new file and sure
enough we do over here there it is let
me just highlight that my new file and
if I then go into the Hadoop system hdfs
DFS minus LS for list we still show
nothing it's still empty so what I can
simply do is I can go hdfs DFS minus put
and then we're going to put my new file
and this is just going to move it from
the Linux system because I'm in this
file folder into the Hadoop file system
and now if we go in and we type in our
list for a new file system you will see
in here that I now have just the one
file on there which is my new file and
very similar to Linux we can do cat and
the cat command simply evokes reading
the file so hdfs DFS minus cat and I had
to look it up remember Cloudera the the
format is going to be a user then of
course the path location and the file
name and in here when we did the list
here's our list so you can see it lists
our file here and we realize that this
is under a user Cloud Dara and so I can
now go user Cloudera my new file and the
minus cat and we'll be able to read the
file in here and you can see right here
this is the file that was in the Linux
system is now copied into the Cloudera
system and it's one three four five that
what I entered in there and if we go
back to the Linux and do list you'll
still see it in here my new file and we
can also do something like this in our
hdfs minus MV and we'll do my new file
and we're going to change it to my new
new file and if we do that underneath
our Hadoop file system the minus MV will
rename it so if I go back here to our
Hadoop file system LS you'll now see
instead of my new file it has my new new
file coming up and there it is my new
new files we've renamed it we can also
go in here and
delete this so I can now come in here so
in our hdfs DFS we can also do a remove
and this will remove the file and so if
we come in here we run this we'll see
that when I come back and do the list
the file is gone and now we just have
another empty folder with our Hadoop
file system and just like any file
system we can take this and we can go
ahead and make directory create a new
directory so MK for make directory we'll
call this my dir so we're going to make
a directory reminder it'll take it just
a second and of course if we do the list
command you'll see that we now have the
directory in there give it just a second
there it comes myder and just like we
did before I can go in here and we're
going to put the file and if you
remember correctly from our files in the
setup I called it my new file so this is
coming from the Linux system and we're
going to put that into my dir that's the
Target in my Hadoop setup and so if I
hit enter on there I can now do the
Hadoop list and that's not going to show
the files remember I put it in a
subfolder so if I do the quadrupt just
this will show my directory and I can do
list and then I can do my dur for my
directory and you'll see underneath the
my directory in the Hadoop file system
it now has my new file put in there and
with any good operating system we need a
minus help so just like you can type in
help in your Linux you can now come in
here and type in hdfs help and it shows
you a lot of the commands in there
underneath the Hadoop file system most
of them should be very similar to the
Linux on here and we can also do
something like this a Hadoop version and
the Hadoop version shows up that we're
in Hadoop 2.60
CDH is it where Cloudera 5 and compiled
by Jenkins and it has a date and all the
different information on our Hadoop file
system so this is some basics in the
terminal window let me go ahead and
close this out because if you're going
to play with this you should really come
in here let me just maximize the
Cloudera and it opens up in a browser
window and so once we're in here again
this is a browser window which you could
access might look like any access for a
Hadoop file system one of the fun things
to do when you're first starting is to
go under hue you'll see it up here at
the top has Cloudera Hue Hadoop near
hbase your Impala your spark these are
standard installs now and Hue is
basically an overview of the file system
and so come up here and you can see
where you can do queries as far as if
you have a hbase or a hive The Hive
database we can go over here to the top
where it says file browser and if we go
under file browser now this is the
Hadoop file system we're looking at and
once we open up the file browser you can
now see there's my directory which we
created and if I click on my directory
there's my new file which is in here and
if I click on my new file it actually
opens it up and you can see from our
Hadoop file system this is in the Hadoop
file system the file that we created so
we covered the terminal window you can
see here's a terminal window up here it
might be if you were in a web browser
it'll look a little different because it
actually opens up as a web browser
terminal window and we've looked a
little bit at Hue which is one of the
most basic components of Hadoop one of
the original components for going
through and looking at your data in your
databases of course now they're up to
the Hue four it's gone through a number
of changes and you can see there's a lot
of different choices in here for other
different tools in the Hadoop file
system and I'll go ahead and just close
out of this and one of the cool things
with the virtual uh box I can either
save the machine State send the shutdown
signal or power off the machine I'll go
and just power off the machine
completely now suppose you have a
library that has a collection of huge
number of books on each floor and you
want to count the total number of books
present on each floor what would be your
approach you would say I will do it
myself but then don't you think that
will take a lot of time and that's
obviously not an efficient way of
counting the number of books in this
huge collection on every floor by
yourself now there could be a different
approach or an alternative to that you
could think of asking three of your
friends or three of your colleagues and
you could then say if each friend could
count the books on every floor then
obviously that would make your work
faster and easier to count the books on
every floor now this is what we mean by
parallel processing So when you say
parallel processing in technical terms
you're talking about using multiple
machines and each machine would be
contributing its RAM and CPU cores for
processing and your data would be
processed on multiple machines at the
same time now this type of process
involves parallel processing in our case
or in our library example where you
would have person one who would be
taking care of books on floor one and
Counting them person two on floor 2 then
you have someone on floor 3 and someone
on floor four so every individual would
be counting the books on every floor in
parallel so that reduces the time
consumed for this activity and then
there should be some mechanism where all
these Counts from every floor can be
aggregated so what is each person doing
here each person is mapping the data of
a particular floor or you can say each
person is doing a kind of activity or
basically a task on every floor and the
task is counting the books on every
floor now then you could have some
aggregation mechanism that could
basically reduce or summarize this total
count and in terms of map reduce we
would say that's the work of reducer so
when you talk about Hadoop map reduce it
processes data on different node
machines now this is the whole concept
of Hadoop framework right that you not
only have your data stored across
machine but you would also want to
process the data locally so instead of
transferring the data from one machine
to machine or bringing all the data
together into some central processing
unit and then processing it you would
rather have the data processed on the
machines wherever that is stored so we
know in case of Hadoop cluster we would
have our data stored on multiple data
nodes on their multiple disks and that
is the data which needs to be processed
but the requirement is that we want to
process this data as fast as possible
and that could be achieved by using
parallel processing now in case of
mapreduce we basically have the first
phase which is your mapping phase so in
case of mapreduce programming model you
basically have two phases one is mapping
and one is reducing now who takes care
of things in mapping phase it is a
mapper class and this mapper class has
the function which is provided by the
developer which takes care of these
individual map tasks which will work on
multiple nodes in parallel your reducer
class us belongs to the reducing phase
so a reducing phase basically uses a
reducer class which provides a function
that will Aggregate and reduce the
output of different data nodes to
generate the final output now that's how
your map reduce Works using mapping and
then obviously reducing now you could
have some other kind of jobs which are
map only jobs wherein there is no
reducing required but we are not talking
about those we are talking about our
requirement where we would want to
process the data using mapping and
reducing especially when data is huge
when data is stored across multiple
machines and you would want to process
the data in parallel so when you talk
about mapreduce you could say it's a
programming model you could say
internally it's a processing engine of a
loop that allows you to process and
compute huge volumes of data and when we
say huge volumes of data we can talk
about terabytes we can talk about
petabytes exabytes and that amount of
data which needs to be processed on a
huge cluster we could also use mapreduce
programming model and run a mapreduce
algorithm in a local mode but what does
that mean if you would go for a local
mode it basically means it would do all
the mapping and reducing on the same
node using the processing capacity that
is RAM and CPU cores on the same machine
which is not really efficient in fact we
would want to have our map reduce work
on multiple nodes which would obviously
have mapping phase followed by reducing
phase and intermittently there would be
data generated there would be different
other phases which help this whole
processing so when you talk about Hadoop
map reduce you are mainly talking about
two main components or two main phases
that is mapping and reducing mapping
taking care of map tasks reducing taking
care of reduced tasks you would have
your data which would be stored on
multiple machines now when we talk about
data data could be in different formats
we could or the developer could specify
what is the format which needs to be
used to understand the data which is
coming in that data then goes through
the mapping internally there would be
some shuffling sorting and then reducing
which gives you your final output so the
way we Access Data from sdfs or the way
our data is getting stored on sdfs we
have our input data which would have one
or multiple files in one or multiple
directories and your final output is
also stored on sdfs to be accessed to be
looked into and to see if the processing
was done correctly so this is how it
looks so you have the input data which
would then be worked upon by multiple
map tasks now how many map tasks that
basically depends on the file that
depends on the input format so normally
we know that in a Hadoop cluster you
would have a file with broken down into
blocks depending on its size so the
default block size is 128 MB which can
then still be customized based on your
average size of data which is getting
stored on the cluster so if I have
really huge files which are getting
stored on the cluster I would certainly
set a higher block size so that my every
file does not have huge number of logs
creating a load on name nodes Ram
because that's tracking the number of
elements in your cluster or number of
objects in your cluster so depending on
your file size your file would be split
into multiple chunks and for every Chunk
we would have a map task running now
what is this map task doing that is
specified within the mapper class so
within the mapper class you have the
mapper function which basically says
what each of these map tasks has to do
on each of the chunks which has to be
processed this data intermittently is
written to hdfs where it is sorted and
shuffled and and then you have internal
phases such as partitioner which which
sides how many reduce chance would be
used or what data goes to which reducer
you could also have a combiner phase
which is like a mini reducer doing the
same reduce operation before it reaches
then you have your reducing phase which
is taken care by a reducer class and
internally the reducer function provided
by developers which would have reduced
task running on the data which comes as
an output from map tasks finally your
output is then generated which is stored
on hdfs now in case of Hadoop it accepts
data in different formats your data
could be in compressed format your data
could be in part k your data could be in
Avro text CSV tsv binary format and all
of these formats are supported however
remember if you are talking about data
being compressed then you have to also
look into what kind of splitability the
compression mechanism supports otherwise
when mapreduce processing happens it
would take the complete file as one
chunk to be processed so sdfs accepts
input data in different formats this
data is stored in sdfs and that is
basically our input which is then
passing through the mapping phase now
what is mapping phase doing as I said it
reads record by record depending on the
input format it reads the data so we
have multiple map tasks running on
multiple chunks once this data is being
read this is broken down into individual
elements and when I say individual
element I could say this is my list of
key value pairs pair so your records
based on some kind of delimiter or
without delimiter are broken down into
individual elements and thus your Mac
creates key value pairs now these key
value pairs are not my final output
these key value pairs are basically a
list of elements which will then be
subjected to further processing so you
would have internally shuffling and
sorting of data so that all the relevant
key value pairs are brought together
which basically benefits the processing
and then you have your reducing which
Aggregates the key value pairs into set
of smaller tuples tuples as you would
say finally your output is getting
stored in the designated directory as a
list of aggregated key value pairs which
gives you your output so when we talk
about mapreduce one of the key factors
here is the parallel processing which it
can offer so we know that we our data is
getting stored across multiple data
nodes and you would have huge volume of
data which is split and randomly
distributed across data nodes and this
is the data which needs to be processed
and the best way would be parallel
processing so you could have your data
getting stored on multiple data nodes or
multiple slave nodes in each slave node
would have again one or multiple disks
to process this data basically we have
to go for parallel processing approach
we have to use the map reduce now let's
look at the mapreduce workflow to
understand how it works so basically you
have your input data stored on sdfs now
this is the data which needs to be
processed it is stored in input files
and the processing which you want can be
done on one single file or it can be
done on a directory which has multiple
files you could also later have multiple
outputs merged which we achieve by using
something called as chaining of mappers
so here you have your data getting
stored on sdfs now input format is
basically something to define the input
specification and how the input files
will be split so there are various input
formats now we can search for that so we
can go to Google and we can basically
search for Hadoop map reduce Yahoo
tutorial this is one of the good links
and if I look into this link I can
search for different input formats and
output formats so let's search for input
format so when we talk about input
format you basically have something to
Define how input files are split so
input files are split up and read based
on what input format is specified so
this is a class that provides following
functionality it selects the files or
other objects that should be used for
input it defines the input split that
break a file into tasks provides a
factory for record reader objects that
read the file so there are different
formats if you look in the table here
and you can see that the text input
format is the default format which reads
lines of a text file and each line is
considered as a record here the key is
the byte offset of the line and the
value is the line content it says you
can have key value input format which
parses lines into key value pairs
everything up to the first tab character
is the key and the remainder is the line
you could also have sequence file input
format which basically works on binary
format so you have input format and in
the same way you can also also search
for output format which takes care of
how the data is handled after the
processing is done so the key value
pairs provided to this output collector
are then returned to Output files the
way they are written is governed by
output format so it functions pretty
much like input format as described in
earlier right so we could set what is
the output format to be followed and
again you have text output sequence file
output format null output format and so
on so these are different classes which
take care of how your data is handled
when it is being read for processing or
how is the data being written when the
processing is done so based on the input
format the file is broken down into
splits and this logically represents the
data to be processed by individual map
tasks or you could say individual mapper
functions so you could have one or
multiple splits which need to be
processed depending on the file size
depending on what properties have been
set now once this is done you have your
input splits which are subjected to
mapping phase internally you have a
record reader which communicates with
the input split and converts the data
into key value pairs suitable to be read
by mapper and what is mapper doing it is
basically working on these key value
pairs the map task giving you an
intermittent output which would then be
forwarded for further processing now
once that is done and we have these key
value pairs which is being worked upon
my map your map tasks as a part of your
mapper function are generating your key
value pairs which are your intermediate
outputs to be processed further now you
could have as I said a combiner phase or
internally a mini reducer phase Now
combiner does not have its own class so
combiner basically uses the same class
as the reducer class provided by the
developer and its main work is to do the
reducing or its main work is to do some
kind of mini aggregation on the key
value pairs which were generated by map
so once the data is coming in from the
combiner then we have internally a
partitional phase which decides how
outputs from combiners are sent to the
reducers or you could also say that even
if I did not have a combiner partitioner
would decide based on the keys and
values based on the type of keys how
many reducers would be
require many reduced tasks would be
required
to earn your output which was generated
by map task now once partitioner has
decided that then your data would be
then sorted and shuffled which is then
fed into the reducer so when you talk
about your reducer it would basically
have one or multiple reduced tasks now
that depends on what or what partitioner
decided or determined for your data to
be processed it can also depend on the
configuration properties which have been
set to decide how many radio Stars
should be used now internally all this
data is obviously going through sorting
and shuffling so that you are reducing
your aggregation becomes an easier task
once we have this done we basically have
the reducer which is the code for the
reducer is provided by the developer and
all the intermediate data has then to be
created to keep to give you a final
output which would then be stored on
sdfs and who does this you have an
internal record writer which writes
these output key value pairs from
reducer to the output files files now
this is how your map reduce Works
wherein the final output data can be not
only stored but then read or accessed
from sdfs or even used as an input for
further mapreduce kind of processing so
this is how it overall looks so you
basically have your data stored on sdfs
based on input format you have the
splits then you have record reader which
gives your data to the mapping phase
which is then taken care by your mapper
function and mapper function basically
means one or multiple map tasks working
on your chunks of data you could
interface which is original which is
which is mandatory then you have a
partitional phase which decides on how
many reduced tasks or how many reducers
would be used to work on your data
internally there is sorting and
shuffling of data happening and then
basically based on your output format
your record reader will write the output
to sdfs directory now internally you
could also remember that data is being
processed locally would have the output
of each task which is being worked upon
stored locally however we do not access
the data directly from data nodes we
access it from sdfs so our output is
stored on sdfs so that is your map
reduce workflow when you talk about
mapreduce architecture now this is how
it would look so you would have
basically a edge node or a client
program or an API which intends to
process some data so it submits the job
to the job tracker or you can say
resource manager in case of Hadoop yarn
framework right now before this step we
can also say that an interaction with
name node would have already happened
which would have given information of
data nodes which have the relevant data
stored then your master processor so in
Hadoop version one we had job tracker
and then the slaves were called task
trackers in Hadoop version 2 instead of
job tracker you have resource manager
answer of task trackers you have node
managers so basically your resource
manager has to assign the job to the
task trackers or node managers so your
node managers as we discussed in yarn
are basically taking care of processing
which happens on every node so
internally there is all of this work
Happening by resource manager node
managers and application Masters and you
can refer to the yarn based tutorial to
understand more about that so here your
processing Master is basically breaking
down the application into tasks what it
does internally is once your application
is submitted you
is handled by resource manager now
forget about the yarn part as of now I
mean who does the negotiating of
resources who allocates them how does
the process
happen on the nodes right so that's all
to do with how yarn handles the
processing request so you have your data
which is stored in sdfs broken down into
one or multiple splits depending on the
input format which has been specified by
the developer your input splits are to
be worked upon by your one or multiple
map tasks which will be running within
the container on the nodes basically you
have the resources which are being
utilized so for each map task you would
have some amount of ram which will be
utilized and then further the same data
which has to go through reducing phase
that is your reduced task will also be
utilizing some RAM and CPU cores now
internally you have these functions
which take care of deciding on number of
reducers doing a mini reduce and
basically reading and processing the
data from multiple data nodes now this
is how your mapreduce programming model
makes parallel processing work or
processes your data which is stored
across multiple machines finally you
have your output which is getting stored
on sdfs
[Music]
so let's have a quick demo on mapreduce
and see how it works on a Hadoop cluster
now we have discussed briefly about
mapreduce which contains mainly two
phases that is your mapping phase and
your reducing phase and mapping phase is
taken care by your mapper function and
your reducing phase is taken care by
your reducer function now in between we
also have sorting and shuffling and then
you have other phases which is
partitioner and combiner and we will
discuss about all those in detail in
later sessions but let's have a quick
demo on how we can run a mapreduce which
is already existing as a package jar
file within your Apache Hadoop cluster
or even in your Cloudera cluster now we
can build our own mapreduce programs we
can package them as jar transfer them to
the cluster and then run it on a Hadoop
cluster on yarn or we could be using
already provided default program so
let's see where they are now these are
my two machines which I have brought up
and basically this would have my Apache
Hadoop cluster running now we can just
do a Simple Start hyphen all dot SH now
I know that this script is deprecated
and it says instead you start DFS and
start yarn but then it will still take
care of static of my cluster on these
two nodes where I would have one single
name node two data nodes one secondary
name node one resource manager and two
node managers now if you have any doubt
in how this cluster came up you can
always look at the previous sessions
where we had a walkthrough in setting up
a cluster on Apache and then you could
also have your cluster running using
Less Than 3 GB of your total machine RAM
and you could have a Apache cluster
running on your machine now once this
cluster comes up we will also have a
look at the web UI which is available
for name node and resource manager now
based on the settings what we have given
our UI will show us details of our
cluster but remember the UI is only to
browse now here my cluster has come up I
can just do a JPS to look at Java
related processes and that will show me
what are the processes which are running
on C1 which is your data node resource
manager node manager and name node and
on my M1 machine which is my second
machine which I have configured here I
can always do a JP s and that shows me
the processes running which also means
that my cluster is up with two data
nodes with two node managers and here I
can have a look at my web UI so I can
just do a refresh and the same thing
with this one just do a refresh so I had
already opened the web pages so you can
always access the web UI using your name
notes host name and 570 Port it tells me
what is my cluster ID what is my block
pool ID it gives you information of what
is the space usage how many live nodes
you have and you can even browse your
file system so I have put in a lot of
data here I can click on browse the file
system and this basically shows me
multiple directories and these
directories have one or multiple files
which we will use for our map reduce
example now if you see here these are my
directories which have some sample files
although these files are very small like
8.7 kilobytes if you look into this
directory if you look into this I have
just pulled in some of my Hadoop logs
and I have put it on my sdfs these are a
little bigger files and then we also
have some other data which we can here
and this is data which I have downloaded
from web now we can either run a map
reduce on a single file or in a
directory which contains multiple files
let's look at that before looking a demo
on map reduce also remember mapreduce
will create a output directory and we
need to have that directory created plus
we need to have the permissions to run
the mapreduce job so by default since
I'm running it using admin ID I should
not have any problem but then if you
intend to run mapreduce with a different
user then obviously you will have to ask
the admin or you will have to give the
user permission to read and write from
sdfs so this is the directory which I've
created which will contain my output
once the map reduce job finishes and
this is my cluster file system if you
look on this UI this shows me about my
yarn which is available for taking care
of any processing it as of now shows
that I have total of 8GB memory and I
have 8v cores now that can be depending
on what configuration we have set or how
many nodes are available we can look at
nodes which are available and that shows
me I have two node managers running each
has 8GB memory and 8-week course now
that's not true actually but then we
have not set the configurations for node
managers and that's why it takes the
default properties that is 8GB RAM and
8B cores now this is my yarn UI we can
also look at scheduler which basically
shows me the different cues if they have
been configured where you will have to
run the jobs we'll discuss about all
these in later in detail now let's go
back to our terminal and let's see where
we can find some sample applications
which we can run on the cluster so once
I go to the terminal I can well submit
the mapreduce job from any terminal now
here I know that my Hadoop
directory is here and within Hadoop you
have various directories we have
discussed that in binaries you have the
commands which you can run in s bin you
basically have the startup scripts and
here you also notice there is a share
directory in the end if you look in the
sale directory you would find Hadoop and
within Hadoop you have various sub
directories in which we will look for
map reduce now this map reduce directory
has some sample jar files which we can
use to run a mapreduce on the cluster
similarly if you are working on a cloud
data cluster you would have to go into
opt Cloudera parcel CDH slash lib and in
that you would have directories for sdfs
mapreduce or sdfs yarn where you can
still find the same jars it is basically
a package which contains your multiple
applications now how do we run a map
reduce we can just type in Hadoop and
hit enter and that shows me that I have
enough option called jar which can be
used to run a jar file now at any point
of time if you would want to see what
are the different classes which are
available in a particular
jar you could always do a jar minus xvf
for example I could say jar x v f and I
could say user local Hadoop share Hadoop
map reduce and then list down your jar
file so I'll say Hadoop mapreduce
examples and if I do this this should
basically unpack it to show me what
classes are available within this
particular jar and it has done this it
has created a meta file and it has
created a org directory we can see that
by doing a LS and here if you look in LS
org since I ran the command from your
phone directory I can look into org
patchy Hadoop examples which shows me
the classes which I have and those
classes can which mapper or reducer
classes so it might not be just mapper
and reducer but you can always have a
look so for example I am targeting to
use word count program which does a word
count on files and gives me a list of
words and how many times they occur in a
particular file or in set of files and
this shows me that what are the classes
which belong to word count so we have a
in some reducer so this is my reducer
class I have tokenizer mapper that is my
mapper class right and basically this is
what is used these classes are used if
you run a word count now there are many
other programs which are part of this
jar file and we can expand and see that
so I can say Hadoop jar and give your
path so I'll say Hadoop jar user local
Hadoop share Hadoop map reduce Hadoop
map reduce examples and if I hit on
enter that will show me what are the
inbuilt classes which are already
available now these are certain things
now there are other jar files also for
example I can look at a dupe and here we
can look at the jar files which we have
in this particular path so this is one
Hadoop mapreduce examples which you can
use you can always look in other jar
files like you can look for Hadoop
mapreduce client job client and then you
can look at the tests one so that is
also an interesting one so you can
always look into Hadoop map reduce
client job client and then you have
something ending with this so if I would
have tried this one using my Hadoop jar
command so in my previous example when
we did this it was showing me all the
classes which are available and that
already has a word count now there are
other good programs which you can try
like terrajen to generate dummy data
Terra saw to check your sorting
performance and so on and tell a
validate to validate the results
similarly we can also do a Hadoop jar AS
as I said on Hadoop mapreduce I think
that was client and then we have job
client and then test start now this has
a lot of other classes which can be used
or programs which can be used for doing
a stress testing or checking your
cluster status and so on one of them
interesting one is test dfsio but let's
not get into all the details in first
instance let's see how we can run a map
reduce now if I would want to run a map
reduce I need to give Hadoop jar and
then my jar file and if I hit on enter
it would say it needs the input and
output it needs which class you want to
run so for example I would say word
count and again if I hit on enter it
tells me that you need to give me some
input and output to process and
obviously this processing will be
happening on cluster that is our yarn
processing framework unless and until
you would want to run this job in a
local mode so there is a possibility
that you can run the job in in local
mode but let's first try how it runs on
the cluster so how do we do that now
here I can do a hdfs LS slash command to
see what I have on my sdfs now through
my UI I was already showing you that we
have set of files and directories which
we can use to process now we can take up
one single file so for example if I pick
up new data and I can look into the
files here what we have and we can
basically run a map reduce on a single
file or multiple files so let's take
this file whatever that contains and I
would like to do a word count so that I
get a list of words and their occurrence
in this file so let me just copy this
now I also need my output to be written
and that will be written here so here if
I want to run a map reduce I can say
Hadoop which we can pull out from
history so Hadoop jar word count now I
need to give my input so that will be
new data and then I will give this file
which we just copied now I am going to
run the word count only on a single file
and I will basically have my output
which will be stored in this directory
the directory which I have created
already Mr output so let's do this
output and this is fair enough now you
can give many other properties you can
specify how many map jobs you want to
run how many reduce jobs you want to run
do you want your output to be compressed
do you want your output to be merged or
many other properties can be defined
when you are specifying word count and
then you can pass in an argument to pass
properties from the command line which
will affect your output now once I go
ahead and submit this this is basically
running a simple inbuilt mapreduce job
on our Hadoop cluster now obviously
internally it will be looking for name
node now we have some issue here and it
says the output already exists what does
that mean so it basically means that
Hadoop will create an output for you you
just need to give a name but then you
don't need to create it so let's give
let's append the output with number one
and then let's go ahead and run this so
I have submitted this command now this
can also be done in background if you
would want to run multiple jobs on your
cluster at the same time so it takes
total input paths to processes one so
that is there is only one split on which
your job has to work now it will
internally try to contact your resource
manager and basically this is done so
here we can have a look and we can see
some counters here now what I also see
is for some property which it is missing
it has run the job but it has run in a
local mode it has run in a local mode so
although we have submitted so this might
be related to my yarn settings and we
can check that so if I do a refresh when
I have run my application it has
completed it would have created an
output but the only thing is it did not
interact with your yarn it did not
interact with your resource manager we
can check those properties and and here
if we look into the job it basically
tells me that it went for mapping and
reducing it would have created an output
it worked on my file but then it ran in
a local mode it ran in a local mode so
mapreduce remember is a programming
model right now if you run it on yarn
you get the facilities of running it on
a cluster where yarn takes care of
resource management if you don't run it
on yarn and run it on a local mode it
will use your machine's RAM and CPU
cores for processing but then we can
quickly look at the output and then we
can also try running this on yarn so if
I look into my hdfs and if I look into
my output Mr output that's the directory
which was not used actually let's look
into the other directory which is ending
with one and that should show me the
output created by this map reduce
although it ran in the local mode it
fetched an input file from your sdfs and
it would have created output in sdfs now
that's my part file which is created and
if you look at part minus r minus these
zeros if you would have more than one
reducer running then you would have
multiple such files created we can look
into this what does this file contain
which should have my word count and here
I can say cat which basically shows me
what is the output created by my map
reduce let's have a look into this so
the file which we gave for processing
has been broken down and now we have the
list of words which occur in the file
plus a count of those words so if there
is some word which is in is more then it
shows me the count so this is a list of
my words and the count for that so this
is how we run a sample mapreduce job I
will also show you how we can run it on
yeah now let's run mapreduce on yarn and
initially when we tried running a map
reduce it did not hit yarn but it ran in
a local mode and that was because there
was a property which had to be changed
in mapreduce hyphen site file so
basically if you look into this file the
error was that I had given a property
which says
mapred.framework.name and that was not
the right property name and it was
ignored and that's why it ran local mode
so I change the property to mapreduce
dot framework.name restarted my cluster
and everything should be fine now and
that mapred hyphen site file has also
been copied across the nodes now to run
a map reduce on a Hadoop cluster so that
it uses yarn and yarn takes care of
resource allocation on one or multiple
machines so I'm just changing the output
here and now I will submit this job
which should first connect to the
resource manager and if it connects to
the resource manager that means our job
will be run using yarn on the Clusters
rather than in a local mode so now we
have to wait for this application to
internally connect to resource manager
and once it starts there we can always
go back to the web UI and check if our
application has reached yarn so it shows
me that there is one input part to be
processed that's my job ID that's my
application ID which you can even
monitor status from the command line now
here the job has been submitted so let's
go back here and just do a Refresh on my
yarn UI which should show me the new
application which is submitted it tells
me that it is an accepted State
application master has already started
and if you click on this link it will
also give you more details of how many
map and reduce tasks would run so as of
now it says the application Master is
running it would
this node which is M1 we can always look
into the logs we can see that there is a
one task attempt which is being made and
now if I go back to my terminal I will
see that it is waiting to get some
resources from the cluster and once it
gets the resources it will first start
with the mapping phase where the mapper
function runs it does the map tasks one
or multiple depending on the splits so
right now we have one file and one split
so we will have just one map task
running and once the mapping phase
completes then it will get into reducing
which will finally give me my output so
we can be toggling through these
sessions so here I can just do a refresh
to see what is happening with my
application is it proceeding is it still
waiting for resource manager to allocate
some resources now just couple of
minutes back I tested this application
on yarn and we can see that my first
application completed successfully and
here we will have to give some time so
that yarn can allocate the resources now
if the resources were used by some other
application they will have to be freed
up now internally Yan takes care of all
that which we will learn more detail in
yarn or you might have already followed
the yarn based session now here we will
have to just give it some more time and
let's see if my application proceeds
with the resources what Yan can allocate
to it sometimes you can also see a
slowness in what web UI shows up and
that can be related to the amount of
memory you have allocated to your notes
now for Apache we can have less amount
of memory and we can still run the
cluster and as I said the memory which
shows up here 16 GB and 16 cores is not
the true one those are the default
settings right but then my yarn should
be able to facilitate running of this
application let's just give it couple of
seconds and then let's look into the
output here again I had to make some
changes in the settings because our
application was not getting enough
resources and then basically I restarted
my cluster now let's submit the
application again to the cluster which
first should contact the resource
manager and then basically the map and
reduce process should start so here I
have submitted an application it is
connecting to the resource manager and
then basically it will start internally
an app master that is application Master
it is looking for the number of splits
which is one it's getting the
application ID and it basically then
starts running the job it also gives you
a tracking URL to look at the output and
now we should go back and look at our
yarn UI if our application shows up here
and we will have to give it a couple of
seconds when it can get the final status
change to running and that's where my
application will be getting resources
now if you closely notice here I have
allocated specific amount of memory that
is 1.5 GB for node manager on every node
and I have basically given two cores
each which my machines also have and my
yarn should be utilizing these resources
rather than going for default now the
application has started moving and we
can see the progress bar here which
basically will show what is happening
and if we go back to the terminal it
will show that first it went in deciding
map and reduce it goes for map once the
mapping phase completes then the
reducing phase will come into existence
and here my job has completed so now it
has basically used we can always look at
how many map and reduced SAS were run it
shows me that there was one map and one
reduced task now with the number of map
tasks depends on the number of splits
and we had just one file which is less
than 128 MB so that was one split to be
processed and reduce task is internally
decided by the reducer or depending on
what kind of property has been set in
Hadoop config files now it also tells me
how many input records were read which
basically means these were the number of
lines in the file it tells me output
records which gives me the number of
total words in the file now they there
might be duplicates and that which is
processed by internal combiner further
processing or forwarding that
information to reducer and basically
ready user works on 335 records gives us
a list of words and their account now if
I do a refresh here this would obviously
show my application is completed it says
succeeded you can always click on the
application to look for more information
it tells me where it ran now we do not
have a history server running as of now
otherwise we can always access more
information so this leads to history
server where all your applications are
stored but I can click on this attempt
tasks and this will basically show me
the history URL or you can always look
into the logs so this is how you can
submit a sample application which is
inbuilt which is available in the jar on
your Hadoop cluster and that will
utilize your cluster to run now you
could always as I said when you are
running a particular job remember to
change the output directory and if you
would not want it to be processing is a
single individual file you could also
point it to a directory that basic means
it will have multiple files and
depending on the file sizes there would
be multiple splits and according to that
multiple map tasks will be selected so
if I click on this this would submit my
second application to the cluster which
should first connect to resource manager
then resource manager has to start an
application Master now here we are
targeting 10 splits now you have to
sometimes give couple of seconds in your
machines so that the resources which
were used are internally already freed
up so that your cluster can pick it up
and then your yarn can take care of
resources so right now my application is
an undefined status but then as soon as
my yarn provides it the resources we
will have the application running on our
yarn cluster so it has already started
if you see it is going further then it
would launch 10 map tasks and it would
the number of reduced tasks would be
decided on either the way your data is
or based on the prop
have been set at your cluster level
let's just do a quick refresh here on my
yarn UI to show me the progress also
take care that when you are submitting
your application you need to have the
output directory mentioned however to
not create it Hadoop will create that
for you now this is how you run a map
reduce without specifying properties but
then you can specify more properties you
can look into what are the things which
can be changed for your mapper and
reducer or basically having a combiner
class which can do a mini reducing and
all those things can be done so we will
learn about that in the later sessions
now we will compare Hadoop version one
that is with mapreduce version one we
will understand and learn about the
limitations of Hadoop version 1 what is
the need of yarn what is yarn what kind
of workloads can be running on yarn what
are yarn components what is yarn
architecture and finally we will see a
demo on yarn so Hadoop person 1 or map
reduce version one well that's outdated
now and nobody is using Hadoop version
one but it would be good to understand
what was in Hadoop version 1 and what
were the limitations of Hadoop version 1
which brought in the thought for the
future processing layer that is yarn now
when we talk about Hadoop we already
know that Hadoop is a framework and
Hadoop has two layers one is your
storage layer that is your hdfs Hadoop
distributed file system which allows for
distributed storage and processing which
allows fault tolerance by inbuilt
replication and which basically allows
you to store huge amount of data across
multiple commodity machines when we talk
about processing we know that map reduce
is the oldest and the most mature
processing programming model which
basically takes care of your data
processing on your distributed file
system so in Hadoop person 1 mapreduce
performed both data processing and
resource management and that's how it
was problematic in mapreduce we had
basically when we talk about the
processing layer we had the master which
was called job tracker and then you had
the slaves which were the task records
so your job tracker was taking care of
allocating resources it was performing
scheduling and even monitoring the jobs
it basically was taking care of a
signing map and reduced tasks to the
jobs running on task trackers and task
trackers which were co-located with data
nodes were responsible for processing
the jobs so task trackers were the
slaves for the processing layer which
reported their progress to the job
tracker so this is what was happening in
Hadoop version one now when we talk
about Hadoop version 1 we would have say
client machines or an API or an
application which basically submits the
job to the master then is job tracker
now obviously we cannot forget that
there would already be an involvement
from name node which basically tells
which are the machines or which are the
data nodes where the data is already
stored now once the job submission
happens to the job tracker job tracker
being the master demon for taking care
of your processing request and also
resource management job scheduling would
then be interacting with your multiple
task trackers which would be running on
multiple machines so each machine would
have a task tracker running and that
task tracker which is a processing slave
would be co-located with the data nodes
now we know that in case of Hadoop you
have the concept of moving the
processing to wherever the data is
stored rather than moving the data to
the processing layer so we would have
task trackers which would be running on
multiple machines and these Stars
trackers would be responsible for
handling the tasks what are these tasks
these are the application which is
broken down into smaller tasks which
would work on the data which is
respectively stored on that particular
node now these were your slave domains
right so your job tracker was not only
tracking the resources so your task
trackers were sending heartbeats they
were sending in packets and information
to the job tracker which would then be
knowing how many resources and when we
talk about resources we are talking
about the CPU course we are talking
about the ram which would be available
on every node so task trackers would be
sending in their resource information to
job tracker and your job tracker would
be already aware of what amount of
resources are available on a particular
node how loaded a particular node is
what kind of work could be given to the
task tracker so job tracker was taking
care of resource management and it was
also breaking the application into tasks
and doing the job scheduling part assign
different tasks to these slave domains
that is your task trackers so job
tracker was eventually overburdened
right because it was managing jobs it
was tracking the resources from multiple
task trackers and basically it was
taking care of job scheduling so job
tracker would be overburdened and in a
case if job tracker would fail then it
would affect the overall processing so
if the master is skilled if the master
demand dies then the processing cannot
proceed now this was one of the
limitations of Hadoop person one so when
you talk about scalability that is the
capability to scale due to a single job
tracker scalability would be hitting a
bottleneck you cannot have a cluster
size of more than 4000 nodes and cannot
run more than 40 000 concurrent tasks
now that's just a number we could always
look into the individual resources which
each machine was having and then we can
come up with an April appropriate number
however with a single job tracker there
was no horizontal scalability for the
processing layer because we had single
processing Master now when we talk about
availability job tracker as I mentioned
would be a single point of failure now
any failure kills all the queued and
running jobs and jobs would have to be
resubmitted now why would we want that
in a distributed platform in a cluster
which has hundreds and thousands of
machines we would want a processing
layer which can handle huge amount of
processing which could be more scalable
which could be more available and could
handle different kind of workloads when
it comes to resource utilization now if
you would have a predefined number of
map and reduce slots for each task
tracker you would have issues which
would relate to resource utilization and
that again is putting a burden on the
master which is tracking these resources
which has to assign jobs which can run
on multiple machine things in parallel
so limitations in running non-map reduce
applications now that was one more
limitation of Hadoop version 1 and
mapreduce that the only kind of
processing you could do is mapreduce and
mapreduce programming model although it
is good it is oldest it has matured over
a period of time but then it is very
rigid you will have to go for mapping
and reducing approach and that was the
only kind of processing which could be
done in Hadoop version one so when it
comes to doing a real-time analysis or
doing ad hoc query or doing a graph
based processing or massive parallel
processing there were limitations
because that could not be done in Hadoop
person 1 which was having mapreduce
version 1 as the processing component
now that brings us to the need for yarn
so yarn it stands for yet another
resource negotiator so as I mentioned
before yarn in Hadoop version one well
you could have applications which could
be written in different programming
languages but then the only kind of
processing which was possible was
mapreduce we had the storage layer we
had the processing but then kind of
limit processing which could be done now
this was one thing which brought in a
thought that why shouldn't we have a
processing layer which can handle
different kind of workloads as I
mentioned might be graph processing
might be real-time processing might be
massive parallel processing or any other
kind of processing which would be a
requirement of an organization now
designed to run mapreduce jobs only and
having issues in scalability resource
utilization job tracking Etc that led to
the need of something what we call as
yarn now from Hadoop version 2 onwards
we have the two main layers have changed
a little bit you have the storage layer
which is intact that is your sdfs and
then you have the processing layer which
is called ya yet another resource
negotiator now we will understand how
yarn works but then yarn is taking care
of your processing layer it does support
mapreduce So mapreduce processing can
still be done but then now you can have
a support to other processing Frameworks
yarn can be used to solve the issues
which Hadoop version 1 was posing
something like Resource Management
something like different kind of
workload processing something like
scalability resource utilization all
that is now taken care by yarn now when
we talk about yarn we can have now a
cluster size of more than 10 000 nodes
and can run more than 100 000 concurrent
tasks that's just to take care of your
scalability when you talk about
compatibility applications which were
developed for Hadoop version 1 which
were primarily mapreduce kind of
processing can run on yarn without any
disruption or availability issues when
you talk about resource utilization
there is a mechanism which takes care of
dynamic allocation of cluster resources
and this basically improves the resource
utilization when we talk about multi
tenancy so basically now the cluster can
handle different kind of workloads so
you can use open source and propriety
data access engines you can perform
real-time analysis you can be doing
graph processing you can be doing ad hoc
querying and this can be supported for
multiple workloads which can run in
parallel so this is what yarn offers so
what is Yan as I mentioned yarn stands
for yet another resource negotiator so
it is the cluster Resource Management
layer for your Apache Hadoop ecosystem
which takes care of scheduling jobs and
assigning resources now just imagine
when you would want to run a particular
application you would basically be
telling the cus cluster that I would
want resources to run my applications
that application might be a mapreduce
application that might be a hive query
which is triggering a mapreduce that
might be a pick script which is
triggering a mapreduce that could be
Hive with days as in execution Engine
That Could Be a spark application that
could be a graph processing application
in any of these cases you would still
you as in in sense client or basically
an API or the application would be
requesting for resources yarn would take
care of that so yarn would provide the
desired resources now when we talk about
resources we are mainly talking about
the network related resources we are
talking about the CPU cores or as in
terms of yarn we say virtual CPU course
we would talk about Ram that is in GB or
MB or in terabytes which would be
offered from multiple machines and Yan
would take care of this so with yarn you
could basically handle different
workloads now these are some of the
workloads which are showing up here you
have the traditional mapreduce which is
mainly batch oriented you could have an
interactive execution engine something
as days you would have H base which is a
column oriented or a four dimensional
database and that would be not only
storing data on sdfs but would also need
some kind of processing you could have
streaming functionalities which would be
from storm or Kafka or spark you could
have graph processing you could have
in-memory processing such as spark and
its components and you could have many
others so these are different Frameworks
which could now run and which can run on
top of ER so how does yarn do that now
when we talk about yarn this is how a
overall yarn architecture looks so at
one end you have the client now client
could be basically your Edge node where
you have some applications which are
running it could be an API which would
want to interact with your cluster it
could be a user triggered application
which wants to run some jobs which are
doing some processing so this client
would submit a job request now what is
resource manager doing this Source
manager is the master of your processing
layer in Hadoop version 1 we basically
had job tracker and then we had task
trackers which were running on
individual nodes so your task trackers
were sending your heartbeats to the job
tracker your task trackers were sending
it their resource information and job
tracker was the one which was tracking
the resources and it was doing the job
scheduling and that's how as I mentioned
earlier job tracker was overburdened so
job tracker is now replaced by your
resource manager which is the master for
your processing layer your task trackers
are replaced by node managers which
would be then running on every node and
we have a temporary demon which you see
here in blue and that's your app master
so this is what we mentioned when we say
yet another resource negotiator so App
Master would be existing in a Hadoop
version 2. now when we talk about your
resource manager resource manager is the
master for processing layer so it would
already be receiving heartbeats and you
can say resource information from
multiple node managers which would be
running on one or multiple machines and
these node managers are not only
updating their status but they are also
giving an information of the amount of
resources they have now when we talk
about resources we should understand
that if I'm talking about this node
manager then this has been allocated
some amount of RAM for processing and
some amount of CPU cores and that is
just a portion of what the complete node
has so if my node has say imagine my
node has around 100 GB RAM and I have
saved 60 cores all of that cannot be
allocated to node manager so node
manager is just one of the com
components of Hadoop ecosystem it is the
slave of the processing layer so we
could say keeping in all the aspects
such as different Services which are
running might be Cloud era or hot and
works related Services running system
processes running on a particular node
some portion of this would be assigned
to node manager for processing so we
could say for example say 60 GB Ram per
node and say 40 CPU cores so this is
what is allocated for the node manager
on every machine similarly we would have
here similarly we would have here so
node manager is constantly giving an
update to resource manager about the
resources what it has probably there
might be some other applications running
and node manager is already occupied so
it gives an update now we also have a
concept of containers which is basically
we will we will talk about which is
about these resources being broken down
into smaller parts so resource man
manager is keeping a track of the
resources which every node manager has
and it is also responsible for taking
care of the job request how do these
things happen now as we see here
resource manager at a higher level you
can always say this is the processing
Master which does everything but in
reality it is not the resource manager
which is doing it but it has internally
different services or components which
are helping it to do what it is supposed
to do now let's look further now as I
mentioned your resource manager has
these services or components which
basically helps it to do the things it
is basically a an architecture where
multiple components are working together
to achieve what yarn allows so resource
manager has mainly two components that
is your scheduler an applications
manager and these are at high level four
main components here so we talk about
resource manager which is the processing
Master you have node managers which are
the processing slaves which are running
on every nodes you have the concept of
container and you have the concept of
application Master how do all these
things work now let's look at yarn
components so resource manager basically
Lee has two main components you can say
which assist resource manager in doing
what it is capable of so you have
scheduler and applications manager now
there is when you talk about resources
there is always a requirement for the
applications which need to run on
cluster of resources so your application
which has to run which was submitted by
client needs resources and these
resources are coming in from multiple
machines wherever the relevant data is
stored and a node manager is running so
we always know that node manager is
co-located with data nodes now what does
the scheduler do so we have different
kind of schedulers here we have
basically a capacity scheduler we have a
fair scheduler or we could have a fee
for scheduler so there are different
schedulers which take care of resource
allocation so your scheduler is
responsible for allocating resources to
various learning applications now
imagine a particular environment where
you have different teams or different
departments which are working on the
same cluster so we would call the
cluster as a multi-tenant cluster and on
the multi-terrent cluster you would have
different applications which would want
to run simultaneously accessing the
resources of the cluster how is that
managed so there has to be some
component which has a concept of pooling
or queuing so that different departments
or different users can get dedicated
resources or can share resources on the
cluster so scheduler is responsible for
allocating resources to various running
applications now it does not perform
monitoring or tracking of the status of
applications that's not the part of
scheduler it does not offer any
guarantee about restarting the failed
tasks due to Hardware or network or any
other failures scheduler is mainly
responsible for allocating resources now
as I mentioned you could have different
kind of schedulers you could have a fee
for scheduler which was mainly in older
version of Hadoop which stands for first
in first out you could have a fair
scheduler which basically means multiple
applications could be running in the
cluster and they would have a fair share
of the resources you could have a
capacity scheduler which would basically
have dedicated or fixed amount of
resources across the cluster now
whichever scheduler is being used
scheduler is mainly responsible for
allocating resources then it's your
applications manager now this is
responsible for accepting job
submissions now as I said at higher
level we could always say resource
managers stay doing everything it is
allocating the resources it is
negotiating the resources it is also
taking care of listening to the clients
and taking care of job submissions but
who is doing is doing real it is these
components so you have applications
manager which is responsible for
accepting job submissions it negotiates
the first container for executing the
application specific application master
it provides the service for restarting
the application Master now how does this
work how do these things happen in
coordination now as I said your node
manager is the slave process which would
be running on every machine slave is
tracking the resources what it has it is
tracking the processes it is taking care
of running the jobs and basically it is
tracking each container resource
utilization so let's understand what is
this container so normally when you talk
about a application request which comes
from a client so let's say this is my
client which is requesting or which is
coming up with an application which
needs to run on the cluster now this
application could be anything it first
contacts your master that's your
resource manager which is the master for
your processing layer now as I mentioned
and as we already know that your name
node which is the master of your a
cluster has the metadata in its Ram
which is aware of the data being split
into blocks the blocks will stored on
multiple machines and other information
so obviously there was a interaction
with the master which has given this
information of the relevant nodes where
the data exists now for the processing
need your client basically the
application which needs to run on the
cluster so your resource manager which
basically has the scheduler which takes
care of allocating resources and
resource manager has mainly these two
components which are helping it to do
its work now for a particular
application which might be needing data
from multiple machines now we know that
we would have multiple machines where we
would have node manager running we would
have a data node running and data nodes
are responsible for storing the data on
disk so your resource manager has to
negotiate shade the resources now when I
say negotiating the resources it could
basically ask each of these node
managers for some amount of resources
for example it would be saying can I
have 1 GB of RAM and one CPU core from
you because there is some data residing
on your machine and that needs to be
processed as part of my application can
I again have 1GB and one CPU core from
you and this is again because some
relevant data is stored and this request
which resource manager makes of holding
the resources of total resources which
the node manager has your resource
manager is negotiating or is asking for
resources from the processing slave so
this request of holding resources can be
considered as a container so resource
manager now we know it is not actually
the resource manager but it is the Apple
education manager which is negotiating
the resources so it negotiates the
resources which are called containers so
this request of holding resource can be
considered as a container so basically a
container can be of different sizes we
will talk about that so resource manager
negotiates the resources with node
manager now node manager which is
already giving an update of the
resources it has what amount of
resources it holds how much busy it is
can basically approve or deny this
request so node manager would basically
approve in saying yes I could hold these
resources I could give you this
container of this particular size now
once the container has been approved or
allocated or you can say granted by your
node manager resource manager now knows
that resources to process the
application are available and guaranteed
by the node manager so resource manager
starts a temporary demon called App
Master so this is a piece of code which
would also be running in one of the
containers it would be running in one of
the containers which would then take
care of execution of tasks in other
containers so your application Master is
per application so if I would have 10
different applications coming in from
the client then we would have 10 app
Masters one app Master being responsible
for per application now what does this
app Master do it basically is a piece of
code which is responsible for execution
of the application so your app Master
would run in one of the containers and
then it would use the other containers
which node manager is guaranteed that it
will give when the request application
request comes to it and using these
containers the App Master will run the
processing tasks Within These they
designated resources so it is mainly the
responsibility of application Master to
get the execution done and then
communicate it to the Masters so
resource manager is tracking the
resources it is negotiating the
resources and once the resources have
been negotiated it basically gives the
control to application Master
application Master is then running
within one of the containers on one of
the nodes and using the other containers
to take care of execution this is how it
looks so basically container as I said
is a collection of resources like CPU
memory your disk which would be used or
which already has the data and network
so your node manager is basically
looking into the request from
application master and it basically is
granting this request or basically is
allocating these containers now again we
could have different sizing of the
containers let's take an example here so
as I mentioned from the total resources
which are available for a particular
node some portion of resources are
allocated to the node manager so let's
imagine this is my node where node
manager as a processing slave is running
so from the total resources which the
node has some portion of RAM and CPU
cores is basically allocated to the node
manager so I could say out of total 100
GB Ram we can say around 60 cores which
the particular node has so this is my
Ram which the node has and these are the
CPU cores which the node has some
portion of it right so we can say might
be 70 or 60 percent of the total
resources so we could say around 60 GB
RAM and then we could say around 40 week
course have been allocated to node
manager so there are these settings
which are given in the yarn hyphen site
file now apart from this allocation that
is 60 GB RAM and 40v cores we also have
some properties which say what will be
the container sizes so for example we
could have a small container setting
which could say my every container could
have 2GB RAM and say one virtual CPU
core so this is my smallest container So
based on the total resources you could
calculate how many such small containers
could be running so if I say 2GB Ram
then I could have around 30 containers
but then I'm talking about one virtual
CPU core so totally I could have around
30 small containers which could be
running in parallel on a particular node
and as of that calculation you would say
10 CPU cores are not being utilized you
could have a bigger container size which
could say I would go for two CPU cores
and 3 GB Ram so 3gb RAM and two CPU
cores so that would give me around 20
containers of bigger size so this is the
container sizing which is again defined
in the yarn hyphen site file so what we
know is on a particular node which has
this kind of allocation either we could
have 30 small containers running or we
could have 20 big containers running and
same would apply to multiple nodes so
node manager based on the request from
application Master can allocate these
containers now remember it is within
this one of these containers you would
have an application Master running and
other containers could be used for your
processing requirement application
Master which is per application it is
the one which uses these resources in
basically manages or uses these
resources for individual application so
remember if we have 10 applications
running on yarn then it would be 10
application Masters one responsible for
each application your application Master
is the one which also interacts with the
scheduler to basically know how much
amount of resources could be allocated
for one application and your application
Master is the one which uses these
resources but it can never negotiate for
more resources to node Manager
application Masters cannot do that
application master has to always go back
to resource manager if it needs more
resources so it is always the resource
manager and internally resource manager
component that is application manager
which negotiates the resources at any
point of time due to some node failures
or due to any other requirements if
application Master needs more resources
on one or multiple nodes it will always
be contacting the resource manager
internally the applications manager for
more containers now this is how it looks
so your client submits the job request
to resource manager now we know that
resource manager internally has
scheduler an applications manager node
managers which are running on multiple
machines are the ones which are tracking
their resources giving this information
to the source manager so that resource
manager or I would say its component
applications manager could request
resources from multiple node managers
when I say request resources it is these
containers so your resource manager
basically will request for the resources
on one or multiple nodes node manager is
the one which approves these containers
and once the container has been approved
your resource manager triggers a piece
of code that is application Master which
obviously needs some resources so it
would run in one of the containers and
will use other containers to do the
execution so your client submits an
application to resource manager resource
manager allocates a container or I would
say this is at a high level right
resource manager is negotiating the
resources and internally who is
negotiating the resources it is your
applications manager who is granting
this request it is node manager and
that's how we can say resource manager
allocates a container application Master
basically contacts the related node
manager because it needs to use the
containers node manager is the one which
launches the container or basically
gives those resources within which an
application can run an application
Master itself will then accommodate
itself in one of the containers and then
use other containers for the processing
and it is within these containers the
actual execution happens now that could
be a map task that could be a reduced
task that could be a spark executor
taking care of smart tasks and many
other processing
all
so before we look into the demo on how
yarn works I would suggest looking into
one of the blogs from Cloudera so you
can just look for yarn untangling and
this is really a good blog which
basically talks about the overall
functionality which I explained just now
so as we mentioned here so you basically
have the master process you have the
worker process which basically takes
care of your processing your resource
manager being the master and node
manager being the slave this also talks
about the resources which each node
manager has it talks about the yarn
configuration file where you give all
these properties it basically shows you
node manager which reports the amount of
resources it has to resource manager now
remember if worker node shows 18 to 8
CPU cores and 128 GB RAM and if your
node manager says 64 V cores and RAM 128
GB then that's not the total capacity
city of your node it has some portion of
your node which is allocated to node
manager now once your node manager
reports that your resource manager is
requesting for containers based on the
application what is a container it is
basically a logical name given to a
combination of vcore and RAM it is
within this container where you would
have basically the process running so
once your application starts and once
node manager is guaranteed these
containers your application or your
resource manager has basically already
started an application Master within the
container and what does that application
Master do it uses the other containers
where the tasks would run so this is a
very good blog which you can refer to
and this also talks about mapreduce if
you have already followed the mapreduce
tutorials in past then you would know
about the different kind of tasks that
is map and reduce and these map and
reduced tasks could be running within
the container in one or multiple as said
it could be map task it could be reduced
task it could be a spark based task
which would be running within the
container now once the task finishes
basically that resources can be freed up
so the container is released and the
resources are given back to yarn so that
it can take care of further processing
if you'll further look in this blog you
can also look into the part 2 of it
where you talk mainly about
configuration settings you can always
look into this which talks about why and
how much resources are allocated to the
node manager it basically talks about
your operating system overhead it talks
about other services it talks about
Cloudera or hortonworks related Services
running and other processes which might
be running and based on that some
portion of RAM and CPU cores would be
allocated to node manager so that's how
it would be done in the yarn hyphen site
file and this basically shows you what
is the total amount of memory and CPU
course which is allocated to node
manager then within every machine where
you have a node manager running on every
machine in the yarn hyphen site file you
would have such properties which would
say what is the minimum container size
what is the maximum container size in
terms of ram what is the minimum for CPU
cores what is the maximum for CPU cores
and what is the incremental size in
where RAM and CPU cores can increment so
these are some of the properties which
Define how containers are allocated for
your application request so have a look
at this and this could be a good
information which talks about different
properties now you can look further
which talks about scheduling if you look
in this particular blog which also talks
about scheduling where it talks about
scheduling in yarn which talks about
Fair scheduler or you basically having
different cues in which allocations can
be done you also have different ways in
which queues can be managed and
different schedulers can be used so you
can all always look at this series of
log you can also be checking for yarn
schedulers and then search for uh Hadoop
definitive guide and that could give you
some information on how it looks when
you look for Hadoop definitive guide so
if you look into this book which talks
about the different resources as I
mentioned so you could have a fee for
scheduler that is first in first out
which basically means if a long running
application is submitted to the cluster
all other small running applications
will have to wait there is no other way
but that would not be a preferred option
if you look in V4 scheduler if you look
for capacity scheduler which basically
means that you could have different
queues created and those queues would
have resources allocated so then you
could have a production queue where
production jobs are running in a
particular queue which has fixed amount
of resources allocated you could have a
develop queue where development jobs are
running and both of them are running in
parallel you could then also look into
Fair scheduler which basically means
again multiple applications could be
running on the cluster however they
would have a fair share so when I say
fair share in brief what it means is if
I had given 50 percent of resources to a
queue for production and 50 of resources
for a queue of development and if both
of them are running in parallel then
they would have access to 50 percent of
cluster resources however if one of the
queue is unutilized then second queue
can utilize all cluster resources so
look into the fair scheduling part it
also shows you about how allocations can
be given and you can learn more about
schedulers and how queues can be used
for managing multiple applications now
we will spend some time in looking into
few ways or few quick ways in
interacting with yarn in the form of a
demo to understand and learn on how yarn
works we can look into a particular
cluster now here we have a designated
cluster which can be used you could be
using the similar kind of commands on
your Apache based cluster or a Cloudera
quick start VM if you already have or if
you have a Cloudera or a hot and works
cluster running there are different ways
in which we can interact with yarn and
we can look at the information one is
basically looking into the admin console
so if I would look into Cloud error
manager which is basically an admin
console for a cloudera's distribution of
Hadoop similarly you could have a
hortonworks cluster than access to the
admin console so if you have even read
access for your cluster and if you have
the admin console then you can search
for yarn as a service which is running
you can click on yarn as a service and
that gives you different tabs so you
have the instances which tells basically
what are the different roles for your
yarn service running so we have here
multiple node managers now some of them
show in stop status but that's nothing
to worry so we have three and six node
managers we have resource manager which
is one but then that can also be in a
high availability where you can have
active and standby you also have a job
history server which would show you the
applications once they have completed
now you can look at the yarn
configurations and as I was explaining
you can always look for the properties
which are related to the allocation so
you can here search for course and that
should show you the properties which
talk about the allocations so here if we
see we can be looking for yarn App
mapreduce application Master resource
CPU course what is the CPU course
allocated for map reduce map task reduce
task you can be looking at yarn node
manager resource CPU course which
basically says every node manager on
every node would be allocated with six
CPU cores and the container sizing is
with minimum allocation of one CPU core
and the maximum could be two CPU cores
similarly you could also be searching
for memory allocation and here you could
then scroll down to see what kind of
memory allocation has been done for the
node manager so if we look further it
should give me information of node
manager which basically says here that
the container minimum allocation is 2GB
the maximum is 3gb and we can look at
node manager which has been given 25 GB
per node so it's a combination of this
memory and CPU cores which is the total
amount of resources which have been
allocated to every node manager now we
can always look into applications tab
that would show us different
applications which are submitted on yarn
for example right now we see there is a
spark application running which is
basically a user who is using spark
shell which has triggered a application
on spark and that is running on yarn you
can look at at different applications
workload information you can always do a
search based on the number of days how
many applications have run and so on you
can always go to the web UI and you can
be searching for the resource manager
web UI and if you have access to that it
will give you overall information of
your cluster so this basically says that
here we have 100 GB memory allocated so
that could be say 25 GB per node and if
we have four node managers running and
we have 24 cores which is six cores per
node if we look further here into nodes
I could get more information so this
tells me that I have four node managers
running and node managers basically have
25 GB memory allocated per node and six
scores out of which some portion is
being utilized we can always look at the
scheduler here which can give us
information what kind of scheduler has
been allocated so we basically see that
there is just a root q and within root
you have default q and you have
basically users queue based on different
users we can always scroll here and that
can give us information if it is a fair
share so here we see that my root dot
default has 50 of resources and the
other queue also has 50 percent of
resources which also gives me an idea
that a fair scheduler is being used we
can always confirm that if we are using
a fair scheduler or a capacity scheduler
which takes care of allocation so search
for scheduler and that should give you
some understanding of what kind of
scheduler is being used and what are the
allocations given for that particular
scheduler so here we have Fair scheduler
it shows me you have under root you have
the root queue which has been given 100
capacity and then you have within that
default which also takes hundred percent
so this is how you can understand about
yarn by looking into the yarn web UI you
can be looking into the configurations
you look at application locations you
can always look at different actions now
since we do not have admin access the
only information we have is to download
the client configuration we can always
look at the history server which can
give us information of all the
applications which have successfully
completed now this is from your yarn UI
what I can also do is I can be going
into Hue which is the web interface and
your web interface also basically allows
you to look into the jobs so you can
click on Hue web UI and if you have
access to that it should show up or you
should have a way to get to your Hue
which is a graphical user interface
mainly comes with your Cloud era you can
also configure that with Apache
hortonworks has a different way of
giving you the web UI access you can
click and get into Hue and that is also
one way where you can look at yarn you
can look at the jobs which are running
if there are some issues with it and
these these are your web interfaces so
either you look from yarn web UI or here
in Hue you have something called as job
browser which can also give you
information of your different
applications which might have run so
here I can just remove this one which
should basically give me a list of all
the different kind of jobs or workflows
which were run so either it was a spark
based application or it was a map reduce
or it was coming from hive so here I
have list of all the applications and it
says this was a map reduce this was a
spark something was killed something was
successful and this was basically a
probably a hive query which triggered a
mapreduce job you can click on the
application and that tells you how many
tasks were run for it so there was a map
task which ran for it you can get into
the metadata information which you can
obviously you can also look from the
yarn UI to look into your applications
which can give you a detailed
information of if it was a map reduce
how many map and reduced us for run what
were the different counters if it was a
spark application it can let you follow
through spark history server or job
history server so you can always use the
web UI to look into the jobs you can be
finding in a lot of useful information
here you can also be looking at how many
resources were used and what happened to
the job was it successful did it fail
and what was the job status now apart
from web UI which always you might not
have access to so in a particular
cluster in a production cluster there
might be restrictions and the
organization might not have access given
to all the users to graphical user
interface like you or might be you would
not have access to the Cloudera manager
or admin console because probably
organization is managing multiple
clusters using this admin console so the
one way which you would have access is
is your web console or basically your
Edge node or client machine from where
you can connect to the cluster and then
you can be working so let's login here
and now here we can give different
commands so this is the command line
from where you can have access to
different details you can always check
by just typing in mapred which gives you
different options where you can look at
the mapreduce related jobs you can look
at different queues if there are queues
configured you can look at the history
server or you can also be doing some
admin stuff provided you have access so
for example if I just say mapred and
queue here this basically gives me an
option says what would you want to do
would you want to list all the queues do
you want information on a particular
queue so let's try a list and that
should give you different queues which
were being used now here we know that
per user a queue dynamically gets
created which is under root users and
that gives me what is the status of the
queue what is the capacity has there
been any kind of maximum capacity or
capping done so we get to see a huge
list of cues which dynamically get
configured in this environment and then
you also look at your root dot default I
could have also picked up one particular
queue and I could have said show me the
jobs so I could do that now here we can
also give a yarn command so let me just
clear the screen and I will say yarn and
that shows me different options so apart
from your web interface something like
web UI apart from your Yarns web UI you
could also be looking for information
using yarn commands here so these are
some list of commands which we can check
now you can just type in yarn and
version if you would want to see the
version which basically gives you
information of what is the Hadoop
version being used and what is is the
vendor specific distribution version so
here we see we are working on cloud
errors distribution 5.14 which is
internally using Hadoop 2.6 now
similarly you can be doing a yarn
application list so if you give this
that could be an exhaustive list of all
the applications which are running or
applications which have completed so
here we don't see any applications
because right now probably there are no
applications which are running it also
shows you you could be pulling out
different status such as submitted
accepted or running now you could also
say I would want to see the services
that I've finished running so I could
say yarn application list and app States
as finished so here we could be using
our Command so I could say yarn
application list and then I would want
to see the app states which gives me the
applications which have finished and we
would want to list all the applications
which finished now that might be
applications which succeeded right and
there is a huge list of application
which is coming in from the history
server which is basically showing you
the huge list of applications which have
completed so this is one way and then
you could also be searching for one
particular application if you would want
to search a particular application if
you have the application ID you could
always be doing a grip that's a simple
way I could say basically let's pick up
this one and if I would want to search
for this if I would want more details on
this I could obviously do that by
calling in my previous command and you
could do a grip if that's what you want
to do and if you would want to search is
there any application which is in the
list of my applications that shows my
application I could pull out more
information about my application so I
could look at the log files for a
particular application by giving the
application ID so I could say yarn law
now that's an option and every time
anytime you have a doubt just hit enter
it will always give you options what you
need to give with a particular command
so I can say yarn log
application ID now we copied an
application ID and we could just give it
here we could give other options like
app owner or if you would want to get
into the Container details or if you
would want to check on a particular node
now here I'm giving yarn logs and then
I'm pointing it to an application ID and
it says the log aggregation has not
completed might be this was might be
this was an application which was
triggered based on a particular
interactive cell or based on a
particular query so there is no log
existing for this particular application
you can always look at the status of an
application you can kill an application
so here you can be saying yarn yarn
application and then what would you want
to do with an application hit and enter
it shows you the different options so we
just tried app States you could always
look at the last one which says status
and then for my status I could be giving
my application ID so that tells me what
is the status of this application it
connects to the resource manager it
tells me what's the application ID what
kind of application it was who ran it
which was the queue where the job was
running what was the start and end time
what is the progress the status of it if
it is finished or if it has succeeded
and then it basically gives me also an
information of where the application
master was running it gives me the
information where you can find this job
details in history server if you are
interested in looking into it also gives
you a aggregate resource allocation
which tells how much GB memory and how
many core seconds it used so this is
basically looking out at the application
details now I could kill an application
if the application was already running I
could always do a yarn application minus
skill and then I could be giving my
application now I could try killing this
however it would say the application is
already finished if I had an application
running and if my application was
already given an application ID by The
Source manager I Could Just Kill it I
can also say yarn node list which would
give me a list of the node managers now
this is what we were looking from the
yarn web UI and we were pulling out the
information so we can get this and kind
of information from your command line
always remember and always try to be
well accustomed with the command line so
you can do various things from the
command line and then obviously you have
the web uis which can help you with a
graphical interface easily able to
access things now you could be also
starting the resource manager which we
would not be doing here because we are
already running in a cluster so you
could give a yarn resource manager you
could get the logs of resource manager
if you would want by giving yarn demin
so we can try that so you can say yarn
and then demon so it says it does not
find the demon so so you can give
something like this get level and here I
will have to give the node and the IP
address where you want to check the logs
of resource manager so you could be
giving this for which we will have to
then get into Cloudera manager to look
into the nodes and the IP address you
could be giving a command something like
this which basically gives you the level
of the log which you have and I got this
resource manager address from the web UI
now I can be giving in this command to
look into the domain log and it
basically says you would want to look at
the resource manager related log and you
have the log 4J which is being used for
logging the kind of level which has been
set as info which can again be changed
in the way you're logging the
information now you can try any other
commands also from yarn for example
looking at the yarn RM admin so you can
always do a yarn RM admin and this
basically gives you a lot of other
informations like refreshing the cues or
refreshing the notes or basically
looking at the admin ACLS or getting
groups so you could always get group
names for a particular user now we could
search for a particular user such as
yarn or hdfs itself so I could just say
here I would want get groups and then I
could be searching for say username hdfs
so that tells me sdfs belongs to a
Hadoop group similarly you could search
for say mapred or you could search for
yarn so these are service related users
which automatically get created and you
can pull out information related to
these you can always do a refresh nodes
kind of command and that is mainly done
internally this can be useful when you
are doing commissioning decommissioning
but then in case of Cloudera or
hortonworks kind of cluster you would
not be manually giving this command
because if you are doing a commissioning
decommissioning from an admin console
and if you are an administrator then you
could just restart the services which
are affected and that will take care of
this but if you were working in an
Apache cluster and if you were doing
commissioning decommissioning then you
would be using in two commands refresh
notes and base basically that's for
refreshing the nodes which should not be
used for processing and similarly you
could have a command refresh notes which
comes with stfs so these are different
options which you can use with your yarn
on the command line you could also be
using curl commands to get more
information about your cluster by giving
curl minus X and then basically pointing
out to your resource manager web UI
address now here I would like to print
out the cluster related metrics and I
could just simply do this which
basically gives me a high level
information of how many applications
were submitted how many are pending what
is the reserved resources what is the
available amount of memory or CPU cores
and all the information similarly you
can be using the same curl commands to
get more information like scheduler
information so you would just replace
the metrics with scheduler and you could
get the information of the different
queues now that's a huge list we can
cancel this and that would give me a
list of all the queues which are
allocated and what are the resources
allocated for each queue you could also
get cluster information on application
IDs and Status running of applications
running in yarn so you would have to
replace the last bit of it and you would
say I would want to look at the
applications and that gives me a huge
list of applications then you can do a
grip and you can be filtering out
specific application related information
similarly you can be looking at the
notes so you can always be looking at
node specific information which gives
you how many nodes you have but this
could be mainly used when you have an
application which wants to or a web
application which wants to use a curl
command and would want to get
information about your cluster from an
HTTP interface now when it comes to app
application we can basically try running
a simple or a sample mapreduce job which
could then be triggered on yarn and it
would use the resources now I can look
at my application here and I can be
looking into my specific directory which
is this one which should have a lot of
files and directories which we have here
now I could pick up one of these and I
could be using a simple example to do
some processing let's take up this file
so there is a file and I could run a
simple word count or I could be running
a hive query which triggers a mapreduce
job I could even run a spark application
which would then show that the
application is running on the cluster so
for example if I would say spark to
Shell now I know that this is an
interactive way of working with spark
but this internally triggers a spark
submit and this runs an application so
here when you do a spark 2 Shell by
default it will contact yarn so it gets
an application ID it is running on yarn
with the master being yarn and now I
have access to the interactive way of
working with spark now if I go and look
into applications I should be able to
see my application which has been
started here and it shows up here so
this is my application
3827 which has been started on yarn and
as of now we can also look into the yarn
UI and that shows me the application
which has been started which basically
has one running container which has one
CPU core allocated 2GB RAM and it's in
progress although we are not doing
anything there so we can always look at
our applications from the yarn UI or as
I mentioned from your applications tab
within yarn Services which gives us the
information and you can even click on
this application to follow and see more
information but you should be given
access to that now this is just a simple
application which I triggered using
spark shell similarly I can basically be
running a map reduce now to run a map
reduce I can say Hadoop jar and that
basically needs a class so we can look
for the default path which is opt
Cloudera Parcels CDH lib Hadoop map
reduce Hadoop map reduce examples and
then we can look at this particular jar
file and if I hit on enter it shows me
the different classes which are part of
this jar and here I would like to use
word count so I could just give this I
could say word count now remember I
could run the job in a particular queue
by giving in an argument here so I could
say minus D mapred dot job dot Q dot
name and then I can point my job to a
particular queue I can even give
different arguments in saying I would
want my mapreduce output to be
compressed or I want it to be showed in
a particular directory and so on so here
I have the word count and then basically
what I can be doing is I can be pointing
it to a particular input path and then I
can have my output which can be getting
stored here again a directory which we
need to choose and I will say output new
and I can submit my job now once I have
submitted my job it connects to resource
manager it basically Gets a Job ID it
gets an application ID it shows you from
where you can track your application you
can always go to the yarn UI and you can
be looking at your application and the
resources it is using so my application
was not a big one and it has already
completed it triggered one map task it
launched one reduce task it was working
on around 12 466 records where you have
then the output of map which is these
many number of output records which was
then taken by combiner and finally by a
reducer which basically gives you the
output so this is my yarn application
which has completed now I could be
looking into the yarn UI and if my job
has completed you might not see your
application here so as of now it shows
up here the word count which I ran it
also shows me my previous Park shell job
it shows me my application is completed
and if you would want further
information on this you can click and go
to the history server if you have been
given access to it or directly go to the
history server web UI where your
application shows up it shows how many
map and reduce tasks it was running you
can click on this particular application
which basically gives you information of
your map and reduce tasks you can look
at different counters for your
application right you can always look at
map specific tasks you can always look
into one particular task what it did on
which node it was running or you can
below looking at the complete
application log so you can always click
on the logs and here you have click here
for full log which gives you the
information and you can always look for
your application which can give you
information of App Master being launched
or you could have search for the word
container so you could see a job which
needs one or multiple containers and
then you could say container is being
requested then you could see container
is being allocated then you can see what
is the container size and then basically
your task moves from initializing to
running in the container and finally you
can even search for release which will
tell you that the container was released
so you can always look into the log for
more information so this is how you can
interact with yarn this is how you can
interact with your command line to look
for more information or using your yarn
web UI or you can also be looking into
your Hue for more information welcome to
scoop tutorial one of the many features
of the Hadoop ecosystem for the Hadoop
file system what's in it for you today
we're going to cover the need for scoop
what is scoop scoop features scoop
architecture scoop import scoop export
scoop processing and then finally we'll
have a little Hands-On demo on scoop so
you can see what it looks like so where
does the need for scoop come in in our
big data Hadoop file system processing
huge volumes of data requires loading
data from diverse sources into Hadoop
cluster you can see here we have our
data processing and this process of
loading data from the heterogeneous
sources comes with a set of challenges
so what are the challenges maintaining
data consistency ensuring efficient
utilization of resources especially when
you're talking about big data we can
certainly use up the resources when
importing terabytes and petabytes of
data over the course of time loading
bulk data to Hadoop was not possible
it's one of the big challenge as it came
up when they first had the Hadoop file
system going and loading data using
script was very slow in other words
you'd write a script in whatever
language you were in and then it would
very slowly load each piece and parse it
in so the solution scoop scooped helped
in overcoming all the challenges to
traditional approach and could lead bulk
data from rdbms to Hadoop very easily so
think your Enterprise server you want to
take the from MySQL or your SQL and you
want to bring that data into your Hadoop
Warehouse your data filing system and
that's where scoop comes in so what
exactly is scoop scoop is a tool used to
transfer bulk of data between Hadoop and
external data stores such as relational
databases and MySQL server or the
Microsoft SQL server or MySQL server so
scoop equals SQL plus Hadoop and you can
see here we have our rdbms all the data
we have stored on there and then your
scoop is the middle ground and it brings
the import into the Hadoop file system
it also is one of the features that goes
out and grabs the data from Hadoop and
exports it back out into an rdbms let's
take a look at scoop features scoop
features has parallel Import and Export
it has import results of SQL query
connectors for all major rdbms databases
Kerberos security integration provides
full and incremental load so we look at
parallel Import and Export scoop uses
yarn yet another resource negotiator
framework to Import and Export data this
provides fault Tolerance on a top of
parallelism scoop allows us to import
the result return from an SQL carry into
the Hadoop file system or the hdfs and
you can see here where the import
results of SQL query come in school
provides connectors for multiple
relational database management system
rdbms's databases such as MySQL and
Microsoft SQL server and it has
connectors for all major rdbms databases
scoop supports Kerberos computer network
Authentication Protocol that allow
allows nodes communicating over a
non-secure network to prove their
identity to one another in a secure
manner scoop can load the whole table or
parts of the table by a single command
hence it supports full and incremental
load let's dig a little deeper into the
scoop architecture we have our client in
this case a hooded wizard behind his
laptop you never know who's going to be
accessing the Hadoop cluster and the
client comes in and sends their command
which goes into scoop the client submits
the import export command to import or
export data data from different
databases is fetched by scoop and so we
have our Enterprise data warehouse
document based systems you have connect
connector for your data warehouse a
connector for document based systems
which reaches out to those two entities
and we have our connector for the rdbms
so connectors help in working with a
range of popular databases multiple
mappers perform map tasks to load the
data onto hdfs the Hadoop file system
and you can see here we have the map
task if you remember from Hadoop Hadoop
is based on mapreduce because we're not
reducing the data we're just mapping it
over it only accesses the mappers and it
opens up multiple mappers to do parallel
processing and you can see here the hdfs
hbase hive is where the target is for
this particular one similarly multiple
map tests will export the data from hdfs
onto rdbms using scoop export command so
just like you can import it you can now
export it using the multiple map
routines scoop import so here we have
our dbms data store and we have the
folders on there so maybe it's your
company's database maybe it's an archive
at Google with all the searches going on
whatever it is usually you think with
scoop you think SQL you think MySQL
server or Microsoft SQL Server that kind
of setup so it gathers the metadata and
you see the scoop import so introspect
database to gather metadata primary key
information and then it submits so you
can see submits map only job remember we
have about mapreduce it only needs the
map side of it because we're not
reducing the data we're just mapping it
over scoop device the input data set
into splits and uses individual map
tests to push the splits into hdfs so
right into the Hadoop file system and
you can see down on the right is kind of
a small depiction of a Hadoop cluster
and then you have scoop export so we're
going to go the other direction and with
the other direction you have your Hadoop
file system storage which is your Hadoop
cluster you have your scoop job and each
one of those clusters then gets a map
mapper comes out to each one of the
computers it has data on it so the first
step is you've got to gather the
metadata so step one you gather the
metadata step two submits map only job
introspect database to gather metadata
primary key information scoop divides
the input data set into splits and uses
individual map tests to push the splits
to rdbms scoop will export Hadoop files
back to rdms tables and you can think of
this in a number of different manners
one of the would be if you're restoring
a backup from the Hadoop file system
into your Enterprise machines there's
certainly many others as far as
exploring data and data science so as we
dig a little deeper into scoop input we
have our connect our jdbc and our URL so
specify the jdbc connect string
connecting manager we specify The
Connection Manager class to use you can
see here driver with the class name
manually specify the jdbc driver class
to use Hadoop map reduce home directory
override Hadoop mapped home username set
authentication username and of course
help print uses instructions and with
the export you'll see that we can
specify the jdbc connect string specify
The Connection Manager class to use
manually specify jdbc driver class to
use you do have to let it know to
override the Hadoop map reduce home and
that's true on both of these and set
authentication username and finally you
can print out all your help set up so
you can see the format for a scoop is
pretty straightforward forward both
Import and Export so let's continue on
our path and look at scoop processing
and what the computer goes through for
that and we talk about school processing
first scoop runs in the Hadoop cluster
it Imports data from the rdbms the nosql
database to the Hadoop file system so
remember it might not be importing the
data from a rdbms it might actually be
coming from a nosql and there's many out
there it uses mappers to slice the
incoming data into multiple formats and
load the data into hdfs it exports data
back into an rdbms while making sure
that the schema of the data in the
database is maintained so now that we've
looked at the basic commands in our
scoop in the scoop processing or at
least the basics as far as theory is
concerned hello Learners simply and
brings you a postgraduate program in
data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below let's jump in and
take
and take
for this demo I'm going to use our
Cloudera quick start if you've been
watching our other demos we've done
you'll see that we've been using that
pretty consistently certainly this will
work in any of your your Horton sandbox
which is also a single node testing
machine Cloudera is one of um there's a
Docker version instead of virtual box
and you can also set up your own Hadoop
cluster plan a little extra time if
you're not an admin it's actually a
pretty significant Endeavor for an admin
if you've been admitting Linux machines
for a very long time and you know a lot
of the commands I find for most admins
it takes them about two to four hours
the first time they go in and create a
virtual machine and set up their own
Hadoop in this case though I mean you're
just learning and getting set up best to
start with Cloudera Cloudera also
includes an installed version of MySQL
that way you don't have to worry install
the the SQL version for importing data
from and two once you're in the Cloudera
quick start you'll see it opens a nice
Centos Linux interface and it has a
desktop setup on there this is really
nice for learnings here not just looking
at command lines and from in here it
should open up by default to Hue if not
you can click on Hue here's a kind of a
fun little web-based interface under Hue
I can go under query I can pick an
editor and we'll go right down to scoop
so now I'm just going to load the scoop
editor in our Hue now I'm going to
switch over and do this all in command
line I just want to show that you can
actually do this in a hue through the
web-based interface the reason I like to
do the command line is specifically on
my computer it runs much quicker or if I
do the command line here and I run it it
tends to have an extra lag or an added
layer in it so for this we're going to
go ahead and open our command line the
second reason I do this is we're going
to need to go ahead and edit our MySQL
so we have something to scoop in other
words I don't have anything going in
there and of course we zoom in we'll
zoom in this and increase the size of
our screen so for this demo our Hands-On
I'm going to use Oracle virtualbox
manager and the Cloudera quick start if
you're not familiar with this we do have
another tutorial we put out and you can
send a note in the YouTube video below
and let our team know and they'll send
you a link or come visit
www.simplylearn.com now this creates a
Linux box on my Windows computer so
we're going to be in Linux and it'll be
the Cloudera version with scoop it will
also be using MySQL MySQL server once
inside the Cloudera virtual box we'll go
under the Hue editor now we're going to
do everything in terminal window I just
want you to be aware that under the Hue
editor you can go under query editor and
you'll see as we come down here here's
our scoop on this so you can run your
Scoop from in here now before we do this
we have to do a little exploration in my
SQL and MySQL server that way we know
what data is coming in so let me go
ahead and open up a terminal window in
Cloudera you have a terminal window at
the top here that you can just click on
and open it up and let me just go ahead
and zoom in on here go View and zoom in
now to get into MySQL server you
typically type in MySQL and this part
will depend on your setup now the
Cloudera quick start comes up that the
username is root and the password is
Cloudera kind of a strange Quirk is that
you can put a space between the minus U
and the root but not between the minus p
and the Cloudera usually you'd put in a
minus capital P and then it prompts you
for your password on here for this demo
I don't worry too much about you knowing
the password on that so we'll just go
right into my SQL Server since this is
the standard password for this quick
start and you can see we're now into
MySQL and we're going to do just a
couple of quick commands in here there's
show databases and you follow by the
semicolon that's standard in most of
these shell commands so it knows it's
the end of your shell command and you'll
see in here in the quick start Cloudera
quick start the MySQL comes with a
standard set of databases these are just
some of these have to do like with the
uzi which is the uzi part of Hadoop
where others of these like customers and
employee fees and stuff like that those
are just for demo purposes they come as
a standard setup in there so that people
going in for the first time have a
database to play with which is really
good for us so we don't have to recreate
those databases and you will see on the
list here we have a retail underscore DB
and then we can simply do uh use retail
underscore DB this will set that as a
default in MySQL and then we want to go
ahead and show the tables and if we show
the tables you can see under the
database the retail DB database we have
categories customers departments order
items orders products so there's a
number of tables in here and we're going
to go ahead and just use a standard SQL
command and if you did our Hive language
you'll note remember it's the same for
hql also on this we're just going to
select star everything from departments
so there's our departments table and
we're going to list everything on the
Departments table and you'll see we have
six lines in here and it has a
department ID and a department name two
for Fitness three for Footwear so on and
so forth now at this point I can just go
ahead and exit but it's kind of nice to
have this data up here so we can look at
it and flip back and forth between the
screens so I'm going to open up another
terminal window and we'll go ahead and
zoom in on this also and it isn't too
important for this particular setup but
it's always kind of fun to know what
your setup you're working with what is
your host name and so we'll go ahead and
just type that in this is a Linux
command and it's uh hostname minus F and
you see we're on quick start Cloudera no
surprise there now this next command is
going to be a little bit longer because
we're going to be doing our first scoop
command and I want to do two of them
we're going to list databases and list
tables it's going to take just a moment
to get through this because there's a
bunch of stuff going on here so we have
scoop we have list databases we have
connect and under the connect command we
need to let it know how we're connecting
we're going to use the jdbc this is a
very standard one jdbc MySQL so you'll
see that if you're doing an SQL database
that's how you start it off with and
then the next part this is where you
have to go look it up it's however it
was created so if your admin created a
MySQL server with a certain setup that's
what you have to go by and you'll see
that usually they list this as localhost
so you'll see something like localhost
sometimes there's a lot of different
formats but the most common is either
localhost or the actual connection so so
in this case we want to go ahead and do
quick start
3306 and so quick start is the name of
the localhost database and how it's
hosted on here and when you set up the
quick start for for Hadoop under
Cloudera it's Port 3306 is where that's
coming in so that's where all that's
coming from and so there's our path for
that and then we have to put in our
password we typically typed password if
you look it up password on the cloud era
quick start is Cloudera and we have to
also let it know the username and again
if you're doing this you'd probably put
in a minus Capital you can actually just
do it for a prompt Cloud for the
password so if you leave that out it'll
prompt you but for this doesn't really
matter I don't care if you see my
password it's the default one for
Cloudera quick start and then the
username on here is simply root and then
we're going to put our semicolon at the
end and so we have here our full setup
and we go ahead and list the databases
and you'll see you may get some warnings
on here I haven't run the updates on the
quick start I suggest you're not running
the updates either if you're doing this
for the first time because it'll do some
reformatting on there and it quickly
pops up and you can see here's all of
our the tables we went in there and if
we go back to on the previous window we
should see that these tables match so
here we come in and here we have our
databases and you can see back up here
where we had the CM customers employees
and so on so the databases match and
then we want to go ahead and list the
tables for a specific database so let's
go ahead and do that I'm a very lazy
typist so I'll put the up arrow in and
you can see here scoop list databases
we're just going to go back and change
this from databases to list tables so we
want to list the tables in here same
connection so most the connection is the
same except we need to know which tables
we're listing an interesting fact is you
can create a table without being under a
database so if you left this blank it
will show the open tables that aren't
connected directly to a database or
under a database but what we want to do
is right past this last slash on the 33
306 we want to put that retail
underscore DB because that's the
database we're going to be working with
and this will go in there and show the
tables listed under that database and
here we go we got categories customers
departments order items and products if
we flip back here real quick there it is
the same thing we had we had categories
customers departments order items and so
on and so let's go ahead and run our
first import command and again I'm that
lazy typer so we're going to do scoop
and instead of list tables we want to go
ahead and import so there's our import
command and so once we have our import
command in there then we need to tell it
exactly what we're going to import so
everything else is the same we're
importing from the retail DB so we keep
that and then at the very end we're
going to tag on dash dash table that
tells it so we can tell it what table
we're importing from and we're going to
import departments
there we go so this is pretty
straightforward because what's nice
about this is you can see the commands
are the same I got the same connection
um I change it for the whatever database
I'm in then I come in here our password
and the username are going to be the
same that's all under the MySQL server
setup and then we let it know what table
we're entering in we run this and this
is going to actually go through the
mapper process in Hadoop so this is a
mapping process it takes the data and it
Maps it up to different parts in the
setup in Hadoop on there and then saves
that data into the Hadoop file system
and it does take it a moment to zip
through which I kind of skipped over for
you since it is running a you know it's
designed to run across the cluster not
on a single node so when you're running
on a single node it's going to run slow
even if you dedicate a couple cores to
it I think I put dedicated four cores to
this one and so you can see right down
here we get to the end it's now mapped
in that information and then we can go
in here we can go under um we can flip
back to our Hue and under Hue on the top
I have there's databases the second icon
over is your Hadoop file system and we
can go in here and look at the Hadoop
file system and you'll see it show up
underneath our documents there it is
departments Cloudera departments and you
can see there's always a delay when I'm
working in Hue which I don't like and
that's the quick start issue that's not
necessarily running out on a server when
I'm running it on a server you pretty
much have to run through some kind of
server interface I still prefer the
terminal window it still runs a lot
quicker but we'll flip back on over here
to the command line and we can do the
Hadoop type in the Hadoop fs and then
list minus LS and if we run this you'll
see underneath our Hadoop file system
there is our departments which has been
added in and we can also do Hadoop fs
and this is kind of interesting for
those who've gone through the Hadoop
file system everything you'll you'll
recognize this on here I'm going to list
it the contents of departments and
you'll see underneath departments we
have part part m0001
002003 and so this is interesting
because this is how Hadoop saves these
files this is in the file system this is
not in Hive so we didn't directly import
this into Hive we put this in the Hadoop
file system depending on what you're
doing you would then write the schema
for Hive to look at the Hadoop file
system certainly visit our Hive tutorial
for more information on hive specific uh
so you can see in here are different
files that it forms that are part of
departments and we can do something like
this we can look at the contents of one
of these files FS minus LS or a number
of the files and we'll simply do the
full path which is user Cloudera and
then we already know the next one is
departments and then after departments
we're going to put slash part star so
this is going to say anything that has
part in it so we have part Dash m000 and
so on we can go ahead and cat use that
cat command or that list command to
bring those up and then we can use the
cat command to actually display the
contents and that's a a Linux command
Hadoop Linux command to catinate not to
be confused with catatonic catastrophic
there's a lot of cat got your tongue and
we see here Fitness Footwear apparel
that should look really familiar because
that's what we had in our MySQL server
when we went in here we did a select all
on here there it is Fitness Footwear
apparel golf outdoors and fan shop and
then of course it's really important
slip back on over here to be able to
tell it where to put the data so we go
back to our import command so here's our
scoop import we have our connect we have
the DB underneath our connection our
MySQL server we have our password our
username the table going where it's
going to I mean the table where it's
coming from and then we can add a Target
on here we can put in Target Dash
directory and you do have to put the
full path that's a Hadoop thing it's a
good practice to be in and we're going
to add it to Department we'll just do
Department one and so here we now add a
Target directory in here in user
Cloudera
this will take just a moment before so
I'll go ahead and skip over the process
since it's going to run very slowly it's
only running on like I said a couple
cores and it's also on a single node and
now we can do the uh Hadoop let's just
do the up Arrow file system list we want
just straight list and when we do the
Hadoop file system minus LS or list
you'll see that we now have Department
one and we can of course do a list
Department one and you can see we have
the files inside Department one and they
mirrored what we saw before with the
same files in there and the part mm0 and
so on if we were to look at them it'd be
the same thing we did before with the
cat so except instead of departments
we'd be Department one there we go one
thing that's going to come up with the
same data we had before now one of the
important things when you're importing
data and it's always a question to ask
is do you filter the data before it
comes in do we want to filter this data
as it comes in so we're not storing
everything in our file system you would
think Hadoop Big Data put it on all in
there I know from experience that
putting it all in there can turn a
couple hundred terabytes into a petabyte
very rapidly and suddenly you're having
to really add on to that data store and
you're storing duplicate data sometimes
so you really need to be able to filter
your data out and so let's go ahead and
use our up Arrow to go to our last
import since it's still a lot the same
stuff so we have all of our commands
under import we have the target we're
going to change this to Department two
so we're going to create a new directory
for this one and then after departments
there's another command that we didn't
really slide in here and that's our
mapping and I'll show you what this
looks like in a minute but we're going
to put M3 in there that doesn't have
nothing to do with the filtering I'll
show you that in a second though what
that's for and we just want to put in
where so where and what is the where in
this case we want to know where
Department ID and if you want to know
where that came from we can flip back on
over here we have Department underscore
IDs this is where that coming from
that's just the name of the column on
here so we come in here to Department ID
is greater than four simple logic there
you can see where you'd use that for
maybe creating buckets for ages uh you
know age from 10 to 15 20 to 30. you
might be looking for I mean there's all
kinds of reasons why you could use the
where command on here in filter
information out maybe you're doing word
counting and you want to know words that
are used less than a hundred times you
want to get rid of the and is and and
all the stuff that's used over and over
again so we'll go ahead and put the
where and then Department ID is greater
than four we'll go ahead and hit enter
on here and this will create our
department to set up on this and I'll go
ahead and skip over some of the runtime
again it runs really slow on a single
node a real quick page through our
commands
here we go our list and we should see
underneath the list the department two
on here now and there it is Department
two and then I can go ahead and do list
Department two you'll see the contents
in here and you'll see that there is
only three maps and it could be that the
data created three Maps but remember I
set it up to only do three mappers so
there's zero one and two and we can go
ahead and do a cat on there remember
this is Department two so we want to
look at all the contents of these three
different files and there it is it's
greater than four so we have golf is
five outdoor six uh fan shop is seven so
we've effectively filtered out our data
and just storing the data we want on our
file system so if you're going to store
data on here the next stage is to export
the data remember a lot of times you
have MySQL server and we're continually
dumping that data into our long-term
storage and access the Hadoop file
system but what happens when you need to
pull that data out and restore a
database or uh maybe you have um you
just merged with a new company a
favorite topic merging companies
emerging databases that's listed under
Nightmare and how many different names
for company can you have so you can see
where being able to export is also
equally important and let's go ahead and
do that and I'm going to flip back over
to my SQL Server here and we'll need to
go ahead and create our database we're
going to export into now I'm not going
to go too much in detail on this command
we're simply creating a table and the
table is going to have it's pretty much
the same table we already have in here
from departments but in this case we're
going to create a table called Dept so
it's the same setup but it's it's just
gonna we're just giving a different name
a different schema and so we've done
that and we'll go ahead and do a select
star from Dept there we go and it's
empty that's what we expect a new
database a new data table and it's empty
in there so now we need to go ahead and
Export our data that we just filtered
out into there so let's flip back on
over here to our our scoop setup which
is just our Linux terminal window and
let's go back up to one of our commands
here's scoop Import in this case instead
of import we're going to take the scoop
and we're going to export so we're going
to just change that export and the
connection is going to remain the same
so same connect same database we're also
we're still doing the retail DB we have
the same password so none of that
changes the big change here is going to
be the table instead of departments
remember we changed it and gave it a new
name and so we want to change it here
also Dept so Department we're not going
to worry about the mapper count and the
where was part of our import there we go
and then finally it needs to know where
to export from so instead of Target
directory we have an export directory
that's where it's coming from still user
Cloudera and we'll keep it as Department
two just so you can see how that data is
coming back with the that we've filtered
in and let's go ahead and run this and
I'll take it just a moment to go through
in steps and again because it's below
I'm just going to go ahead and skip this
so you don't have to sit through it and
once we've wrapped up our export we'll
flip back on over here to mySQL use the
up arrow and this time we're going to
select star from department and we can
see that there it is it exported the
golf outdoors and fan shop and you can
imagine also that you might have to use
the where command in your export also so
there's a lot of mixing the command line
for scoop is pretty straightforward
you're changing the different variables
in there whether you're creating a table
listing a table listing databases very
powerful tool for bringing your data
into the Hadoop file system and
exporting it so now that we've wrapped
up our demo on scoop and gone through a
lot of basic commands let's dive in with
a brief history of Hive so the history
of Hive begins with Facebook Facebook
begin using Hadoop as a solution to
handle the growing big data and we're
not talking about a data that fits on
one or two or even five computers uh are
talking due to the fifth sign if you've
looked at any of our other Hadoop
tutorials you'll know we're talking
about very big data and data pools and
Facebook certainly has a lot of data it
tracks as we know the Hadoop uses map
reduce for processing data mapreduce
required users to write long codes and
so you'd have these really extensive
Java codes very complicated for the
average person to use not all users
reversed in Java and other coding
languages this proved to be a
disadvantage for them users were
comfortable with writing queries in SQL
SQL has been around for a long time the
standard SQL query language Hive was
developed with the vision to incorporate
the concepts of tables columns just like
SQL so why Hive well the problem was for
processing and analyzing data users
found it difficult to code as not all of
them were well versed with the coding
languages you have your processing you
have your analyzing and so the solution
was required a language similar to SQL
which was well known to all the users
and thus the The Hive or hql language
evolved what is Hive Hive is a data
warehouse system which is used for
querying and analyzing large data sets
stored in the hdfs or the Hadoop file
system Hive uses a query language that
we call Hive ql or hql which is similar
to SQL so if we take our user the user
sends out their hive queries and then
that is converted into a mapreduce tasks
and then accesses the Hadoop mapreduce
system let's take a look at the
architecture of Hive architecture of
Hive we have the hive client so that
could be the programmer or maybe it's a
manager who knows enough SQL to do a
basic query to look up the data they
need the hive client supports different
types of client applications in
different languages prefer for
performing queries and so we have our
Thrift application in the hive Thrift
client Thrift is a software framework
Hive server is based on Thrift so it can
serve the request from all programming
language that support threat and then we
have our jdbc application and the hive
jdbc driver jdbc Java database
connectivity jdbc application is
connected through the jdbc driver and
then you have the odbc application or
the hive odbc driver the odbc or open
database connectivity the odbc
application is connected through the
odbc driver with the growing development
of all of our different scripting
languages python C plus plus spark Java
you can find just about any connection
in any of the main scripting languages
and so we have our Hive Services as we
look at deeper into the architecture
Hive supports various services so you
have your Hive server basically your
Thrift application or your hive Thrift
client or your jdbc or your hive jdbc
driver your odbc application or your
hive odbc driver they all Connect into
The Hive server and you have your hive
web interface you also have your CLI now
the hive web interface is a GUI is
provided to execute Hive queries and
we'll actually be using that later on
today so you can see kind of what that
looks like and get a feel for what that
means commands are executed directly in
CLI and then the CLI is a direct
terminal window and I'll also show you
that too so you can see how those two
different interfaces work these then
push the code into the hive driver Hive
driver is responsible for all the
queries submitted so everything goes
through that driver let's take a closer
look at the hive driver The Hive driver
now performs three steps internally one
is a compiler Hive driver passes query
to compiler where it is checked and
analyzed then the optimizer kicks in and
the optimized logical plan in the form
of a graph of mapreduce and hdfs tasks
is obtained and then finally in the
executor in the final step the tasks are
executed when we look at the
architecture we also have to note the
metastore metastore is a repository for
five metadata stores metadata for Hive
tables and you can think of this as your
schema and where is it located and it's
stored on the Apache Derby DB processing
and resource management is all handled
by the mapreduce V1 you'll see mapreduce
V2 the yarn and the teds these are all
different ways of managing these
resources depending on what version of
Hadoop you're in Hive uses mapreduce
framework to process queries and then we
have our distributed storage which is
the hdfs and if you looked at our Hadoop
tutorials you'll know that these are on
commodity machines on our linearly
scalable that means they're very
affordable a lot of time when you're
talking about Big Data you're talking
about a tenth of the price of storing it
on Enterprise computers and then we look
at the data flow And Hive so in our data
flow And Hive we have our Hive in the
Hadoop system and underneath the user
interface or the UI we have our driver
our compiler our execution engine and
our metastore that all goes into the map
reduce and the Hadoop file system so
when we execute a query you can see it
coming in here it goes into the driver
step one step two we get a plan what are
we going to do refers to the query
execution uh then we go to the metadata
it's like well what kind of metadata are
we actually looking at where is this
data located what is the schema on it uh
then this that comes back with the
metadata into the compiler then the
compiler takes all that information and
the send plan and returns it to the
driver the driver then sends the execute
plan to the execution engine once it's
in the execution engine the execution
engine acts as a bridge between Hive and
Hadoop to process the query and that's
going into your map reduce and your
Hadoop file system or your hdfs and then
we come back with the metadata
operations it goes back into the
metastore to update or let it know
what's going on which also goes to the
between it's a communication between the
execution engine and the metastore
execution engine Communications is
bi-directionally with the metastore to
perform operations like create drop
tables metastore stores information
about tables and columns so again we're
talking about the schema of your
database and once we have that we have a
bi-directional send results
communication back into the driver and
then we have the fetch results which
goes back to the client so let's take a
little bit look at the hive data
modeling Hive data modeling so you have
your high data modeling you have your
tables you have your partitions and you
have buckets the tables in however
created the same way it is done in rdbms
so when you're looking at your
traditional SQL server or MySQL server
where you might have Enterprise
equipment and a lot of people people
pulling and moving stuff off of there
the tables are going to look very
similar and this makes it very easy to
take that information and let's say you
need to keep current information but you
need to store all of your years of
transactions back into the Hadoop Hive
so you match those those all kind of
look the same the tables are the same
your databases look very similar and you
can easily import them but you can
easily store them into the hive system
partitions here tables are organized
into partitions for grouping same type
of data based on partition key this can
become very important for speeding up
the process of doing queries so if
you're looking at dates as far as like
your employment dates of employees if
that's what you're tracking you might
add a partition there because that might
be one of the key things that you're
always looking up as far as employees
are concerned and finally we have
buckets data present in partitions can
be further divided into buckets for
efficient querying again there's that
efficiency at this level a lot of times
you're you're working with the
programmer in the admin of your Hadoop
file system to maximize the efficiency
of that file system so it's usually a
two-person job and we're talking about
Hive data modeling you want to make sure
that they work together and you're
maximizing your resources Hive data
types so we're talking about Hive data
types we have our primitive data types
and our complex data types A lot of this
would look familiar because it mirrors a
lot of stuff in SQL in our primitive
data types we have the numerical data
types string data type date time data
type and miscellaneous data type and
these should be very they're kind of
self-explanatory but just in case
numerical data is your floats your
integers your short integers all of that
numerical data comes in as a number a
string of course is characters and
numbers and then you have your date time
step and then we have kind of a general
way of pulling your own created data
types in there that's your miscellaneous
data type and we have complex data types
so you can store arrays you can store
maps you can store structures and even
units in there as we dig into Hive data
types and we have the primitive data
types and the complex data types so we
look at primitive data types and we're
looking at numeric data types data types
like an integer a float a decimal those
are all stored as numbers in the hive
data system a string data type data
types like characters and strings you
store the name of the person you're
working with you know John Doe the city
um
State Tennessee maybe it's Boulder
Colorado USA or maybe it's hyperbad
India that's all going to be string it's
stored as a string character and of
course we have our date time data type
data types like time stamp date interval
those are very common as far as tracking
cells anything like that so you just
think if you can type a stamp of time on
it or maybe you're dealing with the race
and you want to know the interval how
long did the person take to complete
whatever task it was all that is date
time data type and then we talked
miscellaneous data type these are like
Boolean in binary and when you get into
Boolean and binary you can actually
almost create anything in there but your
yes nose zero one now let's take a look
at complex data types a little closer we
have arrays so your syntax is of data
type and it's an array and you can just
think of an array as a collection of
same in
one two three four if they're all
numbers and you have Maps this is a
collection of key value pairs so
understanding Maps is so Central to
Hadoop so we store Maps you have a key
which is a set you can only have one key
per mapped value and so you can in
Hadoop of course you collect the same
keys and you can add them all up or do
something with all the contents of the
same key but this is our map as a
primitive type data type in our
collection of key value Pairs and then
collection of complex data with comment
so we can have a structure we have a
column name data type comment column
comment uh so you can get very
complicated structures in here with your
collection of data in your commented
setup and then we have units and this is
a collection of heterogeneous data types
so the Syntax for this is Union type
data type data typed and so on so it's
all going to be the same a little bit
different than the arrays where you can
actually mix and match different modes
of Hive Hive operates in two modes
depending on the number and size of data
nodes we have our local mode and our map
reduce mode we
took the local mode it is used when
Hadoop is having one data node and the
data is small processing will be very
fast in a smaller data sets which are
present in local machine and this might
be that you have a local file stuff
you're uploading into the hive and you
need to do some processes in there you
can go ahead and run those High
processes and queries on it usually you
don't see much in the way of a single
node Hadoop system if you're going to do
that you might as well just use like an
SQL database or even a Java sqlite or
something python ssqlite so you don't
really see a lot of single node Hadoop
databases but you do see the local mode
in Hive where you're working with a
small amount of data that's going to be
integrated into the larger database and
then we have the map reduce mode this is
used when Hadoop is having multiple data
nodes and the data is spread across
various data nodes processing large data
sets can be more efficient using this
mode and this you can think of instead
of it being one two three or even five
computers we're usually talking with the
Hadoop file system we're looking at 10
computers 15 100 where this data is
spread across all those different Hadoop
nodes difference between Hive and rdbms
remember rdbms stands for the relational
database manager
let's take a look at the difference
between Hive and the rdbms with Hive
Hive enforces schema on read and it's
very important that whatever is coming
in that's when hive's looking at it and
making sure that it fits the mod the
rdbms enforces a schema when it actually
writes the data into the database so
it's read the data and then once it
starts to write it that's where it's
going to give you the error or tell you
something's incorrect about your scheme
Hive data size is in petabytes that is
hard to imagine you know we're looking
at your personal computer on your desk
maybe you have 10 terabytes if it's a
high-end computer we're talking
petabytes so that's hundreds of
computers grouped together when a rdbms
data size is in terabytes very rarely do
you see an rdbms system that's spread
over more than five computers and
there's a lot of reasons for that with
the rdbms it actually has a high-end
amount of rights to the hard drive
there's a lot more going on there your
writing and pulling stuff so you really
don't want to get too big with an RDA
BMS or you're going to run into a lot of
problems with Hive you can take it as
big as you want Hive is based on the
notion of write once and read many times
this is so important and they call it
worm which is right w you once o read R
many times M they refer to it as worm
and that's true of any of you a lot of
your Hadoop setup it's it's altered a
little bit but in general we're looking
at archiving data that you want to do
data analysis on we're looking at
pulling all that stuff off your rdbms
from years and years and years of
business or whatever your company does
or scientific research and putting that
into a huge data pool so that you can
now do queries on it and get that
information out of it with the rdbms
it's based on the notion of read and
write many times
so you're continually updating this
database you're continually bringing up
new stuff new sales the account changes
because they have a different licensing
now whatever software you're selling all
that kind of stuff where the data is
continually fluctuating and then Hive
resembles a traditional database by
supporting SQL but it is not a database
it is a data warehouse this is very
important it goes with all the other
stuff we've talked about that we're not
looking at a database but a data
warehouse to store the data and still
have fast and easy access to it for
doing queries you can think of Twitter
and Facebook they have so many posts
that are archived back historically
those posts aren't going to change they
made the post they're posted they're
there and they're in their database but
they have to store it in a warehouse in
case they want to pull it back up with
the rdbms it's a type of database
management system which is based on the
relational model of data and then with
Hive easily scalable at a low cost again
we're talking maybe a thousand dollars
per terabyte
um the rdbms is not scalable at a low
cost when you first start on the lower
end you're talking about 10 000 per
terabyte of data including all the
backup on the models and and all the
added Necessities to support it as you
scale it up you have to scale those
computers and Hardware up uh so you
might start off with a basic server and
then you upgrade to a sun computer to
run it and you spend you know tens of
thousands of dollars for that Hardware
upgrade with Hive you just put another
computer into your Hadoop file system so
let's look at some of the features of
Hive when we're looking at the features
of Hive we're talking about the use of
SQL like language called Hive ql a lot
of times you'll see that as hql which is
easier than long codes this is nice if
you're working with your shareholders
you come to them and you say Hey you can
do a basic SQL query on here and pull up
the information you need this way you
don't have to take all of your
programmers jump in every time they want
to look up something in the database
they actually now can easily do that if
they're not skill build in programming
and script writing tables are used which
are similar to The rdbms hence easier to
understand and one of the things I like
about this is when I'm bringing tables
in from a MySQL server or SQL Server
there's almost a direct reflection
between the two so when you're looking
at one which is a data which is
continually changing and then you're
going into the archive database it's not
this huge jump where you have to learn a
whole new language you mirror that same
schema into the hdfs into the hive
making it very easy to go between the
two and then using Hive ql multiple
users can simultaneously query data so
again you have multiple clients in there
and they send in their query that's also
true with the rdbms which kind of cues
them up because it's running so fast you
don't notice the lag time well you get
that also with the hql because you add
more computers in the query can go very
quickly depending on how many computers
and how much resources each machine has
to pull the information and Hive
supports a variety of data types
so with Hive it's designed to be on the
Hadoop system which you can put almost
anything into the Hadoop file system so
with all that let's take a look at a
demo on hive ql or hql before I dive
into the Hands-On demo let's take a look
at the website
hive.apache.org that's the main website
since Apache it's an Apache open source
software this is the main software for
the main site for the build and if you
go in here you'll see that they're
slowly migrating Hive into beehive and
so if you see beehive versus Hive note
The Beehive is a new release is coming
out that's all it is it reflects a lot
of the same functionality of Hive it's
the same thing and then we like to pull
up some kind of documentation on
commands and for this I'm actually going
to go to hortonworks Hive cheat sheet
and that's because Horton works when
Cloudera 2 the most common used builds
for Hadoop and for which include High
having all the different Tools in there
and so hortonworks has a pretty good PDF
you can download cheat sheet on there I
believe Cloudera does too but we'll go
ahead and just look at the Horton one
because it's the one that comes up
really good and you can see when we look
at the query language it Compares my SQL
Server to Hive ql or hql and you can see
the basic select we select from columns
from table where conditions exist you
know most basic command on there and
they have different things you can do
with it just like you do with your SQL
and if you scroll down you'll see data
types so here's your integer your flow
your binary double string timestamp and
all the different data types you can use
some different semantics different Keys
features functions for running a hive
query command line setup and of course
the hive shell setup in here so you can
see right here if we Loop through it it
has a lot of your basic stuff and it's
we're basically looking at SQL across a
Horton database we're going to go ahead
and run our Hadoop cluster hive demo and
I'm going to go ahead and use the
Cloudera quick start this is in the
virtual box so again we have an oracle
virtual box which is open source and
then we have our Cloudera quick start
which is the Hadoop setup on a single
node now obviously Hadoop And Hive are
designed to run across a cluster of
computers so we talk about a single node
is for Education testing that kind of
thing and if you have a chance you can
always go back and look at our demo we
had on setting up a Hadoop system in a
single cluster to set a note Down Below
in the YouTube video and our team will
get in contact with you and send you
that link if you don't already have it
or you can contact us at the
www.simplylearn.com now in here it's
always important to note that you do
need on your computer if you're running
on Windows because I'm on a Windows
machine you're going to need probably
about 12 gigabytes to actually run this
it used to be goodbye with a lot less
but as things have evolved they take up
more and more resources and you need the
professional version if you have the
home version I was able to get that to
run but boy did it take a lot of extra
work to get the home version to let me
use the virtual setup on there and we'll
simply click on the Cloudera quick start
and I'm going to start that up and this
is starting up our Linux so we have our
Windows 10 which is a computer I'm on
and then I have the virtual box which is
going to have a Linux operating system
in it and we'll skip ahead so you don't
have to watch the whole install
something interesting to know about the
Cloudera is that it's running on Linux
cintos and for whatever reason I've
always had to click on it and hit the
escape button for it to spin up and then
you'll see the Dos come in here now that
our Cloudera spun up on our virtual
machine with the Linux on we can see
here we have our it uses the Thunderbird
browser on here by default and
automatically opens up a number of
different tabs for us and a quick note
because I mentioned like the
restrictions on getting set up on your
own computer if you have a home edition
computer and you're worried about
setting it up on there you can also go
in there and spin up a one month free
service on Amazon web service to play
with this so there's other options
you're not stuck with just doing it on
the quick start menu you can spin this
up in many other ways now the first
thing we want to note is that we've come
in here into Cloudera and I'm going to
access this in two ways the first one is
we're going to use Hue and I'm going to
open up Hue and I'll take it a moment to
load from the setup on here and Hue is
nice if I go in and use Hue as an editor
into Hive or into the Hadoop setup
usually I'm doing it as a from an admin
side because it has a lot more
information a lot of visuals less to do
with you know actually diving in there
and just executing code and you can also
write this code into files and scripts
and there's other things you can other
ways you can upload it into Hive but
today we're going to look at the command
lines and we'll upload it into Hue and
then we'll go into and actually do our
work in a terminal window Under The Hive
shell now in the Hue browser window if
you go under query and click on the pull
down menu and then you go under editor
and you'll see Hive there we go there's
our Hive setup I go and click on hive
and this will open up our query down
here and now it has a nice little B that
shows our Hive going and we can go
something very simple down here like
show databases and we follow it with the
semicolon and that's the standard in
Hive is you always add our punctuation
at the end there and I'll go ahead and
run this and the query will show up
underneath and you'll see down here
since this is a new quick start I just
put on here you'll see it has the
default down here for the databases
that's the database name I haven't
actually created any databases on here
and then there's a lot of other like
assistant function tables your databases
up here there's all kinds of things you
can research you can look at through Hue
as far as a bigger picture the downside
of this is it always seems to lag for me
whenever I'm doing this I always seem to
run slow so if you're in Cloudera you
can open up a terminal window they
actually have an icon at the top you can
also go under applications and under
applications system tools and terminal
either one will work it's just a regular
terminal window and this terminal window
is now running underneath our Linux so
this is a Linux terminal window or on
our virtual machine which is resting on
our regular Windows 10 machine and we'll
go ahead and zoom this in so you can see
the text better on your own video and I
simply just clicked on view and zoom in
and then all we have to do is type in
Hive and this will open up the shell on
here and it takes it just a moment to
load when starting up Hive I also want
to note that depending on your rights on
the computer you're on in your action
you might have to do pseudohive and put
in your password and username most
computers are usually set up with the
hive login again it just depends on how
you're accessing the Linux system and
the hive shell once we're in here we can
go ahead and do a simple hql command
show databases and if we do that we'll
see here that we don't have any
databases so so we can go ahead and
create a database and we'll just call it
office for today for this moment now if
I do show we'll just do the up Arrow up
arrow is a hotkey that works in both
Linux and in Hive so I can go back and
paste through all the commands I've
typed in and we can see now that I have
my there's of course the default
database and then there's the office
database so now we've created a database
it's pretty quick and easy and we go
ahead and drop the database we can do
drop Database Office now this will work
on this database because it's empty if
your database was not empty you would
have to do Cascade and that drops all
the tables in the database and the
database itself now if we do show
database and we'll go ahead and recreate
our database because we're going to use
the office database for the rest of this
Hands-On demo a really handy command to
Now set with the SQL or hql is to use
office and what that does is that sets
office as the default database so
instead of having to run reference the
database every time we work with a table
we now automatically assumes that's the
database being used whatever tables
we're working on the difference is you
put the database name period table and
I'll show you in just a minute what that
looks like and how that's different if
we're going to have a table and a
database we should probably load some
data into it so let me go ahead and
switch gears here and open up a terminal
window you can just open another
terminal window and it'll open up right
on top of the one that you have Hive
shell running in and when we're in this
terminal window first we're going to go
ahead and just do a list which is of
course a Linux command you can see all
the files I have in here this is the
default load we can change directory to
documents we can list in documents and
we're actually going to be looking at
employee.csv a Linux command is the cat
you can use this actually to combine
documents there's all kinds of things
that cat does but if we want to just
display the contents of our
employee.csv file we can simply do cat
employee CSV and when we're looking at
this we want to know a couple things one
there's a line at the top okay so the
very first thing we notice is that we
have a header line the next thing we
notice is that the data is comma
separated and in this particular case
you'll see a space here generally with
these you got to be real careful with
spaces there's all kinds of things you
got to watch out for because it can
cause issues these spaces won't because
these are all strings that the space is
connected to if this was a space next to
the integer you'd get a null value that
comes into the database without doing
something extra in there now with most
of Hadoop that's important to know that
you're writing the data once reading it
many times and that's true of almost all
your Hadoop things coming in so you
really want to process the data before
it gets into the database and for those
who of you have studied data
transformation that's the adult where
you extract transfer form and then load
the data so you really want to extract
and transform before putting it into the
hive then you load it into the hive with
the transform data and of course we also
want to note the schema we have an
integer string string integer integer so
we kept it pretty simple in here as far
as the way the data is set up the last
thing that you're going to want to look
up is the source since we're doing local
uploads we want to know what the path is
we have the whole path in this case it's
home slash Cloudera slash documents and
these are just text documents we're
working with right now we're not doing
anything fancy so we can do a simple git
edit
employee.csv and you'll see it comes up
here it's just a text document so I can
easily remove these added spaces there
we go and then we go and just save it
and so now it has a new setup in there
we've edited it the G edit is usually
one of the default that loads into Linux
so any text editor will do back to the
hive shell so let's go ahead and create
a table employee and what I want you to
note here is I did not put the semicolon
on the end here semicolon tells it to
execute that line so this is kind of
nice if you're you can actually just
paste it and if you have it written on
another sheet and you can see right here
where I have create table employee and
it goes into the next line on there so I
can do all of my commands at once now
just I don't have any typo errors I went
ahead and just pasted the next three
lines in and the next one is our schema
if you remember correctly from the other
side we had the different values in here
which was ID name Department year of
joining and salary and the ID is an
integer name is a string department
string air joining energy salary an
integer and they're in Brackets we put
close brackets around them and you could
do this all as one line and then we have
row format delimited Fields terminated
by comma and this is important because
the default is tabs so if I do it now it
won't find any terminated Fields so
you'll get a bunch of null values loaded
into your table and then finally our
table properties we want to skip the
header line count equals one now this is
a lot of work for uploading a single
file it's kind of goofy when you're
uploading a single file that you have to
put all this in here but keep in mind
Hive and Hadoop is designed for writing
many files into the database you write
them all in there and then you can
they're saved it's an archive it's a
data warehouse and then you're able to
do all your queries on them so a lot of
times we're not looking at just the one
file coming up we're loading hundreds of
files you have your reports coming off
of your main database all those reports
are being loaded and you have your log
files you have I mean all this different
data is being dumped into Hadoop and in
this case Hive on top of Hadoop and so
we need to let it know hey how do I
handle these files coming in and then we
have the semicolon at the end which lets
us know to go ahead and run this line
and so we'll go ahead and run that and
now if we do a show tables you can see
there's our employee on there we can
also describe if we do describe employee
you can see that we have our ID integer
name string department string year of
joining integer and salary integer and
then finally let's just do a select star
from employee very basic SQL and hql
command selecting data and it's going to
come up and we haven't put anything in
it so as we expect there's no data in it
so if we flip back to our Linux terminal
window you can see where we did the cat
employee.csv and you can see all the
data we expect to come into it and we
also did our PWD and right here you see
the path you need that full path when
you are loading data you know you can do
a browse and if I did it right now with
just the employee.csv is a name it will
work but that is a really bad habit in
general when you're loading data because
it's you don't know what else is going
on on the computer you want to do the
full path almost in all your data loads
so let's go ahead and flip back over
here to our Hive shell we're working in
and the command for this is load data so
that says hey we're loading data that's
a hive command hql and we want local
data so you got to put down local in
path so now it needs to know where the
path is now to make this more legible
I'm just going to go ahead and hit enter
then we'll just paste the full path in
there which I have stored over on the
side like a good prepared demo and
you'll see here we have home Cloudera
documents employee.csv so it's a whole
path for this text document in here and
we go ahead and hit enter in there and
then we have to let it know where the
data is going so now we have a source
and we need a destination and it's going
to go into the table and we'll just call
it employee we'll just match the table
in there and because I wanted to execute
we put the semicolon on the end it goes
ahead and executes all three lines now
if we go back if you remember we did the
select star from employee just using the
up Arrow to page through my different
commands I've already typed in you can
see right here we have as we expect we
have Rose Sam Mike and Nick and we have
all their information showing in our
four rows and then let's go ahead and do
uh select and count we'll just look at a
couple of these different select options
you can do we're going to count
everything from employee now this is
kind of interesting because the first
one just pops up with the basic uh
select because it doesn't need to go
through the full map reduce phase but
when you start doing a count it does go
through the full map reduce setup in the
hive in Hadoop and because I'm doing
this demo on a single node Cloudera
virtual box on top of a Windows 10. all
the benefits of running it on a cluster
are gone and instead is now going
through all those added layers so it
takes longer to run you know like I said
when you do a single node as I said
earlier it doesn't do any good as an
actual distribution because you're only
running it on one computer and then
you've added all these different layers
to run it and we see it comes up with
four and that's what we expect we have
four rows we expect four at the end and
if you remember from our cheat sheet
which we brought up here from Horton's
it's a pretty good one there's all these
different commands we can do we'll look
at one more command where we do the uh
what they call sub queries right down
here because that's really common to do
a lot of sub queries and so we'll do
select star or all different columns
from employee now if we weren't using
the office database it would look like
this from Office dot employee and either
one will work on this particular one
because we have office set as a default
on there so from office employee and
then the command where creates a subset
and in this case we want to know where
the salary is greater than 25
000. there we go and of course we end
with our semicolon if we run this query
you can see it pops up and there's our
salaries the people top earners we have
rows and it and Mike and HR kudos to
them of course they're fictitional I
don't actually we don't actually have a
rose and a mic in those positions or
maybe we do so finally we want to go
ahead and do is we're done with this
table now remember you're dealing with
the data warehouse so you usually don't
do a lot of dropping of tables and
databases but we're going to go ahead
and drop this table here before we drop
it one more quick note is we can change
it so what we're going to do is we're
going to alter table office employee and
we want to go ahead and rename it
there's some other commands you can do
in here but rename is pretty common and
we're going to rename it to and it's
going to stay in office and it turns out
one of our shareholders really doesn't
like the word employee he wants
employees plural it's a big deal to him
so let's go ahead and change that name
for the table it's that easy because
it's just changing the meta data on
there and now if we do show tables
you'll see we now have employees not
employee and then at this point maybe
we're doing some house cleaning because
this is all practice so we're going to
go ahead and drop the table and we'll
drop table employees because we changed
the name in there so if we did employee
just give us an error and now if we do
show tables you'll see all the tables
are gone now the next thing we want to
go and take a look at and we're going to
walk back through the loading of data uh
just real quick because we're going to
load two tables here and let me just
float back to our terminal window so we
can see what those tables are that we're
loading and so up here we have customer
we have a customer file and we have an
order file we want to go ahead and put
the customers and the orders into here
so those are the two we're doing and of
course it's always nice to see what
you're working with so let's do our cat
customer.csv we could always do G edit
but we don't really need to edit these
we just want to take a look at the data
in customer and important in here is
again we have a header so we have to
skip a line comma separated nothing odd
with the data we have our schema which
is integer string integer string integer
so you know you'd want to take that note
that down or flip back and forth when
you're doing it and then let's go ahead
and do cat
order.csv and we can see we have oid
which I'm guessing is the order ID we
have a date up something new we've done
integers and strings but we haven't done
date when you're importing new and you
never worked with the date dates always
one of the more trickier fields to port
in and that's true of just about any
scripting language I've worked with all
of them have their own idea of how dates
supposed to be formatted what the
default is this particular format or its
year and it has all four digits Dash
month two digits Dash day is the
standard import for the hive so if
you'll have to look up and see what the
different formats are if you're going to
do a different format in there coming in
or you're not able to pre-process the
data but this would be a preprocessing
of the data thing coming in if you
remember correctly from our adult which
is uh e just in case you weren't able to
hear me last time
ETL which stands for extract transform
then load so you want to make sure
you're transforming this data before it
gets into here and so we're going to go
ahead and bring both this data in here
and really we're doing this so we can
show you the basic join there is if you
remember from our setup merge join all
kinds of different things you can do but
joining different data sets is so common
so it's really important to know how to
do this we need to go ahead and bring in
these two data sets and you can see
where I just created a table customer
here's our schema the integer name age
address salary here's our delimited by
commas and our table properties where we
skip a line well let's go ahead and load
the data first and then we'll do that
with our order and let's go ahead and
put that in here and I've got it split
into three lines you can see it easily
we've got load data local in path so we
know we're loading data we know it's
local and we have the path here's the
complete path for oops this is supposed
to be order CSV grab the wrong one of
course it's going to give me errors
because you can't recreate the same
table on there and here we go create
table here's our integer date customer
the basic setup that we had coming in
here for our schema row format commas
table properties skip header line and
then finally let's load the data into
our order table load data local in path
home Cloudera documents ordered at CSV
into table order now if we did
everything right we should be able to do
select star from customer and you can
see we have all seven customers and then
we can do select star from order and we
have four orders so this is just like a
quick frame we have you know a lot of
times when you have your customer
databases in business you have thousands
of customers from years and years and
some of them you know they move they
close their business they change names
all kinds of things happen so we want to
do is we want to go ahead and find just
the information connected to these
orders and who's connected to them and
so let's go ahead and do it's a select
because we're going to display
information so select and this is kind
of interesting we're going to do c dot
ID and I'm going to Define c as customer
as a customer table in just a minute
then we're going to do c dot name and
again we're going to define the c c dot
age so this means from the customer we
want to know their ID their name their
age and then you know I'd also like to
know the order amount so let's do o for
DOT amount and then this is where we
need to go ahead and Define what we're
doing and I'll go and capitalize from
customer so we're going to take the
customer table in here and we're going
to name it C that's where the C comes
from so that's the customer table C and
we want to join order as o that's where
our o comes from so the O DOT amount is
what we're joining in there and then we
want to do this on we got to tell it how
to connect the two tables C dot ID
equals o Dot customer underscore ID so
now we know how they're joined and now
remember we have seven customers in here
we have four orders and as it processes
we should get a return of four different
names joined together and they're joined
based on of course the orders on there
and then once we're done we now have the
order number the person who made the
order their age and the amount of the
order which came from the order table uh
so you have your different information
and you can see how the join works here
very common use of tables and hql and
SQL and let's do one more thing with our
database and then I'll show you a couple
other Hive commands and let's go ahead
and do a drop and we're going to drop
Database Office and if you're looking at
this and you remember from earlier this
will give me an error and this is see
what that looks like it says fill to
execute exception one or more tables
exist so if you remember from before you
can't just drop a database unless you
tell it to Cascade that lets it no I
don't care how many tables are in it
let's get rid of it and in Hadoop since
it's an art it's a warehouse a data
warehouse you usually don't do a lot of
dropping maybe at the beginning when
you're developing the schemas and you
realize you messed up you might drop
some stuff but down the road you're
really just adding commodity machines to
take up so you can store more stuff on
it so you usually don't do a lot of
database dropping and some other fun
commands to know is you can do select
round 2.3 as round value you can do a
round off in Hive we can do as floor
value which is going to give us a 2 so
it turns it into an integer versus a
float it goes down you know basically
truncates it but it goes down and we can
also do ceiling which is going to round
it up so we're looking for the next
integer above there's a few commands we
didn't show in here because we're on a
single node as as an admin to help
speciate the process you usually add in
partitions for the data and buckets you
can't do that on a single node because
the when you add a partition it
partitions it across separate nodes but
beyond that you can see that it's very
straightforward we have SQL coming in
and all your basic queries that are in
SQL are very similar to hql let's get
started with pig why Pig what is pig map
reduce versus Hive versus pig hopefully
you've had a chance to do our Hive
tutorial and our map reduce tutorial if
you haven't send a note over to Simply
learn and we'll follow up with a link to
you we'll look at Pig architecture
working a pig pig latin data model Pig
execution modes a use case Twitter and
features a pick and then we'll tag on a
short demo so you can see Pig In Action
so why pick as we all know Hadoop uses
mapreduce to analyze and process big
data processing Big Data consumed more
time so before we had the Hadoop system
they'd have to spend a lot of money on a
huge set of computers and Enterprise
machines so he introduced the Hadoop map
reduce and so afterwards processing Big
Data was faster using mapreduce then
what is the problem with map reduce
prior to 2006 all mapreduce programs
were written in Java non-programmers
found it difficult to write lengthy Java
codes they faced issues in incorporating
map sort reduce to fundamentals of
mapreduce while creating a program you
can see here map phase Shuffle and sort
reduce phase eventually it became a
difficult task to maintain and optimize
a code due to which the processing time
increased you can imagine a manager
trying to go in there and needed in a
simple query to find out data and he has
to go talk to the programmers anytime he
wants anything so that was a big problem
not everybody wants to have a on-call
programmer for every manager on their
team Yahoo faced problems to process and
analyze large data sets using Java as a
cause or complex and lengthy there was a
necessity to develop an easier way to
analyze large data sets without using
time-consuming complex Java modes and
codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it was
developed with a vision to analyze and
process large data sets without using
complex Java codes Pig was developed
especially for non-programmers pig used
simple steps to analyze data sets which
was Timely efficient so what exactly is
pick pig is a scripting platform that
runs on Hadoop clusters designed to
process and analyze large data sets and
so you have your pig which uses SQL like
queries they're definitely not SQL but
some of them resemble SQL queries and
then we use that to analyze our data Pig
operates on various types of data like
structured semi-structured and
unstructured data let's take a closer
look at mapreduce versus Hive versus pig
so we start with a compiled language
your map reduce and we have Hive which
is your SQL like query and then we have
pig which is a scripting language it has
some similarities to SQL but it has a
lot of its own stuff remember SQL like
query which is what Hive is based off
looks for structured data and so when
you get into scripting languages like
Pig now we're dealing more with
semi-structured and even unstructured
data with a Hadoop map reduce we have a
need to write long complex codes with
Hive no need to write complex codes you
could put it in a simple SQL query or
hql Hive ql and in pig no need to write
complex codes as we have pig latin now
remember in the map reduce it can
produce structured semi-structured and
unstructured data and as I mentioned
before Hive can process only structured
data think rows and columns where Pig
can process structured semi-structured
and unstructured data you can think of
structured data as rows and columns
semi-structured as your HTML XML
documents if you have on your web pages
and unstructured could be anything from
groups of documents and written format
Twitter tweets any of those things come
in as very unstructured data and with
our Hadoop mapreduce we have a lower
level of abstraction with both Hive and
pig we have a higher level abstraction
so it's much more easy for someone to
use without having to dive in deep and
write a very lengthy mapreduce code and
those map and reduce codes can take 70
80 lines of code when you can do the
same thing in one or two lines with
however Pig this is the advantage Pig
has over Hive it can process only
structured data in Hive while in pig it
can process structured semi-structured
and unstructured data some other
features to note that separates the
different query languages is we look at
map and reduce map reduce supports
partitioning features as does Hive Pig
no concept of partitioning in pigs it
doesn't support your partitioning
feature your partitioning features allow
you to partition the data in such a way
that it can be queried quicker you're
not able to do that in pig mapreduce
uses Java in Python while Hive uses an
SQL like query language known as Hive ql
or hql Pig Latin is used which is a
procedural data flow language mapreduce
is used by programmers pretty much as
straightforward on Java Hive is used by
data analysts pig is used by researchers
and programmers certainly there's a lot
of mix between all three programmers
have been known to go in and use a hive
for quick query and anybody's been able
to use Pig for a quick query search
under map and reduce code performance is
really good under Hive code performance
is lesser than map and reduce and pick
under Pig Code performance is lesser
than mapreduce but better than Hive so
if we're going to look at speed and time
the map reduce is going to be the
fastest performance on all of those
where Pig will have second and high
follows in the back let's look at
components of pig pig has two main
components we have pig Latin Pig Latin
is a procedural data flow language used
in pig to analyze data it is easy to
program using piglat and it is similar
to SQL and then we have the runtime
engine runtime engine represents the
execution environment created to run Pig
Latin programs it is also a compiler
that produces mapreduce programs uses
hdfs or your Hadoop file system for
storing and retrieving data and as we
dig deeper into the pig architecture
we'll see that we have pig latin scripts
programmers write a script in pig latin
to analyze data using Pig then you have
the grunt shell and it actually says
grunt we start it up and we'll show you
that here in a little bit which goes
into the pig server and this is where we
have our parser parser checks the syntax
of the pig script after checking the
output will be a dag directed acelic
graph and then we have an Optimizer
which optimizes after your dag your
logical plan is passed to The Logical
Optimizer where an optimization takes
place finally the compiler converts the
dag into mapreduce jobs and then that is
executed on the map reduce under the
execution engine the results are
displayed using dump statement and
stored in hdfs using store statement and
again we'll show you that um they kind
of end you always want to execute
everything once you've created it and so
dump is kind of our execution statement
and you can see right here as we were
talking about earlier once we get to the
execution engine and it's coded into
mapreduce then the map reduce processes
it onto the hdfs working a pick Pig
Latin script is written by the users so
you have load data and write Pig script
and pig operations so we look at the
working of pig pig latin script is
written by the users there's step one we
load data and write Pig script and step
two in this step all the pig operations
are performed by parser Optimizer and
compiler so we go into the pig
operations and then we get to step three
execution of the plan in this stage the
results are shown on the screen
otherwise stored in the hdfs as per the
code so it might be of a small amount of
data you're reducing it to and you want
to put that on the screen or you might
be converting a huge amount of data
which you want to put back into the
Hadoop file system for other use let's
take a look at the pig Latin
the data model of pig latin helps pig to
handle various types of data for example
we have Adam Rob or 50. Adam represents
any single value of primitive data type
in pig latin like integer float string
it is stored as a string two bolts so we
go from our atom which are most basic
thing so if you look at just Rob or just
50 that's an atom that's our most basic
object we have in pig latin then you
have a tuple Tuple represents sequence
of fields that can be of any data type
it is the same as a row in rdbms for
example a set of data from a single row
and you can see here we have Rob comma
five and you can imagine with many of
our other examples we've used you might
have the ID number the name where they
live their age their date of starting
the job that would all be one row and
store it as a tuple and then we create a
bag a bag is a collection of tuples it
is the same as a table in rdbms and is
represented by brackets and you can see
here we have our table with Rob 5 mic 10
and we also have a map a map is a set of
key value pairs key is of character
array type and a value can be of any
type is represented by the brackets and
so we have name and age where the key
value is Mike and 10. pig latin has a
fully nestable data model that means one
data type can be nested within another
here's a diagram representation of pig
latin data model and in this particular
example we have basically an ID number a
name an age and a place and we break
this apart we look at this model from
Pig Latin perspective we start with our
field and if you remember a field
contains basically an atom it is one
particular data type and the atom is
stored as a string which it then
converts it into either an integer or
number or character string next we have
our Tuple and in this case you can see
that it represents a row so our Tuple
would be three comma Joe comma 29 comma
California and finally we have our bag
which contains three rows in it in this
particular example let's take a quick
look at Pig execution modes
it works in two execution modes
depending on where the data is reciting
and where the pig script is going to run
we have local mode here the pig engine
takes input from the Linux file system
and the output is stored in the same
file system local mode local mode is
useful in analyzing small data sets
using Pig and we have the mapreduce mode
here the pig engine directly interacts
and executes in hdfs and mapreduce in
the mapreduce mode queries written in
pig latin are translated into mapreduce
jobs in a run on a Hadoop cluster by
default Pig runs in this mode there are
three modes in pig depending on how a
pig latin code can be written we have
our interactive mode batch mode and
embedded mode the interactive mode means
coding and executing the script line by
line when we do our example we'll be in
the interactive mode in batch mode all
scripts are coded in a file with the
extension dot Pig and the file is
directly executed and then there's
embedded mode Pig lets its users Define
their own functions udfs in a
programming language such as Java so
let's take a look and see how this works
in a use case in this case use case
Twitter users on Twitter generate about
500 million tweets on a daily basis the
Hadoop mapreduce was used to process and
analyze this data analyzing the number
of tweets created by a user in the Tweet
table was done using mapreduce in Java
programming language and you can see the
problem it was difficult to perform
mapreduce operations as users were not
well versed with written complex Java
codes so Twitter used Apache pig to
overcome these problems and let's see
how let's start with the problem
statement analyze the user table and
tweet table and find out how many tweets
are created by a person and here you can
see we have a user table we have Alice
Tim and John with their ID numbers one
two three and we have a tweet table in
the Tweet table you have your the ID of
the user and then what they tweeted
Google was a good whatever it was
tennis. spacecraft Olympics politics
whatever they're tweeting about the
following operations were reform for
analyzing given data first the Twitter
data is loaded into the pig storage
using load command and you can see here
we have our data coming in and then
that's going into Pig storage and this
data is probably on an Enterprise
computer so this is actually active
twitters going on and then it goes into
Hadoop file system remember the Hadoop
file system is a data warehouse for
storing data and so the first step is we
want to go ahead and load it into the
pig storage into our data storage system
the remaining operations performed are
shown Below in join and group operation
the tweet and user tables are joined and
grouped using co-group command and you
can see here where we add a whole column
when we go from user names and tweet to
the ID link directly to the name so
Alice was user one Tim was 2 and John 3.
and so now they're listed with their
actual tweet the next operation is the
aggregation the tweets are counted
according to the names the command used
is count so it's very straightforward we
just want to count how many tweets each
user is doing and finally the result
after the count operation is joined with
the user table to find out the username
and you can see here where LS had three
Tim two and John 1. Pig reduces the
complexity of the operations which would
have been lengthy using mapreduce and
joining group operation the tweet and
user tables are joined and grouped using
co-group command the next operation is
the aggregation the tweets are counted
according to the names the command used
as count the result after the count
operation is joined with the user table
to find out the username and you can see
we're talking about three lines of
script versus a mapreduce code of about
80 lines finally we could find out the
number of tweets created by a user in a
simple way so let's go quickly over some
of the features of pig that we already
went through most of these first ease of
programming as Pig Latin is similar to
SQL lesser lines of code need to be
written short development time as the
code is simpler so we can get our
queries out rather quickly instead of
having to have a programmer spend hours
on it handles all kind of data like
structured semi-structured and
unstructured pig lets us create user
defined functions Pig offers a large set
of operators such as join filter and so
on it allows for multiple queries to
process unparallel and optimization and
compilation is easy as it is done
automatically and internally
foreign
let's dive in and show you a quick demo
on some of the commands you can do and
pick today's setup will continue as we
have in the last three demos to go ahead
and use Cloudera quick start and we'll
be doing this in Virtual box we do have
a tutorial in setting that up you can
send a note to our simply learned team
and then get that linked to you once
your Cloudera quick start is uh spun up
and remember this is virtualbox we've
created a virtual machine and this
virtual machine is Centos Linux once
it's spun up you'll be in a full Linux
system here and as you can see we have
Thunderbird browser which opens up to
the Hadoop basic system browser and we
can go underneath the Hue where it comes
up by default if you click on the pull
down menu and go under editor you can
see there's our Impala our Hive Pig
along with a bunch of other query
languages you can use and we're going
under Pig and then once you're in pig we
can go ahead and use our command line
here and just click that little blue
button to start it up and running we
will actually be working in terminal
window and so if you're in the Cloudera
quick start you can open up the terminal
window up top or if you're in your own
setup and you're logged in you can
easily use all of your commands here in
terminal window and we'll zoom in that
way you get a nice view of what's going
on there we go now for our first command
we're going to do a Hadoop command and
import some data into the Hadoop system
in this case a pig input let's just take
a look at this we have Hadoop now let's
know it's going to be a Hadoop command
DFS there's actually four variations of
DFS so if you have hdfs or whatever
that's fine all four of them Point used
to be different setups underneath
different things and now they all do the
same thing and we want to put this file
which in this case is under home
Cloudera documents and Sample and we
just want to take that and put it into
the pig input and let's take a look at
that file if I go under my document
browsers and open this up you'll see
it's got a simple ID name profession and
age we have one Jack engineer 25. and
that was in one of our earlier things we
had in there and so let's go ahead and
hit enter and execute this and now we've
uploaded that data and it's gone into
our Pig input and then a lot of the
Hadoop commands mimic the Linux commands
and so you'll see we have cat as one of
our commands or it has a hyphen before
it so we execute that with Hadoop DFS
hyphen cat slash Pig input because
that's what we called it that's where we
put our sample CSV at and we execute
this you can see from our Hadoop system
it's going to go in and pull that up and
sure enough it pulls out the data file
we just put in there and then we can
simply enter the pig Latin or Pig editor
mode by typing in pick and we can see
here by our grunt I told you that's how
it was going to tell you we're in pig
latin there's our grunt command line so
we are now in the pig shell and then
we'll go ahead and put our load command
in here and the way this works is I'm
going to have office equals load and
here's my load when this case is going
to be Pig input we have that in single
brackets remember that's where the data
is in the Hadoop file system where we
dumped it into there we're going to
using Pig storage our data was separated
as with a comma so there's our comma
separator and then we have as and in
this case we have an ID character array
name character array profession
character array and age character array
and we're just going to do a model's
character arrays just to keep this
simple for this one and then when I hit
put this all in here you can see that's
our full command line going in and we
have our semicolon at the end so when I
hit enter it's now set office up but it
hasn't actually done anything yet it
doesn't do anything until we do dump
office so there's our Command to execute
whatever we've loaded or whatever setup
we have in here and we run that you can
see it go through the different
languages and this is going through the
map reduce remember we're not doing this
locally we're doing this on the Hadoop
tube setup and once we finished our dump
you can see we have ID name profession
age and all the information that we just
dumped into our pick oh we can now do
let's say oh let's say we have a request
just for we'll keep it simple in here
but just for the name and age and so we
can go office we'll call it each as our
variable underscore each and we'll say
for each office generate name comma H
and for each means that we're going to
do this for each row and if you're
thinking map reduce you know that this
is a map function because it's mapping
each row and generating name and age on
here and of course we want to go ahead
and close it with the semicolon and then
once we've created our query or the
command line in here let's go ahead and
dump office underscore each and with our
semicolon and this will go through our
map reduce setup on here and if we were
on a large cluster the same processing
time would happen in fact it's really
slow because have multiple things on
this computer in this particular virtual
box is only using a quarter of my
processor it's only dedicated to this
and you can see here there it is name
and age and it also included the top row
since we didn't delete that out of there
or tell it not to and that's fine for
this example but you need to be aware of
those things when you're processing a
significantly large amount of data or
any data and we can also do office and
we'll call this DSC for descending so
maybe the boss comes to you and says hey
can we order office by ID descending and
of course your boss you've taught them
how to your shareholder it sounds a
little derogatory I say boss you've
talked to the shareholder and you said
and you've taught him a little bit of
Pig Latin and they know that they can
now create office description and we can
order office by ID description and of
course once we do that we have to dump
office underscore description so that
it'll actually execute and there goes
into our map reduce it'll take just a
moment for it to come up because again
I'm running on
quarter of my processor and you can see
we now have our IDs in descending order
returned let's also look at and this is
so important with anytime you're dealing
with big data let's create office with a
limit and you can of course do any of
this instead of with office we could do
this with office descending so you get
just the top two IDs on there but we're
going to limit just to two and of course
to execute that we have to dump office
underscore limit and you can just think
of dumping your garbage into the pig pen
for the pig to eat there we go dump
office limit two and that's going to
just limit our office to the top two and
for our output we get our first row
which had our ID name profession and age
and our second row which is Jack who's
an engineer let's do a filter we'll call
it office underscore filter you guessed
it equals filter office by profession
equals and keep note this is uh similar
to how python does it with the double
equal signs for equal for doing a true
fall statement so for your logic
statement remember to use two equal
signs in Pig and we're going to say it
equals doctor so we want to find out how
many doctors do we have on our list and
we'll go ahead and do our dump we're
dumping all our garbage into the pig pen
and we're letting Pig take over and see
what it can find out and see who's a
doctor on our list and we find uh
employee ID number two Bob is a doctor
30 years old for this next section we're
going to cover something we see a lot
nowadays in data analysis and that's
word counting tokenization that is one
of the next big steps as we move forward
in our data analysis where we go from
say stock market analysis of highs and
lows and all the numbers to what are
people saying about companies on Twitter
what are they saying on the web pages
and Facebook suddenly you need to start
counting words and finding out how many
words are totals I mean you're in the
first part of the document and so on
we're going to cover a very basic word
count example and in this case I've
created a document called wordrows.txt
and you can see here we have simply
learned as a company supporting online
learning simply learn helps people
attain their certifications simply learn
as an online community I love simply
learn I love programming I love data
analysis and I went and saved this into
my documents folder so we could use it
and let me go ahead and open up a new
terminal window for our word count let
me go ahead and close the old one so
we're going to go in here and instead of
doing this as Pig we're going to do pig
minus X local and what I'm doing is I'm
telling the pig to start the pig shell
but we're going to be looking at files
local to our virtual box or the Centos
machine and let me go ahead and hit
enter on there just map
this up there we go and it will load Pig
up and it's going to look just the same
as the pig we were doing which was
defaulted to high to our Hadoop system
to our hdfs this is now defaulted to the
local system now we're going to create
lines we're going to load it straight
from the file remember last time we took
the hdfs and loaded it into there and
then loaded it into Pig since we've gone
the local we're just going to run a
local script we have lines equals load
home the actual full path home Cloudera
documents and I called it wordrows.txt
and as line is a character array so each
line and I've actually you can change
this to read each document I certainly
have done a lot of document analysis and
then you go through and do word counts
and different kind of counts in there so
once we go ahead and create our line
instead of doing the dump we're going to
go ahead and start entering all of our
different setups for each of our steps
we want to go through and let's just
take a look at this next one because the
load is straightforward we're loading
from this particular file since we're
locals loading it directly from here
instead of going into the Hadoop file
system and it says as and then each line
is read as a character array now we're
going to do words equal for each of the
lines generate Flat tokenize Line space
as word now there's a lot of ways to do
this this is if you're a programmer
you're just splitting the line up by
spaces there's actual ways to tokenize
it you gotta look for periods
capitalization there's all kinds of
other things you play with with this but
for the most basic word count we're just
going to separate it by spaces the
flattened takes the line and just
creates a it flattens each of the words
out so this is we're just going to
generate a bunch of words for each line
and then each each of those words is as
a word a little confusing in there but
if you really think about it we're just
going down each line separating it out
and we're generating a list of words one
thing to note is the default for
tokenize you can just do tokenized line
without the space in there if you do
that it'll automatically tokenize it by
space you can do either one and then
we're going to do group we're going to
group it by words so we're going to
group words by word so when we we split
it up each token is a word and it's a
list of words and so we're going to
grouped equals group words by word so
we're going to group all the same words
together and if we're going to group
them then we want to go ahead and count
them and so for count we'll go ahead and
create a word count variable and here's
our four each so for each grouped
grouped is our line where we group all
the words in the line that are similar
we're going to generate a group and then
we're going to count the words for each
grouped so for each line regroup the
words together we're going to generate a
group and that's going to count the
words we want to know the word count in
each of those and that comes back in our
word count and finally we want to take
this and we want to go ahead and dump
word count and this is a little bit more
what you see when you start looking at
run scripts you'll see right here these
these lines right here we have have each
of the steps you take to get there so we
load our file for each of our lines
we're going to generate and tokenize it
into words then we're going to take the
words and we're going to group them by
same words for each grouped we're going
to generate a group and we're just going
to count the words so we're going to
summarize all the words in here and
let's go ahead and do our dump word
count which executes all this and it
goes through our mapreduce it's actually
a local Runner you'll see down here you
start seeing where they still have
mapreduce but it's a special Runner
we're mapping it that's a part of each
row being counted and grouped and then
when we do the word count that's the
reducer the reducer creates these keys
and you can see I is used three times a
came up once and came up once is to
continue on down here to attain online
people company analysis simply learn
they took the top rating with four
certification so all these things are
encountered in the how many words are
used in a data analysis this is probably
the very the beginnings of data analysis
where you might look at it and say oh
they mentioned love three times so
whatever's going on in this post it's
about love and uh what do they love and
they need my name might attach that to
the different objects in here so you can
see that pig latin is fairly easy to use
there's nothing really you know it may
it takes a little bit to learn the
script depending on how good your memory
is as I get older my memory leaks a
little bit more so I don't memorize it
as much but that was pretty
straightforward the script we put in
there and then it goes through the full
map reduce localized run comes out and
like I said it's very easy to use that's
why people like Pig Latin is because
it's intuitive one of the things I like
about Pig Latin is when I'm
troubleshooting when we're
troubleshooting a lot of times you're
working with a small amount of data and
you start doing one line at a time and
so I can go lines equal load and there's
my loaded text and maybe I'll just dump
lines and then it's going to run it's
going to show me all the lines that I'm
working on in the small amount of data
and that way I can test that if I got an
error on there that said oh this isn't
working maybe I'll be like oh my gosh
I'm in map reduce or I'm in the basic
grunt shell instead of the local path
current so maybe will generate an error
on there and you can see here it just
shows each of the lines going down Hive
versus pig on one side we'll have our
sharp Stinger on our black and yellow
friend and on the other side our thick
hide on our Pig let's start with an
introduction to hbase back in the days
data used to be less and was mostly
structured we can see we have structured
data here we usually had it like in a
database where you had every field was
exactly the correct length so if you had
a name field at exactly 32 characters I
remember the old access database in
Microsoft the files are small if we had
you know hundreds of people in one
database that was considered Big Data
this data could be easily stored in
relational database or rdbms and we talk
about relational database you might
think of Oracle you might think of SQL
Microsoft SQL MySQL all of these have
evolved even from back then to do a lot
more today than they did but they still
fall short in a lot of ways and they're
all examples of an R DMS or relationship
database then internet evolved and he
huge volumes of structured and
semi-structured data got generated and
you can see here with the
semi-structured data we have email if
you look at my spam folder you know
we're talking about all the HTML Pages
XML which is a lot of time is displayed
on our HTML and help desk Pages Json all
of this really has just even in the last
each year it almost doubles from the
year before how much of this is
generated so storing and processing this
data on an rdbms has become a major
problem and so the solution is we use
Apache hbase Apache hbase was a solution
for this let's take a look at the
history the hbase history and we look at
the hbase history we're going to start
back in 2006 November Google released a
paper on big table and then in 2017 just
a few months later age-based prototype
was created as a Hadoop contribution
later on in the Year 2007 in October
first usable a space along with the
Hadoop .15 was released and then in
January of 2008 hbase became the
sub-project of Hadoop and later on that
year in October all the way into
September the next year hbase was
released the 0.81 version the 0.19
version and 0.20 and finally in May of
2010 hbase became Apache top level
project and so you can see in the course
of about four years hbase started off as
just an idea on paper and has evolved
all the way till 2010 as a solid project
under the Apache and since 2010 it's
continued to evolve and grow as a major
source for storing data in
semi-structured data so what is hbase
hbase is a column oriented database
management system derived from Google's
no SQL database bigtable that runs on
top of the Hadoop file system or the
hdfs it's an open source project that is
horizontally scalable and that's very
important to understand that you don't
have to buy a bunch of huge expensive
computers you're expanding it by
continually adding commodity machines
and so it's a linear cost expansion as
opposed to being exponential no SQL
database written in Java which permits
faster querying us so job is to back in
for the hbase setup and it's well suited
for sparse data sets so it can contain
missing or n a values and this doesn't
Boggle it down like it would in other
database companies using hbase so let's
take a look and see who is using this no
SQL database for their servers and for
storing their data and we have
hortonworks which isn't a surprise
because they're one of the like Cloudera
hortonworks they are behind Hadoop and
one of the big developments and backing
of it and of course Apache hbase is the
open source behind it and we have
Capital One as Banks you also see Bank
of America where they're collecting
information on people and tracking it so
their information might be very sparse
they might have one Bank way back when
they collected information as far as the
person's family and what their income
for the whole family is and their
personal income and maybe another one
doesn't collect the family income as you
start seeing where you have data that is
very difficult to store where it's
missing a bunch of data how spots using
it Facebook certainly all of your
Facebook Twitter most of your social
medias are using it and then of course
there's JP Morgan Chase and Company
another bank that uses the hbase as
their data warehouse for nose SQL let's
take a look at an hbase use case so we
can dig a little bit more into it to see
how it functions telecommunication
company that provides mobile voice and
multimedia Services across China the
China mobile and China mobile they
generate billions of call detailed
records or CDR and so these cdrs and all
these records of these calls and how
long they are and different aspects of
the call maybe the tower they're
broadcasted from all of that is being
recorded so they can track it a
traditional database systems were unable
to scale up to the vast volumes of data
and provide a cost-effective solution no
good so storing in real-time analysis of
billions of call records was a major
problem for this company solution Apache
hbase hbase stores billions of rows of
detailed call records hp's perform forms
fast processing of Records using SQL
queries so you can mix your SQL and
nosql queries and usually just say no
SQL queries because of the way the query
Works applications of hbase one of them
would be in the medical industry hbase
is used for storing genome sequences
storing disease history of people of an
area and you can imagine how sparsat is
as far as both of those a genome
sequence might be only have pieces to it
that each person is unique or is unique
to different people and the same thing
with disease you really don't need a
column for every possible disease a
person could get you just want to know
what those diseases those people have
had to deal with in that area e-commerce
hbase is used for storing logs about
customer search history performs
analytics and Target advertisement for
Better Business insights sports hbase
stores match details in the history of
each match uses this data for better
prediction so when we look at age base
we all want to know what's the
difference between hbase versus rdbms
that is a relational database management
system hbase versus rdbms so the a space
does not have a fixed schema a schema
less defines only column families and
we'll show you what that means later on
and rdbms has a fixed schema which
describes the structure of the tables
and you can think of this as you have a
row and you have columns and each column
is a very specific structure how much
data can go in there and what it does
with the hbase it works well with
structured and semi-structured data with
the rdbms it works only well with
structured data with the age space it
can have denormalized data it can
contain missing or null values with the
rdbms it can store only normalized data
now you can still store a null value in
the rdbms but it still takes up the same
space as if you're storing a regular
value in many cases and it also for the
hbase is built for y tables it can be
scaled horizontally for instance if you
were doing a tokenizer of words and word
clusters you might have 1.4 million
different words that you're pulling up
and combinations of words so with an
rdbms it's built for thin tables that
are hard to scale you don't want to
store 1.4 million columns in your SQL
it's going to crash and it's going to be
very hard to do searches with the age
base it only stores that data which is
part of whatever row you're working on
let's look at some of the features of
the hbase it's scalable data can be
scaled across various nodes as it is
stored in the hdfs and I always think
about this it's a linear add-on for each
terabyte of data I'm adding on roughly a
thousand dollars in commodity Computing
with an Enterprise machine we're looking
at about 10 000 at the lower end for
each terabyte of data and that includes
all your backup and redundancy so it's a
big difference it's like a tenth of the
cost to store it across the hbase it has
automatic failure support right ahead
log across clusters which provides
automatic support against failure
consistent read and write hbase provides
consistent read and write of the data
it's a Java API for client access
provides easy to use Java API for
clients block cache and Bloom filters so
the hbase supports block caching and
Bloom filters for high volume query
optimization let's dig a little deeper
into the hbase storage a space column
oriented storage and I told you we're
going to look into this to see how it
stores the data and here you can see you
have a row key this is really one of the
important references is each row has to
have its own key or your row ID and then
you have your column family and in here
you can see we have column family one
column family two column family three
and you have your column qualifiers so
you can have in column family one you
can have three columns in there and
there might not be any data in that so
when you go into column family one and
do a query for every column that
contains a certain thing that row might
not have anything in there and not be
queried where in column family two maybe
you have column one filled out and
column three filled out and so on and so
forth and then each cell is connected to
the row where the data is actually
stored let's take a look at this and
what it looks like when you fill the
data in so in here we have a row key
with a row ID and we have our employee
ID one two three that's pretty
straightforward you probably would even
have that on an SQL server and then you
have your column family this is where it
starts really separating out your column
family might have personal data and
under personal data you would have name
City age you might have a lot more than
just that you might have number of
children you might have degree all those
kinds of different things that go under
personal data and some of them might be
missing you might only have the name and
the age of an employee you might only
have the name the city and how many
children and not the age and so you can
see with the personal data you can now
collect a large variety of data and
stored in the hbase very easily and then
maybe you have a family of professional
data your designation your salary all
the stuff that the employee is doing for
you in that company let's dig a little
deeper into the hbase architecture and
so you can see here what looks to be a
complicated chart it's not as
complicated
from the Apache a space we have the
Zookeeper which is used for monitoring
what's going on and you have your age
Master this is the hbase master of
science regions and load balancing and
then underneath the region or the hbase
master then under the H master or H base
Master you have your reader server
serves data for read and write and the
region server which is all your
different computers you have in your
Hadoop cluster you'll have a region an H
log you'll have a store memory store and
then you have your different files for
each file that are stored on there and
those are separated across the different
computers and that's all part of the
hdfs storage system so we look at the
Architectural Components or regions and
we're looking at we're drilling down a
little bit hbase tables are divided
horizontally by a row so you have a key
range into regions so each of those IDs
you might have IDs one to twenty twenty
one two fifty or whatever they are
regions are assigned to the nodes in the
cluster called region servers a region
contains all rows in the table between
the region start key and the End Key
again 1 to 10 11 to 20 and so forth
these servers serve data for read and
write and you can see here we have the
client and the get and the git sends it
out and it finds out where that startup
is between which start keys and end keys
and then it pulls the data from that
different region server and so the
region sign data definition language
operation create delete are handled by
the H master so the H Master is telling
it what are we doing with this data
what's going out there assigning and
reassigning regions for Recovery or load
balancing and monitoring all servers so
that's also part of it so you know if
your IDs if you have 500 IDs across
three servers you're not going to put
400 IDs on server 1 and 100 on the
server 2 and leaves Region 3 and Region
4 empty you're going to split that up
and that's all handled by the H master
and you can see here it monitors region
servers assigns regions to region
servers assigns regions to Regions
servers and so forth and so forth hspace
has its distributed environment where
age Master alone is not sufficient to
manage everything hence zookeeper was
introduced it works with h master so you
have an active age Master which sends a
heartbeat signal to zookeeper indicating
that is active and the Zookeeper also
has a heartbeat to the region server so
the region servers send their status to
zookeeper indicating they are ready for
read and write operation inactive server
acts as a backup if the active h Master
fails it'll come to the rescue active
Ace master and region servers connect
with a session to zookeeper so you see
your active h Master selection region
server session they're all looking at
the Zookeeper keeping that pulse an
active age master and region server
connects with a session to the Zookeeper
and you can see here where we have
ephemeral nodes for active sessions via
heartbeats to indicate that the region
servers are up and running so let's take
a look at hbase read or write going on
there's a special hbase catalog table
called The Meta table which holds a
location of the regions in the cluster
here's what happens the first time a
client reads or writes data to age base
the client gets the region server the
host the meta table from zookeeper and
you can see right here the client has a
request for your region server and goes
hey zookeeper can you handle this the
Zookeeper takes a look at it and goes ah
metal location is stored in Zookeeper so
it looks at its metadata on there and
then the metadata table location is sent
back to the client the client will query
The Meta server to get the region server
corresponding to the row key if it wants
to access the client caches this
information along with the meta table
location and you can see here the client
going back and forth to the region
server with the information and it might
be going across multiple region servers
depending on what you're querying so we
get the region server for row key from
The Meta table that's where that row key
comes in and says hey this is where
we're going with this and so once it
gets the row key from the corresponding
region server we can now put row or get
Row from that region server let's take a
look at the hbase meta table special HP
catalog table that maintains a list of
all the region servers in the hbase
storage system so you see here we have
the meta table we have a row key and a
value table key region region server so
the meta table is used to find the
region for the given table key and you
can see down here you know our meta
table comes in is going to fire out
where it's going with the region server
and we look a little closer at the write
mechanism in hbase we have write a
headlock our wall as you abbreviate it
kind of a way to remember wall is right
ahead log is a file used to store new
data that is yet to be put on permanent
storage it is used for Recovery in the
case of failure so you can see here
where the client comes in and it
literally puts the new data coming in
into this kind of temporary storage or
the wall on there once it's gone into
the wall then the memory store mem store
is the right cache that stores a new
data that has not yet been written to
disk there is one mem store per column
family per region and once we've done
that we have three ack once the data is
placed in MIM store the client then
receives the acknowledgment when the
minister reaches the threshold it dumps
or commits the data into H file and so
you can see right here we've taken our
or has gone into the wall the wall then
Source it into the different memory
stores and then the memory stores it
says Hey we've reached we're ready to
dump that into our H files and then it
moves it into the age files age files
store the Roses data as stored key value
on disk so here we've done a lot of
theory let's dive in and just take a
look and see what some of these commands
look like and what happens in our age
base when we're manipulating a nosql
setup
[Music]
so if you're learning a new setup it's
always good to start with where is this
coming from it's open source by Apache
and you can go to
hbase.apache.org and you'll see that it
has a lot of information you can
actually download the hbase separate
from the Hadoop although most people
just install the Hadoop because it's
bundled with it and if you go in here
you'll find a reference guide and so you
can go through the Apache reference
guide and there's a number of things to
look at but we're going to be going
through Apache H based shell that's what
we're going to be working with and
there's a lot of other interfaces on the
setup and you can look up a lot of the
different commands on here so we go into
the Apache hbase reference guide we can
go down to read hbase shell commands
from a command file you can see here
where it gives you different options of
formats for putting the data in and
listing the data certainly you can also
create files and scripts to do this too
but we're going to look at the basics
we're going to go through this on a
basic hbase shell and one last thing to
look at is of course if you continue
down the setup you can see here where
they have more detail tell as far as how
to create and how to get to your data on
your hbase now I will be working in a
virtual box and this is by Oracle you
can download the Oracle virtual box you
can put a note in below for the YouTube
as we did have a previous session on
setting up virtual setup to run your
Hadoop system in there I'm using the
Cloudera quick start installed in here
there's Hortons you can also use the
Amazon web service there's a number of
options for trying this out in this case
we have Cloudera on the Oracle virtual
box the virtual box has Linux Centos
installed on it and then the Hadoop it
has all the different Hadoop flavors
including hbase and I bring this up
because my computer is a Windows 10 the
operating system of the virtual box is
Linux and we're looking at the hbase
data warehouse and so we have three very
different entities all running on my
computer and that can be confusing if
it's a first time in and working with
this kind of setup now you'll notice in
our Cloudera setup they actually have
some hbase monitoring so I can go
underneath here and click on hbase and
master and it'll tell me what's going on
with my region servers it'll tell me
what's going on with our backup tables
right now I don't have any user tables
because we haven't created any and this
is only a single node and a single hbase
tour so you're not going to expect
anything too extensive in here since
this is for practice and education and
perhaps testing out package you're
working on it's not for really you can
deploy Cloudera of course but when you
talk about a quick start or a single
node setup that's what it's really for
so we can go through all the different
hbase and you'll see all kinds of
different information with zookeeper if
you saw it flash by down here what
version we're working in some zookeepers
part of the hbase setup where we want to
go is we want to open up a terminal
window and in Cloudera it happens to be
up at the top and when you click on here
you'll see your Cloudera terminal window
open and let me just expand this so we
have a nice full screen and then I'm
also going to zoom in that way I have a
nice big picture and you can see what
I'm typing and what's going out on and
to open up your a H base shell simply
type h base shell to get in and hit
enter and you'll see it takes just a
moment to load and we'll be in our age
based shell for doing hbase commands
once we've gotten into our H base shell
you'll see you'll have the hbase prompt
information ahead of it we can do
something simple like list this is going
to list whatever tables we have it so
happens that there's a base table that
comes with hbase now we can go ahead and
create and I'm going to type in just
create what's nice about this is it's
going to throw me kind of a it's going
to say hey there's no just straight
create but does come up and tell me all
these different formats we can use for
create so we can create our table and
our families you had splits names
versions all kinds of things you can do
with this let's just start with a very
basic one on here and let's go ahead and
create and we'll call it new table now
this is to call it new TBL for table new
table and then we also want to do let's
do knowledge so let's take a look at
this I'm creating a new table and it's
going to have a family of knowledge in
it and let me hit enter it's going to
come up it's going to take it a second
to go ahead and create it now we have
our new table in here so if I go list
you'll now see table and new table so
you can now see that we have the new
table and of course the default table
that's set up in here and we can do
something like uh describe we can
describe and then we're going to do new
TBL and when we describe it it's going
to come up it's going to say hey name I
have knowledge data block encoding none
Bloom filter row or replications Go
version all the different information
you need new minimum version zero
forever deleted cells false block size
in memory and you can look this stuff up
on Apache dot org to really track it
down one of the things that's important
to note is versions so you have your
different versions of the data that's
stored and that's always important to
understand that we might talk about that
a little bit later on and then after we
describe it we can also do a status the
status says I have one active Master
going on that's our age base as a whole
we can do status summary I should do the
same thing as status so we got the same
thing coming up and now that we've
created let's go ahead and put something
in it so we're going to put new TBL and
then we want Row one you know what
before I even do this let's just type
input and you can see when I type in put
it gives us like a lot of different
options of how it works and different
ways of formatting our data as it goes
in and all of them usually begin with
the new table new TBL then we have in
this case we'll call it Row one and then
we'll have knowledge if you remember we
created knowledge already and we'll do
knowledge Sports and then in knowledge
and sports we're going to set that equal
to Cricut so we're going to put
underneath this our knowledge setup that
we have a thing called Sports in there
and we'll see what this looks like in
just a second let's go ahead and put in
we'll do a couple of these let's see
let's do another row one and this time a
set of sports Let's Do Science you know
this person not only you know we have
Row one which is both knowledgeable and
cricket and also in chemistry so it's a
chemist who plays Cricut in row one and
let's see if we have let's do another
row one just to keep it going and we'll
do science in this case let's do physics
not only in chemistry but also physicist
I have quite a joy in physics myself so
here we go we have Row one there we go
and then let's do row two let's see what
that looks like when we start putting in
row two and in row two this person is
has knowledge in economics this is a
master of business and how or maybe it's
Global economics maybe it's just for the
business and how it fits in with the
country's economics and we call it
macroeconomics so guess it is for the
whole country there so we have knowledge
economics macroeconomics and then let's
just do one more we'll keep it as row
two and this time our Economist is also
a musician so we'll put music and they
happen to have knowledge and they enjoy
oh let's do pop music they're into the
current pop music going on so we've
loaded our database and you'll see we
have two rows Row one and row two in
here and we can do is we can list the
contents of our database by simply doing
scan uh scan and then let's just do scan
by itself so you can see how that looks
you can always just type in there and it
tells you all the different setups you
can do with scan and how it works in
this case we want to do scan new TBL and
in our scan new TBL we have Row one row
one row two row two and you'll see Row
one has a column called knowledge
science time step value crickets value
physics so it has information is when it
was created when the timestamp is Rule
one also has knowledge Sports and a
value of qriket it so we have sports and
Science and this is interesting because
if you remember up here we also gave it
originally we told it to come in here
and have chemistry we had science
chemistry and science physics and we
come down here I don't see the chemistry
why because we've now replaced chemistry
with physics so the new value is physics
on here let me go ahead and clear down a
little bit and in this we're going to
ask the question is enabled new table
when I hit enter in here you're going to
see it comes out true and then we'll go
ahead and disable it let's go ahead and
disable new table make sure I have our
quotes around it and now that we've
disabled it what happens when we do the
scan we do the scan new table and hit
enter you're going to see that we get an
error coming up so once it's disabled
you can't do anything with it until we
re-enable it now before we enable the
table Let's do an alteration on it and
here's our new table and this should
look a little familiar because it's very
similar to create we'll call this test
info we'll hit enter in there it'll take
just a moment for updating and then we
want to go ahead and enable it so let's
go ahead and enable our new table so
it's back up and running and then we
want to describe describe new table and
we come in here you'll now see we have
name knowledge and under there we have
our data encoding and all the
information under knowledge and then we
also have down below test info so now we
have the name test info and all the
information concerning the test info on
here and we'll simply enable it new
table so now it's enabled oops I already
did that I guess we'll enable it twice
and so let's start looking at well we
had scan new table and you can see here
where it brings up the information like
this but what if we want to go ahead and
get a row so we'll do R1 and when we do
hbase R1 you can see we have knowledge
science and it has a timestamp value
physics and we have knowledge Sports and
it has a time stamp on it and value
Cricut and then let's see what happens
when we put into to our new table and in
here we want Row one and if you can
guess from earlier because we did
something similar we're going to do
knowledge economics and then it's going
to be instead of I think it was what
macroeconomics is now market economics
and we'll go back and do our git command
and now see what it looks like and we
can see here where we have knowledge
economics it has a timestamp value
market economics physics and Cricut and
this is because we have economic science
and sports those are the three different
columns that we have and then each one
has different information in it and so
if you've managed to go through all
these commands and look at Basics on
here you'll now have the ability to
create a very basic hbase setup no SQL
setup based on your columns and your
rows and just for fun we'll go back to
the Cloudera where they have the website
up for the hbase master status and I'll
go ahead and refresh it and then we can
and go down here and you'll see user
tables table set one and we can click on
details and here's what we just did it
goes through so if you're the admin
looking at this you can go oh someone
just created new TBL and this is what
they have underneath of it in their new
table nobody you are an aspen data
analyst a season ID professional or a
business leader looking to Leverage The
Power of Big Data the data engineering
postgraduate program by simply learn in
collaboration with party University and
IBM please tell it to meet your needs
this program offers an exceptional
opportunity for professional growth and
exposure with the focus on practical
learning and in alignment with industry
leading certifications from AWS and
Azure now this program equips
participants with essential skills
required to excel in this field of data
engineering by enrolling in this applied
learning program individuals can gain
master or crucial data engineering
techniques and Technologies enroll today
and take the next step towards becoming
a proficient Big Data professional check
out the course Link in the description
box below for more details today we're
covering the a Hadoop ecosystem at least
a very fundamentals of all the different
parts that are in the Hadoop ecosystem
and it's very robust it's grown over the
years with different things added in
there's a lot of overlapping in a lot of
these tools but we're just going to
cover these basic tools so you can see
what's available in the Hadoop ecosystem
so let's go back to our Hadoop ecosystem
as you can see we have all our different
setup and let's focus on the Hadoop part
of it first before we look at the
different tools we start with the Hadoop
or hdfs is for data storage write once
read many times you can store a lot of
data on it affordably distributed file
system and so we talk about the Hadoop
file system it stores different formats
of data on various machines and so you
have like a huge cluster of computers
and you're able to store Word documents
spreadsheets structured data
non-structured data semi-structured and
in the Hadoop file system there's the
two different sets of servers in there
there's the name node which is the
master we talked about that that's your
Enterprise computer and the other
component is your data nodes and so
you'll usually have like I said one to
two you'll have a name node and maybe a
backup name node and then you'll have as
many data nodes as you want and you can
just keep adding them that's what makes
it so affordable you know you have a
rack of computers and you go oh I need
more space you just add another Rack in
so it's very affordable and very easy to
expand and the way the Hadoop file
system itself Works behind the hood is
it splits the data into multiple blocks
by default it's 128 megabytes and the
120 megabytes it is a default setting
you can change that that works for most
data there's reasons for either
processing speed or for better
distribution of the data so if you have
little tiny blocks of data that are less
than 128 megabytes if you have a lot of
those you might want to go down in size
and vice versa for larger blocks and you
can see right here we have 300 megabytes
and it takes that piece of data and it
just divides it into blocks of data and
each one 128 128 and 44 which if he had
together equals 300 megabytes and now
that you understand the Hadoop file
system or at least the basic overview of
it it's important to note what it sits
on what's actually making all this work
on the back end and this is yarn a
Hadoop yarn is a cluster Resource
Management so it's how it manages this
whole cluster right here that we just
looked at and yarn stands for yet
another resource negotiator love the
title it reminds me of an Iron Man movie
with you know Tony Starks and Jarvis
just a rather intelligent system or
whatever stood for but yarn has become
very widely used and it's actually used
as a back end for a lot of other
packages so it's not just in Hadoop but
Hadoop is where it came from and where
it's set up and there's some other ones
another popular one is mesos which I'll
mention again briefly and so the yarn it
handles the cluster of notes it's the
one the when you hear yarn if someone is
going Hey where's our Ram at where's our
hard drives at or if you have a solid
state disk drive your SD more is is that
at how much memory do I have what can I
put where and so here we go nice image
of it RAM memory resources so it's
allocating all these different resources
for different applications is what it's
doing when we talk about the back to the
two major components the two major
components is your resource manager
that's on the master server or your
Enterprise computer and then that one is
in control and managing what's going on
with all of your nodes data processing
in Hadoop mapreduce and we're going to
talk about the map reduce here in just a
second the Hadoop data processing is all
built upon map reduce map reduce
processes a large volumes of data in
purely distributed manner this is very
core to Hadoop but before we go on
because there's other tools out there
and things are slowly shifting and
there's all kinds of new things one of
the things you want to look at is not
just how the map reduce works but start
thinking map reduce one of the best
pieces of advice I had from a one of my
mentors in data science was think map
reduce this is really what you should be
learning but it is an actual process in
the Hadoop system so we have our big
data and the Big Data Maps out and so
this is the first step is if I'm looking
at my data how do I map that data out
what am I looking at and it could be
something as simple as I just loaded
into you know I'm just looking at one
line at a time but it could be that I'm
looking at the data one line at a time
but I only need columns one and four
maybe I'm looking at it one column at a
time but I need the total of column one
added together and column one over
column two so you can start to see and
get some very complicated mapping here
but the mapping is what do you do with
each line of data each piece of data if
you're in a spreadsheet it's easy to see
you have a row whatever you do to that
row that's what you're mapping because
it doesn't look at anything else it
doesn't know anything else all it knows
is what's on that row if you're looking
at documents maybe it's pulling one
document at a time and so your map is
then a document and then it takes those
and we Shuffle and sort them how do we
sort them around whether you're grouping
them together whether you're taking the
whatever information you mapped out of
it word counts you're counting the word
a so letter A comes out with how many
for each mapping if you have 54 A's per
the one document 53 in the other ones
that's what's coming out of that mapping
and going into the shuffle and sort and
so if it's counting A's and B's and C's
it'll Shuffle and sort all the A's
together all the B's together if you're
running a big data for running
agriculture apples and oranges so it
puts all the apples in one all the stuff
you mapped out that you said hey these
are all apples these are all oranges
let's shuffle them and sort them
together and then we reduce and reduce
so each of these groups reduce into the
data you want out so you have map
Shuffle sort reduce and some important
things to know about the Hadoop file
system because I'm going to mention
spark in a little bit is a Hadoop file
system manages to do a lot of this by
writing it to the hard drive so if
you're running a really low budget which
nobody does anymore with the Hadoop file
system and you have all your commodity
machines are all low they only have
eight gigabytes memory instead of 128
gigabytes this process uses the hard
drive and so it pulls it into the RAM
for your mapping it Maps a one piece of
data then it writes that map to the hard
drive then it takes that mapping from
the hard drive loads it up and shuffles
it and writes it back to the hard drive
to a different spot and then takes that
information and starts processing it in
the reduce and then it rates the reduced
answer to the hard drive it runs slower
because you're accessing to and from
your hard drive or solid state drive if
you have an SD card in there but you can
also utilize it's a lot more affordable
you know it's like I said you having
that higher end of ram cost even on a
commodity machine she could save a lot
of money nowadays it's so affordable
people run the spark setup on there
which does the same thing but in Ram and
again we'll talk about spark in just a
little bit and of course the final thing
is an output so again your reducer
written to the hard drive and then your
reduce is brought together to form one
output what is the maximum number of
oranges sold per an area I don't know
I'm making that up but the things about
Hadoop is it covers so many different
things that anything you can think of
you can put in Hadoop the question is do
you need to do you have enough data or
enough need for the high-end processing
so again look at mapper reduce but think
of this not just as mapreduce start
thinking map and reduce when you think
big data we're going to start getting
into the tools because you had all those
pictures of all those Cool Tools we have
so we're going to look at the first two
of those tools and in the tools we have
scoop and Flume and this is for your
data collection and ingestion we're
going to bring spark up in just a second
can also play a major role in this
because spark is its own animal that
sprung from Hadoop connects into Hadoop
but it can run completely independent
but scoop and Flume are specific to
Hadoop in their ways to bring in
information into the Hadoop file system
so we talk about these we'll start with
scoop scoop is used to transfer data
between Hadoop and external data stores
such as relational databases and
Enterprise data warehouses and so you
can see right here here's our Hadoop
data and it's connecting up there and
it's either pushing the data or pulling
the data and we have a relational
database and Enterprise data warehouse
and there's that magic word Enterprise
that means these are the servers that
are very high-end so maybe you have a
high-end SQL server or a biosql server
or whatever over there and this is
what's coming and going from the scoop
it Imports data from external data
stores into the hdfs hive and hbase
those are two specific The Hive setup is
your SQL basically and you can see a
nice little image here here's somebody
on their laptop which would be
considered the client machine putting
together their code they push the scoop
the scoop goes into the task manager the
task manager then goes hey what have I
got for the hbase And Hive system in our
Hadoop system and then it reaches out to
the Enterprise data warehouse or into
the document based system or
relationship based database relationship
is your non-sql document is just what it
sounds like because you have HTML
documents or Word documents or text
documents and it's able to map those
tasks and then bring those into the
Hadoop file system now Flume is a
distributed service for collecting
aggregating and moving large amounts of
log data so kind of focused a little bit
on a slightly different set of data
although you'll find these overlap a lot
you can certainly use Flume to do a lot
of things you can do in scoop and vice
versa Flume ingests the data so we're
looking at like say a Json call to a
website XML documents unstructured and
semi-structured data is most commonly
digested by Flume best example I saw was
a Twitter account Twitter feeds into a
Hadoop system so it ingests online
streaming data from social media Twitter
log files so we want to know what's
going on with error codes on your
servers and all those log files web
server what's going on in your web
server bring all this stuff in and just
dump it into the Hadoop data file system
to be looked at and processed later and
it's a web server cloud social media
data again all those different sources
it can be it's kind of endless you know
it just depends on what your company
needs versatility of Hadoop is what
makes it such a powerful source to add
into a company and so it comes in there
you have your Source it goes through the
channels it then goes through kind of a
sync feature to make sure everything is
in sync and then it dumps it into the
Hadoop file system so we've covered in
the Hadoop file system the first two
things let's look at some of the
scripting languages they have and so we
have the two here and you can also think
of these it actually says scripting and
SQL queries a lot of times they're both
referred to as queries so you have both
the Pig and the hive and take is used to
analyze data in Hadoop it provides a
high-level data processing language to
perform numerous operations on the data
and it's made out of pig Latin language
for scripting Pig Latin compiler
converts Pig Latin code to execute code
and then you have your ETL the ETL
provides a platform for building data
flow for ETL and ETL is like the catch
three letters now on any job interview I
look at it says ETL it just means
extract transfer and load so all we're
doing is extracting the data
transferring it to where we need it and
then loading it into in this case a
Hadoop file system and there's other
pieces to that you know it's actually a
big thing because whenever you're
extracting data do you want to dump all
the data or do you want to do some kind
of pre-processing so you only bringing
in what you want and then one of the
cool things about Pig Latin is 10 lines
of pig latin script is around 200 lines
of map reduce job again the map reduce
is the back end processes that go go on
so if we have pig and I'll be honest
with you pig is very easy to use but as
a scripter programmer I find it's more
for people who just need a quick pull of
the data and able to do some very basic
things very easily so if you're doing
some very high-end processing and model
building you usually end up in something
else so pigs great for that like if
you're in the management you need to
build a quick query report pig is really
good for that and so it definitely has
its place it's definitely a very useful
script to know and the pig latin scripts
I call it the grunt shell I guess it
goes with pig because they grunt you
have your pig server you have a parser
an Optimizer a compiler an execution
engine and that's all part of the Apache
Pig and this then goes into the map
reduce which then goes into the Hadoop
file system in the Hadoop and you'll see
Apache with a lot of these because it's
under the open source uh hadoops under
the Apache open source so all this stuff
is you'll see under Apache with the name
tag on there if you're in no Pig Hive is
the other one that's really popular for
easy query Hive facilitates Reading
Writing and managing large data sets
residing in the distributed storage
using SQL Hive query language and this
is important because a lot of times
you're coming from an SQL Server there's
your Enterprise set up and now you're
archiving a history of what's going on
on that server so you're continually
pulling data using scoop onto your into
your hbase hive database and as it comes
in there it'd be nice to just use that
same query to pull the data out of the
Hadoop file system and that's exactly
what it does so you have Hive command
line you can also use a JBC or odbc
driver and those drivers like if you're
working in Java or you're working in
Python you can use those drivers to
access Hive so you don't have to go
through the hive command line but it
makes it real quick I can take a high of
command line and just punch in a couple
words and pull up my data and so it
provides user-defined functions
UDF for data mining document indexing
log processing again anything that is in
some kind of SQL format if it's stored
that properly on the Hadoop file system
you can use your hive to pull it out and
you see even a more robust image of
what's in the hive there's your client
machine that's you on your laptop or
you're logged in wherever you're at
writing your script that goes into the
driver where you have your compiler your
Optimizer executor you also have your
jdbc odbc connections which goes into
the high Thrift server which then goes
into the driver your hive web interface
so you can just log in on the web and
start typing away your SQL commands and
that again goes to the driver and all
those pieces and those pieces go to your
job tracker your name node and your
actual Hadoop file system so spark
real-time data analysis spark is an open
source distributed computing engine for
processing and analyzing huge volumes of
real-time data so it runs 100 times
faster than map reduce map reduce is the
basics of the Hadoop system which is
usually set up in a Java code in spark
the map reduce instead of running what
happens in mapreduce as it goes pulls it
into the ram writes it to the hard drive
reads it off the hard drive shuffles it
around writes it back to the hard drive
pulls it off the hard drive does its
processing and you get the impression
it's going in and out of ram to the hard
drive and back up again so if you don't
have a lot of RAM and you have older
computers and you don't have the ram to
process something then spark and Hadoop
are going to run the same speed
otherwise spark goes hey let's keep this
all on the RAM and run faster and that's
what we're talking about it provides
in-memory computation of data so it's
fast you can see the guy running through
the door there speedy versus a very slow
Hadoop wandering around when it's used
to process and analyze real-time
streaming data such as stock market and
baking data so the spark can have its
own stuff going on and then it can
access the Hadoop database it can pull
data just like scoop and Flume do so it
has all those features in it and you can
even run Spa park with its own yarn
outside of Hadoop there's also a spark
on mesos which is another resource
manager although yarn is most commonly
used and currently spark pretty much
becomes installed with Hadoop and so
your spark running on top of Hadoop will
use the same nodes it has its own
manager and it utilizes the ram so
you'll have both you can have the spark
on top of there and here we go you have
your driver program your spark context
it goes into your cluster manager your
yarn then you have your worker nodes
which are going to be executing tasks
and cash and it's important to remember
when you're running the spark setup even
though these are commodity machines
we're still usually a lot of them are
like 128 gigabytes of RAM so that spark
can run these high-end processes
certainly if it's not access very much
and you're building a Hadoop cluster you
could drop that Ram way down if you're
doing just queries without any kind of
processing on there and you're not doing
a lot of queries but you know generally
spark you're looking at higher end
there's still commodity machines to do
your work and now we get into the Hadoop
machine learning and so machine learning
is its own animal I keep saying that
these are all kind of unique things
mahaut is being phased out I mean people
still use it it's still important if you
can write your basic machine learning
that has most of the tools in there you
certainly can do it in mahout how it
again writes to the hard drive reads
from the hard drive you can do all that
in spark and it's faster because it's
just in Ram and so spark has its own
machine learning tools in it you can
also use Pi spark which then accesses
all the different python tools and
there's there's just so many tools you
can dump into spark the how is very
limited but it's also very basic which
in itself can be really good sometimes
simple is better so my how it is used to
create scalable and distributed machine
learning algorithms so here we have our
mahout environment it builds a machine
learning application so you're doing
linear regression you're doing
clustering you're doing classification
models it has a library that contains
inbuilt algorithms for all of these and
then it has collaborative filtering and
again there's our classification and
there's our clustering regression so you
have your different machine learning
tools we can easily classify a large
amount of data using mahout so this
brings us to the next set of tools we
have or the next tool which is the
Apache ambari abari is an open source
tool responsible for keeping track of
running applications and their statuses
think of this as like a traffic cop more
like a security guard you can open it up
and see what's going on and so the
Apache ambari manages monitors and
Provisions Hadoop clusters provides a
central Management Service to start stop
and configure Hadoop services so again
it's like a traffic cop hey stop over
there start keep going hey what's going
on over that area in the through Lane on
the high speed freeway and you can open
this up and you have a really easy view
of what's going on and so you can see
right here you have the ambari web the
ambari web is what you're looking at
that's your interface that you've logged
into the the ambari server this will be
usually on the master node it's usually
if you're going to install Lombardo you
might as well just install it on the
same node and it connects up to the
database and then it has agents and each
agent takes a look and see what's going
on with its host server and if you're
going to have two more systems and we
talked about spark streaming there's
also Kafka and Apache for streaming data
coming in and kafka's distributed
streaming platform to store and process
streams of records and it's written in
Scala builds real-time streaming data
pipelines that reliably get data between
applications builds real-time streaming
applications that transforms data into
streams so it kind of goes both ways on
there so Kafka uses a messaging system
for transferring data from one
application to another so we have our
sender we have our message queue and
then we have our receiver pretty
straightforward very solid setup on
there with the Kafka Patty storm storm
is a processing engine that processes
real-time streeting at a very high speed
and it's written in Co closure they
utilize the function based programming
style of closure and it's based on lisp
to give it the speed that's why Apache
storm is built on there so you could
think of Kafka as a slow-moving very
solid communication Network where Storm
is looking at the real-time data and
grabbing that streaming data that's
coming in fast so it has a ability to
process over a million jobs in a
fraction of seconds on a node so it's
massive it can really reach out there
and grab the data and it's integrated
with Hadoop to harness higher
throughputs so this is the two big
things about storm if you are pulling I
think they mentioned stock coming in or
something like that where you're looking
for the latest data popping up storm is
a really powerful tool to use for that
so we've looked at a couple more tools
for bringing data in we probably should
talk a little bit about security
security has in Hadoop has the Apache
Ranger and Apache knocks are the two
most popular one and the ranger the
Apache Ranger Ranger is a framework to
enable Monitor and manage data
securities across the Hadoop platform so
the first thing it does is it provides
centralized Security Administration to
manage all security related tasks the
second thing it does is it has a
standardized authorization across all
Hadoop components and third it uses
enhanced support for different
authorization methods role-based Access
Control attribute-based Access Control
Etc so your Apache Ranger can go in
there and your administrator coming in
there can now very easily monitor who
has what rights and what they can do and
what they can access so there's also
Apache Knox is an application Gateway
for interacting with the rest apis and
the uis of Hadoop developers and so we
have our application programmer
interfaces our user interfaces and we
talk about rest apis this means we're
pulling this is looking at the actual
data coming in what applications are
going on so if you have an application
where people are pulling data off of the
Hadoop system or pushing data into the
dupe system the nox is going to be on
that setup and it delivers three groups
of user-facing services one proxy
Services provides access to Hadoop via
proxying the HTTP request to
authentication Services authentication
for rest API access and web SSO flow for
user interfaces so there's our rest and
finally Client Services client
development can be done with the
scripting through DSL or using the nox
shell classes so the first one is if you
have a website coming in and out your
HTTP request again your three different
services or what's coming in and out of
the Hadoop file system so there's a
couple of the security setups let's go
ahead and take a look at workflow system
the uzi and there's some other ones out
there Uzi is the one that's specific to
Hadoop there's also like a zookeeper out
there and some other ones uzi's pretty
good Uzi is a workflow scheduler system
to manage Hadoop jobs and so you have a
workflow engine and a coordinator engine
so it consists of two parts what's going
on and coordinating what's going on and
uh directed acyclic graph dags which
specifies a sequence of actions to be
executed these consist of workflow jobs
triggered by time and data available and
If you're not familiar with directed
acylic graphs or dags or whatever
terminology you want to throw at this
this is basically a flow chart you start
with process a the next process would be
process B when process a is done and it
might be that process C can only be done
when process d e and f are done so you
want to be able to control this you
don't want it to process the machine
learning script and then pull the data
in that you want to process it on you
want to make sure it's going in the
right order so these consist of workflow
jobs and they're triggered by time and
data availability so maybe you're
pulling stocks in the middle of the
night and once the stock is all pulled
so there's our time sequence it says Hey
they've been posted on the other
websites they post them usually after
the stock market closes the highs and
lows and everything then once that data
has been brought then you know in a Time
specifics bring the data at a certain
time once the data is available then we
want to trigger our machine learning
script for what's going to happen next
and so we can see here we have a start
our map reduce program our action node
and it begins and either we have a
success then we notify the client of
success usually an email sent out in
successful completion or we don't have a
success we have an error notify client
of error email action node kill
unsuccessful termination and usually at
this point they say email action
notification but I'm mostly that's
usually a pager system and so you see
all the tech guys running to the server
room or wherever you know where our
pager just went off we got to figure out
what went down you know the other one is
you just look at the next morning you go
oh let's make sure everything went
through this morning and check all your
successes with the error usually it's
sent to your pager and your emergency
call to open up in the middle of the
night log in so that concludes our basic
setup with the Hadoop ecosystem so a
quick recap on the Hadoop ecosystem we
covered going looking at the middle part
we had the Hadoop as a file system and
how it stores data across multiple
servers saves money because it's about a
tenth of the cost of using Enterprise
computers we looked at yarn cluster
resource management and how that works
to hold everything together and then we
looked at a lot of data processing how
does it process in and out of the Hadoop
System including the map and reduce
setup which is the Hadoop basic in Java
and for that we looked at data
collection and ingestion with scoop and
Flume we looked at queries using the
scripting language Pig and the SQL
queries through Hive we glanced at spark
remember spark usually comes installed
now at the Hadoop system because it does
so much of its own processing it covers
a lot of the data in real-time data
analysis setup on there we looked at how
machine learning we looked at Apache
ambari for management and monitoring
kind of your security guard and traffic
control just like we have scoop and
Flume which brings data in there we
looked at Kafka and Apache storm term
which is for streaming data and then we
looked at Apache Ranger and Apache Knox
for security and finally we went in
through the we took a glance at Uzi for
your workflow system now whether you are
an aspirin data analyst a seasoned ID
professional or a business leader
looking to Leverage The Power of Big
Data the data engineering postgraduate
program by simply learn in collaboration
with Purdue University and IBM please
tell her to meet your needs this program
offers an exceptional opportunity for
professional growth and exposure with
the focus on practical learning and in
alignment with industry leading
certifications from AWS and Azure now
this program equips participants with
essential skills required to excel in
this field of data engineering by
enrolling in this applied learning
program individuals can gain Master
crucial data engineering techniques and
Technologies annual today and take the
next step towards becoming a proficient
Big Data professional check out the
course Link in the description box below
for more details
what is data science let's start with
some of the common definitions that's
doing the rounds some say that data
science is a powerful new approach for
making discoveries from data
others term it as an automated way to
analyze enormous amounts of data and
extract information from it
still others refer to it as a new
discipline which combines aspects of
Statistics mathematics programming and
visualization to gain insights
now that you have looked at some of its
definitions let's learn more about data
science
when domain expertise and scientific
methods are combined with technology we
get data science which enables one to
find solutions for existing problems
let's look at each of the components of
data science separately the first
component is domain expertise and
scientific methods data scientists
should also be domain experts as they
need to have a passion for data and
discover the right patterns in them
traditionally domain experts like
scientists and statisticians collected
and analyzed the data in a laboratory
setup or a controlled environment
the data was then subject to relevant
laws or mathematical and statistical
models to analyze the data set and
derive relevant information from it for
instance they use the models to
calculate the mean median mode standard
deviation and so on of a data set it
helped them test their hypothesis or
create a new one
in the next slide we will see how data
science technology has now made this
process faster and more efficient but
before we do that let's understand the
different types of data analysis an
important aspect of data science
data analysis can either be descriptive
where one studying a data set to explain
what happened or be predictive where one
creates a model based on existing
information to predict the outcome and
behavior
it can also be prescriptive where one
suggests the action to be taken in a
given situation using the collected
information
we now have access to tools and
techniques that process data and extract
the information we need for instance
there are data processing tools for data
wrangling we have new and flexible
programming languages that are more
efficient and easier to use
with the creation of operating systems
that support multiple OS platforms it's
now easier to integrate systems and
process Big Data application designs and
extensive software libraries help
develop more robust scalable and
data-driven applications
data scientists use these Technologies
to build data models and run them in an
automated fashion to predict the outcome
efficiently
this is called machine learning which
helps provide insights into the
underlying data
they can also use data science
technology to manipulate data extract
information from it and use it to build
tools applications and services but
technological skills and domain
expertise alone without the right
mathematical and statistical knowledge
might lead data scientists to find
incorrect patterns and convey the wrong
information
now that you have learned what data
science is it will be easier to
understand what a data scientist does
data scientists start with a question or
a business problem
then they use data acquisition to
collect data sets from The Real World
the process of data wrangling is
implemented with data tools and modern
technologies that include data cleansing
data manipulation data Discovery and
data pattern identification
the next step is to create and train
models for machine learning
they then design mathematical or
statistical models
after designing a data model it's
represented using data visualization
techniques
the next task is to prepare a data
report
after the report is prepared they
finally create data products and
services
let us now look at the various skills a
data scientists should have
data scientists should ask the right
questions for which they need domain
expertise the Curiosity to learn and
create Concepts and the ability to
communicate questions effectively to
domain experts
data scientists should think
analytically to understand the hidden
patterns in a data structure
they should Wrangle the data by removing
redundant and irrelevant data collected
from various sources
statistical thinking and the ability to
apply mathematical methods are important
traits for a data scientist
data should be visualized with graphics
and proper storytelling to summarize and
communicate the analytical results to
the audience
to get these skills they should follow a
distinct roadmap it's important they
adopt the required tools and techniques
like Python and its libraries they
should build projects using real world
data sets that include data.gov NYC open
data Gap minder and so on
they should also build data-driven
applications for Digital Services and
data products
scientists work with different types of
data sets for various purposes now that
big data is generated every second
through different media the role of data
science has become more important
so you need to know what big data is and
how you are connected to it to figure
out a way to make it work for you
every time you record your heartbeat
through your phone's biometric sensors
post or tweet on The Social Network
create any blog or website switch on
your phone's GPS Network upload or view
an image video or audio in fact every
time you log into the internet you are
generating data about yourself your
preferences and your lifestyle
big data is a collection of these and a
lot more data that the world is
constantly creating in this age of the
internet of things or iot big data is a
reality and a need
big data is usually referenced by three
vs volume velocity and variety
volume refers to the enormous amount of
data generated from various sources
big data is also characterized by
velocity huge amounts of data flow at a
tremendous speed from different devices
sensors and applications
to deal with it an efficient and timely
data processing is required
variety is the third V of Big Data
because big data can be categorized into
different formats like structured
semi-structured and unstructured
structured data is usually referenced to
as rdbms data which can be stored and
retrieved easily through sqls
semi-structured data are usually in the
form of files like XML Json documents
and nosql database
text files images videos or multimedia
content are examples of unstructured
data
in short big data is a very large
information database usually stored on
distributed systems or machines
popularly referred to as Hadoop clusters
but to be able to use this database we
have to find a way to extract the right
information and data patterns from it
that's where data science comes in data
science helps to build information
driven Enterprises
let's go on to see the applications of
data science in different sectors
social network platforms such as Google
Yahoo Facebook and so on collect a lot
of data every day which is why they have
some of the most advanced data centers
spread across the world
having data centers all over the world
and not just in the US help these
companies serve their International
customers better and faster without any
network latency they also help them deal
effectively with the enormous amount of
data so what do all these different
sectors do with all this big data
their team of data scientists analyze
all the raw data with the help of modern
algorithms and data models to turn it
into information
they then use this information to build
Digital Services data products and
information driven webs
now let's see how these products and
services work we'll first look at
LinkedIn let's suppose that you are a
data scientist based in New York city so
it's quite likely that you would want to
join a group or build connections with
people related to data science in New
York City
now what LinkedIn does with the help of
data science is that it looks at your
profile your posts and likes the city
you are from the people you are
connected to and the groups you belong
to then it matches all that information
with its own database to provide you
with information that is most relevant
to you
this information could be in the form of
news updates that you might be
interested in Industry connections or
professional groups that you might want
to get in touch with or even job
postings related to your field and
designation these are all examples of
data services let's now look at
something that we use every day Google's
search engine
Google's search engine has the most
unique search algorithm which allows
machine learning models to provide
relevant search recommendations even as
the user types in his or her query
this feature is called autocomplete it
is an excellent example of how powerful
machine learning can be
there are several factors that influence
this feature
the first one is query volume Google's
algorithms identify unique and
verifiable users that search for any
particular keyword on the web
based on that it builds a query volume
for instance Republican debate 2016
Ebola threat CDC or the center of
Disease Control and so on are some of
the most common user queries
another important factor is a
geographical location
the algorithms tag a query with the
locations from where it is generated
this makes a query volume location
specific
it's a very important feature because
this allows Google to provide relevant
search recommendations to its user based
on his or her location
and then of course the algorithms
consider the actual keywords and phrases
that the user types in
it takes up those words and crawls the
web looking for similar instances
the algorithms also try to filter or
scrub out inappropriate content
for instance sexual violent or
terrorism-related content hate speeches
and legal cases are scrubbed out from
the search recommendations
but how does data science help you
today even the healthcare industry is
beginning to tap into the various
applications of data science
to understand this let's look at
wearable devices these devices have
biometric sensors and a built-in
processor to gather data from your body
when you are wearing them
they transmit this data to the big data
analytics platform via the iot Gateway
ideally the platform collects hundreds
of thousands of data points and the
collected data is ingested into the
system for further processing
the big data analytics platform applies
data models created by data scientists
and extracts the information that is
relevant to you
it sends the information to the
engagement dashboard where you can see
how many steps you want what your heart
rate is over a period of Time how good
your sleep was how much calories you
burned and so on
knowing such details would help you to
set personal goals for a healthy
lifestyle and reduce overall health care
and insurance costs it would also help
your doctor record your vitals and
diagnose any issue
the finance sector can easily use data
science to help it function more
efficiently
suppose a person applies for a loan the
loan manager submits the application to
the Enterprise infrastructure for
processing
the analytics platform applies data
models and algorithms and creates an
engagement dashboard for the loan
manager
the dashboard would show the applicant's
credit reports credit history amount if
approved and risks associated with him
or her
the loan manager can now easily take a
look at all the relevant information and
decide whether the loan can be approved
or not
governments across different countries
are gradually sharing large data sets
from various domains with the public
this kind of transparency makes the
government seem more trustworthy it
provides the country data that can be
used to prepare itself for different
types of issues like climate change and
Disease Control
it also helps encourage people to create
their own digital products and services
the U.S government hosts and maintains
data.gov a website that offers
information about the federal government
it provides access to over 195 000 data
sets across different sectors
the US government has kicked off a
number of strategic initiatives in the
field of data science that includes U.S
digital service and open data
we have seen how data science can be
applied across different sectors
let's now take a look at the various
challenges that a data scientist faces
in the real world while dealing with
data sets data quality the quality of
data is mostly not up to the set
standards you will usually come across
data that is inconsistent inaccurate and
complete not in the desirable format and
with anomalies
integration data integration with
several Enterprise applications and
systems is a complex and painstaking
task
unified platform data is distributed to
Hadoop distributed file system or hdfs
from various sources to ingest process
analyze and visualize huge data sets
the size of these Hadoop clusters can
vary from few nodes to thousand nodes
the challenge is to perform analytics on
these large data sets efficiently and
effectively
this is where python comes into play
with its powerful set of libraries
functions modules packages and
extensions
python can efficiently tackle each stage
of data analytics that includes data
acquisition python libraries such as
Scrappy comes handy here
data wrangling python data frames are
very efficient in handling large data
sets and makes data wrangling easier
with its powerful functions
explore
matplotlib libraries are very rich when
it comes to data exploration
model
scikit learns statistical and
mathematical functions to help to build
models for machine learning
visualization modern libraries such as
Boca creates very intuitive and
interactive visualization
its huge set of libraries and functions
make big data analytics seem easy and
hence solves a bigger problem
python applications and programs are
portable and helps them scale out on any
big data platform
python is an open source programming
language that lets you work quickly and
integrate systems more effectively
now that we have talked about how the
python libraries help the different
stages of data analytics let's take a
closer look at these libraries and how
they support different aspects of data
science
numpy or numerical python is the
fundamental package for scientific
computing
scipy is the core of scientific
Computing libraries and provides many
user-friendly and efficiently designed
numerical routines
matplotlib is a python 2D plotting
Library which produces publication
quality figures in a variety of hard
copy formats and interactive
environments across platforms
scikit-learn is built on numpy scipy and
matplotlib for data mining and data
analysis
pandas is a library providing high
performance easy to use data structures
and data analysis tools for python
all these libraries modules and packages
are open source and hence using them is
convenient and easy
there are numerous factors which
positions python well and makes it the
tool for data science
python is easy to learn it's a general
purpose function and object-oriented
programming language
as python is an open source programming
language it is readily available easy to
install and get started it also has a
large presence of Open Source Community
for software development and support
Python and its tools enjoy
multi-platform support
applications developed with pycon
integrate easily with other Enterprise
systems and applications
there are a lot of tools platforms and
products in the market from different
vendors as they offer great support and
services
Python and its libraries create unique
combinations for data science because of
all these benefits it's usually popular
among academicians mathematicians
statisticians and technologists
python is supported by well-established
data platforms and processing Frameworks
that help it analyze data in a simple
and efficient way
Enterprise Big Data platform
Cloudera is the Pioneer in providing
enterprise-ready Hadoop Big Data
platform and supports python
hortonworks is another Hadoop Big Data
platform provider and supports python
mapreduce map R is also committed to
Python and provides the Hadoop Big Data
platform
big data processing framework
mapreduce spark and Flink provides very
robust and unique data processing
framework and support python
Java Scala and python languages are used
for big data processing framework
but to access Big Data you have to use a
big data platform which is a combination
of the Hadoop infrastructure also known
as Hadoop distributed file system or
hdfs and an analytics platform
Hadoop is a framework that allows data
to be distributed across clusters of
computers for faster cheaper and
efficient computing
it's completely developed and coded in
Java one of the most popular analytics
platforms is Spark
it easily integrates with hdfs
it can also be implemented as a
standalone analytics platform and
integrate it with multiple data sources
it helps data scientists perform their
work more efficiently
spark is built using Scala
since there is a disparity in the
programming language that data
scientists use and that of the Big Data
platform it impedes data access and Flow
as python is a data scientist's first
language of choice both Hadoop and Spark
provide python apis that allow easy
access to the Big Data platform
consequently a data scientist need not
learn Java or Scala or any other
platform-specific data languages and can
instead focus on performing data
analytics
there are several motivations for python
Big Data Solutions
big data is a continuously evolving
field which involves adding new data
processing Frameworks that can be
developed using any programming language
moreover new innovation and research is
driving the growth of Big Data Solutions
and platform providers
it would be difficult for data
scientists to focus on analytics if they
have to constantly upgrade themselves on
information or under the hood
architecture or implementation of the
platform therefore it's important to
keep the entire data science platform
and any language agnostic to simplify a
data scientist's job
consequently almost all major vendors
solution providers and data processing
framework developers are providing
python apis this allows a data scientist
to perform big data analytics using only
python rather than learning other
languages like Java or Scala to help
them work on the big data platform
let's look at an example and understand
how data is stored across hadoop's
distributed clusters
big data is generated from different
data sources a large file usually
greater than 100 megabytes gets routed
from a name node to data nodes
name nodes hold the metadata information
about the files stored on data nodes it
stores the address and information of a
block of file and the data node
associated with it
data nodes hold the actual data blocks
the file is split into multiple smaller
files usually of 64 megabytes or 128
megabyte size
it's then copied to multiple physical
servers the smaller files are also
called file blocks one file block gets
replicated to different servers the
default replication factor is three
which means a single file block gets
copied at least three times on different
servers or data nodes
there is also a secondary name node
which keeps a backup of all the metadata
information stored on the main or
primary node this node can be used if
and when the main name node fails
now that you have understood a little
about hdfs let's look at the second core
component of Hadoop mapreduce the
primary framework of the hdfs
architecture
a file is split into three blocks as
split 0 split 1 and split 2.
when a request comes in to retrieve the
information the mapper task is executed
on each data node that contains the file
blocks
the mapper generates an output
essentially in the form of key value
pairs that are sorted copied and merged
once the mapper task is complete the
reducer works on the data and stores the
output on hdfs this completes the
mapreduce process
let's discuss the mapreduce functions
mapper and reducer in detail
the mapper
Hadoop ensures that mappers run locally
on the nodes which hold a particular
portion of the data to avoid the network
traffic
multiple mappers run in parallel and
each mapper processes a portion of the
input data
the input and output of the mapper are
in the form of key value pairs note that
it can either provide zero or more key
value pairs as output
the reducer after the map phase all
intermediate values for an intermediate
key are combined into a list which is
given to a reducer all values associated
with a particular intermediate key are
directed to the same reducer this step
is known as Shuffle and sort there may
be a single reducer or multiple reducers
note that the reducer also provides
outputs in the form of zero or more than
one final key value pairs
these values are then returned to hdfs
the reducer usually emits a single key
value pair for each input key
you have seen how mapreduce is critical
for hdfs to function a good thing is you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python with the help of Hadoop streaming
API
Hadoop streaming acts like a bridge
between your python code and the Java
based hdfs and lets you seamlessly
access Hadoop clusters and execute
mapreduce tasks
you have seen how mapreduce is critical
for hdfs to function thankfully you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python
shown here are some user-friendly python
functions that are written for the
mapper class
suppose we have the list of numbers we
want to square
we have the square function defined as
shown on the screen
we can call the map function with a list
and the function which is to be executed
on each item in that list
the output of this process is as shown
on the screen
reducer can also be written in Python
here we would like to sum the squared
numbers of the previous map operation
this can be done using the sum operation
as shown on the screen
we can now call the reduce function with
the list of data which is to be
aggregated and aggregator function in
our case sum is used for this purpose
Big Data analysis requires a large
infrastructure Cloudera provides
enterprise-ready Hadoop Big Data
platform which supports python as well
to execute Hadoop jobs you have to first
install Cloudera
it's preferable to install Cloud era's
virtual machine on a Unix system as it
functions best on it
to set up the Cloudera Hadoop
environment visit the Cloudera link
shown here
select quick start download for CDH 5.5
and VMware from the drop down lists
click the download now button
once the VM image is downloaded please
use 7-Zip to extract the files to
download and install it visit the link
shown on screen
Cloudera VMware has some system
prerequisites
the 64-bit virtual machine requires a
64-bit host operating system or Os and a
virtualization product that can support
a 64-bit guest OS
to use a VMware VM you must use a player
compatible with workstation 8.x or
higher such as player 4.x or higher or
Fusion 4.x or higher
you can use older versions of
workstation to create a new VM using the
same virtual disk or vmdk file but some
features in VMware tools will be
unavailable
the amount of ram required will vary
depending on the runtime option you
choose
to launch the VMware Player you will
either need a VMware Player for Windows
and Linux or VMware Fusion for Mac
so please visit the VMware link shown on
screen to download the relevant VMware
Player Now launch the VMware Player with
the Cloudera VM
the default username and password is
Cloudera
click the terminal icon as shown here it
will launch the Unix terminal for Hadoop
hdfs interaction
to verify that the Unix terminal is
functioning correctly type in PWD which
will show you the present working
directory you can also type in LS space
hyphen LRT to list all the current files
folders and directories these are some
simple unix commands which will come in
handy later while you are implementing
mapreduce tasks
you have seen how the Hadoop distributed
file system works along with mapreduce
the data is written on and read by disks
mapreduce jobs require a lot of disk
read and write operations which is also
known as disk IO or input and output
reading and writing to a disk is not
just expensive it can also be slow and
impact the entire process and operation
this is specifically true for iterative
processes Hadoop is built for write once
read many type of jobs which means it's
best suited for jobs that don't have to
be updated or accessed frequently but in
several cases particularly in analytics
and machine learning users need to write
and rewrite commands to access and
compute on the same data more than once
every time such a request is sent out
mapreduce requires that data is read and
or written onto disks directly note that
though the time to access or write on
disks is measured in milliseconds when
you are dealing with large file sizes
the time Factor gets compounded
significantly this makes the process
highly time consuming
in contrast Apache spark uses resilient
distributed data sets or rdds to carry
out such computations
rdds allow data to be stored in memory
which means that every time users want
to access the same data a disk i o
operation is not required make an easily
access data stored in the cache
accessing the cache or Ram is much
faster than accessing disks for instance
if disk access is measured in
milliseconds in-memory data access is
measured in sub milliseconds this
radically reduces the overall time taken
for iterative operations on large data
sets in fact programs on spark run at
least 10 to 100 times faster than on
mapreduce that's why spark is gaining
popularity among most data scientists as
it is more time efficient when it comes
to running analytics and machine
learning computations
one of the main differences in terms of
Hardware requirements for mapreduce and
Spark is that while mapreduce requires a
lot of servers and CPUs spark
additionally requires a large and
efficient Ram
let's understand resilient distributed
data sets in detail as you have already
seen the main programming approach of
spark is rdd
rdds are fault tolerant collections of
objects spread across a cluster that you
can operate on in parallel they are
called fault tolerant because they can
automatically recover from machine
failure
you can create an rdd either by copying
the elements from an existing collection
or by referencing a data set stored
externally say on an hdfs
rdds support two types of operations
Transformations and actions
Transformations use an existing data set
to create a new one for example map
creates a new rdd containing the results
after passing the elements of the
original data set through a function
some other examples of Transformations
are filter and join
actions compute on the data set and
return the value to the driver program
for example reduce Aggregates all the
rdd elements using a specified function
and returns this value to the driver
program
some other examples of actions are count
collect and Save
it's important to note that if the
available memory is insufficient then
spark writes the data to disk
here are some of the advantages of using
spark it's almost 10 to 100 times faster
than Hadoop mapreduce
it has a simple data processing
framework it provides interactive apis
for python that allow faster application
development
it has multiple tools for complex
analytics operations these tools help
data scientists perform machine learning
and other analytics much more
efficiently and easily than most
existing tools
it can easily be integrated with the
existing Hadoop infrastructure
Pi spark is the python API used to
access the spark programming model and
perform data analysis
let's take a look at some transformation
functions and action methods which are
supported by pi spark for data analysis
these are some common transformation
functions
map returns rdd formed by passing data
elements from The Source data set
filter
returns rdd based on selected criteria
flatmap
Maps items present in the data set and
returns a sequence
Reduce by key
returns key value pairs where values for
each key is aggregated by a given reduce
function
let's now look at some common action
functions
collect returns all elements of the data
set as an array
count Returns the number of elements
present in the data set
first
Returns the first element in the data
set
take
Returns the number of elements as
specified by the number in the
parentheses
spark context or SC is the entry point
to spark for the spark application and
must be available at all times for data
processing
there are mainly four components in
spark tools
spark SQL it's mainly used for querying
the data stored on hdfs as a resilient
distributed data set or rdd in spark
through integrated apis in Python Java
and Scala
spark streaming it's very useful for
data streaming process and where data
can be read from various data sources
ml lib it's mainly used for machine
learning processes such as supervised
and unsupervised learning
graph x it can be used to process or
generate graphs with rdds
let's set up the Apache spark
environment and also learn how to
integrate spark with jupyter notebook
first visit the Apache link and download
Apache spark to your system
now use 7-Zip software and extract the
files to your system's local directory
to set up the environment variables for
spark first set up the user variables
click new and then enter spark home in
the variable name and enter the spark
installation path as variable value
now click on the path and then click new
and enter the spark bin path from the
installed directory location
now let's set up the pi spark notebook
specific variables
this will integrate The Spark engine
with jupyter notebook
type in pi spark it will launch a
jupyter notebook after a while
create a python notebook and type in SC
command to check the spark context now
whether you are an aspirin data analyst
a seasoned ID professional or a business
leader looking to Leverage The Power of
Big Data the data engineering
postgraduate program by simply learn in
collaboration with Purdue University and
IBM please tell her to meet your needs
this program offers an exceptional
opportunity for professional growth and
exposure with the focus on practical
learning and in alignment with industry
leading certifications from AWS and
Azure now this program equips
participants with essential skills
required to excel in this field of data
engineering by enrolling in the supplied
learning program individuals can gain
Master crucial data engineering
techniques and Technologies enroll today
and take the next step towards becoming
a proficient Big Data professional check
out the course Link in the description
box below for more details now let's
look at some general Hadoop questions so
what are the different vendor specific
distributions of Hadoop now all of you
might be aware that Hadoop or Apache
Hadoop is the core distribution of
Hadoop and then you have different
vendors in the market which have
packaged the Apache Hadoop in a cluster
management solution which allows
everyone to easily deploy manage monitor
upgrade your clusters so here are some
vendor specific distributions we have
Cloudera which is the dominant one in
the market we have Horton works and now
you might be aware that clouder and
hortonworks have merged so it has become
a bigger entity you have map R you have
Microsoft is your IBM's infosphere and
Amazon web services so these are some
popularly known vendor-specific
distributions if you would want to know
more about the Hadoop distributions you
should basically look into Google and
you should check for Hadoop different
distributions Wiki page so if I type
Hadoop different distributions and then
I check for the the Wiki page that will
take me to the distributions and
Commercial support page and this
basically says that the sold products
that can be called a release of Apache
Hadoop come from apache.org so that's
your open source community and then you
have various vendor specific
distributions which basically are
running in one or the other way Apache
Hadoop but they have packaged it as a
solution like a installer so that you
can easily set up clusters on set of
machines so have a look at this page and
read through about different
distributions of Hadoop coming back
let's look at our next question so what
are the different Hadoop configuration
files now whether you're talking about
Apache Hadoop Cloudera hortonworks map r
or no matter which other distribution
these config files are the most
important and existing in every
distribution of Hadoop so you have
Hadoop environment.sh wherein you will
have environment variable such as your
Java path what would be your process ID
path where will your logs get stored
what kind of metrics will be collected
and so on your core hyphen site file has
the hdfs path now this has many other
properties like enabling trash or
enabling High availability or discussing
or mentioning about your zookeeper but
this is one of the most important file
you have hdfs hyphen site file now this
file will have other information related
to your Hadoop cluster such as your
replication Factor where will name node
store its metadata on disk if a data
node is running where would data node
store its data if a secondary name node
is running where would that store a copy
of name nodes metadata and so on your
mapred hyphen site file is a file which
will have properties related to your map
reduce processing you also have Masters
and slaves now these might be deprecated
in a vendor-specific distribution and in
fact you would have a yarn hyphen site
file which is based on the yarn
processing framework which was
introduced in Hadoop version 2 and this
would have all your resource allocation
and resource manager and node manager
related property again if you would want
to look at default properties for any
one of these for example let's say hdfs
hyphen site file I could just go to
Google and type in one of the properties
for example I would say DFS dot name
node.name dot directory and as I know
this property belongs to hdfs hyphen
site file and if you search for this it
will take you to the first link which
says stfs default XML you can click on
this and this will show you all the
properties which can be given in your
stfs hyphen site file it also shows you
which version you are looking at and you
can always change the version here so
for example if I would want to look at
2.6.5 I just need to change the version
and that should show me the properties
similarly you can just give a property
which belongs to say core hyphen site
file for example I would say FS dot
default fs and that's a property which
is in core hyphen sci-fi and somewhere
here you would see core minus default
dot XML and this will show you all the
properties so similarly you could search
for properties which are related to yarn
hyphen site file or mapred hyphen site
file so I could say yarn dot resource
manager and I could look at one of these
properties which will directly take me
to yarn default XML and I can see all
the properties which can be given in
yarn and similarly you could say map
reduce dot job dot reduces and I know
this property belongs to mapreduce
hyphen site file and this takes you to
the default XML so these are important
config files and no matter which
distribution of Hadoop you are working
on you should be knowing about these
config files whether you work as a
Hadoop admin or you work as a Hadoop
developer knowing these config
properties would be very important and
that would also showcase your internal
knowledge about the configs which drive
your Hadoop cluster let's look at the
next question so what are the three
modes in which Hadoop can run so you can
have Hadoop running in a standalone mode
now that's your default mode it would
basically use a local file system and a
single Java process so when you say
Standalone mode it is as you downloading
Hadoop related package on one single
machine but you would not have any
process running that would just be to
test Hadoop functionalities you could
have a pseudo distributed mode which
basically means it's a single node
Hadoop deployment now Hadoop as a
framework has many many services so it
has a lot of services and those Services
would be running irrespective of your
distribution and each service would then
have multiple processes so your pseudo
distributed mode is a mode of cluster
where you would have all the important
processes belonging to one or multiple
Services running on a single node if you
would want to work on a pseudo
distributed mode and using a cloud era
you can always go to Google and search
for cloudera's quick start VM you can
download it by just saying Cloudera
quick start VM and you can search for
this and that will allow you to download
a quick start VM follow the instructions
and you can have a single node Cloudera
cluster running on your virtual machines
for more information you can refer to
the YouTube tutorial where I have
explained about how to set up a quick
start VM coming back you could have
finally a production setup or a fully
distributed mode which basically means
that your Hadoop framework and its
components would be spread across
multiple machines so you would have
multiple services such as hdfs yarn
Flume scope Kafka hbase Hive Impala and
for these Services there would be one or
multiple processes distribute across
multiple nodes so this is normally what
is used in production environment so you
could say Standalone would be good for
testing pseudo distributed could be good
for testing and development and fully
distributed would be mainly for your
production setup now what are the
differences between regular file system
and http
when you say regular file system you
could be talking about a Linux file
system or you could be talking about a
Windows based operating system so in
regular file system we would have data
maintained in a single system so the
single system is where you have all your
files and directories so it is having
low fault tolerance right so if the
machine crashes your data recovery would
be very difficult unless and until you
have a backup of that data that also
affects your processing so if the
machine crashes or if the machine fails
then your processing would be blocked
now the biggest challenge with regular
file system is the seek time the time
taken to read the data so you might have
one single machine with huge amount of
disks and huge amount of ram but then
the time taken to read that data when
all the data is stored in one machine
would be very high and that would be
with least fault tolerance if you talk
about sdfs your data is distributed so
sdfs stands for Hadoop distributed file
system so here your data is distributed
and maintained on multiple systems so it
is never one single machine it is also
supporting reliability so whatever is
stored in hdfs say a file being stored
depending on its size is split into
blocks and those blocks will be spread
across multiple nodes not only that
every block which is stored on a node
will have its replicas stored on other
nodes replication Factor depends but
this makes sdfs more reliable in cases
of your slave nodes or data nodes
crashing you will rarely have data loss
because of Auto replication feature now
time taken to read the data is
comparatively more as you might have
situations where your data is
distributed across the nodes and even if
you are doing a parallel read your data
read might take more time because it
needs coordination for multiple machines
however if you are working with huge
data date which is getting stored it
will still be beneficial in comparison
to reading from a single machine so you
should always think about its
reliability through Auto replication
feature its fault tolerance because of
your data getting stored across multiple
machines and its capability to scale so
when you talk about sdfs we are talking
about horizontal scalability or scaling
out when you talk about regular file
system you are talking about vertical
scalability which is scaling up now
let's look at some specific sdfs
questions what is this why is sdfs Fault
tolerant now as I just explained in
previous slides your sdfs is Fault
tolerant as it replicates data on
different data nodes so you have a
master node and you have multiple slave
nodes or data nodes where actually the
data is getting stored now we also have
a default block size of 128 MB that's
the minimum since Hadoop version 2. so
any 5 file which is up to 128 MB would
be using one logical block and if the
file size is bigger than 128 MB then it
will be split into blocks and those
blocks will be stored across multiple
machines now since these blocks are
stored across multiple machines it makes
it more fault tolerant because even if
your machines fail you would still have
a copy of your block existing on some
other machine now there are two aspects
here one we talk about the first rule of
replication which basically means you
will never have two identical blocks
sitting on the same machine and the
second rule of replication is in terms
of rack awareness so if your machines
are placed in racks as we see in the
right image you will never have all the
replicas placed on the same rack even if
they are on different machines so it has
to be fault tolerant and it has to
maintain redundancy so at least one
replica will be placed on some other
node on some other rack that's how sdfs
is Fault tolerant now here let's
understand the architecture of sdfs now
as I mentioned earlier you would in a
Hadoop cluster the main service is your
hdfs so for your sdfs service you would
have a name node which is your master
process running on one of the machines
and you would have data nodes which are
your slave machines getting stored
across or getting or the processes
running across multiple machines each
one of these processes has an important
role to play when you talk about sdfs
whatever data is written to hdfs that
data is split into blocks depending on
its size and the blocks are randomly
distributed across nodes with auto
replication feature these blocks are
also Auto replicated across multiple
machines with the first condition that
no two identical blocks will sit on the
same machine now as soon as the cluster
comes up your data nodes which are part
of the cluster and based on config files
would start sending their heartbeat to
the name node and this would be every
three seconds what does name node do
with that name node will store this
information in its Ram so name node
starts building a metadata in its RAM
and that metadata has information of
what are the data nodes which are
available in the beginning now when a
data writing activity starts and the
blocks are distributed across data nodes
data nodes every 10 seconds will also
send a block report to name node so name
node is again adding up this information
in its Ram or the metadata in Ram which
earlier had only data node information
now name node will also have information
about what are the files the files are
split in which blocks the blocks are
stored on which machines and what are
the file permissions now while name node
is maintaining this metadata in Ram name
node is also maintaining metadata in
disk so that is what we see in the red
box which basically has information of
whatever information was written to hdf
so to summarize your name node has
metadata in Ram and metadata in disk you
your data nodes are the machines where
your blocks or data is actually getting
stored and then there is a auto
replication feature which is always
existing unless and until you have
disabled it and your read and write
activity is a parallel activity however
replication is in sequential activity
now this is what I mentioned about when
you talk about name node which is the
master process hosting metadata in disk
and RAM so when we talk about disk it
basically has a edit log which is your
transaction log and your FS image which
is your file system image right from the
time the cluster was started this
metadata in disk was existing and this
gets appended every time read write or
any other operations happen on stfs
metadata in Ram is dynamically built
every time the cluster comes up which
basically means that if your cluster is
coming up name node in the initial few
seconds or few minutes would be in a
safe mode which basically means it is
busy registering the information from
data nodes so name node is one of the
most critical processes if name node is
down and if all other processes are
running you will not be able to access
the cluster name nodes metadata in disk
is very important for name node to come
up and maintain the cluster name nodes
metadata in Ram is basically for all or
satisfying all your client requests now
when we look at data nodes as I
mentioned data nodes hold the actual
data blocks and they are sending these
block reports every 10 seconds so the
metadata in name nodes Ram is constantly
getting updated and metadata in disk is
also constantly getting updated based on
any kind of write activity happening on
the cluster now data node which is
storing the block will also help in any
kind of read activity whenever a client
requests so whenever a client or an
application or an API would want to read
the data it would first talk to name
node name node would look into its
metadata on RAM and confirm to the
client which machines could be reached
to get data that's where your client
would try to read the data from sdfs
which is actually getting the data from
data nodes and that's how your read
write requests are satisfied now what
are the two types of metadata in name
node server holds as I mentioned earlier
metadata in disk very important to
remember edit log NFS image metadata in
Ram which is information about your data
nodes files files being split into
blocks blocks residing on data nodes and
file permissions so I will share a very
good link on this and you can always
look for more detailed information about
your metadata so you can search for sdfs
metadata directories explained now this
is from hortonworks however it talks
about the metadata in disk which name
node manages and details about this so
have a look at this link if you are more
interested in learning about metadata on
disk coming back let's look at the next
question what is the difference between
Federation and high availability now
these are the features which were
introduced in Hadoop version 2. both of
these features are about horizontal
scalability of name node prior to
version 2 the only possibility was that
you could have one single Master which
basically me means that your cluster
could become unavailable if name node
would crash so Hadoop version 2
introduced two new features Federation
and high availability however High
availability is a popular one so when
you talk about Federation it basically
means any number of name nodes so there
is no limitation to the number of name
nodes your name nodes are in a Federated
cluster which basically means name nodes
still belong to the same cluster but
they are not coordinating with each
other so whenever a write request comes
in one of the name node picks up that
request and it guides that request for
the blocks to be written on data nodes
but for this your name node does not
have to coordinate with other name node
to find out if the block ID which was
being assigned was the same one as
assigned by other name node so all of
them below belong to a Federated cluster
they are linked via a cluster ID so
whenever an application or an API is
trying to talk to Cluster it is always
going via an cluster ID and one of the
name node would pick up the read
activity or write activity or processing
activity so all the name nodes are
sharing a pool of metadata in which each
name node will have its own dedicated
pool and we can remember that by a term
called namespace or name service so this
also provides High fault tolerance
suppose your one name node goes down it
will not affect or make your cluster
unavailable you will still have your
cluster reachable because there are
other name nodes running and they are
available now when it comes to
heartbeats all your data nodes are
sending their heartbeats to all the name
nodes and all the name nodes are aware
of all the data nodes when you talk
about high availability this is where
you would only have two name nodes so
you would have an active and you would
have a standby now normally in any
environment you would see a high
availability setup with zookeeper so
zookeeper is a centralized coordination
service so when you talk about your
active and stand by name notes election
of a name node to made as active and
taking care of a automatic failover is
done by your zookeeper High availability
can be set up without zookeeper but that
would mean that a admins intervention
would be required to make a name known
as active from standby or also to take
care of failover
now at any point of time in high
availability a active name node would be
taking care of storing the edits about
whatever updates are happening on sdfs
and it is also writing these edits to a
shared location standby name node is the
one which is constantly looking for
these latest updates and applying to its
metadata which is actually a copy of
whatever your active name node has so in
this way your standby name node is
always in sync with the active name node
and if for any reason active name node
fails your standby name node will take
over and become the active remember
zookeeper plays a very important role
here it's a centralized coordination
service one more thing to remember here
is that in your high availability
secondary name node will not be allowed
so you would have a active name node and
then you will have a standby name node
which will be configured on a separate
machine and both of these will be having
access to a shared location now that
shared location could be NFS or it could
be a quorum of Journal nodes so for more
information refer to the tutorial where
I have explained about sdfs high
availability and Federation now let's
look at some logical question here
I have a input file of
how many input splits would be created
by sdfs
what would be the size of each input
split so for this you need to remember
that by default the minimum block size
is 128 MB now that's customizable if
your environment has more number of
larger files written on an average then
obviously you have to go for a bigger
block
s if your environment has a lot of files
being written but these files are of
smaller size you could be okay with 128
MB remember in Hadoop every entity that
is your directory on sdfs file on sdfs
and a file having multiple blocks each
of these are considered as objects and
for each object
Loops name nodes Ram 150 bytes is
utilized so if your block size is very
small then you would have more number of
blocks which would directly affect the
name nodes of M if you keep a block size
very high that will reduce the number of
blocks but remember that might affect in
processing because processing also
depends on split
split more number of splits more the
parallel processing so setting of block
size has to be done with consideration
about your parallelism requirement and
your name nodes Ram which is available
now coming to the question if you have a
file of 350 MB that would be split into
three blocks and here two blocks would
have 128 MB data and the third block
although the block size would still be
128 it would have only 94 MB of data so
this would be the split of this
particular file now let's understand
about rack awareness how does rack
awareness work or why do we even have
racks so organizations always would want
to place their nodes or machines in a
systematic way there can be different
approaches you could have a rack which
would have machines running on the
master processes and the intention would
be that this particular rack could have
higher bandwidth more cooling dedicated
power supply top of rack switch and so
the second approach could be that you
could have one master process running on
one machine of every rack and then you
could have other slave processes running
now when you talk about your rack
awareness one thing to understand is
that if your machines are placed within
racks and we are aware that Hadoop
follows Auto replication the rule of
replication in a rack aware cluster
would be that you would never have all
the replicas placed on the same rack so
if we look at this if we have block a in
blue color you will never have all the
three blue boxes in the same rack even
if they are on different nodes because
that makes us that makes it less fault
tolerant so you would have at least one
copy of block which would be stored on a
different rack on a different note now
let's look at this so basically here we
are talking about replicas being placed
in such a way now somebody could ask a
question can I have my block and its
replicas spread across three racks and
yes you can do that but then in order to
make it more redundant you are
increasing your bandwidth requirement so
the better approach would be two blocks
on the same rack on different machines
one copy on a different track now let's
proceed how can you restart name node
and all demons in Hadoop so if you were
working on an Apache Hadoop cluster then
you could be doing a start and stop
using Hadoop demon scripts so there are
these Hadoop demon scripts which would
be used to start and stop your Hadoop
and this is when you talk about your
Apache Hadoop so let's look at one
particular file which I would like to
show you more information here and this
talks about your different clusters so
let's look into this and so let's look
at the start and stop and here I have a
file let's look at this one and this
gives you highlights so if you talk
about Apache Hadoop this is how the
setup would be done so you would have it
download the Hadoop tar file you would
have to unturn it edit the config files
you would have to do formatting and then
start your cluster and here I have said
you using scripts so this is in case of
Apache Hadoop you could be using a start
all script that internally triggers
start DFS and start yarn and these
scripts start DFS internally would run
Hadoop demon multiple times based on
your configs to start your different
processes then your start yarn would run
yarn demand script to start your
processing related processes so this is
how it happens in Apache Hadoop now in
case of cloud era or hortonworks which
is basically a vendor specific
distribution you would have say multiple
Services which would have one or
multiple demons running across the
machines let's take an example here that
you would have machine 1 Machine 2 and
machine 3 with your processes spread
across however in case of cloud era and
hot remarks these are cluster Management
Solutions so you would never be involved
in running a script individually to
start and stop your processes in fact in
case of Cloudera you would have a
Cloudera SCM server running on one of
the machines and then Cloudera SCM
agents running on every machine if you
talk about hortonworks you would have
ambari server and ambari agent running
so your agents which are running on
every machine are responsible to monitor
the processes send also their heartbeat
to the master that is your server and
your server is the one or a service
which basically will give instructions
to the agents so in case of vendor
specific distribution your start and
stop of processes is automatically taken
care by these underlying services and
these Services internally are still
running these commands however only in
Apache Hadoop you have to manually
follow these to start and stop coming
back we can look into some command
related questions so which command will
help you find the status of blocks and
file system health so you can always go
for a file system check command now that
can show you the files for a particular
sdfs path it can show you the blocks and
it can also give you information on
status such as under replicated blocks
over replicated blocks misreplicated
blocks default replication and so on so
your fsck file system check utility does
not repair if there is any problem with
the blocks but it can give you
information of blocks related to the
files on which machines they have stored
if they are replicated as per the
replication factor or if there is any
problem with any particular replica now
what would happen if you store too many
small files in a cluster and this
relates to the block information which I
gave some time back so remember Hadoop
is coded in Java so here every directory
every file and file Creator block is
considered as an object and for every
object within your Hadoop cluster name
nodes Ram gets utilized so more number
of blocks you have more would be usage
of name node slam and if you're storing
too many small files it would not affect
your disk it would directly affect your
name node slam that's why in production
clusters admin guys or infrastructure
specialist will take care that everyone
who is writing data to hdfs follows a
quota system so that you could be
controlled in the amount of data you
write plus the count of data and
individual writes on hdfs now how do you
copy data from local system onto sdfs so
you can use a put command or a copy from
local and then given your local path
which is your source and then your
destination which is your sdfs path
remember you can always do a copy from
local using a minus F option that's a
flag option and that also helps you in
writing the same file or a new file to
hdfs so with your minus F you have a
chance of overwriting or rewriting the
data which is existing on sdfs so copy
from local or minus put both of them do
the same thing and you can also pass an
argument when you are copying to control
your replication or other aspects of
your file now when do you use DFS admin
refresh notes or RM admin refresh notes
so as the command says this is basically
to do with refreshing the node
information so your refresh notes is
mainly used when say a commissioning or
decommissioning of nodes is done so when
in node is added into the cluster or
when a node is removed from the cluster
you are actually informing Hadoop master
that this particular node would not be
used for storage and would not be used
for processing now in that case you
would be once you are done with the
process of commissioning or
decommissioning you would be giving
these commands that is refresh nodes and
other element refresh notes so
internally when you talk about
commissioning decommissioning there are
include and exclude files which are
updated and these include and exclude
files will have entry of machines which
are being added to the cluster or
machines which are being removed from
the cluster and while this is being done
the cluster is still running so you do
not have to restart your master process
however you can just use this refresh
commands to take care of your commenting
decommissioning activities now is there
any way to change replication of files
on sdfs after they are already written
and the answer is of course yes so if
you would want to set a replication
Factor at a cluster level and if you
have admin access then you could edit
your sdfs hyphen site file or you could
say Hadoop hyphen site file and that
would take care of replication Factor
being set at Master Level however if you
would want to change the replication
after the data has been written you
could always use a set rep command so
set rep command is basically to change
the replication after the data is
written you could also write the data
with a different replication and for
that you could use a minus D DFS dot
replication and give your application
Factor when you are writing data to the
cluster so in Hadoop you can let your
data be replicated as per the property
set in the config file you could write
the data with a different replication
you could change the replication after
the data is written so all these options
are available now who takes care of
replication consistency in a Hadoop
cluster and what do you mean by under
over replicated blocks now as I
mentioned your fsck command can give you
information of over or under replicated
blocks now in a cluster it is always and
always name node which takes care of
replication
so for example if you have set up a
replication of three and since we know
the first rule of replication which
basically means that you can not have
two replicas residing on the same node
it would mean that if your replication
is 3 you would need at least three data
nodes available now say for example you
had a cluster with 3 notes and
replication was set to 3 at one point of
time one of your name node crashed and
if that happens your blocks would be
under replicated that means there was a
replication Factor set but now your
blocks are not replicated or there are
not enough replicas as per the
replication Factor set this is not a
problem your master process or name node
will wait for some time before it will
start the replication of data given so
if a data road is not responding or if a
disk has crashed and if name node does
not get information
replica name node will wait for some
time and then it will start
re-replication of those missing blocks
from the available nodes however while
name node is doing it the blocks are in
under replicated situation now when you
talk about over replicated this is a
situation where name node realizes that
there are extra copies of block now this
might be the case that you had three
nodes running with the replication of
three one of the node went down
to a network failure or some other issue
within few minutes name node
re-replicated the data and then the
failed node is back with its set of
blocks again name node is smart enough
to understand that this is a over
replication situation and it will delete
set of blocks from one of the nodes it
might be the node which has been
recently added it might be your old node
which has joined your cluster again or
any node that depends on the load on a
particular node now we discussed about
Hadoop we discussed about sdfs now we
will discuss about mapreduce which is
the programming model and you can say
processing framework what is distributed
cache in mapping
we know that
may be processed might be existing on
multiple nodes so when you would have
your mapreduce program running it would
basically read the data from the
underlying disks
now this could be a costly operation if
every time the data has to be read from
disk so distributed cash is a mechanism
wherein data set or data which is coming
from the disk can be cached and
available for all worker nodes now how
will this benefit so when a map reduce
is running instead of every time reading
the data from disk it would pick up the
data from distributed cache and this
this will benefit your map reduce
so distributed Cache can be set in your
job conf where you can specify that a
file should be picked up from
distributed cache now let's understand
about these roles so what is a record
reader what is a combiner what is a
partitioner and what kind of roles do
they play in a map reduce processing
Paradigm or mapreduce operation
record reader communicates with the
input split and it basically converts
the data into key value Pairs and these
key value pairs are the ones which will
be worked upon by the mapper your
combiner is an optional phase it's like
mini reduce so combiner does not have
its own class it relies on the reducer
class basically your combiner would
receive the data from your map tasks
which would have completed works on it
based on whatever reducer class mentions
and then passes its output to the
reduced surface partitioner is basically
a phase which decides how many reduced
tasks would be used aggregate or
summarize your data so partitioner is a
phase which would decide based on the
number of keys based on the number of
map tasks your partitioner would decide
if one or multiple reduced tasks
take care of
process so either it could be
partitioner which decides on how many
reduce tasks would run or it could be
based on the properties which we have
set within the cluster which will take
care of the number of reduced tasks
which would be used always remember your
partitioner decides how outputs from
combiner are sent to reducer and to how
many reducers it controls the
partitioning of keys of your
intermediate map outputs so map phase
whatever output it generates is an
intermediate output and that has to be
taken by your partitioner or by a
combiner and then partitioner to be sent
to one or multiple reduce tasks this is
one of the common questions which you
might face why is mapreduce slower in
processing so we know mapreduce goes for
parallel processing we know we can have
multiple map tasks running on multiple
nodes at the same time we also know that
multiple reduced tasks could be running
now why does then mapreduce become a
slower approach
first of all your map reduce is a batch
oriented operation now mapreduce is very
rigid and it strictly uses mapping and
reducing phases so no matter what kind
of processing you would want to do you
would have to still provide the mapper
function and the reducer function to
work on data not only this whenever your
map phase completes the output of your
map phase which is an intermittent
output would be written to hdfs and
thereafter underlying disks and this
data would then be shuffled and sorted
and picked up for reducing phase so
every time your data being written to
hdfs and retrieved from sdfs makes
mapreduce a slower approach the question
is for a map release job is it possible
to change the number of mappers to be
created Now by default you cannot change
the number of map tasks because number
of map tasks depends on the input splits
however there are different ways in
which you can either set a property to
have more number of map tasks which can
be used or you can customize your code
or make it use a different format which
can then control the number of map tasks
by default number of map tasks are equal
to the number of splits of file you are
so if you have a 1GB of file that is
split into eight blocks or 128 MB there
would be eight map tasks running on the
cluster these map tasks are basically
running your mapper function if you have
a hard coded properties in your mapred
hyphen site file to specify more number
of map tasks then you could control the
number of map tasks let's also talk
about some data types so when you
prepare for Hadoop when you want to get
into Big Data field you should start
learning about different data
now there are different data formats
such as Avro parquet
have a sequence file or binary format
and these are different formats which
are used now when you talk about your
data types in Hadoop the SE are
implementation of your writeable and
writable comparable interfaces so for
every data type in Java you have a
equivalent in Hadoop so end in Java
would be indirectable in Hadoop float
would be float writable long would be
long writeable double writeable Boolean
writable array writable map writable and
object
these are your different data types
types or mapreduce program and these are
implementation of writable and writable
comparable interfaces what is
speculative execution
now imagine you have a cluster which has
huge number of nodes and your data is
spread across multiple slave machines or
multiple nodes now at a point of time
due to a disk degrade on network issues
or machine heating up or more load being
on a particular node there can be a
situation where your data node will
execute task in a slower manner now in
this case if speculative execution is
turned on there would be a shadow task
or a another
similar task running on some other node
for the same processing so whichever
task finishes first will be accepted and
the other task would be killed
speculative execution could
if you are working in a intensive
workload kind of environment where if a
particular node is slower you could
benefit from a unoccupied or a node
which has less load to take care of your
processing going further this is how we
can understand so node a which might be
having a slower task you would have a
scheduler which is maintaining or having
knowledge of what are the resources
available so if speculative execution as
a property is turned on then the task
which was running slow a copy of that
task or you can say shadow task would
run on some other node and whichever
task completes first will be considered
this is what happens in your speculative
execution now how is identity mapper
different from chain mapper now this is
where we are getting deeper into
mapreduce Concepts so when you talk
about mapper identity mapper is the
default mapper which is chosen when no
mapper is specified in mapreduce driver
class so for every mapreduce program you
would have a map class which is taking
care of your mapping phase which
basically has a mapper function and
which would run one or multiple map
tasks right your programming your
program would also have a reduce class
which would be running a reducer
function which takes care of reduced
tasks running on multiple nodes now if a
mapper is not specified within your
driver class so driver class is
something which has all information
about your flow what's your map class
what is your reduce class what's your
input format what's your output format
what are the job configurations and so
on so identity mapper is the default
mapper which is chosen when no mapper
class is mentioned in your driver class
it basically implements an identity
function which directly writes all its
key pairs into output and it was defined
in old map reduce API in this particular
package but when you talk about chaining
mappers or chain mapper this is
basically a class to run multiple
mappers in a single map task or
basically you could say multiple map
tasks would run as a part of your
processing the output of first mapper
would become as an input to Second
mapper and so on and this can be defined
in the under mentioned class or
what are the major configuration
parameters required in the map reduce
program obviously we need to have the
input location we need to have the
output location so input location is
where the files will be picked up from
and this would preferably on sdfs
directory output location is the path
where your job output would be written
by your mapreduce program you also need
to specify input and output formats if
you don't specify the defaults are
considered then we need to also have the
classes which have your map and reduce
functions and if you intend to run the
code on a cluster you need to package
your class in a jar file export it to
your cluster and then this jar file
would have your mapper reducer and
Driver classes so these are important
configuration parameters which you need
to consider for a map reduce program now
what is the difference or what do you
mean by map side join and reduce side
join map side join is basically when the
join is performed at the mapping level
or at the mapping phase or is performed
by the mapper so each input data will
which is being worked upon has to be
divided into same number of partitions
input to each map is in the form of a
structured partition and is in sorted
order so Maps I join you can understand
it in a simpler way that if you compare
it with rdbms Concepts where you had two
tables which were being joined it will
always be advisable to give your bigger
table as the left side table or the
first table for your join condition and
it would be your smaller table on the
left side and your bigger table on the
right side which basically means the
smaller table could be loaded in memory
and could be used for joining so map
side join is a similar kind of mechanism
where input data is divided into same
number of partitions when you talk about
reduced side join here the join is
performed by the reducer so it is easier
to implement than map side join as all
the sorting and shuffling will send the
values or send all the values having
identical keys to the same reducer so
you don't need to have your data set in
a structured form so look into your map
site drawing or reduce side join and
other joints just to understand how
mapreduce Works however I would suggest
not to focus more on this because
mapreduce is still being used for
processing but the amount of mapreduce
based processing has decreased overall
or across the industry now what is the
role of output committer class in a
mapreduce job so committer as the name
says describes the commit of task output
for a mapreduce job so we could have
this as mentioned or capacity Hadoop map
reduce output committer you could have a
class which extends your output
committer class
so mapreduce relies on this map reduce
relies on the output committer of the
job to set up the job initialization
cleaning up the job after the job
completion that means all the resources
which were being used by a particular
job setting up the task temporary output
checking whether a task needs a commit
committing the task output and
discarding the
top so this is a very important class
and can be used within your mapreduce
job what is the process of spilling in
mapreduce what does that mean so
spilling is basically a process of
copying the data from memory buffer to
disk when obviously the buffer usage
reaches a certain threshold so if there
is not enough memory in your buffer in
your memory then the content which is
stored in buffer or memory has to be
flushed out so by default a background
thread starts spilling the content from
memory to disk after 80 percent of
buffer size is filled now when is the
buffer being used so when your map
reduce processing is happening the data
from data is being read from the disk
loaded into the buffer and then some
processing happens same thing also
happens when you are writing data to the
cluster so you can imagine for a hundred
megabytes size buffer the spilling will
start after the content of buffer
reaches 80 megabytes this is
customizable how can you set the mappers
and reducers for a mapreduce job so
these are the properties so number of
mappers and reducers as I mentioned
earlier can be customized so by default
your number of map tasks depends on the
split and number of reduced tasks
depends on the partitioning phase which
decides number of reduced tasks which
would be used depending on the key
word we can set these properties either
in the config files or provide them on
the command line or also make them part
of our code and this can control the
number of map tasks or reduce tasks
which would be run for a particular job
let's look at one more interesting
question what happens when a node
running a map task fails before sending
the output to the reducer so there was a
node which was running a map task and we
know that there could be one or multiple
map tasks running on multiple nodes and
all the map tasks have to be completed
before the further stages that such as
combiner or reducer come into existence
so in a case if in node crashes where a
map task was assigned to it the whole
task will have to be run again on some
other node so in Hadoop version 2 yarn
framework has a temporary demon called
application master so your application
Master is taking care of execution of
your application and if a particular
task on a particular node failed due to
unavailability of node it is the role of
application Master to have this task
scheduled on some other node now can we
write the output of mapreduce in
different formats of course we can so
Hadoop supports various input and output
formats so you can write the output of
mapreduce in different formats so you
could have the default format that is
text output format wherein records are
written as line of text you could have
sequence file which is basically to
write sequence files or your binary
format files where your output files
need to be fed into another mapreduce
jobs
go for a map file output format to write
output as map files you could go for a
sequence file as a binary output format
so that's again a variant of your
sequence file input format it basically
writes keys and values to
s so when we talk about binary format we
are talking about a non-human readable
format DB output format now this is
basically used when you would want to
write data to say relational databases
or say no SQL databases such as hbase so
this format also sends the reduce output
to a SQL table now let's learn a little
bit about yarn yarn which stands for yet
another resource negotiator it's the
processing framework so what benefits
did yarn bring in Hadoop version 2 and
how did it solve the issues of mapreduce
version 1. so map reduce version 1 had
major issues when it comes to
scalability or availability because
sorry in Hadoop version 1 you had only
one master process for processing layer
and that is your job tracker so your job
tracker was listening to all the task
trackers which were running on multiple
machines so your job tracker was
responsible for resource tracking and
job scheduling in yarn you still have a
processing Master but that's called
resource manager instead of job tracker
and now with Hadoop version 2 you could
even have resource manager running in
high availability mode you have node
managers which would be running on
multiple machines and then you have a
temporary demon called application
master so in case of Hadoop version 2
your resource manager or Master is only
handling the client connections and
taking care of tracking the resources
the jobs scheduling or basically taking
care of execution across multiple nodes
is controlled by application Master till
the application completes
so in yarn you can have different kind
of resource allocations that could be
done and there is a concept of container
so container is basically a combination
of RAM and CPU cores yarn can run
different kind of workloads so it is not
just map reduce kind of workload which
can be run on Hadoop version 2 but you
would have graph processing massive
parallel processing you could have a
real-time processing and huge processing
applications could run on a cluster
based on yarn so when we talk about
scalability in case of your Hadoop
version 2 you can have a cluster size of
more than 10 000 nodes and can run more
than 100
000 concurrent tasks and this is because
for every application which is launched
you have this temporary demon called
application
so if I would have 10 applications
running I would have 10 app Masters
running taking care of execution of
these applications across multiple nodes
compatibility so Hadoop version 2 is
fully compatible with whatever was
developed as per Hadoop version 1 and
all your processing needs would be taken
care by yarn
so Dynamic allocation of cluster
resources taking care of different
workloads allocating resources across
multiple machines and using them for
execution all that is taken care by yarn
multi-tenancy which basically means you
could have multiple users or multiple
teams
you could have open source and
proprietary data access engines and all
of these could be basically hosted using
the same cluster now how does yarn
allocate resources to an application
with help of its architecture so
basically you have a client or an
application or an API which talks to
resource manager resource manager is as
I mentioned managing the resource
allocation in the Clusters when you talk
about resource manager you have its
internal two components one is your
scheduler and one is your applications
manager so when we say resource manager
being the master is tracking the
resources The Source manager is the one
which is negotiating the resources with
slave it is not actually resource
manager who is doing it but these
internal components so you have a
scheduler which allocates resources to
various running applications so
scheduler is not bothered about tracking
your resources or basically tracking
your applications so we can have
different kind of schedulers such as
[Music]
first out you could have a fair
scheduler or you could have a capacity
scheduler and these schedulers basically
control how resources are allocated to
multiple applications when they are
running in parallel so there is a queue
mechanism so scheduler will schedule
resources based on requirements of
application but it is not monitoring or
tracking the status of application
your applications manager is the one
which is accepting the job submissions
it is monitoring and restarting the
application Masters so it's application
manager which is basically then
launching a application Master which is
responsible for an application so this
is how it looks so whenever a job
submission happens we already know that
resource manager is aware of the
resources which are available with every
node manager so on every node which has
fixed amount of RAM and CPU cores some
portion of resources that is your RAM
and CPU cores are allocated to node
manager now resource manager is already
aware of how much resources are
available across nodes so whenever a
client request comes in resource manager
will make a request to node manager it
will basically request node manager to
hold some resources for processing node
manager would basically approve or
disapprove this request
or holding the sources and these
resources that is a combination of RAM
and CPU cores are nothing but containers
we can allocate containers of different
sizes within yarn hyphen site file so
your node manager based on the request
from resource manager guarantees the
container which would be available for
processing that's when your resource
manager starts a temporary demon called
application Master to take care of your
execution so your app Master which was
launched by resource manager or we can
say internally applications manager will
run in one of the containers because
application Master is also a piece of
code so it will run in one of the
containers and then other containers
will be utilized for execution this is
how yarn is basically taking care of
your allocation your application Master
is managing resource needs it is the one
which is interacting with scheduler and
if a particular node crashes it is the
responsibility of App Master to go back
to the master which is resource manager
and negotiate for more resources so your
app Master will never ever negotiate
Resources with node manager directly it
will always talk to resource manager and
the source manager is the one which
negotiates the resources container as I
said is a collection of resources like
your RAM CPU Network bandwidth and your
container is located based on the
availability of resources on a
particular node so which of the
following has occupied the place of a
job tracker of mapreduce so it is your
resource manager so resource manager is
the name of the master process in Ado
question now if you would have to write
yarn commands to check the status of an
application so we could just say yarn
application minus status and then the
application ID and you could kill it
also from the command line remember your
yarn has a UI and you can even look at
your applications from the UI you can
even kill your applications from the UI
UI however knowing the command line
commands would be very useful can we
have more than one resource manager in a
yarn-based cluster yes we can that is
what Hadoop version 2 allows as have you
can have a high availability yarn faster
where you have a active and standby and
the coordination is taking care by your
zookeeper at a particular time there can
only be one active resource manager and
if active resource manager fails your
standby resource manager comes and
becomes active however zookeeper is
playing a very important role remember
Zoo Giver is the one which is
coordinating the server State and it is
doing the election of active to standby
failover what are the different
schedulers available in yarn so you have
a fee for scheduler that is first in
first out and this is not a desirable
option because in this case a longer
running application might block all
other small running applications
capacity scheduler is basically a
scheduler where dedicated queues are
created and they have fixed amount of
resources so you can have multiple
applications accessing the cluster at
the same time and they would be using
their own cues and the resources
allocated to them if you talk about Fair
scheduler you don't need to have a fixed
amount of sources you can just have a
percentage and you could decide what
kind of fairness is to be followed which
basically means that if you were
allocated 20 gigabytes of memory however
the cluster has 100 gigabytes and the
other team was assigned 80 gigabytes of
memory then you have 20 access to the
cluster another team has 80 percent
however if the other team does not come
up or does not use the cluster in a fair
scheduler you can go up to maximum of
100 percent of your cluster to find out
more information about your schedulers
you could either look in Hadoop
definitive guide or what you could do is
you could just go to Google and you
could type for example yarn scheduler
let's search for yarn scheduler and then
you can look in Hadoop definitive guide
and so this is your Hadoop definitive
guide and it beautifully explains about
your different schedulers how do
multiple applications run and that could
be in your fifo kind of scheduling it
could be in capacity scheduler or it
could be in a fair scheduling so have a
look at this link it's a very good link
you can also search for yarn untangling
and this is a Blog of four or this is a
series of four blocks where it's
beautifully explained about your yarn
how it works how the resource allocation
happens what is a container and what
runs within the container so you can
scroll down you can be reading through
this and you can then also search for
part two of it which talks about
allocation and so so coming back
we basically have these schedulers what
happens if a resource manager fails
while executing an application in a high
availability cluster so in a high
availability cluster we know that we
would have two resource managers one
being active one being standby and
zookeeper which is keeping a track of
the server States so if a RM fails in
case of high availability the standby
will be elected as active and then
basically your resource manager or the
standby would become the active one and
this one would instruct the application
Master to a bot in the beginning then
your resource manager recovers its
running state so there is something
called as RM State Store where all the
applications which are running their
status is stored so resource manager
recovers its running state by looking at
your state store by taking advantage of
container statuses and then continues to
take care of your process now in a
cluster of 10 data nodes each having 16
GB and 10 cores what would be total
processing capacity of the cluster take
a minute to think 10 data nodes 16 GB
Ram per node 10 cores so if you mention
the answer as 160 GB RAM and 100 cores
then you went wrong now think of a
cluster which has 10 data nodes each
having 16 GB RAM and 10 cores remember
on every node in a Hadoop cluster you
would have one or multiple processes
running those processes would need RAM
the machine itself which has a Linux
file system would have its own processes
so that would also be having some RAM
usage which basically means that if you
talk about 10 data nodes you should
deduct at least
at least 30 percent towards the
overheads towards the cloud database
Services towards the other processes
which are running and in that case I
could say that you could have 11 or 12
GB available on every machine for
processing and say six or seven cores my
10 and that's your processing capacity
remember the same thing applies to the
disk usage also so if somebody asks you
in a 10 data node cluster where each
machine has 20 terabytes of disks what
is my total storage capacity available
for stfs so the answer would not be 200
you have to consider the overheads and
this is basically which gives you your
processing capacity now let's look at
one more question so what happens if
requested memory or CPU cores Beyond or
goes beyond the size of container now as
I said you can have your configurations
which can say that in a particular data
node which has 100 GB Ram I could
allocate say 50 GB for the processing
like out of 100 cores I could say 50
cores for processing so if you have 100
GB RAM and 100 cores you could ideally
allocate 100 for processing but that's
not ideally possible so if you have 100
GB Ram you would go for 50 GB and if you
have 100 cores you would go for 50 cores
now within this RAM and CPU course you
you have the concept of containers right
so container is a combination of RAM and
CPU cores so you could have a minimum
size container and maximum size content
now at any point of time if your
application starts demanding more memory
or more CPU cores and this cannot fit
into a container location your
application will fail your application
will fail because you requested for a
memory or a combination of memory and
CPU cores which is more than the maximum
container size so look into this yarn
tangling website which I mentioned and
look for the second blog in those series
which explains about these allocation
now here we will discuss on hive Peg
hbase and these components of two which
are being used in the industry for
various use cases let's look at some
questions here and let's look how you
should prepare for them so first of all
we will learn on hive which is a data
warehousing package so the question is
what are the different components of a
hive architecture now when we talk about
Hive we already know that Hive is a data
warehousing package which basically
allows you to work on structured data or
data which can be structurized so
normally people are well versed with
querying or basically processing the
data using SQL queries a lot of people
come from database backgrounds and they
would find it comfortable if they know
structured query language Hive is one of
the data warehousing package which
resides within a Hadoop ecosystem it
uses hadoop's distributed file system to
store the data and it uses rdbm mess
usually to store the metadata although
metadata can be stored locally also so
what are the different components of a
hive architecture so it has a user
interface so user interface calls the
execute interface to the driver this
creates a session to the query and then
it sends the query to the compiler to
generate an execution plan for it
usually whenever Hive is set up it would
have its metadata stored in an rdbms now
to establish the connection between
rdbms and Hadoop we need odbc or jdbc
connector jar file and that connector
jar file has a driver class now this
driver class is mandatory to create a
connection between Hive and Hadoop so
user interface creates this interface
using the driver now we have metastore
metastore stores the metadata
information so any object which you
create such as database stable indexes
their metadata is stored in metastore
and usually this meta store is stored in
an rdbms so that multiple users can
connect to Hive so your meta store
stores the metadata information and
sends that to the compiler for execution
of a query what does the compiler do it
generates the execution plan it has a
tag now tag stands for direct cycle
graph so it has a tag of stages where
each stage is either a metadata
operation a map or reduced job or an
operation on sdfs and finally we have
execution engine that acts as a bridge
between Hive and Hadoop to process the
query so execution engine communicates
bi-directionally with metastore to
perform operations like create or drop
tables so these are four important
components of Hive architecture now what
is the difference between external table
and manage stable and Hive so we have
various kinds of table in Hive such as
external table manage table partition
table the major difference between your
managed and external table is in respect
to what happens to the data if the table
is dropped usually whenever we create a
table in Hive it creates a manage table
or we could also call that as an
internal table now this manages the data
and moves it into warehouse directory by
default whether you create a manage
stable or external table usually the
data can reside in hive's default
Warehouse directory or it could be
residing in a location chosen however
when we talk about manage table if one
drops a manage table not only the
metadata information is deleted but the
tables data is also deleted from sdfs if
we talk about external table it is
created with an external keyword
explicitly and if an external table is
dropped nothing happens to the data
which resides in sdfs so that's the main
difference between your managed and
external table what might be the use
case if somebody asks you there might be
a migration kind of activity or you are
interested in creating a lot of tables
using your queries so in that case you
could dump all the data on sdfs and then
you could create a table by pointing to
a particular directory or multiple
directories now you could then do some
testing of your tables and would decide
that you might not need all the tables
so in that case it would be advisable to
create external tables so that even if
the table is later dropped the data on
sdfs will be intact unlike your manage
table where dropping of table will
delete the data from sdfs Also let's
learn a little bit on partition so what
is partition And Hive and why is
partitioning required in high life if
somebody asks you that now normally in
world of rdbms partition is the process
to group similar type of data together
and that is usually done on basis of a
column or what we call as partitioning
key now each table usually has one
column in context of rdbms which could
be used to partition the data and why do
we do that so that we can avoid scanning
the complete table for a query and
restrict the scan to set of data or to a
particular partition in Hive we can have
any number of partition keys so
partitioning provides granularity in
Hive table it reduces the query latency
by scanning only relevant partition data
instead of whole data set we can
partition at various levels now if I
compare rdbms with Hive in case of rdbms
you could have one column which could be
used for partitioning and then then you
could be squaring the specific partition
so in case of rdbms your partition
column is usually a part of the table
definition so for example if I have an
employee table I might have employee ID
employee name employee age and employee
salary has four columns and I would
decide to partition the table based on
salary column now why would I partition
it because I feel that employee table is
growing very fast it is or it will have
huge amount of data and later when we
query the table we don't want to scan
the complete table so I could split my
data into multiple partition based on a
salary column giving some ranges in Hive
it is a little different in Hive you can
do partitioning and there is a concept
of static and dynamic partitioning but
in Hive the partition column is not part
of table definition so you might have an
employee table with employee ID name a
each and that that's it that would be
the table definition but you could then
have partitioning done based on salary
column which will then create a specific
folder on sdfs in that case when we
query the data we can see the partition
column also showing up so we can
partition the transaction data for a
bank for example based on month like
Chan Feb Etc and any operation regarding
a particular month will then allow us to
query that particular folder that is
where partitioning is useful now why
does Hive not store metadata information
in a CFS if somebody asks you so we know
that hives data is stored in sdfs which
is Hadoop distributed file system
however the metadata is either stored
locally and that mode of high would be
called as embedded mode or you could
have hives metadata stored in rdbms so
that multiple clients can initiate a
connection now this metadata which is
very important for Hive would not be
stored in sdfs so we already know that
sdfs read and write operations are time
consuming it is a distributed file
system and it can accommodate huge
amount of data so Hive stores metadata
information in metastore using rdbms
instead of sdfs so this allows to
achieve low latency and faster data
access
now if somebody asks what are the
components used in Hive query processor
so usually we have the main components
are your parser your execution engine
logical plan generation Optimizer and
type checking so whenever a query is
submitted it will go through a parser
and parser would check the syntax it
would check for objects which are being
queried and other things to see if the
query is fine now internally you have a
semantic analyzer which will also look
at the query you have an execution
engine which basically will work on the
execution part that is the best
generated execution plan which could be
used to get the results for the query
you could also have user defined
functions which a user would want to use
and these are normally created in Java
or Java programming language and then
basically these user defined functions
are added to the class path now you
would have a logical plan generation
which which basically looks at your
query and then generates a logical plan
or the best execution path which would
be required to get to the results
internally there is a physical plan
generated which is then looked in by
Optimizer to get the best path to get to
the data and that might also be checking
your different operators which you are
using within your query finally we would
also have type checking so these are
important components in Hive so somebody
might ask you if you are querying your
data using Hive what are the different
components involved or if you could
explain what are the different
components which work when a query is
submitted so these are the components
now let's look a scenario based question
Suppose there are a lot of small CSV
files which are present in a is DFS
directory and you want to create a
single Hive table from these files so
data in these files have Fields like
registration number name email address
so if this is what needs to be done what
will be your approach to solve it where
will you create a single Hive table for
lots of small files without degrading
the performance of the system so there
can be different approaches now we know
that there are a lot of small CSV files
which are present in a directory so we
know that when we create a table in Hive
we can use a location parameter so I
could say create table give a table name
give the column and their data types I
could specify the delimiters and finally
I could say location and then point it
to a directory on sdfs and this
directory might be the directory which
has lot of CSV files so in this case I
will avoid loading the data in the table
because table being Point table pointing
to the directory will directly pick up
the data from one or multiple files and
we also know that Hive does schema check
on read so does not do a schema check on
write so in case there were one or two
files which did not follow the schema of
the table it would not prevent data
loading data would anyways be loaded
only when you query the data it might
show you null values if data which was
loaded does not follow the schema of the
table this is one approach what is the
other approach so let's look at that you
can think about sequence file format
which is basically a smart format or a
binary format and you can group these
small files together to form a sequence
file now this could be one other smarter
approach so we could create a temporary
table so we could say create table give
a table name give the column names and
their data types we could specify the
delimiters as it shows here that is row
format and Fields terminated by and
finally we can store that as text file
then we can load data into this table by
giving a local file system path and then
we can create a table that will store
data in sequence file format so my point
one is storing the data in this text
file 0.3 would be storing the data in
sequence file format so we say create
table give the specifications we say row
format delimited fields are terminated
by comma stored as sequence file then we
can move the data from test table into
test sequence file table so I could just
say insert overwrite my new table as
select star from other tape remember in
Hive you cannot do insert update delete
however if the table is existing you can
do a insert overwrite from an existing
table into a new table so this could be
one approach where we could have lot of
CSV files or smaller files club together
as one big sequence file and then store
it in the table now if somebody asks you
write a query to in insert a new column
that is integer data type into a hive
table and the requirement might be that
you would want to insert this table at a
position before an existing column now
that's possible by doing an alter table
giving your table name and then
specifying change column giving you a
new column with the data type before an
existing column this is a simple way
where you can insert a new column into a
hive table what are the key differences
between Hive and pick now some of you
might have heard High Visa data
warehousing package and Peg is more of a
scripting language both of them are used
for data analysis or Trend detection
hypothesis testing data transformation
and many other use cases so if we
compare Hive and pick Hive uses a
declarative language called Hive ql that
is Hive querying language similar to SQL
and it is for reporting or for data
analysis even for data transformation or
for your data extraction big uses a high
level procedural language called Pig
Latin for programming both of them
remember use mapreduce processing
framework so when we run a query in Hive
to process the data or when we create
and submit a big script both of them
trigger a mapreduce job unless and until
we have set them to Local mode Hive
operates on the server side of the
cluster and basically works on
structured data or data which can be
structuralized pig usually works or
operates on the client side of the
cluster and allows both structured
unstructured or even I could say
semi-structured data Hive does not
support Avro file format by default
however that can be done by using the
write serializer deserializer so we can
have Hive table related data stored in
Avro format in sequence file format in
parquet format or even as a text file
format however when we are working on
smarter formats like Avro or sequence
file or parquet we might have to use
specific serializers deserializers for
Avro this is the package which allows us
to use Avro format Pig supports Agro
format by default Hive was developed by
Facebook and it supports partitioning
and Peg was developed by Yahoo and it
does not support partitioning so these
are high level differences there are
lots and lots of differences remember
Hive is more of a data warehousing
package and Peg is more of a scripting
language or a strictly procedural flow
following scripting language which
allows us to process the data now let's
get more and let's get more deeper and
learn about big which is as I mentioned
a scripting language which can be used
for your data processing it also uses
map reduce although we can even have big
run in a local mode let's learn about
pig in the next section now let's learn
on some questions about Pig which is a
scripting language and it is extensively
used for data processing and data
analysis so the question is how is
Apache Pig different from mapreduce now
we all know that mapreduce is a
programming model it is it's quite rigid
when it comes to processing the data
because you have to do the mapping and
reducing you have to write huge code
usually mapreduce is written in Java but
now it can also be written in Python it
can be written in Scala and other
programming languages so if we compare
pick with mapreduce pig obviously is
very concise it has less lines of code
when compared to mapreduce now we also
know that big script internally will
trigger a mapreduce job however user
need not know about mapreduce
programming model they can simply write
simple scripts in Pig and that will
automatically be converted into
mapreduce however mapreduce has more
lines of code Peak is high level
language which can easily perform join
operations or other data processing
operations map reduce is a low level
language which cannot perform job join
operations easily so we can do join
using mapreduce however it's not really
easy in comparison to Pig now as I said
on execution every Pig operator is
converted internally into a mapreduce
job so every big script which is run
which would be converted into mapreduce
job now map reduce overall is a batch
oriented processing so it takes more
time to compile it takes more time to
execute either when you run a mapreduce
job or when it is triggered by Pink
script big works with all versions of
Hadoop and when we talk about mapreduce
program which is written in one Hadoop
version may not work with other versions
it might work or it might not it depends
on what are the dependencies what is the
compiler you are using what programming
language you have used and what version
of Hadoop you are working on so these
are the main differences between Apache
Pig and mapreduce what are the different
ways of executing pick script so you
could create a script file store it in
dot pick or dot text and then you could
execute it using the pick command you
could be bringing up the grunt shell
that is Pig's shell now that usually
starts with mapreduce mode but then we
can also bring it up in a local mode and
we can also run pick embed it as an
embedded script in other programming
language so these are the different ways
of executing your pick script now what
are the major components of pig
execution environment this is this is a
very common question interviewers would
always want to know different components
of Hive different component currents of
pig even different components which are
involved in Hadoop ecosystem so when we
want to learn about major components of
big execution environment here also so
you have pick scripts now that is
written in pig latin using built-in
operators and user-defined functions and
submitted to the execution environment
that's what happens when you would want
to process the data using pick now there
is a parser which does type checking and
checks the syntax of the script the
output of parser is a tag direct a
cyclic graph so look in Wikipedia for
dag so dag is basically a sequence of
steps which run in One Direction then
you have an Optimizer now this Optimizer
performs optimization using merge
transform split Etc it aims to reduce
the amount of data in the pipeline
that's the whole purpose of Optimizer
you have a internal compiler so pick
compiler converts the optimized code
into a mapreduce job and here user need
not know the mapreduce programming model
or how it works or how it is written
they all need to know about running the
pick script which would be internally
converted into a mapreduce job and
finally we have an execution engine so
mapreduce jobs are submitted to the
execution engine to generate the desired
results so these are major components of
pick execution environment now let's
learn about different complex data types
in big big supports various data types
the main ones are Tuple bag and map what
is Tuple or Tuple as you might have
heard a tuple is an ordered set of
fields which can contain different data
types for each field so in Array you
would have multiple elements but that
would be of same types list can also
have different types your Tuple is a
collection which has different fields
and each field can be of different type
now we could have an example is 1 comma
3 or 1 comma 3 comma a string or a float
element and all of that form a tuple bag
is a set of tuples so that's represented
by curly braces so you could also
imagine this like a dictionary which has
various different correction elements
what is a map map is a set of key value
pairs used to represent data so when you
work in Big Data field you need to know
about different data types which are
supported by Peg which are supported by
Hive which are supported in other
components of Ado so pupil pack map
array array buffer you can think about
list you can think about dictionaries
you can think about map which is key
value pair so these are your different
complex data types other than the
primitive data types such as integer
character string Boolean float and so on
now what are the various diagnostic
operators available in Apache pick so
these are some of the operators or
options which you can give in a pick
script you can do a thumb now dump
operator runs the pig latin scripts and
displays the result on the screen so
either I could do a dumb and see the
output on the screen or I can even do a
dump into and I could store my output in
a particular file so we can load the
data using load operator in Pig and then
Pig also has different internal storage
like Json loader or pick storage which
can be used if you are working on
specific kind of data and then you could
do a dump either before processing or
after processing and dump would produce
the result the result could be stored in
a file or seen on the screen you also
have a describe operator now that is
used to view the schema of a relation so
you can load the data and then you can
view the schema of relation using
describe operator explain as we might
already know displays the physical
logical and mapreduce execution plans so
normally in rdbms when we use X-Plane we
would like to see what happens behind
the scenes when a particular script or a
query runs so we could load the data
using load operator as in any other case
and if we would want to display The
Logical physical and mapreduce execution
plans we could use explain operator
there is also an illustrate operator now
that gives the step-by-step execution of
sequence of statements so sometimes when
we would want to analyze our script to
see how good or bad they are or would
that really serve our purpose we could
use illustrate and again you can test
that by loading the data using load
operator and you could just use a
illustrate operator to have a look at
the step-by-step execution of the
sequence of statements which you would
want to execute so these are different
diagnostic operators available in Apache
pick now if somebody asks State the
usage of group order by and distinct
keywords in big script so as I said big
is a scripting language so you could use
various operators so group basically
collects various records with the same
key and groups the data in one or more
relations here is an example you could
do a group data so that is basically a
variable or you can give some other name
and you can say group relation Name by H
now say I have a file where I have field
various fields and one of the field is a
relational name so I could group that by
a different field order by is used to
display the contents of relation in a
sorted order whether ascending or
descending so I could create a variable
called relation 2 and then I could say
order relation name one by ascending or
descending order distinct basically
removes the duplicate records and it is
implemented only on entire records not
on individual records so if you would
like want to find out the distinct
values and relation name field I could
use distinct what are the relational
operators in pig so you have various
relational operators which help data
scientists or data analysts or
developers who are analyzing the data
such as go Group which joins two or more
tables and then performs group operation
on the join table result you have cross
it is used to compute the cross product
that is a Cartesian product of two or
more relations for each is basically to
do some iteration so if it will iterate
through tuples of a relation generating
a data transformation so for example if
I say variable a equals and then I load
a file in a and then I could create a
variable called P where I could say for
each a I would want to do something say
group join is to join two or more tables
in a relation limit is to limit the
number of output tuples or output
results split is to split the relation
into two or more relations Union is to
get a combination that's it will merge
the contents of two or more relations
and order is to get a sorted result so
these are some relational operators
which are extensively used in pig for
analysis what is the use of having
filters in Apache pick now say for
example I have some data which has three
Fields here product quantity and this is
my phone sales data so filter operator
could be used to select the required
values from a relation based on a
condition it also allows you to remove
unwanted records from data file so for
example filter the products where
quantity is greater than thousand so I
see that I have one row wherein or
multiple rows where the quantity is
greater than thousands such as fifteen
hundred Seventeen hundred twelve hundred
so I could create a variable called a I
would load my file using pick storage as
I explained earlier big storage is an
internal parameter which can be used to
specify the delimiters now here my
delimiter is comma so I could say using
pick storage as and then I could specify
the data type for each field so here
being integer product being character
array and quantity being integer then B
I could say filter a whatever we have in
a by quantity greater than thousand so
it's very concise it's very simple and
it allows us to extract and process data
in a simpler way now Suppose there is a
file called test dot txt having 150
records in sdfs so this is a file which
is stored on sdfs and it has 150 records
where we can consider every record being
one line and if somebody asks you to
write a pick command to retrieve the
first 10 records of the file first we
will have to load the data so I could
create a variable called test underscore
data and I would say load my file using
pick storage specifying the delimiter S
comma as and then I could specify my
Fields what whatever Fields our file
have and then I would want to get only
10 records for which I could use the
limit operator so I could say limit on
test data and give me 10 records this is
very simple and we can extract 10
records from 150 records which are
stored in the file on sdfs now we have
learned on Pig we have learned some
questions on hive you could always look
more in books like programming in Hive
or programming in pick and look for some
more examples and try out these examples
on a existing Hadoop setup now let's
learn on hbase which is a nosql database
now edgebase is a four dimensional
database in comparison to your rdbms
which usually are two dimensional so
rdbms have rows and columns but hbase
has four coordinates it has row key
which is always unique column family
which can be any number column
qualifiers which can again be any number
per column family and then you have a
version so these four coordinates make H
base a four dimensional key value store
or a column family store which is unique
for storing huge amount of data and
extracting data from hbase there is a
very good link which I would suggest
everyone can look at if you would want
to learn more on edgebase and you could
just say hbase mapper and this basically
brings up a documentation which is from
mapper but then that's not specific to
map R and you can look at this link
which will give you a detailed
explanation of HP is how it works what
are the Architectural Components and how
data is stored and how it makes edgebase
a very powerful nosql database so let's
learn on some of the important or
critical questions on hbase which might
be asked by the interviewer in an
interview when you are applying for a
Big Data admin or a developer position
role so what are the key components of
hbase now as I said this is one of the
favorite questions of interviewers where
they would want to understand your
knowledge on different components for a
particular service edgebase as I said is
a nosql database and that comes as a
part of service with Cloudera or
hortonworks and with Apache Hadoop you
could also set up Edge base as an
independent package so what are the key
components of hbase edgebase has a
region server now edgebase follows the
similar kind of topology like Hadoop now
Hadoop has a master process that is name
node and slave processes such as data
nodes and secondary name node in the
same way edgebase also has a master
which is Edge master and the slave
processes are called region servers so
these region servers are usually
co-located with data nodes however it is
not mandatory that if you have 100 data
nodes you would have 100 region servers
so it purely depends on admin so what
does this region server contain so
region server contains hbase tables that
are divided horizontally into regions or
you could say group of rows is called
regions so in edgebase you have two
aspects one is group of columns which is
called column family and one is group of
rows which is called regions now these
regions or these rows are grouped based
on the key values or I would say row
Keys which are always unique when you
store your data in edgebase you would
have data in the form of rows and
columns so group of rows are called
regions or you could say these are
horizontal partitions of the table so a
region server manages these regions on
the Node where a data node is running a
region server can have up to thousand
regions it runs on every node and
decides the size of region so region
server as I said is a slave process
which is responsible for managing HPS
data on the Node each region server is a
worker node or a worker process
co-located with data node which will
take care of your read write update
delete request from the clients now when
we talk about more components of
edgebase as I said you have HP H master
so you would always have a connection
coming in from a client or an
application what does etch Master do it
assigns regions it monitors the region
servers it assigns regions to region
servers for load balancing and it cannot
do that without the help of Zookeeper so
if we talk about components of hbase
there are three main components you have
zookeeper you have etch master and you
have region server region server being
the slave process your Edge Master being
the master process which takes care of
all your table operations assigning
regions to the region servers taking
care of read and write requests which
come from client and for all of this
Edge Master will take in help of
Zookeeper which is a centralized
coordination service so whenever a
client wants to read or write or change
the schema or any other metadata
operations it will contact H Master Edge
Master internally will contact zookeeper
so you could have edgebase setup also in
high availability mode where you could
have a active Edge master in a backup
Edge Master you would have a zookeeper
Quorum which is the way zookeeper works
so zookeeper is a centralized
coordination service which will always
run with a quorum of processes so
zookeeper would always run with odd
number of processes such as 3 5 and 7
because zookeeper works on the concept
of maturity consensus now zookeeper
which is a centralized coordination
service is keeping a track of all the
servers which are alive available and
also keeps a track of their status for
every server with zookeeper is
monitoring zookeeper keeps a session
alive with that particular server Edge
Master would always check with zookeeper
which region servers are available alive
so that regions can be assigned to the
region server at one end you have region
server which are sending their status to
the Zookeeper indicating if they are
ready for any kind of read or write
operation and at other end Edge Master
is querying the Zookeeper to check the
status now zookeeper internally manages
a meta table now that meta table will
have information of which regions are
residing on which region server and what
rookies those regions contain so in case
of a read activity Edge Master will
zookeeper to find out the region server
which contains that meta table once etch
Master gets the information of meta
table it can look into the meta table to
find out the row keys and the
corresponding region servers which
contain the regions for those row Keys
now if we would want to understand row
key and column families in hbase let's
look at this and it would be good if we
could look this on an Excel sheet so row
key is always unique it acts as a
primary key for any hbase table it
allows a logical grouping of cells and
make sure that all cells with the same
row key are co-located on the same
server so as I said you have four
coordinates for hbase you have a row key
which is always unique you have column
families which is nothing but group of
columns and when I say column families
one column family can have any number of
columns so when I talk about hbase H
base is four dimensional and in terms of
H base it is also called as a column
oriented database which basically means
that every Row in one column could have
a different data type now you have a row
key which uniquely identifies the row
you have column families which could be
one or many depending on how the table
has been defined and a column family can
have any number of columns or I could
say for every row within a column family
you could have different number of
columns so I could say for my Row one I
could just have two columns such as name
and City within the column family for my
Row 2 I could have name City age
designation salary for my third row I
could have thousand columns and all that
could belong to one column family so
this is a horizontally scalable database
so column family consists of group of
columns which is defined during table
creation and each column family can have
any number of column qualifiers
separated by a delimiter now a
combination of row key column family
column qualifier such as name City age
and the value within the cell is makes
the hbase a unique four dimensional
database for more information if you
would want to learn on hbase please
refer this link which is hbase mapper
and this gives a complete hbase
architecture that has three components
of name node three components that is
name node region servers and zookeeper
how it works how Edge base Edge Master
interacts with zookeeper what zookeeper
does in coordination how are the
components working together and how does
hbase take care of read and write coming
back and continuing why do we need to
disable a table so there are different
table operations what you can didn't do
in hbase and one of them is disabling a
table now if you would want to check the
status of table you could check that my
is disabled and giving the table name or
is enabled and the table name so the
question is why do we need to disable a
table now if we would want to modify a
table or we are doing some kind of
Maintenance activity in that case we can
disable the table so that we can modify
or changes settings when a table is
disabled it cannot be accessed through
the scan command now if we have to write
a code to open a connection in hbase now
to interact with hbase one could either
use a graphical user interface such as
Hue or you could be using the command
line hbase shell or you could be using
hbase admin API if you are working with
Java or say happy base if you are
working with python where you may want
to open a connection with hbase so that
you can work with database
programmatically in that case we have to
create a configuration object that is
configuration my conf and then create a
configuration object and then you can
use different classes like Edge table
interface to work on a new table you
could use H column qualifier and many
other classes which are available in
hbase admin API what does replication
mean in terms of hbase so edgebase as I
said Works in a cluster way and when you
talk about cluster you could always set
up a replication from one hbase cluster
to other edgebase cluster so this
replication feature in hbase provides a
mechanism to copy data between clusters
or sync the data between different
clusters this feature can be used as a
disaster recovery solution that provides
High availability for edgebase so if I
have hbase cluster 1 where I have one
master and multiple region servers
running in a Hadoop cluster I could use
the same Hadoop cluster to create a
hbase replica cluster or I could have a
totally different hbase replica cluster
where my intention is that if things are
changing in a particular table in
cluster one I would want them to be
replicated across different cluster so I
could alter the hbase table and set the
replication scope to 1. now a
replication scope of 0 indicates that
table is not replicated but if we set
the replication to 1 we basically will
have to set up ahbs cluster where we can
replicate hbase tables data from cluster
1 to Cluster so these are the commands
which can be used to enable replication
and then replicate the data of table
across clusters can we Import and Export
in hbase of course we can it is possible
to Import and Export tables from one
edgebase cluster to other hbase cluster
or even within a cluster so we can use
the hbase export utility which comes in
this particular package give a table
name and then a Target location so that
will export the data of hbase table into
a directory on sdfs then I could create
a different table which would follow
some kind of same definition as the
table which was exported and then I
could use import to import the data from
the directory on sdfs to my table if you
would want to learn more on hbase Import
and Export you could look at hbase
import operations let's search for the
link and this is the link where you
could learn more about hbase import
export utilities how you could do a bulk
import bulk export which internally uses
mapreduce and then you could do a Import
and Export into edgebase tables moving
further what do we mean by compaction in
edgebase now we all know that hbase is a
nosql database which can be used to
store huge amount of data however
whenever a data is written in hbase it
is first written to what we call as
write ahead log and also to mem store
which is write cache now once the data
is written involved and your mem store
it is offloaded to form an internal
edgebase format file which is called H5
and usually these edge files are very
small in nature so we also know that
sdfs is good when we talk about few
number of larger files in comparison to
large number of smaller files due to the
limitation of name nodes memory
compaction is process of merging hbase
files that is the smaller edge files
into a single large file this is done to
reduce the amount of memory required to
store the files and number of disk seeks
needed so we could have lot of H files
which get created when the data is
written to hbase and these smaller files
can then be compacted through a major or
minor compaction creating one big Edge
file which internally would then be
written to sdfs and sdfs format of
blocks that is the benefit of compaction
there is also a feature called Bloom
filter so how does Bloom filter work so
Bloom filter or hbase Bloom filter is a
mechanism to test whether a h file
contains a specific row or a row column
cell Bloom filter is named after its
creator button covered Blue it is a data
structure which predicts whether a given
element is a member of a set of data it
provides an in-memory index structure
that reduces the disk that reads and
determines the probability of finding a
row in a particular file this is one of
very useful features of edgebase which
allows for faster access and avoids disk
syncs thus edgebase have any concept of
namespace so namespace is when you have
similar elements grouped together so
namespace yes hbase supports such names
space so namespace is a logical grouping
of tables analogous to a database in
rdbms so you can create hbase namespace
to the schema of rdbms database so you
could create a namespace by saying
create namespace and giving it a name
and then you could also list the tables
within a namespace you could create
tables within a specific namespace now
this is usually done in production
environment where a cluster might be
multi-tenant cluster and there might be
different users of the same nosql
database in that case admin would create
specific namespace and for specific
namespace you would have different
directories on hdfs and users of a
particular business unit or a team can
work on their edgebase objects within a
specific name space this is a question
which is again very important to
understand about the rights or reads so
how does right ahead log wall hell when
a region server crashes now as I said
when a write happens it will happen into
mem store and wall that is your edit log
or write ahead log so whenever a write
happens it will happen in two places mem
store which is the right cache and wall
which is a edit log only when the data
is written in both these places and
based on the limitation of mem store the
data will be flushed to create an
edgebase format file called H5 these
files are then compacted and created
into one bigger file which will then be
stored on sdfs and sdfs data as we know
is stored in the form of blocks on the
underlying data nodes so if a region
server hosting a mem store crashes now
where is region server running that
would be co-located with data node so if
a data node crashes or if a region
server which was hosting the mem store
Write cash crashes data in memory the
data that in memory which was not not
persisted is lost now how does edgebase
recover from this as I said your data is
written into wall and mem store at the
same time HBS recovers against that by
writing to wall before the write
completes so whenever a write happens it
happens in mem store and wall at the
same time HBS cluster keeps a wall to
record changes as they happen and that's
why we call it as also an edit log if
hbase goes down or the node that goes
down the data that was not flushed from
mem store to Edge file can be recovered
by replaying the right ahead log and
that's the benefit of your edit log or
write ahead log now if we would have to
write hbase command to list the contents
and update the column families of a
table I could just do a scan and that
would give me complete data of a table
if you are very specific and if you
would want to look at a particular row
then you could do a get table name and
then give the rocky however you could do
a scan to get the complete data of a
particular table you could also do a
describe to see what are the different
column families and if you would want to
alter the table and add a new column
family it is very simple you can just
say alter give the HP stable name and
then give you a new column family name
which will then be added to the table
what are catalog tables in hbase so as I
mentioned your zookeeper knows the
location of this internal catalog table
or what we call as The Meta table now
catalog tables in edgebase have two
tables one is hbase meta table and one
is hyphen root the catalog table Edge
base meta exists as an hbase table and
is filtered out of hbase shells list
command so if I give a list command on
edgebase it would list all the tables
which h space contains but not the meta
table it's an internal table this meta
table keeps a list of all regions in the
system and location of hbase method
stored in Zookeeper so if somebody wants
to find out or look for particular rows
they need to know the regions which
contain that data and those regions are
located on region server to get all this
information one has to look into this
meta table however we will not be
looking into meta table directly we
would just be giving a write or a read
operation internally your hbase master
queries the Zookeeper zookeeper has the
information of where the meta table
exists and that meta table which is
existing on region server contains
information of row keys and the region
servers where those rows can be found
your root table keeps a track of
location of The Meta table what is hot
spotting in hbase and how to avoid hot
spotting now this is a common problem
and always admin guys or guy who are
managing the infrastructure would think
about it so one of the main idea is that
edgebase would be leveraging the benefit
of sdfs you are all read and write
requests should be uniformly distributed
across all of the regions and region
servers otherwise what's the benefit of
having a distributed cluster so you
would have your data stored across
region servers in the form of regions
which are horizontal partitions of the
table and whenever read and write
requests happen they should be uniformly
distributed across all the regions in
the region servers now hotspotting
occurs when a given region serviced by a
region server receives most or all of
read and write request which is
basically a unbalanced way of read write
operations now hotspot can be avoided by
designing the row key in such a way that
data being written should go to multiple
regions across the cluster so you could
do techniques such as salting hashing
reversing the key and many other
techniques which are employed by users
of hbase we need to just make sure that
when the regions are distributed across
region servers they should be spread
across region servers so that your read
and write request can be satisfied from
different region servers in parallel
rather than all read and write request
hitting the same region server
overloading the region server which may
also lead to the crashing of a
particular page and server so these were
some of the important questions of hbase
and then there are many more please
refer to the link which I specified in
during my discussion and that gives you
a detailed explanation of how which ways
works you can also look into hbase
definitive guide by O'Reilly or hbas in
action and these are really good books
to understand about hbase internals and
how it works now that we have learned on
hive which is a data warehousing package
we have learned on Pig which is a
scripting or a scripting language which
allows you to do data analysis and we
have learned some questions on a nosql
database just note it that there are
more than 225 nosql databases existing
in market and if you would want to learn
and know about more nosql databases you
can just go to Google and type no SQL
databases org and that will take you to
the link which is for nosql databases
and this shows there are more than 225
nosql databases existing in market and
these are for different use cases used
by different users and for with
different features so have a look at
this link now when you talk about data
ingestion so let's look at data
ingestion and this is one good link
which I would suggest to have a look at
which lists down around 18 different
injection tools so when you talk about
different data ingestion tools some are
for structured data some are for
streaming data some are for data
governance some are for data ingestion
and transformation and so on so have a
look at this link which also gives you a
comparison of different data ingestion
tools so here let's learn about some
questions on scope which is one of the
data ingestion tools mainly used for
structured data or you could say data
which is coming in from rdbms or data
which is already structured and you
would want to ingest that you would want
to store that on sdfs which could then
be used for Hive which could be used for
any kind of processing using mapreduce
or Hive or pig or spark or any other
processing Frameworks or you would want
to load that data into say high variety
Stables scoop is mainly for structured
data it is extensively used when
organizations are migrating from rdbms
to a big data platform and they would be
interested in ingesting the data that is
doing Import and Export of data from
rdbms to sdfs or vice versa so let's
learn about some important questions on
scope which you may be asked by an
interviewer when you apply for a big
data related position how is scoop
different from Flume so this is a very
common question which is asked scoop
which is mainly for structured data so
scoop works with rdbms it also works
with nosql databases to Import and
Export data so you can import data into
sdfs you can import data into Data
browsing packets such as Hive directly
or also in edgebase and you could also
export data from Hadoop ecosystem to
your rdbms however when it comes to flow
Flume is more of a data ingestion tool
which works with streaming data or
unstructured data so data which is
constantly getting generated for example
log files or metrics from server or some
chat messenger and so on so if you are
interested in working on capturing and
storing the streaming data in a storage
layer such as sdfs or edgebase you could
be using Flume there could be other
tools also like Kafka or storm or chukwa
or some nifi and so on scoop however is
mainly for structured data you're
loading data in scope is not event
driven so it is not based on event it
basically works on data which is already
stored in rdbms in terms of Flume it is
completely event driven that is as the
messages or as the events happen as the
data is getting generated you can have
that data ingested using flow Zoom scoop
works with structured data sources and
you have various scope connectors which
are used to fetch data from external
data structures or rdbms so for every
rdbms such as MySQL Oracle db2 Microsoft
SQL Server you have different connectors
which are available Flume it works on
fetching streaming data such as tweets
or log files or server metrics from your
different sources where the data is
getting generated and if you are
interested in not only ingesting that
data which is getting generated in a
streaming fashion but if you would be
interested in processing the data as it
arrives scoop can import data from rdbms
onto sdfs and also export it back to
rdbms Flume is then used for streaming
data now you could have one to one one
to many or many to one kind of relation
so in terms of Flume you have components
such as your Source sync and channel
that's the main difference between your
scoop and flow what are the different
file formats to import data using scope
well there are lots and lots of formats
in which you can import data into Scope
when you talk about scoop you can have
delimited text file format now that's
the default import format it can be
specified explicitly using as text file
argument so when I want to import data
from an rdpms I could get that data in
hdfs using different compression schemes
or in different formats using the
specific arguments so I could specify an
argument which will write string based
representation of each record to Output
files with delimiters between individual
columns and rows so that is the default
format which is used to import data in
using scoop so to learn more about your
scoop and different arguments which are
available you can click on scoop dot
apache.org you can look into the
documentation and I would suggest
choosing one of the versions and looking
into the user guide and here you can
search for arguments and look for
specific control arguments which show
how you can import data using scoop so
here we have common arguments and then
you also have import control arguments
wherein we have different options like
getting data as Avro as sequence file as
text file or parquet file these are
different formats you can also get data
in default compression scheme that is
gzip or you can specify compression
codec and then you can specify What
compression mechanism you would want to
use when you are importing your data
using school when it comes to default
format for Flume we could say sequence
file which is a binary format that
stores individual records in record
specific data types so these data types
are manifested as Java classes and scoop
will automatically generate these data
types for you so scoop does that when we
talk about your sequence file format in
terms of your scoop you could be
extracting storage of all data in binary
representation so as I mentioned you can
import data in different formats such as
Avro parquet sequence file that is
binary format or machine readable format
and then you could also have data in
different compression schemes let me
just show you some quick examples here
so if I look in uh the content and here
I could search for a scoop based file
where I have listed down some examples
so if I would want to use different
compression schemes here are some
examples I will look at these so I'm
doing a scope import I'm also giving an
argument so that scope which also
triggers a map reduced job or I would
say map only job so when you run a scoop
import it triggers a map only job no
reduce happens here and you could
specify this parameter or this argument
on the command line mapreduce dot
framework.name so that you could run
your map only job in a local mode to
save time or that would interact with
yarn and run a full-fledged map only job
we can give the connection and then
connect to whatever rdbms we are
connecting mentioning the database name
give your username and password give the
table name give it Target directory or
it would create a directory same as the
table name which would work only once
and then I could say minus Z to get
Theta in a compressed format that is
gzip or I could be specifying
compression codec and then I could
specify What compression codec I would
want to use say Snappy is lz4 default I
could also run a query by giving a scope
import and when I am specifying a query
I if you notice I have not given any
table name because that would be
included in the query I can get my data
as a sequence file format which is a
binary format which will create a huge
file so we could also have compression
enabled and then I could say the output
of my map job should use a compression
at record level for my data coming in
sequence file so sequence file or a
binary format supports compression at
record level or at Block Level I could
get my data in a Avro file where data
has embedded schema within the file or a
parquet file also so these are different
ways in which you can set up different
compression schemes or you can even get
data in different formats and you could
be doing a simple scope import for these
looking further what is the importance
of eval tool in scope so there is
something called as eval tool so scoop
eval tool allows users to execute user
defined queries against respective
database servers and preview the result
in the console so either I could be
running a straight away query to import
the data into mystfs or I could just use
scoop eval connect to my external rdbms
specify my username and password and
then I could be giving in a query to see
what would be the result of the query
which we intend to import now let's
learn about how scoop imports and
exports data between rdbms and sdfs with
its architecture so rdbms as we know has
your database structures your tables
which all of them are logical and
internally there is always metadata
which is stored your scope import
connects to an external rdbms and for
this connection it uses an internal
connector jar file which has a driver
class so that's something which needs to
be set up by admin but they need to make
sure that whichever rdbms you intend to
connect to they need to have the jdbc
connector for that particular rdbms
stored within the scoop lib folder so
scoop import gets the metadata and then
for your scoop command it converts that
into a map only job which might have one
or multiple map tasks now that depends
on your scoop command you could be
specifying that you would want to do a
import only in one task or in multiple
tasks these multiple map tasks will then
run on a section of data from rdbms and
then store it in sdfs so at high level
we could say scoop will introspect
database to get gather the metadata it
divides the input data set into splits
and this division of data into splits
mainly happens on primary key column of
the table now if somebody might ask what
if my table in rdbms does not have a
primary key column then when you are
doing a scope import either you will
have to import it using one mapper task
by specifying hyphen hyphen m equals one
or you would have to say split by
parameter to specify a numeric column
from rdbms and that's how you can import
the data let me just show you a quick
example on this so I could just look in
again into the scoop command file and
here we could be looking at an example
so if you see this one here we are
specifying minus minus m equals 1 which
basically means I would want to import
the data using one map task now in this
case whether the table has a primary key
column or does not have a primary key
column will not matter but if I say a
minus minus ms6 where I am specifying
multiple map tasks to be imported then
this will look for a primary key column
in the table which you are importing now
if the table does not have a primary key
column then I could be specifying a
split by and then specify the column so
that the data could be split into
multiple chunks and multiple map tasks
would take it now if the second scenario
is your table does not have a primary
key column and it does not have a
numeric column on which you could do a
split by in that case and if you would
want to use multiple mappers you could
still say split by on a textual column
but you will have to add this property
so that it allows splitting the data
which is
non-numeric all of these options are
given in the scope apache.org link going
further how scoop imports and exports
data between rdbms and sdfs with its
architecture so as I said it submits the
map only job to the cluster and then it
basically does a import or export so if
we are exporting the data from sdfs in
that case again there would be a map
only job it would look at multiple
splits of the data which is existing
which your map only job would process
through one or one table map task and
then export it to rdbms suppose you have
a database test DB in MySQL we if
somebody asked you to write a command to
connect this database and import tables
to scoop so here is a quick example as I
showed you in the command file so you
could say scoop import this is what we
would want to do you connect using jdbc
now this will only work if the jdbc
connector already exists within your
scoop lib directory admin has to set up
that so you can connect to your rdbms
you can point to the database so here
our database name is test underscore DB
I could give username and then either I
could give password on the command line
or just say capital P so that I would be
prompted for the password and then I
could give the table name which I would
want to import I could also be
specifying minus minus M and specify how
many map tasks do I want to use for this
import as I showed in previous screen
how to export a table back to rdbms now
for this we need the data in a directory
on hdfs so for example there is a
departments table in retail database
which is already imported into scoop and
you need to export this table back to
rdbms so this is the content of the
table now create a new Department table
in rdbms so I could create a table
specifying the column names whether that
supports null or no if that has a
primary key column which is always
recommended and then I can do a scope
export I can connect to the rdbms
specifying my username and password
specify the table into which you want to
export the data and then you give export
directory pointing to a directory on
hdfs which contains the data this is how
you can export data into table seeing
example on this so I could again look
into my file and here I have an example
of import this is where you are
importing data directly into Hive and
you have scoop import where you are
importing data directly into hbase table
and you can then query your hbase table
to look at the data you could also do a
export by running your map only job in a
local mode connecting to the rdbms
specifying your username specifying the
table where you would want to export and
the directory on scfs where you have
kept the relevant data this is a simple
example of export looking further what
is the role of jdbc driver in scoop
setup so as I said if you would want to
use scoop to connect to an external
rdbms we need the jdbc odbc connector
jar file now one or admin could download
the jdbc connector jar file and then
place the jar file within the scoop lib
directory wherever scoop is installed
and this jdbc connector jar file
contains a driver now jdbc driver is a
standard Java API which is used for
accessing different databases in rdpms
so this connector jar file is very much
required and this connector jar file has
a driver class and this driver class
enables the connection between your
rdbms and your Hadoop structure each
database vendor is responsible for
writing their own implementation that
will allow communication with the
corresponding database and we need to
download the drivers which allow our
scope to connect to external rdpms so
your jdbc driver alone is not enough to
connect to scope we also need connectors
to interact with different database so a
connector is a pluggable piece that is
used to fetch metadata and allow scoop
to overcome the differences in SQL
dialects so this is how connection can
be established so normally your admins
would when they are setting up scoop and
Hadoop they would download say MySQL
jdbc connector and this is how they
would go to the MySQL connectors if you
are connecting to mySQL similar early
for your other rdpms you could be say
going in here you could be looking for a
previous version depending you could be
going for platform independent and then
you could be downloading the connected
jar file now if you want our this jar
file you would see a MySQL connector jar
and if we look in com dot
mysql.jdbc.com.mysql.jdbc.river so this
is the package which is within the
connector jar file and this has the
driver class which allows the connection
of your scoop with your rdbms so these
things will have to be done by your
admin so that you can have your scoop
connecting to an external rdbms now how
do you update the columns that are
already exported so if I do a export and
I put my data in rdbms can I really
update the columns that are already
exported yes I can using a update key
parameter so scoop export command
Remains the Same the only thing I will
have to specify now is the table name
your Fields terminated by if you have a
specific delimiter and then you can say
update key and then the column name so
this allows us to update the columns
that are already exported in rdbms what
is code gen so scoop commands translate
into your mapreduce job or map only job
so code gen is basically a tool in scoop
that generates data access objects Dao
Java classes that encapsulate an
interpret imported records so if I do a
scoop code gen connect to an rdbms using
my username and give a table this will
generate a Java code for employee table
in in the test database so this code gen
can be useful for us to understand what
data we have in this particular table
finally can scoop be used to convert
data in different formats I think I
already answered that right if no which
tools can be used for this purpose so
scoop can be used to convert data in
different formats and that depends on
the different arguments which you use
when you do a import such as avrofile
parquet file binary format with record
or Block Level compression so if you are
interested in knowing more on different
data formats then I think I can suggest
a link for that and we can say Hadoop
for maths
I think it is Tech Mackie afro parquet
Let's see we can find out the Link tech
Maggie yeah this is a very good link
which specifies or talks about different
data formats which you should know such
as your text file format different
compression schemes how is data
organization what are the common formats
what do you have in text file structured
binary sequence files with compression
without compression what is record level
what is Block Level what is a Avro data
file what is a sequel what is a parquet
data file or a columnar format another
formats like orc RC and so on so please
have a look at this so with this we've
come to the end of this Advanced Big
Data full course if you have found this
session informative and interesting
please consider subscribing to YouTube
channel let us know our queries in the
comment section below and our team of
experts will be more than happy to
resolve all the queries at the earliest
until next time thank you and and keep
learning staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign