hey everyone welcome back to our Channel
today we have got something really
exciting and store for you we are diving
head first into the fascinating world of
machine learning with the machine
learning crash course whether you are a
beginner looking to grasp the basics or
an Enthusiast eager to brush up on your
skills this video is your ultimate guide
to understand the fundamentals of
machine learning and before starting if
you are one of the aspired in machine
learning Enthusiast looking for online
training and graduating from the best
universities or a professional who
Willits to switch careers in machine
learning by learning from the experts
then try giving a short to Simply learn
postgraduate program in Ai and machine
learning in collaboration with berdu
University and IBM the course link is
mentioned in the description box below
that will navigate you to the course
page where you can find a complete
overview of the program being offered
and one more announcement that is if you
want to be a ml Enthusiast looking for
online training and graduating from the
best universities then we have a simply
learn skel Tech AI ml boot camp and the
boot camp link is also mentioned in the
description box below that will navigate
you to the Boot Camp Page and you can
find a complete overview of the program
being offered now let's take a minute to
hear from our Learners who have
experienced massive success in the
careers by opting out for the boot camp
and the postgraduate program you need to
keep updating your skills that course
material was comprehensive and the
faculty was extremely experienced uh The
Faculty was able to adjust their
teaching style in order to cater to the
overall skill set of the class in the
rapidly evolving world of technology
it's important to keep upskilling for
every working professional stay relevant
continue learning and before we get
started make sure to hit that subscribe
button and ring the notification Bell so
you never miss out on our future content
now without further ado let's embark on
this incredible journey into the world
of machine learning here we have our um
it looks a little bit like Frankenstein
our Frankenstein looking robot today let
me tell you what is machine learning
machine learning works on the
development of computer programs that
can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in two days this was
back in July 29th
2016 so that's pretty impressive this
amount of time to continue to grow in
his development and it's smart enough to
leave spaces in the brick work for
wiring and plumbing and can even cut and
shape bricks to size Amazon Echo relies
on machine learning and with more data
it becomes more accurate play your
favorite music order pizza from dominoes
voice control your home request rides
from Uber have you ever wondered the
difference between AI machine learning
and deep learning artificial
intelligence a technique which enables
machines to mimic human Behavior this is
really important because this is how we
are able to gauge how well our
computations or what we're working on
works is the fact that we're mimicking
human behavior we're using this to
replace human work and make it more
efficient and make it more streamlined
and more accurate and so the center of
artificial intelligence is the big
picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to train itself and
perform tasks Alpha go natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we'd
have a very hard time tracking and it's
able to come up with an answer from that
now let's see how machine learning works
first we start with training the data
once we've trained the data the train we
go into the machine learning algorithm
which then puts the data into a
processing which then goes down to
machine another machine learning
algorithm and and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training data and we find out it didn't
do a good job predicting it and it gets
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can so
without much further Ado let's get
started so deep learning is considered
to be a part of machine learning so this
diagram very nicely depicts what deep
learning is at a very high level you
have the all incompass ing artificial
intelligence which is more a concept
rather than a technology or a technical
concept right so it is it is more of a
concept at a very high level artificial
intelligence under the herd is actually
machine learning and deep learning and
machine learning is a broader concept
you can say or a broader technology and
deep learning is a subset of machine
learning the primary difference between
machine learning and deep learning is
that deep learning uses neural networks
and it is suitable for handling large
amounts of unstructured data and the
last but not least one of the major
differences between machine learning and
deep learning is that in machine
learning the feature extraction or the
feature engineering is done by the data
scientists manually but in deep learning
since we use neural networks the feature
engineering happens automatically so
that's a little bit of a a quick
difference between machine learning and
deep learning and this diagram very
nicely depicts the relation between
artificial and intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time and it can do
a lot of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this it traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's the
reason deep learning is required
compared to let's say traditional
machine learning it can also perform
complex uh algorithms more complex than
let's say what machine learning can do
and it can achieve best performance with
large amounts of data so the more you
have the data let's say reference data
or label data the better the system will
do because the training process will be
that much better and last but not least
with deep learning you can really avoid
the manual process of feature extraction
those are some of the reasons why we
need deep learning some of the
applications of deep learning deep
learning has made major inroads and it
is a major area in which deep learning
is applied is Healthcare and within
Healthcare particularly oncology uh
which is uh basically cancer related
stuff one of the issues with cancer is
that a lot of cancers today are curable
they can be cured they are detected
early on and the challenge with that is
when a Diagnostics is performed let's
say an image has been taken of a patient
to detect whether there is cancer or not
you need a specialist to look at the
image and determine whether it is the
patient is fine or there is any onset of
cancer and the number of Specialists are
limited so if we use deep learning if we
use automation here or if we use
artificial intelligence here then the
system can with a certain amount of the
good amount of accuracy determine
whether a particular patient is having
cancer or not so the prediction or the
detection process of a disease like
cancer can be expedited the detection
process can be expedited can be faster
without really waiting for a specialist
we can obviously then once the
application once the artificial
intelligence detects or predicts that
there is an onset of cancer this can be
crosschecked by a doctor but at least
the initial screening process can be
automated and that is where the current
focus is with respect to deep learning
in healthcare what else robotics is
another area deep learning is majorly
used in robotics and you must have seen
nowadays robots are everywhere humanoids
the industrial robots which are used for
manufacturing process you must have
heard about Sophia who got citizenship
with Saudi Arabia and so on there are
multiple such robots which are knowledge
oriented but there are also industrial
robots are used in Industries in the
manufacturing process and increasingly
in security and also in defense for
example image processing video is fed to
them and they need to be able to detect
object objects obstacles and so on and
so forth so that's where deep learning
is used they need to be able to hear and
make sense of the sounds that they are
hearing that needs deep learning as well
so robotics is a major area where deep
learning is applied then we have
self-driving cars or autonomous cars you
must have heard of Google's autonomous
car which has been tested for millions
of miles and pretty much incident free
there were of course a couple of
incidents here and there but it is uh
considered to be fairly safe and there
are today a lot of Automotive companies
in fact pretty much every automotive
company worth its name is investing in
self-driving cars or autonomous cars and
it is predicted that in the next
probably 10 to 15 years these will be in
production and they will be used
extensively in real life right now they
are all in R&D and in test phases but
pretty soon these will be on the road so
this is another area where deep learning
is used and how is it used where is it
used within autonomous driving the car
actually is fed with video of
surroundings and it is supposed to
process that information process that
video and determine if there are any
obstacles it has to determine if there
are any cars in the side will detect
whether it is driving in the lane also
it has to determine whether the signal
is green or red so that accordingly it
can move forward or wait so for all
these video analysis deep learning is
used in addition to that the training
overall training to drive the car
happens in a deep learning environment
so again a lot of scope here to use deep
learning couple of other applications
are machion translation today we have a
lot of information and very often this
information is in one particular
language and more specifically in
English and people need information in
in various parts of the world it is
pretty difficult for human beings to
translate each and every piece of
information or every document into all
possible languages there are probably at
least hundreds of languages or if not
more to translate each and every
document into every language is uh
pretty difficult therefore we can use
deep learning to do pretty much like a
realtime translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the Fly for
example you go to somewhere like China
and you want to know what is written on
a sign board now it is impossible for
somebody to translate that and put it on
the web or something like that so you
have an application which is strain to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate that
so therefore deep learning is used in
such situations as well and that is
again on demand so it is not like you
have to translate all these documents
from other languages into English in one
sh and keep it somewhere that is again
an pretty much an impossible task but on
a need basis so you have systems that
are trained to translate on the flag so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it it needs a lot
of effort to train such neural land it
has been proved that it is possible so
this is a relatively new area and and on
the same lines colorization of images so
these two images on the left hand side
is a grayscale image or a black and
white image this was colored by a neural
or a deep learning application as you
can see this done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning human versus artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every Nick and Nook we've gone
to the Moon uh we've gone into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you
human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
tasks that require human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer where is
identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give him a
bad Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by there's a
few hours and suddenly you wonder how
many cars buses trucks and so on passed
by in the six hours now chances are
you're not going to sit by the road for
six hours and count buses cars and
trucks unless you're working for the
city and you're trying to do City
Planning and you want to know hey do we
need to add a new truck route maybe we
need a bicycle Lan they have a lot of
bicyclists here that kind of thing so
maybe City Planning would be great for
this machine machine learning well the
way machine Learning Works is we have
labeled data with
features okay so you have a truck or a
car a motorcycle a bus or a bicycle and
each one of those are labeled it comes
in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle let's look a little bit
more in depth on this in the model here
it actually the features we're looking
at would be like the tires someone sits
there and figures out what a tire looks
like takes a lot of work if you try to
try to figure the difference between a
car tire a bicycle tire a motorcycle
tire uh so in the M machine learning
field this could take a long time if
you're going to do each
individual aspect of a car and try to
get a result on there and that's what
they did do that was a a very this is
still used on smaller amounts of data
where you figure out what those features
are and then you label them deep
learning so with deep learning learning
one of our Solutions is to take a very
large unlabeled data set and we put that
into a training model using artificial
neural networks and then that goes into
the neural network itself when we create
a neural network and you'll see um the
arrows are actually kind of backward but
uh which actually is a nice point
because when we train the neural network
we put the bicycle in and then it comes
back and says if it said truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weight it's going backward they
call it back propagation and let it know
it's a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the model so you need
to have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28x 28 pixels
that's a lot of information there um
could you imagine trying to guess that
this is a bicycle image by looking at
each one of those pixels and trying to
figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in is all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and outc comes and says okay this
is a
bicycle a lot of times they call this
the magic Black Box why because as we
watch it go across here all these
weights and all the math behind this and
it's not it's a little complicated on
the math side you really don't need to
know that when you're programming or
doing working with the Deep learning but
it's like magic you you don't know you
really can't figure out what's going to
come out by looking what's in each one
of those dots and each one of those
lines lines are firing and what's going
in between them so we like to call it
the magic box uh so that's where deep
learning comes in and in the end it
comes up and you have this whole neural
notwork comes up and it says okay we
fire all these different pixels and we
connects all these different dots and
gives them different weights and it says
okay this is a bicycle and that's how we
determine what the object is present in
the data with deep learning machine
learning we're going to take a step into
machine learning here and you'll see how
these fit together in a minute the
system is able to make predictions or
take decisions based on past data that's
very important for machine learning is
that we're looking at stuff and based on
what's been there before we're creating
a decision on there we're creating
something out of there we're coloring a
beach ball we're telling you what the
weather is in Chicago what's nice about
machine learning is a very powerful
processing capability it's quick and
accurate outcomes so you get results
right away once you program the system
the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do problem solved in an end
to end method so instead of having to
break it apart and you have the first
piece coming in and you identify tires
and the second piece is identifying uh
labeling handlebars and then you bring
that together that if it has handlebars
and tires it's a bicycle and if it has
something that looks like a large square
is probably a truck the neural networks
does this all in one Network you don't
really know what's going on in all those
weights and all those little bubbles uh
but it does it pretty much in one
package that's why the neural network
systems are so big nowadays and coming
into their own best features are
selected by the system and it this is
important they kind of put it it's on a
bullet on the side here it's a subset of
machine learning this is important when
we talk about deep learning it is a form
of machine learning there's lots of
other forms of machine learning data
analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test um you put in your
information you then have your group of
uh test and then you held some aside you
see how does it do it's very quick to
test it and see what's going on with
your deep learning and your neural
network are they really all that
different AI versus machine learning
versus deep learning concepts of AI
so we have concepts of II you'll see
natural language processing uh machine
learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books it goes through some deep learning
the Digital Data is analyzed text mining
comes through the particular book you're
looking for maybe it's a genre books is
identified and in this case uh we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens uh so
we look at some of the pieces here this
information goes into uh as far as this
example the translation of the
handwritten printed data to digital form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
uh what you're looking for and say oh
you're looking for auto repair and
computer so you're looking for automated
cars once it's identified then of course
it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as a big picture
how does this compare to The Human
Experience in either uh doing the same
thing as a human we do or it does it
better than us and machine learning
which has a lot of tools uh is something
that learns from data past experiences
it's programmed it's uh comes in there
and it says hey we already had these
five things happen the sixth one should
be about the same and then uh then
there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the uh backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the processing
machine learning uh spam detection very
common machine learning doesn't really
have the human interaction part so this
is the part where it goes and says okay
that's a Spam that's not a Spam and it
puts it in your spam
folder search engine result refining uh
another example of machine learning
whereas it looks at your different
results and it Go and it uh is able to
categorize them as far as this had the
most hits this is the least viewed this
has five stars um you know however they
want to weit it uh all exam good
examples of machine learning and then
the Deep learning uh deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so it's able to
look at the exit in the middle
and it goes okay we want to know what
that is in French and it's able to push
that out in French French and learn how
to do
that and then we have chatbots um I
remember when Microsoft first had their
little paperclip um boy that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being were you just doing a search on
certain words it's now able to start
linking those words together and form a
sentence in that chat box types of AI
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to reciter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at like a neural network
that's been programmed to identify cars
it doesn't remember all those pictures
it has no memory as far as the hundreds
of pictures you process through it all
it has is this is the pattern I use to
identify cars as the final output for
that neural network we looked at so when
they talk about limited memory this is
what they're talking about they're
talking about I've created this based on
all these things but I'm not going to
remember any one
specifically theory of Mind systems
being able to understand human emotions
and how they affect decision-making to
adust adust their behaviors according to
their human understanding this is
important because this is our page mark
this is how we know whether it is an
artificial intelligence or not is it
interacting with humans in a way that we
can understand uh without that
interaction is just an object uh so we
talk about theory of mind we really
understand how it interfaces that whole
if you're in web development user
experience would be the term I would put
in there so the theory of mind would be
user experience how's the whole UI
connected together and one of the final
things is as we get into artificial
intelligence is systems being aware of
themselves understanding their internal
States and predicting other people's
feelings and act appropriately so as
artificial intelligence continues to
progress uh we see ones are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based Bas on
all that information predicting which
action would work the best what would
help people uh if I know that you're
having a cup of coffee first thing in
the morning is what makes you happy as a
robot I might make you a cup of coffee
every morning at the same time uh to
help your life and help you grow that'd
be the self-awareness is being able to
know all those different things types of
machine learning and like I said on the
last slide this is very important this
is very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first one is supervised learning systems
are able to predict future outcome based
on past data requires both an input and
an output to be given to the model for
it to be trained so in this case we're
looking at anything where you have 100
images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
eff
uh you'll heard the term cluster how do
you cluster things together some of
these things go together some of these
don't this is unsupervised where can
look at an image and start pulling the
different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them CU it's
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees all we told it was hey separate at
this picture by things that don't match
and things that do match and come
together and finally there's
reinforcement learning systems are given
no training it learns on the basis of
the reward punishment it received for
performing its Last Action it helps
increase the efficiency of a tool
function or a program reinforced
learning or reinforcement learning is
kind of you give it a yes or no yes you
gave me the right response no you didn't
and then it looks at that and says oh
okay so based on this data coming in and
uh what I gave you was a wrong response
so next time I'll give you a different
one comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
we're grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe have like the case of
one of the things 500 points of data and
30 different fields it starts getting
really confusing there in artificial
intelligence or machine learning and the
Deep learning aspect really shines when
you get to that larger data that's
really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop
with no problem and do the calculations
there where with the machine learning
usually needs a higher-end system to
work it takes a lot more processing
power to build those neural networks and
to train them it goes through a lot of
data when we're talking about the
general machine learning tools most
features need to be identified in
advanced and manually coded so there's a
lot of human work on here the machine
learns the features from the data it is
provided so again it's like a magic box
you don't have to know what a tire is it
figures it out for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts and the
problem is solved in an endtoend manner
so you only have one neural network or
two neural networks that is bringing the
data in and putting it out it's not
going through a lot of different
processes to get there and remember you
can put machine learning and deep
learning together so you don't always
have just the Deep learning solving the
problem you might have solving one piece
of the puzzle
with regular machine learning and most
of machine learning tools out there they
take longer to test and understand how
they work and with the Deep learning
it's pretty quick once you build that
neural network you test it and you know
so we're dealing with very crisp rules
limited
resources you have to really explain how
the decision was made when you use most
machine learning tools but when you use
the Deep learning tool inside the
machine learning tools the system takes
care of it based on its own logic and
reasoning and again it's like a magic
black box you really don't know how it
came up with the answer you just know it
came up with the right answer a glimpse
into the future so a quick glimpse into
the future artificial intelligence using
it to detecting crimes before they
happen humanoid AI helpers which we
already have a lot of there'll be more
and more maybe it'll actually be
Androids that'd be cool to have an
Android that comes and gets stuff out of
my fridge for me machine learning
increasing efficiency in health care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant now let's
look into the types of machine learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggest you have to
supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled finally there's reinforcement
learning wherein the system learns on
its own let's talk about all these types
in detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train this is how the
training is done we provide a data set
that contains pictures of a kind of a
fruit say an apple then we provide
another data set which lets the model
know that these pictures where that of a
fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can tell you what fruit it is and
it will remember this and apply this
knowledge in future as well that's how
supervised Learning Works you are
training the model to do a certain kind
of an operation on its own this kind of
a model is generally used into filtering
spam mails from your email account as
well yes surprise aren't you so let's
move on to unsupervised learning now
let's say we have a data set which is
cluttered in this case we have a
collection of pictures of different
fruits we feed these data to the model
and the model analyzes the data to
figure out patterns in it in the end it
categorizes the photos into three types
as you can see in the image based on
their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the cyh by
movies lately let's find out how it
works imagine a newborn baby you put a
burning candle in front of the baby the
baby does not know that if it touches
the flame its fingers might get burned
so it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works we provide
the machine with a data set wherein we
ask it to identify a particular kind of
a fruit in this case an Apple so what it
does as a response it tells us that it's
a mango but as we all know it's a
completely wrong answer so as a feedback
we tell the system that it's wrong it's
not a mango it's an apple what it does
it learns from the feedback and keeps
that in mind when the next time when we
ask a same question it gives us the
right answer it is able to tell us that
it's actually an apple that is a
reinforced response so that's how
reinforced learning works it learns from
his mistakes and experiences this model
is used in games like Prince of Persia
or Assassin's Creed or FIFA where in the
level of difficulty increases as you get
better with the games just to make it
more clear for you let's look at a
comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this this is
actually an apple that is called label
data so the system learns from the label
data and makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised learning
machine learning has improved our lives
in a number of wonderful ways today
let's talk about some of these I'm Rahul
from Simply learn and these are the top
10 applications of machine learning
first let's talk about virtual personal
assistants Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever noticed
that these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to L's cricket ground the first thing I
would probably do is to get on Google
Maps so search
it and let's put it
here
so here we have the path you should take
to get to Lodge cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicate that
they are slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as I told you
before red means heavily congested
yellow means slow moving and blue means
clear so how exactly is Google able to
tell you that the traffic is clear slow
moving or heavily congested so this is
with the help of machine learning and
with the help of two important measures
first is the average time that's taken
on specific days at specific times on
that route the second one is the
real-time location data of vehicles from
Google Maps and with the help of sensors
some of the other popular map services
are Bing Maps maps.me and here we go
next up we have social media
personalization so say I want to buy a
drone and I'm on Amazon and I want to
buy a DJI mavic Pro the thing is it's
close to one lap so I don't want to buy
it right now but the next time I'm on
Facebook I'll see an advertisement for
the product next time I'm on YouTube
I'll see an advertisement even on
Instagram I'll see an advertisement so
here with the help of machine learning
Google has understood that I'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which I've already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity fake
account where these accounts only last
for how long the transaction takes place
and stop existing after that and man in
the middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
shortterm memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and ischemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assistant is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other and before
starting if you're one of the aspiring
machine learning Enthusiast looking for
online training and graduating from the
best universities or a professional who
elicits to switch careers in machine
learning by learning from the experts
then try giving a short to Simply learn
postgraduate program in Ai and machine
learning in collaboration with birw
University and IBM the course link is
mentioned in the description box below
that will navigate you to the course
page where you can find a complete
overview of the program being offered
and one more announcement that is if you
want to be a ml Enthusiast looking for
online training and graduating from the
best universities then we have a simply
learn skel Tech a ml boot camp and the
boot camp link is also mentioned in the
description box below that will navigate
you to the Boot Camp Page and you can
find a complete overw of the program
being offered data denotes the
individual pieces of factual information
collected from various sources it is
stored processed and later used for
analysis and so we see here uh just a
huge grouping of information a lot of
Tex stuff money dollar signs
numbers uh and then you have your
performing analytics to drive insights
and hopefully you have a nice share your
shareholders gather it at the meeting
and you're able to explain it in
something they can understand so we talk
about data types of data we have in our
types of data we have a qualitative
categorical you think nominal or ordinal
and then you have your quantitative or
numerical which is discrete or
continuous and let's look a little
closer at those data type vocabulary
always people's favorite is the
vocabulary words okay not mine uh but
let's di dive into this what we mean by
nominal nominal they are used to label
various uh label our variables without
providing any measurable value uh
country gender race hair color Etc it's
something that you either mark true or
false this is a label it's on or off
either they have a red hat on or they do
not uh so a lot of times when you're
thinking nominal data labels uh think of
it as a true false kind of setup and we
look at ordinal this is categorical data
with a set order or a scale to it uh and
you can think of salary range is a great
one uh movie ratings Etc you see here
the salary R if you have 10,000 to
20,000 number of employees earning that
rate is 150 20,000 to 30,000 100 and so
forth some of the terms you'll hear is
bucket uh this is where you have 10
different buckets and you want to
separate it into something that makes
sense into those 10 buckets and so when
we start talking about orinal a lot of
times when you get down to the brast
bones again we're talking true false uh
if you're a member of the 10 to 20K
range uh so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket quantitative numerical data
uh falls into two classes discrete or
continuous and so data with a final set
of values which can be categorized class
strength questions answered correctly
and runs hit and cricet a lot of times
when you see this you can think integer
uh and a very restricted integer I.E you
can only have 100 questions um on a test
so you can it's very discreet I only
have a 100 different values that it can
attain so think usually you're talking
about integers but within a very small
range they don't have an open end or
anything like that uh so discret is very
solid simple to count set number
continuous on the other hand uh
continuous data can take any numerical
value within a range so water pressure
weight of a person Etc usually we start
thinking about float values where they
can get phenomenally small in their in
what they're worth and there's a whole
series of values that falls right
between discrete and
continuous um you can think of the stock
market you have dollar amounts it's
still discreet but it starts to get
complicated enough when you have like
you know jump in the stock market from
$525 33 to
$580 67 there's a lot of Point values in
there it' still be called discret but
you start looking at it as almost
continuous because it does have such a
variance in it now uh we talk about no
we did we went over nominal and ordinal
uh almost true false charts and we
looked at quantitative and numerical
data which we start to get into numbers
discreet you can usually a lot of times
discret will be put into it could be put
into true false but usually it's not uh
so we want to address this stuff and the
first thing we want to look at is the
very basic which is your algebra so
we're going to take a look at linear
algebra you can remember back when
you're ukian geometry uh we have a line
well let's go through this we have a
linear algebra is the domain of
mathematics concerning linear equations
and their representations in Vector
spaces and through matrixes I told you
we're going to talk about
matrixes uh so a linear equation is
simply um uh 2x + 4 y - 3 Z = 10
very linear 10x + 12.4 y = z and now you
can actually solve these two equations
by combining them uh and that's where
we're talking about a linear
equation and the vectors we have a plus
bals c now we're starting to look at a
direction and these values usually think
of an XY zplot um so each one is a
direction and the actual distance of
like a triangle AB is C and then your
Matrix can describe all kinds of things
um I find matrixes uh confuse a lot of
people not because they're particularly
difficult but because of the magnitude
and the different things they're used
for and a matrix is a a chart or a um
you know think of a spreadsheet but you
have your rows and your columns and
you'll see here we have a * b equals c
very important to know your counts uh so
depending depending on how the math is
being done what you're using it for
making sure you have the same rows and
number of columns or a single number
there's all kinds of things that play in
that that can make matrixes confusing uh
but really it has a lot more to do with
what domain you're working in uh are you
adding in multiple polom where you have
like uh uh ax^2 plus b y plus you know
you start to see that can be very
confusing versus a very straightforward
Matrix and let's just go a little deeper
into these cuz these are such primary
this is what we're here to talk about is
these different math uh mathematical
computations that come up so we're
looking at linear equations let's dig
deeper into that one an equation having
a maximum order of one is called a
linear
equation uh so it's linear because when
you look at this we have uh ax plus Bal
C which is a one variable we have two
variable ax+ b y = c ax plus b y + z c
zal d and so forth but all of these are
to the power of one you don't see x s
you don't see X cubed so we're talking
about linear equations that's what we're
talking about in their addition if you
have already dived into say neural
networks you should recognize this ax
plus b y plus CZ um setup plus The
Intercept uh which is basically your
your neural network each node adding up
all the different inputs and we can
drill down into that most common formula
is your y = mx +
C so you have your y equals the M which
is your slope your X Value Plus C which
is your um Y intercept they kind of
labeled it wrong
here threw me for a loop but the the C
would be your Y intercept so when you
set x equal to 0er y equals c and that's
that's your Y intercept right there uh
and that's they they just had reverse
value of y when x equals 0 equals the Y
intercept which is C and your slope
gradient line which is your M so you get
your y = 2x + 3 and there's lots of easy
ways to compute this this why this is
why we always start with the most basic
one when we're solving one of these
problems and then of course the um one
of the most important takeaways is the
slope gradient of the line uh so the
slope is very important that M value uh
in this case we went ahead and solved
this if you have y = 2x + 3 you can see
how it has a nice line graph here on the
right so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and
rows so we're talking M rows by in
columns here A1 is denotes the element
of the first row in the First Column
similarly a12 and it's really pronounced
a11 in this particular setup so it's a
row one column one a 12 is a of Row one
one column two uh first row and second
column and so
on and there's a lot of ways to denote
this I've seen these as like a capital
letter a smaller case a for the top row
or I mean you can see where they can go
all kinds of different directions as far
as the
value you just take a moment to realize
there needs to be some designation as
far as what row it's in and what column
it's
in and we have our uh basic operations
we have addition so when you think about
addition you have uh
two matrixes of 2 by two and you just
add each individual number in that
Matrix and then when you get to the
bottom you have uh in this case the
solution is 12 10 + 2 is 12 5 + 3 is 8
and so on and the same thing with
subtraction now again you're counting
matrixes you want to check your um
dimensions of the Matrix the shape
you'll see shape come up a lot in
programming so we're talking about
Dimensions we're talking about the shape
if the two shapes are equal this is what
happens when you add them together or
subtract them and we have multiplication
when you look at the multiplication you
end up with a very slightly different
setup going now if we look at our last
one we um uh we're like why this always
gets to me when we get to matrixes they
don't really say why you multiply
matrixes um you know my first thought is
1 * 2 4 * 3 but if you look at this we
get 1 * 2 + 4 * 3 1 * 3 + 4 *
5 uh 6 * 2 + 3 * 3 6 * 3 + 3 * 5 if
you're looking at these matrixes uh
think of this more as an equation and so
we have uh if you remember when we back
up here for our multiple line equations
let's just go back up a couple slides
where we were looking at uh two variable
so this is a two variable equation ax
plus b y =
c um and this is a way way to make it
very quick to solve these variables and
that's why you have the Matrix and
that's why you
do the multiplication the way they do
and this is the dotproduct of uh 1 * 2 +
4 *
3 1 * 3 + 4 *
5 uh 6 * 2 + 3 * 3 6 * 3 + 3 * 5 and it
gives us a nice little U 14 23 21 and 33
over here which then can be used and
reduced down to a simple um formula as
far as solving the variables as you have
enough inputs uh and then in Matrix
operations when you're dealing with a
lot of matrixes uh now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so when we're talking about
multiplication we're talking about
solving uh for equations when you're
finding the product you are just finding
1 * 2 keep that in mind because that
does come up I've had that come up a
number of times where I am altering data
and I get confused as to what I'm doing
with it uh transpose flipping the Matrix
over its diagonal comes up all the time
where you have you still have 12 but
instead of it being a 128 it's now 1214
821 you're just flipping the columns and
the rows uh and then of course you can
do an inverse um changing the signs of
the values across this main diagonal and
you can see here we have the inverse a
to the minus one and ends up with with
uh instead of 12 8 14 12 it's now - 22
-2 vectors uh Vector just means we have
a value and a
direction and we have down four numbers
here on our
Vector uh in mathematics a
one-dimensional matrix is called a
vector uh so if you have your X plot and
you have a single value that value is
along the x- axis and it's a single
Dimension if you have two two Dimensions
you can think about putting them on a
graph you might have X and you might
have y and each value denotes a
direction and then of course the actual
distance is going to be the hypothesis
of that triangle uh and you can do that
with three dimensionals x y and z uh and
you can do it all the way to nth
Dimensions so when they talk about the K
means uh for categorizing and how close
data is together they will compute that
based on the Pythagorean theorem so you
would take uh the square of each value
add them all together and find the
square root and that gives you a
distance as far as where that point is
where that Vector exist or an actual
point value and then you can compare
that point value to another one and it
makes a very easy comparison versus
comparing uh 50 or 60 different numbers
and that brings us up to ige vectors and
ige values uh igene vectors the vectors
that don't change their span while
transformation and IG values the scalar
values that are associated to the
vectors conceptually you can think of
the vector as your picture you have a
picture it's um uh two Dimensions X and
Y and so when you do those two
dimensions and those two values or
whatever that value is um that is that
point but the values change when you
skew it and so if we take and we have a
vector a and that's a set value
uh B is um your is your you have a and b
which is your IG Vector two is the igene
value so we're altering all the values
by two that means we're um maybe we're
stretching it out One Direction making
it tall uh if you're doing picture
editing um that that's one of the places
this comes in but you can see when
you're
transforming uh your different
information how you transform it is then
your IGN value and you can see here a
vector after line trans transition uh we
have 3A a is the aene vector 3 is the
igene value so a doesn't change that's
whatever we started with that's your
original picture and three uh is skewing
it One Direction and maybe uh B is being
skewed another Direction and so you have
a nice tilted picture because you've
altered it by those by the igene
values so let's go ahead and pull up a
demo on linear algebra and to do this
I'm going to go through my trusted
Anaconda into my Jupiter notebook and
we'll create a new uh notebook called
linear algebra since we are working in
Python uh we're going to use our numpy I
always import that as NP our numpy array
probably the most popular um module for
doing matrixes and things
in given that this is part of a series
I'm not going to go too much into numpy
uh we are going to go and create two
different variables a for a numpy array
1015 and b
29 we'll go ahead and run this and you
can see there's our two arrays 1015 29
and I went ah and added a space there in
between so it's easier to read and since
it's the last line we don't have to put
the print statement on it unless you
want we can simp but we can simply do a
plus b so when I run this uh we have 105
29 and we get 30 24 which is what you
expect 10 plus + 20 15 + 9 you could
almost look at this addition as being
um just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do
A.T plus
B.T remember that t flips them and so if
we do that we now get them uh we now
have 3024 going the other way we could
also do something kind of fun there's a
lot of different ways to do this
uh as far as a plus b I can also do A+
B.T and you're going to see that that
will come out the same the 3024 whether
I transpose A and B or transpose them
both at the
end and likewise we can very easily
subtract two vectors I can go a minus B
and we run that and we get min - 106 now
remember this is the last line in this
particular section that's why I don't
have to put the print around it uh
and just like we did before we can
transpose either the individual or we
can transpose the main setup and then we
get a minus 106 going the other
way now we didn't mention this in our
notes but you can also do a scalar
multiplication let me just put down the
scaler so you can remember that uh what
we're talking about here is I have uh
this array here U and and if I go a * U
uh we'll take the value two we'll
multiply it by every value in here so 2
* 30 is 60 2 *
15 and just like we did
before um this happens a lot because
when you're doing matrixes you do need
to flip them you get 6030 coming this
way so in numpy uh we have what they
call Dot
product and uh with this in a
two-dimensional vectors it is the
equivalent of to matrix
multiplication remember we were talking
about matrix
multiplication uh where it is the well
let's walk through
it we'll go ahead and start by defining
two um numpy arrays we'll have uh 10 20
25 6 or our U and our V uh and then
we're going to go ahead and do if we
take the values uh and if you remember
correctly
an array like this would be 10 * 25 + 20
* 6 we'll go ahead and uh print
that there we
go and then we'll go ahead and do the
np. dot of VI
comma
V and we'll find when we do this we go
ah and run this uh we're going to get uh
370
370 so this is a strain multiplication
where they use it to solve uh linear
algebra uh when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
uh value for our Dot
multiplication and we did um addition
earlier which was just your basic
addition uh and of course the Matrix you
can get very complicated on these or in
this case we'll go ahead and do um let's
create two complex
matrixes this one is a matrix of um you
know 1210 46 431 we'll just print out a
so you see what that looks like here's
print
a we print a out you can see that we
have a um
2x3 layer Matrix for a and we can also
put together always kind of fun when
you're playing with print values uh we
could do something like this we could go
in here there we go uh we could print a
we have it end with uh equals a run and
this kind of gives it a nice look uh
here's your Matrix that's all this is
comma and means it just tags it on the
end that's all all that is doing on
there and then we can simply add in what
is a plus b and you should already guess
because this is the same as what we did
before there's no difference uh when we
do a simple vector addition we have 12 +
2 is 14 10 + 8 is 18 and so on and just
like we did the uh Matrix addition we
can also do a minus B and do our Matrix
subtraction and we look at this uh we
have what 12 - 2 is 10 10 minus 8 um
where are
we oh there we go 8 H confusing what I'm
looking looking at I should have
reprinted out the original numbers uh
but we can see here 12 - 2 is of course
10 10 - 8 is 2 uh 4 - 46 is - 42 and so
forth so same as a subtraction as before
we just call it Matrix subtraction it's
identical now if you remember up here we
had a scalar addition where we're adding
just one number to a matrix you can also
do scalar
multiplication uh and so simply if you
have a single value a and you have B
which is your array we can also do a * B
when we run that uh you can see here we
have 2 * 4 is 8 uh 5 * 4 is 20 and so
forth you're just multiplying the four
across each one of these values and this
is an interesting one that comes up a
little bit of a brain teaser is uh
Matrix and Vector
multiplication and so we're looking at
this uh we are just do regular arrays it
doesn't necessarily have to be a numpy
array we have a which has our um array
of arrays and B which is a single array
and so we can from
here do the
dot
ab and this is going to return two
values and the first value is that it's
you could say it's like a um we're doing
the this array B array first with a and
then with the second one and so it
splits it up so you have a matrix of
vector multiplication and you can mix
and match when you get into really
complicated uh backend stuff this
becomes more common because you're now
you got layers upon layers of data and
so you you'll end up with a matrix and a
set of vector matrices do you want to
multiply now keep in mind that if you're
doing data science a lot of times you're
not looking at this this is what's going
on behind the scenes so if you're in um
the s kit looking at sklearn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on other
times you might find yourself having to
do part of this and manipulate the data
around so it fits right and then you go
back in and you run it through the S kit
and if we can do um up here where we did
a uh Matrix and Vector multiplication we
can also do Matrix to matrix
multiplication and if we run this where
we have the two matrixes uh you can see
we have very complicated array that of
course comes out on there for our DOT
and just to reiterate it we have our
transpose of Matrix which is your T and
so if we create a matrix a and we do a
transpose it you can see how it flips it
from 5 10 15 20 253 2 5 15 25 10 20 30
uh rows and
columns and certainly with the math uh
this comes up a lot um it also comes up
a lot with XY plotting when you put into
Pi plot you have one format where
they're looking at Pairs and numbers and
then they want all of x's and all y's so
you know the transpose is an important
tool both for your math and for plotting
and all kinds of things another tool
that we didn't discuss uh is your
identity
Matrix uh and this one is more
definition uh the identity Matrix um we
have here one where we just did uh two
so it comes down as one0 0 one uh 1 0 0
1 0 it creates a diagonal of one and
what that is is when you're doing your
identities you can be comparing all your
different features to the different
features and how they correlate and of
course when you have a feature one
compared to feature one to itself it is
always one uh where usually it's between
zero one depending on how well
correlates so when we're talking about
identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is and then another thing we can
do uh to kind of wrap this up we'll hit
you with the most complicated uh um
piece of this puzzle here is an inverse
um a
matrix and let's just go ahead and put
the um oh it's a linky
description let's go and put the
description this is straight out of the
uh the website for um numpy uh so given
a square Matrix a here's our Square
Matrix a which is 2 1 0 0 1
0121 keep in mind 3x3 it's Square it's
got to be equal it's going to return the
Matrix a inverse satisfying a um a
inverse so here's our matrix
multiplication um and then of course it
equals the dot uh yeah a inverse of a
um with an identity shape of uh a do
shape zero this is just reshaping the
identity that's a little complicated
there uh so we go and have our here's
our array uh we'll go and run this and
you can see what we end up with is we
end up with uh an array 0.5 minus 0.5
and so farth with our 211 going down
2101
0121 um getting into a little deep on
the math understanding when you need
this is probably really is is what's
really important when you're doing data
science versus uh handwriting this out
and looking up the math and handwriting
all the pieces out you do need to know
about the linear algorithm inverse of a
uh so if it comes up you can easily pull
it up or at least remember where to look
it up we took a look at the algebra side
of it let's go ahead and take a look at
the calculus side of uh what's going on
here with the machine learning so
calculus oh my goodness and different
equations you got to throw that in there
cuz that's all part of the bag of tricks
especially when you're doing large
neural networks but also comes up in
many other areas the good news is most
of it's already done for you in the
backend uh so when it comes up you
really do need to understand from the
data science not data analytics data
analytics means you're digging deep into
actually solving these math equations u
and a neural network is just a giant
differential
equation uh so we talk about calculus uh
we're going to go ahead and understand
it by talking about cars versus time and
speed uh so helps to calculate the
spontaneous rate of
change uh so suppose we plot a graph of
the speed of a car with respect to time
so as you can see here going down the
highway probably merged into the highway
from an on-ramp so I had to accelerate
so my speed went way up uh stuck in
traffic merged into the traffic traffic
opens up and I accelerate again up to
the speed limit and uh maybe a Peter's
off up there so you can look at this as
as um the speed versus time I'm getting
faster and faster because I'm
continually accelerating and if I hit
the brakes it' go the other way so the
rate of change of speed with respect of
time is nothing but acceleration how
fast are we accelerating the
acceleration is the area between the
start point of X and the end point of
Delta x uh so we can calculate a simple
if you had X and Delta X we could put a
line there and that slope of the line is
our
acceleration now that's pretty easy when
you're doing linear algebra but I don't
want to know it just for that line in
those two points I want to know it
across the whole of what I'm working
with that's where we get into calculus
so when we talk about the distance
between X and Delta X it has to be the
smallest possible near to zero in order
to approximate the
acceleration
uh so the idea is that instead of I mean
if you ever did took a basic calculus
class they would draw bars down here and
you would divide this area up um let's
go back up a screen you divide this area
of this time period up into maybe 10
sections and you'd use that and you
could calculate the acceleration between
each one of those 10 sections kind of
thing uh and then we just keep making
that space smaller and smaller until
Delta X is almost uh infinitism small
and so we get a function of a uh equals
a limit as H goes to zero of a function
of a plus h minus a function of a over H
and that is you're Computing the slope
of the
line we're just Computing that slope
under smaller and smaller and smaller
samples uh and that's what calculus is
calculus is the integral you can see
down here we have our nice uh integral
sign looks like a giant s and that's
what that means is that we've Tak this
down to as small as we can for that
sampling uh so we're talking about
calculus we're finding the area under
the slope is the main process in the
integration similar small intervals are
made of the smallest possible length of
X Plus Delta X where Delta X approaches
almost an Infinitis small space and then
it helps to find the overall
acceleration by summing up all the
lengths together uh so we're summing up
all the accelerations from the beginning
to the end
and so here's our integral we sum of a
of x * D ofx = A + C uh that is our
basic calculus here so when we talk
about multivariate calculus uh
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations um
uh change in W over change of time
equals change of w over change of Z the
differential of Z to DX differential of
x to DT it gets pretty complicated uh
and it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f ofx of Y of D of a equals
the sum from C to D and A to B of f ofx
y DX Dy equals uh the sum of a to B sum
of C to D of FX y Dy
DX understanding the very specifics of
every everything going on in here and
actually doing the math is use the
calculus one calculus 2 and differential
equations uh so you're talking about
three fulllength courses to dig into and
solve these math equations what we want
to take from here is we're talking about
calculus uh we're talking about summing
of all these different slopes and so
we're still solving a linear uh
expression we're still solving y = mx +
b but we're doing this for infinitism
small x's and then we want to sum them
up that's what this integral sign means
the the sum of a of x d of xal a plus
c and when you see these very
complicated uh multivariate
differentiation using the chain rule uh
when we come in here and we have the
change of w to the change of T equals
the change of w DZ uh and so forth
that's what's going on here that's what
these means we're basically looking for
the area under the curve which really
comes to to how is the change changing
speed's going up how is that changing
and then you end up with a multiple
layer so if I have three layers of
neural networks how is the third layer
changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
uh multivariate integration um with
integrals the good news is we can solve
this uh mathematically and that's what
we do when you do neural networks in
reverse propagation uh so the nice thing
is that you don't have to solve this on
paper unless you're a data analysis and
you're working on the back end of
integrating these formulas and building
the script to actually build them so we
talk about applications of calculus uh
it provides us the tools to build an
accurate predictive model um so it's
really behind the scenes we want to
guess at what the change or the change
or the change
is that's a little goofy I I know I just
threw that out there it's kind of a
metat term but if you can guess how
things are going to to change then you
can guess what the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables so there's are multiple
variables going in there if uh one
variable is changing how does it affect
the other
variable and then in gradient descent
calculus is used to find the local and
Global
Maxima and this is really big uh we're
going actually going to have a whole
section here on gradient dis desent
because it is really I mean I talked
about neural networks and how you can
see how the different layers go in there
but gradient descent is one of the most
key things for trying to guess the best
answer to something so let's take a look
at the code behind gradient descent and
uh before we open up the code let's just
do real quick uh gradient
descent let's say we have a curb like
this and and most common is that this is
going to represent your error
oops error there we go error uh hard to
read there and I want to make the error
as low as possible and so what I'm
looking at it is I want to find this
line here which is the minimum value so
we're looking for the minimum and it
does that by uh sampling there and then
it based on this it guesses it might be
someplace here and it goes hey this is
still going down it goes here and then
goes back over here and then goes a
little bit closer and it's just playing
a high low until it gets to that spot
that bottom spot and so we want to
minimize the error and uh on the flip
note you could also want to be
maximizing something you want to get the
best output of it uh that's simply uh
minus the value uh so if you're looking
for where the peak is this is the same
as a negative for where the valley is
I'm looking for that Valley uh that's
all that is and this is a way of finding
it so the cool thing is um all the heavy
lifting is done um I actually ended up
putting together one of these a while
back is was when I didn't know about ssy
kick and I was just starting boy is a
long while back and uh is playing high
low how do you play high low not get
stuck in The Valleys uh figure out these
curves and things like that well you do
that in the back end is all the calculus
and differential equations to calculate
this out the good news is you don't have
to do
those uh so instead we're going to put
together the code and let's go
ahead and see what we can do with
that so uh guys in the back put together
a nice little piece of code here which
is kind of fun uh some things we're
going to note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
going to find these things are stumbling
blocks uh the first one is current X
where do we start at uh keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start um and we started at this
because the algorithm starts at x equals
3 so we arbitrarily picked five learning
rate is uh how many bars to skip going
one way or the other uh I'm in fact I'm
going to separate that a little bit
because these two are really important
um if we're dealing with something like
this where we're talking about um well
here's our here's the function we're
going to use our um gradient of our
function um 2 * x + 5 keep it simple so
that's a function we're going to work
with so if I'm dealing with increments
of a th000 0.1 is going to be a very
long time and if I'm dealing with
increments of
0.001 uh 0.1 is going to skip over my
answer so I won't get a very good answer
um and then we look at Precision this
tells us when to stop the algorithm so
again very specific to what you're
working on uh if you're working with
money and you don't convert it into a
float value uh you might be dealing with
0.001 which is a penny that might be
your Precision you're working with um
and then of course a previous step size
Max iterations uh we want something to
cut out at a certain point usually
that's built into a lot of minimization
functions and then here's our actual uh
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
iters is less than Max Max
it say that 10 times fast um we're just
saying if it's uh if we're if we're
still greater than our Precision level
we still we got to keep digging deeper
um and then we also don't want to go
past a thou or whatever this is a
million or 10,000 uh running that's
actually pretty high um we almost never
do Max iterations more than like a 100
or 200 rare occasions you might go up to
4 500 if it's depending on the problem
you're working with uh so we have our
previous equals our current that way we
can track
TimeWise uh the current now equals the
current minus the rate times the formula
of our previous set
so now we've generated our new version
uh previous step size equals the
absolute current
previous uh so we're looking for the
change in X it equals iterations plus
one that's so we know to stop if we get
too far and then we're just going to
print the local minimum occurs at X on
here and if we go ahead and run
this uh you can see right here it gets
down to this point and it says hey um
local minimum is -
33222 for this particular series we
created uh and this is created off of
our formula here the Lambda X2 * x + 5
now when I'm running this stuff uh
you'll see this come up a
lot in uh with the SK learn kit and and
one of the nice reasons of breaking this
down the way we did is I could go over
those top pieces uh those top pieces are
everything when you start looking at
these minimization tool in built-in code
and so from um we'll just do it's
actually
docs. cp.org and we're looking at the
pyit there we go um optimize
minimize you can only minimize one value
you have the function that's going in
this function can be very complicated uh
so we used a very simple function up
here it could be yeah there's all kinds
of things that could be on there and
there's a number of methods to solve
this as far as how they shrink down uh
and your X notot there's your there's
your start value so your function your
start value um there's all kinds of
things that come in here that we can
look at which we're not going to um
optimization automatically creates
constraints bounds some of this it does
automatically but you really the big
thing I want to point out here is you
need to have a starting point you want
to start with something that you already
know is mostly the answer uh if you
don't then it's going to have a heck of
a time trying to calculate it
out or you can write your own little
script that does this and and does a
high low guessing and tries to find the
max value that brings us to statistics
what this is kind of all about is
figuring things out lot of vocabulary
and statistics uh so statistics well I
guess it's all relative it's definitely
not an edel class uh so a bunch of stuff
going on statistics statistics concerns
with the The Collection organization
analysis interpretation and presentation
of data that is a mouthful um so we have
from end to end where where does it come
from is it valid what does it mean how
do we organize it um how do we analyze
it then you got to take those analysis
and interpret it into something that uh
people can use kind of reduce it to
understandable um and nowadays you have
to be able to present it if you can't
present it no one else is going to
understand what the heck you
did so when we look at the terminologies
uh there's a lot of terminologies
depending on what domain you're working
in so clearly if you're working in um a
domain that deals
with viruses and te- cells and and how
does you know where does it come from
and you're studying the different people
then you're going to have a population
if you are working with um mechanical
gear um you know a little bit different
if you're looking for the wobbling
statistics uh to know when to replace a
rotor on a machine or something like
that uh that can be a big deal you know
we have these huge fans that turn in our
sewage processing systems and so those
fans they start to wobble and hum and do
different things that the sensors pick
up at one point do you replace them
instead of waiting for it to break in
which case it cost a lot of money
instead of replacing a bushing you're
replacing the whole fan unit uh an
interesting project that came up for our
city a while back uh so population all
objects are measurements whose
properties are being observed uh so
that's your population all the objects
it's easy to see it with people because
we have our population and large um but
in the case of the sewer fans we're
talking about how the fan units that's
the population of fans that we're
working
with you have a parameter a matric uh
that is used to represent a population
or
characteristic you have your sample a
subset of the population studied you
don't want to do them all because then
you don't have a if you come up with a
conclusion for everyone you don't have a
way of testing it so you take a sample
uh sometimes you don't have a choice you
can only take a sample of what's going
on you can't U study the whole
population and a variable a metric of
interest for each person or object in a
population types of sampling we have
probabilistic approach uh selecting
samples from a larger population using a
method based on the theory of
probability and we'll go into a little
bit more deeper on these we have random
systematic stratified and then you have
nonprobabilistic approach selecting
samples based on the subjective Judgment
of the researcher rather than random
selection uh it has to do with
convenience trying to reach a quota um
or
snowball um and they're very biased
that's one of the reasons you'll see
this big stamp on it says biased uh so
you got to be very careful on that so
probabilistic sampling uh when we talk
about a random sampling we select random
siiz samples from each group or category
so we it's as random as you can get uh
we talk about systematic sampling we're
selecting random siiz samples from each
group or category with a fixed periodic
interval uh so we kind of split it up
this would be like a Time setup or our
different categories and you might ask
your question what is a category or
group uh if you look at I'm going to go
back of a window let's say we're
studying um economics of different of an
area um we know pretty much that based
on their culture where they came from
they might need to be separated and so
uh and when I say separated I don't mean
separated from their their uh place
where they live I mean as far as the
analysis we want to look at the
different groups and make sure they're
all represented so if we had like an 80%
uh of a group that is say Hispanic and
or Indian and also in that same area we
have 20 20% who are let's call our exp
Patriots they left America and they're
nice and uh your Caucasian group we
might want to sample a group that is
representative of both uh so we're
talking about stratified sampling and
we're talking about groups those are the
groups we're talking about and that
brings us to stratified sampling
selecting approximately equal size
samples from each group or
category uh this way we can actually
separate the categories and give us an
insight into the different cultures and
how that might affect them in that area
uh so you can see these are very very
different kind of depends on what you're
working with um as far as your data and
what you're studying and so we can see
here just to go a little bit more we'd
have selecting 25 employees from a
company of 250 employees randomly don't
care anything about them what groups are
in which office are in nothing um and we
might be selecting one employee from
every 50 unique employees in a company
of 250 employees and then we have
selecting one employee from every branch
in the company office so we have all the
different branches there's our group or
a categories by the branch and the
category could depend on what you're
studying so it has a lot of variation on
there you see this kind of grouping and
category Rising is also used to generate
a lot of
misinformation uh so if you only study
one group and you say this is what it is
then everybody assumes that's what it is
for everybody and so you got to be very
careful of that and it's very unethical
thing to kind of
do so types of Statistics uh when we
talk about statistics we're going to
talk about descriptive and inferential
statistics there are so many different
terms in statistics to break it up uh so
we so we're talking about a particular
setup so we're talking about descriptive
and inferential uh statistics you the
base of the word describe is pretty
solid you're describing the data what
does it look like with inferential
statistics we're going to take that from
the small population to a large
population so if you're working with a
drug company uh you might look at the
data and say these people were helped by
this drug they did uh 80% better as far
as their health or 80% better survival
rate than the people um who did not have
the drug so we can fer that that drug
will work in the greater populace and
will help people so that's where you get
your inferential uh so we are predicting
how it's going to affect the greater
population so descriptive statistics it
is used to describe the basic features
of data and form the basis of
quantitative analysis of data so we have
a measure of central Tendencies we have
your mean median and mode and then we
have a measure of spread like your range
your interquartile range your variance
and your standard
deviation and we're going to look at all
these a little deeper here in a second
uh but one of them you can think of is
um how the data difference differences
you know what's the max Min range all
that stuff is your spread and anything
that's just a single number is usually
your central Tendencies measure of
central Tendencies so we talk about the
mean it is the average of the set of
values considered what is the average
outcome of whatever is going on and your
median separates the higher half and the
lower half of
data uh so where's the center point of
all your different data points so your
mean might have some a couple really big
numbers that skew it uh so that the
average is much higher than if you took
those outliers out where the median
would by separating the high from the
low might give you a much lower number
you might look at it and say oh that's
that's odd why is the average so much
higher than the median
well it's because you have some outliers
or why is it so much lower and then the
mode is the most frequent appearing
value uh this is really interesting if
you're studying economics and how people
are doing you might find that the most
common um income like in the US was uh 1
24,000 a year where the average was
closer to 880,000 and it's like wow what
a difference well there's some people
have a lot of money and so that skews
that way up so the average person is not
making that kind of money and then you
look at the median income and you're
like well the median income is a little
bit closer to the average uh so it does
create a very interesting way of looking
at the data again these are all uh
Central Tendencies single numbers you
can look at for the whole spread of the
data and we look at the measure of
central Tendencies the mean is the
average marks of a students in a
classroom so here we have the mean some
of the marks of the students total
number of students and as we talked
about the median uh we have 0 through 10
and we take half the numbers and put
them on one side of the line half the
numbers on the other side of the line uh
we end up with five in the middle and
then the mode what Mark was scored by
most of the students in a test in a
simple case where most people scored
like an 82% and got certain problems
wrong easy to figure out uh not so easy
when you have different areas where like
you have like the um oh let's go back to
economy a little bit more difficult to
calculate if you have a large L group
that scores makes 30,000 and a slightly
bigger group that makes 26,000 so what
do you put down for the mode uh
certainly there's a number of ways to
calculate that and there's actually a
different variations depending on what
you're doing so now we're looking at a
measure of spread uh range what's the
difference between the highest and the
lowest value first thing you want to
look at you know it's uh we had
everybody in the test scored between 60
and 100% somebody got 100% or maybe 60
to 90% it was so hard that a lot of
people could not get 100
% um you have your enter quartile range
quartiles divide a rank ordered data set
into four equal
parts very common thing to do as part of
all the basic packages whether you're
working in uh data frames with pandas
whether you're working in Scala whether
you're working in R um you'll see this
come up where they have range your men
your Max and then it'll have your inter
quartile range how does it look like in
each quarter of data VAR
measures how far each number in the set
is from the mean and therefore from
every other number in the
Set uh so you have like a how much
turbulence is going on in this
data and then the standard deviation it
is to measure the variance or the
dispersion of a set of values from the
mean and you'll usually see uh if I'm
doing a graph I might have the value
graphed um and then based on the the
error I might gra graph the standard
deviation in the error on the graph
graph as a background so you can see how
far off it is uh so standard deviation
is used a lot so measurement of spread
uh marks of a student out of 100 uh we
have here from 50 to 63 or 50 to 90 uh
so the range maximum marks minimum marks
we have 90 to 45 and the spread of that
is 45 90 minus 45 and then we have the
interquartile range using the same marks
over there you can see here where the
median is and then there's the first
quarter the second quarter and the third
quarter based on splitting it apart by
those values and to understand the
variance and standard deviation we first
need to find out the mean uh so here's
our our you know calculating the average
there we end up at approximately 66 for
the average and then we look at that in
the variance once we know the means we
can do equals the marks minus the mean
squared Y is a squared uh because one
you want to make sure it's you don't
have like if you if you're putting all
this stuff together you end up with an
error as far as one's negative one's
positive one's a little higher one's a
little lower uh so you always see the
squared value and over the total
observations and so the standard
deviation equals the square root of the
variance which is approximately 16 and
if you were looking at um a predictable
model you would be looking at the
deviation based on the error how much
error does it have uh that's again
really important to know if your if your
prediction is predicting something
what's a chance of it being way off or
just a little bit
off now that we've looked at the um
tools as far as some of the basics for
doing your statistics and what we're
talking about let's go ahead and pull up
a little demo and show you what that
looks like in Python code uh so you can
get some little handson here for that
let's go back into our Jupiter notebook
in Python now almost all of this you can
do in numpy last time we worked um in
numpy this time we're going to go ahead
and use pandas and if you remember from
pandas on here uh this is basically a
data frame rows columns let's just go
ahead and do a print
dfad and run
that and you can see we have uh the name
Jane Michael William Rosie Hannah and
their salaries on here and of course
instead of having to do all those hand
calculations and add everything together
and divide by the total we can do
something very simple on this uh like
use the command mean in pandas and so if
I go ahe and do this print DF pick our
column salary because we want to find
the means of that
callary we want to find the means of
that column uh and we go and print this
out and you can see that the uh average
income on here is
71,000 uh and let's just go ahead and do
this we'll go ahead and put in uh
means and if we if we're going to do
that we also might want to find the
median and the median is uh very
similar except it actually is just
median uh we're used to means and
average it's kind of interesting that
those are they use the two different
words uh there can be in some
computation slight differences but for
the most part the means is the average
uh and then the median oops let's put
a median here
DF salary that way it displays a little
better we can see the median is 54 ,000
so the halfway mark is significantly
below the average why because we have
somebody in here who makes
189,000 darn you Rosy for throwing off
our numbers um but that's something You'
want to notice this is this is the
difference between these is huge and so
is what is the meaning behind that when
you're studying a populace and looking
at uh the different data coming in and
of course we also want to find out hey
what's the most uh common income that
people make in this little tiny sample
and so we'll go ahead and do the mode
and you can see here with the mode uh
it's at
50,000 so this is this is very telling
that most people are making
50,000 the middle point is at 54,000 so
half the people are making more than
that what that tells me is that if the
most common income is way is below the
median then there's a few there's a you
know there's a lot of high salaries
going up but there's some really low
salaries in there and so this trend
which is very common in statistic you
when you're analyzing the economy and
different people's income is pretty
common and the bigger difference between
these is also very important when we're
studying statistics uh and when you hear
someone just say hey the average income
was you might start asking questions at
that point why aren't you talking about
the median income why aren't you talking
about the mode the most common income
what are you hiding uh and if you're
doing these analysis you should be
looking at these saying hey why why are
this discrepancies why are these so
different and of course with any uh
analysis it's important to find out the
minimum and the maximum so we'll go
ahead it it's just simply uh um Min will
pull up your minimum and then Max pulls
up the maximum pretty straightforward on
as far as um translating it knowing what
your you know what the your lowest value
and what your highest value is here um
which you'll use to generate like a
spread later on and real quick on no
mode uh note that it puts mode zero like
I said there's a couple different ways
you can compute the mode um although you
know standard one's pretty good we can
of course do the range which is your max
minus your Min so now we have a range of
149,000 between the upper end and the
lower end and you might want to be
looking up the individual values on all
of these but turns out there is a
describe feature in
pandas and so in pandas we can actually
do DF salary describe and if we do this
you can see we have that there's seven
uh setups here's our mean um our
standard deviation which we didn't
compute yet which would just be a STD
and you got to be a little careful
because when it computes it it looks for
axes and things like that uh we have our
minimum value and here's our
quartiles uh our maximum value and then
of course the name salary uh so these
are these are the basic statistics you
can pull them up and like just describe
this is a dictionary so I could actually
do something like um in here I could
actually go uh count and run and now it
just prints the count uh so because this
is a dictionary you can pull any one of
these values out of here it's kind of a
quick and dirty way to pull all the
different information and then split it
up depending on what you need now if I
just walked in and gave you this
information um in a meeting at some
point you would just kind of fall asleep
that's what I would do anyway um so we
want to go ahead and and see about
graphing it here and we'll go ahead and
put it into a history gram and plot that
graph on it of the salaries and let's
just go ahead and put that in here so we
do our map plot inline remember that's a
Jupiter's notebook thing uh a lot of the
new version of the map plot Library does
it automatically but just in case I
always put it in there uh import matplot
Library pip plot is PLT that's my
plotting and then we have our data frame
uh I don't I guess I really don't need
to respell the data frame maybe we could
just remind ourself what's in it so
we'll go ahead and just uh
print DF that way we still have it and
then we have our salary DF salary
salary. plot history title salary
distribution color gray
uh plot axv line salary the mean value
so we're going to take the mean value um
color violet line style Dash this is
just all making it pretty uh what color
dash line line width of two that kind of
thing and the median and let's go ahead
and run this just so you can see what
we're talking
about and so up here we are taking on
our
plot um so here's the data here's our
our data frame printed out so you can
see see it with the salaries we're look
at the salary distribution and just look
at this the way the the salary is
distributed um you have our u in this
case we did Let's see we had red for the
median we have
Violet for our average or mean and you
can just see how it really here's our
outlier here's our person who makes a
lot of money here's the um average and
here's a median um and so you look at
this you can say wow um based on the
average it really doesn't tell you much
about what people are really taking home
all it does is tell you how much money
is in this you know what the average
salary is so some of the things you want
to take away in addition to this is that
it's very easy to plot um an axv line
these are these up and down lines for
your markers um and as you just display
the data I mean you can add all kinds of
things to this and get really
complicated keeping it simple is pretty
straightforward I look at this and I can
see we have a major outlier out here we
can definitely do a histogram and stuff
like that um but you know picture is
worth a thousand words what you really
want to make sure you take away is that
we can do a basic describe which pulls
all this information out and we can
print any of the individual information
from the describe uh because this is a
dictionary and so if we want to go ahead
and look up um the me value we can also
do describe mean so if you're doing a
lot of Statistics uh being able
to doesn't have the print on there so
it's only going to print um the last one
which happens to be the mean uh you can
very easily reference any one of these
and then you can also if you're doing
something a little bit more complicated
and you don't need just the basics you
can come through and pull any one of the
individual
um references from the from the pandas
on here so now we've had a chance to
describe our data uh let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you can see here we have a nice little
picture movie ratings and um if we took
this group of people and said hey how
many people like the movie dislike it
can't say and then you ask just a random
person who comes out of the movie who
hasn't been in this study uh you can
infer that 55% chance of saying liked
35% chance of saying disliked or a 10 or
11% chance of can't say so that's real
basics of what we're talking about is
you're going to infer that the next
person is going to follow these
statistics uh so let's look at Point
estimation uh it is a process of finding
an approximate value for a population's
parameter like mean or average from
random samples of the population let's
take an example of testing vaccines for
coid 19 uh vaccines and flu bugs all
that it's a pretty big thing of how do
you test these out and make sure they're
going to work on the populace a group of
people are chosen from the population
medical trials are performed results are
generalized for the whole population so
here's our protected here's our small
group up here where we've selected them
we run medical trials on them and then
the results work for the population you
nice diagram with the arrows going back
and forth and the very scary coid virus
in the middle of one and let's take a
look at the applications of inferential
statistics very Central is what they
call hypothesis
testing uh and the confidence interval
which go with that and then as we get
into probability we get into our
binomial theorem our normal distribution
and Central limit theorem hypothesis
testing hypothesis testing is used to
measure the plausibility of a hypothesis
assumption by using sample data
now when we talk about theorems Theory
hypothesis uh keep in mind that if you
are in a philosophy class theory is the
same as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things up to debate so hypothesis is the
same as a Phil philosophical class
calling a theory where theory in science
is not the same theory in science has
this has been well proven gravity is a
theory uh so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collap collapsing in
your area that is a philosophical debate
very important I've heard people mix
those up and it is a pet peeve of mine
when we talk about hypothesis testing
the steps involved in hypothesis testing
is first we formulate a hypothesis we
fig figure out the right test to test
our hypothesis we execute the test and
we make a decision and so when you're
talking about hypothesis you usually
trying to disprove it if you can't
disprove it and it works for all the
facts then you might call that a theorem
at some point so in a use case uh let's
consider an example we have four
students we're given a task to clean a
room every day sounds like working with
my kids they decided to distribute the
job of cleaning the room among
themselves they did so by making four
chits which has their names on it and
the name that gets picked up has to do
the cleaning for that day Rob took the
opportunity to make chits and wrote
everyone's name on it so here's our four
people Nick Rob emia imia and
summer now Rick Emilia and summer are
asking us to decide whether Rob has done
some Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits so
the probability of Rob not doing work on
day one is uh three out of four there's
a 75 chance that he didn't do work uh
two days 34s time 34 equal.
56 3 days you have 34 34 34 which equals
point4 2 uh when you get to day 12 it's
0.032 Which is less than
0.05 remember this 05 uh that comes up a
lot when we're talking about um certain
values when we're looking at statistics
Rob is cheating as he wasn't chosen for
12 consecutive days that's a very high
probability when on day 12 he still
hasn't gotten the job cleaning the
room so we come up to our important
important terminologies
we have null
hypothesis a general statement that
states that there is no relationship
between two measured phenomenon or no
ass Association among the groups
alternative hypothesis contrary to the
null hypothesis it states whenever
something is happening a new theory is
preferred instead of an old one and so
the two hypotheses go hand in hand uh so
your null this is always interesting in
in we're talking about data science and
the math behind it it's about proving
that the things have no correlation null
hypothesis says these two have zero
relation to each other where the
alternative hypothesis says hey we found
a relation this is what it is we have P
value the P value is a probability of
finding the observed or more extreme
results when the null hypothesis of a
study question is true and the T value
it is simply the calculated difference
represented in units of standard error
the greater the magnitude of T the
greater the evidence against the null
hypothesis and you can look at the T
values being specific to the test you're
doing where the P value is derived from
your T value and you're looking for what
they call the 5% or the
0.05 showing that it has a high
correlation so digging in deeper let's
assume that a new drug is developed with
the goal of lowering the blood pressure
more than the existing drug and this is
a good one because uh the null value
here isn't that you don't have any drug
the null value value here is it is
better than the existing drug the new
drug doesn't lower the blood pressure
more than the existing drug now if we
get that uh that says our null
hypothesis is correct there is no
correlation and the new drug is not
doing its job the alternative hypothesis
the new drug does significantly lower
the blood pressure more than the
existing drug uh yay we got a new drug
out there and that's our alternative
hypothesis or the H1 or
ha and we look at the T value results
from the evidence like medical trials
showing positive results which will
reject the null
hypothesis and again they're looking for
um a 0.05 or 5% and the T value
comparing all the positive test results
and finding means of different samples
in order to test hypothesis so this is
specific to the test how uh what
percentage of increase did they have and
this leads us to the confidence
intervals uh a confidence interval is a
range of values we are sure our true
values of observations lie in let's say
you asked a dog owner around you and
asked them how many cans of food do you
buy for your uh per year for your dog
through calculations you got to know
that the on an average around 95% of the
people bought around 200 to 300 cans of
food hence we can say that we have a
confidence interval of
2300 where 95% of our values lie in that
data spread uh and this the graph really
helps a lot so you can start seeing what
you're looking at here where you have
the 95% you have your peak in this case
it's a normal distribution so you have
the nice bell curve equal on both sides
it's not asymmetrical and 95% of all the
values lie within a very small range and
then you have your outliers the 2.5%
going each way so we touched upon
hypothesis uh we're going to move into
probability uh so you have your
hypothesis once you've generated your
hypothesis we want to know the
probability of something occurring
probability is a measure of the
likelihood of an event to occur any
event can be predicted with total
certainty and can only be predicted as a
likelihood of its occurrence so any
event cannot be predicted with total
certainty it can only be predicted as a
likelihood of its occurrence uh score
prediction how good you're going to do
in whatever U sport you're in weather
prediction stock prediction if you've
studied physics and Chaos Theory even
the location of the chair your on has a
probability that it might move 3 ft over
granted that probability is one in like
uh I think we calculated as under one in
trillions upon trillions so it's the
better the probability the more likely
it's going to happen there are some
things that have such a low probability
that we don't see them so we talk about
a random variable uh random variable is
a variable whose possible values are
numerical outcomes of a random phenomena
so uh we have the coin toss how many
heads will occur in this a series of 20
coin flips probably you know the on
average they're 10 but you really can't
know cuz it's very random how many times
are red ball is picked from a bag of
balls if there's equal number of of red
balls and blue balls and green balls in
there how many times do sum of digits on
two dice uh result are five each um so
you know there's how often are you going
to roll two fives on your PAAD di so in
a use case uh let's consider the example
of rolling two dice we have a random
variable outcome equals y you can take
values 2 3 4 5 6 7 8 9 10 11 12 so we
have a random variable and a combination
of dice and instead of looking at how
many times um both dice were roll five
let's go ahead and look at a total sum
of five and you have in as far as your
random variables you can have 1 4al 5 4
1 2 3 32 so four of those rols can be
four if you look at all the different
options you have four of those random
rols can be a five and if we look at the
total
number which happens to be 36 different
options uh you can see that we have four
out of 36 chance every time you roll the
dice that you're going to roll a total
of five you're going to have an outcome
of five and uh we'll look a little
deeper as to what that means uh but you
could think of that at what point if
someone never rolls a five or they
always roll a five can you say hey that
person's probably cheating uh we'll look
a little closer at the behind that but
let's just consider this is one of the
cases is rolling two dice and gambling
there's also a binomial distribution it
is a probability of getting success or
failure as an outcome in an experiment
or trial that is repeated multiple times
and the key is is by meaning two
binomial uh so passing or failing an
exam winning or losing a game getting
either head or tells so if you ever see
binomial distribution is based on a um
true false kind of setup you win or lose
let's consider a uh use case and let's
consider the game of football between
two clubs Barcelona and Dortmund the
teams will have to play a total of four
matches and we have to find out the
chances of Barcelona winning the series
so we look at the total games and we're
looking at five different games or
matches let's say that the winning
chance for Barcelona is 75% or 75 that
means at each game they have a 7 5%
chance that they're going to win that
game and losing chances are 25% or .25
clearly 75 plus. 25 equals 1 so that
accounts for 100% of the game
probability for getting K wins in in
matches is
calculated and we we're talking like so
if you have five games uh and you want
to know if I play um how many wins in
those five games should I get what's a
percentage on those and the probability
for getting K wins and in matches is
calculated by PX = k = n c k p to the k
q to the N minus K here p is the
probability of success and Q is the
probability of failure and so we can do
total games of Nal 5 where k equals 0 1
2 3 4 5 P which is the chance of winning
is 75 Q the chance of losing equal 1
minus P which equal 1 - 075 which equal
.25 five the probability that Barcelona
will lose all of the matches can then
just plug in the numbers and we end up
with a
0
9765 625 so very small chance they're
going to lose all their
matches and we can plug in uh the value
for two matches probability that
Barcelona will win at least two matches
is
0878 and of course we can go on to
probability that Barcelona will win
three matches the 2 six and of course
four matches and so on and it's always
nice to take this information um and
let's find the cumulated discrete
probabilities for each of the outcomes
where Barcelona has won three or more
matches xal 3 xal 4 xal 5 and we end up
with the p equal. 264 plus 395 + 237
which equals
89 in reality the probability of
Barcelona winning the series is much
higher than 7 five and it's always nice
to uh put out a nice graph so you can
actually see the number of winds to the
probability and how that panss out with
our binomial case continuing in our
important terminology location the
location of the center of the graph
depends on the mean value and uh this is
some very important things so much of
the data we look at and when you start
looking at probabilities almost always
has a normalized look like the graph in
the middle
uh but you do have left skewed where the
data is skewed off to the left and you
have more stuff happening off to the
left and you have right skewed data and
so when this comes up and these
probabilities come up where they're
skewed it's really important to take a
closer look at that uh mostly you end up
with a normalized set of data but you
got to also be aware that sometimes it's
a skewed data and then the height height
of the slope inversely depends upon the
standard
deviation so you can see down here the
standard deviation is really large it
kind of pushes it out and if the
standard deviation is small then most of
your data is going to hit right there in
the middle you going to have a nice Peak
um and so being aware of this that you
might have a probability that fits
certain data but it has a lot of
outliers so you're if you have a really
high standard
deviation um if you're doing stock
market
analysis this means your predictions are
probably not going to make you much
money uh where if you're have a very
small deviation you might be right on
Target and set to become a millionaire
which lead leads us to the zcore zcore
tells you how far from the mean a data
point is it is measured in terms of
standard deviations from the mean around
68% of the results are found between one
standard deviation around 95% of the
results are found between two standard
deviations and you read the symbols of
course they love to throw some Greek
letters in there we have uh mu minus 2
Sigma mu is just a quick way it's that
kind of funky U it just means means the
mean uh and then the sigma is a standard
deviation and that's the o with the
little arrow off to the right or the
little waggly Tail Going up the o with a
with a line on it uh so muus 2 Sigma is
your uh 95% of the results are found
between two standard
deviations Central limit theorem this
goes back to the skew if you remember we
were looking at the skew values on this
previous slide have left skewed Norm
normalized and right skewed when we're
talking about it being skewed or not
skewed the distribution of the sample
means will be approximately normally
distributed evenly distributed not
skewed if you take large random samples
from the population with the mean mu and
the standard deviation Sigma with
replacement and you can see here um uh
of course we have our uh mu minus 2
Sigma and the spread down here the mean
the median and the mode and so when
you're talking about very large
populations these numbers should come
together and you shouldn't have a skewed
value if you do that's a flag that
something's wrong that's why this is so
important to be aware of what's going on
with your data where your samples are
coming from and the math behind it and
if we're going to do all this we got to
jump into conditional probability the
conditional probability of an event a is
a probability that the event will occur
given the knowledge that an EV event B
has already occurred and you'll see this
as baze theorem B A ye s Bay uh and this
is red I mean you have these funky
looking little p brackets AB this is the
probability of a being true while B is
already
true and you have the probability of B
being true when a is already true so p b
of a probability of a being true divided
by the probability of B being true
and we talk about B's theorem which
occurred back in the 1800s when he
discovered this this is such an
important formula and it's really it's
not if you actually do the math you
could just kind of do um um XY equal JK
and then you divide them out and you're
going to see the same math but it works
with probabilities which makes it really
nice and so if you have a you might have
uh eight or nine different studies going
on in different areas different people
have done the study they brought them
together um if we look at today's Co
virus the virus spread uh certainly the
studies done in China versus the studies
the way they're done in the US that data
is different in each of those studies
but if you can find a place where it
overlaps where they're studying the same
thing together you can then compute the
changes that you need to make in one
study to make them equal and this is
also true if you have a study of uh um
one group and you want to find out more
about it so this formula is very
powerful uh it really has to do with the
data collection part of the math and
data science and understanding where
your data is coming from and how you're
going to combine different studies in
different
groups and we'll go and go into a use
case uh let's find out the chance of a
person getting lung disease due to
smoking uh and this is kind of
interesting the way they word this um
let's say that according to medical
report provided by the hospital states
that around 10% of all patients they
treated suffered lung lung disease uh so
we have kind of a generic medical report
they further found out uh by a survey
that 15% of the patients that visit them
smoke so we have 10% that are lung
disease and um 15% of the patients smoke
and finally 5% of the people continued
smoke even when they had lung disease uh
not the brightest Choice um but you know
it is an addiction so it can be really
difficult to kick and so we can look at
the probability of a uh prior
probability of 10% people having lung
disease and then probability B
probability that a patient smokes is
15% uh and the probability of b um if B
then a the probability of a patient
smokes even though they have lung
disease is
5% and probability of a is B probability
that the patient will have lung disease
if they smoke and and then when you put
the formulas together uh you get a nice
solution here you get the probability of
a of probability that the patient will
have lung disease if they smoke and you
can just plug the numbers right in and
we get a 3.33% chance hence there is a
3.33% chance that a person who smokes
will get a lung disease so we're going
to pull up a little python code always
my favorite roll up the sleeves keep in
mind we're going to be doing this um
kind of like the backend way so that you
can see what's going on and then later
on we're going to create um we'll get
into another demo which shows you some
of the tools that are already pre-built
for this let's start by creating a set
so we're going to create a set with
curly braces this means that our set has
um only unique values so you have a list
uh you have your tupal which can never
change and then you have um in this case
the the set so 47 you can't create a 47
comma four it'll delete the four out so
it's only unique values and if you use
dictionaries quick reminder this should
look familiar because it is a dictionary
uh where you have a value and that value
is assigned to or that key is assigned
to a value uh so you could have a key
value set up as a dictionary so it's
like a dictionary without the value it's
just the keys and they all have to be
unique
and if we run this we have a set of
47 we can also take a list a regular um
setup and I'm going to go ahead and just
throw in another number in here four and
run it uh and you can see here if I take
my list 1 2 3 4 4 and I convert it to a
set and here it is my set from list
equals set my
list the result is 1 2 3 4 so it just
deletes that last four right out out of
there and with the sets you can also go
in there and um print here is my set my
Set uh three is in the set and then if
you do three in my
set that's going to be a logic
function uh and one in my set six is not
in the set and so forth if we run
this we get uh three is in the set true
one is in the set false cuz 357 is
another one six is in the Set uh six is
not in the set so not in my set you can
also use this with the list we could
have just used 357 and it would have
have um the same response on there is
three and usually do if three is in but
three in my set is still works on a just
a regular list and we'll go ahead and do
a little iteration we're going to do
kind of the dice one remember um uh 1 2
3 4 5 6 and so we're going to bring in
an iteration tool tool and import
product as
product and uh I'll show you what that
means in just a second so we have our
two dice we have dice a and it's going
to be a set of values um you can only
have one value for each one that's why
they put it in a set and if you remember
from range it is up to seven so this is
going to be 1 2 3 4 5 six it will not
include the seven and the same thing for
our dice
B and then we're going to do is we're
going to create
list which is the product of A and B so
what's um a plus b and if we go ahead
and run this uh it'll print that out and
you'll see um in this case when they say
product because it's an iteration
tool we're talking about creating a
tuple of the two so we've now created a
tupal of all possible outcomes of the
dice where dice a is 1 2 3 1 to six and
dice B is 1 to six and you can see 1: 1
1: two one to three and so forth you
remember we had a slide on this earlier
where we talked about um the different
all the different outcomes of a dice we
can play around with this a little bit
uh we can do in dice equals two Divi
dice faces 1 2 3 4 5
six uh another way of doing what we did
before and then we can create an event
space where we have a set which is the
product of the dice faces repeat equals
indise and we'll go and just run this
and you can see here it just again puts
it through all the different possible
variables we can have and then if we
wanted to take the same uh set on here
and print them all out like we had
before uh we can just go through for
outcome and event space outcome and
equals so the event space is
creating uh sequence and as you can see
here when we print it out it Stacks them
versus going through and putting them in
a nice
line and we'll go ahead and do something
um let's go print since we have the End
Printing with a comma that just means
it's just going to it's not going to hit
the return going down to the next line
uh and we'll go ahead and do the
length of our event space uh that'll be
an important variable we're going to
want to know in a
minute and of course if I get carried
away with my typing of length uh we'll
print it twice and it'll give me an
error uh so we have 36 different
possible variations
here and we might want to calculate
something like um what about the
multiple of three what if we want to
have uh the probability of the multiple
of three in our
setup and so uh we can put together the
code for the outcome in event space of X
Y equals outcome if x +
y remainder three so we're going to
divide by three and look at the
remainder and it equals
zero
then it's a fabral outcome and we're
going to pop that outcome on the end
there and we'll turn it into a set so
the favor alcome equals a set not
necessary uh because we know it's not
going to be repeating itself but just in
case we'll go ahead and do
that and if we want to print out the
outcome we can go ahead and see what
that looks like and you can see here
these are all U multiples of three uh 1
+ 2 is 3 5 + 4 is which divided by 3 is
three and so
forth and just like we looked up the
length uh of the one before let's go
ahead and print the
length of our um f outcome so we can see
what that looks
like there we
go and of course I did forget to add the
print in the middle cuz We're looping
through and putting an end on the on the
setup on there so we're going to put the
print in there here and if I run this
you can see uh
um we end up with 12 so we have 36 total
options uh we have 12 that are multiple
that um add up to a multiple of
three and we can easily conver compute
the probability of this uh by simply
taking the length of our favorable
outcome over the length of the event
space and if we print print it out let
me put that in there
probability last line so we just type it
in we end up with a 3333 chance that's
roughly a
third and we might want to make this
look nice so let's go ahead and put in
another line there the probability of
getting the sum which is a multiple of
three
is
3333 we can compute the same thing for
five
dice and if we do this for five dice and
go and run it U you can see we just have
a huge amount of choices uh so it just
goes on and on down here and we can look
at the uh length of the event
space and we have over
7,776 choices that's a lot of
choices and if we want to ask the
question like we did above uh what is
the where the sum is a multiple of five
but not a multiple of three we can go
through all of these different options
and then uh you can see here D1 D2 D3 D4
D5 equals the outcome and if uh you add
these all together and
the division by five does not have a
remainder of zero but the remainder is
also of a division by three is not equal
to zero so the multiple of five is equal
to zero but the multiple three is not we
can just appin that on here and then we
can look at that uh favorable outcome
we'll go ahead and set that we'll just
take a look at this what's our length of
our favorable
outcome it's always good to see what
we're working with and so we have 904
out of
776 and then of course we can just do a
simple division to get the probability
on here what's the probability that
we're going to roll a multiple of five
when you add them
together but not a multiple of three and
so we're just going to divide those two
numbers and you can see here we get .11
16255 or
11.62% and so you can really have a nice
visual that this is not really
complicated math right here on
probabilities uh it's just how many
options do you have and and how many of
those are you possibly going to be able
to um come up with with the solution
you're looking for and this leads us to
a confusion Matrix a confusion Matrix is
a table which is used to describe the
performance of a classification model on
a set of test data for which the True
Values are known and so you'll see in
the left we have the predicted and the
actual and we have a negative uh false
negative positive true positive um and
then we have false positive and true
negative and you can think of this as
your predicted model what does that mean
that means if you divided your data and
you used two3 of it to create the model
you might then test it against an actual
case for the last third to see how well
it comes out how many times was it uh
true positive versus uh false positive I
gave it false positive response and you
can imagine in medical uh situations
this is a pretty big deal you don't want
to give a false positive so you might
adjust your model accordingly so you
don't have a false positive say with the
co virus test it'd be better to have a
false negative and they go back and get
retested than to have 30% false
positives where then the test is pretty
much invalid so in a Ed Case uh like
cancer prediction let's consider an
example where a cancer prediction model
is put to the test for its accuracy and
precision actual result of a person's
medical report is compared with the
prediction made by the machine learning
model and so you can see here here's our
actual predicted uh whether they have
cancer or not you know cancer a big one
you don't want to have a u false
positive I mean a false negative in
other words you don't want to have it
tell you that you don't have cancer when
you do so that would be something you'd
really be looking for in this particular
domain you don't want a false
negative uh and this is again you know
you've created a model you have hundreds
of people or thousands of pieces of data
that come men there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many um negative results
do you have that are either telling
people they have cancer that don't or
telling people that don't have cancer
that they do and then we can take these
numbers and we can feed them into our
accuracy our precision and our
recall uh so accuracy precision and
recall accuracy metric to measure how
accurately the results are predicted and
this is your um total um true where you
got the right results you add them
together the true positive the true
negative over all the results so what
percentage of them were accurate versus
what were
wrong we talk about Precision is a
metric to measure how many of the
correctly predicted cases are actually
turned out to be
positive uh so we have a Precision on
true positive again if you're talking
about like uh coid testing with the
viruses uh you really want this to be a
a high number you want this true um that
to be the center point where you might
have the opposite if you're dealing with
cancer where you want no false
negatives uh so this is your metric on
here Precision is your test positive uh
true positive plus uh false positive and
then your recall how many of the actual
positive cases we were able to predict
quickly with our model
uh so test positive is the test positive
plus the false negative on there and
before starting if you are one of the
aspiring machine learning Enthusiast
looking for online training and
graduating from the best universities or
a professional who assits to switch
careers in machine learning by learning
from the experts then try giving a short
to Simply learn postgraduate program in
Ai and machine learning in collaboration
with berdu University and IBM the course
link is mentioned in the description box
below that will navigate you to the
course page where you can find a
complete overview of the program being
offered and one more announcement that
is if you want to be a ml Enthusiast
looking for online training and
graduating from the best universities
then we have a simply learn skel Tech a
ml boot camp and the boot camp link is
also mentioned in the description box
below that will navigate you to the Boot
Camp Page and you can find a complete
overview of the program being offered
what are the different types of machine
learning algorithms machine learning
algorithms are broadly classified into
three types the supervised learning
unsupervised learning and reinforcement
learning supervised learning in turn
consists of techniques like regression
and classification and unsupervised
learning we use techniques like
Association and clustering and
reinforcement learning is a recently
developed technique and it is very
popular in gaming some of you must have
heard about alphago so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning
we do not have past labeled data
historical labeled data so we use
techniques like Association and
clustering to maybe form clusters or new
classes maybe and then we move from
there in case of reinforcement learning
the system learns pretty much from
scratch there is an agent and there is
an environment the agent is given a
certain Target and it is rewarded when
it is moving towards that Target and it
it is penalized if it is moving in a
direction which is not achieving that
Target so it's more like a carrot and
stick model so what is the difference
between these three types of algorithms
supervised algorithms or supervised
learning algorithms are used when you
have a specific Target value that you
would like to predict the target could
be categorical having two or more
possible outcomes or classes if you will
that is what is classification or the
target could be a a value which can be
measured and that's where we use
regression like for example whether
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or this is known as labeled
information if you have the labeled
information then you use supervised
learning in case of unsupervised
learning we have input data but we don't
have the labels or what the output is
supposed to be so that is when we use
unsupervised learning techniques like
clustering and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum that's an example of
reinforcement learning when we look at a
different machine learning algorithms we
can divide them into three areas
supervised
unsupervised reinforcement we we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and class ification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polinomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like like
how much food do we give our crops when
do we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 - y1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at at that we put the
red point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some python script
that this math that you're going to see
here is already done automatically for
you you don't have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was one
we got Y = 2 when X was uh 2 y was 4 and
so on and so on if we go ahead and plot
this data on a graph we can see how it
forms a nice line through the middle you
can see where it's kind of grouped going
upwards to the right the next thing we
want to know is what the means is of
each of the data coming in the X and the
Y the means doesn't mean anything other
than the average so we add up all the
numbers and divide by the total so 1+ 2+
3+ 4 + 5 over 5 = 3 and the same for y
we get four if we go ahead and plot the
means on the graph we'll see we get 3
comma 4 which draws a nice line down the
middle a good estimate here we're going
to dig deeper into the math behind the
regression line now remember before I
said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x * Y and then it
takes all of X and adds them up all of Y
adds them up all of X squ adds them up
and so on and so on and you can see we
have the sum of equal to 15 the sum is
equal to 20 all the way up to x * Y
where the sum equals 66 this all comes
from our formula for calculating a
straight line where y equals the slope *
X plus the coefficient C so we go down
below and we're going to compute more
like the averages of these and we're
going to explain exactly what that is in
just a minute and where that information
comes from is called the square means
error but we'll go into that in detail
in a few minutes all you need to do is
look at the formula and see how we've
gone about Computing it line by line
instead of trying to have a huge set of
numbers pushed into it and down here
you'll see where the slope m equals and
on the top part if you read through the
brackets you have the number of data
points times the sum of x * Y which we
computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X2 easily computed as 86 for
the sum minus I'll take all that and
subtract the sum of x^2 and we end up as
we come across with our formula you can
plug in all those numbers which is very
easy to do on the computer you don't
have to do the math on a piece of paper
or calculator and you'll get a slope of
6 and you'll get your C coefficient if
you continue to follow through that
formula you'll see it comes out as equal
to 2.2 continuing deeper into what's
going behind the scenes let's find out
the predicted Val values of Y for
corresponding values of X using the
linear equation where M = 6 and C = 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y = 6 *
or x = 1+ 2.2 = 2.8 so on and so on and
here the Blue Points represent the
actual y values and the brown points
represent the predicted y values based
on the model we created the distance
between the actual and predicted values
is known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as e
squ if we put these into a nice chart
where you can see X and you can see Y
where we actual values were and you can
see y predicted you can easily see where
we take Yus y predicted and we get an
answer what is the difference between
those two and if we square that y- y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2.4 we check this
error for each line and conclude the
best fit line having the least e Square
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square distance between the data
points and the regression line now we
only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc what you want to take away from this
is whatever formula is being used you
can easily using a computer programming
and iterating through the data calculate
the different parts of it that way these
complicated formulas you see with the
different summations and absolute values
are easily computed one piece at a time
up until this point we've only been
looking at two values X and Y well in
the real world it's very rare that you
only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look at
what happens when you have multiple
inputs so in multiple linear regression
we have uh well we'll start with the
simple linear regression where we had y
= m + x + C and we're trying to find the
value of y now with multiple linear
regression we have multiple variables
coming in so instead of having just X we
have X1 X2 X3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M1 M2 M3 and we still just have
the single coefficient so when you're
dealing with multiple linear regression
you basically take your single linear
regression and you spread it out so you
have y = M1 * X1 + M2 * X2 so on all the
way to m to the n x to the n and then
you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R&D trying to figure out its profit
we're going to start looking at the
expenditure of the company we're going
to go back to that we're going to
predict his profit but instead of
predicting it just on the R&D D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it CU it's such a
visual to look at and so easy to use um
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the nump which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import matplot
library. pyplot as PLT and then caborn
as SNS caborn works with the matplot
library so you have to always import
matplot library and then caborn sits on
top of it and we'll take a look at what
that looks like you could use any of
your own plotting libraries you want
there's all kinds of ways to look at the
data these are just very common ones and
the caborn is is so easy to use it just
looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign map plot Library
inline that is only because I'm doing an
inline IDE my interface in the Anaconda
Jupiter notebook requires I put that in
there or you're not going to see the
graph when it comes up let's go ahead
and run this it's not going to be that
interesting because we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setups the next step is load the data
set and extract independent and
dependent variables now here in the
slide you'll see companies equals pd.
read CSV and it has a long line there
with the file at the end 1,000
companies. CSV you're going to have to
change this to fit whatever setup you
have and the file itself you can request
just go down to the commentary below
this video and put a note in there and
simply learn we'll try to get in contact
with you and Supply you with that file
so you can try this coding yourself so
we're going to add this code in here and
we're going to see that I have companies
equals pd. reader CSV and I've change
this path to match my computer c/s
simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the ication and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a panda data set
but we can easily extract the data from
our pandas data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spin
Administration marketing spend State
profit it even notes that the top are
call colum names it was never told that
but pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of of
19226 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
uh notebook and see what we got going
and so we have the visualization and
again we're using SNS which is pandas as
you can see we imported the M plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file and
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code cbor is great
because it actually recognizes the panda
data frame so I can just take the
companies core for coordinates and I can
put that right into the Seaborn and when
we run this we get this beautiful plot
and let's just take a look at what this
plot means if you look at this plot on
mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns in
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard hard data
some hard values so let's see what that
looks like we're going to set up our
linear regression model in two steps the
first one is we need to prepare some of
our data so it fits correctly and let's
go ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E en coder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we we have avoiding dummy variable
trap this is very important why because
the computer is automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x = x
colon comma 1 col all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label en coding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we be very unethical
without testing it to see how good our
fit is and then we'll go ahead and
create our multiple linear regression
model and train it and set it up let's
go ahead and paste this next piece of
code in here and I'll go ahead and
shrink it down a Siz or two so it all
fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y y test that is the
standard way that they usually referenes
when we're doing different uh models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our R&D spin our Administration
or marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals 0.2 that basically means 20% So
20% of the rows are going to be tested
we're going to put them off to the side
so since we're using a th lines of data
that means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ah and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good luck and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
where we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor do fet in this case we do
XT train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit inter equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different ID that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do the
regressor intercept and let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y = M1 * X1 + M2 * X2 and so
on and so on plus C the coefficient so
these variables fit right into this
formula y = slope 1 * col colum 1
variable plus slope 2 * column 2
variable all the way to the m into the n
and x to the N plus C the coefficient or
in this case you have -
8.89 to the^ of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus1
0309 point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us so we know are true
the Y predict of those 200 values is
what we think it was true and when we go
ahead and run this we see we get a.
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squ value of 91 or 92 as we got on our
model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model all right what is logistic
regression as mentioned earlier logistic
regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if it it's going to
break down in the near future so this is
like a classification problem find out
whether your car will break down or not
so how are we going to perform this
classification so here's how it looks if
we plot the information
along the X and Y AIS X is the number of
years since the last service was
performed and Y is the probability of
your car breaking down and let's say
this information was this data rather
was collected from several car users
it's not just your car but several car
users so that is our labeled data so the
data has been collected and um for for
the number of years and when the car
broke down and what was the probability
and that has been plotted along X and Y
AIS so this provides an idea or from
this graph we can find out whether your
car will break down or not we'll see how
so first of all the probability can go
from 0 to one as you all aware
probability can be between 0 and one and
as we can imagine it is intuitive as
well as the number of years are on the
Lower Side maybe one year two years or 3
years till after the service the chances
of your car breaking down are very
limited right so for example chances of
your car breaking down or the
probability of your car breaking down
within 2 years of your last service are
0.1 probability similarly 3 years is
maybe3 and so on but as the number of
years increases let's say if it was six
or 7 years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the Y that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or or not yes or no that
is what we are talking about so here the
outcome is discrete and not a continuous
value so this is how the logistic
regression curve looks let me explain a
little bit what exactly and how exactly
we are going to uh determine the class
at the outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so that can
cause confusion so let's try to remove
that confusion so what is linear
regression linear regression is a
process is once again and algorithm for
supervised learning however here you're
going to find a continuous value you're
going to determine a continuous value it
could be the price of a real estate
property it could be your hike how much
hike you're going to get or it could be
a stock price these are all continuous
values these are not discrete compared
to a yes or a no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to to find out
what should be the salary hike of an
employee so they collect all the details
of their existing employees their
ratings and their salary hikes what has
been given and that is the labeled
information that is available and the
system learns from this it is trained
and it learns from this labeled
information so that when a new employees
information is fed based on the rating
it will determine what should be the
height so this is a linear regression
problem and a linear regression example
now salary is a continuous value you can
get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discret values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on that
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the label data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see it
doesn't look very right because looks
like there will be lot of error the root
mean square error if you remember for
linear regression would be very very
high and also the the values cannot go
beyond zero or Beyond one so the graph
should probably look somewhat like this
clipped at 0 and one but still the
straight line doesn't look right
therefore instead of using a linear
equation we need to come up with
something different and therefore the
logistic regression model looks somewhat
like this so we calculate the
probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and U this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odds so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from
0er to Infinity so when this probability
is zero then the odds will the value of
the odds is equal to zero and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line Looks So
Y is equal to Beta 0 + beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 + beta 1 x which is the
equation of the straight line and now
from here if you want to find out the
value of PX we will see we can take the
exponential on both sides and then if we
solve that equation we will get the
equation of PX like this PX is equal to
1 by 1 + e^ of minus beta 0 + beta 1 x
and recall this is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logis IC regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and U linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let
let's take an example and see how we can
apply logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and um show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accur accuracy and then we use it
for for inference right so that is
typical methodology of uh uh training
testing and then deploying of machine
learning models so let's uh take a look
at the code and uh see what we are doing
so I'll not go line by line but just
take you through some of the blocks so
first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using mat plot lip some of the images or
a sample of these images and um then we
split the data into training and test as
I mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Mak
is that first of all this as you can see
this is a matrix and the size of the
Matrix depends on how many outputs we
are expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonal should have the
maximum numbers and the others other
than diagonal the cells other than the
diag should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that uh like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like 100%
accurate model okay so that's uh gist of
how to use this Matrix how to use this
confusion Matrix I know the name uh is a
little funny sounding confusion Matrix
but actually it is not very confusing
it's very straightforward so you just
plotting what has been predicted and
what is the labeled information or what
is the actual data that's also known as
the ground truth sometimes okay these
are some fancy terms that are used so
predicted label and the actual label
that's all it is okay yeah so we are
showing a little bit more information
here so 38 have been predicted and here
you will see that all of them have been
predicted correctly there have been 38
zeros and the predicted value and actual
value is is exactly the same whereas in
this case right it has there are I think
37 + 5 yeah 42 have been fed the images
42 images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by 8 images
so that's that what is reflected in this
shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numai and map plot and we will take a
look at uh some of the sample images
that we have loaded so the this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the way it works is the
pixel information so as you can see here
this is an 8 by 8 pixel kind of a image
and uh the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn for which
number what is the p pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and test so that the training
data set is used to train the system so
we pass this probably multiple times uh
and then we test it with the test data
set and the split is usually in the form
of there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20 23 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is labeled data for each
of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train Yore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is
1383 by 64 and ycore train has 1383
because there is nothing like the second
part is not required here and then xcore
test shape we see is 414 so actually
there are 414 observations in test and
1383 observations in train so that's
basically what these four lines of code
are are saying okay then we import the
logistic regression library and uh which
is a part of psychic learn so we we
don't have to implement the logistic
regression process itself we just call
these the function and uh let me go
ahead and execute that so that uh we
have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
uh shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as uh
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matx
matrix by running confusion Matrix and
passing these uh values so we have so
this confusion uncore Matrix method
takes two parameters one is the Yore
test and the other is the prediction so
what is a Yore test these are the
labeled values which we already know for
the test data set and predictions are
what the system has predicted for the
test data set okay so this is known to
us and this is what the system has uh
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and um the key point out here
is that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is a first thing
right so if you add up all these numbers
that will be equal to the number of
observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers uh which means the accuracy is
very low the DI indicates a correct
prediction this means that the actual
value is same as the predicted value
here again actual value is same as the
predicted value and so on right so the
moment you see a number here that means
the actual value is something and the
predicted value is something else right
similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44+ 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually usually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
are all the values are here most of the
values so which means reasonably this
seems to be reasonably accurate and yeah
basically the accuracy score is 94% this
is calculated as I mentioned by adding
all these numbers divided by the total
test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this is also
been predicted correctly nine and actual
value is nine and this is the image of
one and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images and before
starting if you're are one of the
aspiring machine learning Enthusiast
looking for online training and
graduating from the best universities or
a professional where heits to switch
careers in machine learning by learning
from the experts then try giving a short
to Simply learn postgraduate program in
Ai and machine learning in collaboration
with berdu University and IBM the course
link is mentioned in the description box
below that will navigate you to the
course page where you can find a
complete overview of the program being
offered and one more announce oun ment
that is if you want to be a ml
Enthusiast looking for online training
and graduating from the best
universities then we have a simply learn
skel Tech a ml boot Cam and the boot cam
link is also mentioned in the
description box below that will navigate
you to the Boot Camp Page and you can
find a complete overview of the program
being offered so first we will import
some major libraries of python so here I
will write
import pandas as
PD
and
import numai as
NP then
import
cbor that's
SNS okay then
import SK
learn dot model
selection
P train
underscore test underscore
split before that I will
import mat plot
Lim P
plot as
PLT okay
then I will write here from
Escalon dot
matrix
import
accuracy
four then
from
Escalon dot
matrix
import classif
ification
report and import R then import
string
okay then press enter so it is
saying okay here I have to write
from everything seems
good loading let's
see okay till then numai is a python
Library used for working with arrays it
also has function for working with
domain of linear algebra and
matrices it is an open source project
and you can use it
freely n stand for numerical
python pandas so panda is a software
Library written for Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operation for manipulating
numerical tables and time
series then cbon an open source python
Library based on M plot lib is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbon functions
with ease then M BR lip for Python and
its numerical extension numpy met plot
lib is a cross platform for the data
visualization and graphical charting
package as a result it presents a strong
open source suitable for metlab the apis
for met plot lib allow programmers to
incorporate graphs into gii applications
then this train test split we may build
our training data and the test data with
the aid of Escalon train test split
function this is so because the original
data set often serves as both the
training data and the test data starting
with a single data set we divide it into
two data sets to obtain the information
needed to create a model like H and test
accuracy score the accuracy score is
used to gge the model's Effectiveness by
calculating the ratio of total true
positive to Total true negative across
all the model prediction this re regular
expression the functions in the model
allow you to determine whether a given
text fits a given regular expression or
not which is known as
re okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating data the Str
Str class is a built in string class in
Python because python strings are
immutable they cannot be modified after
they have been
formed okay so now let's import the data
set we will be going to import two data
set one for the fake news and one for
the True News or you can say not fake
news okay so I will write here
efcore P equals
to PD
do read undor
CSV or what can I say
DF
okay
_
fake
okay then fake dot CSV you can download
this data set from the description box
below then data dot true equals to pd.
read
CSV sorry
CSC then fake news sorry
true true.
CSV okay then press
enter so these are the two data set you
can download these data set from the
description box below so let's see the
board data set okay then I will write
here
dataor
fake do
head so this this is the fake data okay
then data underscore
true
do and this is the true
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here data let's go
fake
cl equals
to0 then data underscore true
and
plus = to
1
okay
then I will write here data underscore
fake dot shape and data underscore
true do
shap okay then press enter
so the shape method return the shape of
an array the shape is a tle of integers
these number represent the length of the
corresponding array dimension in other
words a tle containing the quantities of
entries on each axis is an array shape
Dimension so what's the meaning of
shape in the fake word in this data set
we have 2 3 4 8 1 rows and five columns
and in this data set true we have 2 1 41
7 rows and five columns
okay so these are the rows column rows
column for the particular data
set so now let's move and let's remove
the last 10 rows for the manual testing
okay then I will write here data
underscore
fake let's go
manual
testing to dataor fake
dot
tail for the last 10 rows I have to
write here
10 okay so for
I in
range 2 3
4 8 1 sorry
0o comma 2
3
470 comma
minus1
okay
then DF uncore not DF
data underscore
fake dot
drop
one is inste of one I can write here
I
comma X is = to
0 in
place equal to
true then
data not
here data underscore
same I will write for I will copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true here I can write
true
okay then I have to change a
[Music]
number 2
1
416
write 21 4
6 minus
one
same so press
enter x equal to Z
Z INX maybe you mean d0 or of
this okay we will put here double
course I'm putting
this
f. drop i z in place
okay also write equals
toal yeah
so okay axis is not
defined now it's working
so let me
see now data
underscore pi.
shape
okay and data dot
true data underscore
true dot
shape as you can see
10 rows are deleted from each data
set yeah so I will write here data
underscore fake underscore
manual
testing
class =
to0 and data
underscore true
underscore manual _
testing CL equals
to
one
okay just ignore this
fing then let's
see data
underscore
bore
manual testing
doad as you can see we have this and
then data dot sorry underscore
truecore
manual
testing dot at
done
okay this is this is the uh true data
set so here I will merge data uncore
merge
to PD
dot
concat concat is used for the
concatination
data underscore
fake data
underscore comma
XIs = to
zero then data underscore
merge do
head the top 10
rows yeah
as you can see the data is merged
here okay first it will come for the
fake news and then with the for the True
News and let's merge true and fake data
frames
okay we did this
and let's Mery column then
data do
merge dot columns or let's see the
columns it is not defined what data
underscore
much these are the column name title Tex
subject date class
okay
now let's remove those those columns
which are not required for the further
process so here I will write data
underscore or equals to data underscore
March
prop title we don't
need
then subject we don't need
then
one so let's check some null
values it's giving
error of
this
that's good then
data dot
isnull
sum
Center so no null values okay then let's
do the random shuffling of the data
frames okay for that we have to write
here data equals to data do sample
one
then data okay
data do
head okay now you can see here the
random shuffling is
done and one for the true data set and
zero for the fake news one okay
then let me write here data
dot
reset underscore
index
Place equals to
True data dot
drop
comma X's = to
1 then comma in
place equals to
True
okay then let me see columns now data
dot
columns so here we have two columns only
rest we have deleted
Okay so
me see data dot
add yeah everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write
[Music]
here
what
okay you can use any
name
text and text equals
to text.
lower okay and
texts to r dot for the
substring remove these
things uh from
the datas okay so for that I'm writing
herea okay then text equals to R do
substring
comma comma
text okay then I have to write text
equals
to I do sub
string
www
dot
s+
comma
text okay then text equals
to I do
substring
then
comma okay then text equals to r dot
substring
then
percentage
s
again percentage for r. SK
function right here
string do
punctuation
comma then comma then
text right
right then text equals to R do
substring and
N
comma XT equals to r
dot
subring WR here
and again
D then
again
okay then
comma then again
text okay then at the end I have to
write here return return text so
everything like uh this this type of
special character will be removed from
the data set okay let's run this let's
see yeah so here I will write DF sorry
not DF
data
data
then
text to
data dot
apply to the function name wordp word
opt
okay press enter yeah so now let's uh
Define the dependent and independent
variables okay x equals to
data
text and yals
to
data
class okay then splitting training and
testing data
okay sorry so here I will write xcore
train comma xcore
test uh then Yore
train comma Yore testal to train
underscore testore
split then X comma y comma
test let's go size equals to
0.25 okay press
enter so now let's convert XEX to
vectors for that I have to write
here that is
X so here I will write from Escalon
dot
feature
extraction do text
import D
vectorizer
okay then
vectorization
equals to T
FID
vectorizer okay
then V
underscore
train equals
to
vectorization
Z1
factorization do fit
then
transform xcore
train okay then X Vore test equals
to
factorization dot
transform xcore test
okay then press
enter
uh so now let's see our first model
logistic
regression so here I will write
from skar
dot linear uncore
model OKAY
import
logistic
then allot equals
to
logistic
regression have to write here LR
dot
fit then XV
Dot
not DOT train
comma X Vore
test okay let
Center xv.
train here I have to write y
train okay and press
enter will work so here I will write
prediction
underscore linear
regression
equu l r do
predict Vore
test okay let's see the accuracy
score for that I have to write
lr.
score then XV underscore
test comma
Yore
test okay let's see the accuracy so here
as you can see accuracy is quite good
98% now let's
print the
classification
p
Yore test
comma prediction of linear regression
okay so this is you can see Precision
score then F1 score then support value
accuracy okay so now we will uh do this
same for the decision free gradient
boosting classifier random Forest
classifier okay then we will do model
testing then we will predict this
school
okay so now for the decision tree
classification so for that I have to
import from
skon dot
tree
import
decision
three
classifier okay then at the short form I
will write here I will copy it from here
then okay then I have to write the same
as this so I will copy it from
here
and
yeah
let's change linear
regression to isry
classific
okay then I will write here
same go
DT equals to dt.
predict
3core
test
let
be still loading it's it will take
time
okay till then let me write here for the
accuracy.
score
Vore test comma
y okay
let's wait
okay
run the accuracy so as you can see
accuracy is good than this linear
regression okay logistic
regression okay so let
me you the let me
predict
print
so this is the accuracy score this is
the all the
report
yeah now let's move for the uh gradient
boosting
classifier okay for that I write from
Escalon Dot andem
symbol
Port
radiant
boosting
classifier
classify I will write here
GB equals to let me copy it from here
okay we give here
random score
State equals to
zero wait wait wait wait so I will write
here GB dot
fit X Vore train comma ycore train okay
okay then press
enter here I will write predict
underscore
GB to GB dot pit sorry
predict
three DOT
test doore
test till then it's loading so I I will
write here uh for the score then I will
add GB DOT
score then Vore test
comma Yore test okay so let's wait it is
running this
part till then let me write for the
printing
this
okay it's taking
time taking time still taking
time what if I will run
this it's not coming because of
this yeah it's done now so you can see
the accurate
is uh not good
than decision tree but yeah it is also
good
99.4 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from Escalon
Dot
symbol
import
random
Forest
classifier
okay then here I will write
RF
to right I will copy it from
here
then random
state
to Z
then RF dot
fit 3core
train comma
Yore
train okay then press
enter
and
predict
underscore
RC
RF equals
to RF do
predict 3core test
okay till then I will write it it's
still loading it will take time so till
then I will write for the score score
accuracy
score XV uncore test comma Yore
test okay then I will write here till
then
print
classification
port and Yore
test
comma
will take time little
bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you but after writing the code
so so I have made two two functions one
for the output label and one for the
manual testing okay so it will predict
the all the from the all models from
the repeat so it will
predict the the news is fake or not from
all the models okay so for that let me
write here
newss to
string
what
okay then I will write here manual
underscore
testing
okay
so here I will you can add any news from
the you can copy it from the Internet or
whatever from from wherever you want so
I'm just copying from the internet okay
from the Google the news which is not
fake okay I'm adding which is not fake
because I already know I searched on
Google so I'm entering this so just run
it let's see what is
showing okay string input object is not
callable okay let me check this
first
okay I have to give here s Str
only yeah let's
check okay I have to add here again the
script yeah manual testing is not
defined let me see manual
testing
okay I have to edit
something it is just GB and it is just
RF GBC is not Define okay okay so what I
have to do I have to remove
this
this okay everything seems
sorted
now as I said to you I just copied this
news from the internet I already know
the news is not fake so it is showing
not a fake news okay so now what I will
do I will
copy one fake news from the
internet and let's see it is detecting
it or not
okay so let me run
this and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand
till here so I hope you guys must have
understand how to detect a fake news
using machine learning you can you can
copy any news from the internet and you
can check it is fake or not and before
starting if you are one of the aspiring
machine learning Enthusiast looking for
online training and graduating from the
best universities or a professional
elicits to switch careers in machine
learning by learning from the experts
then try giving a short to Simply learn
postgraduate programming Ai and machine
learning in collaboration with berdu
University and IBM the course link is
mentioned in the description box below
that will navigate you to the course
page where you can find a complete
overview of the program being offered
and one more announcement that is if you
want to be a ml Enthusiast looking for
online training and graduating from the
best universities then we have a simply
learn skel Tech AI ml boot camp and the
boot cam link is also mentioned in the
description box below that will navigate
you to the Boot Camp Page and you can
find a complete overview of the program
being offered hello everyone welcome to
the session I'm moan from Simply learn
and today we'll talk about interview
questions for machine learning now this
video will probably help you when you're
attending interviews for machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked uh questions and to help
you and in answering these questions we
tried our best to give you the best
possible answers but of course what is
more important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of get
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine learning but
what is important is you would probably
be better of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is the third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say past data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is label data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right fact
okay so that is what is supervised
learning when you have existing labeled
data which you then use to train your
model that is known as supervised
learning and unsupervised learning is
when you don't have this labeled data so
you have data it is not labeled so the
system has to figure out a way to do
some analysis on this okay so that is
unsupervised learning and you can then
add a few things like what are the ways
of performing uh supervised learning and
unsupervised learning or what are some
of the techniques so supervised learning
we we perform or we do uh regression and
classification and unsupervised learning
uh we do clustering okay and clustering
can be of different types similarly
regression can be of different types but
you don't have to probably elaborate so
much if they are asking uh for uh just
the different types you can just mention
these and just at a very high level you
can but if they want you to elaborate
give example ex then of course I think
there is a different question for that
we will see that later then the third so
we have supervised then we have
unsupervised and then reinforcement you
need to provide a little bit of
information around that as well because
it is sometimes a little difficult to
come up with a good definition for
reinforcement learning so you may have
to little bit elaborate on how
reinforcement learning works right so
reinforcement learning works in in such
a way that it basically has two parts to
it one is the agent and the environment
and the agent basically is working
inside of this environment and it is
given a Target that it has to achieve
and uh every time it is moving in the
direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal okay then it is rewarded and
every time it is going in a direction
where it is away from the goal then it
is punished so that is the way you can a
little bit explain and uh this is used
primarily or very very impactful for
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphago used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting may be a
little difficult to understand
overfitting is a situation where the
model has kind of memorize the data so
this is an equivalent of memorizing the
data so we can draw an analogy so so
that it becomes easy to explain this now
let's say you're teaching a child about
some recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples okay so this is a a small
child and for the first time you're
teaching the child to recognize fruits
then so what will happen so this is very
much like that is your training data set
so what you will do is you'll take a
basket of fruits which consists of
apples oranges and pineapples okay and
you take this basket to this child and
uh there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so for
and then again you keep repeating that
right so till that basket is over this
is basically how training work in
machine learning also that's how TR
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then the process
what has happened the child has pretty
much memorized these so even before you
finish that basket right by the time you
are halfway through the child has
learned about recognizing the Apple
orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exact ly 100% accurately
it will identify it will say the child
will say this is an apple this is an
orange and if you show a pineapple it
will say this is a pineapple right so
that means it has kind of memorized this
data now let's say you bring another
basket of fruits and it will have a mix
of maybe apples which were already there
in the previous set but it will also
have in addition to Apple it will
probably have a banana or maybe another
fruit like a jack fruit right so so this
is an equivalent of your test data set
which the child has not seen before some
parts of it it probably has seen like
the apples it has seen but this banana
and jack fruit it has not seen so then
what will happen in the first round
which is an equivalent of your training
data set towards the end it has 100% it
was telling you what the fruits are
right Apple was accurately recognized
orange were was accurately recognized
and pineapples were accurately
recognized right so that is like 100%
accuracy but now when you get another a
fresh set which were not a part of the
original one what will happen all the
apples maybe it will be able to
recognize correctly but all the others
like the jack fruit or the banana will
not be recognized by the child right so
this is an analogy this is an equivalent
of overfitting so what has happened
during the training process it is able
to recognize or reach 100% accuracy
maybe very high accuracy okay and we
call that as very low loss right so that
is the technical term so the loss is
pretty much zero and accuracy is pretty
much 100% whereas when you use testing
there will be a huge error which means
the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like drop out in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have th000 records with labeling
information now one way of doing it is
you use all the Thousand records for
training and then maybe right which
means that you have exposed all this
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may pass this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times till the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that here that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like a realistic situation
because when the model is deployed that
is what will happen it will receive some
new new data not the data that it has
already seen right so this is a
realistic test so you put some new data
so this data which you have set aside is
for the model it is new and if it is
able to accurately predict the values
that means your training has worked okay
the model got drained properly but let's
say while you're testing this with this
test data you're getting lot of errors
that means you need to probably either
change your model or retrain with more
data and things like that now coming
back to the question of how do you split
this what should be the ratio there is
no fixed uh number again this is like
individual preferences some people split
it into 50/50 50% test and 50% training
Some people prefer to have a larger
amount for training and a smaller amount
for test so they can go by either 6040
or 7030 or some people even go with some
odd numbers like
6535 or uh
63333 which is like 1/3 and 2 2 thir so
there is no fixed rule that it has to be
something the ratio has to be this you
can go by your individual preferences
all right then you may have questions
around uh data handling data
manipulation or what do you call data
management or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were the this
many rows where the data was missing
that would be in fact a perfect way to
respond to this question but if you
don't have that obviously if you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or correct
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having this missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but uh when you do some initial
calculation how many records are corrupt
or how many rows or observations as we
call it has this missing data let's
assume it is very minimal like 10% okay
now between these two cases how do you
so let's assume that this is not a
mission critical situation and in order
to fix this 10% % of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50% of your data is like that
because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10% of the
records column one is missing and
another 10% column two is missing
another 10% column 3 is missing and so
on and so forth so it adds up to maybe
half of your data set so you cannot
completely remove half of your data set
then the whole purpose is lost okay so
then how do you handle then you need to
come up with ways of filling up this
data with some meaningful value right
that is one way of handling so when we
say meaningful value what is that
meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for a TOS right there may be situations
where if the missing values instead of
putting mean maybe a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this question okay so then the
next question can be how can you choose
a classifier based on a training set
data size so again this is one of those
questions uh where you probably do not
have like a one size fits all on first
of all you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to the this question
is you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just byy if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that yes this is the
classifier that will work here no that's
not the right way so you need to still
you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion Matrix so the question
can be explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by taking an example and
drawing like a small diagram otherwise
it can really become tricky so my
suggestion is to take a piece of pen and
paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and it
is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
PS and how many no so you have that
information and you have the predicted
values how many yes and how many no
right so the total actual values the
total yes is 12 + 11 13 and they are
shown here and the actual value no no
are 9 + 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and uh out
of this what is the meaning convey so
there are two of three things that needs
to be explained outright the first thing
is for a model to be accurate rate the
values across the diagonal should be
high like in this case right that is one
number two the total sum of these values
is equal to the total observations in
the test data set so in this case for
example you have 12 + 3 15 + 10 25 so
that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let's me uh
take a pen so your your diagonal values
is equal to if I say d is equal to 12 +
9 so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw a diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you once
you find the diagonal values that is
equal to your percentage okay all right
so the next question can be a related
question about false positive and false
negative so what is false positive and
what is false negative now once again
the best way to explain this is using a
piece of paper and Pen otherwise it will
be pretty difficult to to explain this
so we use the same example of the
confusion Matrix and uh we can explain
that so a confusion Matrix looks
somewhat like this and um when we just
take yeah it looks somewhat like this
and we continue with the previous
example where this is the actual value
this is the predicted value and uh in
the actual value we have 12 + 1 13 yeses
and 3 + 9 12 Nos and the predicted
values there are 12 + 3 15 yeses and uh
1 + 9 10 NOS okay now this particular
case which is the false positive what is
a false positive first of all the second
word which is positive okay is referring
to the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so so
if we say true positive positive we need
to take for the predicted part so
predicted positive is here okay and then
the first term is for the actual so true
positive so true in case of actual is
yes right so true positive is this one
okay and then in case of actual the
negative now we are talking about let's
say true negative true negative negative
is this one and the true comes from here
so this is true negative right nine is
true negative the ACT value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set said to check the
accuracy whether it is working fine or
not so you test the model before you
actually put it into production right so
once you test the model you're satisfied
it's working fine then you go to the
next level which is putting it for
production and then in production
obviously new data will come and uh the
inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values whether it is
regression or classification now so this
can be an iterative process so it is not
a straightforward process where you do
the training do the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is a deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you are different defining deep
learning so first is of course deep
learning is a subset of machine learning
so machine learning is still the bigger
let's say uh scope and deep learning is
one one part of it so then what exactly
is the difference deep learning is
primarily when we are implementing these
our algorithms or when we are using
neural networks for doing our training
and classification and regression and
all that right so when we use neural
network then it is considered as deep
learning and the deep comes from the
fact that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning uh the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so so we have our training label
data and uh this data has several let's
say if it is a regular table it has
several columns now each of these
columns actually has information about a
feature right so if we are trying to
predict the height weight and so on and
so forth so these are all features of
human beings let's say we have sensus
data and we have all the so those are
the features now there may be probably
50 or 100 in some cases there may be 100
such features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many features and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking primity of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and uh the problem
is divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural
length so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
highend machines uh because it needs a
lot of computing power and the Machine
learning features are or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of meas machine learning or some
real life applications of machine
learning in modern business the question
may be worded in different ways but the
the meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cas es if
you will for supervised machine learning
the very common one is email spam
detection so you want to train your
application or your system to detect
between spam and non-spam so this is a
very common business application of
supervised machine learning so how does
this work the way it works is that you
obviously have historical data of your
emails and they are categorized as spam
and not spam so that is what is the
labeled information and then you feed
this information or the all these emails
as an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervisor machine learning here the way
it works is that existing images it
could be x-ray images it be MRI or any
of these images are available and they
are tacked saying that okay this x-ray
image is defective or the person has an
illness or it could be cancer whichever
illness right so it is stacked as
defective or clear or good image and
defective image something like that so
we come up with a binary or it could be
multiclass as well saying that this is
defective to 10% this is 25% and so on
but let's keep it simple you can give an
example of just a binary classification
that would be good enough so you can say
that in healthcare Diagnostics using
image we need to detect whether a person
is ill or whether a person is having
cancer or not so here the way it works
is you feed labeled images and you allow
the model to learn from that so that
when New Image is fed it will be able to
predict whether this person is having
that illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggest it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existance is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the labeled data is
not that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images we
can just let's say take X-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
defective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semisupervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no uh quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so un supervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying laptops are also buying the mouse
so that means there is an association
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and unsed
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labeled data now
very often people say yeah we have
historical data and if we run it it is
supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we put in that keyword
labeled okay so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is inde used by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person a video that
fire can burn the F burn his finger or
fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN and another is K
means K stands for K nearest neighbors
and K means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them K andn is completely
different K means clustering is
completely different K&N is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a
unsupervised okay when you have K and n
when you want to implement KN andn which
is basically K nearest neighbors the
value of K is a number so you can say k
is equal to three you want to implement
KN andn with K is equal to 3 so which
means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular class but it is possible that
they may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign a odd number to so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K as
three the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
three of them belong to class two so
which means that this object will belong
to class two right so you see that so
this the class allocation can vary
depending on the value of K now K means
on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
the similarity of course is the the
letter K and in case of K means also we
specify its value and it could be three
or five or seven there is no technical
limit as such but it can be any number
of clusters that uh you can create okay
so based on the value that you provide
the system will create that many
clusters of similar objects so there is
a similarity to that extent that K is a
number in both the cases but but
actually these two are completely
different processes we have what is
known as KN based classifier and people
often get confused thinking that KN base
is the name of the person who found this
uh classifier or who developed this
classifier which is not 100% true base
is the name of the person b y s is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classifier na based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of one feature
of a class is not related to the
presence of any other feature of maybe
other classes right so which is not a
very strong or not a very what do you
say accurate assumption because these
features can be related and so on but
even if you go with this assumption this
whole algorithm works very well even
with this assumption and uh that is the
good side of it but the term comes from
there so that is the explanation that
you can give then there can be question
around reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score a high value High number
or it could be that your uh number of
lives should be as high as possible
don't lose life so these could be some
of them more advanced examples could be
for driving in the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the Debs for example it is rewarded and
every time it takes a step which goes
against that goal right contrary or the
reverse Direction it is penalized okay
so it is like a carrot and stick system
now how do you use this to create a game
of chess so to create a system to play a
game of chess now the way this works is
and this could probably go back to this
Alpha go example where alphao defeated a
human Champion so the way it works is in
reinforcement learning the system is
allowed for example if in this case
we're talking about Chess so we allow
the system to first of all watch playing
a game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being play the game because this is
reinforcement uh learning is pretty much
all visual so when you teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent works on that
so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you're playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alpha go
has done it played millions of games
between itself and the system right so
for the game of chess also you need to
do something like that you need to allow
the system to play chess and then learn
on its own over a period of repetition
so I think you can probably explain it
uh to this much to this extent and it
should be uh sufficient
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are Prov provided
obviously something like uh you know
Amazon website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
gender and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine work all right
then the question can be something very
basic like when will you go for
classification versus regression right
when do you do classification instead of
regression or when you use
classification instead of regression now
yes so so this is basically going back
to the understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values and other is the
continuous values so for regression if
you are trying to find some continuous
values you use regression whereas if
you're trying to find some discrete
values you use classification and then
you need to illustrate what are some of
the examples so classification is like
let's say there are images and you need
to put them into classes like cat dog
elephant tiger something like that so
that is a classification problem or it
can be that is a multiclass
classification problem it could be
binary classification problem like for
example whether a customer will buy or
he will not buy that is a classification
binary classification it can be in the
weather forecast area now weather
forecast is again combination of
regression and classification because on
the one hand you want to predict whether
it's going to rain or not that's a
classification problem that's a binary
classification right whether it's going
to rain or not rain however you also
have to predict what is going to be the
temperature tomorrow right now
temperature is a continuous value you
can't answer the temperature in a yes or
no kind of a response right so what will
be the temperature tomorrow so you need
to give a number which can be like 20
30 or whatever right so that is where
you use regression one more example is
stock price prediction so that is where
again you will use regression so these
are the various examples so you need to
illustrate with examples and make sure
you include those key words like
discrete and continuous so the next
question is more about a little bit of a
design related question to understand
your Concepts and things like that so it
is how will you design a spam filter so
how do you basically design or develop a
spam filter so I think the main thing
here is he is looking at probably
understanding your Concepts in terms of
uh what is the algorithm you will use or
what is your understanding about
difference between classification and
regression uh and things like that right
and the process of course the
methodology and the process so the best
way to go about responding to this is we
say that okay this is a classification
problem because we want to find out
whether an email is a spam or not spam
so that we can apply the filter
accordingly so first thing is to
identify what type of a problem it is so
we have identified that it is a
classification then the second step
maybe to find out what kind of algorithm
to use now since this is a binary CL
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector missions for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and uh
then you split that into training and
test data sets you use your training
data set to train your model that or
your algorithm that you have used rather
the model actually so and you actually
will have three models let's say you are
trying to test out three algorithms so
you will obviously have three models so
you need to try all three models and
test them out as as well see which one
gives the best accuracy and then you
decide that you will go with that model
okay so training and test will be done
and then you zero in on one particular
model and then you say okay this is the
model will we use we will use and then
go ahead and Implement that or put that
in production so that is the way you
design a Spam fi the next question is
about random Forest so what is random
Forest so this is a very straightforward
question however the response you need
to be again a little careful while we
all know what is random ROM Forest
explaining this can sometimes be tricky
so one thing is random Forest is kind of
in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and uh trees will basically you
will use for doing if it is
classification mostly it is
classification you will use the the
trees for classification and then you
use voting for finding the the final
class so that is the underl but how will
you explain this how will you respond to
this so first thing obviously we will
say that random Forest is one of the
algorithms and the more important thing
that you need to probably the
interviewer is is waiting to hear is
Ensemble learner right so this is one
type of Ensemble learner what is
Ensemble learner Ensemble learner is
like a combination of algorithms so it
is a learner which consists of more than
one algorithm or more than one maybe
models okay so in case of random Forest
the algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a a random Forest is an emble
learner there are other types of
Ensemble Learners where we have like we
use different algorithms itself so you
have one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to hear this
word Ensemble Learners and so then you
go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees okay and
each observation you pass through this
forest and each observation let's say it
is a classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and 10 of the trees say it is one
you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way random Forest works for
classification if it is a regression
problem it's somewhat similar but only
thing is instead of vot what we will do
is so in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
100 trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the 100 trees right so
this is also Al a way of reducing the
error so maybe if you have only one tree
and if that one tree makes error it is
basically 100% wrong or 100% right right
but if you have on the other hand if you
have a bunch of trees you are basically
mitigating that error reducing that
error okay so that is the way random
Forest works so the next question is
considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to out right say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a one size
fits all so we need to first of all you
can probably shorten the list in terms
of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably uh shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithms
right so for example if it a
classification you cannot use linear
regression algorithm there or if it is a
regression problem you cannot use svm or
maybe no you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100%
decide on saying that this is the exact
algorithm that I'm going to use so the
way to go about is you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then there can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example bias in machine learning it
occurs when the predicted values are far
away from the actual value so that is a
bias okay and whereas they are all all
the values are probably they are far off
but they are very near to each other
though the predicted values are close to
each other right while they are far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance now variance is when the
predicted values are all over the place
right so the variance is high that means
it may be close to the Target but it is
kind of very scattered so the point the
predicted values are not close to each
other right in case of buyers the
predicted values are close to each other
but they are not close to the Target but
here they may be close to the Target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
tradeoff between bias and variance yes I
think this is a interesting question
because these two are are heading in
different directions so for example if
you try to minimize the bias variance
will keep going high and if you try to
minimize the variance buys will keep
going high and there is no way you can
minimize both of them so you need to
have a tradeoff saying that okay this is
the level at which I will have my buyers
and this is the level at which I will
have variance so the trade-off is that
pretty much uh that you you decide what
is the level you will tolerate for your
buyers and what is the level you will
tolerate for variance and a combination
of these two in such a way that your
final results are not way off and having
a trade off will ensure that the results
are consistent right so that is
basically the output is consistent and
which means that they are close to each
other and they are also accurate that
means they are as close to the Target as
possible right so if either of these is
high then one of them will go off the
track define precision and Recall now
again here I think uh it would be best
to draw a diagram and take a the
confusion Matrix and it is very simple
the definition is like a formula your
Precision is true positive by true
positive plus false positive and your
recall is true positive by true positive
plus false negative okay so that's you
can just show it in a mathematical way
that's pretty much uh you know that can
be shown that's easiest way to define so
the next question can be about decision
tree what is decision tree pruning and
why is it so basically decision trees
are really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and the conditions can become
very complicated and this can also lead
to overfitting which is basically that
during training you will get 100%
accuracy but when you're doing testing
you'll get a lot of Errors so that is
the reason pruning needs to be done so
the purpose or the reason for doing uh
decision tree pruning is to reduce
overfitting or to cut down on
overfitting and what is decision tree
pruning it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the Leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing those internal
nodes all right so the next question can
be what is logistic regression uh so
basically logistic regression is um one
of the techniques used for performing
classification especially binary
classification now there is something
special about log listic regression and
there are a couple of things you need to
be careful about first of all the name
is a little confusing it is called
logistic regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if if
it's really you know if it is required
and they can also ask this like a trick
question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from log so that the mathematical
derivation Wass log and therefore the
name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between 0
and one and we can set a threshold that
can vary typically it is 0 five so any
value above 0.5 is considered as one and
if the probability is below 0. five it
is considered as zero so that is the way
we calculate the probability or the
system calculates the probability and
based on the threshold it sets a value
of zero or one which is like a binary
classification zero or one okay then we
have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is a k nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classif
classify objects okay we need to
classify objects so again it will be
very helpful to draw a diagram you know
if you're explaining I think that will
be the best way so draw some diagram
like this and let's say we have three
clusters or three classes existing and
now you want to find for a new item that
has come you want to find out which
class this belongs to right so you go
about as the name suggests you go about
finding the nearest neighbors right the
points which are Clos to this and how
many of them you will find that is what
is defined by K now let's say our
initial value of K was five okay so you
will find the K the five nearest data
points so in this case as it is
Illustrated these are the five nearest
data points but then all five do not
belong to the same class or cluster so
there are one belonging to this cluster
one the second one belonging to this
cluster two three of them belonging to
this third cluster Okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
a odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as I
was talking about right so we assign it
to this third class so in this as it is
uh that's how it is decided okay so K
nearest neighbor so first thing is to
identify the number of neighbors that
are mentioned as K so in this case it is
K is equal to 5 so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that okay and and then the uh
new data point is assigned to that class
okay and that's a wrap for a machine
learning crash course we hope you found
this video informative and inspiring
remember the world of machine learning
is vast and continually evolving so
don't hesitate to keep exploring and
experimenting if you enjoyed this crash
course give us a thumbs up and share it
with your friends who might also be
interested in diving into machine
learning also don't forget to leave your
thoughts and questions in the comment
section below we love hearing from you
and are always here to help until then
stay safe and keep learning staying
ahead in your career requires continuous
learning and upscaling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on on
the path to Career Success click the
link in the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here