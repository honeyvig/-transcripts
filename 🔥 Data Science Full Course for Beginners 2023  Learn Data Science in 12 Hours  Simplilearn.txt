foreign
's YouTube channel in this session we
will learn the data science full course
but before we begin if you are an
aspiring data scientist who is looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
now with that having said back to the
PPT now we will get started by having a
brief introduction to data science
followed by that we will understand what
is artificial intelligence machine
learning and deep learning followed by
that we will understand the thinnest
lines of differences between the
artificial intelligence machine learning
deep learning and data science followed
by that we will have the basics of
python for data science then we will
understand the data analyst data
scientist data engineer job profiles and
what are the differences between them
then we will get started with data
science life cycle moving ahead we will
have statistics for data science types
of distribution and statistics base
theorem then we will have the brief
understanding of artificial intelligence
through the artificial intelligence
quick tutorial then we have the machine
learning introduction followed by that
we will have a brief understanding of
the different types of machine learning
and then we will learn supervised and
unsupervised machine learning and the
differences between them next we have
the reinforced machine learning and
followed by that we will understand the
differences between supervised machine
learning unsupervised machine learning
and reinforced learning then we have the
regression analysis linear regression
logistic regression linear versus
logistic regression classification of
machine learning and decision Tree in
machine learning moving ahead we will
understand random Forest algorithm K
means clustering algorithm knife-based
classifier and once we are done with all
these algorithms we will start off by
Deep learning then we will learn the
natural language processing and the
important python libraries for data
science followed by that we will have a
brief understanding of what exactly is
data visualization and one of the most
important data visualization tool the
Tableau once we have a better
understanding of the the fundamentals we
will start off by learning how to build
a data science engineer resume so once
you are good with the resume building
then we have the most important aspect
of this particular fill course which is
the most frequently Asked data science
interview questions by the most biggest
and top industries in the company and
these interview questions should keep
you on the safer side to back your dream
job opportunity I hope I made myself
clear with the agenda with that let me
tell you guys that we have daily updates
on multiple Technologies if you're a
technique in a continuous hunt for
latest technology Trends then consider
getting subscribed to our YouTube
channel and don't forget to hit that
Bell icon to never miss an update from
Simply learn now over to our training
experts do you know how Netflix decides
which movie is soak your taste better or
how how Google Maps can provide roads
with low traffic at given moment in
seconds this is all possible thanks to
data science the popularity of
exploded in the 2010s causing an
approximate 15 year old year growth in
the job market and is expected to grow
even higher in the coming decade now
what exactly is data science data
science is the field of study that works
with massive amounts of data utilizing
relevant tools and techniques to derive
valuable data the day-to-day work of a
data scientist involves collecting
analyzing and interpreting this data to
help businesses achieve their goals
companies across various Industries
generate vast amounts of data so its
application range all the way from
education Healthcare and entertainment
to finance and marketing when it comes
to the suffix of data science it
comprises many Technologies like
artificial intelligence machine learning
natural language processing and deep
learning each of them fulfilling
different purposes and having varied
functionalities the role of a data
scientist was voted the sexiest job of
the 21st century and it is for a good
reason but demand for professionals who
can make sense of vast amount of data is
more than ever in this day and age
companies like Microsoft Google and
Amazon hire data scientists as they deal
with problem solving models involving
massive amounts of data daily this is
why data science professionals are in
high demand with average salary is going
as high as
1864 dollars per annum with 2.5 million
terabytes of data being transferred over
the internet on a daily basis the data
science Industry is related to grow even
higher promoting further Innovation and
job opportunities
Google had gathered five exabytes of
data between the beginning of time 2003.
this amount of data started to be
produced every two days in 2010 and
every 40 minutes by 2021 the
responsibility of a data scientist is to
gather clean and present data and have a
keen business sense and analytical
abilities let us have a discussion about
it in the upcoming slides how can you
become a data scientist as a beginner I
guarantee you after watching this video
you will have a clear understanding on
how to drive your career as a data
scientist hey guys welcome to Simply
learn before proceeding please make sure
you subscribe to Simply learns YouTube
channel and press the Bell icon to never
miss any updates today we are going to
cover significance of data scientists in
Industries after that prerequisites and
Technologies required for a data
scientist and finally salary of a data
scientist I have a query for you which
technology is used by Google Google Maps
to predict traffic jams deep learning
machine learning natural language
processing data structure please leave
the answer in the comment section below
and stay tuned to get the answer
significance now we will see how top
industries are involved in the field of
data science by 2025 the data science
Industry is anticipated to grow to a
value of 16 billion dollars there is an
abundance of data science jobs all over
the world now let's list out crucial
areas where data science is used media
and entertainment the major player in
the media and entertainment sector such
as YouTube Netflix hot star Etc have
begun to use data science to better
understand their audience and provide
them with recommendations that are both
relevant and personalized e-commerce
data science has aided retail companies
in better meeting their expectations as
they bring a unique combination of deep
data knowledge technology skills and
statistical experience data scientists
are in high demand in the retail
industry top recruiters are Amazon
Flipkart Walmart myntra Etc digital
marketing large volumes of data are
currently being fetched from its users
through search Pages social networks
online traffic display networks movies
web pages Etc a high level of business
intelligence is needed to analyze such
large amount of data and this can only
be done with the proper use of data
science approaches top recruiters are
Amazon Flipkart Facebook Google Etc
cyber security data science and AI are
now being used by the cyber security
industry to prevent the growing usage of
algorithms for harmful purposes top
recruiters includes IBM Microsoft
Accenture Cisco and many more before
moving forward what is the response to
the Google Map question that I asked
answer is machine learning
coming to prerequisites now that when no
significance of data science in
Industries let us explore the
prerequisites and Technologies required
for a data scientist seeing the demand
of data scientists in every industry it
is obvious that the scope of a data
scientist is very high so how to start
there is no necessity that you should be
knowing any technology or programming
language you can be a Layman too data
scientists typically have a variety of
educational and professional experiences
most should be proficient in four
crucial areas important skill is
mathematical expertise three concepts
like linear algebra multivariable
calculus and optimization technique are
crucial because they aid in our
understanding of numerous machine
learning algorithms that are crucial to
data science
similar to that knowing statistics is
crucial because they are used in data
analysis additionally important to
statistics probability is regarded as a
must of for mastering machine learning
next is computer science in the field of
computer science there is a lot to learn
but one of the key inquiries that arises
in relation to programming is r or
Python language there are many factors
to consider when deciding which language
to choose for data science because both
have a comprehensive collection of
libraries to implement complex machine
learning algorithms in addition to
learning a programming language you
should learn the following computer
science skill fundamentals of algorithm
and data structures distributed
computing machine learning deep learning
Linux SQL mongodb Etc domain expertise
most individuals wrongly believe that
domain expertise is not crucial to data
science yet it is consider the following
scenario if you are interested in
working as a data scientist dentist in
the banking Industries and you already
know a lot about it for instance you are
knowledgeable about stock trading
Finance Etc this will be very
advantageous for you and the bank itself
will favor you over other applicants and
finally communication skill it covers
both spoken and written Communication in
a data science project the project must
be explained to others when finding from
the analysis have been reached this can
occasionally be a report that you
provide to your team or employer at work
sometimes it might be a blog entry it is
frequently a presentation to a group of
co-workers regardless a data science
project always involves some form of
communication of the Project's findings
therefore having a good communication
skill is a requirement for being a data
scientist apart from all this practicing
is very important keep using different
tools also start reading blogs on data
science start building projects on data
science which can be added to your
resume also you can find many
interesting courses on data science by
simply learn salary reward is the result
of good work now we shall discuss
salaries that a data scientist will get
it should come as no surprise that data
scientists may add significantly to a
business every step of the process from
data processing to data cleansing
requires persistence a lot of arithmetic
and statistics as well as scattering of
engineering skills one of the most
important factors in a data scientist
salary is experience at the beginner
level a data scientist can make 95 000
annually the typical annual compensation
for a mid-level data scientist is
between 130 000 and 195 000 a seasoned
data scientist typically earns between
165 000 and 250 000 per year
in India at the beginner level a data
scientist can make 9 lakh forty thousand
rupees on average per year at mid level
data scientists will get 20 lakhs rupees
per annum and if you are at the advanced
level you will get paid an average of
rupees 25 lakhs annually this salary
will vary in different countries the top
hiring businesses in the US that provide
the highest salaries for data scientists
are apple with 180 000 per annum next is
Twitter with 170 dollars per annum meta
technology
170 dollars annually
LinkedIn 160 000 per annum crypto
technology provides 17 lakh 50 000 per
annum IBM provides 14 lakhs per annum
and Accenture will provide you with 19
lakhs per annum and finally American
Express will provide an average of 13
lakhs per annum if you are an aspiring
data scientist who is looking out for
online training and certification in
data science from the best universities
and Industry experts then search now on
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below deep learning deep
learning was first introduced in the
1940s deep learning did not develop
suddenly it developed slowly and
steadily over seven decades many Theses
and discoveries were made on deep
learning from the 1940s to 2000 thanks
to companies like Facebook and Google
the term deep learning has gained
popularity and may give the perception
that it is a relatively New Concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that imitates how
humans gain certain types of knowledge
deep learning includes statistics and
predictive modeling deep learning makes
processes quicker and simpler which is
advantageous to data scientists to
gather analyze and interpret massive
amounts of data having the fundamentals
discussed let's move into the different
types of deep learning neural networks
are the main component of deep learning
but neural networks comprise three main
types which contain artificial neural
networks or Ann convolution neural
networks or CNN and recurrent neural
networks or RNN artificial neural
networks are inspired biologically by
the animal brain convolutional neural
networks surpass other neural networks
when given inputs such as images Voice
or audio it analyzes images by
processing data recurrent neural
networks uses sequential data or series
of data convolutional neural networks
and recurrent neural networks are used
in natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren McCulloch and logician Walter
Pitts attempted to quantitatively map
out how humans make decisions and carry
out thinking processes therefore the
term machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
system's accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using label data machines predict output
based on this data now coming to
unsupervised learning models are not
supervised using a training data set it
is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in robotics
now coming to the evolution of AI the
potential of artificial intelligence
wasn't explored until the 1950s although
the idea has been known for centuries
the term artificial intelligence has
been around for a decade still it wasn't
until British polymath Alan Turing posed
the question of why machines couldn't
use knowledge like humans do to solve
problems and make decisions
we can Define artificial intelligence as
a technique of turning a computer-based
robot to work and act like humans
now let's have a glance at the types of
artificial intelligence
weak AI performs only specific tasks
like Apple Siri Google assistant and
Amazon's Alexa you might have used all
of these Technologies but the types I am
mentioning after this are under
experiment General AI can also be
addressed as artificial general
intelligence it is equivalent to human
intelligence hence an AGI system is
capable of carrying out any task that a
human can
strong AI aspires to build machines that
are indistinguishable from the human
mind
both General and strong AI are
hypothetical right now rigorous research
is going on on this matter there are
many branches of artificial intelligence
which include machine learning deep
learning natural language processing
robotics expert systems fuzzy logic
therefore the correct answer for which
is not a branch of artificial
intelligence is option a data analysis
now that we have covered deep learning
machine learning and artificial
intelligence the final topic is data
science
Concepts like deep learning machine
learning and artificial intelligence can
be considered a subset of data science
let us cover the evolution of data
science the phrase data science was
coined in the early 1960s to
characterize a new profession that would
enable the comprehension and Analysis of
the massive volumes of data being
gathered at the time since its
Beginnings data science has expanded to
incorporate ideas and methods from other
fields including artificial intelligence
machine learning deep learning and so
forth data science can be defined as the
domain of study that handles vast
volumes of data using modern tools and
techniques to find unseen patterns
derive meaningful information and make
business decisions therefore data
science comprises machine learning
artificial intelligence and deep
learning there are a lot of areas where
data science can be used one of the very
common one is fraud detection or fraud
prevention there are a lot of fraudulent
activities or transactions primarily on
the Internet it's very easy to commit
fraud and therefore we can use data
science to either prevent or detect
fraud there are certain algorithms
machine learning algorithms that can be
used like for example some outlier
techniques clustering techniques that
can be used to detect fraud and prevent
fraud as well so who is the data
scientist rather it is actually a very
generic role that defines somebody who
is working with data is known as a data
scientist but they can be very specific
activities and the roles can be actually
much more specific what exactly a person
does within the area of data science can
be much more specific but broadly
anybody working in the area of data
science is known as a data scientist so
what does a data scientist do these are
some of the activities data acquisition
data preparation data mining data
modeling and then model maintenance we
will talk about each of these in a great
detail but at a very high level the
first step obviously is to get the raw
data which is known as data acquisition
it can be in all kinds of format and it
could be multiple sources but obviously
that raw data cannot be used as it is
for performing data mining activities or
data modeling activities so the data has
to be glanced and prepared for using in
the data models or in the data mining
activity so that is the data preparation
then we actually do the data mining
which can also include some exploratory
activities and then if we have to do
stuff like machine learning then you
need to build a machine learning model
and test the model get insights out of
it and then if the model is fine you
deploy it and then you need to maintain
the model because over a period of time
it is possible that you need to tweak
the model because of change in the
process or changing the data and so on
so that all comes under the model
maintenance so let's take deeper look at
each of these activities let's start
with data acquisition so the stage of
data acquisition basically the data
scientist will collect raw data from all
possible sources so this could be
typically an rdbms which is a relational
database or it can also be a non-rdb Ms
or it could be flat files or
unstructured data and so on so we need
to bring all that data from different
sources if required you need to do some
kind of homogeneous formatting so that
it all fits into a you know looks at
least format from a format perspective
it looks homogeneous so that may be
requiring some kind of transformation
very often this is loaded into what is
known as data warehouse so this can also
be sometimes referred to as ETL or
extract transform and load so a data
warehouse is like a common place where
data from different sources is brought
together so that people can perform data
science activities like reporting or
data mining or statistical analysis and
so on so data from various sources put
in a centralized place which is known as
a data warehouse so that is also known
as ETL and in order to do this there can
be data scientists can take help of some
ETL tools there are some existing tools
that a data scientist can take help of
like for example data stage or Talent OR
Informatica these are pretty good tools
for performing this ETL activities and
getting the data the next stage now that
you have the raw data into a data
warehouse you still probably are not in
a position to straight away use this
data for performing the data mining
activities so that is where data
preparation comes into play and there
are multiple reasons for that one of
them could be the data is dirty there
are some missing values and so on and so
forth so a lot of time is actually spent
in this particular stage so a data
scientist spends a lot of time almost 60
to 70 percent of the time in this part
of the project or the process which is
data preparation so there are again
within this there can be multiple sub
activities starting from let's say data
cleaning you will probably have missing
values the data there is some columns
the values are missing or the values are
incorrect there are null values and so
on so forth so that is basically the
data cleaning part of it then you need
to perform certain Transformations like
for example normalizing the data and so
on right so you could probably have to
modify a categorical values into
numerical values and so on and so forth
so these are transformational activities
then we may have to handle outliers so
the data could be such that there are a
few values which are Way Beyond the
normal behavior of the data for whatever
reason either people have keyed in wrong
values or for some reason some of the
values are completely out of range so
those are known as outliers so there are
certain ways of handling these outliers
and detecting and handling these
outliers so this is a part of what is
known as exploratory analysis so you
quickly explore the data to find out
others so and you can use visual tools
like plots and identify what are the
outliers and see how we can get rid of
the outliers and so on then the next
part could the data Integrity data
Integrity is to validate for example if
there are some primary keys that all the
primary keys are populated there are
some foreign keys and at least most of
the foreign Keys should be populated and
otherwise when we are trying to query
the data you may get wrong values and so
on so that is the data Integrity part of
it and then we have what is known as
data reduction sometimes we may have
duplicate values we may have columns
that may be duplicated because they are
coming from different sources the same
values are there and so on so a lot of
this can be done using what is known as
data reduction and thereby you can
reduce the size of the data drastically
because very often because this could be
written in data which can be removed so
let's take a look at what are the
various techniques that are used for
data cleaning so we need to ensure that
the data is valid and it is consistent
and uniform and accurate so these are
the various parameters that we need to
ensure as a part of the data cleaning
process now what are the techniques that
that are used for data cleaning or so we
will see what each of these are in this
particular case and so what is the data
set that we have we have data about a
bank and its customer details so let's
take an example and see how we go about
cleaning the data and in this particular
example we are assuming we are using
python so let's assume we loaded this
data which is the raw file.csv this is
how the customer data looks like and we
will see for example we take a closer
look at the geography column we will see
that there are quite a few blank spaces
so how do we go about when we have some
blank spaces or if it is a string value
then we put a empty string here or we
just use a space or empty string if
there are numerical values then we need
to come up with a strategy for example
we put the mean value so wherever it is
missing we find the mean for that
particular column so in this case let's
assume we have credit score and we see
that quite a few of these values are
missing so what do we do here we find
the mean for this column for all the
existing values and we found that the
mean is equal to 638.6 so we kind of
write a piece of code to replace
wherever there are blank values Nan is
basically like null and we just go ahead
and say fill it with the mean value so
this is the piece of code we are writing
to fill it so all the blanks or all the
null values get replaced with the mean
value now one of the reasons for doing
this is that very often if you have some
such situation many of your statistical
functions may not even work so that's
the reason you need to fill up these
values or either get rid of these
records or fill up these values with
something meaningful so this is one
mechanism which is basically using a
mean there are few others as we move
forward we can see what are the other
ways for example we can also say that
any missing value in a particular row if
even one column the value is missing you
just drop that particular row or delete
all rows where even a single column has
missing values so that is one way of
dealing now the problem here can be that
if a lot of data has let's say one or
two columns missing and we drop many
such rows then overall you may lose out
on let's say 60 of the data as some
value or the other missing sixty percent
of the rows then it may not be a good
idea to delete all the rows like in that
manner because then you're losing pretty
much 60 percent of your data Therefore
your analysis won't be accurate but if
it is only five or ten percent then this
will work underway is only to drop
values where or rather dropped rows
where all the columns are empty which
makes sense because that means that
record is of really no use because it
has no information in it so there can be
some situations like that so we can
provide a condition saying that drop the
records where all the columns are blank
or not applicable we can also specify
some kind of a threshold let's say you
have 10 or 20 columns in a row you can
specify that maybe five columns are
blank or null then you drop that record
so again we need to take care that such
a condition such a situation the amount
of data that has been removed or
excluded is not large if it is like
maybe five percent maximum 10 percent
then it's okay but by doing this if you
are losing out on a large chunk of data
then it may not be a good idea you need
to come up with something better what
else we need to do next is the data
preparation part is done so now we get
into the data mining part so what
exactly we do in data mining primarily
we come up with ways to take meaningful
decisions so data mining will give us
insights into the data what is existing
there and then we can do additional
stuff like maybe machine learning and so
on to get perform Advanced analytics and
so on so one of the first steps we do is
what is known as data Discovery and
which is basically like exploratory
analysis so we can use tools like
Tableau for doing some of this so let's
just take a quick look at how we go
about that so Tableau is excellent data
mining or actually more of a reporting
or a bi tool and you can download a
trial version of Tableau at tableau.com
or there is also a tableau public which
is free and you can actually use an and
play around however if you want to use
it for Enterprise purpose then it is a
commercial software so you need to
purchase license and you can then run
some of the data mining activities say
your data source your data is in some
Excel sheet so you can select the source
as Microsoft Excel or any other format
and the data will be brought into the
Tableau environment and then it will
show you what is known as dimensions and
measures so dimensions are all the
descriptive columns so and Tableau is
intelligent enough to actually identify
these dimensions and measure so measures
are the numerical value so as you can
see here our customer ID gender
geography these are all Dimensions
non-numerical values whereas age balance
credit score and so on are numeric
values so they come under measures so
you got your data into Tableau and then
you want to let's say build a small
model and you want to let's say solve a
particular problem so what is the
problem problem statement all right
let's say we want to analyze why
customers are leaving the bank which is
known as exit and we want to analyze and
see what are some of the factors for
exiting the bank and we want to Let's
assume consider these three of them like
let's say gender credit card and
geography these as a criteria and
analyze if these are in any way
impacting or have some bearing on the
customer exiting or the customer exit
Behavior okay so let's use Tableau and
very quickly we will be able to find out
how these parameters are affecting all
right so let's see so this is our
customer data so from our Excel sheet we
have data set about let's say 10 000
rows and we want to find out what is the
criteria let's start with gender let's
say we want to first use gender as a
criteria so Tableau really offers an
easy drag and drop kind of a mechanical
so that makes it really really easy to
perform this kind of analysis so what we
need to do is exit it says whether the
customer has exited or not so it has a
value of 0 and 1 and then of course you
have gender and so on so we will take
these two and simply drag and drop okay
so exit it and then we will put gender
and if we drag and drop it to the
analysis side of tabloop all right so
here what we are doing is we are showing
male female as two different columns
here and zero for people who did not
exist and one for people who exited and
that is color coded so the blue color
means people who did not exist and this
yellow color means people who did exit
all right so now if we pull the data
here create like bar graphs this is how
it would look so what is yellow let's go
back so yellow is uh who exited and for
the mail only 16.45 percent have exited
and we can also to draw a reference line
that will help us or even provide
aliases so these are a lot of fancy
stuff that is provided by Tableau you
can create aliases and so that it looks
good rather than basic labels and you
can also add a reference line so you add
a reference line something like this
from here we can make out that on an
average female customers exit more than
the male customers right so that is what
we are seeing here on an average so we
have analyzed based on gender we do see
that there is some difference in the
male and female Behavior now let's take
the next criteria which is the credit
card so let's see if having a credit
card has any impact on the customer exit
Behavior so just like before we drag and
drop the credit card has credit card a
column if we drag and drop here and then
we will see that there is pretty much no
difference between people having credit
card and not having credit card
20.81 percent of people who have credit
card have exited and similarly 20.18
percent of people who have credit card I
have also exited so the credit card is
not having much of an impact that's what
this piece of analysis shows last we
will basically go and check how the
geography is impacting so once again we
can drag and drop geography column onto
this side and if we see here there are
geographies like I think there are about
three geographies like France Germany
and Spain and we see that there is some
kind of uh impact with the geography as
well okay so what we derive from this is
that the credit card is really we can
ignore the credit card variable or
feature from our analysis because that
doesn't have any impact but gender and
geography we can keep and do further
analysis okay all right so what are some
of the advantages of data mining bit
more detail analysis can help us in
predicting the future Trends and it also
helps in identifying customer Behavior
patterns okay so you can take informed
decisions because the data is telling
you or providing you with some insights
and then you take a decision based on
that if there is any fraudulent activity
data mining will help in quickly
identifying such a fraud as well and of
course it will also help us in
identifying the right algorithm for
performing more Advanced Data Mining
activities like machine learning and so
on all right so the next activity now
that we have the data we have prepared
the data and performed some data mining
activity the next step is model building
let's take a look at model building so
what is a model building if we want to
perform a more detailed data mining
activity like maybe perform some machine
learning then you need to build a model
and how do you build a model first thing
is you need to select which algorithm
you want to use to solve the problem on
hand and also what kind of data that is
available and so on and so forth so you
need to make a choice of the algorithm
and based on that you go ahead and
create a model train the model and so on
now machine learning is kind of at a
very high level classified into
supervised and unsupervised so if we
want to predict a continuous value could
be a price or a temperature or a height
or a length or things like that so those
are continuous values and if you want to
find some of those then you use
techniques like regression linear
regression simple linear regression
multiple linear regression and so on so
these are the algorithms on the other
hand there will be situations or there
may be situations where you need to
perform unsupervised learning case of
unsupervised learning you don't have any
historical labeled data so to learn from
so that is when you use unsupervised
learning and some of the algorithms in
unsupervised learning are clustering K
means clustering is the most common
algorithm used in unsupervised learning
and similarly in supervised learning if
you want to perform some activity on
categorical values like for example that
is not measured but it is counted like
you want to classify whether this image
is a cat or a dog whether you want to
classify whether this customer will buy
the product or not or if you want to
classify whether this email is a spam or
not spam so these are examples of
categorical values and these are
examples of classification then you have
algorithms like logistic regression K
nearest neighbor or knnn and support
Vector machine so these are some of the
algorithms that are used in this case
and similarly in case of unsupervised
learning if you need to perform on
categorical values you have some
algorithms like Association analysis and
hidden Markov model okay so in order to
understand this better let's take an
example I and take you through the whole
process and then we will also see how
the code can be written to perform this
now let's take our example here where we
want to perform a supervised learning
which is basically we want to do a
multi-linear regression which means
there are multiple independent variables
and then you want to perform a linear
regression to predict certain value so
in this particular example we have World
happiness data so this is the data about
the happiness quotient of people from
various countries and we are trying to
predict and see whether our how our
model will perform so what is the
question that we need to ask first of
all how to describe the data and then
can we make a predictive model to
calculate the happiness score right so
based on this we can then decide on what
algorithm to use and what model to use
and so on so variables that are
available or used in this model this is
a list of variables that are available
there is a happiness rank I'll load the
data and or I'll show you the data in
little bit so it becomes clear what are
these so that is what is known as
happiness rank happiness score which is
happiness score is more like absolute
value whereas rank is what is the
ranking and then which country we are
talking about and within that country
which region and what kind of economy
and whether the family which family and
health details and freedom trust
generosity and so on and so forth so
there are multiple variables that are
available to us and the specific details
probably are not required and there can
be
um in another example the variables can
be completely different so we don't have
to go into the details of what exactly
these variables are but it's just enough
to understand that we have a bunch of
these variables and now we need to use
either all or some of these variables
and then which we also sometimes refer
to as features and then we need to build
our model and train our model all right
so let's assume we will use python in
order to perform this analysis or
perform this machine learning activity
and I will actually show you in our lab
in in a little bit this whole thing we
will run the Live code but quickly I
will run you through the slides and then
we will go into the lab so what are we
doing here first thing we need to do is
import a bunch of libraries in Python
which are required to perform our
analysis most of these are for
manipulating the data the preparing the
data and then scikit learn or sklearn is
the library which you will use actually
for this particular machine learning
activity which is linear regression so
we have numpy we have pandas and so on
and so forth so all these libraries are
imported and then we load our data and
the data is in the form of a CSV file
and there are different files for each
year so we have data for 2015 16 and 70
and so we will load this data and then
combine them concatenate them to prepare
a single data frame and here we are
making an assumption that you are
familiar with python so it becomes
easier if you are familiar with Python
programming language or at least some
programming language so that you can at
least understand by looking at the code
so we are reading the file each of these
files for each year and this is
basically we are creating a list of all
the names of the columns that we will be
using later on we will see in the code
so we have loaded 2015 then 2016 and
then also 2017. so we have created three
data frames and then we concatenate all
these three data frames this is what we
are doing here then we identify which of
these columns are required which for
example some of the categorical values
do we really need we probably don't then
we drop those columns so that we don't
unnecessarily use all the columns and
make the computation complicate we can
then create some plots using plotly
library and it has some powerful
features including creation or creation
of maps and so on just to understand the
pattern the happiness quotient or how
the happiness is across all the
countries so it's a nice visualization
we can see each of these countries how
they are in terms of their happiness
score this is the legend here so the
lighter colored countries have lower
ranking and so these are the lower
ranking ones and these are higher
ranking which means that the ones with
these dark colors are the happiest ones
so as you can see here Australia and
maybe this side uh us and so on are the
happiest ones okay the other thing that
we need to do is the correlation between
the happiness score and happiness rank
we can find a correlation using a
scatter plot and we find that yes they
are kind of inversely proportioned which
is obvious so if the score is high
happiness score is high then they are
ranked number one for example highest
scored as number one so that's the idea
behind this so the happiness score given
here and the happiness rank is actually
given here so they are inversely
proportional because the higher the
score the absolute value of the rank
will be lower right so number one has
the highest value of the score and so on
so they are inversely correlated but
there is a strong what this graph shows
is that there is a strong correlation
between happiness Rank and happiness
score and then we do some more plots to
visualize this we determine that
probably Rank and score are pretty much
conveying the same message so we don't
need both of them so we will kind of
drop one of them and that is what we are
doing here so we drop the happiness Rank
and similarly so this is one example of
how we can remove some columns which are
not adding value so we will see in the
code as well how that works moving on
this is a correlation between pretty
much each of the columns with the other
columns so this is a correlation you can
plot using plot function and we will see
here that for example happiness score
and happiness score are correlated
strong score relation right because
every variable will be highly correlated
to itself so that's the reason so the
darker the color is the higher the
correlation and as so the and
correlation in numerical terms goes from
0 to 1. so 1 is the highest value and it
can only be between 0 and 1 correlation
between two variables can be only have a
value between 0 and 1. so the numerical
value can go from 0 to 1 and 1 here is
dark color and 0 is kind of dark but it
is blue colors from Red it goes down the
dark blue color indicates pretty much no
correlation so from this heat map we see
that happiness and economy and family
are probably also Health probably are
the most correlated and then it keeps
decreasing after Freedom kind of keeps
decreasing and coming to predict
zero all right so that is a correlation
graph and then we can probably use this
to find out which are the columns that
need to be dropped which do not have
very high correlation and we take only
those columns that we will need so this
is the code for dropping some of the
columns once we have prepared the data
when we have the required columns then
we use scikit-learn to actually split
the data first of all this is a normal
machine learning process you need to
split the data into training and test
data set in this case we are splitting
into 80 20 so 80 is the training data
set and 20 is the test data set so
that's what we are doing here so we use
train test flip method or function so
you have all your training data in X
underscore train the labels in y
underscore train similarly X underscore
test has the test data the inputs
whereas the labels are in y underscore
test so that's how and this value
whether it is 80 20 or 50 50 that is all
individual preference so in our case we
are using 80 20. all right and then the
next is to create a linear regression
instance so this is what we are doing we
are creating an instance of linear
regression and then we train the model
using the fit function and we are
passing X and Y which is the x value and
the label data regular input and the
label data label if information then we
do the test We Run The or we perform the
evaluation on the test data set so this
is what we are doing with the test data
set and then we will evaluate how
accurate the model is and using the
scikit-learn functionality itself we can
also see what are the various parameters
and what are the various coefficients
because in linear regression you will
get like a equation of like a straight
line Y is equal to beta0 plus beta 1 x 1
plus beta 2 x 2 so those beta 1 beta 2
Beta 3 are known as the coefficients and
beta0 is The Intercept after the
training you can actually get these
information of the model what is The
Intercept value what are the
coefficients and so on by using these
functions so let's take quickly go into
the lab and take a look at our code okay
so this is my lab this is my Jupiter
notebook where the code I have the
actual code and I will take you through
this code to run this linear regression
on the world happiness data so we will
import a bunch of libraries numpy pandas
plot plotly and so on also yeah
scikit-learn that's also very important
so that's the first step then I will
import my data and the data is in three
parts there are three files one for each
year 2015 2016 and 2017 and it is a CSV
file so I've imported my data let's take
a look at the data quickly glance at
data so this is how it looks we have the
country region happiness Rank and then
happiness score there are some standard
errors and then what is the per capita
family and so on so and then we will
keep going we will create a list of all
these column names we will be using
later so for now just I will run this
code and no need of major explanation at
this point we know that some of these
columns probably are not required so you
can use this drop functionality to
remove some of the columns which we
don't need like for example region and
standard error will not be contributing
to our model so we will basically drop
those values out here so we use the drop
and then we created a vector with these
names column names that's what we are
passing here instead of giving the names
of the columns here we can pass a vector
so that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also 2017 and then
we will concatenate this data so let's
do that so we have now data frame called
Happiness which is a concatenation of
both all the three files let's take a
quick look at the data now so most of
the unwanted columns have been removed
and you have all the data in one place
for all the three years and this is how
the data looks and you want to take a
look at the summary of The Columns you
can say describe and you will get this
information for example for each of the
columns what is the count what's what is
the mean value standard deviation
especially the numeric values okay not
the categorical values so this is a
quick way to see how the data is and
initial little bit of exploratory
analysis can be done here so what is the
maximum value what's the minimum value
and so on for each of the columns all
right so then we go ahead and create
some visualizations using plotly so let
us go and build a plot so if we see here
now this is the relation correlation
between happiness Rank and happiness
score this is what we have seen in the
slides as well we can see that there is
a tight correlation between them only
thing is it is inverse correlation but
otherwise they are very tightly
correlated which also says that they
both probably provide the same
information so there is no no not much
of value add so we'll go ahead and drop
the happiness rank as well from our
columns so that's what we are doing here
and now we can do the creation of the
correlation heat map let us plot the
correlation heat map to see how each of
these columns is correlated to the
others and we as we have seen in the
slides this is how it looks so happiness
score is very highly correlated so this
is the legend we have seen in the slide
as well so blue color indicates pretty
much zero or very low correlation deep
red color indicates very high
correlation and the value correlation is
the numeric value and the value goes
from 0 to 1. if the two items or two
features or columns are highly
correlated then they will be as close to
one as possible and two columns that are
not at all correlated will be as close
to zero as possible so that's how it is
for example here happiness score and
happiness score every column or every
feature will be highly correlated to
itself so it is like between them there
will be correlation value will be one so
that's why we see deep red color but
then others are for example with higher
values are economy and then health and
then maybe family and freedom so these
are generosity and Trust are not very
highly correlated to happiness score so
that is uh one quick exploratory
analysis you can do and therefore we can
drop the country and the happiness rank
because they also again don't have any
major impact on the analysis on our
analysis so now we have prepared our
data there was no need to clean the data
because the data was clean but if there
were some missing values and so on as we
have discussed in the slides we would
have had to perform some of the data
cleaning activities as well but in this
case the data was clean all we needed to
do was just the preparation part so we
removed some unwanted columns and we did
some exploratory data analysis now we
are ready to perform the machine
learning activity so we use scikitler
for doing the machine learning circuit
learn is python library that is
available for performing our machine
learning once again we will import some
of these libraries like pandas and numpy
and also scikit-learn first step we will
do is split the data in 2080 format so
you have all the test data which is 20
of the data is test data and 80 percent
is your training data so this test size
indicates how much of it is in the what
is the size of the test data the
remaining which is uh here we are saying
0.2 therefore that means training is 0.8
so training data is eighty percent all
right so we have executed that split the
data and now we create an instance of
the linear regression model so LM is our
linear regression model and we pass X
and Y the training data set and call the
function fit so that the model gets
trained so now once that is done
training is done training is completed
and now what we have to do is we need to
predict the values for the test data so
the next step is using so you see a fit
will basically run the Training Method
predict will actually predict the values
so we are passing the input values which
is the independent variables and we are
asking for the values of the dependent
variable which is which we are capturing
in y underscore thread and we use the
predict method here lm.predict so this
will give us all the predicted y values
and remember we already have y
underscore test has the actual values
which are the labels so that we can use
these two to compare and find out how
much of it is error so that's what we
are doing here we are trying to find the
difference between the predicted value
and the actual value y underscore test
is the actual value for the test data
and Y underscore predict is the
predicted value we just found out the
predictor right so we will run that and
we can do a quick check as to how the
data looks how is the difference so in
some cases it is positive some cases it
is negative but in most of the cases I
think the difference is very small this
is exponential to the power of 0 minus 0
4 and so on so looks like our model has
performed reasonably well we can now
check some of the parameters of our
model like the intercept and the
coefficients so that's what we are doing
here so these are the coefficients of
the various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variables so
the coefficients against the
corresponding independent variable so
1.0051 is the coefficient for economy
0.9983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of scikit-learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that the root mean square error
is pretty low which is a good sign and
which is a one of the measures of how
well our model is performing we can do
one more quick plot to just see how the
actual values and the predicted values
are looking and once again you can see
that as we we have seen from the root
mean square error root mean square error
is very very low that means that the
actual values and the predicted values
are pretty much matching up almost
matching up and this plot also shows the
same so this line is going through the
predicted values and the actual values
and the differences very very low so
again this is actually later this is one
example where the accuracy is high and
the predicted values are pretty much
matching with the actual values but in
real life you may find that these values
are a slightly more scattered and you
may get the error value can be
relatively On The Higher Side the root
mean Square okay so this was a good and
quick example of the code to perform
data science activity or machine
learning or data mining activity in this
case we did what is known as linear
regression so let's go back to our
slides and see what else is there so we
saw this these are the coefficients of
each of the features in our code and we
have seen the root mean square error as
well and with we can take a few hundred
countries certain values and actually
predict to see if how the model is
performing and I think we have done this
as well and in this case as we have seen
pretty much the predicted values and the
actual values are pretty much matching
which means our model is almost 100
accurate as I mentioned in real life it
may not be the case but in this
particular case we have got a pretty
good model which is very good also
subsequently we can assume that this is
how the equation in linear regression
the model is nothing but an equation
like Y is equal to beta0 plus beta 1 x 1
plus beta 2 x 2 where plus beta 3 x 3
and so on so this is what we are showing
here so this is our intercept which is
beta0 and then we have beta 1 into
economy value beta 2 into the family
value beta 3 into health value and so on
so that is what is shown here okay so I
think the next step once we have the
result from the data mining or machine
learning activity the next step is to
communicate these results to the
appropriate stakeholders so that is what
we will see here now so how do we
communicate usually you take these
results and then either prepare a
presentation or put it in a document and
then show them these actionable results
or actionable insights and you need to
find out who are your target audience
and put all the results in context and
maybe if there was a problem statement
you need to put this results in the
context of the problem statement what
was our initial goal that we wanted to
achieve so that we need to communicate
here based on remember we started off
with what is the question and what is
the data and so on and then what is the
answer so we we need to put the results
and then what is the methodology that we
have used all that has to be input and
clearly communicated in business terms
so that the people understand very well
from a business perspective so once the
model building is done once the results
are published and communicated the last
part is maintenance of this model now
very often what can happen is the model
may have to be subsequently updated or
modified because of multiple reasons
either the the data has changed the way
the data comes has changed or the
process has changed or for whatever
reason the accuracy may keep changing
once you have a trained model the for
example we got a very high accuracy but
then over a period of time there can be
various factors which can cause that so
from time to time we need to check
whether the model is performing well or
not the accuracy needs to be tested once
in a while and if required you may have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model the latest data that
you have and then you deploy it you
build the model train it and then you
deploy it so that is like the
maintenance cycle that you may have to
take the model data analyst versus data
engineer versus data scientist which one
to choose
this is one of the most popular
questions asked by Learners looking for
a career in data and Analytics
I'm sure YouTube would have come across
these job roles in the ever-growing data
science landscape though they all deal
with data these jobs are not the same
there are significant differences
between what a data analyst data
engineer and a data scientist does
we will look at these job rules and the
differences in detail
first
let's look at some data analytics and
data science trends
the analytics and data science Market is
thriving
data analytics data engineering and data
science are the key trends in today's
accelerating Market as per statista.com
the global big data analytics Market
Revenue will grow at a Cher of 30
percent with Revenue reaching over 68
billion US Dollars by 2025.
according to Tech navio the Enterprise
data management Market is expected to
increase by
64.08 billion US Dollars by 2025 as per
markets and markets.com the big data
market size is projected to grow
from 162.6 billion US dollars in 2021 to
273.4 billion US dollars in 2026.
now another report from Research Drive
says that the data science platform
Market is estimated to reach
224.3 billion US Dollars by 2026.
so with so much data available and
companies making huge Investments to
drive business insights the job
opportunities for data analysts data
engineers and data scientists are going
to increase in 2022 and over the coming
years
now
let's learn the major differences
between data analyst versus data
engineer versus data scientist
so who are they
a data analyst analyzes and interprets
vast volumes of data in order to extract
meaningful information out of it
they find solutions to a business
problem and make critical business
decisions
the insights provided by data analysts
are important to companies that want to
understand the needs of their end
customers
we're talking about who are data
Engineers a data engineer on the other
hand builds infrastructure and scalable
pipelines to manage the flow of data and
prepare it for analysis
so basically they optimize the systems
that enable data analysts and data
scientists to perform their job
efficiently
data scientists are professionals who
analyze and visualize existing data and
use algorithms to build predictive
models for making future decisions
they also engage with Business Leaders
to understand their needs and present
complex findings
with that let's look at the primary
roles and responsibilities of
these three job roles
data analysts are responsible to collect
clean store and process data the
Discover hidden patterns from data by
performing exploratory data analysis and
visualize data by creating charts and
graphs
acquiring data from primary and
secondary sources is one of their key
tasks
the build reports and dashboards and
also maintain databases
now talking about the roles and
responsibilities of a data engineer
a data engineer performs data
acquisition the design build and test
data as well to develop and maintain
data architecture
data Engineers are tasks with testing
integrating managing and optimizing data
from a variety of sources so they
integrate data into existing data
pipelines prepare data for modeling and
perform various ETL operations
now talking about the roles and
responsibilities of a data scientist
so data scientists develop machine
learning models to identify Trends in
data for making decisions
that develop hypothesis and use the
knowledge of Statistics data
visualization and machine learning to
forecast the future for the business
data scientists visualize data and use
storytelling techniques and also write
programs to automate data collection and
processing
now move on to the skills possessed by
data analysts data engineers and data
scientists
to become a data analyst you need to
have good hands-on experience with
writing SQL queries
you should have excellent Microsoft
Excel skills for analyzing data data
analysts are also good at programming
and they need to know how to visualize
data solve business problems and possess
domain knowledge
data Engineers should have a solid
understanding of SQL mongodb and
programming
they need to have a good command of data
architecture scripting data warehousing
and ETL data Engineers are also good at
Hadoop based Analytics
now talking about the skills for a data
scientist so a data scientist should
have experience with programming in
Python and r
this would have a very good
understanding of mathematics and
statistics as well
data scientists need to possess
analytical thinking and data
visualization skills as well
machine learning deep learning and
decision making are other critical
skills every data scientist should have
now we look at the salaries of a data
scientist a data analyst as well as a
data engineer
so a data analyst in the United States
earns over seventy thousand dollars per
annum while in India a data analyst can
earn nearly 7 lakh 25 000 rupees per
annum
a data engineer in the United States can
earn over 112 500 per year and in India
you can earn over 9 lakh rupees per
annum
talking about the salary of a data
scientist a data scientist in the United
States earns over 117 thousand dollars
per annum and in India a data scientist
can earn over 11 lakh rupees per annum
so coming to the final section of this
video we'll look at the top companies
hiring for data analysts data engineers
and data scientists
so we have the first company as Google
then we have Tesla next we have the
e-commerce giant Amazon
the internet Giant
Facebook or the social media giant
Facebook we have the tech giant Oracle
we also have Verizon and Airbnb so these
are some of the top companies that hire
for the three roles if you are an
aspiring data scientist who is looking
out for online training and
certification in data science from the
best universities and Industry experts
then search now on simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
now let's talk about the life cycle of a
data science project okay the first step
is the concept study in this step it
involves understanding the business
problem asking questions get a good
understanding of the business model meet
up with all the stakeholders understand
what kind of data is available and all
that is a part of the First Step so here
are a few examples we want to see what
are the various specifications and then
what is the end goal what is the budget
is there an example of this kind of a
problem that has been maybe solved
earlier so all this is a part of the
concept study and another example could
be a very specific one to predict the
price of a 1.35 carat diamond and there
may be relevant information inputs that
are available and we want to predict the
price the next step in this process is
data preparation data Gathering and data
preparation also known as data munging
or sometimes it is also known as data
manipulation so what happens here is the
raw data that is available may not be
usable in its current format for various
reasons so that is why in this step a
data scientist would explore the data he
will take a look at some sample data
maybe there are millions of Records pick
a few thousand records and see how the
data is looking are there any gaps is
the structure appropriate to be fed into
the system are there some columns which
are probably not adding value may not be
required for the analysis very often
these are like names of the customers
they will probably not add any value or
much value from an analysis perspective
this structure of the data Maybe the
data is coming from multiple data
sources and the structures may not be
matching what are the other problems
there may be gaps in the data so the
data all the columns all the cells are
not filled if you're talking about
structured data there are several blank
records or blank columns so if you use
that data directly you'll get errors or
you will get inaccurate results so how
do you either get rid of that data or
how do you fill this gaps with something
meaningful so all that is a part of data
munging or data manipulation so these
are some additional sub topics within
that so data integration is one of them
there are any conflicts in the data
there may be data may be redundant yeah
data resonant redundancy is another
issue then maybe you have let's say data
coming from two different systems and
both of them have customer table for
example customer information so when you
merge them there is a duplication issue
so how do we resolve that so that is one
data transformation as I said there will
be situations where data is coming from
multiple sources and then when we merge
them together they may not be matching
so we need to do some transformations to
make sure everything is similar we may
have to do some data reduction
data sizes too big you may have to come
up with ways to reduce it meaningfully
without losing information then data
cleaning so there will be the wrong
values or your null values or missing
values so how do you handle all of that
a few examples of very specific stuff so
if there are missing values how do you
handle missing values or null values
here in this particular slide we are
seeing three types of issues one is
missing value then you have null value
you see the difference between the two
right so in a missing value there is
nothing blank null value it says null
now the system cannot handle if there
are null values similarly there is
improper data so it's supposed to be
numeric value but there is a string or a
non-numeric value so how do we clean and
prepare the data so that our system can
work flawlessly so there are multiple
ways and there is no one common way of
doing this it can vary from Project to
project it can vary from what exactly is
the problem we are trying to solve it
can vary from data scientist to data
scientist organization to organization
so these are like some standard
practices people come up with and and of
course there will be a lot of trial and
errors somebody would have tried out
something and it worked and it'll
continue to use that mechanism so that's
how we need to take care of data
cleaning now what are the various ways
of doing you know if values are missing
how do you take care of that now if the
data is too large and only a few records
have some missing values then it is okay
to just get rid of those entire rows for
example so if you have a million records
and out of which 100 records don't have
full data so there are some missing
values in about 100 cards so it's
absolutely fine because it's a small
percentage of the data so you can get
rid of the entire records which are
missing values but that's not a very
common situation very often you will
have multiple or at least you know large
number of a data set for example a lot
of million records you may have 50 000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so so
how do you handle such situations so
there are again multiple ways of doing
it one is you can probably if a
particular values are missing in a
particular column you can probably take
the mean value for that particular
column and fill all the missing values
with the mean value so that first of all
you don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taking the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation you need to split the
data into training and test data set the
reason being if you try to test with a
data set which the system has already
seen as a part of training then it will
tend to give a reasonably accurate
results because it has already seen that
and that is not a good measure of the
accuracy of the system so typically you
take the entire data set the input data
set and split it into two parts and
again the ratio can vary from person to
person individual preferences some
people like to split it into 50 50 some
people like it as
63.33 and 33.3 is basically two third
and one third and some people do it as
80 20 80 for training and 24 testing so
you split the data perform the training
with the 80 percent and then use the
remaining 20 for testing all right so
that is one more data preparation
activity that needs to be done before
you start analyzing or applying the data
or putting the data through the model
then the next step is model planning now
this models can be statistical models
this could be machine learning models so
you need to decide what kind of models
you're going to use again it depends on
what is the problem you're trying to
solve if it is a regression problem you
need to think of a regression algorithm
and come up with a regression model so
it could be linear regression or if you
are talking about classification then
you need to pick up an appropriate
classification algorithm like logistic
regression or decision tree or svm and
then you need to train that particular
model so that is the model building or
model planning process and the cleaned
up data has to be fed into the model and
apart from cleaning you may also have to
in order to determine what kind of model
you will use you have to perform some
exploratory data analysis to understand
the relationship between the various
variables and see if the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data analysis
so what exactly is exploratory data
analysis is basically to as the name
suggests you're just exploring you just
receive the data and you're trying to
explore and find out what are the data
types and what is the is the data clean
in in each of the columns what is the
maximum minimum value so for example
there are out of the box functionality
available in tools like R so if you just
ask for a summary of the table it will
tell you for each column it will give
some details as to what is the mean
value what is the maximum value and so
on so this exercise or this
exploratory analysis is to get an
understanding of your data and then you
can take steps to during this process
you find there are a lot of missing
values you need to take step fix those
you will also get an idea about what
kind of model to be used and so on and
so forth what are the various techniques
used for exploratory data analysis
typically these would be visualization
techniques like you use histograms you
can use box plots you can use Scatter
Plots so these are very quick ways of
identifying the patterns or a few of the
trends of the data and so on and then
once your data is ready you you decided
on the model what kind of model what
kind of algorithm you're going to use if
you're trying to do machine learning you
need to pass your 80 the training data
or rather you use the training data to
train your model and the training
process itself is iterative so the
training process you may have to perform
multiple times and once the training is
done and you feel it is giving good
accuracy then you move on to test so you
take the remaining 20 of the data
remember we split the data into training
and test so the test data is now used to
check the accuracy or how well our model
is performing and if there are further
issues let's say a model is still during
testing in the accuracy is not good then
you may want to retrain your model or
use a different model so this whole
thing again can be iterative but if the
test process is is passed or if the
model passes the test then it can go
into production and it will be deployed
all right so what are the various tools
that we use for model planning R is an
excellent tool in a lot of ways whether
you're doing regular statistical
analysis or machine learning or any of
these activities are in along with our
studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detailed analysis and
machine learning and so phone then of
course you can write python programs
python offers a rich library for
performing data analysis and machine
learning and so on Matlab is a very
popular tool as well especially during
education so this is a very easy to
learn tool so Matlab is another tool
that can be used and then last but not
least SAS SAS is again very powerful it
is proprietary tool and it has all the
components that are required to perform
very good statistical analysis or
perform data science so those are the
various tools that would be required for
or that that can be used for model
building and so the next step is model
building so we have done the planning
part we said okay what is algorithm we
are going to use what kind of model we
are going to use now we need to actually
train this model or build the model
rather so that it can then be deployed
so what are the various ways or where
what are the various types of model
building activities so it could be let's
say in this particular example that we
have taken you want to find out the
price of 1.35 carat diamond so this is
let's say a linear regression problem
you have data for various carrots of
diamond and you use that information you
pass it through a linear regression
model or you create a real linear
regression model which can then predict
your price for 1.35 carat so this is one
example of model building and then
little bit details of how linear
regression works so linear regression is
basically coming up with a relation
between an independent variable and a
dependent variable so it is pretty much
like coming up with equation of a
straight line which is the best fit for
the given data so like for example here
Y is equal to MX plus C so Y is is the
dependent variable and X is the
independent variable we need to
determine the values of M and C for our
given data so that is what the training
process of this model does at the end of
the training process you have a certain
value of M and c and that is used for
predicting the values of any new data
that comes all right so the way it works
is we use the training and the test data
set to train the model and then validate
whether the model is working fine or not
using test data and if it is working
fine then it is taken to the next level
which is put in production if not the
model has to be retrained if the 3C is
not good enough then the model is
retrained maybe with more data or you
come up with a newer model or algorithm
and then repeat the process so it is an
iterative process once the training is
completed training and test and this
model is deployed and we can use this
particular model to determine what is
the price of 1.35 carat diamond remember
that was our problem statement so now
that we have the best fit for this given
data we have the price of 1.35 carat
diamond which is 10 000 so this is one
example of how this whole process works
now how do we build the model there are
multiple ways you can use Python for
example and use libraries like pandas or
numpy to build a model and implement it
this will be available as a separate
tutorial a separate video in this
playlist so stay tuned for that moving
on once we have the results the next
step is to communicate this results to
the appropriate stakeholders so which is
basically taking this results and
preparing like a presentation or or a
dashboard and communicating these
results to the concerned people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and present it to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to overcome this problem
or solve this problem so that is the
pretty much once that is accepted and
the last step is to operationalize so if
everything is fine your data scientists
presentations are accepted then they put
it into factors and thereby they will be
able to improve or solve the problem
that they stated in step one okay so
quick summary of the life cycle you have
a concept study which is basically
understanding the problem asking the
right questions and trying to see if
there is enough data to solve this
problem and then even maybe gather the
data then data preparation the raw data
needs to be manipulated you need to do
data managing so that you have a data in
a certain proper format to be used by
the model or our analytics system and
then you need to do the model planning
what kind of a model what algorithm you
will use for a given problem and then
the model building so the exact
execution of that model happens in step
four and you implement and execute that
model and put the data through the
analysis in this step and then you get
the results this results are then
communicated packaged and presented and
communicated to the stakeholders and
once that is accepted that is
operationalized so that is the finals
let's begin this lesson by defining the
term statistics statistics is a
mathematical science pertaining to the
collection presentation analysis and
interpretation of data it's widely used
to understand the complex problems of
the real world and simplify them to make
well-informed decisions
several statistical principles functions
and algorithms can be used to analyze
primary data build a statistical model
and predict the outcomes
an analysis of any situation can be done
in two ways statistical analysis or a
non-statistical analysis
statistical analysis is the science of
collecting exploring and presenting
large amounts of data to identify the
patterns and Trends statistical analysis
is also called quantitative analysis
non-statistical analysis provides
generic information and includes text
sound still images and moving images
non-statistical analysis is also called
qualitative analysis
although both forms of analysis provide
results statistical analysis gives more
insight and a clearer picture a feature
that makes it vital for businesses
there are two major categories of
Statistics descriptive statistics and
inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set
suppose you want to study the height of
students in a classroom in the
descriptive statistics you would record
the height of every person in the
classroom and then find out the maximum
height minimum height and average height
of the population
inferential statistics generalizes the
larger data set and applies probability
Theory to draw a conclusion it allows
you to infer population parameters based
on the sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the inner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential statistics
you would categorize height as tall
medium and small and then take only a
small sample from the population to
study the height of students in the
classroom
the field of Statistics touches our
lives in many ways from the daily
routines in our homes to the business of
making the greatest cities run the
effect of Statistics are everywhere
there are various statistical terms that
one should be aware of while dealing
with statistics
population sample variable quantitative
variable qualitative variable discrete
variable continuous variable
a population is the group from which
data is to be collected
a sample is a subset of a population
a variable is a feature that is
characteristic of any member of the
population differing in quality or
quantity from another member
a variable differing in quantity is
called a quantitative variable for
example the weight of a person number of
people in a car
a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an accident
a discrete variable is one which no
value can be assumed between the two
given values for example the number of
children in a family
a continuous variable is one in which
any value can be assumed between the two
given values for example the time taken
for a 100 meter run
typically there are four types of
statistical measures used to describe
the data they are measures of frequency
measures of central tendency measures of
spread measures of position
let's learn each in detail
frequency of the data indicates the
number of times a particular data value
occurs in the given data set
the measures of frequency are number and
percentage
central tendency indicates whether the
data values tend to accumulate in the
middle of the distribution or toward the
end
the measures of central tendency are
mean median and mode
spread describes how similar or varied
the set of observed values are for a
particular variable
the measures of spread are standard
deviation variance and quartiles the
measure of spread are also called
measures of dispersion
position identifies the exact location
of a particular data value in the given
data set
the measures of position are percentiles
quartiles and standard scores
statistical analysis system or SAS
provides a list of procedures to perform
descriptive statistics they are as
follows
proc print proc contents proc means proc
frequency proc univariate proc G chart
proc box plot proc G plot
proc print it prints all the variables
in a SAS data set
proc contents it describes the structure
of a data set
proc means it provides data
summarization tools to compute
descriptive statistics for variables
across all observations and within the
groups of observations
proc frequency it produces one way to
in-way frequency and cross tabulation
tables frequencies can also be an output
of a SAS data set
proc univariate it goes beyond what proc
means does and is useful in conducting
some basic statistical analyzes and
includes high resolution graphical
features
proc G chart the g-chart procedure
produces six types of charts block
charts horizontal vertical bar charts Pi
donut charts and star charts
these charts graphically represent the
value of a statistic calculated for one
or more variables in an input SAS data
set
the trig variables can be either numeric
or character
proc box plot the box plot procedure
creates side-by-side box and whisker
plots of measurements organized in
groups a box and whisker plot displays
the mean quartiles and minimum and
maximum observations for a group
proc G plot G plot procedure creates
two-dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axis plots
against the second vertical axis bubble
plots and logarithmic plots
in this demo you'll learn how to use
descriptive statistics to analyze the
mean from the electronic data set
let's import the electronic data set
into the SAS console
in the left plane right-click the
electronic.xl SX dataset and click
import data
the code to import the data generates
automatically copy the code and paste it
in the new window
the proc means procedure is used to
analyze the mean of the imported data
set
the keyword data identifies the input
data set in this demo the input data set
is electronic
the output obtained is shown on the
screen
note that the number of observations
mean standard deviation and maximum and
minimum values of the electronic data
set are obtained
this concludes the demo on how to use
descriptive statistics to analyze the
mean from the electronic data set so far
you've learned about descriptive
statistics let's now learn about
inferential statistics
hypothesis testing is an inferential
statistical technique to determine
whether there is enough evidence in a
data sample to infer that a certain
condition holds true for the entire
population
to understand the characteristics of the
general population we take a random
sample and analyze the properties of the
sample we then test whether or not the
identified conclusions correctly
represent the population as a whole
the population of hypothesis testing is
to choose between two competing
hypotheses about the value of a
population parameter
for example one hypothesis might claim
that the wages of men and women are
equal while the other might claim that
women make more than men
hypothesis testing is formulated in
terms of two hypotheses null hypothesis
which is referred to as H null
alternative hypothesis which is referred
to as H1
the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary
the alternative hypothesis assumed to be
true when the null hypothesis is proven
false
let's understand the null hypothesis and
alternative hypothesis using a general
example
null hypothesis attempts to show that no
variation exists between variables and
alternative hypothesis is any hypothesis
other than the null
for example say a pharmaceutical company
has introduced a medicine in the market
for a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe
if the medicine is proved to be safe
then it is referred to as null
hypothesis
to reject null hypothesis we should
prove that the medicine is unsafe if the
null hypothesis is rejected then the
alternative hypothesis is used
before you perform any statistical tests
with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types
they are categorical or nominal
variables ordinal variables interval
variables and ratio variables
nominal variables are ones which have
two or more categories and it's
impossible to order the values examples
of nominal variables include gender and
blood group
ordinal variables have values ordered
logically however the relative distance
between two data values is not clear
examples of ordinal variables include
considering the size of a coffee cup
large medium and small and considering
the ratings of a product bad good and
best
interval variables are similar to
ordinal variables except that the values
are measured in a way where their
differences are meaningful
with an interval scale equal differences
between scale values do have equal
quantitative meaning
for this reason an interval scale
provides more quantitative information
than the ordinal scale
the interval scale does not have a true
zero point a true zero point means that
a value of zero on the scale represents
zero quantity of the construct being
assessed
examples of interval variables include
the Fahrenheit scale used to measure
temperature and distance between two
compartments in a train
ratio scales are similar to interval
scales and that equal differences
between scale values have equal
quantitative meaning
however ratio scales also have a true
zero point which give them an additional
property for example the system of
inches used with a common ruler is an
example of a ratio scale
there is a true zero point because zero
inches does in fact indicate a complete
absence of Link
in this demo you'll learn how to perform
the hypothesis testing using SAS
in this example let's check against the
length of certain observations from a
random sample
the keyword data identifies the input
data set
the input statement is used to declare
the Aging variable and cards to read
data into SAS
let's perform a t-test to check the null
hypothesis
let's assume that the null hypothesis to
be that the mean days to deliver a
product is six days
so null hypothesis equals six Alpha
value is the probability of making an
error which is five percent standard and
hence Alpha equals 0.05
the variable statement names the
variable to be used in the analysis
the output is shown on the screen
note that the p-value is greater than
the alpha value which is 0.05 therefore
we fail to reject the null hypothesis
this concludes the demo on how to
perform the hypothesis testing using SAS
let's now learn about hypothesis testing
procedures there are two types of
hypothesis testing procedures they are
parametric tests and non-parametric
tests
in statistical inference or hypothesis
testing the traditional tests such as
t-test and Anova are called parametric
tests they depend on the specification
of a probability distribution except for
a set of free parameters
in simple words you can say that if the
population information is known
completely by its parameter then it is
called a parametric test
if the population or parameter
information is not known and you are
still required to test the hypothesis of
the population then it's called a
non-parametric test
non-parametric tests do not require any
strict distributional assumptions there
are various parametric tests they are as
follows t-test Anova
chi-squared linear regression let's
understand them in detail
t-test
a t-test determines if two sets of data
are significantly different from each
other
the t-test is used in the following
situations
to test if the mean is significantly
different than a hypothesized value
to test if the mean for two independent
groups is significantly different
to test if the mean for two dependent or
paired groups is significantly different
for example
let's say you have to find out which
region spends the highest amount of
money on shopping
it's impractical to ask everyone in the
different regions about their shopping
expenditure
in this case you can calculate the
highest shopping expenditure by
collecting sample observations from each
region
with the help of the t-test you can
check if the difference between the
regions are significant or a statistical
fluke
Anova
Anova is a generalized version of the
t-test and used when the mean of the
interval dependent variable is different
to the categorical independent variable
when we want to check variance between
two or more groups we apply the Anova
test
for example let's look at the same
example of the t-test example now you
want to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical fluke
chai square
chi-square is a statistical test used to
compare observed data with data you
would expect to obtain according to a
specific hypothesis
let's understand the chi-square test
through an example
you have a data set of mail Shoppers and
female shoppers let's say you need to
assess whether the probability of
females purchasing items of 500 or more
is significantly different from the
probability of males purchasing items of
500 or more
linear regression
there are two types of linear regression
simple linear regression and multiple
linear regression
simple linear regression is used when
one wants to test how well a variable
predicts another variable
multiple linear regression allows one to
test how well multiple variables or
independent variables predict a variable
of interest
when using multiple linear regression We
additionally assume the predictor
variables are independent
for example finding relationship between
any two variables say sales and profit
is called Simple linear regression
finding relationship between any three
variables say sales cost telemarketing
is called multiple linear regression
some of the non-parametric tests are
wilcoxan rank sum test and kresco
Wallace h-test
wilcoxen rank some test the wilcoxan
signed rank test is a non-parametric
statistical hypothesis test used to
compare two related samples or matched
samples to assess whether or not their
population mean ranks differ in wilcoxon
rank some test you can test the null
hypothesis on the basis of the ranks of
the observations
crusco Wallace h test
Wallace h-test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples
the advantages of parametric tests are
as follows
provide information about the population
in terms of parameters and confidence
intervals
easier to use in modeling analyzing and
for describing data with Central
Tendencies and data transformations
Express the relationship between two or
more variables
don't need to convert data into rank
order to test
the disadvantages of parametric tests
are as follows
only support normally distributed data
only applicable on variables not
attributes
let's Now list the advantages and
disadvantages of non-parametric tests
the advantages of non-parametric tests
are as follows simple and easy to
understand
do not involve population parameters and
sampling Theory
make fewer assumptions
provide results similar to parametric
procedures
the disadvantages of non-parametric
tests are as follows
not as efficient as parametric tests
difficult to perform operations on large
samples manually
we'll discuss the types of distribution
in statistics
but before we move ahead let's have a
brief introduction on what is
probability distribution
a probability distribution is a list of
all of the possible outcomes of a random
variable along with their corresponding
probability values
and it is used in many fields but we
rarely do explain what they are so in
this video we'll discuss the three main
types of probability distribution that
is normal binomial and poisson
distribution
so let's move ahead
so what is normal distribution
normal distribution is a continuous
probability density that has a
probability density function which gives
us a symmetrical bell curve
now data can be distributed or spread
out in different ways but there are many
cases where the data tends to be around
a central value with no bias to the left
or right which means that it doesn't
show any particular spikes towards the
left or the right and it gets close to a
normal distribution half of the data
will fall on the left of the mean and
the other half will fall on the right
now let's take a look at a graph which
shows the height distribution in a class
as you can see the average height is in
the middle and the data to the left of
the average height represents the short
people and the data to the right of it
represents the taller people
the y-axis shows us the likelihood of
any of these Heights occurring the
average height has the most distribution
or it has the most number of cases in
the class
and as the height decreases or increases
the number of people who have that
height also decreases
this kind of a distribution is called a
normal distribution where the average or
the mean is always the highest point and
any other point after that or before
that is significantly lower
the resulting data gives us a bell curve
and as we can see there is no abrupt
bias or spike in the data anywhere
except for the average height
so this kind of a curve is called a bell
curve and it's usually seen in a normal
distribution
the reason we call this a normal
distribution is because the data is
normally distributed with the average
being the highest and all the other data
points having a lower likelihood
now we came across two terms which are
associated with normal distribution
continuous probability density
and probability density function
what is continuous probability density
continuous probability density is a
probability distribution with a random
variable X can take any given value
because there are infinite values that X
could assume the probability of X taking
on any specific value is zero for
example let's say you have a continuous
probability density for men's height
what is the probability that a man will
have the exact height of 70 inches
it is impossible to find this out
because the probability of one man
measuring exactly 70 inches is very low
it is more probable that he will measure
around 70.1 inches or maybe
69.97 inches and it doesn't stop there
the fact is that it's impossible to
exactly measure any variable that's on a
continuous scale and because of this
it's impossible to figure out the
probability of one exact measurement
which is occurring in a continuous
probability density
next we have the probability density
function it's nothing but a function or
an expression which is used to define
the range of values that a continuous
random variable can take an example of
this would be to Gorge the risk and
reward of a stock
a probability density function is a
statistical measure which is used to
gauge the likelihood of a discrete value
a discrete variable can be measured
exactly while a continuous variable can
have infinite values
however for both continuous as well as
discrete variables we can define a
function
which gives us the range of values
within which
these variables will fall and that
function is known as the probability
density function
now let's take a look at standard
deviation
what is standard deviation
standard deviation is used to measure
how the values in your data differ from
one another or how spread out your data
is
a standard deviation is a statistic that
measures the dispersion of a data set
relative to its mean the standard
deviation is calculated as the square
root of variance by determining each
data Point's deviation relative to the
mean if the data points are further from
the mean that means that there's a
higher deviation within the data set and
then the data is said to be more spread
out
this leads to a higher standard
deviation too
let's take an example of income in rural
and urban areas in rural areas let's say
such as farming areas the income doesn't
differ that much more or less everyone
earns the same
because of this our bell curve has a
very low standard deviation and it has a
very narrow Peak
however in urban areas the well
distribution is very uneven some people
can have very high incomes and can be
earning a lot while other people can
have very low incomes the furthermore
the data distribution between these two
income points is going to be more spread
out because there are a lot more people
living there who work in various fields
and who have various incomes because of
this our standard deviation is more
spread out and a bell curve will also
have a wider Peak
now how can we find the standard
deviation standard deviation is obtained
by subtracting each data value from the
mean and finding the squared average of
these values let's look at how we can do
this with the help of an example these
values correspond to the height of
various dogs
we can find the mean by finding the
average of all these values which is
nothing but adding all the values and
dividing it by the total number of
values the mean that we get is 394. this
means that the average height of a dog
is 394 millimeters
to find the standard deviation first we
need to subtract the height from the
mean this will tell us how far from the
mean our data points actually are
next we will square up all of these
differences and add them up and again
divide it by the total number of values
that we have this is called the variance
the variance that we get in this case is
21704
finally when we find the square root of
this value we will get the standard
deviation the standard deviation here is
147. the standard deviation will tell us
how our data points differ from the
average
and it gives us the Basic Value
suggesting how spread out our data is
from the very middle or from the mean so
when we plot these values this value 147
will mean that a curve will have a width
of 147 points around the mean
now what is the standard normal
distribution
the standard normal distribution
is a type of normal distribution that
has a mean of 0 and a standard deviation
of 1. this means that the normal
distribution has its Center at 0 and it
has intervals which increase by 1. all
normal distributions like the standard
nominal distribution are uni model and
symmetrically distributed with a
bell-shaped curve however a normal
distribution can take on any value as
its mean and standard deviation
in the standard normal distribution
however the mean and standard deviation
are always fixed when you standardize a
normal distribution the mean becomes 0
and the standard deviation becomes 1.
this allows you to easily calculate the
probability of certain values occurring
in your distribution
or to compare data sets with different
mean and standard deviations the curve
shows a standard normal distribution as
you can see again the data is centered
at 0.
this does not mean that the data
necessarily starts at zero this means
that after standardizing this point is
where a mean will lie in a standard
terminal distribution the standard
deviation is one so all the data points
will increase or decrease in steps of 1.
let's better understand the standard
normal distribution with the help of an
example again as you can see the data is
centered around 0 which is nothing but
the mean
let's again consider the weights of
students in class 8th the average weight
here is around 50 kgs and the data
increases and decreases in steps of 5.
the data over here in this curve is
evenly distributed along these steps
this is what a standard normal
distribution will look like
we already know that the mean of our
data is 50 and because the data is
increasing and decreasing in equal steps
we can just standardize it and take it
to mean that the data is increasing and
decreasing in steps of 1. this is what a
standard normal distribution looks looks
like and when you have a data which
looks like this you can always
standardize it and convert it into a
standard normal distribution now
standard normal distribution has a
couple of properties which makes
calculation comparatively easy
the first one is that 68 percent of the
values fall within the first standard
deviation which means that 68 percent of
all data values on this Curve will fall
between the range of -1 to 1 or the
first interval ranging from -1 to 1.
the second property is that 95 of the
rest of the values are within the second
standard deviation or from the second
negative point to the
second positive point
and finally
99.7 of the values fall within the third
standard deviation or from the third
negative point to the third positive
point this makes calculations and
standard normal distribution fairly easy
you can compare scores on different
distributions with different means and
standard deviations you can normalize
scores for statistical decision making
using standard normal distribution you
can find the probability of observations
in a distribution which fall above or
below a given value and find you can
find the probability that it means
significantly differs from a population
mean
now let's take a look at z-score
so what is the z-score a z-score is used
to tell us how far from the mean a data
point actually is
it is calculated using the mean and
standard deviation so it can be said
that the Z score is how many standard
deviations below the mean our data is
basically by using the Z score we can
get an approximate location of where our
data point lies on the graph with
regards to the mean
now the set score is given by
subtracting the data point from the mean
and dividing it by standard deviation
this can also be written as x minus mu
divided by Sigma now any normal
distribution can be standardized by
converting its values into Z scores the
z-score will tell you how many standard
deviation from the mean each values lie
while data points are referred to as X
and normal distribution they are called
Z or Z scores in the Z distribution a
z-score is a standard score that will
tell you how many standard deviations
away from the mean an individual point
will lie a positive Z score will mean
that your x value is greater than the
mean and a negative Z score will mean
that your x value is less than the mean
a z-score of 0 will mean that your x
value is equal to the mean
and again to standardize a value from a
normal distribution all we have to do is
convert it to a z-score by subtracting
the mean from our individual value and
dividing it by the standard deviation
now let's see how we can find the Z
score from data points with the help of
a solved example
let's do a case study
in this case study we'll be taking the
summary of daily travel time of a person
who's commuting to and from work all
these values are in minutes and using
these values we have to calculate the
mean the standard deviation and the Z
score these values are as shown as we
can see there are 13 values in total
let's start by finding the mean the mean
is the average and can be gotten by
adding all of these values and dividing
it by the total number of values
this gives us a value of 38.6
the mean tells us the average of all our
data points which means on an average he
travels
for 38.6 minutes to reach work
next let's subtract the individual
values from our mean
and calculate the variance and standard
deviation
the values on the left give us the
values that we get after subtracting it
from the mean and the variance can be
calculated by squaring all of these
values adding up all of the squared
values and dividing it by the total
number of values
at the end of the day we get a variance
of 140.
to calculate the standard deviation all
we have to do is take a square root of
the variance which gives us a value of
11.8
now the means signifies the average of
our values and we already know this it
gives us the average time which is taken
to travel but the standard deviation
will tell us the average value of how
much our data points differ from the
mean it tells us the deviation within
our own data and it tells us
how far away on an average a point is
from the mean now the value that we get
is 11.8 which means that on an average a
single data point is around 11.8 data
points away from the mean
now let's calculate the Z score
the z-score is given by subtracting
individual data points from the mean and
dividing it by the standard deviation we
know that we have a standard deviation
of 11.8 and a mean of 38.6
using these values we can calculate the
Z scores for individual X values
now we know that a negative Z score
means that our x value is lower than our
mean but what does the number 1.06 mean
this means that the Z score for 26 is
1.06 standard deviations
away from the mean the negative symbol
here means that our x value is less than
the mean
and by how less
1.06 times the standard deviation now we
know that the negative value of a z
score means that
our x value is less than our mean but
what does the number 1.06 mean
this means that the z-score is 1.06
times the standard deviation less than
the mean
the same thing can be said for the
z-score of 33 it is 0.47 times the
standard deviation less
than the mean
the z-score of 65 is 2.23 times the
standard deviation more than the mean
that means it has to be added to the
mean the reason that we know it's more
than the mean is because this has a
positive value so this means that using
Z scores we can know where a data points
fall relative to other points on the
graph the set score will tell us how far
away from the mean a point is in steps
of a standard deviation
Basics and terminology
the first one is outcome
whenever we do an experiment like
flipping a coin or rolling a dice we get
an outcome for example if we flip a coin
we get an outcome of heads or tails and
if you roll a die we get an outcome of
one two three four five or six
random experiment
a random experiment is any well-defined
procedure that produces an observable
outcome that could not be perfectly
predicted in advance
a random experiment must be well defined
to eliminate any vagueness or surprise
it must produce a definite observable
outcome so that you know what happened
after the random experiment is run
random events
consider a simple example
let us say that we toss a coin up in the
air
what can happen when it gets back it
will either gives a head or a tail
these two are known as outcome and the
occurrence of an outcome is an event
thus the event is the outcome of some
phenomenon
the last one is sample space
a sample space is a collection or a set
of possible outcomes of a random
experiment
the sample space is represented using
the symbol S
the subset of all possible outcomes of
an experiment is called events and a
sample space may contain a number of
outcomes that depends on the experiment
if it contains a finite number of
outcomes then it is known as a discrete
or finite sample spaces
now let's discuss what is random
variable
a random variable is a numerical
description of the outcome of a
statistical experiment
a random variable that may assume only a
finite number of values is said to be
discrete one that may assume any value
in some interval on the real number line
is said to be continuous
let's see an example
let X be a random variable defined as a
sum of numbers when two dices are rolled
X can assume the values 2 3 4 5 6 7 8 9
10 11 and 12. notice there is no one
here because the sum of the two dice can
never be one
now that we know the basics let's move
on to binomial distribution
the binomial distribution is used when
there are exactly two mutually exclusive
outcomes of a trial these outcomes are
appropriately labeled success and
failure the value of distribution is
used to obtain the probability of
observing X successes in N number of
trials with the probability of success
on a single trial denoted by P
the bannable distribution assumes that P
is fixed for all the trials
here's a real life example of a binomial
distribution
suppose you purchase a lottery ticket
then either you are going to win the
lottery or not in other words the
outcome will be either success or
failure that can be proved through
binomial distribution
there are four important conditions that
needs to be fulfilled for an experiment
to be a binomial experiment
the first one is there should be a fixed
number of entras carried out
the outcome of a given trial is only two
that is either a success or a failure
the probability of success remains
constant from trial to trial it does not
changes from one trial to another
and the trials are independent the
outcome of a trial is not affected by
the outcome of any other trial
to calculate the binomial coefficient we
use the formula which is NCR into P to
the power R into 1 minus P to the power
n minus r where I is the number of
success in N number of Trials and P is
the probability of success one minus P
denotes the probability of a failure now
let's use this formula to solve an
example
suppose a dice tossed three times what
is the probability of No 5 turning up
one five and three fives turning up
to calculate the Note 5 turning up here
RS is equal to 0 and N is equal to 3.
substituting the value in the formula we
have three C 0 into 1 by 6 to the power
0 into 5 by 6 to the power 3 where 1 by
6 is the probability of success and 5 by
6 is the probability of failure
calculating this equation we'll get the
value to be 0.5787
in a similar manner to calculate the
probability of 1 5 turning up we'll
replace r with 1 and N will be 3 so p x
1 will be is equal to 3 c 1 into 1 by 6
to the power 1 into 5 by 6 to the power
2 which will come out to be 0.347
and for three five turning up we
substitute r equal to 3 and the formula
will remain the same and we'll get the
value to be 0.0046
now that we are done with the concepts
of animal probability distribution
here's the problem for you to solve
post your answers in the comment section
and let us know
a poison distribution is a probability
distribution used in statistics to show
how many times an event is likely to
happen over a given period of time
to put it another way it's a count
distribution
distribution are frequently used to
comprehend independent event at a
constant rate over a given interval of
time
the positive distribution was developed
by French mathematician Simon Dennis
person in 1837.
a poison distribution is used in cases
where the chances of any individual
event being a success is very small
the number of defective pencils per box
of a 6000 pencil
the number of plane crash in India in
one year
all the number of printing mistakes in
each page of a book
all of these examples can have use of
poison distribution
the poison distribution can be used to
calculate How likely it is that
something will happen X number of times
a random variable X has a poisson
distribution with parameter Lambda and
the formula for that is e to the power
minus Lambda into Lambda to the power x
divided by X factorial
where X can be the number of times the
event is happening
the value of e is taken as 2.7182
let's discuss some application of
poisson distribution
if you want to calculate the number of
deaths per day or week due to rare
disease in the hospital you can use the
poison distribution in a similar manner
the count of bacteria for cc in blood or
the number of computers infected as
virus per week
the number of mishandled baggage per
thousand passengers can also have an
application for poisson distribution
let's discuss one example to see how you
can calculate the poisson distribution
suppose on an average a cancer kills 5
people each year in India
what is the probability that a one
person is killed this year
we'll assume all these events are
independent random events
so by the formula
we have X is equal to 1 because we have
to calculate the probability of one
person that is killed this year
so p x equal to 1 will be equal to e to
the power minus 5 into 5 to the power 1
divided by 1 factorial which will come
out to be
0.033 which will be near to 3.3 percent
so the probability that only one person
is killed this year due to cancer is 3.3
percent if you are an aspiring data
scientist who is looking out for online
training and certification in data
science from the best universities and
Industry experts then search number
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below
hello everyone welcome to another
session by simply learn today we are
going to discuss the base theorem an
important sub topic that comes under
probability Theory we'll start this
video by talking about probability and
condition probability
after that we'll move on to the base
theorem and understand its formula and a
real life example where the base theorem
can be used
so let's get started what is probability
probability is a branch of mathematics
concerning numerical descriptions of How
likely an event is to occur or How
likely it is Data preparation is true
the probability of an event is a number
between 0 and 1.
we're roughly speaking 0 indicates the
impossibility of the event and one
indicates certainty
the higher the probability of an event
the more likely is that the event will
occur
let's look at an example
a simple example is the tossing of a
fair unbiased coin
since the coin is fair the outcome that
is heads and the tails are both equally
probable
the probability of heads equals the
probability of the Tails and since no
other outcomes are possible the
probability of either heads or tails can
be said to be one by two which is also
fifty percent the probability of an
event can be calculated by number of
ways it can happen divided by the total
number of outcomes
now that we know about the probability
let's see if you can answer this
question what is the probability of
drawing a Jack and a queen consecutively
from a deck of 52 cards without
replacement
hair added options
post your answers in the comment section
and let us know
now let's move on to conditional
probability
let A and B be the two events associated
with a random experiment
then the probability of is occurrence
under the condition that B has already
occurred and probability of B is not
equal to zero is called the conditional
probability
it is denoted by P a slash B
this we can say that p a slash B is
equal to p a intersection B divided by P
of B where p a slash B is the
probability of occurrence of a given
that B has already occurred and PB is
the probability of occurrence of B
to know more about conditional
probability you can check our previous
video which is specifically on condition
probability
now let's move on to base theorem
the base theorem is a mathematical
formula for calculating condition
probability in probability and
statistics
in other words it is used to figure out
how likely an event is associated on its
proximity to another
base law or base rule are the other
names of this theorem the formula for
the base theorem can be written in a
variety of ways
the most common version is p a slash B
is equal to P of B A into P of a divided
by P of B
where v a slash B is the conditional
probability of event a occurring given
that b is true and P A and B of B are
the probabilities of A and B occurring
independently of one another
let's solve a problem using the base
theorem to understand it better
there is a cricket match tomorrow and in
recent years it has rained only 5 days
each year unfortunately the
meteorologist has predicted the rain for
tomorrow
now when it rains the meteorologists
correctly forecasts rain 90 of the time
and when it doesn't rain he incorrectly
forecast rain 10 percent of the time
let's calculate what is the probability
that it will rain on the match day
so the two sample spaces here are the
events that it rains and it does not
rain additionally a third event is also
there that meteorologists predicts the
rain so the notation for these events
appear below event A1 is equal to it
rains in the match Day event A2 that it
does not rain on the match day and event
B is the meteorologist predicting the
rain now in terms of probability we know
the following
probability of A1 is 5 by 365 that it
rains five days in a year which will
come out to be 0.0136
pa2 is 360 by 365 that is no days for
360 days in an year which will come out
to be 0.986
p b slash A1 is 0.9
this signifies when it rains the
meteorologist predicts that a 90 of the
time
in a similar manner p b by A2 is 0.1
that it does not train the meteorologist
predicts the rain 10 percent of the time
combining all this we can calculate p a
1 slash B that is the probability It
Will Rain on the given Match Day given a
forecast of Rain by meteorologist the
answer can be determined using the base
theorem as shown below
so here's the formula of the base
theorem and putting all the values that
we have calculated in the previous Slide
the probability that it will rain on the
match day given a forecast of the rain
by metallurgists will come out to be
0.111
which will be equal to 11.11 percent
so there's an 11 chance that it will
rain on the match day given that the
meteorologist has predicted the rain I
hope this example is clear to you it's a
weekend engine decided to watch the
latest movie recommended by Netflix at
his friends place
before heading out he asked Siri about
the weather and realized it would rain
so he decided to take his Tesla for the
long journey and switch to autopilot on
the highway
after coming home from the eventful day
he started wondering how technology has
made his life easy he did some research
on the internet and found out that
Netflix Siri and Tesla are all using AI
so what is AI
AI or artificial intelligence is nothing
but making computers-based machines
think and act like humans
artificial intelligence is not a new
term John McCarthy a computer scientist
coined the term artificial intelligence
back in 1956 but it took time to evolve
as it demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistants broadly classifying
there are three types of AI
artificial narrow intelligence also
called weak AI is the stage where
machines can perform a specific task
Netflix Siri chat Bots facial
recommendation systems are all examples
of artificial narrow intelligence
next up we have artificial general
intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human can
we are halfway in a successfully
implementing this phase IBM's Watson
supercomputer and gpt3 fall under this
category and lastly artificial super
intelligence it is the stage where
machines surpass human intelligence you
might have seen this in movies and
imagined how the world would be if
machines occupy it fascinated by this
John did more research and found out
that machine learning deep learning and
natural language processing are all
connected with artificial intelligence
a subset of AI is the process of
automating and enhancing how computers
learn from their experiences without
human health machine learning can be
used in email spam detection medical
diagnosis Etc
learning can be considered a subset of
machine learning it is a field that is
based on learning and improving on its
own by examining computer algorithms
while machine learning uses simpler
Concepts deep learning works with
artificial neural networks which are
designed to imitate the human brain this
technology can be applied in face
recognition speech recognition and many
more applications natural language
processing popularly known as NLP can be
defined as the ability of machines to
learn human language and translate it
chat Bots fall under this category
artificial intelligence is advancing in
every crucial field like healthcare
education robotics banking e-commerce
and the list goes on like in healthcare
AI is used to identify diseases helping
healthcare service providers and their
patients make better treatment and
lifestyle decisions
coming to the education sector AI is
helping teachers automate grading
organizing and facilitating parent
Guardian conversations
in robotics ai-powered robots employ
real-time updates to detect obstructions
in their path and instantaneously design
their routes artificial intelligence
provides Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance
with this growing demand for AI more and
more Industries are looking for AI
Engineers who can help them develop
intelligent systems and offer them
lucrative salaries going north of one
hundred and twenty thousand dollars
future of AI looks promising with the AI
Market expected to reach 190 billion
dollars by 2025. we know humans learn
from their past experiences
and machines follow instructions given
by humans
but what if humans can train the
machines to learn from their past data
and to what humans can do and much
faster well that's called machine
learning but it's a lot more than just
learning it's also about understanding
and reasoning so today we will learn
about the basics of machine learning
so that's Paul he loves listening to new
songs
he either likes them or dislikes them
Paul decides this on the basis of the
song's Tempo genre intensity and the
gender of voice for Simplicity let's
just use Tempo and intensity for now so
here Tempo is on the x-axis ranging from
relaxed to fast whereas intensity is on
the y-axis ranging from light to Soaring
we see that Paul likes the song with
fast tempo and soaring intensity while
he dislikes a song with relaxed Tempo
and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's name it as song a song
a has fast tempo and a soaring intensity
so it lies somewhere here looking at the
data can you guess whether Paul will
like the song or not correct so Paul
likes this song by looking at Paul's
past choices we were able to classify
the unknown song very easily right let's
say now Paul is reasons to a new song
Let's label it as song b so song b lies
somewhere here with medium Tempo and
medium intensity neither relaxed nor
fast neither light nor soaring now can
you guess where the ball likes it or not
not able to guess where this ball will
like it or dislike it other choices
unclear correct we could easily classify
song A but when the choice became
complicated as in the case of song b yes
and that's where machine learning comes
in let's see how in the same example for
song p if you draw a circle around the
song b we see that there are four words
for like whereas one vote for dislike if
we go for the majority votes we can say
that Paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called K
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become applicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
project for it more the data better the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you 1 million
coins of three different currencies say
one rupee one Euro and one dirham each
coin has different weights for example a
coin of one rupee weighs 3 grams one
Euro weighs 7 grams and one dirham
weighs 4 grams your model will predict
the currency of the coin here your
weight becomes the feature of coins
while currency becomes their label when
you feed this data to the machine
learning model it learns which feature
is associated with which sleep for
example it will learn that if a coin is
of 3 grams it will be a 1 rupee Point
let's give a new coin to the machine on
the basis of the weight of the new coin
your model will predict the currency
hence supervised learning uses labels
data to train the model here the machine
knew the features of the object and also
the labels associated with those
features on this note let's move to
unsupervised learning and see the
difference suppose you have Cricut data
set of various players with their
respective scores and wickets taken when
we feed this data set to the machine the
machine identifies the pattern of player
performance so it plots this data with
the respective wickets on the x-axis
while it runs on the y-axis by looking
at the data you will clearly see that
there are two clusters the one cluster
are the players who scored High runs and
took less wickets while the other
cluster is of the players who scored
less runs but took many wickets so here
we interpret these two clusters as
batsman and Bowlers the important point
to note here is that there were no
labels of batsman and Bowlers hence the
learning with only labeled data is
unsupervised learning so we saw
supervised learning where the data was
labeled and the unsupervised learning
where the data was unlabeled and then
there is reinforcement learning which is
reward-based learning or we can say that
it works on the principle of feedback
here let's say you provide the system
with an image of a dog and ask it to
identify it the system identifies it as
a cat so you give a negative feedback to
the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it will be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to a machine
learning model which then gives the
output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario 1 Facebook recognizes your
friend in a picture from an album of
tagged photographs
scenario 2 Netflix recommends new movies
based on someone's Past movie choices
scenario 3 analyzing Bank data for
suspicious transactions and flagging the
fraud transactions think wisely and
comment below your answers moving on
don't you sometimes wonder how is
machine learning possible in today's era
well that's because today we have
humongous data available everybody's
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis also the memory handling
capabilities of computers have largely
increased which helps them to process
such huge amount of data at hand without
any delay and yes computers now have
great computational Powers so there are
a lot of applications of machine
learning out there to name a few machine
learning is used in healthcare where
Diagnostics are predicted for doctor's
review the sentiment analysis that the
tech Giants are doing on social media is
another interesting application of
machine learning fraud detection in the
finance sector and also to predict
customer churn in the e-commerce sector
while booking a cab you must have
encountered search pricing often where
it says the fare of your trip has been
updated continue booking yes please I'm
getting late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad weather
Rush R Etc so they use the search
pricing model to ensure that those who
need a cab can get one also it uses
predictive modeling to predict where the
demand will be high with a goal that
drivers can take care of the demand and
search pricing can be minimized great
hey Siri can you remind me to book a cab
at 6 pm today okay I'll remind you
thanks no problem if you are an aspiring
data scientist who is looking out for
online training and certification in
data science from the best universities
and Industry experts then search no more
simply learns post graduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below
let's dive in a little deeper and see
how machine Learning Works let's say you
provide a system with the input data
that carries the photos of various kinds
of fruits now you want the system to
figure out what are the different fruits
and group them accordingly so what the
system does it analyzes the input data
then it tries to find patterns patterns
like shapes size and color
based on these patterns the system will
try to predict the different types of
fruit and segregate them
finally it keeps track of all such
decisions it took in the process to make
sure it's learning the next time you ask
the same system to predict and segregate
the different types of fruits it won't
have to go through the entire process
again
that's how machine Learning Works
now let's look into the types of machine
learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggests you have
to supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well
yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed these data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it
this kind of a model is used by Flipkart
to figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby you put a
burning candle in front of the baby the
baby does not know that if it touches
the flame its fingers might get burnt so
it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from his mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA wherein the level of
difficulty increases as you get better
with the games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learning
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to find out hidden
patterns or structures in data
I think this would have made a lot of
things clear for you regarding
supervised and unsupervised learning
now let's talk about a question that
everyone needs to answer before building
a machine learning model
what kind of a machine learning solution
should we use
yes you should be very careful with
selecting the right kind of solution for
your model because if you don't you
might end up losing a lot of time energy
and processing costs
I won't be naming the actual Solutions
because you guys aren't familiar with
them yet so we will be looking at it
based on supervised unsupervised and
reinforcement learning
so let's look into the factors that
might help us select the right kind of
machine learning solution
first factor is the problem statement
describes the kind of model you will be
building or as the name suggests it
tells you what the problem is for
example let's say the problem is to
predict the future stock market prices
so for anyone who is new to machine
learning would have trouble figuring out
the right solution
but with time and practice you will
understand that for a problem statement
like this solution based on supervised
learning would work the best for obvious
reasons
then comes the size quality and nature
of the data if the data is cluttered you
go for unsupervised if the data is very
large and categorical we normally go for
supervised learning Solutions
finally we choose the solution based on
their complexity
as for the problem statement wherein we
predict the stock market prices it can
also be solved by using reinforcement
learning but that would be very very
difficult and time consuming unlike
supervised learning
algorithms are not types of machine
learning in the most simplest language
they are methods of solving a particular
problem
so the first kind of method is
classification which falls under
supervised learning classification is
used when the output you are looking for
is a yes or a no or in the form a or b
or true or false like if a shopkeeper
wants to predict if a particular
customer will come back to his shop or
not he will use a classification
algorithm
the algorithms that fall under
classification are decision tree knife
base random Forest logistic regression
and KNN
the next kind is regression this kind of
a method is used when the predicted data
is numerical in nature like if the
shopkeeper wants to predict the price of
a product based on its demand it would
go for regression
the last method is clustering
clustering is a kind of unsupervised
learning again it is used when the data
needs to be organized
most of the recommendation system used
by Flipkart Amazon Etc make use of
clustering you
another major application of it is in
search engines the search engines study
your old search history to figure out
your preferences and provide you the
best search results
one of the algorithms that fall under
clustering is k-means
now that we know the various algorithms
let's look into four key algorithms that
are used widely
we will understand them with very simple
examples
the four algorithms that we will try to
understand are K nearest neighbor linear
regression decision tree and knife base
let's start with our first machine
learning solution K nearest neighbor
K nearest neighbor is again a kind of a
classification algorithm as you can see
on the screen the similar data points
form clusters
the blue one
the red one
and the green one there are three
different clusters
now if we get a new and unknown data
point it is classified based on the
cluster closest to it or the most
similar to it
K in KN is the number of nearest
neighboring data points we wish to
compare the unknown data with
let's make it clear with an example
let's say we have three clusters in a
cost to durability graph first cluster
is of footballs
the second one is of tennis balls
and the third one is of basketballs
from the graph we can say that the cost
of footballs is high and the durability
is less the cost of tennis balls is very
less but the durability is high and the
cost of basketballs is as high as the
durability
now let's say we have an unknown data
point
we have a black spot which can be one
kind of the pause but we don't know what
kind it is
so what we'll do we'll try to classify
this using KNN
so if we take K is equal to 5 we draw a
circle keeping the unknown data point in
the center and we make sure that we have
five balls inside that Circle in this
case we have a football a basketball and
three tennis balls now since we have the
highest number of tennis balls inside
the circle
the classified ball would be a tennis
ball
so that's how K nearest neighbor
classification is done
linear regression is again a type of
supervised learning algorithm this
algorithm is used to establish linear
relationship between variables one of
which would be dependent and the other
one would be independent
like if we want to predict the weight of
a person based on his height weight
would be the dependent variable and
height would be independent
let's have a look at it through an
example
let's say we have a graph here showing a
relationship between height and weight
of a person
let's put the y-axis as h
and the x-axis as weight
so the green dots are the various data
points
these green dots are the data points
and D is the mean squared error that is
the perpendicular distances from the
line to the data points are the error
values
this error tells us how much the
predicted values vary from the original
value
Let's ignore this blue line for a while
so let's say if this is our regression
line
you can see the distance from all the
data points from this line is very high
if we take this line as a regression
line the error in the prediction will be
too high
so
in this case the model will not be able
to give us a good prediction
let's say we draw another regression
line here like this
even in this case you can see that the
perpendicular distance of the data
points from the line is very high
so the error value will still come as
high as the last one
so this model will also not be able to
give us a good prediction
so what to do
so finally we draw a line which is this
blue line so here we can see that the
distance of the data points from the
line is very less relative to the other
two lines we drew
so the value of D for this line will be
very less
so in this case if we take
any value on the x axis the
corresponding value on the y-axis will
be our prediction
and given the fact that the D is very
low our prediction should be good also
this is how regression works we draw a
line a regression line that is in such a
way that the value of D is the least
eventually giving us good predictions
this algorithm that is decision tree is
a kind of an algorithm you can very
strongly relate to it uses a kind of a
branching method to realize the problem
and make decisions based on the
conditions
let's take this graph as an example
imagine yourself sitting at home being
bored you feel like going for a swim
what you do is you check if it's sunny
outside so that's your first condition
if the answer to that condition is yes
you go for a swim if it's not Sunny then
the next question you would ask yourself
is if it's raining outside so that's
condition number two
if it's actually raining you cancel the
plan and stay indoors if it's not
raining then you would probably go
outside and have a walk
so that's the final note
that's how decision tree algorithm works
you probably use this every day it
realizes a problem and then takes the
decisions based on the answers to every
conditions
nightbis algorithm is mostly used in
cases where a prediction needs to be
done on a very large data set it makes
use of conditional probability
conditional probability is the
probability of an event say a happening
given that another event B has already
happened
this algorithm is most commonly used in
filtering spam mails in your email
account
let's say you receive a mail
the model goes through your old spam
Mill records
then it uses base theorem to predict if
the present male is a spam mail or not
so p c of a is the probability of even C
occurring when a has already occurred p
a of C is the probability of event a
occurring when C has already occurred
and P C is the probability of even C
occurring and Pa is a probability of
event a operating
let's try to understand knife base with
a better example Knight base can be used
to determine on which days to play
cricket
based on the probabilities of a day
being rainy windy or sunny the model
tells us if a match is possible
if we consider all the weather
conditions to be event a for us
and the probability of a match being
possible even C
so the model applies the probabilities
of event A and C into the base theorem
and predicts if a game of cricket is
possible on a particular day or not
in this case if the probability of C of
a is more than 0.5 we can be able to
play a game of cricket if it's less than
0.5 we won't be able to do that that's
how nivas algorithm works
we're going to cover reinforcement
learning today and what's in it for you
we'll start with why reinforcement
learning we'll look at what is
reinforcement learning
we'll see what the different kinds of
learning strategies are that are being
used today in computer models under
supervised versus unsupervised versus
reinforcement
will cover important terms specific to
reinforcement learning we'll talk about
markov's decision process and we'll take
a look at a reinforcement learning
example but we'll teach a tic-tac-toe
how to play why reinforcement learning
training a machine learning model
requires a lot of data which might not
always be available to us further the
data provided might not be reliable
learning from a small subset of actions
will not help expand the vast realm of
solutions that may work for a particular
problem
you can see here we have the robot
learning to walk
very complicated setup when you're
learning how to walk and you'll start
asking questions like if I'm taking one
step forward and left what happens if I
pick up a 50 pound object how does that
change how a robot would walk
these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expand the vast
realm of solutions that may work for a
particular problem
and we'll see here it learned how to
walk this is going to slow the growth
that technology is capable of machines
need to learn to perform actions by
themselves and not just learn off humans
and you see the objective climb a
mountain a real interesting point here
is that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it
most of the models the non-reinforcement
models in computer machine learning
aren't able to do that very well there's
a couple of them that can be used or
integrated to see how it goes is what
we're talking about with reinforcement
learning so what is reinforcement
learning
reinforcement learning is a sub-branch
of machine learning that trains a model
to return an Optimum solution for a
problem by taking a sequence of
decisions by itself
consider a robot learning to go from one
place to another
the robot is given a scenario must
arrive at a solution by itself the robot
can take different paths to reach the
destination
it will know the best path by the time
taken on each path it might even come up
with a unique solution all by itself
and that's really important is we're
looking for Unique Solutions we want the
best solution but you can't find it
unless you try it so we're looking at
our different
systems or different model we have
supervised versus unsupervised versus
reinforcement learning and with the
supervised learning that is probably the
most controlled environment we have a
lot of different supervised learning
models whether it's linear regression
neural networks there's all kinds of
things in between decision trees the
data provided is labeled data with
output values specified and this is
important because we talk about
supervised learning you already know the
answer for all this information you
already know the picture has a
motorcycle in it so your supervised
learning you already know that the
outcome for tomorrow for you know going
back a week you're looking at stock you
can already have like the graph of what
the next day looks like so you have an
answer for it
and you have labeled data which is used
you have an external supervision and
solves Problems by mapping labeled input
to known output
so very controlled
unsupervised learning and the entire
learning is really interesting because
it's now taking part in many other
models they start within you can
actually insert an unsupervised learning
model
in almost either supervised or
reinforcement learning as part of the
system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering output
so you can look at this and you can
think some of these things go with each
other they belong together so it's
looking for what connects in different
ways and there's a lot of different
algorithms that look at this when you
start getting into those there's some
really cool images that come up of what
unsupervised learning is how it can pick
out say the area of a donut one model
will see the area of the donut and the
other one will divide it into three
sections based on its location versus
what's next to it so there's a lot of
stuff that goes in with unsupervised
learning
and then we're looking at reinforcement
learning probably the biggest industry
in today's market in machine learning or
growing Market is very it's very infant
stage as far as how it works and what
it's going to be capable of the machine
learns from its environment using
rewards and errors used to solve reward
based problems no predefined data is
used no supervision follows Trail and
error problem solving approach so again
we have a random at first you start with
a random I try this it works and this is
my reward doesn't work very well maybe
or maybe it doesn't even get you where
you're trying to get it to do and you
get your reward back and then it looks
at that and says well let's try
something else and it starts to play
with these different things finding the
best route
so let's take a look at important terms
in today's reinforcement model
and this has become pretty standardized
over the last few years so these are
really good to know we have the agent
agent is the model that is being trained
via reinforcement learning so this is
your actual entity that has however
you're doing it with using a neural
network or a
cue table or whatever combination
thereof this is the actual agent that
you're using this is the model
and you have your environment the
training situation that the model must
optimize to is called its environment
and you can see here I guess we have a
robot just trying to get uh chest full
of gyms or whatever and that's the
output and then you have your action
this is all possible steps that can be
taken by the model and it picks one
action and you can see here that's
picked three different uh routes to get
to the chest of diamonds and gems
we have a state the current position
condition returned by the model
and you could look at this if you're
playing like a video game this is the
screen you're looking at so when you go
back here the environment is a whole
game board so if you're playing one of
those Mobius games
you might have the whole game board
going on but then you have your current
position where are you on that game
board what's around that what's around
you
um if you were talking about a robot the
environment might be moving around the
yard where it is in the yard and what it
can see what input it has in that
location that would be the current
position condition returned by the model
and then the reward to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if didn't do as good trying
to maximize the reward and have the best
reward possible
and then policy policy determines how an
agent will behave at any time it acts as
a mapping between action and present
State this is part of the model what is
your action that you're you're going to
take what's the policy you're using to
have an output from your agent one of
the reasons they separate policy as its
own entity
is that you usually have a prediction
of a different options and then the
policy well how am I going to pick the
best based on those predictions I'm
going to guess at different options
and we'll actually weigh those options
in and find the best option we think
will work so it's a little tricky but
the policy thing is actually pretty cool
how it works let's go ahead and take a
look at a reinforcement learning example
and just in looking at this we're going
to take a look consider what a dog that
we want to train so the dog would be
like the agent so you have your your
puppy or whatever and then your
environment is going to be the whole
house or whatever it is or where you're
training them and then you have an
action we want to teach the dog to fetch
so action equals fasting uh and then we
have a little biscuits so we can get the
dog to perform various actions by
offering incentives such as a dog
biscuit as a reward
the dog will follow a policy to maximize
this reward and hence we'll follow every
command and might even learn new actions
like begging by itself so yeah you know
so we started off with fetching it goes
oh I get a biscuit for that it tries
something else and you get a handshake
or begging or something like that and it
goes oh this is also reward based and so
it kind of explores things to find out
what will bring it is biscuit and that's
very much like how reinforced model goes
is it looks for different rewards how do
I find can I try different things and
find a reward that works
the dog also will want to run around and
play an explore its environment this
quality of model is called exploration
so there's a little Randomness going on
in Exploration
and explores new parts of the house
climbing on the sofa doesn't get a
reward in fact it usually gets kicked
off the sofa
so let's talk a little bit about
markov's decision process markov's
decision process
is a reinforcement learning policy used
to map a current state to an action
where the agent continuously interacts
with the environment to produce new
Solutions and receive rewards and you'll
see here's all of our different
vocabulary we just went over we have a
reward our state our agent our
environment interaction and so even
though the environment kind of contains
everything
that you really when you're actually
writing the program your environment's
going to put out a reward and state that
goes into the agent
the agent then looks at this state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or it didn't get rewarded and
then looks at the state and then it
comes back and if you remember from
policy the policy comes in and then we
have a reward the policy is that part
that's connected at the bottom
and so it looks at that policy and it
says hey what's a good action that will
probably be similar to what I did or
sometimes they're completely random but
what's a good action that's going to
bring me a different reward
so taking the time to just understand
these different pieces as they go
is pretty important in most of the
models today and so a lot of them
actually have templates based on this
you can pull in and start using
um pretty straightforward as far as once
you start seeing how it works you can
see your environment sends it says hey
this is what the agent did this if
you're a character in a game this
happened and it shoots out a reward in a
state the agent looks at the reward
looks at the new state and then takes a
little guess and says I'm going to try
this action and then that action goes
back into the environment it affects the
environment the environment then changes
depending on what the action was and
then it has a new state and a new reward
that goes back to the agent so in the
diagram Sean we need to find the
shortest path between node A and D each
path has a reward associated with it and
the path with a maximum reward is what
we want to choose
the node a b c d denote the nodes to
travel from node A to B is an action
reward is the cost of each path and
policy is each path taken
and you can see here a can go to B
or a can go to C right off the bat or it
can go right to D and if explored all
three of these you would find that a
going to D was a zero reward a going to
C and D would generate a different
reward
or you could go acbd there's a lot of
options here and so when we start
looking at this diagram you start to
realize
that even though today's reinforced
learning models do really good at
finding an answer they end up trying
almost all the different directions you
see
and so they take up a lot of work or a
lot of processing time for reinforcement
learning they're right now in their
infant stage and they're really good at
solving simple problems and we'll take a
look at one of those in just a minute in
a tic-tac-toe game but you can see here
once it's gone through these and it's
explored it's going to find the ACD is
the best reward it gets a full 30 points
for it so let's go ahead and take a look
at a reinforcement learning
demo
in this demo we're going to use
reinforcement learning to make a tic tac
toe game you'll be playing this game
Against the Machine learning model
and we'll go ahead and we're doing it in
Python so let's go ahead and go through
my always not always I actually have a
lot of python tools let's go through
Anaconda which will open up a Jupiter
notebook seems like a lot of steps but
it's worth it to keep all my stuff
separate and it's also has a nice
display when you're in the jupyter
notebook for doing python
so here's our Anaconda Navigator I open
up the notebook which is going to take
me to a web page and I've gone in here
and created a new python folder in this
case I've already done it and enabled it
to change the name to Tic-tac-toe and
then for this example we're going to go
ahead and
import a couple things we're going to
import numpy as NP we'll go ahead and
import pickle numpy of course is our
number array and then uh pickle is just
a nice way sometimes for
storing different information different
states that we're going to go through on
here
and so we're going to create a class
called State we're going to start with
that
and there's a lot of lines of code to
this class that we're going to put in
here don't let that scare you too much
there's not as much here it looks like
there's going to be a lie here but there
really is just a lot of setup going on
in the in our class State and so we have
up here we're going to initialize it
um
we have our board
it's a Tic Tac Toe board so we're only
dealing with nine spots on the board we
have player one player two
uh is end we'll create a board hash uh
we'll look at that in just a minute
we're just going to store some
information in there symbol of player
equals one
so there's a few things going on as far
as the initialization uh then something
simple we're just going to get the hash
of the board we're going to get the
information from the board on there
which is columns and rows we want to
know when a winner occurs so if you get
three in a row that's what this whole
section here is for let me go ahead and
scroll up a little bit and you can get a
copy of this code if you send a note
over to Simply learn we'll send you over
this particular file and you can play
with it yourself and see how it's put
together I don't want to spend a huge
amount of time on this because this is
just some real General python coding
but you can see here we're just going
through all the rows and you add them
together and if it equals three three in
a row the same thing with columns
diagonal so you gotta check the diagonal
that's what all this stuff does here is
it just goes through the different areas
actually let me go ahead and put
and then it comes down here and we do
our sum and it says true minus three
just says did somebody win or is it a
tie so you got to add up all the numbers
on there anyway just in case they're all
filled up
and next we also need to know available
positions these are ones that don't no
one's ever used before this way when you
try something or the computer tries
something it's not going to give it an
illegal move that's what the available
positions is doing then we want to
update our state
and so you have your position going in
we're just sending in the position that
you just chose and you'll see there's a
little user interface we put in there we
can pick the row and column in there
and again
I mean this is a lot of code so really
it's kind of a thing you'd want to go
through and play with a little bit and
just read through it get a copy of it a
great way to understand how this works
and here is a given reward so we're
going to give a reward result equals
self winner this is one of the hearts of
what's going on here is we have a result
a self.winner
so if there's a winner then we have a
result if the result equals one here's
our feedback
if it doesn't equal one then it gets a
zero so it only gets a reward in this
particular case if it wins
and that's important to know because
different
systems of reinforced learning
do rewarding a lot differently depending
on what you're trying to do this is a
very simple example with a three by
three board imagine if you're playing a
video game uh certainly you only have so
many actions but your environment is
huge you have a lot going on in the
environment and suddenly a reward system
like this is going to be just it's going
to have to change a little bit it's
going to have to have different rewards
and different setup and there's all
kinds of advanced ways to do that as far
as weighing you add weights to it and so
they can add the weights up depending on
where the reward comes in so it might be
that you actually get a reward in this
case you get the reward at the end of
the game
and I'm spending just a little bit of
time on this this is an important thing
to note but there's different ways to
add up those rewards it might have like
if you take a certain path the first
reward is going to be weighed a little
bit less than the last reward because
the last reward is actually winning the
game or scoring or whatever it is so
this reward system gets really
complicated on some of the more advanced
setups
um in this case though
you can see right here that they give a
0.1 and a 0.5 reward
um just for getting picking the right
value and something that's actually
valid instead of picking an invalid
value so rewards again that's like key
it's huge how do you feed the rewards
back in
then we have a board reset that's pretty
straightforward it just goes back and
resets the board to the beginning
because it's going to try out all these
different things whilst learning it's
going to do it by trial and error so you
have to keep resetting it and then of
course there's the play we want to go
ahead and play rounds equals 100 depends
on what you want to do on here you can
set this different you obviously set
that to higher level but this is just
going to go through and you'll see in
here that we have player one and player
two
this isn't this is a computer playing
itself one of the more powerful ways to
learn to play a game or even learn
something that isn't a game is to have
two of these models
that are basically trying to beat each
other and so they always they keep
finding exploring new things this one
works for this one so this one tries new
things it beats this we've seen this in
chess I think was a big one where they
had the two players in chess with
reinforcement learning it was one of the
ways they train one of the top computer
chess playing algorithms
uh so this is just what this is it's
going to choose an action it's going to
try something and the more it try stuff
um the more we're going to record the
hash we actually have a board hash where
the self get the hash set up on here
where it stores all the information
and then once you get to a win one of
them wins it gets the reward uh then we
go back and reset and try again and then
kind of the fun part we actually get
down here is we're going to play with a
human so we'll get a chance to come in
here and see what that looks like when
you put your own information in and then
it just comes in here and does the same
thing it did above it gives it a reward
for its things or sees if it wins or
ties
it looks at available positions all that
final fun stuff
and then finally we want to show the
board so it's going to print the board
out each time
really as an integration is not that
exciting what's exciting in here is one
looking at this reward system whoops
Play One More up the reward system is
really the heart of this how do you
reward the different setup and the other
one is when it's playing it's got to
take an action
and so what it chooses for an action is
also the heart of reinforcement learning
how do we choose that action and those
are really key to right now where
reinforcement learning is
in
today's technology is figuring this out
how do we reward it and how do we guess
the next best action so we have our
environment and you can see the
environment is we're going to be or the
state which is kind of like what's going
on we're going to return the state
depending on what happens
and we want to go ahead and create our
agent in this case our player so each
one is let me go and grab that and so we
look at a class player
um
this is where a lot of the magic is
really going on is what it how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state that it can look
at and a reward uh so we want to take a
look at this we have a name self-state
this is class player and when you say
class player we're not talking about a
human player we're talking about just a
uh the computer players and this is kind
of interesting so remember I told you
depending on what you're doing there's
going to be a Decay gamma
um
explore rate
these are what I'm talking about is how
do we train it
um
as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let's see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10 is going to count more than
the first step
and here's our we're going to you know
get the board information coming in and
then choose an action this was the
second part that I was talking about
that was so important
so once you have your training going on
we have to do a little Randomness and
you can see right here is our NP random
uniform so it's picking out a random
number
take a random action this is going to
just pick which row and which column it
is and so choosing the action this one
you can see we're just doing random
States Choice link to positions action
position
and then it skips in there and takes a
look at the board for p in positions
it's actually storing the different
boards each time you go through so it
has a record of what it did so it can
properly weigh the values
and this simply just depends a hash date
what's the last date pinned it to the uh
to our states on here here's our
feedback
rewards the reward comes in and it's
going to take a look at this and say is
it none what is the reward and here is
that formula remember I was telling you
about up here
that was important because it has
Decay gamma times the reward
this is where
as it goes through each step and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier you have step one
and this might have a reward of two you
have step two I should probably should
have done ABC this has a step three
step four and so on until you get to
step in
and this might have a reward of 10. so
reward at 10
we're going to add that but we're not
adding let's say this one right here
let's see this reward here before 10 was
um let's say it's also 10. it just makes
the the
math easy so we had 10 and 10. we had 10
this is 10 and 10 n whatever it is but
it's time it's 0.9 so instead of putting
a full 10 here
we only do 9. that's a 0.9 times
10.
and so this formula as far as the Decay
times the reward minus the cell State
value
it basically adds in it says here's one
or here's two I'm sorry I should have
done this a b c would have been easier
so the first move goes in here and it
puts 2 in here
uh then we have ourself
setup on here you can see how this gets
pretty complicated in the math but this
is really the key is how do we train
our states and we want the the final
State the win to get the most points if
you win you get most points and the
first step gets the least amount of
points
so you're really training this almost in
Reverse you're retraining you're
training it from the last place where
you have like it says okay this is now I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh play on the mind when
you're trying to figure this stuff out
and of course we want to go ahead and
reset the board down here and save the
policy load policy
these are the different things that are
going in between the agent and the state
to figure out what's going on let's go
ahead and load that up and then finally
we want to go ahead and create a human
player
and the human player is going to be a
little different
in that you choose an action row and
column here's your action if action is
if action in positions meaning positions
that are available
you return the action
if not it just keeps asking you until
you get the action that actually works
and then we're going to go ahead and
append to the hash state which we don't
need to worry about because it Returns
the action up here
and feed forward
again this is because it's a human
at the end of the game bat propagate and
update State values this part isn't
being done because it's not programming
uh
the model the models is getting its own
Rewards
so we've gone ahead and loaded this in
here so here's all our pieces and the
first thing we want to do
is set up a P1 player one P2 player two
and then we're going to send our players
to our state so now it has P1 P2 and
it's going to play and it's going to
play 50 000 rounds now we can probably
do a lot less than this and it's not
going to get the full results in fact
you know what uh let's go ahead and just
do five just to play with it because I
want to show you something here
oops somewhere in there I forgot to load
something
to run this run
oops forgot a reference there for the
board rows and columns three by three
there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning
so now I've only set this up with
um
see where we go in here I've only set
this up to train
five times
and the reason I did that is we're going
to uh
come in and actually play it and then
I'm going to change that and we can see
how it differs on there
there we go and did you make it through
a run and we're going to go ahead and
save the policy
so now we have our player one and our
player two policy the way we set it up
it has two separate policies loaded up
in there
and then we're going to come in here and
we're going to do uh player one is going
to be the computer experience rate zero
load policy one human player human and
we're going to go ahead and play this
now remember I only went through it um
just one round of training in fact
minimal training and so it puts an X
there and I'm going to go ahead and do
row 0 column one
you can see this is very basic on here
and so I put in my zero and then I'm
going to go zero block it zero zero
and you can see right here let me win uh
just like that I was able to win zero
two and human wins
so I only trained it five times we're
going to run this again and this time
instead of five let's do five thousand
or fifty thousand I think that's what
the guys in the back had
and this takes a while to train it
this is where reinforcement learning
really falls apart look how simple this
game is we're talking about a three by
three set of columns
and so for me to train it on this I
could do a q table which would take
which would go much quicker you could
build a quick Q table with almost all
the different options on there and uh
you would probably get a the same result
much quicker we're just using this as an
example so when we look at reinforcement
learning you need to be very careful
what you apply it to it sounds like a
good deal until you do like a large
neural network where you're doing you
set the neural network to a learning
increment of one so every time it goes
through it learns
and then you do your actions you pick
from the learning setup and you actually
try actions on the learning setup until
you get the what you think is going to
be the best action
so you actually feed what you think is
right back through the neural network
there's a whole layer there which is
really fun to play with
and then it has an output well think of
all those processes I mean that is just
a huge amount of work it's going to do
uh let's go ahead and Skip ahead here
and give it a moment it's going to take
a a minute or two to go ahead and run
now to train it we went ahead and let it
run and it took a while this this took
um I got a pretty powerful processor and
it took about five minutes plus to run
it
and we'll go ahead and uh
run our player setup on here oops it
brought in the last whoops I bought in
the last round so give me just a moment
to re do the policy save there we go I
forgot to save the policy back in there
and then go ahead and run our player
again
so we've saved the policy then we want
to go ahead and load the policy for P1
as a computer
and we can see the computer's gone in
the bottom right corner I'm going to go
ahead and go 1 1 which is the center
and it's gone right up the top and if
you have ever played tic-tac-toe you
know the computer has me but we'll go
ahead and play it out row zero
column two
there it is and then it's gone here and
so I'm going to go ahead and go row 0 1
2 no 0 1 there we go and column 0.
that's where I want it
oh and it says okay you your action
there we go boom so you can see here
we've got a didn't catch the win on this
it said tie
um kind of funny they didn't catch the
win on there
but if we play this a bunch of times
you'll find it's going to win more and
more the more we train it the more the
reinforcement happens
this lengthy training process is really
the stopper on reinforcement learning as
this changes reinforcement learning will
be one of the more powerful packages
evolving over the next decade or two in
fact I would even go as far as to say it
is the most important machine learning
tool in artificial intelligence tool out
there as it learns not only a simple
tic-tac-toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to do
so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with run it you can test it out
you can do you know test it for
different uh values you can switch from
P1 computer
where we loaded the policy one to load
the policy two and just see how it
varies there's all kinds of things you
can do on there supervised learning uses
label data to train machine learning
models label data means that the output
is already known to you
the model just needs to map the inputs
to the outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal
below you can see we have our trained
model that identifies the picture of a
cat
unsupervised learning uses unlabeled
data to train machines unlabeled data
means there is no fixed output variable
the model learns from the data discovers
patterns and features in the data and
Returns the output here is an example of
an unsupervised learning technique that
uses the images of vehicles to classify
if it's a bus or a truck
so the model learns by identifying the
parts of a vehicle such as the length
and width of the vehicle the front and
rear end covers roof hoods the types of
Wheels used Etc based on these features
the model classifies if the vehicle is a
bus or a truck
reinforcement learning trains a machine
to take suitable accents and maximize
reward in a particular situation
it uses an agent and an environment to
produce actions and Rewards the agent
has a start and an end state but there
might be different paths for reaching
the end State like a maze in this
learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle
in the example shown the model tries to
predict the shape of the object which is
a square here
now
let's look at the different machine
learning algorithms that come under
these learning techniques
some of the commonly used supervised
learning algorithms are linear
regression logistic regression support
Vector machines K nearest neighbors
decision tree random forest and knife
base
examples of unsupervised learning
algorithms are key means clustering
hierarchical clustering DB scan
principle component analysis and others
choosing the right algorithm depends on
the type of problem you are trying to
solve
some of the important reinforcement
learning algorithms are Q learning Monte
Carlo sarsa and deep Q Network now let's
look at the approach in which these
machine learning techniques work
so supervised learning takes labeled
inputs and Maps it to known outputs
which means you already know the target
variable
unsupervised learning finds patterns and
understands the trends in the data to
discover the output so the model tries
to label the data based on the features
of the input data
while reinforcement learning follows
trial and error method to get the
desired solution after accomplishing a
task the agent receives an award
an example could be to train a dog to
catch the ball if the dog learns to
catch a ball you give it a reward such
as a biscuit
now let's discuss the training process
for each of these learning methods
so supervised learning methods need
external supervision to train machine
learning models and hence the name
supervised
they need guidance and additional
information to return the result
unsupervised learning techniques do not
need any supervision to train models
they learn on their own and predict the
output
similarly reinforcement learning methods
do not need any supervision to train
machine learning models
and with that
let's focus on the types of problems
that can be solved using these three
types of machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next slide
and unsupervised learning is used for
clustering and Association problems
while reinforcement learning is reward
based so for every task or for every
step completed there will be a reward
received by the agent
and if the task is not achieved
correctly
there will be some penalty used
now let's look at a few applications of
supervised unsupervised and
reinforcement learning
as we saw earlier supervised learning
are used to solve classification and
regression problems for example You can
predict the weather for a particular day
based on humidity precipitation wind
speed and pressure values
you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or benign
now talking about the applications of
unsupervised learning we have customer
segmentation So based on customer
Behavior likes dislikes and interests
you can segment and cluster similar
customers into a group another example
where unsupervised learning algorithms
are used is customer churn analysis
now let's see what applications we have
in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks
if you are an aspiring data scientist
who is looking out for online training
and certification in data science from
the best universities and Industry
experts then search number simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
often professionals want to know if
there is a relationship between two or
more variables for instance is there a
relationship between the grade on the
third French exam a student takes and
the grade on the final exam if yes then
how is it related and how strongly
regression can be used here to arrive at
a conclusion this is an example of
bivariate data that is two variables
however statisticians are mostly
interested in multivariate data a
regression analysis is used to predict
the value of one variable the dependent
variable on the basis of other variables
the independent variables in the
simplest form of regression linear
regression you work with one independent
variable
the formula for simple linear regression
is shown on the screen in the next
screen we'll look at a few examples of
regression analysis
regression analysis is used in several
situations such as those described on
the screen in example 1 using the data
given on the screen you have to analyze
the relation between the size of a house
and its selling price for a realtor in
example 2 you need to predict the exam
scores of students who study for 7.2
hours with the help of the data shown on
the slide
a couple more examples are given on the
screen in example 3 based on the
expected number of customers and the
previous day's data given you need to
predict the number of burgers that will
be sold by a KFC Outlet in example four
you have to calculate the life
expectancy for a group of people with
the average length of schooling based on
the data given
let's look at the two main types of
regression analysis simple linear
regression and multiple linear
regression both of these statistical
methods use a linear equation to model
the relationship between two or more
variables simple linear regression
considers one quantitative and
independent variable X to predict the
other quantitative but dependent
variable y multiple linear regression
considers more than one quantitative and
qualitative variable
to predict a quantitative and dependent
variable y
we'll look at the two types of analyzes
in more detail in the slides that follow
in simple linear regression the
predictions of the explained variable y
when plotted as a function of the
explanatory of variable X from a
straight line the best fitting line is
called the regression line the output of
this model is a function to predict the
dependent variable on the basis of the
values of the independent variable
the dependent variable is continuous and
the independent variable can be
continuous or discrete
let's look at the different kinds of
linear and non-linear analyzes
list of linear techniques are simple
method of least squares coefficient of
multiple determination standard error of
the estimate dummy variable and
interaction similarly there are many
non-linear techniques available such as
polynomial logarithmic square root
reciprocal and exponential
to understand this model we'll first
look at a few assumptions the simple
linear regression model depicts the
relationship between one dependent and
two or more independent variables the
assumptions which justify the use of
this model are as follows linear and
additive relationship between the
dependent and independent variables
multivariate normality little or no
collinearity in the data little or no
autocorrelation in the data homo
sedasticity that is a variance of Errors
same across all values of X the equation
for this model is shown on the screen
a more descriptive graphical
representation of simple linear
regression is given on the screen beta
naught represents the slope the slope
with two variables implies that one unit
changes in X result in a two unit change
in y
beta 1 represents the estimated change
in the average value of y as a result of
one unit change in x
Epsilon represents the estimated average
value of y when the value of x is zero
this demo will show the steps to do
simple linear regression in r
in this demo you'll learn how to do
simple linear regression
let's use X and Y vectors that we have
created in the previous demo
we also ensured there exists a
relationship between X and Y visually by
plotting a graph
to build simple linear regression model
let's use the LM function
to see how the linear model fits into X
and Y let's plot the linear line by the
a b line function
let's use the predict function to test
or predict the linear model we can pass
a known variable to predict the unknown
variables
let's look at an example of a common use
for linear regression profit estimation
of a company if I was going to invest in
a company I would like to know how much
money I could expect to make so we'll
take a look at a venture capitalist firm
and try to understand which companies
they should invest in so we'll take the
idea that we need to decide the
companies to invest in we need to
predict the profit the company makes and
we're going to do it based on the
company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our r d which is
your research and development we have
our marketing we might have the location
we might have what kind of administrator
situations going through based on all
this different information we would like
to calculate the profit now in actuality
there's usually about 23 to 27 different
markers that they look at if they're a
heavy duty investor we're only going to
take a look at one basic one we're going
to come in and for Simplicity let's
consider a single variable R and D and
find out which companies to invest in
based on that so we take a r d and we're
plotting The Profit based on the r d
expenditure how much money they put into
the research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we draw a line right through
the data when you look at that you can
see how much they invest in the r d is a
good marker as to how much profit
they're going to have we can also note
that companies spending more on R D make
good profit so let's invest in the ones
that spend a higher rate in their r d
what's in it for you first we'll have an
introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rainfall how much would be the crop
yield so here we have our crops we have
our rainfall and we want to know how
much we're going to get from our crops
this year so we're going to introduce
two variables independent and dependent
the independent variable is a variable
whose value does not change by the
effect of other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does control the crop
so we talk about the independent
variable controlling the depend variable
let's Define dependent variable as a
variable whose value change when there
is any manipulation the values of the
independent variables it is often
denoted as Y and you can see here our
crop yield this dependent variable and
it is dependent on the amount of
rainfall received now that we've taken a
look at a real life example let's go a
little bit into the theory and some
definitions on machine learning and see
how that fits together with linear
regression numerical and categorical
values let's take our data coming in and
this is kind of random data from any
kind of project we want to divide it up
into numerical and categorical so
numerical is numbers age salary height
where categorical would be a description
the color a dog's breed gender
categorical is limited to very specific
items where numerical is a range of
information now that you've seen the
difference between numerical and
categorical data let's take a look at
some different machine learning
definitions we look at a different
machine learning algorithms we can
divide them into three areas supervised
unsupervised reinforcement we're only
going to look at supervise today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information until after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polynomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it'd be multiple
linear regression and finally we have
polynomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the DDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of house as
a builder with cell and what price in
the coming months score predictions
Cricket fever to predict the number of
runs a player would score in the coming
matches based on the previous
performance I'm sure you can figure out
other applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some euclidean geometry the simplest
form of a simple linear regression
equation with one dependent and one
independent variable is represented by y
equals m times X plus C and if you look
at our model here we plotted two points
on here X1 and y1 X2 and Y2 y being the
dependent variable remember that from
before and next being the independent
variable so y depends on whatever X is m
in this case is the slope of the line
where m equals the difference in the Y2
minus y1 and X2 minus X1 and finally we
have C which is the coefficient of the
line or where it happens to cross the
zero axes let's go back and look at an
example we used earlier of linear
regression we're going to go back to
plotting the amount of crop yield based
on the amount of rainfall and here we
have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y-axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes
I want you to note that when we get into
the case study and we actually apply
some python script that this math you're
going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was one we got y equals 2
when X was 2 y was 4 and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is whether the means is of each of
the data coming in the X and the Y the
means doesn't mean anything other than
the average so we add up all the numbers
and divide by the total so one plus two
plus three plus four plus five over five
equals three and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get three
comma four which draws a nice line down
the middle a good estimate here we're
going to dig deeper into the math behind
the regression line now remember before
I said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whiz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the X Y points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x times Y and
then it takes all of X and adds them up
all of Y adds them up all of x squared
adds them up and so on and so on and you
can see we have the sum of equal to 15
the sum is equal to 20. all the way up
to x times Y where the sum equals 66
this all comes from our formula for
calculating a straight line where y
equals the slope times X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we'll explain
exactly what that is in just a minute
and where that information comes from is
called the square means error but we'll
go into that in detail in a few minutes
all you need to do is look at the
formula and see how we've gone about
Computing it line by line instead of
trying to you have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x times Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 times 20. and on
the bottom we have the number of lines
times the sum of x squared easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of x
squared and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 0.6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 0.6 and C equals 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2
equals 2.8 and so on and so on and here
the Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e-square if
we put these into a nice chart we can
see X and you can see Y what the actual
values were and you can see why it
predicted you can easily see where we
take y minus y predicted and we get an
answer what is the difference between
those two and if we square that y minus
y prediction squared we can then sum
those squared values that's where we get
the 0.64 plus the 0.36 plus 1 all the
way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least squared distance between
the data points and the regression line
now we only looked at the most commonly
used formula for minimizing the distance
there are lots of ways to minimize the
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc which you want to take away from
this is whatever formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have well we'll start with
the simple linear regression where we
had y equals M plus X plus C and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just X we have X1 X2 X3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have M1 M2 M3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
M1 times X1 plus M2 times X2 so on all
the way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R and D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict as profit but instead of
predicting it just on the R and D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data to see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it in and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy because it's such a
visual to look at and it's so easy to
use just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
were importing first we're going to
import numpy as NP and then I want you
to skip one line and look at import
pandas as PD these are very common tools
that you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your SK learn toolbox you always import
that right off the beginning pandas
although you don't have to have it for
your sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
so we usually like to use the pandas
when we can and I'll show you what that
looks like the other three lines are for
us to get a visual of this data and take
a look at it so we're going to import
matplotlibrary.pi plot as PLT and then
Seaborn as SNS Seabourn works with the
matplot library so you have to always
import matplot library and then Seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seaborn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed matplot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in the slide you'll
see companies equals pd.read CSV and it
has a long line there with the file at
the end one thousand companies.csv
you're going to have to change this to
fit whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals
pd.reader underscore CSV and I've
changed this path to match my computer C
colon slash simply learn slash 1000
underscore companies.csv and then below
there we're going to set the x equals to
companies under the I location and
because this is companies as a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except up
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a panda's data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies dot head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R and D
spend Administration marketing spend
State profit it even notes that the top
top are column names it was never told
that but pandas is able to recognize a
lot of things that they're not the same
as the data rows why don't we go ahead
and open this file up in a CSV so you
can actually see the raw data so here
I've opened it up as a text editor and
you can see at the top we have r d spend
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
165
349.20 compared to the administration
cost of 136
897.80 so on so on helps to create the
the profit of 192 261 and 83 cents that
makes no sense to me whatsoever no pun
intended so let's flip back here and
take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so at this point we're
going to use a single line of code to
get a lot of information so we can see
where we're going with this let's go
ahead and paste that into our notebook
and see what we got going and so we have
the visualization and again we're using
SNS which is pandas as you can see we
imported the matplot Library dot Pi plot
is PLT which then the Seaborn uses and
we imported the Seaborn as SNS and then
that final line of code helps us show
this in our inline coding without this
it wouldn't display and you could
display it to a file in other means and
that's the matplot library in line with
the Amber sign at the beginning so here
we come down to the single line of code
Seabourn is great because it actually
recognizes the panda data frame so I can
just take the comp companies dot core
for coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one we have
the columns and the rows we have R and D
spending we have Administration we have
marketing spending and profit and if you
cross index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so r d spending is going
to be the same as r d spending and the
same thing with Administration costs so
right down the middle you get this dark
row or dark diagonal row that shows that
this is the highest corresponding data
that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with r d spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder instead of equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies dot
head and we printed the top five rows of
data we have our columns going across we
have column zero which is R and D
spending column one which is
Administration column two which is
marketing spending and column three is
State and you'll see under State we have
New York California Florida now to do a
linear regression model it doesn't know
how to process New York it knows how to
process a number so the first thing
we're going to do is we're going to
change that New York California and
Florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in Brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
to take the label coder and we're going
to fit and transform the X also the
third row so we're going to take that
third row we're going to set it equal to
a transformation and that transformation
basically tells it that instead of
having a New York it has a zero or a one
or a two and then finally we need to do
a one hot encoder which equals one hot
encode or categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it is just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computers
automatically transformed our header
into the setup and it's automatically
transformed all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a predictive model of
data we always want to split it up so we
have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our RND spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test a split we take X
and we take y we've already created
those X has the columns with the data in
it and Y has a column with profit in it
and then we're going to set the test
size equals 0.2 that basically means 20
percent so twenty percent of the rows
are going to be tested we're going to
put them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the sklearn dot linear
underscore model we're going to import
linear regression now I don't know if
you recall from earlier when we were
doing all the math let's go ahead and
flip back there and take a look at that
do you remember this or we had this long
formula on the bottom and we were doing
all this summarization and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor.fet in this case we do X
train and Y train because we're using
the training data X being the data n and
y being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a y
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor dot predict X test going in
and this gives us y predict now because
I'm in Jupiter in line I can just put
the variable up there and when I hit the
Run button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
jupyter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side is going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when I run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 times X1 plus M2
times X2 and so on and so on plus C the
coefficient so these variables fit right
into this formula y equals slope one
times column one variable plus slope two
times column two variable all the way to
the m into the n and x to the N plus C
the coefficient or in this case you have
minus 8.89 to the power of two etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squared value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from
sklearn.metrix we're going to import R2
score that's the r squared value we're
looking at the error so in the R2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us so we know are true the Y predict
of those 200 values is what we think it
was true and when we go ahead and run
this we see we get a
0.9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93 percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the R2 score and if
r squared value of 0.91 or 92 as we got
on our model could remember it does have
a random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model let's take a look at what we went
over today and take a look at our key
takeaways first if you are an aspiring
data scientist who is looking out for
online training and certification in
data science from the best universities
and Industry experts then search now on
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below
what is logistic regression let's say we
have to build a predictive model or a
machine learning model to predict
whether the passengers of the Titanic
ship have survived or not the Shipwreck
so how do we do that so we use logistic
regression to build a model for this how
do we use logistic regression so we have
the information about the passengers
their ID whether they have survived or
not their class and name and so on and
so forth and we use this information
where we already know whether the person
has survived or not that is the labeled
information and we help the system to
train based on this information with
based on this labeled data this is known
as label data and during the process of
building the model we probably will
remove some of the non-essential
parameters or attributes here we only
take those attributes which are really
required to make these predictions and
once we train the model we run new data
data through it whereby the model will
predict whether the passenger has
survived or not so let's see what we
will learn in this video we will talk
about what is supervised learning and we
will go into details about
classification which is one of the
techniques for supervised learning and
then we will further focus on logistic
regression is which is one of the
algorithms for performing
classifications especially binary
classification then we will compare
linear and logistic regression and what
are some of the logistic regression
applications and finally we will end
with a use case or a demo of actual
python code for doing logistic
regression in jupyter number all right
so let's start with what is supervised
learning supervised learning is one of
the two main types of machine learning
methods here we use what is known as
labeled data to help the system learn
this is very similar to how we human
beings learn so let's say you want to
teach a child to recognize an apple how
do we do that we never tell the child
okay this is an apple has a certain
diameter on the top a certain diameter
at the bottom and this has a certain RGB
color no we just show an apple to the
child and tell the child this is Apple
and then next time when we show an apple
child immediately recognizes yes this is
an app supervised learning works very
similar on the similar lines so where
does logistic regression fit into the
overall machine learning process machine
learning is divided into two types
mainly two types there is a third one
called reinforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
users techniques like classification and
regression now supervised learning is
used when you have labeled data you have
history historical data then you use
supervised learning when you don't have
labeled data then you used unsupervised
learning it's it's supervised learning
there are two types of techniques that
are used classification and regression
based on what is the kind of problem we
have solved let's say we want to take
the data and classify it it could be
binary classification like a 0 or a one
an example of classification we have
just seen whether the passenger has
survived or not survived like a zero or
one that is known as binary
classification regression on the other
hand is you need to predict a value what
is known as a continuous value
classification is for discrete values
regression is for continuous values
let's say you want to predict a share
price or you want to predict the
temperature that will be there what will
be the temperature tomorrow that is
where you use the regression whereas
classification are discrete values is
will the customer buy the product or
will not buy the product will you get a
promotion you will not get a promotion I
hope you're getting the idea or it could
be multi-class classification as well
let's say you want to build an image
classification model so the image
classification model would take an image
as an input and classify into multiple
classes whether this image is of a cat
or a dog or an elephant or a tiger so
there are multiple classes so not
necessarily binary classification so
that is known as multi-class
classification so we are going to focus
on classification because logistic
regression is one of the algorithms used
for classification now the name may be a
little confusing in fact whenever people
come across logistic regression it
always causes confusion because the name
has regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you are wondering is there
something similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we are going to
focus on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few addition trees are
quite popular and very easy to
understand and therefore they use for
classification then we have K nearest
neighbors this is another algorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into little bit
of details about logistic regression all
right what is logistic regression as I
mentioned earlier positive regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if it is going to break down in the near
future so this is like a classification
problem find out whether your car will
break down or not so how are we going to
perform this classification so here's
how it looks if we plot the information
along the X and Y axis X is the number
of years since the last service was
performed and Y is the probability of
your car breaking down and let's say
this information was this data rather
was collected from several car users
it's not just your car but several car
users so that is our labeled data so the
data has been collected and for for the
number of years and when the car broke
down and what was the probability and
that has been plotted along X and Y axis
so this provides an idea or from this
graph we can find out whether your car
will break down or not we'll see how so
first of all the probability can go from
zero to one as you all aware probability
can be between 0 and 1 and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year two years or three years
still after the service the chances of
your car breaking down are very limited
right so for example chances of your car
breaking down on the probability of your
car breaking down within two years of
your last service are 0.1 probability
similarly three years is maybe 0.3 and
so on but as the number of years
increases let's say if it was six or
seven years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the why that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or not yes or no that is
what we are talking about so here the
outcome is discrete and not a continuous
way so this is how the logistic
regression curve looks let me explain a
little bit what exactly how exactly we
are going to
recommend the class at the outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be 0 or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be 0 and any value above 0.5 is
considered to be 1. so an output of
let's say 0.8 will mean that the car
will break down so that is considered as
an output of 1 and let's say an output
of 0.29 is considered as 0 which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you are going to
find a continuous value you are going to
determine a continuous value it could be
the price of a real estate property it
could be your height how much height
you're going to get or it could be a
stock price these are all continuous
values these are not discrete compared
to a yes or no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is straight and it learns from
this labeled information so that when a
new employee's information is fed based
on the rating it will determine what
should be the high so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get five
thousand five thousand five hundred five
thousand six hundred it is not discrete
like a cat or a dog or an apple or a
banana these are discrete or a yes or a
no these are discrete values right so
this way you are trying to find
continuous values is where we use linear
regression so let's say just to extend
on this scenario we now want to find out
whether this employee is going to get a
promotion or not so we want to find out
if that is the discrete problem right a
yes or no kind of a problem in this case
we actually cannot use linear regression
even though we may have labeled data so
this is the label date So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employer's
got promotion or not yes no right so
there is nothing in between and what is
the employee's rating and ratings can be
continuous that is not an issue but the
output is discrete in this case whether
employee got promotion yes no okay so if
we try to plot that and we try to find a
straight line this is how it would look
and as you can see it doesn't look very
right because looks like there will be
lot of errors this root mean square
error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0 and 1. but still the straight line
doesn't look right therefore instead of
using a linear equation we need to come
up with something different and
therefore the logistic regression model
looks somewhat like this so we calculate
the probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and this is how the math behind logistic
regression looks so we are trying to
find the odds for a particular event
happening and this is the formula for
finding the odd so the probability of an
event happening divided by the
probability of the event not happening
so P if it is a probability of the event
happening probability of the person
getting a promotion and divided by the
probability of the person not getting a
promotion that is 1 minus p
this is how you measure the odds now the
values of the odds range from 0 to
Infinity so when this probability is 0
then the odds will the value of the odds
is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1. now this is how an
equation of a straight line love so Y is
equal to beta0 plus beta 1 x where beta0
is the y-intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of p x by 1 minus p x
this is an extension of the calculation
of odds that we have seen right and that
is equal to beta0 plus beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of p x you will see we can
take the exponential on both sides and
then if we solve that equation we will
get the equation of p x like this p x is
equal to 1 by 1 plus e to the power of
minus beta0 plus beta 1 x and recall
this is nothing but the equation of the
line which is equal to y y is equal to
Beta 0 plus beta 1 X so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression and all right and if
this is plotted this is how the sigmoid
curve is obtained so let's compare
linear and logistic regression how they
are different from each other let's go
back so linear regression is solved or
used to solve regression problems and
logistic regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and linear regression
helps to estimate the dependent variable
when there is a change in the
independent variable whereas here in
case of logistic regression it helps to
calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called or linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction whether it's going to rain or
not drain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not it
is going to be sunny or not it is going
to snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are multi
class classification so logistic
regression is not used in its original
form but it is used in a slightly
different form so we say whether it is a
dog or not a dog I hope you understand
so instead of saying is it a dog or a
cat or a elephant we convert this into
saying so because we need to keep it to
Binary classification so we say is it a
dog or not a dog is it a cat or not a
cat so that's the way logistic
regression can be used for classifying
objects otherwise there are other
techniques which can be used for
performing multi-class classification
Healthcare plastic regression is used to
find the survival rate of a patient so
they take multiple parameters like drama
score and age and so on and so forth and
they try to predict the rate of survival
all right now finally let's take an
example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jupyter notebook and show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an eight by eight image and
there the image has a number one two
three four and you need to train your
model to predict what this number is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set and
test set with the training set we train
our model and then with the test set we
test the model till we get good accuracy
and then we use it for for inference
right so that is typical methodology of
training testing and then deploying of
machine learning models so let's take a
look at the code and see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images or a
sample of these images and then we split
the data into training and test as I
mentioned earlier and we can do some
exploratory analysis and then we build
our model we train our model with the
training set and then we test it with
our test set and find out how accurate
our model is using the confusion Matrix
the heat map and use heat map for
visualizing this and I will show you in
the code what exactly is the confusion
Matrix and how it can be used for
finding the accuracy in our exam sample
we get an accuracy of about 0.94 which
is pretty good or 94 which is pretty
good all right so what is the confusion
Matrix this is an example of a confusion
Matrix and this is used for identifying
the accuracy of a classification model
or like a logistic regression model so
the most important part in a confusion
Matrix is that first of all this as you
can see this is a matrix and the size of
the Matrix depends on how many outputs
we
so the the most important part here is
that the model will be most accurate
when we have the maximum numbers in its
diagram like in this case that's why it
has almost 93 94 percent because the
diagonals should have the maximum
numbers and the others other than
diagnose the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and actual value are the same so
along the diagonals that is true which
means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed are rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two misclassifications
okay so that is the meaning of saying
that the maximum number should be in the
diagonal so if you have all of them so
for an ideal model which has let's say
100 accuracy everything will be only in
the diagonal there will be no numbers
other than zero in all other cells so
that is like a hundred percent accurate
model okay so that's uh just of how to
use this Matrix how to use this
confusion Matrix I know the name uh is a
little funny sounding confusion Matrix
but actually it is not very confusing
it's very straight forward so you are
just plotting what has been predicted
and what is the labeled information on
what is the actual data that's also
known as the ground truth sometimes okay
these are some fancy terms that are used
so predicted label and the actual name
that's all right okay yeah so we are
showing a little bit more information
here so 38 have been predicted and here
you will see that all of them have been
predicted correctly there have been 38
zeros and
the predicted value and the actual value
is is exactly the same whereas in this
case right it has uh there are I think
37 plus 5 yeah 42 have been fed the
images 42 images are of Digit 3 and the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in jupyter Notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correct a or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and then the
last line in this block is to load the
digits so let's go ahead and run this
code then here we will visualize the
shape of these digits so we can see here
if we take a look this is how the shape
is 1797 by 64. these are like eight by
eight images so that's that's what is
reflected in this shape now from here
onwards we are basically once again
importing some of the libraries that are
required like numpy and matplot and we
will take a look at some of the sample
images that we have loaded so this one
for example creates a figure and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so so these are about five
images sample images that we are looking
at zero one two three four so this is
how the images this is how the data is
okay and based on this we will actually
train our logistic regression model and
then we will test it and see how well it
is able to recognize so the way it works
is the pixel information so as you can
see here this is an eight by eight pixel
kind of a image and the each pixel
whether it is activated or not activated
that is the information available for
each pixel now based on the pattern of
this activation and non-activation of
the various pixels this will be
identified as a zero for example right
similarly as you can see so overall each
of these numbers actually has a
different pattern of the pixel
activation and that's pretty much that
our model needs to learn for which
number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and tests so that the training
data set is used to train the system so
we pass this probably multiple times and
then we test it with the test data set
and the split is usually in the form of
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as 20.23
that means 23 percent of that entire
data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and we pass the data that we
want to split and the the results will
be stored in X underscore train and Y
underscore train for the training data
set and what is X underscore train these
are these are the features right which
is like the independent variable and why
underscore train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in X underscore train and
since this is a labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that that's this is what will
be used for compare Verizon to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way X underscore train
y underscore train is for the training
data set X underscore test y underscore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so X
underscore train the shape is 1383 by 64
and Y underscore train has 1383 because
there is nothing like the second part is
not required here and then X underscore
test shape we see is 414 so actually
there are 414 observations in test and
1383 observations in train so that's
basically what these four lines of code
are are saying okay then we import the
logistic regression library and which is
a part of scikit-learn so we we don't
have to implement the logistic
regression process itself we just call
these uh the function and let me go
ahead and execute that so that we have
the logistic regression Library imported
now we create an instance of logistic
regression right so logistic regr is a
is an instance of logistic regression
and then we use that for training our
model so let me first execute this code
so these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line way is where we are passing
our data the training data set right
this is our the the predictors and this
is our Target we are passing this data
set to train our model all right so once
we do this in this case the data is not
large but by and large and the training
is what takes usually a lot of time so
we spend in machine learning activities
in machine learning projects we spend a
lot of time for the training part of it
okay so here the data set is relatively
small so it was pretty quick so all
right so now our model has been trained
using the training data set and we want
to see how accurate this is so what
we'll do is we will test it out in
probably faces so let me first try out
how well this is working for one image
okay I will just try it out with one
image my the first entry in my test data
set and see whether it is uh correctly
predicting or not so and in order to
test it so for training purpose we use
the fit method there is a method called
fit which is for training the model and
once the training is done if you want to
test for a particular value new input
you use the predict method okay so let's
run the predict method and we pass this
particular image and we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 seems to be fine so let me just
go ahead and test the entire data set
okay that's basically what we will do so
now we want to find out how accurately
this has is performed so we use the
score method to find what is the
percentages of accuracy and we see here
that it has performed up to 94 percent
accurate okay so that's on this part now
what we can also do is we can
um also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well so that we
can also visualize how well this model
has done so let me execute this piece of
code which will basically import some of
the libraries that are required and we
we basically create a confusion Matrix
an instance of confusion matrix by
running confusion Matrix and passing
these values so we have so this
confusion underscore Matrix method takes
two parameters one is the Y underscore
test and the other is the prediction so
what is the Y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us and this is what the system has
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
of them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that so this means that the
actual value is same as the predicted
value here again actual value is same as
the predictive value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 plus 44 Plus 43 and so
on and divide that by the total number
of test observations that will give you
the percentage accuracy using a
confusion Matrix now let us visualize
this confusion Matrix tricks in a
slightly more sophisticated way using a
heat map so we will create a heat map
with some We'll add some colors as well
it's uh it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks and as you can see here the
diagonals again are all the values are
here most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94 percent this is calculated
as I mentioned by adding all these
numbers divided by the total test value
so the total number of observations in
test data set okay so this is the
confusion Matrix for logistic regression
all right so now that we have seen the
confusion Matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we picked up randomly a few of them so
this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of 9. so this has also been
predicted correctly nine and actual
value is 9 and this is the image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images
so we put them aside about side we have
our linear regression which is a
predictive number
used to predicted dependent output
variable based on Independent input
variable
accuracy is a measured using least
squares estimation
so that's where you take it you could
also use absolute value the least
squares is more popular there's reasons
for that mathematically and also for
computer runtime
but it does give you an accuracy based
on the the least Square estimation the
best fit line is a straight line and
clearly that's not always used in all
the regression models there's a lot of
variations on that the output is a
predicted integer value
again this is we're talking about we're
talking about linear regression and
we're talking about regression it means
the numbers coming out linear usually
means we're looking for that line
versus a different model and it's used
in business domain forecasting stocks
it's used as a basis of almost of most
predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at a linear regression
model
for instance if you do just the high
lows of the stock exchange and you're
going to take a lot more of that if you
want to make money off the stock
you'll find that the linear regression
model fits uh probably better than
almost any of the other models or even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value a low
value volume that kind of thing so when
you're looking at something that's
straight numbers and are connected in
that way usually you're talking about a
linear regression model that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in and so in this case
you might have a label you know an image
or something like that is is probably
the very popular thing right now
labeling broccoli and vegetables or
whatever
accuracy is measured using maximum
likelihood estimation the best fit is
given by a curve and we saw that um
we're talking about linear regression
you definitely aren't talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
euclidean line but it's now got that
sigmoid activation which turns it into a
heavily weighted curve and the output is
a binary value between 0 and 1. and it's
used for classification image processing
as I mentioned is is what people usually
think of
although they use it for classification
of
like a window of things so you could
take a window of stock history and you
could clap generate classifications
based on that and separate the data that
way if it's going to be that this
particular pattern occurs is going to be
upward trending or downward trending
in fact a number of stock
Traders use that not to tell them how
much to bid or what to bid but they use
it as to whether it's worth looking at
the stock or not whether the Stock's
going to go down or go up and it's just
a zero one do I care do I even want to
look at it so let's do a demo so you can
get a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history
we will also classify on a mushroom data
set to find the poisonous and
non-poisonous mushrooms
and when you look at these two datas the
first one we're looking at the price so
the price is a number so let's predict
the price which the insurance should be
sold to and the second one is we're
looking at either as poisonous or it's
not poisonous so first off before we
begin the demo I'm in the Anaconda
Navigator
in this one I've loaded the python 3.6
and using the Jupiter notebook and you
can use jupyter notebook by itself you
can use the Jupiter lab which allows
multiple tabs it's basically the
notebook with tabs on it
but the Jupiter notebook is just fine
and it'll go into Google Chrome which is
what I'm using for my Internet Explorer
and from here we open up new and you'll
see python3 and again this is loaded
with python 3.6
and we're doing the linear versus logic
regression or logit you'll see l-o-g-i-t
um is one of the one of the names that
kind of pops up when you do a search on
here but it is a logic we're looking at
the logistic regression models
and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
and so in programming we got a lot of
stuff to unfold here in our in our
startup as we pre-load all of our
different parts
and let's go ahead and break this up we
have at the beginning import pandas
so this is our data frame uh it's just a
way of storing the data think of a you
talk about a data frame think of a
spreadsheet a rows and columns it's a
nice way of viewing the data
and then we have we're going to be
bringing in our pre-processing labeling
coder I'll show you what that is when we
get down to it it's easier to see in the
data but there's some data in here like
sex it's male or female so it's not like
an actual number it's a easier one or
the other that kind of stuff ends up
being encoded that's what this label
encoder is right here
we have our test split model
if you're going to build a model you do
not want to use all the data you want to
use some of the data and then test it to
see how good it is and if it can't have
seen the data you're testing on until
you're ready to test it on there and see
how good it is
and then we have our logistic regression
model our categorical one and then we
have our linear regression model these
are the two these right here let me just
um
clear all that there we go these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for a specific
number
and then finally usually at the very end
we have to take and just ask how
accurate is our model did it work if
you're trying to predict something in
this case we're going to be doing
um
Insurance costs how close to the
insurance cost does it measure that we
expect it to be you know if you're an
insurance company you don't want to
promise to pay everybody's medical bill
and not be able to
and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as far as whether you're going to get a
you know poisonous mushroom and die or
not God
so we'll look at both of those and we'll
get talk a little bit more about the
shortcomings and the value of these
different processes so let's go ahead
and run this this is loaded the data set
on here and then because we're in
jupyter Notebook I don't have to put the
print on there we just do data set and
by and it prints out all the different
data on here and you can see here for
our insurance because that's what we're
starting with we're loading that with
our pandas
and it prints it in a nice format where
you can see the age sex body mass index
number of children smoker so this might
be something that the insurance company
gets from the doctor it says hey we're
gonna this is what we need to know
to give you a quote for what we're going
to charge you for your insurance
and you can see that it has 1 338 rows
and seven columns you can count the
columns one two three four five six
seven so there's seven columns on here
and the column we're really interested
in is charges I want to know what the
charges are going to be what can I
expect
not a very good Arrow drawn
what to expect them to charge on there
so is this going to be you know is this
person going to cost me uh 16 884
dollars or is this person only going to
cost me uh
3866. how do we guess that so that we
can guess what the minimal charge is for
their insurance
thing you really need to notice on this
data and I mentioned it before but I'm
going to mention it again because
pre-processing data is so much of the
work in data science
um sex well how do you how do you deal
with female versus male are you a smoker
yes or no what does that mean region how
do you look at regions not a number how
do you draw a line between Southwest and
Northwest
um you know they're objects it's either
your Southwest or your Northwest it's
not like I'm southwest I guess you could
do longitude and latitude but the data
doesn't come in like that it comes in as
true false or whatever you know it's
either your Southwest or your Northwest
so we need to do a little bit of
pre-processing of the data on here to
make this work
oops there we go okay so let's take a
look and see what we're doing with
pre-processing
and again this is really where you spend
a lot of time with data science is
trying to understand how and why you
need to do that and so we're going to do
you'll see right up here
label uh and then we're going to do the
do a label encoder one of the modules we
brought in so this is sklearns label
encoder
I like the fact that it's all pretty
much automated but if you're doing a lot
of work with label encoder you should
start to understand how that fits
and then we have uh label dot fit
right here where we're going to go ahead
and do the data set dot sex dot drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at male or female
and so it usually just converts it to a
zero one because there's only two
choices on here
same thing with the smoker it's zero or
one so we're going to transfer the
transa change the smoker 0 1 on this
and then finally we did region down here
region does it a little bit different
we'll take a look at that and it it's I
think in this case it's probably going
to do it because we did it on this label
transform
with this particular setup it gives each
region a number like 0 1 2 3. so let's
go ahead and take a look and see what
that looks like go and run this
and you can see that our new data set
has age that's still a number six is
zero or one so zero is female one is
male
number of children we left that alone uh
smoker one or zero says no or yes on
there we actually just do one for no
zero or no yeah one for no I'm not sure
how it organized them but it turns the
smoker into zero or one yes or no
and then region it did this as 0 1 2 3
so it's three regions
now a lot of times in in when you're
working with data science and you're
dealing with regions or even word
analysis
um instead of doing one column and
labeling it zero one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true zero
one but for this this particular setup
this will work just fine on here now
that we spent all that time getting it
set up uh here's the fun part uh here's
the part where we're actually using our
setup on this and you'll see right here
we have our
why linear regression data set drop the
charges because that's what we want to
predict
and so our X I'm sorry our X linear data
set drop the charges because that's
we're going to predict we're predicting
charges right here so we don't want that
as our input for our features
and our y output is charges that's what
we want to guess we want to guess what
the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
0.3 means 30 percent we're going to take
30 percent of our data and it's going to
be as the trade as the testing site so
here's our y test and our X test down
there
um and so that part our model will never
see it until we're ready to test to see
how good it is and then of course right
here you'll see our training set and
this is what we're going to train it
we're going to trade it on seventy
percent of the data
and then finally the big ones this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model
and then we're going to fit the data on
here
and then at this point I always like to
pull up
um if you if you if you're working with
a new models good to see where it comes
from and this comes from the site kit
learn
and this is the SK learn linear model
linear regression that we imported
earlier
and you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers I mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit
um and what you one of the things
when I'm looking at this as I look for
methods
and you'll see here's our fit that we're
using right now and here's our predict
and we'll actually do a little bit in
the middle here as far as looking at
some of the parameters hidden behind it
the math that we talked about earlier
and so we go in this we go ahead and run
this
you'll see it loads the linear
regression model and just has a nice
output that says hey I loaded the linear
regression model
and then the second part is we did the
fit and so this model is now trained our
linear model is now trained on the
training data
and so one of the things we can look at
is the for idx and column name and
enumerate X linear train columns
kind of an interesting thing this prints
out the coefficients so when you're
looking at the back end of the data you
remember we had that formula BX X1 plus
bxx2 plus the inner plus the intercept
and so forth these are the actual
coefficients that are in here this is
what it's actually multiplying these
numbers by
and you can see like region gets a minus
value so when it heads it up I guess a
region you can read a lot into these
numbers uh it gets very complicated
there's ways to mess with them if you're
doing a basic linear regression model
usually don't look at them too closely
but you might start looking in these and
saying hey you know what uh smoker look
how smoker impacts the cost
it's just massive so this is a flag that
hey the value of the smoker really
affects this model
and then you can see here where the body
mass index so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor and then you can see
down here we have sexist in a factor
also and it just it changes as you go in
there negative number it probably has
its own meaning on there
again it gets really complicated when
you dig into the workings and how the
linear model works on that and so we can
also look at the intercept this is just
kind of fun so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that
and so you can see right here we can go
back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out
and if you're going to create a model to
predict something we'll go ahead and
predict it here's our y prediction value
linear model predict
and then we'll go ahead and create a new
data frame in this case from our X
linear test group
we'll go ahead and put the cost back
into this data frame and then the
predicted cost we're going to make that
equal to our y prediction
and so when we pull this up you can see
here that we have the actual cost and
what we predicted the cost is going to
be
there's a lot of ways to measure the
accuracy on there but we're going to go
ahead and jump into our mushroom data
and so in this you can see here we've
run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here
now with mushrooms we want a yes or no
we want to know whether we can eat them
or not
and so here's our mushroom file we're
going to run this take a look at the
data and again you can ask for a copy of
this file send a note over to
simplylearn.com
and you can see here that we have a
class the cap shape cap surface and so
forth so there's a lot of feature in
fact there's 23 different columns in
here going across
and when you look at this I'm not even
sure what these particular like p e e p
e I don't even know what the class is on
this
I'm going to guess by the notes that the
class is poisonous or edible
so if you remember before we had to do a
little pre-coding on our data same thing
with here we have our cap shape which is
b or X or k
we have cap color these really aren't
numbers so it's really hard to do
anything with just a a single number so
we need to go ahead and turn those into
a label encoder which again there's a
lot of different encoders with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer value
in fact if you look at all the columns
all of our columns are labels and so
we're just going to go ahead and loop
through all the columns in the data and
we're going to transform it into a label
encoder and so when we run this you can
see how this gets shifted from
xbxxk to zero one two three four five or
whatever it is class is 0 1 1 being
poisonous zero looks like it's editable
and so forth on here so we're just
encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring zero one two three
four five in this particular example
it's not going to make that big of a
difference how we encode it
and then of course we're looking for the
class whether it's poisonous or edible
so we're going to drop the class in our
X Logistics model and we're going to
create our y Logistics model is based on
that class so here's our x y
and just like we did before we're going
to go ahead and split it
using 30 percent for tests
seventy percent to program the model on
here
and that's right here whoops there we go
there's our train and test
and then you'll see here on this next
setup
this is where we create our model all
the magic happens right here
we go ahead and
create a logistics model I have up the
max iterations if you don't
change this for this particular problem
you'll get a warning that says this has
not converged
because that that's what it does is it
goes through the math and it goes hey
can we minimize the error and it keeps
finding a lower and lower error and it
still is changing that number so that
means it hasn't conversed yet it hasn't
find the lowest amount of error it can
and the default is 100. there's a lot of
settings in here so when we go in here
to let me pull that up from the sklearn
so we pull that up from the SK learn
model
you can see here we have our logistic it
has our different settings on here that
you can mess with most of these work
pretty solid on this particular setup so
you don't usually mess a lot usually I
find myself adjusting the iteration and
I'll get that warning and then increase
the iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our methods
and you can see there's a lot of methods
available on here and certainly there's
a lot of different things you can do
with it
but the most basic thing we do is we fit
our model make sure it's set right and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced but for the most part
these are the two which I use when I'm
going into one of these models and
setting them up
so let's go ahead and close out of our
sklearn setup on there
and we'll go ahead and run this
and you can see here it's now loaded
this up there we now have a
logistic model and we've gone ahead and
done a predict here also just like I was
showing you earlier
so here's where we're actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
I think I just showed you graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the model score on this one let me let
me just go ahead and run the model score
and it says that it's pretty accurate
we're getting a roughly 95 percent
accuracy well that's good one 95
accuracy
95 accuracy might be good for a lot of
things
but when you look at something as far as
whether you're going to pick a mushroom
on the side of the trail and eat it we
might want to look at the confusion
Matrix and for that we're going to put
in our y logistic test the actual values
of edible and unedible and we're going
to put in our prediction value
and if you remember on here let's see I
believe it's poisonous was one zero is
edible
so let's go ahead and run that 0 1 0 is
good so here is the confusion Matrix and
this is if you're not familiar with
these we have true true true false
true false false false
so it says out of the edible mushrooms
we correctly labeled 1201 mushrooms
edible that were edible and we correctly
measured
1113 poisonous mushrooms as poisonous
but here's the kicker
I labeled
56 edible mushrooms as being
poisonous well that's not too big of a
deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous
so probably not the best choice to use
this model to predict whether you're
going to eat a mushroom or not
and you'd want to dig a Little Deeper
before you start picking mushrooms off
the side of the trail so a little
warning there when you're looking at any
of these data models looking at the
error and how that error fits in with
what domain you're in domain in this
case being edible mushrooms
be a little careful make sure that
you're looking at them correctly
so we've looked at uh edible or not
edible we've looked at a regression
model as far as the N values what's
going to be the cost and what our
predicted cost is so we can start
figuring out how much to charge these
people for their insurance
and so these really are the fundamentals
of data science when you pull them
together when I say data science to talk
about your machine learning code
classification is probably one of the
most widely used tools in machine
learning in today's world and is also
one of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithms and there are
a lot out there so we're only going to
touch briefly on a variety of them so
you can see how they the different
flavors work and we'll have some
Hands-On Demos in Python embedded
throughout the tutorial classification
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and and where
you end when you start running these
machine learning algorithms and
classification and the classification
algorithms is a little black box in a
lot of respects and we'll look into that
you can see what I'm talking about when
we start swapping in and out different
models let's say we are given the task
of classifying a given bunch of fruits
and vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be
grouped together and so we have a data
set we'll call it the bunch is divided
into clusters one of which consists of
the fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out an images
to see what is malignant what does B9
very popular one can you classify
flowers the iris data set certainly in
Wildlife can you classify different
animals and track where they're going
classification is really the bottom
starting point or the Baseline for a lot
of machine learning in setting it up and
trying to figure out how we're going to
break the data up so we can use it in a
way that is beneficial so here the
fruits and the vegetables are grouped
into clusters and each clusters has a
specific characteristic I.E whether they
are a fruit or a vegetable and you can
see we have a pile of fruits and
vegetables we feed it into the algorithm
and the algorithm separates them out and
you have fruits and vegetables so some
important terminologies before we dig
into how it sorts them out and what that
all means we look at the terminologies
you have a classifier that's the
algorithm that is used to map the input
data to a specific category the
classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times a classifier
is part of of the classification model
and then you choose which classifier you
use after you choose which model you're
using where features are what goes in
labels are what comes out so you
classifier model is right in the middle
of that that's that little black box we
were just talking about clusters they
are a group of data points which have
some common characteristics binary
classification it is a classification
condition with two outcomes which are
either true or false multi-label
classification this is a classification
condition where each sample is assigned
to a set of labels or targets
multi-class classification the
classification with more than two
classes here each sample is assigned to
one and only one label when we look at
this group of terminologies a few
important things to notice going from
the top clusters when we cluster data
together we don't necessarily have to
have an end goal we just want to know
what features clustered together these
features then are mapped to the outcome
we want in many cases the first step
might not even care about the outcome
only about what data connects with other
data and there's a lot of clustering
algorithms out there that do just the
clustering part binary classification it
is a classification condition with two
outcomes which are either true or false
we're talking usually it's a it's either
a cat or it's not a cat it's either a
dog or it's not a dog that's the kind of
thing we talk about binary
classification and then that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multi-class classification where
each one's a binary you can either be a
cat or a dog you can't be both a cat and
a dog real world applications so to make
sense of this of course challenge is
always in the details is to understand
how we apply this in the real world so
in real world applications we use this
all the time we have email spam
classifier so you have your email inbox
coming in it goes through the email
filter that we usually don't see in the
background and it goes this is either
valid email or it's a Spam and it puts
it in the spam filter if that's what it
thinks it is Alexa's voice classifier
Google Voice any of the voice
classifiers they're looking for points
so they try to group words together and
then they try to find those groups so
words trigger a classifier so it might
be that the classifier is to open your
tasks program or open your text program
so she can start sending a text
sentimental sentiment analysis is really
big when we're tracking products we're
tracking marketing trying to understand
whether something is liked or disliked
is huge that's like one of the biggest
driving forces in sales nowadays and you
almost have to have these different
filters going on if you're running a
large business of any kind fraud
detection you can think of banks they
find different things on your bank
statement and they detect that there's
something going on there they have
algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with a classification
when the wild animals in our area are
out like foxes we actually have a
mountain lion that lives in the area so
it's nice to know when he's here
handwriting prediction classifying A B C
D and then classifying words to go with
that so let's go ahead and roll our
sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms let's go back
and take a look at our definitions we
have a classifier and a classification
model so we're looking at the classifier
an algorithm that is used to map the
input data to a specific category one of
those algorithms is a logistic
regression the logistic regression is
the classification algorithm used to
model the probability of a certain class
or event existing such as pass fail or
win lose Etc it provides its output
using the logistic function or sigmoid
function to return the probability value
that can then be mapped to two or more
discrete classes a sigmoid function is
an activation function that fits the
variable and limit the output to a range
between 0 and 1. a standard sigmoid
function or logistic function is
represented by the formula FX equals 1
over 1 plus e to the minus X where X is
the equation of the line and E is the
exponential just taking a quick look at
this you can think of this as being a
point of uncertainty and so as we get
closer and closer to the middle of the
line it's either activated or not and we
want to make that just shoot way up so
you'll see a lot of the activation form
this kind of have this nice S curve
where it approaches one and approaches
zero and based on that there's only a
small region of error and so you can see
in the sigmoid logistic function the one
over one plus e minus x to the minus X
you can see in frames that that nice S
curve we also can use a tangent
variation there's a lot of other
different models here as far as the
actual algorithm this is the most
commonly used one let's go ahead and
roll up our sleeves and take a look at a
demo that is going to use a logistic
regression so we're going to have the
activation formula and the model because
you have to have you have to have both
for this we will go into our Jupiter
notebook now I personally use the
Anaconda navigator to open up the
Jupiter notebook to set up my IDE as a
web-based it's got some advantages that
it's very easy to display in but it also
has some disadvantages in that if you're
trying to do multi-threads and
multi-processing you start running into
to single get issues with python and
then I jump to pycharm really depends on
whatever ID you want just make sure
you've installed numpy and the sklearn
modules into your python in whatever
environment you're working in so that
you'll have access to that for this demo
now the team in the back has prepared my
code for me which I'll start bringing in
one section at a time so we can go
through it before we do that it's always
nice to actually see where this
information is coming from and what
we're working with so the first part is
we're going to import our packages which
you need to install into your Python and
that's going to be your numpy we usually
use numpy as in p and then from SK learn
the learn model we're going to use a
logistic regression and from
sklearnmetrics we're going to import the
classification report confusion Matrix
and if we go ahead and open up S the
site kit-learn.org and go under their
API you can see all the different
features and models they have and we're
looking at the linear model logistic
regression one of the more common
classifiers out there and if we go ahead
and go into that and dig a Little Deeper
you'll see here where they have the
different settings it even says right
here note the regularization is applied
by default so by default that is the
activation formula being used now we're
not going to spend we might come back to
this look at some of the other models
because it's always good to see what
you're working with but let's go ahead
and jump back in here and we have our
Imports we're going to go ahead and run
those so these are now available to us
as we go through our Jupiter notebook
script and they put together a little
piece of data for us this is simply
going through zero to one actually let's
go ahead and print this out over here
we'll go ahead and print X just so you
can see we're actually looking for and
when we run this you can see that we
have our X 0 1 2 through 9 we reshaped
it the reason for this is just looking
for a row of data usually we have
multiple features we just have the one
feature which happens to be zero through
nine and then we have our 10 answers
right down here 0 1 0 0 1 1 1 1 1. you
can bring in a lot of different data
depending on what you're working with
you can make your own you can instead of
having this as just a single you could
actually have like multiple features in
here but we just have the one feature
for this particular demo and this is
really where all the magic happens right
here and I told you it's like a black
box that's the part that is is kind of
hard to follow and so if you look right
here we have our model we talked about
the model right there and then we went
ahead and set it for Library linear as I
showed you earlier that's actually
default so it's not that important
random State equals zero this stuff you
don't worry too much about and then with
the site kit learn you'll see the model
fit this is very common into site kit
they use similar stuff in a lot of other
different packages but you'll you'll see
that that's very common you have to fit
your data and that means we're just
taking the data and we're fitting our X
right here which is our features that's
our X and here's why these are the
labels we're looking for so before we
were looking at is it fraud is it not is
it cat is it not that kind of thing and
this is looking at 0 1 so we want to
have a binary set up on this and we'll
go ahead and run this you can see right
here it just tells us what we loaded it
with as our defaults and that this model
has now been created and we've now fit
our data to it and then comes the fun
part you work really hard to clean your
data to bake it and cook it there's all
kinds of I don't know why they go with
the cooking terms as far as how we get
this data formatted then you go through
and you pick your model you pick your
solver and you have to test it to see
hey which one's going to be best and so
we want to go ahead and evaluate the
model and you do this is that once
you've figured out which one is going to
work the best for you you want to
evaluate it so you can compare it to
your last Model and you can either
update it to create a new one or maybe
change the solver to something else I
mentioned tangent and that's one of the
other common ones that's commonly used
with language for some reason the
tangent even though it looks almost to
me identical to the one we're using with
the sigmoid function if for some reason
it activates better with language even
though it's a very small shift in the
actual math behind it we already looked
at the data early but we'll go and look
at it again just you can see we look at
we have our rows of 0 1 row it only has
one entity and we have our output that
matches these rows and these do have to
match you'll get an error if you put in
something with a different shape so if
you have 10 rows of data and nine
answers it's going to give you an error
because you need to have 10 answers for
it a lot of times you separate this too
when you're doing larger models but for
this we're just going to take a quick
look at that the first thing we want to
start looking that is The Intercept one
of the features inside our linear
regression model we'll go ahead and run
that and print it you'll see here we
have an intercept of minus
1.516 and if we're going to look at The
Intercept we should also look at our
coefficients and if you run that you'll
see that we get a list we get the our
coefficient is the 0.7035 you can just
think of this as your euclidean geometry
for very basic model like this where it
intercepts the Y at some point and we
have a coefficient multiplying by it a
little more complicated in the back end
but that is the just this simple model
which is the one feature in there and
we'll go ahead and reprint the Y because
I want to put them on top of each other
with the Y predict and so these were the
Y values we put in and this is the Y
predict we had coming out and you can
see here we go there's the Y actual and
there's what the prediction comes in now
keep in mind that we used the ACT actual
complete data as part of our training
that is if you're doing a real model a
big stopper right there because you
can't really see how good it did unless
you split some data off to test it on
this is the first step is you want to
see how your model actually tests on the
data you trained it with and you can see
here there is this point right here
where it has it wrong and this point
right here where it also has it wrong
and it makes sense because we're going
our input is 0 1 0 through 9 and it has
to break it somewhere and this is where
the break is so it says this half the
data is going to be zero because that's
what it looked like to me if I was
looking at it without an algorithm and
this data is probably going to be one
and I I didn't I forgot to point this
out so let's go back up here I just kind
of glanced over this window here where
we did a lot of stuff let's go back and
just take a look at that what was done
here is we ran a prediction so this is
where our predict comes in is our
model.predict so we had a model fit we
created the model we programmed it to
give us the right answer now we go ahead
and predict what we think it's going to
be there's our model that predict
probability of X and then we have our y
predict which is very similar but this
has to do more with the probability
numbers so if you remember down below we
had the setup where we're looking at
that sigmoid function that's what this
is returning and the Y predict is
returning a 0 or a one and then we have
our confusion Matrix we'll look at that
and we have our report which just
basically Compares our y to our y
predict which we just did it's kind of
nice a simple data so it's really easy
to see what we're doing that's why we do
use the simple data this can get really
complicated when you have a lot of
different features and things going on
in splits so here we go we've had our I
printed out our actual and our
prediction so this is the actual data
this is what the predict ran and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that this is great if
you have a lot of data to look at but
you can see right here a confusion
Matrix says
if you remember from the confusion
Matrix we have the two this is 2 correct
one two and uh it's been a while since I
looked at a confusion Matrix there's the
two and then we have this one which is
our six that's where the six comes from
and then we have this one which is the
one false this is the two one so we have
this one here and this one here which is
misclassified this really depends on
what data you're working with as to what
your is important you might be looking
at this model and if this model this
confusion Matrix comes up and says that
you misclassified even one person as
being non-malignant cancer that's a bad
model I wouldn't want that
classification I'd want this number to
be zero I wouldn't care if this false
positive was off by a little bit more
long as I knew that I was correct on the
important factor that I don't have
cancer so you can see that this
confusion Matrix really aims you in the
right direction of what you need to
change in your model how you need to
adjust it and then there's of course a
report reports are always nice if you
notice we generated a report earlier
we'll go and just print the report up
and you can remember this is our report
it's a classification report y comma y
predict so we're just putting in our two
values basically what we did here
visually with our actual and our
predicted value and we'll go ahead and
run the report and you can see it has
the Precision the recall your F1 score
your support translated into a accuracy
macro average and weighted average so it
has all the numbers a lot of times when
working with clients or with the
shareholders in the company this is
really where you start because it has a
lot of data and they can just kind of
stare at it and try to figure it out and
then you start bringing in like the
confusion Matrix I almost do this in
Reverse as to what they show I would
never show your shareholders The
Intercept of the coefficient that's for
your internal team only working on
machine language but the confusion
Matrix and the report are very important
those are the two things you really want
to be able to show on these and you can
see here we did a decent job of
classifying the data managed to get a
significant portion of it correct we had
or was it accuracy here is a 0.80 F1
score that kind of thing so you know
it's a pretty accurate model of course
this is pretty goofy because it's very
simple model and it's just splitting the
model between ones and zeros so that was
our demo of the on logistic regression
on there let's go and take a look at K
nearest neighbors this one is another
very highly used and important algorithm
to understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based on the similarity measure the K
nearest neighbor finds out the class of
the new data point by by finding its
nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the new data
point then the k n classifies the new
data point as Class A the K and K
nearest neighbors is the number of
nearest neighbors we are looking for I
hear I.E if we say k equals three this
means that we are looking for nearest
three neighbors of unclassified data
point usually we take the K value
between 3 to 10 as it leads to a better
result a smaller value of K means that
noise will have a bigger influence on
the result and a larger value of K makes
it computationally expensive hence the
data scientists before the range of K
between 3 and 10. when we talk about
noise remember the data we just looked
at was 0 1 1 0 0 it had some some values
where it cut it and set everything to
the right is a one everything to the
left is a zero but it had some ones and
zeros mixed in there that's called noise
that's what they're talking about is
there's some things that are right in
the middle on the classification which
makes it very hard to classify so
suppose we're trying to find the class
for a new Point indicated by the red
color and you can see it's kind of right
between the cat right between the dogs
let k equal 3 so we are finding the 3nn
for the red data point but by looking at
the plot on the right we can see that
the red data point belongs to the class
dogs as it has two votes for class dog
and one vote for class cat and if you
ask the question well what are you
measuring the distance what is that
distance it could be the measurements of
the ears whether they're pointed or
floppy that might be one of the features
you're looking at is how floppy the ears
are another one might be the whiskers
versus the nose and then you take those
measurements and using the one of the
most common things in K means
measurement is the euclidean geometry
you can figure out the distance between
those points there's a lot of different
algorithms for that but you can think
about it that you do have to have some
kind of solid data to measure and so we
can conclude that the new data point
belongs to the class dog so let's go
ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our Jupiter notebook and open
up a new Python Programming script page
of course once we're in here we'll want
to look at the site kit learn I did just
a quick search for SK neighbors KN
neighbors classifier this actually is
the older version 0.0 no 0.023 is the
one we want and you'll see here that we
have all their defaults in Neighbors
equals five it defaults we were talking
about that between three and ten there's
different ways to weigh it there's an
algorithm based on it I mentioned
euclidean geometry finding the distance
there's other algorithms for figuring
out what that distance is and how to
weight those and there's a lot of other
parameters you can adjust for the most
part the K means uh basic setup is a
good good place to start and just let
the defaults go we might play with some
of those we'll see what the guys in the
back did and from here we're going to
import numpy we're going to use pandas
if you haven't been running pandas
pandas is our data frame which sits on
top of numpy's data frames are you know
numpy's is our number array and this is
our data frame matplot library because
we're going to plot some graphs
everybody likes some pretty pictures it
makes it a lot easier to see what's
going on when you have a nice display
and that's also what the Seaborn is in
here in the setup that sits on top of
the matplot library the ones we really
want to look at right here are the what
we're bringing in from sklearn these
ones right here so from sklearn we're
going to load I mentioned the breast
cancer that's a very popular one because
it has I believe it's 36 measurements so
there's 36 features and unless you're a
expert you're not going to know what any
of those features really mean you can
sort of guess but there's special
measurements they take of when they take
a image and of course our confusion
Matrix so that we can take a look and
see what the data looks like and how
good we did and then we have our KN at
Neighbors classifier on here and then I
mentioned that whenever you do training
and testing you want to split the data
up you don't want to train the data and
then test it on the same data that just
tells you how good your training model
is it doesn't tell you whether it
actually works on unknown data and so
this just splits it off so that we can
train it and then we can take a look at
data we don't have in there and see how
good it did and we'll go ahead and load
our data up so here's our our setup on
that oops there we go so we're going to
go ahead and load the data up we have
our x value and that's going to come
from our breast cancer.data and column
breast cancer feature names so there's
our actual all our different features
we'll print that out here in a second
and then we have our mean area mean
compactness so I guess we're going to
take the data and we're only going to
use a couple of the columns this just
makes it easier to read of course when
you actually we're going to do this
you'd want to use all your columns and
then we have our Y and this is simply
whether it's either malignant or B9 and
then we want to go ahead and drop the
first line because that's how it came in
on there and we'll go ahead let's just
take a look at this a little closer here
let's go and run this real quick and
just because I like to see my data
before I run it we can look at this and
we can look at the original features
remember we're only going to use two
features off of here just to make it a
little easier to follow and here's the
actual data and you can see that this is
just this massive stream of data coming
in here it's going to just skip around
because there's so much in there to set
up I think there's like 500 if I
remember correctly and you can see
here's all the different measurements
they take but we don't we don't really
need to see that on here we're just
going to take take a look at just the
two columns and then also our solution
we'll go ahead and just do a Quick Print
y on here so you can see what the Y
looks like and it is simply just zero
zero zero you know
b90001 so a one means it's B9 0 means
it's malignant that's what we're looking
at on that go and cut that out of there
the next stage is to go ahead and split
our data I mentioned that earlier we'll
just go ahead and let them do the
splitting forest for us we have X train
X test y train y test and so we go ahead
and train test split X Y random State
equals one it makes it nice and easy for
us we're going to run that and so now we
have our training and our testing train
means we're going to use that to train
the model and then we're going to use
the test to test to see how good our
model does and then we'll go ahead and
create our model here's our knnn model
the K Neighbors classifier in Neighbors
equals five the metrics is euclidean
remember I talked about euclidean this
is simply your c squared equals a
squared plus b squared plus uh a squared
equals B squared plus c squared plus c
squared plus D Squared and then you take
the square root of all that that's what
they're talking about here it's just the
length of the hypotenuse of a triangle
but you can actually do that in multiple
Dimensions just like you do in two
Dimensions with the regular triangle and
here we have our fit this should start
to look familiar since we already did
that in our last example that's very
standard for S PSI kit and any other one
although sometimes the fit algorithms
look a little bit more complicated
because they're doing more things on
there especially when you get into
neural networks and then you have your
key neighbors it just tells you we've
created a k neighbor setup they kind of
wanted us to reformat the Y but it's not
that big of a deal for this and it comes
out and shows you that we're using the
euclidean metric for our measurement so
now we've created a model here's our
live model we fitted the data to it we
say hey here's our training data let's
go ahead and predict it so we're going
to take our y predict equals k n and
predict y test so this is data we have
not this model has not seen this data
and so we're going to create a whole new
set of data off of there now before we
look at our prediction in fact let's um
I'm going to bring this down and put it
back in here later let's take a look at
our x-test data versus the Y test what
does it look like and so we have our
mean area we're going to compare it to
our mean compactness we're going to go
ahead and run that and we can see here
the data if you look at this just
eyeballing it we put it in here we have
a lot of blue here and we have a lot of
orange here and so these dots in the
middle especially like this one here and
these here these are the ones that are
going to give us false negatives and so
we should expect this is your noise this
is where we're not sure what it is and
then B9 is in this case it's done in
blue and malignant is done in one so if
you look at hit there's two points based
on these features which makes it really
hard to have a hundred percent where the
hundred percent is down here or up here
that's kind of thing I'd be looking for
when we're talking about cancer and
stuff like that we really don't want any
false negatives you want everything you
false positive great you're going to go
in there and have another setup in there
where you might get a get an autopsy or
something like that done on it again
that's very data specific on here so now
let's go ahead and pull in and get our
our prediction in here and we'll create
our y prediction we'll go and run that
so now this is loaded with what we think
the unknown data is going to be and we
can go ahead and take that and go ahead
and plot it because it's always nice to
have some pretty pictures and when we
plot it we're going to do the mean area
versus mean compactness again you look
at this map and you can see that there's
some clear division here we can clearly
say on some of the stuff that our y
prediction if we look at this map up
here and this map down here we probably
got some pretty good deal it looks
pretty good like they match a lot this
is of course just eyeballing it really
you don't want to eyeball these things
you want to show people the pictures so
they can see it you can say hey this is
what it looks like but we really want
the confusion Matrix and we do the Y
test and the Y predict we can see in the
confusion Matrix here it did pretty good
and we'll just go ahead and point that
out real quick here's our 42 which is
positive and our 79 and if I remember
correctly I'd have to look at the data
which one of these is a false negative I
believe it's the nine that's scary I
would not want to be one of those nine
people told that I don't have cancer and
then suddenly find out I do so we would
need to find a way to sort this out and
there is different ways to do that a
little bit past this but you can start
messing with the actual euclidean
geometry and the activation
measurements and start changing those
and how they interact but that's very
Advanced there's also other ways to
classify them or to create a whole other
class right here of we don't knows those
are just a couple of the solutions you
might use for that but for a lot of
things this works out great you can see
here you know maybe you're trying to
sell something well if this was not life
dependent and this was if I display this
ad 42 of these people are going to buy
it and if I display this other ad if I
don't display it 79 people are going to
go a different direction or whatever it
is so maybe you're trying to display
whether they're going to buy something
if you add it on to the website in which
case that's really a good numbers you've
just added a huge number of cells to
your company so that was our K nearest
neighbors let's go ahead and take a look
at support Vector machines so support
Vector machines is the main objective of
a support Vector machine algorithm is to
find a hyperplane in an n-dimensional
space n is the number of features that
distinctly classifies the data points if
you remember we were just looking at
those nice graphs we had earlier in fact
let me go ahead and flip back on over
there if we were looking at this data
here we might want to try to find a nice
line through the data and that's what
we're talking about with this next setup
so the main objective support Vector
machine algorithm is to find a
hyperplane in an n-dimensional space n
is the number of features that
distinctly classifies the data points to
separate the two classes of data there
are many hyperplanes that can be chosen
our objective is to find the plane that
has the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyperplane
depends on the number of features if
there are two input features then the
hyperplane is just a line if there are
three features then the hyperplane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support vector classifier
or hard margin that's what I just showed
you on the other data you can see here
we look for a line that splits the data
evenly the problem with hard margin is
that it doesn't allow outliers and
doesn't work with none linearly
separable data and we just were looking
at that let me flip back on over here
and when we look at this setup in here
and we look at this data here we go look
how many outliers are in here these are
all with all these blue dots on the
wrong side of the line would be
considered an outlier and the same with
the red line and so it becomes very
difficult to divide this data unless
there's a clear space there we go
therefore we introduce soft margins
which accept the new data point and
optimize a model for non-linear data
points soft margins pass through the
data points at the border of the classes
the support Vector machine can be used
to separate the two classes of shapes
here we can see that although triangles
and diamond shapes have pointy edges but
we are able to classify them in two
categories using a support Vector
machine let's go ahead and see what that
looks like in a demo and flip back on
over to our jupyter notebook we always
want to start with taking a look at the
sklearn API in this case the svm SVC
there is a significant number of
parameters because SK the svm has been a
lot of development in the recent years
and has become very popular if we scroll
all the way down to methods you'll see
right here as our fit and our predict
that's what you should see in most of
the scikit learn packages so this should
look very familiar to what we've already
been working on and we'll go ahead and
bring in our import and run that we
should already have pandas numpy our
matplot library which we're going to be
using to graph some of the things a
little bit different right here you'll
see that we're going to go ahead and
bring in one of the fun things you can
do with test data is make circles
circles are really hard to classify you
have a ring on the inside and a ring on
the outside and you can see where that
can cause some issues and we'll take a
look at that a little closer in a minute
here's our svm setup on there and then
of course our metrics because we're
going to use that to take a closer look
at things so we go and run in this
whoops already did once we've gone ahead
and done that we're going to go ahead
and start making ourselves some data and
this part probably a little bit more
complicated than we need I'm not going
to go too much in detail on it we're
going to make a mesh grid and we'll see
what that looks like in a minute we're
defining the minimums you can see in
here create a measurement of points
here's our parameters of x y and H it's
going to return xxyy you can also send a
note to us make sure you get a copy of
this if you want to dig deeper into this
particular code especially and we have a
y Min y Max y Min y Max plus one here's
our mesh grid we're actually going to
make XX and YY plot the Contours all the
way through and this is just a way of
creating data it's kind of a fun way to
create some data we go plot Contours ax
c l f x x y y and return it out add some
perimeters here so that when we're doing
it we have our setup on there train
property and then we'll go ahead and
make our data just throw that right in
there too in the same setup and run that
so now we have X and Y we're going to
make our circles we have n samples
equals samples in this case we're going
to have 500 samples we went ahead and
gave it a noise of 0.05 random State
123. these are all going into let's go
back up here we go make our mesh grid
make circles there it is so this is
going into our make circles up here and
this is part of that setup on this and
then once we've gone ahead and make the
circle let's go ahead and plot it that
way you can see we're talking about
right now what I'm saying is really
confusing because without a visual it
doesn't really mean very much what we're
actually doing so I'm going to go ahead
and run this with the plot and let's go
back up here and just take a look we've
made our Circle we have our n samples
equals samples we're gonna have 500
we're going to have training property
0.8 here's our data frame we go and load
it into the data frame so we can plot it
groups DF Group by label this is kind of
a fun way if you have multiple columns
you can really quickly pull whatever
setup is in there and then we go ahead
and plot it and you can see here we have
two rings that are formed and that's all
this is doing is just making this data
for us this is really hard data to
figure out a lot of programs get
confused in this because there's no
straight line or anything like that but
we can add planes and different setups
on here and so you can see we have some
very interesting data we have our zero
is in blue and our ones in the yellow in
the middle and the data points are an x
y coordinate plot on this one of the
things we might want to do on here is go
ahead and find the Min to Max ratio set
up in there and we can even do let's
just do a print X you can see what the
data looks like that we're producing on
this there we go so x equals x minus X
Min over x max minus X Min all we're
doing is putting this between 0 and 1.
whatever this data is we want a 0 to 1
setup on here and if you look at this
all our data is 0.8.5 that's what this
particular line is doing that's a pretty
common thing to do in processing data
especially in neural networks neural
networks don't like big numbers they
create huge biases and cause all kinds
of problems and that's true in a lot of
our models some of the models it doesn't
really matter but when you're doing
enough of these you just start doing
them you just start putting everything
between zero and one there's even some
algorithms to do that in the sklearn
although it's pretty as you can see it's
pretty easy to do it here so let's go
ahead and jump into the next thing which
is a linear kind of setup C equals 1.0
this is the svm regularization parameter
we're going to use models here's our svm
and this one scroll just a notch there
we go so here we are with our model
we're going to create the the setup with
the SVC kernel is linear and we'll come
back to that because that's an important
uh setup in there as far as what our
kernel is and you'll you'll see why
that's so important here in a minute
because we're going to look at a couple
of these and so this one is we're
actually going to be changing how that
processes it uh and then our C here's
our one our Point O and then the rest of
this is plotting we're just adding
titles making the Contours so it's a
pretty graph you can actually spend this
would be a whole class just to do all
the cool things you can do with Scatter
Plots and regular plots and colors and
things like that in this case we're
going to create a graph with a nice
white line down the middle so that you
can see what's going on here and when we
do this you can see that as it as it
split the data the linear did just that
it drew a line through the data and it
says this is supposed to be blue and
this is supposed to be red and it
doesn't really fit very well that's
because we used a linear division of the
data and it's not very linear data on it
it's anything but linear so when we when
we look at that it's like well okay that
didn't work what's the next option well
there's a lot of choices in here one of
them is just simply we can change this
from the kernel being linear to poly and
we'll just go back up here and use the
same chart oops here we go so here we go
linear kernel we'll change this to poly
and then when we come in here and create
our model here's our model up here
linear we can actually just go right in
and change this to the poly model and if
you remember when we go back over to the
SVC and let's scroll back up here
there's a lot of different options oops
even further okay so we come up here and
we start talking about the kernel here's
the kernel there's linear there's poly
RBF sigmoid pre-computed there's a lot
of different ways to do this setup and
their actual default is RBF very
important to note that so when you're
running these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the svm the kernel is so important
you really need to know that and we
switched our kernel to poly and when we
run this you can see it's changed a
little bit we now have quite an
interesting looking diagram and you can
see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is we do a polyfit you can see
if you have a split in data where
there's a group in the middle this one
kind of data and the groups on the
outside are different the polyfit or the
poly kernels what's going to be fit for
that so if that doesn't work then what
are we going to use well they have the
RBF kernel and let's go ahead and take a
look and see what the RBF looks like and
let me go and turn there we go turn my
drawing off and the RBF kernel oops
RBF and then of course for our title
it's always nice to have it match with
the r b f kernel and we go ahead and run
this and you can see that the RBF kernel
does a really good job it actually has
divided the date app on here and this is
the kind of what you expect here was
that ring here's our inner ring and an
outer ring of data so the RBF fits this
data package quite nicely and that we
talk about svm it really is powerful in
that it has this kind of sorting feature
to it in its algorithms this is
something that is really hard to get the
SK means to do or the k-means setup and
so when you start looking at these
different machine learning algorithms
understanding your data and how it's
grouped is really important it makes a
huge difference as far as what you're
Computing and what you're doing with it
so that was a demo on the support Vector
certainly you could have done continued
on that and done like a confusion Matrix
and all that kind of fun stuff to see
how good it was and split the data up to
see how it vectorizes the visual on
that's so important it makes a big
difference just to see what it looks
like in that giant donut and why it it
does Circle so well or your poly version
or your linear version so we've looked
at some very numerical kind of setups
where there's a lot of math involved
euclidean geometry that kind of thing a
totally different machine learning
algorithm for approaching this is the
decision trees and there's also forests
to go with the decision trees they're
based on multiple trees combined the
decision tree is a supervised learning
algorithm used for classification it
creates a model that predicts a value of
a Target variable by learning simple
decision rules inferred from the data
features a decision tree is a
hierarchical tree structure where an
internal node represents features or
attribute the branch represents a
decision Rule and each Leaf node
represents the outcome and you can see
here where they have the first one yes
or no and then you go either left or
right and so forth one of the coolest
things about decision trees is and I'll
see people actually run a decision tree
even though their final model is
different because the decision tree
allows you to see what's going on you
can actually look at it and say why did
you go right or left what was the choice
where's that break and that is really
nice if you're trying to share that
information with somebody else as to why
when you start getting into the why this
is happening decision trees are very
powerful so the topmost note of a
decision tree is known as the root node
it learns to partition on the basis of
the attribute value it partitions a tree
in a recursive manner so you have your
decision node if you get yes you go down
to the next note that's the decision
note and either yes you go to if it ends
on a leaf node then you know your answer
which is yes or no so there's your
there's your in classification setup on
there here's an example of a decision
tree that tells whether I'll sleep or
not at a particular evening mine would
be depending on whether I have the news
on or not do I need to sleep no okay
I'll work uh yes is it raining outside
yes I'll sleep no I'll work so I guess
if it's not raining outside it's harder
to fall asleep whether you have that
nice ring coming in and again this is
really cool about a decision tree is I
can actually look at it and go oh I like
to sleep when it rains outside so when
you're looking at all the data you can
say oh this is where the switch comes in
when rains outside I'll sleep really
good if it's not raining or if I don't
need sleep then I'm not going to sleep
I'm going to go work so let's go ahead
and take a look at that that looks like
in the code just like we did before we
go ahead and open up the site kit set up
just to say what the decision tree
classifier has you have your parameters
which will look a little bit more in
depth at as we write the code but it has
different ways of splitting it the
strategy used to choose a split at each
node Criterion max depth remember the
tree how far down do you want it do you
want to take up the space of your whole
computer with a and and map every piece
of data or you know the smaller that
number is the smaller the level the tree
is and the less processing it takes but
is also more General so you're less
likely to get as in depth and answer
um and then of course minimal samples
you need for it to split samples for the
leaves there's a lot of things in here
as far as what how big the tree is and
how to define it and when do you define
it and how to weight it they have their
different attributes which you can dig
deeper into that can be very important
if you want to know the why of things
and then we go down here to our methods
and you'll see just like everything else
we have our fit method very important
and our predict the two main things that
we use what is what we're going to
predict our X to be equal to and we'll
go ahead and go up here and start
putting together the code we're going to
import our numpy our pandas there's our
confusion Matrix our train test Split
Decision tree classifier that's the big
one that we're actually working with
that's the line right here where we're
going to be oops decision tree there it
is decision tree classifier that's the
one I was looking for and of course we
want to know the accuracy and the
classification report on here and we're
going to do a little different than we
did in the other examples and there's a
reason for this let me go and run this
and load this up here uh we're going to
go ahead and build things on functions
and this is when you start splitting up
into a team this is the kind of thing
you start seeing a lot more both in
teams and for yourself because you might
want to swap one data to test it on a
different data depending on what's going
on so we're going to have our import
data here the data set length the
balance and so forth this just returns
balanced data let me just go ahead and
print because I'm curious as to what
this looks like import data and it's
going to return the balance data so if I
run that if we go ahead and print this
out here and run that you can see that
we have a whole bunch of data that comes
in there and some interesting setup on
here has let's see brrr I'm not sure
exactly what that represents on here uh
one one one one one two and so forth so
we have a different set of data here the
shape is uh five columns one two three
four five seems to have a number at the
beginning which I'm going to guess
brl a letter I mean and then a bunch of
numbers in there one one one one let's
see down here we got five five five uh
set up on this and let's see balance
data and since it said balance data I'm
going to guess that b means balanced R
means you need to move it right and L
means it needs to be moved left or
skewed to the left I'm not sure which
one ah let's go and close that out and
we'll go ahead and create a function to
split the data set X balance data equals
data values y balance equals data values
of zero there's that letter remember
left right and balance and then we're
looking for the values of one through
five and we go ahead and split it just
like you would X train y train set
random State 100 test size is 0.3 so
we're taking thirty percent of the data
and it's going to return your X your y
your y train your uh your X train your X
test your y train your y test again we
do this because if you're running a lot
of these you might want to switch how
you split the data and how you train it
I tend to use a bifold method I'll take
a third of the data and I'll train it on
the other two thirds and test it on that
third and then I'll switch it I'll
switch which third is a test data and
then I can actually take that
information and correlate it and it
gives me a really robust package for
figuring out what the complete accuracy
is but in this case we're just going to
go ahead this is our function for
splitting data and this is where it kind
of gets interesting because remember
we're talking a little bit about the
different settings in our model and so
in here we're going to create a decision
tree but we're going to use the genie
setup and where did that come from
what's the genie on here so if we go
back to the top of their page and we
have what Criterion are we going to use
we're going to use Genie they have Genie
and entropy those are the two main ones
that they use for the decision tree so
this one's going to be Genie and if
we're going to have a function that
creates the Genie model and it even goes
down here and here's our fit train in
the Genie model uh we'll probably also
want to create one for entropy sometimes
I even just I might even make this just
one function with the different setups
and I know one of my one of the things I
worked on recently I had to create a one
that tested across multiple models and
so I would send the parameters to the
models or I would send this part right
here where it says decision tree
classifier that whole thing might be
what I send to create the model and I
know it's going to fit we're going to
have our X train and we're gonna have
our predict and all that stuff is the
same so you can just send that model to
your function for testing different
models again this just gives you one of
the ways to do it and you can see here
we're going to chain train with the
genie and we're also going to change
train in with the entropy to see how
that works and if you're going to have
your models going two separate models
you're sending there we'll go ahead and
create a prediction this simply is our y
predict equals our whatever object we
sent whatever model we sent here the clf
object and predict against our X test
and you can see here print y predict and
return y predict set up on here and
we'll load that definition up and then
if you're going to have a function that
runs a predict and print some things out
we should also have our accuracy
function so here's our calculate the
accuracy what are we sending we're
sending our y test data this could also
be y actual and Y predict and then we'll
print out a confusion Matrix and then
we'll print out the accuracy of the
score on here and print a report
classification report bundle it all
together there so if we bring this all
together we have all the steps we've
been working towards which is importing
our data by the ways you'll spend eighty
percent of your time importing data in
most machine learning setups and cooking
it and burning it and getting it
formatted so that it it works with
whatever models you're working with the
decision tree has some cool features in
that if you're missing data it can
actually pick that up and just skip that
and says I know how to split this
there's no way of knowing whether it
rained or didn't rain last night so I'll
look at something else like whether you
watched TV after eight o'clock you know
that blue screen thing so we have our
function importing our data set we bring
in the data we split the data so we have
our X text test and Y train and then we
have our different models our clf Genie
so it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy and then
once we have that we have our function
for making the prediction and we have
our function for calculating the
accuracy and then if you're going to
have that we should probably have our
main code involved here this probably
looks more familiar if you're depending
on what you're working on if you're
working on like a pie charm then you
would see this in throwing something up
real quick in Jupiter notebook uh so
here's our our main data import which
we've already defined we get our split
data we create our Genie we create our
entropy so there's our two models going
on here there's our two models so these
are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie and we want to go ahead
with the genie and print our our
predictions Y X test to the genie and
calculate the accuracy on here and then
we want to print the results using
entropy so this is just the same thing
coming down like we did with the genie
we're going to put out our my predict
entropy and our calculations so let's go
ahead and run that and just see what
this piece of code does we do have like
one of our data needs to be is getting a
warning on there that's nothing major
because it's just a simple warning
probably an update of a new version is
coming out uh and so here we are we have
have our data set it's got 625 you can
actually see an example of the data set
B meaning balanced I guess and here's
our five data points one one one means
it's balanced it's skewed to the right
with one one one two and so forth on
here and then we're going to go ahead
and predict from our prediction whether
it's to the right or to the left you
could think of a washing machine that's
cute that's banging on one side of the
thing or maybe it's an automated car or
down the middle of the road that's in
balance and it starts going veering to
the right so we need to correct for it
and when we print out the confusion
Matrix we have three different variables
RL and B so we should see the three
different variables on here and you have
as far as whether it predicts in this
case the balance there's not a lot of
balanced loads on here and didn't do a
good job guessing whether it's balanced
or not that's what I take from this
first one the second one I'm guessing is
the right so it did pretty good job
guessing the right balance you can see
that a bunch of of them came up left
unbalanced probably not good for an
automated car because it tells you 18
out of the uh 18 missed things and tells
you to go the wrong direction and here
we are going the other way 19 to 71 and
of course we can back that up with an
accuracy report on here and you can see
the Precision how well the left and
right balance is 79 79 precision and so
forth and then we went and used the
entropy and let me just see if we can
get so we can get them both next to each
other here's our entropy of our the
first setup our first model which is the
genie Model 67 18 1971 63 22 2070 pretty
close the two models that you know
that's not a huge difference in numbers
this second one of entroped slightly it
looks like slightly worse because it did
one better as far as the right balance
and did what is this four worse on the
left balance or whatever so slightly
worse if I was guessing between these
I'd probably use the first one they're
so close though that wouldn't be it
wouldn't be a Clear Choice as to which
one worked better and there's a lot of
numbers you can play with here which
might give better results depending on
what the data is going in now one of the
takeaways you should have from the
different category routines we ran is
that they run very similar you certainly
change the perimeters in them as to
whether you're using what model you're
using and how you're using it and what
data they get applied to but when you're
talking about the site kit learn package
it does such an awesome job of making it
easy you split your data up you train
your data and you run the prediction and
then you see what kind of accuracy what
can confusion confusion Matrix It
generates so we talk about algorithm
selection logistic regression K nearest
neighbors logistic regression is used
when we have a binomial outcome for
example sample to predict whether an
email is Spam or not whether the tumor
is malignant or not the logistic
regression works really good on that you
can do it in a k nearest neighbors also
the question is which one will it work
better in I find a logistic regression
models work really good in a lot of raw
numbers so if you're working with say
the stock market is this a good
investment or a bad investment so that's
one of the things that it handles the
numbers better Kate nearest neighbors
are used in scenarios where
non-parametric no fixed number of
parameters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection so K means
really good in finding the patterns I've
seen that as a preprocessor to a lot of
other processors where you use the K
nearest neighbors to figure out what
data groups together very powerful
package support Vector machines support
Vector machines are used whenever the
data has higher Dimensions the human
genome micro array svms are extensively
used in the hard handwriting recognition
models and you can see that we were able
to switch between the parabolic and the
circular setup on there where you can
now have that donut kind of data and be
able to filter that out with the support
Vector machine and then decision trees
are mostly used in operational
researches specifically in decision
analysis to help identify a strategy
most likely to reach any goal they are
preferred where the model is easy to
understand
I like that last one it's a good
description is it easy to understand so
you have data coming in when am I going
to go to bed you know was it raining
outside you can go back and actually
look at the pieces and see those
different decision modes takes a little
bit more to dig in there and figure out
what they're doing but you can do that
and you can actually help you figure out
why people love it for the Y Factor so
uh strengths and limitations big one on
all of these the streets and limitations
we talk about logistic regressions uh
the strings are is easy to implement and
efficient to train it is relatively easy
to regularize the data points remember
how we put everything between zero and
one when you look at logistic regression
models you don't have to worry about
that as much limitations as a high
Reliance on proper representation of
data It could only predict a categorical
outcome with the K nearest neighbors it
doesn't need a separate training period
new data can be added seamlessly without
effect the accuracy of the model kind of
an interesting thing because you can do
partial training that can become huge if
you're running across a really large
data sets or the data's coming in it can
continually do a partial fit on the data
with the K nearest neighbors and
continue to adjust that data uh it
doesn't it doesn't work on high
dimensional and large data sets we were
looking at the breast cancer 36
different features what happens when you
have 127 features or a million features
and you say well what do you have a
million features in well if I was
analyzing uh log the legal documents I
might have a tokenizer that splits a
words up to be analyzed and that
tokenizer might create one million
different words available that might be
in the document for doing weights
sensitive to noisy data outliers and
missing values that's a huge one with K
nearest neighbors they really don't know
what to do with the missing value how do
you compute the the distance if you
don't know what the value is
uh The svm Works more efficiently on
high dimensional data it is relatively
memory efficient so it's able to create
those planes with only a few different
variables in there as opposed to having
to store a lot of data for different
features and things like that it's not
suitable for a large data sets the svm
you start running this over gigabytes of
data causes some huge issues
underperforms if the data has noise or
overlapping that's a big one we were
looking at that where the svm splits it
and it creates a soft buffer but what
happens when you have a lot of stuff in
the middle that's hard to sort out and
doesn't know what to do with that causes
svm to start crashing or not perform as
well decision trees handles non-linear
perimeters and missing values
efficiently the missing values is huge
I've seen this in was it the wine
tasting data sets where they have three
different data sets and they share
certain features but then each one has
some features that aren't in the other
other ones and it has to figure out how
to handle those well the decision tree
does that automatically instead of
having to figure a way to fill that data
in before processing like you would with
the other models it's easy to understand
and has less training period so it
trades pretty quickly comes up there and
just keeps forking the tree down and
moving the parts around and so it
doesn't have to go through the data
multiple times guessing and adjusting it
just creates the tree as it goes
overfitting and high variants are the
most annoying part of it that's that's
an understatement uh that has to do with
how many leaves and how many decisions
you have it do the more you have the
more overfit it is to the data it also
just in making the choices and how the
choices come in it might overfit to a
specific feature because that's where it
started at and that's what it knows and
it really is challenged with large data
sets they've been working on that with
the data Forest but it's not suitable
for large data sets it's really
something you'd probably run on a single
machine and not across not a cross
across a data pool or anything if you
are an aspiring data scientist who is
looking out for online training and
certification in data science from the
best universities and Industry experts
than search number simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below so
the decision tree one of the many
powerful tools in the machine learning
library begins with a problem I think I
have to buy a car so in making this
question you want to know how do I
decide which one to buy and you're going
to start asking questions is a mileage
greater than 20 is a price less than 15
will be sufficient for six people does
it have enough airbag analog breaks all
these questions come up then as we feed
all this data in we make a decision and
that decision comes up oh hey this seems
like a good idea here's a car so as
we're going through this decision
process using a decision tree we're
going to explore lower this maybe not in
buying a car but in how to process data
what's in it for you let's start by
finding out what is machine learning and
why we even want to know about it for
processing our data and we'll go into
the three basic types of machine
learning and the problems that are used
by Machine learning to solve finally
we'll get into what is a decision tree
what are the problems that decision tree
solves what are the advantages and
disadvantages of using a decision tree
and then we want to dig in a little deep
into the mechanics how does the decision
tree work and then we'll go in and do a
case loan repayment prediction what we
actually are going to put together some
python code and show you the basic
python code for generating a decision
tree what is machine learning there are
so many different ways to describe what
is machine learning in today's world and
illustrate it we're going to take a
graphic here and making decisions or
trying to understand what's going on and
really underlying machine learning is
people want to wish they were smarter
wish we could understand the world
better so you can see a guy here who's
uh saying hey how can I understand the
world better and someone comes up and
says let's use artificial intelligence
machine learning is a part of artificial
intelligence and now he's gives a big
smile on his face because now he has
artificial intelligence to help him make
his decisions and they can think in new
ways so this brings in new ideas so what
is machine learning this is a wonderful
graph here you can see where we have a
learn predict decide these are the most
basic three premises of machine learning
in learning we can describe the data in
new ways and able to learn new aspects
about what we're looking at and then we
can use that to predict things and we
can use that to make decisions so maybe
it's something that's never happened
before but we can make a good guess
whether it's going to be a good
investment or not it also helps us
categorize stuff so we can remember it
better so it's easier to pull it out of
the catalog we can analyze data in new
ways we never thought possible and then
of course there's the very large growing
industry of recognize we can do facial
recognition driver recognition automated
car recognition all these are part of
machine learning going back to to our
guy here who's in his ordinary system
and would like to be smarter make better
choices what happens with machine
learning is an application of artificial
intelligence wherein the system gets the
ability to automatically learn and
improved based on experience so this is
exciting because you have your ordinary
guy who now has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you'd already have a
list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learn learning tool and that
lets you predict on the next person
whether they're going to be able to make
their payments or not on their loan if
you have one category we already know
the answers the next one would be you
don't know the answers you just have a
lot of information coming in
unsupervised learning allows you to
group liked information together so if
you're analyzing photos it might group
all the images of trees together and all
the images of houses together without
ever knowing what a house or a tree is
which leads us to the third type of
machine learning the third type of
machine learning is reinforcement
learning unlike supervised or
unsupervised learning you don't have the
data prior to starting so you get the
data one line at a time and then whether
you make a good choice or a bad choice
the machine learning tool has to then
adjust accordingly so you get a plus or
minus feedback you can liken this to the
way a human learns we experience life
one minute at a time and we learn from
that and either our memories is good or
we learn to avoid some problems in
machine learning to understand where the
decision tree fits into our machine
learning tools we have to understand the
basics of some of the machine learning
problems and three of the primary ones
fall underneath classification problems
with categorical Solutions like yes or
no true or false one or zero this might
be does it belong to a particular group
yes or no then we have regression
problems where there's a continuous
value needs to be predicted like product
prices profit and you can see here this
is a very simple linear graph you can
guess what the next value is based on
the first four it kind of follows a
straight line going up and clustering
this is problems where the data needs to
be organized to find specific patterns
like in the case of product
recommendation they group all the
different products that people just like
you viewed on a shopping site and say
people who bought this also bought this
the most commonly used for the decision
trees for classification for figuring
out is it red or is it not is it a fruit
or is it a vegetable yes or no true
false left or right zero one and so we
talk about classification we're going to
look at the basic machine learning these
are the four main tools used in
classification there's a Nave Bays
logistic regression decision tree and
random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or a curve
through the data they work Wonderful in
a lot of problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest but
today we're just going to focus on the
decision tree
what is a decision tree let's go through
a very simple example before we dig in
deep decision tree is a tree shape
diagram used to determine a course of
action each branch of the tree
represents a possible decision or
currents or reaction let's start with a
simple question how to identify a random
vegetable from a shopping bag so we have
this group of vegetables in here and we
can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is the diameter greater than two if
false is going to be a what looks to be
a red chili and if it's true it's going
to be a bell pepper from the capsicum
family so it's a capsicum
problems that decision tree can solve so
let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression when we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if-then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error
before we dig deeper into the mechanics
of the decision tree let's take a look
at the advantages of using a decision
tree and we'll also take a glimpse at
the disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and non-linear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction
if we're going to look at the advantages
of a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data
decision tree important terms before we
dive in further we need to look at some
basic terms we need to have some
definitions to go with our decision tree
in the different parts we're going to be
using we'll start with entropy entropy
is a measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side High height is
less than 10 true or false and as you
see as we split it the entropy continues
to be less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because it'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went on
the different parts of our tree and they
call the leaf node Leaf node carries a
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node
how does a decision tree work wonder
what kind of animals I'll get in the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is a measure of decrease in entropy
after splitting so the formula for
entropy is the sum that's what this
symbol looks like that looks like kind
of like a e the funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math don't really have to memorize this
math and just be aware that it's there
and this is what's going on in the
background and so we have three drafts
two tigers one monkey two elephants a
total of eight animals gather and if we
plug that into the formula we get an
entropy that equals three over eight so
we have three drafts a total of eight
times the log usually they use base two
on the log so log base 2 of 3 over 8
plus in this case let's say it's the
elephants two over eight two elephants
over a total of eight times log base two
two over eight plus one monkey over
total of eight log base 2 1 over 8 and
plus 2 2 over 8 of the Tigers log base 2
over 8. and if we plug that into our
computer or calculator I obviously can't
do logs in my head we get an entropy
equal to
0.571 the program will actually
calculate the entropy of the data set
similarly after every split to calculate
the gain now we're not going to go
through each set one at a time to see
what those numbers are we just want you
to be aware that this is a Formula or
the mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we'll try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
is probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has to decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as the condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes of
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with a
hundred percent accuracy that was easy
use case loan repayment prediction let's
get into my favorite part and open up
some Python and see what the programming
code and the scripting looks like in
here we're going to want to do a
prediction and we start with this
individual here who's requesting to find
out how good his customers are going to
be whether they're going to repay their
loan or not for the bank and from that
we want to generate a problem statement
to predict if a customer will repay loan
amount or not and then we're going to be
using the decision tree algorithm in
Python let's see what that looks like
and let's dive into the code in our
first few steps of implementation we're
going to start by importing the
necessary packages that we need from
Python and we're going to load up our
data and take a look at what the data
looks like so the first thing I need is
I need something to edit my Python and
run it in so let's flip on over and here
I'm using the Anaconda Jupiter notebook
now you can use any python IDE you like
to run it in but I find the Jupiter
notebooks really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we we start let's talk a little
bit about what we're bringing in and
then we're going to do a couple things
in here we have to make a couple changes
as we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with mathematics
especially with a very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to taking your basic data and storing it
in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just a minute we're going
to change that and we'll go over that
too and then there's also the SK dot
tree import decision tree classifier
that's the actual tool we're using
remember I told you don't be afraid of
the mathematics it's going to be done
for you well the decision tree
classifier has all that mathematics in
there for you so you don't have to
figure it back out again and then we
have sklearn.metrix for accuracy score
we need to score our our setup that's
the whole reason we're splitting it
between the training and testing data
and finally we still need the sklearn
import tree and that's just a basic tree
function that's needed for the decision
tree classifier and finally we're going
to load our data down here and I'm going
to run this and we're going to get two
things on here one we're going to get an
error and two we're going to get a
warning let's see what that looks like
so the first thing we had is we have an
error why is this error here well it's
looking at this it says I need to read a
file and when this was written the
person who wrote it this is their path
where they stored the file
so let's go ahead and fix that
and I'm going to put in here my file
path I'm just going to call it full file
name and you'll see it's on my C drive
and this is very lengthy setup on here
where I stored the data2.csv file
don't worry too much about the full path
because on your computer it'll be
different the data.2 CSV file was
generated by simply learn
if you want a copy of that you can
comment down below and request it here
in the YouTube
and then if I'm going to give it a name
full file name
I'm going to go ahead and change it here
to full
file name so let's go ahead and run it
now and see what happens
and we get a warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection
so if we go up here we have
sklearn.cross validation and if you
research this and go to the sklearn site
you'll find out that you can actually
just swap it right in there with model
selection
and so when I come in here and I run it
again
that removes a warning what they've done
is they've had two different developers
develop it in two different branches
and then they decided to keep one of
those and eventually get rid of the
other one that's all that is and very
easy and quick to fix
before we go any further I went ahead
and opened up the data from this file
remember the the data file we just
loaded on here the data
underscore2.csv let's talk a little bit
more about that and see what that looks
like both as a text file because it's a
comma separated variable file and in a
spreadsheet this is what it looks like
as a basic text file you can see at the
top they've created a header and it's
got one two three four five columns and
each column has data in it and let me
flip this over because we're also going
to look at this in an actual spreadsheet
so you can see what that looks like and
here I've opened it up in the open
Office calc which is pretty much the
same as Excel and zoomed in and you can
see we've got our columns and our rows
of data a little easier to read in here
we have a result yes yes no we have
initial payment last payment credit
score house number if we scroll way down
we'll see that this occupies a thousand
and one lines of code or lines of data
with the first one being a column and
then 1000 lines of data
now as a programmer
if you're looking at a small amount of
data I usually start by pulling it up in
different sources so I can see what I'm
working with
but in larger data you won't have that
option it'll just be too too large so
you need to either bring in a small
amount that you can look at it like
we're doing right now or we can start
looking at it through the python code so
let's go ahead and move on and take the
next couple steps to explore the data
using python let's go ahead and see what
it looks like in Python to print the
length and the shape of the data so
let's start by printing the length of
the database we can use a simple Lin
function from python
and when I run this you'll see that it's
a thousand long and that's what we
expected there's a thousand lines of
data in there if you subtract the column
head this is one of the nice things when
we did the balance data from the panda
read CSV you'll see that the header is
row zero so it automatically removes a
row
and then shows the data separate it does
a good job sorting that data out for us
and then we can use a different function
and let's take a look at that and again
we're going to utilize the tools in
panda
and since the balance underscore data
was loaded as a panda data frame
we can do a shape on it and let's go
ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data.head and this is a hand
as print statement of its own so it has
its own print feature in there and then
we went ahead and gave a label for a
print job here of data set just a simple
print statement
and when we run that
and let's just take a closer look at
that let me zoom in here
there we go
Candace does such a wonderful job of
making this a very clean
readable data set so you can look at the
data you can look at the column headers
you can have it when you put it as the
head it prints the first five lines of
the data
and we always start with zero so we have
five lines we have zero one two three
four instead of one two three four five
that's a standard scripting and
programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
site to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces
so first we have our X and Y
where did X and Y come from well X is
going to be our data
and Y is going to be the answer or the
target you can look at it source and
Target
in this case we're using X and Y to
denote the data in and the data that
we're actually trying to guess what the
answer is going to be and so to separate
it we can simply put in x equals the
balance of the data dot values the first
brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns one through
five remember always start with zero
zero is a yes or no and that's whether
the loan went default or not so we want
to start with one if we go back up here
that's the initial payment and it goes
all the way through the house number
well if we want to look at one through
five we can do the same thing for Y
which is the answers and we're going to
set that just equal to the zero row so
it's just the zero row and then it's all
rows going in there so now we've divided
this into two different data sets
one of them with the
data going in and one with the answers
next we need to split the data
and here you'll see that we have it
split into four different parts
the first one is your X training your X
test your y train your y test
simply put we have X going in where
we're going to train it and we have to
know the answer to train it with
and then we have X test where we're
going to test that data and we have to
know in the end what the Y was supposed
to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to
0.3 so that's roughly 30 percent will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we get to actually build
our decision tree and they've called it
here clf underscore entropy that's the
actual decision tree or decision tree
classifier and in here they've added a
couple variables which we'll explore in
just a minute and then finally we need
to fit the data to that so we take our
clf entropy that we created and we fit
the X train and since we know the
answers for X trade or the Y train we go
and put those in and let's go ahead and
run this and what most of these sklearn
modules do is when you set up the
variable in this case when we set the
clf entropical decision tree classifier
it automatically prints out what's in
that decision tree there's a lot of
variables you can play Within here and
it's quite beyond the scope of this
tutorial able to go through all of these
and how they work but we're working on
entropy that's one of the options we've
added that it's completely a random
state of 100 so 100 percent and we have
a max depth of three now the max depth
if you remember above when we were doing
the different graphs of animals means
it's only going to go down three layers
before it stops and then we have minimal
samples of leaves as five so it's going
to have at least five leaves at the end
so I'll have at least three splits I'll
have no more than three layers and at
least five end leaves with the final
result at the bottom now that we've
created our decision tree classifier not
only created it but trained it let's go
ahead and apply it and see what that
looks like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here
and before we run it let's just take a
quick look at what it's doing here we
have a variable y predict that we're
going to do
and we're going to use our variable clf
entropy that we created
and then you'll see dot predict and it's
very common in the SK learn modules that
they're different tools have the predict
when you're actually running a
prediction
in this case we're going to put our X
test data in here
now if you delivered this for use an
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess
whether the person is going to be pay
them back or not in this case we need to
test out the data and just see how good
our sample is how good of our tree does
at predicting the loan payments and
finally since Anaconda Jupiter notebook
is works as a command line for python we
can simply put the Y predict en to print
it I could just as easily have put the
print
and put brackets around y predict en to
print it out we'll go ahead and do that
it doesn't matter which way you do it
and you'll see right here that it runs a
prediction this is roughly 300 in here
remember it's 30 percent of a thousand
so you should have about 300 answers in
here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit
there we go
so you have a nice full picture
and we'll see here we're just going to
do a print accuracy is
and then we do the accuracy score
and this was something we imported
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from
that's coming from here down here from
sklearn.metrix import accuracy score
and you could probably run a script make
your own script to do this very easily
how accurate is it how many out of 300
do we get right and so we put in our y
test that's the one we ran the predict
on and then we put in our y predict en
that's the answers we got and we're just
going to multiply that by a hundred
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage and let's run that
and see what it looks like
and if you see here we got an accuracy
of 93.6667
so when we look at the number of loans
and we look at how good our model fit we
can tell people it has about a 93.6
fitting to it so just a quick recap on
that we now have accuracy set up on here
and so we have created a model that uses
a decision tree algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about 94.6 percent the bank can now use
this model to decide whether it should
approve the loan request from a
particular customer or not and so this
information is really powerful we may
not be able to as individuals understand
all these numbers because they have
thousands of numbers that come in but
you can see that this is a smart
decision for the bank to use a tool like
this to help them to predict how good
their profit is going to be off of the
loan balances and how many are going to
default or not so we've had a lot of fun
learning about decision trees so let's
take a look at the key takeaways that
we've covered today what is machine
learning we covered up some different
aspects of machine learning and what
that is utilized in your everyday life
and what you can use it for for
predicting for describing for guessing
what the next outcome is for storing
information we looked at the three main
types of machine learning supervised
learning unsupervised learning and
reinforced learning we looked at
problems in machine learning and what it
solves classification regression and
clustering finally we went through how
does the decision tree work where we
looked at the Hunter and he's trying to
sort out the different animals and what
kind of animals they are and then we
rolled up our sleeves and did our python
coding and actually applied it to a data
set for random Forest currently today is
used in remote sensing for example
they're used in the etm devices if
you're a space buff that's the enhanced
thematic mapper they use on satellites
which see far outside the human Spectrum
for looking at land masses and they
acquire images of the Earth's surface
the accuracy is higher and training time
is less than many other machine learning
tools out there also object detection
multi multi-class object detection is
done using random Forest algorithms a
good example is a traffic we try to sort
out the different cars buses and things
and it provides a better detection and
complicated environments to very
complicated up there and then we have
another example connect and let's take a
little closer look at connect connect to
use a random Forest as part of the game
console and what it does is it tracks
the body movements and it recreates it
in the game and let's see what that
looks like we have a user who performs a
step in this case it looks like Elvis
Presley going there
that is then recorded So the connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have prints going on this one
from Elvis Presley to Prince it's great
so Mark's user based on the accuracy if
we look at that a little closer we have
a training set to identify body parts
where are the hands where the feet
what's going on with the body
that then goes into a random Forest
classifier that learns from it once
we've trained the classifier
and then identifies the body parts while
the person's dancing and it's able to
represent that in a computer format and
then based on that it scores the game
and how accurate you are as being Elvis
Presley or prince in your dancing let's
take an overview of what we're going to
cover today what's in it for you we're
going to start with is what is machine
learning we're not going to go into
detail on that we're going to
specifically look how the random Force
fits in the machine learning hierarchy
then we're going to look at some
applications of random Forest what is
classification which is his primary use
why use random Force what's the benefits
of it and how does it actually come
together what is random forest and then
we'll get into random forest and the
decision tree how all that's like the
final step in how it works and finally
we'll get some python code in there and
we'll use the case the iris flower
analysis now if you don't know what any
of these terms mean or where we're going
with this don't worry we're going to
cover all the basics and have you up and
running and even having doing some basic
script in Python by the end let's take a
closer look at types of machine learning
specifically we're going to look at
where the decision tree fits in with the
different machine learning packages out
there we'll start with the basic types
of machine learning there's supervised
learning where you have lots of data and
you're able to train your models there's
unsupervised learning where it has to
look at the data and then divide it
based on its own algorithms without
having any training and then there's
reinforcement learning where you get a
plus or negative if you have the answer
correct this particular tool belongs to
the supervised learning let's take a
closer look at that what that means in
supervised learning supervised learning
falls into two groups classification and
regression we'll talk about regression a
little later and how that differs this
particular format goes underneath
classification so we're looking at
supervised learning and classification
in the machine learning tools
classification is a kind of problem
wherein the outputs are categorical in
nature like yes or no true or false
false or 0 or 1. in that particular
framework there's the k n n where the NN
stands for nearest neighbor Nave Bays
the decision tree which is part of the
random Forest that we're studying today
so why random Forest It's always
important to understand why we use this
tool over the other ones what are the
benefits here and so with the random
Forest the first one is there's no
overfitting if you use of multiple trees
reduce the risk of overfitting training
time is less overfitting means that we
have fit the data so close to what we
have as our sample that we pick up on
all the weird parts and instead of
predicting the overall data you're
predicting the weird stuff which you
don't want high accuracy runs
efficiently on large database for large
data it produces highly accurate
predictions in today's world of Big Data
this is really important and this is
probably where it really shines this is
where why random forests really really
comes in it estimates missing data data
in today's world is very messy so when
you have a random Forest it can maintain
the accuracy when a large proportion of
the data is missing what that means is
if you have data that comes in from five
or six different areas and maybe they
took one set of Statistics in one area
and they took a slightly different set
of Statistics in the other so they have
some of the same shared data but one is
missing like the number of children in
the house if you're doing something over
demographics and the other one is
missing the size of the house it will
look at both of those separately and
build two different trees and then it
can do a very good job of guessing which
one fits better even though it's missing
that data let us dig deep into the
theory of exactly how it works and let's
look at what is random forests random
forests or random decision Forest is a
method that operates by constructing
multiple decision trees the decision of
the majority of the trees is chosen by
the random Forest as a final decision in
this uh we have some nice Graphics here
we have a decision tree and they
actually use a real tree to denote the
decision tree which I love and given a
random some kind of picture of a fruit
this decision tree decides that the
output is it's an apple and we have a
decision tree two where we have that
picture of the fruit goes in and this
one decides that it's a limit and the
decision three tree gets another image
and it decides it's an apple and then
this all governs together and what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision Tree in looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagrammed used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like they switched from
lemons to oranges we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes
hmm orange or red well if it's true then
it comes out as the orange and if it's
false that leaves apples
so in this example it sorts out the
fruit in the bowl or the images of the
fruit a decision tree these are very
important terms to know because these
are very Central to understanding the
decision train when working with them
the first is entropy everything on the
decision tree and how it makes this
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set then
they also have Information Gain
the leaf node the decision node and the
root node we'll cover these other four
terms as we go down the tree but let's
start with entropy so starting with
entropy we have here a high amount of
Randomness what that means is that
whatever is coming out of this decision
if it was going to guess based on this
data it wouldn't be able to tell you
whether it's a lemon or an apple it
would just say it's a fruit
so the first thing we want to do is we
want to split this apart and we take the
initial data set we're going to create a
data set one and a data set two we just
split it in two and if you look at these
new data sets after splitting them the
entropy of each of those sets is much
less so for the first one whatever comes
in there it's going to sort that data
and it's going to say okay if this data
goes this direction it's probably an
apple and if it goes into the other
direction it's probably a lemon so that
brings us up to Information Gain it is a
measure of decrease in the entropy after
the data set is split what that means in
here is that we've gone from one set
which has a very high entropy to two
lower sets of entropy and we've added in
the values of E1 for the first one and
E2 for the second two which are much
lower and so that information gain is
increased greatly in this example and so
you can find that the information grain
simply equals decision E1 minus E2 as
we're going down our list of definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision so we look down here to the
leaf node we finally get to our set one
or our set two when it comes down there
and it says okay this object's gone into
set one if it's gone into set one it's
going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right it'll either be an
apple or lemons those Leaf nodes are
those final decisions or classifications
that's the definition of leaf node in
here if we're going to have a final Leaf
where we make the decision we should
have a name for the nodes above it and
they call those decision nodes a
decision node decision node has two or
more branches and you can see here where
we have the five apples and one lemon
and in the other case the five lemons
and one apple they have to make a choice
of which tree it goes down based on some
kind of measurement or information given
to the tree and that brings us to our
last definition the root node the
topmost decision node is known as the
root node and this is where you have all
of your data and you have your first
decision it has to make with the first
split in information
so far we've looked at a very general
image with the fruit being split let's
look and see exactly what that means to
split the data and how do we make those
decisions on there let's go in there and
find out how does a decision tree work
so let's try to understand this and
let's use a simple example and we'll
stay with the fruit we have a bowl of
fruit and so let's create a problem
statement and the problem is we want to
classify the different types of fruits
in the bowl based on different features
the data set in the bowl is looking
quite messy and the entropy is high in
this case so if this ball was our
decision maker it wouldn't know what
choice to make it has so many choices
which one do you pick Apple grapes or
lemons and so we look in here we're
going to start with a training set
so this is our data that we're training
our data with and we have a number of
options here we have the color and under
the color we have red yellow purple we
have a diameter three three one three
three one and we have a label Apple
lemon Grapes apple lemon grapes and how
do we split the data we have to frame
the conditions to split the data in such
a way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
three color equals yellow red diameter
equals one and when we look at that
you'll see over here we have one two
three four threes that's a pretty Hardy
selection so let's say the condition
gives us a maximum gain of three so we
have the most pieces fall into that
range so our first split from our
decision node is we split the data based
on the diameter is it greater than or
equal to three if it's not that's false
it goes into the grateful and if it's
true it goes into a bowl fold of lemon
and apples the entropy after splitting
has decreased considerably so now we can
make two decisions if you look at
they're very much less chaos going on
there this node has already attained an
entropy value of zero as you can see
there's only one kind of label left for
this Branch so no further splitting is
required for this node however this node
on the right is still requires a split
to decrease the entropy further so we
split the right node further are based
on color if you look at this if I split
it on color that pretty much cuts it
right down the middle it's the only
thing we have left in our choices of
color and diameter too and if the color
is yellow it's going to go to the rifle
and if it's false it's going to go to
the left Bowl so the entropy in this
case is now zero so now we have three
moles with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with a hundred
percent accuracy and we can predict the
Apple also with 100 accuracy along with
our grapes up there so we've looked at
kind of a basic tree in our forest but
what we really want to know is how does
a random Forest work as a whole so to
begin our random Forest classifier let's
say we already have built three trees
and we're going to start with the first
tree that looks like this just like we
did in the example this tree looks at
the diameter if it's greater than or
equal to three it's true otherwise it's
false so one side goes to the smaller
diameter one side goes to larger
diameter and if the color is orange it's
going to go to the right true we're
using oranges now instead of lemons and
if it's red it's going to go to the left
false we build a second tree very
similar but split differently instead of
the first one being split by a diameter
this one when they created it if you
look at that first Bowl it has a lot of
red objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees so
you can see them all in one image so
this would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe as a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have they put the color in
there so ignore the color down there but
the diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is the diameter greater than equal to
three is a color orange well it doesn't
really know on this one but if you look
at the value and say true and go to the
right tree two classifies it as cherries
is a color equal red is is a shape a
circle true it is a circle so this would
look at it and say oh that's a cherry
and then we go to the other classifier
and it says is the diameter equal one
well that's false does it grow in the
summer true so it goes down and looks at
as oranges so how does this random
Forest work the first one says it's an
orange the second one said it was a
cherry and the third one says it's an
orange
and you can guess that if you have two
oranges and one says it's a cherry when
you add that all together the majority
of the vote says orange so the answer is
it's classified as an orange even though
we didn't know the color and we're
missing data on it I don't know about
you but I'm getting tired of fruit so
let's switch and I did promise you we'd
start looking at a case example and get
into some python coding today we're
going to use the case the iris flower
analysis
oh this is the exciting part as we roll
up our sleeves and actually look at some
python coating before we start the
python coding we need to go ahead and
create a problem statement wonder what
species of Iris do these flowers belong
to let's try to predict the species of
the flowers using machine learning in
Python let's see how it can be done so
here we begin to go ahead and Implement
our python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter note
book and I've already opened up a new
page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets import load Iris now
this isn't the actual data this is just
the module that allows us to bring in
the data the load Iris and the iris is
so popular it's been around since 1936
when Ronald Fisher published a paper on
it and they're measuring the different
parts of the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import our random forest
classifier from the sklearn module so
sklearn dot Ensemble import random force
classifier and then we want to bring in
two more modules and these are probably
the most commonly used modules in Python
and data science with any of the other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD p D is a common term used
for pandas and pandas is basically
creates a data format for us where when
you create a pandas data frame it looks
like an Excel spreadsheet and you'll see
that in a minute when we start digging
deeper into the code panda is just
wonderful because it plays nice with all
the other modules in there and then we
have numpy which is our numbers Python
and the numbers python allows us to do
different mathematical sets on here
we'll see right off the bat we're going
to take our NP and we're going to go
ahead and Seed the randomness with it
with zero so
np.random.seed is seating that as zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
Jupiter notebook and here we go we've
gone ahead and run the Imports now I'm
going to paste the code down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the IRS
data now we're actually assigning that
data to the variable Iris and then we're
going to go ahead and use the DF to
Define data frame
and that's going to equal PD and if you
remember that's pandas as PD so that's
our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Irish feature names
and we're going to do the DF head and
let's run this so you can understand
what's going on here
the first thing you want to notice is
that our DF has created what looks like
an Excel spreadsheet and in this Excel
spreadsheet we have set the columns so
up on the top you can see the four
different columns and then we have the
data iris.data down below it's a little
confusing without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print olivirus
and see what that looks like
so when I print all a virus I get this
long list of information and you can
scroll through here and see all the
different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary you'll have a
key or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of simple length simple width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back
to the Head
and this is one of the neat features of
pandas and Panda data frames
is when you do df.head or the panda
dataframe dot head it will print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
header set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero one two three four not
one two three four five so now we've got
our IRS data imported into a data frame
let's take a look at the next piece of
code in here and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
jupyter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have a DF species and this is
interesting because right here you'll
see where I have DF species in Brackets
which is the key code for creating
another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are acceptable let's go ahead and
run this code and see how this changes
and what we've done is we've added the
target from the iris data set as another
column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our our neural
network to look for these this different
data and know what a Sentosa is or a
Vera color which we'll see in just a
minute or virginica those are the three
that are in there and now we're going to
add one more column I know we're
organizing all this data over and over
again it's kind of fun there's a lot of
ways to organize it what's nice about
putting everything onto one data frame
is I can then do a printout and it shows
me exactly what I'm looking at and I'll
show you that where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy DOT random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length DF comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take 75 percent of the data roughly
because there's a Randomness involved
and we're going to use that to train it
and then the other 25 percent we're
going to hold off to the side and use
that to test it later on on so let's
flip back on over and see what the next
step is so now that we've labeled our
database for which is training and which
is testing let's go ahead and sort that
into two different variables train and
test and let's take this code and let's
bring it into our project and here we go
let's paste it on down here and before I
run this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def dot
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train one's called
test 75 in train 25 in test
and then to sort that out
we're going to do that by doing DF our
main original data frame with the iris
data in it and if DF is trained equals
true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and let's make it readable to
humans I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the DF columns
and we know the first four of them 0 1 2
3 that'd be the first four are going to
be the features or the titles of those
columns and so when I run this
you'll see down here that it creates an
index sepa length sepa width petal
length and petal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing I want you to notice here is
that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the a terminal
window if you just put the name of it
it'll print it out this is the same as
doing print
features
and the shorthand is you just put
features in here if you're actually
writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when I run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we've set y equal
to PD dot factorize train species of
zero so let's break this down just a
little bit we have our pandas right here
PD factorize what does factorize doing
I'm going to come back to that in just a
second let's look at what train species
is and why we're looking at the group
zero on there
and let's go up here and here is our
species
remember this and we created this whole
column here for species
and then it has cytosis cytosis cytosis
cytosa and if you scroll down enough
you'd also see virginica and Vera color
we need to convert that into something
the computer understands zeros and ones
so the trained species of zero because
this is in the format of a of an array
of arrays so you have to have the zero
on the end and then species is just that
column factorize goes in there and looks
at the fact that there's only three of
them so when I run this you'll see that
y generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go we have two
lines of code oh my goodness that was a
lot of work to get to two lines of code
but there is a lot in these two lines of
code so let's take a look and see what's
going on here and put this into our full
script that we're running and let's
paste this in here and let's take a look
and see what this is we have we're
creating a variable clf and we're going
to set this equal to the random forest
classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals two if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to across the system and
things like that and then the random
state is just how it starts zero is fine
for here
but let's go ahead and run this
we also have clf.fit train features
comma Y and before we run it let's talk
about this a little bit more clf dot fit
so we're fitting we're training it we
are actually creating our random Forest
classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
our test off to the side and we're going
to take our training set with the
features and then we're going to go
ahead and put that in and here's our
Target the Y so the Y is 0 1 and 2 that
we just created and the features is the
actual data going in that we put into
the training set let's go ahead and run
that
and this is kind of an interesting thing
because it printed out the random Force
classifier
and everything around it
and so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and it printed out y instead of print
y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals two
you can see the random State equals zero
those are the two that we sent in there
you would really have to dig deep to
find out all these the different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like Max features
is auto so all the features that we're
putting in there is just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process in leaf nodes that's
the end nodes remember we had the fruit
and we're talking about the leaf nodes
like I said there's a lot in this we're
looking at a lot of stuff here so you
might have in this case there's probably
only think three leaf nodes maybe four
you might have thousands of leaf nodes
at which point you do need to put a cap
on that and say okay you can only go so
far and then we're going to use all of
our resources on processing this and
that really is what most of these are
about is limiting the process and and
making sure we don't overwhelm a system
and there's some other settings in here
again we're not going to go over all of
them warm start equals false alarm start
is if you're programming it one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the SK learn and if you're digging in
deep and running a major project on here
for today though all we need to do is
fit or train our features and our Target
y so now we have our training model
what's next if we're going to create a
model
we now need to test it remember we set
aside the test feature test group 25 of
the data so let's go ahead and take this
code and let's put it into our script
and see what that looks like okay here
we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the setosa the
virginica and the Versa color and what
we're putting into our predict is the
test features and I always kind of like
to know what it is I am looking at so
real quick we're going to do test
features and remember features is an
array
of simple length sepal width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing so
what features looks like this is just
playing with the with pandas data frames
you'll see that it's an index so when
you put an index in like this
into test features into test it then
takes those columns and creates a panda
data frames from those columns and in
this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5.2 and we're going to put
those in and we're going to predict what
our new Forest classifier is going to
come up with and this is what it
predicts it predicts uh zero zero zero
one two one one two two two and and
again this is the flower type sotosa
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf DOT
predict probability so we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10.
so you'll see down here instead of
looking at all of them which was what 27
you'll see right down here that this
generates a much larger field on the
probability and let's take a look and
see what that looks like and what that
means
so when we do the predict underscore
praba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for setosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's uh you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we can
and we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5 and so if
we're going to vote and we have two
equal votes it's going to go with the
first one so it says uh satosha gets
zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to it a little bit so here's Iris
dot Target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the setosa
virginica versus color and then we're
going to go ahead and do the prediction
again we've run we could have just hit a
variable equal to this instead of
re-running it each time but we're going
ahead and run it again clf dot predict
test features remember that Returns the
zeros the ones in the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this
it distributes it it comes out as an
array and the array is setosis cytosis
cytosa we're only looking at the first
five we could actually do let's do the
first 25 just so we can see a little bit
more on there and you'll see that it
starts mapping it to all the different
flower types the Versa color and the
virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it and let's talk about both these
sections of code here
and how they go together the first one
is our predictions and I went ahead and
did predictions through 25 let's just do
five
and so we have cytosis cytosis cytosis
cytosis that's what we're predicting
from our test model
and then we come down here we look at
test species I remember I could have
just done
test.species.head and you'll see it says
cytosis cytosis cytosis cytosa and they
match so the first one is what our
forest is doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our jupyter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the jupyter notebook I can do a control
minus you can see the whole line there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species
so across the top you'll see the Sentosa
versus color virginica and the actual
species cystosa versicolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versicolor where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30. if you had 13 plus 5 plus 12
you get 30. and then we notice here
where it says virginica but it was
supposed to be versacolor this is
inaccurate so now we have two two
inaccurate predictions and 30 accurate
predictions so we'll say that the model
accuracy is 93 that's just 30 divided by
32 and if we multiply by a hundred we
can say that it is 93 percent accurate
so we have a 93 accuracy with our model
I did want to add one more quick thing
in here on our scripting before we wrap
it up so let's flip back on over to my
script in here we're going to take this
line of code from up above I don't know
if you remember it but predicts equals
the iris dot Target underscore names so
we're going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then I actually want to see what the
answer is so let's go ahead and type in
preds and print that out and when I run
this
you'll see that I've now predicted two
flowers so maybe I measured in my front
yard as versacolor and versacolor
not surprising since I put the same data
in for each one
this would be the actual end product
going out to be used on data that you
don't know the answer for
so that's going to conclude our
scripting part of this and let's just go
ahead and take a look at the key
takeaways with today's tutorial we have
Solutions under classification so we
looked at where the random Forest fits
in in the bigger model as far as
supervised learning and part of the
machine learning class and in this case
it's in classification and why a random
Forest the three main points has very
little overfitting if any it has a high
accuracy and in my opinion one of the
most powerful tools is it estimates
missing data we saw that with the
missing color of the fruit we talked
about what is a random Forest versus a
tree and then we went into how does a
decision tree work how does a random
Forest work we put all those trees
together and then we took a look at some
basic python coding in the iris example
so what is Gaming's clustering k-means
clustering is an unsupervised learning
algorithm in this case you don't have
labeled data unlike in supervised
learning so you have a set of data and
you want to group them and as the name
suggests you want to put them into
clusters which means objects that are
similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically is a
number so we need to tell the system how
many clusters we need to perform so if K
is equal to 2 there will be two clusters
if K is equal to 3 3 clusters and so on
and so forth that's what the k stands
for and of course there is a way of
finding out what is the best or Optimum
value of K for a given data we will look
at that so that is K means cluster so
let's take an example K means clustering
is used in many many scenarios but let's
take an example of Cricket the game of
cricket let's say you received data of a
lot of players from maybe all over the
country or all over the world world and
this data has information about the runs
scored by the people or by the player
and the wickets taken by the player and
based on this information we need to
Cluster this data into two clusters
batsman and Bowlers so this is an
interesting example let's see how we can
perform this so we have the data which
consists of primarily two
characteristics which is the runs at the
wickets so the bowlers basically take
wickets and the batsmen score runs there
will be of course a few Bowlers who can
score some runs and similarly there will
be some batsmen who will Who would have
taken a few wickets but with this
information we want to Cluster those
players into batsmen and Ballers so how
does this work let's say this is how the
data is so there are information there
is information on the y-axis about the
runs code and on the x-axis about the
wickets taken by the players so if we do
a quick plot this is how it would look
and when we do the clustering we need to
have the Clusters like shown in the
third diagram or TMS we need to have a
cluster which consists of people who
have scored High runs which is basically
the batsman and then we need a cluster
with people who have taken a lot of
wickets which is typically the bowlers
there may be a certain amount of overlap
but we will not talk about it right now
so with cayman's clustering we will have
here that means K is equal to 2 and we
will have two clusters which is batsman
and Bowlers so how does this work the
way it works is the first step in
k-means clustering is the allocation of
two centroids randomly so two points are
assigned as so-called centroids so in
this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the central idea okay so these
points though in our presentation here
we have shown them one point closer to
these data points and another closer to
these data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
center right this point is taken and the
distance is formed from this centroid
and the center and so on and so forth so
for every point the distance is measured
from both the centroids and then
whichever distance is less that point is
assigned to that centroid so for example
in this case visually it is very obvious
that all these data points are assigned
to this centroid and all these data
points are assigned to this centroid and
that's what is represented here in blue
color and in this yellow color the next
step is to actually determine the
central point or the actual centroid for
these two clusters so we have this one
initial cluster this one initial cluster
but as you can see these points are not
really the centroid centroid means it
should be the central position of this
data set Central position of this data
set so that is what needs to be
determined as the next step so the
central point on the actual centroid is
determined and the original randomly
allocated centroid is repositioned to
the actual centroid of this new clusters
and this process is actually repeated
now what might happen is some of these
points may get reallocated in our
example that is not happening probably
but it may so happen that the distance
is found between each of these data
points once again with these centroids
and if there is if it is required some
points may be reallocated we will see
that in a later example but for now we
will keep it simple so this process is
continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
Clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which means that the
algorithm has converged and we have the
final cluster with the data points and
the centroids so this is what you're
going to learn from this session we will
talk about the types of clustering what
is k-means clustering application of
k-means clustering gaming clustering is
done using distance measure so we will
talk about the common distance measures
and then we will talk about how k-means
clustering works and go into the details
of k-means clustering algorithm and then
we will end with the demo and a use case
for k-means clustering so let's begin
first of all what are the types of
clustering there are primarily two
categories of clustering hierarchical
clustering and then partitional
clustering and each of these categories
are further subdivided into
agglomerative and divisive clustering
and k-means and fuzzy Siemens clustering
let's take a quick look at what each of
these types of clustering are
in hierarchical clustering the Clusters
have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottom-up
approach we begin with each element as a
separate cluster and merge them into
successively larger clusters so for
example we have a b c d e f which start
by combining B and C Form 1 cluster DNA
form one more then we combine d e and f
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top-down
approach we begin with the whole set and
proceed to divide it into successively
smaller cluster so we have ABCDEF we
first take that as a single cluster and
then break it down
A B C D E and F then we have partitional
clustering split into two subtypes
k-means clustering and fuzzy c means in
k-min's clustering the objects are
divided into the number of clusters
mentioned by the number K that's where
the K comes from so if we say k is equal
to 2 the objects are divided into two
clusters C1 and C2 and the way it is
done is the features or characteristics
are compared and all objects having
similar characteristics are clubbed
together so that's how K means
clustering is done we will see it in
more detail as we move forward and fuzzy
c means is very similar to k-means in
the sense that it clubs objects that
have similar characteristics together
but while in k-min's clustering two
objects cannot belong to or any object a
single object cannot belong to two
different clusters in c means objects
can belong to more than one cluster so
that is the primary difference between
k-means and fuzzy c means so what are
some of the applications of k-means
clustering k-min's clustering is used in
a variety of examples or variety of
business cases in real life starting
from academic performance diagnostic
systems search engines and wireless
sensor networks and many more so let us
take a little deeper look at each of
these examples academic performance So
based on the scores of the students
students are categorized into a b c and
so on clustering forms a backbone of
search engines when a search is
performed the search results need to be
grouped together the search engines very
often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially k-means clustering uses
distance measure so let's take a look at
what is distance pressure so while these
are the different types of clustering in
this video we will focus on k-means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is euclidean distance there is
Manhattan distance then we have squared
euclidean distance measure and cosine
distance measure these are some of the
distance measures supported by k-means
clustering let's take a look at each of
these what is euclidean distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
minus y1 whole square plus x 2 minus X1
whole Square so this is an extension of
that formula so that is the euclidean
distance between two points what is the
squared euclidean distance measure it's
nothing but the square of the euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the
y-axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
k-means now let's go and check how
exactly K means clustering works okay so
this is how k-mains clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are a couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or 4 or 5 to start
with and then as we progress we keep
changing until we get the best clusters
or there is a technique called elbow
technique whereby we can determine the
value of K what should be the best value
of K how many clusters should be formed
so once we have the value of K we
specify that and then the system will
assign that many centroid so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that have been formed it
calculates the main position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are reallocated to the
new centroids and the mean position or
the new centroid is calculated once
again if the centroid moves then the
iteration continues which means the
convergence has not happened the
clustering has not converged so as long
as there is a movement of the centroid
this iteration keeps happening but once
the centroid stops moving which means
that the cluster has converged or the
clustering process has converged that
will be the end result so now we have
the final position of the centroid a and
the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
formed there is a parameter called
within sum of squares at the lower this
value is the better the cluster is that
means all these points are very close to
each other so we use this within sum of
squares as a measure to find the optimum
number of class Masters that can be
formed for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within SS or WSS that is taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the y-axis the within sum of squares
or WSS and on the x-axis we have the
number of clusters so as you can imagine
if you have K is equal to 1 which means
all the data points are in a single
cluster the witnesses value will be very
high because they are probably scattered
all over the moment you split it into
two there will be a drastic fall in the
within SS value and that's what is
represented here but then as the value
of K increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either 2 or 3 or at
the most 4 but beyond that increasing
the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned but now in this step the
actual centroid will be calculated which
may be for each of these data sets
somewhere in the middle so that's like
the main point that will be calculated
and the centroid will actually be
positioned or repositioned there
same with C2
so the new centroid for this group is C2
in this new position and C1 is in this
new position once again the distance of
each of the data points is calculated
from these centroids now remember it's
not necessary that the distance Still
Remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is the repetitive
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these days
data points as a part of each cluster so
I hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 n number of points
as our inputs and we want to split this
into K clusters or we want to create K
clusters so the first step is to
randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroid c 1 C
to C3 up to c k a and find which is the
lowest distance and assign X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and so there are K groups all
these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the mean position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as the part of Step
4 we repeat step two and three so we
again calculate the distance of X1 from
the centroid C1 C2 C3 and then C which
is the lowest value and assign X1 to
that calculate the distance of X2 from
C1 C to C3 or whatever up to c k and
find whichever is the lowest distance
and assign X2 to that centroid and so on
in this process there may be some
reassignment X1 Pro was probably
assigned to Cluster C2 and after doing
this calculation maybe now X1 is
assigned to C1 so that kind of
reallocation may happen so we repeat the
steps 2 and 3 till the position of the
centroids don't change or stop changing
and that's when we have convergence so
let's take a detailed look at it at each
of these steps so we randomly pick K
cluster centers we call them centroids
because they are not initially they are
not really the centroids so we let us
name them C1 C2 up to CK and then step
two we assign each data point to the
closest Center so what we do we
calculate the distance of each x value
from each C value so the distance
between X1 C1 distance between X1 C2 X1
C3 and then we find which is the lowest
value right that's the minimum value we
find and assign X1 to that particular
centroid then we go next to X2 find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to c k and then
assign it to the point or to the
centroid which has the lowest value and
so on so that is Step number two in Step
number three We Now find the actual
centroid for each group so what has
happened as a part of Step number two we
now have all the points all the data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will have K
groups but remember the initial values
of the C1 C2 were not really the
centroids of these groups right we
assign them randomly so now in step 3 we
actually calculate the centroid of each
group which means the original point
which we thought was the centroid will
shift to the new position which is the
actual centroid for each of these groups
okay and we again calculate the distance
so we go back to step 2 which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in Step 2 here okay so that might
change then we perform step 3 once again
to find the new centroid of this new
group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of k-means clustering
we will actually see some live demos and
python notebook using python notebook
but before that let's find out what's
the problem that we are trying to solve
the problem statement is let's say
Walmart wants to open a chain of stores
across the State of Florida
and it wants to find the optimal store
locations now the issue here is if they
open too many stores close to each other
obviously the they will not make profit
but if they if the stores are too far
apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use
k-means cluster to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
libraries like numpy matplotlab and so
on and we are loading the data that is
available in the form of let's say the
addresses for simplicity's sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups already being
formed so you can actually get an idea
about how the cluster would look and how
many clusters what is the optimal number
of clusters and then starts the actual
k-means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct clusters and which is
if we can say that this is how the
population is probably distributed
across Florida State and the centroids
are like the location where the store
should be the optimum location where the
store should be so that's the way we
determine the best locations for the
store and that's how we can help Walmart
find the best locations for the stores
in Florida area so now let's take this
into python notebook let's see how this
looks when we are learning running the
code live all right so this is the code
for k-means clustering in jupyter
Notebook we have a few examples here
which we will demonstrate how k-means
clustering is used and even there is a
small implementation of k-means
clustering as well okay so let's get
started okay so this block is basically
importing the various libraries that are
required like matplotlib and numpy and
so on and so forth which would be used
as a part of the code then we are going
and creating blobs which are similar to
clusters now this is a very neat feature
which is available in scikit-learn make
blobs is a nice feature which creates
clusters of data sets so that's a
wonderful functionality that is readily
available for us to create some test
data kind of thing
so that's exactly what we are doing here
we are using make blobs and we can
specify how many clusters we want so
centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard k-means functionality that
is readily available so we really don't
have to implement k-means itself the
k-means functionality or the or the
function is readily available you just
need to feed the data and we'll create
the Clusters so this is the code for
that we import k-means and then we
create an instance of k-means and we
specify the value of K this n underscore
clusters is the value of K remember K
means in K means K is basically the
number of clusters that you want to
create and it is a integer value so this
is where we are specifying that so we
have K is equal to 4. and so that
instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training kind of
thing but that's the call okay so we are
calling fit and what we are doing here
we are just passing the data so X has
these values the data that has been
created right so that is what we are
passing here and this will go ahead and
create the Clusters and then we are
using
after doing uh fit We Run The predict
which basically assigns for each of
these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or I will
actually start from zero cluster 0 1 2
and 3 maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y
underscore K means when we call predict
that is what it does and we can take a
quick look at these y underscore K means
or with the cluster numbers that have
been assigned for each observation so
this is the cluster number assigned for
observation one maybe this is for
observation two observation three and so
on so we have how many about I think 300
samples right so all the 300 samples
there are 300 values here each of them
the cluster number is given and the
cluster number goes from 0 to 3 so there
are four clusters so the numbers go from
0 1 2 3 so that's what is seen here okay
now so this was a quick example of
generating some dummy data and then
clustering that okay and this can be
applied if you have proper data you can
just load it up into X for example here
and then run the game so this is the
central part of the k-means clustering
program example so you basically create
an instance and you mention how many
clusters you want by specifying this
parameter and underscore clusters and
that is also the value of K and then
pass the data to get the values now the
next section of this chord is the
implementation of a k means now this is
kind of a rough implementation of the
k-means algorithm so we will just walk
you through I will walk you through the
code uh at each step what it is doing
and then we will see a couple of more
examples of how k-means clustering can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing k-means clustering and
there is a function for a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what k-means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to the so that is
basically available as a standard
function and we will be using that here
so as explained in the slides the first
step that is done in case of k-means
clustering is to randomly assign some
centroids so as a first step we randomly
allocate a couple of centroids which we
call here we are calling as centers
and then we put this in a loop and we
take it through an iterative process
for each of the data points we first
find out using this function pairwise
distance argument for each of the points
we find out which one which Center or
which are randomly selected centroid is
the closest and accordingly we assign
that data or the data point to that
particular centroid or cost staff and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
center position right so we calculate
the new centroid and then we check if
the new centroid is the coordinates or
the position is the same as the previous
centroid the positions we will compare
and if it is the same that means the
process has converged so remember we do
this process till the centroids or that
centroid doesn't move anymore right so
the centroid gets relocated each time
this reallocation is done so the moment
it doesn't change anymore the position
of the centroid doesn't change anymore
we know that convergence has occurred so
till then so you see here this is like
an infinite Loop while true is an
infinite Loop it only breaks when the
centers are the same the new center and
the old Center positions are the same
and once that is done we return the
centers and the labels now of course as
explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in if
the change will be very minor so in that
case also with that is actually
convergence right so for example the
change is point zero zero zero one we
can consider that as convergence
otherwise what will happen is this will
either take forever or it will be never
ending so that's small flaw here so that
is something additional checks may have
to be added here but again as mentioned
this is not the most sophisticated
implementation this is like a kind of a
rough implementation of the k-means
cluster ing okay so if we execute this
code this is what we get as the output
so this is the definition of this
particular function and then we call
that find underscore clusters and we
pass our data X and the number of
clusters which is 4 and if we run that
and plot it this is the output that we
get so this is of course each cluster is
represented by a different color so we
have a cluster in green color yellow
color and so on and so forth and these
big points here these are the centroids
this is the final position of the
centroids and as you can see visually
also this appears like a kind of a
center of all these points here right
similarly this is like the center of all
these points here at so this is the
example or this is an example of a
implementation of k-means clustering
right and next we will move on to see a
couple of examples of how k-means
clustering is used in maybe some real
life scenarios or use cases in the next
example or demo we are going to see how
we can use k-means clustering to perform
color compression we will take a couple
of images so there will be two examples
and we will try to use cayman's
clustering to compress the colors this
is a common situation in image
processing when you have an image with
millions of colors but then you cannot
render it on some devices which may not
have enough memory so that is the
scenario where where something like this
can be used so before again we go into
the python notebook let's take a look at
quickly the the code as usual we import
the libraries and then we import the
image
and then we will flatten it so the
reshaping is basically we have the image
information stored in the form of pixels
and if the images like for example 427
by 640 and it has three colors so that's
the overall dimension of the of the
initial image we just reshape it and
then feed this to our algorithm and this
will then create clusters of only 16
clusters so this this colors there are
millions of colors and now we need to
bring it down to 16 colors so we use K
is equal to 16 and this is uh when we
visualize this is how it looks there are
these are all about 16 million possible
colors the input color space has 16
million possible colors and we just
subcompress it to 16 colors so this is
how it would look when we compress it to
16 colors and this is how the original
image looks and after compression to 16
colors this is uh the new image looks as
you can see there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit so this is an example which
we are going to now see in Python
notebook let's go into the python node
and once again as always we will import
some libraries and load this image
called
flower.jpg okay so let me load that and
this is how it looks this is the
original image which has I think 16
million colors and this is the shape of
this image which is basically what is
the shape is nothing but the overall
size right so this is 427 pixel by 640
pixel and then there are three layers
which is this three basically is for RGB
which is red green blue so color image
will have that right so that is the
shape of this now what we need to do is
data let's take a look at how data is
looking so let me just create a new cell
and show you what is in data basically
we have captured this information
so data is what let me just show you
here
all right so let's take a look at China
what are the values in China and if we
see here this is how the data is stored
this is nothing but the pixel values
okay so this is like a matrix and each
one has about for for this 427 by 640
pixel XL all right so this is how it
looks now the issue here is these values
are large the numbers are a large so we
need to normalize them to between 0 and
1 right so that's why we will basically
create one more variable which is data
which will contain the values between 0
and 1 and the way to do that is divide
by 255 so we divide China by 255 and we
get the new values in data so let's just
run this piece of code and this is the
shape so we now have also yeah what we
have done is we changed using reshape we
converted into the three dimensional
into a two-dimensional data set and let
us also take a look at how
let me just insert
are probably a cell here and take a look
at how data is looking all right so this
is how data is looking and now you see
this is the values are between 0 and 1.
right so if you earlier noticed in case
of china the values were large numbers
now everything is between 0 and 1. this
is one of the things we need to
meet all right so after that the next
thing that we need to do is to visualize
this and we can take random set of maybe
10 000 points and plot it and check and
see how this looks so let us just plot
this
there so this is how the original the
color the pixel distribution is these
are two plots one is red against Green
and another is red against Blue and this
is the original distribution of
the car so then what we will do is we
will use k-means clustering to create
just 16 clusters for the various colors
and then apply that to the image now
what will happen is since the data is
large because there are millions of
colors using regular k-means may be a
little time consuming so there is
another version of k-means which is
called mini batch came in so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors would look so this is
red against Green and this is red
against Blue there is quite a bit of
similarity between this original color
schema and the new one right so it
doesn't look very very completely
different or anything like
now we apply this the newly created
colors to the image and we can take a
look how this is looking now we can
compare both the images so this is our
original image and this is our new image
so as you can see there is not a lot of
information that has been lost it pretty
much looks like the original image yes
we can see that for example here there
is a little bit it appears a little
dullish compared to this one right
because we kind of took off some of the
finer details of the color but overall
the high level information has been
maintained at the same time the main
advantage is that now this can be this
is an image which can be rendered on a
device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the Summer Palace in China and we repeat
the same process this this is a high
definition color image with millions of
colors and also three-dimensional
had now we will reduce that to 16 colors
using k-means clustering and we do the
same process like before we reshape it
and then we cluster the colors to 16 and
then we render the image once again and
we will see that the color the quality
of the image is slightly deteriorates as
you can see here this has much finer
details in this which are probably
missing here but then that's the
compromise because there are some
devices which may not be able to handle
this kind of a high density images so
let's run this chord in Python notebook
all right so let's apply the same
technique for another picture which is
even more intricate and has probably
much complicated color schema so this is
the image now once again we can take a
look at the shape which is 427 by 640 by
3 and this is the new data would look
somewhat like this compared to the
flower image so we have some new values
here and we will also bring this as you
can see the numbers are much big so we
will much bigger so we will now have to
scale them down to values between 0 and
1 and that is done by dividing by 255 so
let's go ahead and do that
and reshape it okay so we get a
two-dimensional Matrix and we will then
as a next step we will go ahead and
visualize this how it looks the 16
colors and this is basically how it
would look 16 million colors and now we
can
create the Clusters out of this the 16
k-means clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
ahead and apply this
visualize how it is looking for with the
with the new just the 16 color so once
again as you can see this looks much
richer in color but at the same time and
this probably doesn't have as we can see
it doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done using
k-means clustering and we have also seen
in the previous examples of how to
implement k-means the code to roughly
how to implement k-means clustering and
we use some sample data using blob to
just execute the cayman's clustering all
right so with that let's move on so
let's summarize what we have learned
learned in this video we started with an
example of how we can apply k-means
clustering taking the Cricut example
then we understood what are the types of
clustering two major categories
hierarchical clustering and partitional
clustering which in turn had two sub
categories agglomerative and divisive
and then k-means and fuzzy C then we
understood the distance measures what
are the different types of distance
measures supported by k-means clustering
and we focused on cayman's clustering we
talked about its applications and how
exactly the process flow works for the K
Mains clustering and then finally we
ended up with a demo and a couple of use
cases if you are an aspiring data
scientist who is looking out for online
training and certification in data
science from the best universities and
Industry experts then search number
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below have you ever
wondered how your mail provider
implements spam filtering or how online
news channels perform news text
classification or how companies perform
sentimental analysis of Their audience
on social media all of this and more is
done through a machine learning
algorithm called naive Bayes classifier
welcome to naive Bayes tutorial my name
is Richard kirschner I'm with the simply
learned team that's
www.simplylearn.com get certified get
ahead what's in it for you we'll start
with what is naive Bayes a basic
overview of how it works we'll get into
naive Bayes and machine learning where
it fits in with our other machine
learning tools why do we need naive
Bayes and understanding naive Bayes
classifier a much more in depth of how
the math works in the background finally
we'll get into the advantages of the
Nave Bays class of a fire in the machine
learning setup and then we'll roll up
our sleeves and do my favorite part
we'll actually do some python coding and
do some text classification using the
naive base what is naive Bayes let's
start with a basic introduction to the
Bayes theorem named after Thomas Bayes
from the 1700s who first coined this in
the western literature naive Bayes
classifier works on the principle of
conditional probability as given by the
Bayes theorem before we move ahead let
us go through some of the simple
Concepts and the probability that we
will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head heads they come
up as head tell tell head and Telltale
when doing the math on probability we
usually denote probability as a p a
capital P so the probability of getting
two heads equals one-fourth you can see
in our data set we have two heads and
this occurs once out of the four
possibilities and then the probability
of at least one tail occurs three
quarters of the time you'll see on three
of the twin tosses we have tails in them
and out of four that's three fourths and
then the probability of the second coin
being head given the first coin is tail
is one half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the Bayes
theorem gives us a conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Bayes theorem as you can see on
the left the probability of of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred times the probability of a
over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply a
Bayes theorem to our example here we
have our two quarters and we'll notice
that the first two probabilities of
getting two heads and at least one tail
we compute directly off the data so you
can easily see that we have one example
HH out of four one fourth and we have
three with tails in them giving us three
quarters or three-fourths 75 percent the
second condition the second set three
and four we're gonna explore a little
bit more in detail now we stick to a
simple example with two coins because
you can easily understand the math the
probability of throwing a tail doesn't
matter what comes before for it and the
same with the hit so still going to be
50 percent or one half but when that com
when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being tells given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
Bayes and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervisor's learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease fees or
other elements and news classification
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with
the naive Bayes understanding naive
Bayes classifier now we already went
through a basic understanding with the
coins and the two heads and two tells
and head tail tail heads Etc we're going
to do just a quick review on that and
remind you that the naive Bayes
classifier is based on the Bayes theorem
which gives a conditional probability of
event a given event B and that's where
the probability of a given b equals the
probability of B given a times
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we can multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easy come up with a new formula for the
probability of B to me staring at these
algebraic functions kind of gives me a
slight headache
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy ba now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery in day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
that's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a 20 percent
chance that they're not going to make a
purchase no matter what day of the week
it is and finally we look at the
probability of B of A in this case we're
going to look at the probability of the
weekday and not buying two of the no
buys were done out of the weekend out of
the six people who did not make
purchases so when we look at that
probability of the weekday without a
purchase is going to be 0.33 or 33
percent let's take a look at this at
different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
look we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.36 7 which
equals or 0.33 times 0.2 over 0.367
which equals 0.179 or roughly 17 to 18
percent and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we're actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday do they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
the probability of a over the
probability of B occurring now this is
basic algebra so we can multiply this
information together so when you see the
probability of a given B in this case
the condition is b c and d or the three
different variables we're looking at and
when you see the probability of B that
would be the conditions we're actually
going to multiply those three separate
conditions out probability of you'll see
that just a second in the formula times
the full probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of freedom delivery times
the probability is the day equal holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the 6 out of 30. we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 23 3 out of 30 had a
yes for free delivery and 11 out of 30
were on a holiday we plug all those
numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals
0.178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're owning a
shop that's like is the bottom line is
you want to make some money so you can
keep your shop open and have a living
now I promised you that we were going to
be finishing up the math here with a few
pages so we're going to move on and
we're going to do two steps the first
step is I want you to understand why you
went under why you want to use the naive
Bayes what are the advantages of naive
bays and then once we understand those
advantages we just look at that briefly
then we're going to dive in and do some
python coding advantages of naive Bayes
classifier so let's take a look at the
six advantages of the naive Bayes
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top of the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because
the two overlap they can then predict
the unknowns for the group that they
haven't done the second study on or vice
versa so it's very powerful in that it
is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
walk through it since we walk through
all the information on it but we're
going to do a python code doing text
classification very popular for doing
the naive Bayes so we're going to use
our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've trained it and we've shown you a
graphical what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup do we want the graphs to
show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have a
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so those you'll
see that those three lines of code here
are just importing the graph function so
we can graph it and as a data science
test you always want to graph your data
and have some kind of visual it's really
hard just to shove numbers in front of
people and they look at it and it
doesn't mean anything and then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.miscellaneous
and it goes all the way down to talk
politics.miscellaneous talk
religion.missella genius these are the
categories we've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one size so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train at
chrisstrain.data and we're just going to
look at data piece number five and let's
go ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
length of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewording the Second Amendment IDs
vtt line 58 lines 58 in article
Etc I'm going to scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we
have the print module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
is and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model.predict most of the sklearn use
the term dot predict to let us know that
we've now trained the model and now we
want to get some answers and we're going
to put our test data in there because
our test data is the stuff we held off
to the side we didn't train it on there
and we don't know what's going to come
up out of it and we just want to find
out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our jupyter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix and we
have our test Target our test data that
was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in and we create
our SN heat map the SNS is our Seaborn
which sits on top of the pi plot so we
create a sns.heat map we take our
confusion Matrix and it's going to be
met.t and do we have other variables
that go into the sns.heat map we're not
going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the zero zero zero one format d and c
bar equals false have to do with the
format if you take those out you'll see
that some things disappear and then the
X tick labels and the y t labels those
are our Target names and you can see
right here that's the alt atheism comp
graphics composms windows.miscellaneous
and then finally we have our plt.x label
remember the SNS or the Seaborn sits on
top of our matplot library our PLT and
so we want to just tell that X label
equals a true is is true the labels are
true and then the Y label is prediction
label so when we say a true this is what
it actually is and the prediction is
what we predicted and let's look at this
graph because that's probably a little
confusing the way we rattled through it
and what I'm going to do is I'm going to
go ahead and flip back to the slides
because they have a black background
they put in there that helps it shine a
little bit better so you can see the
graph a little bit easier so in reading
this graph what we want to look at is
how the color scheme has come out and
you'll see a line right down the middle
diagonally from upper left to bottom
right what that is is if you look at the
labels we have our predicted label on
the the left and our true label on the
right those are the numbers where the
prediction and the true come together
and this is what we want to see is we
want to see those lit up that's what
that heat map does is you can see that
it did a good job of finding those data
and you'll notice that there's a couple
of red spots on there where it missed
you know it's a little confused we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this function let me just expand that
just a notch here there we go I like
mine in big letters predict categories
we want to predict the category we're
going to send it as a string and then
we're sending it train equals train we
have our training model and then we had
our pipeline model equals model this way
we don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model dot predict s so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
I'll put that through our naive Bayes
and from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train.target namespredict of zero and
remember that the train.target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual number numbers are let's
say 0 equals alt atheism so we're going
to convert that 0 to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going to go
ahead and type in predict category and
let's just do the predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train.target underscore names
it'll automatically print that for you
in your own IDE you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify text into different
groups based on which category they
belong to using the naive Bayes
classifier now we did throw in the
pipeline the TF IDF vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bayes setup or
classifier but they're important to know
one of the main uses for the naive Bays
is with the TF IDF tokenizer vectorizer
where it tokenizes the word and has
labels and we use the pipeline because
you need to push all that data through
and it makes it really easy and fast you
don't have to know those to understand
naive Bayes but they certainly help for
understanding the industry and data
science and we can see their categorizer
our naive Bayes classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our
trained model let's go ahead and wrap it
up and let's just go through what we
covered I would give you an introduction
to naive Bayes and how it's used to
perform basic classification as a
classifier we went through the basic
formula the probability of a given B and
the probability of B given a the basics
of the naive Bayes we touch a little bit
on some of the different uses for the
naive Bayes we also went over the
advantages of it and where it really
shines especially when we talk about
real-time processing naive Bayes is very
fast we talked about the shopping demo
remember that if you want to try that on
your own send a note down below let us
know and we'll get you that data set and
we also went through the python my
favorite part the text classification so
we learned all kinds of things in there
and walking through a real life scenario
where you'd use the naive Bayes at
welcome to the session on deep learning
my name is Mohan and in this video we
are going to to talk about what deep
learning is all about some of you may be
already familiar with the image
recognition how does image recognition
work you can train a application or your
machine to recognize whether a given
image is a cat or a dog and this is how
it works at a very high level it uses
artificial neural network it is trained
with some known images and during the
training it is told if it is recognizing
correctly or not and then when new
images are submitted it recognizes
correctly based on the accuracy of
course so a little quick understanding
about artificial neural networks so this
is the way it does is you provide a lot
of training data also known as labeled
data for example in this case these are
the images of dogs and the network
extracts some features that makes a dog
a dog right so that is known as feature
extraction and based on that when you
submit a new image of dog the basic
features remain pretty much the same it
may be a completely different image but
the features of a dog still remain
pretty much the same in various
different images let's say compared to a
cat and that's the way artificial neural
network works we'll go into details of
this very shortly and once the training
is done with training data we then test
it with some test data too which is
basically completely new data with which
the system has not seen before unlike
the training data and then we find out
whether it is predicting correctly or
not thereby we know whether the training
is complete or it needs more training so
that's at a very high level artificial
neural network works so this is what we
are going to talk about today our agenda
look something like this what is deep
learning why do we need deep learning
and then what are the applications of
deep learning one of the main components
The Secret Sauce in deep learning is
neural networks so we're going to talk
about what is neural Network and how it
works and some of its components like
for example the activation function the
gradient descent and so on and so forth
so that as a part of working of a neural
network we will go into little bit more
details how this whole thing works so
without much further Ado let's get
started so deep learning is considered
to be a part of machine learning so this
diagram very nicely depicts what deep
learning is at a very high level you
have the all-encompassing artificial
intelligence which is more a concept
rather than a technology or a technical
concept right so it is it's more of a
concept at a very high level artificial
intelligence under the herd is actually
machine learning and deep learning and
machine learning is a broader concept
you can say or a broader technology and
deep learning is a subset of machine
learning the primary difference between
machine learning and deep learning is
that deep learning uses neural networks
and it is suitable for handling large
amounts of unstructured data and last
but not least one of the major
differences between machine learning and
deep learning is that in machine
learning the feature extraction or the
feature engineering is done by the data
scientists manually but in deep learning
since we use neural networks the feature
engineering happens automatically so
that's a little bit of a quick
difference between machine learning and
deep learning and this diagram very
nicely depicts the relation between
artificial intelligence machine learning
and deep learning now why do we need
deep learning machine learning was there
for quite some time and it can do a lot
of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's recently
planning is required compared to let's
say traditional machine learning it can
also perform complex algorithms more
complex than let's say what machine
learning can do and it can achieve best
performance with the large amounts of
data so the more you have the data let's
say reference data or label data the
better the system will do because the
training process will be that much
better and last but not least with deep
learning you can really avoid the manual
process of feature extraction those are
some of the reasons why we need deep
learning some of the applications of
deep learning deep learning has made
major inroads and it is a major area in
which deep learning is applied is
Healthcare and within Healthcare
particularly oncology which is basically
cancer related stuff one of the issues
with cancer is that a lot of cancers
today are curable they can be cured they
are detected early on and the challenge
with that is when a Diagnostics is
performed let's say an image has been
taken of a patient to detect whether
there is cancer or not you need a
specialist to look at the image and
determine whether it is the patient is
fine or there is any onset of cancer and
the number of Specialists are limited so
if we use deep learning if we use
automation here or if we use artificial
intelligence here then the system can
with a certain amount of the good amount
of accuracy determine whether a
particular patient is having cancer or
not so the prediction or the detection
process of a disease like cancer can be
expedited the detection process can be
expedited can be faster without really
waiting for a specialist we can
obviously then once the application once
the artificial intelligence detects or
predicts that there is an onset of a
cancer this can be cross-checked by a
doctor but at least the initial
screening process can be automated and
that is where the current focus is with
respect to deep learning in healthcare
what else robotics is another area deep
learning is majorly used in robotics and
you must have seen nowadays robots are
everywhere humanoids the industrial
robots which are used for manufacturing
process you must have heard about Sophia
who got a citizenship with Saudi Arabia
and so on there are multiple such robots
which are knowledge oriented but there
are also industrial robots are used in
Industries in the manufacturing process
and increasingly in security and also in
defense for example image processing
video is fed to them and they need to be
able to detect objects of vehicles and
so on and so forth so that's where deep
learning is used they need to be able to
hear and make sense of the sounds that
they are hearing that needs deep
learning as well so robotics is a major
area where deep learning is applied then
we have self-driving cars or autonomous
cars you must have heard of Google's
autonomous car which has been tested for
millions of miles and pretty much
incident free there were of course a
couple of incidents here and there but
it is considered to be fairly safe and
there are today a lot of Automotive
companies in fact pretty much every
automotive company worth its name is
investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these will be in production and they
will be used extensively in real life
right now they are all in r d and in
test phases but pretty soon these will
be on the road so this is another area
where deep learning is used and how is
it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars and the
site will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
wait so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens in a deep learning
environment so again a lot of scope here
to use deep learning couple of other
applications are mission translation so
today we have a lot of information and
very often this information is in one
particular language and more
specifically in English and people need
information in in various parts of the
world it is pretty difficult for human
beings to translate relate each and
every piece of information or every
document into all possible languages
there are probably at least hundreds of
languages or if not more to translate
each and every document into every
language is pretty difficult therefore
we can use deep learning to do pretty
much like a real-time translation
mechanism so we don't have to translate
everything and keep it ready but we
train applications or artificial
intelligence systems that will do the
translation on the Fly for example you
go to somewhere like China and you want
to know what is written on a signboard
that is impossible for somebody to
translate that and put it on the web or
something like that so you have an
application which is strained to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate so
therefore deep learning is used in such
situations as well and that is again on
demand so it is not like you have to
translate all these documents from other
languages into English in one shot and
keep it somewhere that is again pretty
much an impossible task but on a neat
basis so you have systems that are
trained to translate on the fly so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it needs a lot of
effort to train such neural land it has
been proved that it is possible so this
is a relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black and white
image this was colored by a neural net
or a deep learning application as you
can see it's done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning now one of the major Secret
Sauce of deep learning is neural network
deep learning works on neural network or
consists of neural network so let us see
what is neural network neural network or
artificial neural network is designed or
based on the human brain now human brain
consists of billions of small cells that
are known as neurons artificial neural
networks is in a way trying to simulate
the human brain so this is a quick
diagram of biological neuron biological
neuron consists of the major part which
is the cell nucleus and then it has some
tentacles kind of stuff on the top
called dendrite and then there is like a
long tail which is known as the axon
further again at the end of this axon
are what are known as synapses these in
turn are connected to the dendrites of
the next neuron and all these neurons
are interconnected with each other
therefore they are like billions of them
sitting in our brain and they're all
active they're working they based on the
signals they receive signals as inputs
from other neurons or maybe from other
parts of the body and based on certain
criteria they send signals to the
neurons at the other end so they they
get either activated or they don't get
activated based on so it is like a
binary Gates so they get activated or
not activated based on the inputs that
they receive and so on so we will see a
little bit of those details as we move
forward in our artificial neuron but
this is a biological neurons is the
structure of a biological neuron and
artificial neural network is based on
the human brain the smallest component
of artificial neural network is an
artificial neuron as shown here
sometimes is also referred to as
perceptron now this is a very high level
diagram the artificial neuron has a
small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as X1 X2 and so on each of the
inputs are multiplied by what is known
as weights which are represented as W1
W2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like X1
into W1 plus X2 into W2 and so on the
products are then added and then there
is a bias that is added to that in the
next slide we will see that passes
through an activation function and the
output comes as a y which is the output
and based on certain criteria the cell
gets either activated or not activated
so this output would be like a zero or a
one binary format okay so we will see
that in a little bit more detail but
let's do a quick comparison between
biological and artificial neurons just
like a biological neuron there are
dendrites and then there is a cell
nucleus and synapse and an axon we have
in the artificial neuron as well these
inputs come like that right if you will
act like the dendrites there is a like a
central unit which performs the
summation of these weighted inputs which
is basically W 1 x 1 W 2 x 2 and so on
and then a bias is added here and then
that passes through what is known as an
activation function okay so these are
known as the weights W1 W2 and then
there is a bias which will come out here
and that is added the biases by the way
common for a particular neuron so there
won't be like B1 b2b3 and so on only
weights will be one per input the bias
is common for the entire neuron it is
also common for or the value of the bias
Remains the Same for all the neurons in
a particular layer we will also see this
as we move forward and we see deeply
neural network where there are multiple
neurons so that's the output now the
whole exercise of training a neuron is
about changing these weights and biases
as I mentioned artificial neural network
will consist of several such neurons and
as a part of the training process these
weights keep changing initially they are
assigned some random values through the
training process the weights the whole
process of training is to come up with
the optimum values of W1 W2 and WN and
then the B for or the bias for this
particular neuron such that it gives an
accurate output as required so let's see
what exactly that means the training
process this is how it happens it takes
the inputs each input is multiplied by a
weight and these weights during training
keep changing so initially they are
assigned some random values and based on
the output whether it is correct or
wrong there is a feedback coming back
and that will basically change these
weights until it starts giving the right
output that is represented in here as
Sigma I going from 1 to n if there are n
inputs a w i into x i so this is the
product of W1 X1 W2 X2 and so on right
and there is a bias that gets added here
and that entire thing goes to what is
known as an activation function so
essentially this is Sigma of w i x i
plus a value of bias which is a b so
that entire thing goes as an input to an
activation function now this activation
function takes this as an input gives
the output as a binary output it could
be a 0 or a one there are of course to
start with let's assume it's a binary
output later we will see that there are
different types of activation functions
so it need not always be binary output
but to start with let's keep Simple so
it decides whether the neuron should be
fired or not so that is the output like
a binary output 0 or 1. all right so
again let me summarize this so it takes
the inputs so if you are processing an
image for example the inputs are the
pixel values of the image X1 X2 up to XL
there could be hundreds of these so all
of those are fed as so these are some
values and these pixel values again can
be from 0 to 256. each of those pixel
values are then multiplied with what is
known as a weight this is a numeric
value can be any value so this is a
number W1 similarly W2 is a number so
initially some random values will be
assigned and each of these weights are
multiplied with the input value and
their sum this is known as the weighted
sum so that is performed in this kind of
the central unit and then a bias is
added remember the bias is common for
each neuron so this is not the bias
value is not one bias value for per
input so just keep that in mind the bias
value that is one bias per neuron so it
is like this summation plus bias is the
output from this section this is not the
complete output of the neuron but this
is the bias for output for step one that
goes as an input to what is known as
activation function and that activation
function results in an output usually a
binary output like a zero or a one which
is known as the firing of the neuron
okay good so we talked about activation
function so what is an activation
function an activation function
basically takes the weighted sum which
is we saw W1 X1 W2 X2 the sum of all
that plus the bias so it takes that as
an input and it generates a certain
output now there are different types of
activation functions and the output is
different for different types of
activation functions moreover why is an
activation function required it is
basically required to bring in
non-linearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today relu has kind of
taken over so relu is by far the most
popular activation function that is used
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0
and 1. threshold function is the value
will be 0 up to a certain value and
Beyond that this is also known as a step
function and beyond that it will be 1 in
case of sigmoid there is a gradual
increase but in case of threshold it's
like also known as a step function there
is a rapid or instantaneous chain from 0
to 1 whereas in sigmoid we will see in
the next slide there is a gradual
increase but the value in this case is
between 0 and 1 as well Now relu
function on the other hand it is equal
to basically if the input is zero or
less than 0 then the output is zero
whereas if the input is greater than 0
then the output is equal to the input I
know it's a little confusing but in the
next slides where we show the relu
function it will become clear similarly
hyperbolic tangent this is similar to
sigmoid in terms of the shape of the
function however while Sigma it goes
from 0 to 1 hyperbolic tangent goes from
-1 to 1 and here again the increase or
the change from minus 1 to 1 is gradual
and not like threshold or step function
where it happens instantaneously so
let's take a little detail look at some
of these functions so let's start with
the sigmoid function so this is the
equation of a sigmoid function which is
1 by 1 plus e to the power of minus X so
X is the value that is the input it goes
from 0 to -1 so this is sigmoid function
the equation is 5X is equal to 1 by 1
plus e to the power of minus X and as
you can see here this is the input on
the x axis as X is the value is coming
from in fact it can also go negative
this is negative actually so this is the
zero so this is the negative value of x
so as X is coming from negative value
towards 0 the value of the output slowly
as it is approaching 0 it it slowly and
very gently increases and actually at
the point let me just use a pen at the
Point here it is it is 0.5 it is
actually 0.5 okay and slowly gradually
it increases to 1 as the value of X
increases but then as the value of y x
increases it tapers off it doesn't go
beyond one so that is the specialty of
sigmoid function so the output value
will remain between 0 and 1 it will
never go below zero or above one okay
that so that is sigmoid function now
this is threshold function or this is
also referred to as a step function and
here we can also set the threshold in
this case it is that's why it's called
the threshold function normally it is 0
but you can also set a different value
for the threshold now the difference
between this and the sigmoid is that
here the changes Rapid or instantaneous
as the x value comes from negative up to
zero it remains 0 and at 0 it pretty
much immediately increases to 1 okay so
this is a mathematical representation
edition of threshold function Phi X is
equal to 1 if x is greater than equal to
0 and 0 if x is less than 0. so for all
negative values it is 0 which since we
have set the threshold to be zero so as
soon as it reaches 0 it becomes 1. you
see the difference between this and the
previous one which is basically the
sigmoid where the increase from 0 to 1
is gradual and here it is instantaneous
and that's why this is also known as a
step function threshold function or step
function this is a relu is one of the
most popular activation functions today
this is the definition of relu Phi X is
equal to Max of X comma 0 what it says
is if the value of x is less than 0 then
Phi X is zero the moment it increases
goes beyond zero the value of Phi X is
equal to X so it doesn't stop at one
actually it goes all the way so as the
value of X increases the value of Y will
also increase infinitely so there is no
limit here unlike your sigmoid or
threshold or the next one which is
basically hyperbolic tangent okay so in
case of relu remember there is no upper
limit the output is equal to either 0 in
case the value of x is negative or it is
equal to the value of x so for example
here if the value of x is 10 then the
value of y is also 10 right okay so that
is relu and there are several advantages
of relu and it is much more efficient
and provides much more accuracy compared
to other activation functions like
sigmoid and so on so that's the reason
it is very popular all right so this is
hyperbolic tangent activation function
the function looks similar to sigmoid
function the curve if you see the shape
it looks similar to sigmoid function but
the difference between hyperbolic
tangent and sigmoid function is that in
case of sigmoid the out output goes from
0 to 1 whereas in case of hyperbolic
tangent it goes from -1 to 1. so that is
the difference between hyperbolic
tangent and sigmoid function otherwise
the shape looks very similar there is a
gradual increase unlike the step
function where there was an instant
increase or instant change here again
very similar to sigmoid function the
value changes gradually from -1 to 1. so
this is the equation of hyperbolic
tangent activation function yes then
let's move on this is a diagrammatic
representation of the activation
function and how the overall data or how
the overall progression happens from
input to the output so if we get the
input from the input layer by the way
the neural network has three layers
typically there will be three layers
there is an input layer there is an
output layer and then you have the
hidden layer so the input come from the
input layer and they get process in the
hidden layer and then you get the output
in the output layer so let's take a
little bit of a detailed look into the
working of a neural network so let's say
we want to classify some images between
dogs and cats how do we do this this is
known as a classification process and we
are trying to use neural networks and
deep learning to implement this
classification so how do we do that so
this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two hidden layers and what we
do is we provide labeled training data
which means these images are fed to the
network with the label saying that okay
this is a cat the neural network is
allowed to process it and come up with a
prediction saying whether it is a cat or
a dog and obviously in the beginning
there may be mistakes a cat may be
classified as a dog so we then say that
okay this is wrong this output is wrong
but every time it predicts correctly we
say yes this output is correct so that
learning process so it will go back make
some changes to its weights and biases
we again feed these inputs and it will
give us the output we will check whether
it is correct or not and so on so this
is a iterative process which is known as
the training process so we are training
the neural network and what happens in
the training process these weights and
biases you remember there were weights
like W1 W2 and so on so these weights
and biases keep changing every time you
feed these which is known as an Epoch so
there are multiple iterations every
iteration is known as an Epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so once again what
is the input this input could be like
1000 images of cats and dogs and they
are labeled because we know which is a
cat and which is a dog and we feed those
thousand images the neural network will
initially assign some weights and biases
for each neuron and it will try to
process extract the features from the
images and it will try to come up with a
prediction for each image and that
prediction that is calculated by the
network is compared with the actual
value whether it is a cat or a dog and
that's how the error is calculated so
let's say there are thousand images and
in the first run only 500 of them have
been correctly classified that means we
are getting only 50 accuracy so we feed
that information back to the network
further update these weights and biases
for each of the neurons and we run this
these inputs once again it will try to
calculate extract the features and it
will try to predict which of these is
cats and dogs and this time let's say
out of 1700 of them have been predicted
correctly so that means in the second
iteration the accuracy has increased
from 50 to 70 percent all right then we
go back again we feed this maybe for a
third iteration fourth iteration and so
on and slowly and steadily the accuracy
of this network will keep increasing and
it may reach probably you never know 90
95 and there are several parameters that
are known as Hyper parameters that need
to be changed and tweaked and that is
the overall training process and
ultimately at some point we say okay you
will probably never reach 100 accuracy
but then we set a limit saying that okay
if we receive 95 percent accuracy that
is good enough for our application and
then we say okay our training process is
done so that is the way training happens
and once the training is done now with
the training data set the system has
let's say seen all these thousand images
therefore what we do is the next step
like in any normal machine learning
process we do the testing where we take
a fresh set of images and we feed it to
the network the fresh set which it has
not seen before as a part of the
training process and this is again
nothing new in deep learning this was
there in machine learning as well so you
feed the test images and then find out
whether we are getting a similar
accuracy or not so maybe that accuracy
May reduce a little bit while training
you may get 98 and then for test you may
get 95 percent but there shouldn't be a
drastic drop like for example you get 98
in training and then you get 50 or 40
with the test that means your network
has not learned you may have to retrain
your network so that is the way neural
network training works and remember the
whole process is about changing these
weights and biases and coming up with
the optimal values of these weights and
biases so that the accuracy is the
maximum possible all right so a little
bit more detail about how this whole
thing works so this is known as forward
propagation which is the data or the
information is going in the forward
Direction the inputs are taken weighted
summation is done bias is added here and
then that is fed to the activation
function and then that is that comes out
as an output so that is forward
propagation and the output is compared
with the actual value and that will give
us the error the difference between them
is the error and in technical terms that
is also known as our cost function and
this is what we would like to minimize
there are different ways of defining the
cost function but one of the simplest
ways is mean square error so it is
nothing but the square of the difference
of the errors or the sum of the squares
of the difference of the errors and this
is also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
Square now there are different ways of
defining cost function it need not
always be the mean square error but the
most common one is this so you define
this cost function function and you ask
the system to minimize this error so we
use what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate or the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate is very small
then it will probably take very long to
train whereas if the learning rate is
very high then it will appear to be
faster but then it will probably never
what is known as converge now what is
Convergence now we are talking about a
few terms here convergence is like this
this is a representation of convergence
so the whole idea of gradient descent is
to optimize the cost function or
minimize the cost function in order to
do that we need to represent the cost
function as this curve we need to come
to this minimum value that is what is
known as the minimization of the cost
function now what happens if we have the
learning rate very small is that it will
take very long to come to this point on
the other hand if you have large Higher
Learning rate what will happen is
instead of stopping here it will cross
over because the learning rate is high
and then it has to come back so it will
result in what is known as like an
oscillation so it will never come to
this point which is known as convergence
instead it will go back and fought so
these are known as Hyper parameters the
learning rate and so on and these have
to be those numbers or those values we
can determine typically using trial and
error out of experience we we try to
find out these values so that is the
gradient descent mechanism to optimize
the cost function and that is what is
used to train our neural network this is
another representation of how the
training process works and here in this
example we are trying to classify these
images whether they are cats or dogs and
as you can see actually each image is
fat in each time one image is fed rather
and these values of X1 X2 up to X N are
the pixel values within this image okay
so those values are then taken and for
each of those values a weight is
Multiplied and then it goes to the next
layer and then to the next layer and so
on ultimately it comes as the output
layer and it gives an output as whether
it is a dog or a cat remember the output
will never be a named output so these
would be like a zero or a one and we say
Okay 0 corresponds to dogs and one
corresponds to cats so that is the way
it typically happens this is a binary
classification we have similar
situations where there can be multiple
classes which means that there will be
multiple more neurons in the output
layer okay so this is once again a quick
representation of how the forward
propagation and the backward propagation
works so the information is going in
this direction which is basically the
forward propagation and at the output
level we find out what is the cost
function the difference is basically
sent back as part of the backward
propagation and gradient descent then
adjust the weights and biases for the
next iteration this happens iteratively
till the cost function is minimized and
that is when we say the whole the
network has converged or the training
process has converged and there can be
situations where convergence may not
happen in rare cases but by and large
the network will converge and after
maybe a few iterations it could be tens
of iterations or hundreds of iterations
depending on what exactly the number of
iterations can vary and then we say okay
we are getting a certain accuracy and we
say that is our threshold maybe 90
accuracy we stop at that and we say that
the system is trained the train model is
then deployed for production and so on
so that is the way the neural network
training happens okay so that is the way
classification Works in deep learning
using neural network and this slide is
an animation of this whole process as
you can see the forward propagation the
data is going forward from the input
layer to the output layer and there is
an output and the error is calculated
the cost function is calculated and that
is fed back as a part of backward
propagation and that whole process
repeats once again okay so remember in
neural networks the training process is
nothing but the finding the best values
of the weights and biases for each and
every neuron in the network that's all
training of neural network consists of
finding the optimal values of the
weights and biases so that the accuracy
is maximum Star Wars fans would be
familiar with the golden life-sized
Hospitality robot C-3PO while Star Wars
might be said in a galaxy far far away
the reality of having machines talk and
respond to us in a human-like manner is
already a reality which keeps getting
more and more realistic with every
passing day the people you ask for
queries on websites your smart
assistants even calls made over the
internet all of them have one thing in
common none of them are actually human
now you must be thinking if they are not
human how do they manage to sound and
seem so human-like how do they respond
to me so intelligently and how are they
so articulate this my friends is the
magic of natural language processing
what is NLP natural language processing
or NLP refers to the branch of
artificial intelligence that gives the
machines the ability to read understand
and derive meaning from Human languages
NLP combines the field of linguistics
and computer science to decipher
language structure guidelines and to
make models which can comprehend break
down and separate significant details
from text and speech every day humans
interact with each other through public
social media transferring vast
quantities of freely available data to
each other this data is extremely useful
in understanding human behavior and
customer habits data analysts and
machine learning experts utilize this
data to give machines the ability to
mimic human linguistic Behavior this
helps save millions in terms of Manpower
and time as you don't need to always
have a person present at the other end
of a phone NLP is also a lot more
widespread than you may realize you use
it every day in seemingly normal and
insignificant situations don't know how
to correctly spell a word autocorrect
has you covered need to see if your
article or thesis will get flagged for
copyright violations that's okay a
plagiarism Checker will search through
the web and find any cases of published
documents which may match your work line
by line while NLP seems really cool yet
a Cutting Edge and complicated
technology concept it is actually pretty
easy to learn you start off with a
document or an article to make your
algorithm understand what is going on in
it you need to process it into a form
which is easily comprehensible by the
machine
this is no different than making a child
learn to read for the first time
you start off by performing segmentation
which is to break the entire document
down into its constituent sentences
you can do this by segmenting the
article along its punctuations like full
stops and commas
for the algorithm to understand these
sentences we get the words in a sentence
and to explain them individually to our
algorithm so we break down our sentence
into its constituent words and store
them this is called tokenizing where
each word is called a token we can make
the learning process Faster by getting
rid of non-essential Words which do not
add much meaning to our statement and
are just there to make our statement
sound more cohesive these words such as
are and the are called stop words now
that we have the basic form of our
document we need to explain it to our
machine we first start off by explaining
that some words like skipping skips
skipped are the same word with added
prefixes and suffixes this is called
stemming we also identify the base words
for different word tense mood gender Etc
this is called limitization stemming
from the base word Lemma now we explain
the concept of nouns verbs articles and
other parts of speech to the Machine by
adding these tags to our words this is
called part of speech tagging next we
introduce our machine to pop culture
references and everyday names by
flagging names of movies important
personalities or locations Etc that may
occur in the document this is called
named entity tagging once we have our
base words and tags we use a machine
learning algorithm like naive Bayes to
teach our model humans sentiment and
speech at the end of the day most of the
techniques used in NLP are simple
grammar techniques that we have been
taught in school
here is a question for you which of
these NLP techniques is used to obtain
words from sentences a stemming B
tokenization
c limitization d segmentation
give it a thought and leave your answers
in the comments section below three
lucky winners will receive Amazon gift
vouchers with the increasing demand for
automated language Solutions companies
are looking for NLP experts to join them
and are prepared to offer highly
lucrative salaries as well
if you want to learn more about NLP you
can check out simplylearn's postgraduate
program in Ai and machine learning in
collaboration with IBM in this program
you will learn about Frameworks like
Keras and tensorflow and get hands-on
experience in deep learning to become a
truly experienced AI engineer python is
the
widely used programming language today
when it comes to
science tasks and changes by
its audience most data scientists out
there are already leveraging the power
of python every day hi I'm a big chef
from Simply learn and well after some
thought and a bit more research I was
finally able to narrow down my choice of
top python libraries for data science
what are they let's find out so let's
talk about this amazing Library
tensorflow which is also one of my
favorites so tensorflow is a library for
high performance numerical computations
with around 35
000 GitHub comments and a Vibrant
Community of around 1500 contributors
and it's used across various scientific
domains it's basically a framework where
we can Define and run computations which
involves tensors and tensors we can say
are partially defined computational
objects again where they will eventually
produce a value that was about
tensorflow let's talk about the features
of tensorflow so tensorflow is majorly
used in deep learning models and neural
networks where we have other libraries
like torch and and thiano also but
tensorflow has hands down better
computational graphical visualizations
when compared to them also tensorflow
reduces the error largely by 50 to 60
percent in neural machine translations
it's highly parallel in a way where you
can train multiple neural networks and
multiple gpus for highly efficient and
scalable models this parallel Computing
feature of tensorflow is also called
pipelining also tensorflow has the
advantage of seamless performance as
it's packed by Google it has quicker
updates frequent new releases with the
latest of features now let's look at
some applications tensorflow is
extensively used in speech and image
recognition text-based applications time
series analysis and forecasting and
various other applications involving
video detection so favorite thing about
tensorflow that is already popular among
the machine learning community and most
are open to trying it and some of us are
already using it now let's look at an
example of a tensorflow model in this
example we will not dive deep into the
explanation of the model as it is beyond
the scope of this video so here we're
using amnest dataset which consists of
images of handwritten digits handwritten
digits can be easily recognized by
building a simple tensorflow model let's
see how when we visualize our data using
matplotlib Library the inputs will look
something like this then we create our
tensorflow model to create a basic
tensorflow model we need to initialize
the variables and start a session then
after training the model we can validate
the data and then predict the accuracy
this model has predicted 92 accuracy
let's see which is pretty well for this
model so that's all for tensorflow if
you need to understand this tutorial in
detail then you can go ahead and watch
our deep learning tutorial from Simply
learn as shown in the right corner
interesting right let's move on to the
next library now let's talk about a
common yet a very powerful python
Library called numpy numpy is a
fundamental package for numerical
competition in Python it stands for
numerical python as the name suggests it
has around 18 000 comments on GitHub
with an active community of 700
contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numpy addresses the slowness problem
partly by providing these
multi-dimensional arrays that we talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now let's talk about
features of number it's very easy to
work with large arrays and mattresses
using numpy numpy fully supports
object-oriented approach for example
coming back to ndre once again it's a
class possessing numerous methods and
attributes ndra provides for larger and
repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods I
always wanted to get rid of loops and
vectorization of numpy clearly helps me
with that now let's talk about the
application locations of nampai numpy
along with pandas is extensively used in
data analysis which forms the basis of
data science it helps in creating the
powerful n-dimensional array whenever we
talk about numpy the mention of the
array we cannot do it without the
mention of the powerful n-dimensional
array also number is extensively used in
machine learning when we are creating
machine learning models as in where it
forms the base of other libraries like
sci-fi scikit-learn
Etc when you start creating the machine
learning models in data science you will
realize that all the models will have
their basis numpy or pandas also when
numpa is used with SCI pi and matplotlib
it can be used as a replacement of
Matlab now let's look at a simple
example of an array in numpad as you can
see here there are multiple array
manipulation routines like their basic
examples where you can copy the values
from one array to another we can give a
new shape to an and Ari from maybe one
dimensional do we can make it as a two
dimensional array we can return a copy
of the array collapse into one dimension
now let's look at an example where this
is Jupiter notebook and we will just
create a basic array and uh for detailed
explanation you can watch our other
videos which Targets on these
explanations of each libraries so first
of all whenever we are using any library
in Python we have to import it so now
this NP is the Alias which we will be
using let's create a simple array
let's look what is the type of this
array
so this is an indiary type of array Also
let's look what's the shape of this
array
so this is a shape of the array now here
we saw that we can expand the shape of
the array
so this is where you can change the
shape of the array using all those
functions now let's create an array
using arrange functions if I give
arrange 12 it will give me a one day
array of 12 numbers like this now we can
reshape this array
to 3 comma 4 or we can write it here
itself
so this is how a range function and the
reshape function works for numpy now
let's discuss the next Library which is
PSI Pi so this is another free and open
source python Library extensively used
in data science for high level
computations so this Library as the name
suggests stands for Scientific Python
and it has around 19 000 commits on
GitHub with an active community of 600
contributors it is extensively used for
scientific and Technical computations
also as it extends numpy it provides
many user-friendly and efficient
routines for scientific calculations now
let's discuss about some features of
scipy so sci-fi has this collection of
algorithms and functions which is built
on the numpy extension of python
secondly it has various high level
commands for data manipulation and
visualization also the ndmh function of
scipy is very useful in
multi-dimensional image processing and
it includes built-in functions for
solving differential equations linear
algebra and many more so that was about
the features of scipy now let's discuss
its applications so cyber is used in
multi-dimensional image operations it
has functions to read images from disk
into numpy arrays to write arrays to
discuss images resize images
Etc solving differential equations
Fourier transforms then optimization
algorithms linear algebra Etc let's look
at a simple example to learn what kind
of functions are there in sci-fi here
I'm importing the constants package of
scipy Library so in this package it has
all the constants
so here I'm just mentioning C or H or
any and this Library already knows what
it has to fetch like speed of light
Planck's constant Etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science life
cycle yes pandas is the most popular and
widely used python library for data
science along with numpy and matplotlib
the name itself stands for python data
analysis with around 17 000 comets on
GitHub and an active community of 1200
contributors it is heavily used for data
analysis in cleaning as it provides fast
flexible data structures like data
frames series which are designed to work
with structured data very easily and
intuitively now let's talk about some
features of pandas so pandas offers this
eloquent syntax and Rich functionalities
like there are various methods in pandas
like Drop n a fill any which gives you
the freedom to deal with missing data
also Partners provides a powerful apply
function which lets you create your own
function and run it across a series of
data now forget about writing those four
Loops while using pandas also this
library's high level abstraction over
low level numpy which is written in pure
C then it also contains these high level
data structures and manipulation tools
which makes it very easy to work with
pandas like their data structures and
series now let's discuss the
applications of pandas so pandas is
extensively used in general data
wrangling in data cleaning then pandas
also Finds Its usage in ETL jobs for
data transformation and data storage as
it has excellent support for loading CSV
files into its data frame format then
pandas is used in a variety of academic
and Commercial domains including
statistics Finance Neuroscience
economics web analytics Etc then pandas
is also very useful in Time series
specific functionality like date range
generation moving window linear
regression date shifting Etc now let's
look at a very simple example of how to
create a data frame so data frame is a
very useful data structure in pandas and
it has very powerful functionalities so
here I'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
I'm using Jupiter notebook again and in
this before using pandas here I am
importing the pandas Library
let me go and run this so in data frame
we can import a file a CSV file Excel
files there are many functions doing
these things and we can also create our
own data and put it into Data frame so
here I am taking random data and putting
in a data frame also I'm creating an
index and then also giving the column
names so PD is the Alias we've given for
pandas random data of 6x4 index which is
taking a range of six numbers and column
name I'm giving as ABCD now let's go
ahead and look at it
so here it has created a data frame with
my column names ABCD my list has six
numbers 0 to 5 and a random data of six
by four so data frame is just another
table with rows and columns where you
can do various functions over it also I
can go ahead and describe this data
frame to see so it's giving me all these
functionalities where count and mean and
standard deviation
Etc okay so that was about pandas now
let's talk about next library and the
last one so matplotlib for me is the
most fun Library out of all of them why
because it has such powerful yet
beautiful visualizations we'll see in
the coming slides plot and mac.lib
suggests that it's a plotting library
for python it has around 26 000 comments
on GitHub and a very Vibrant Community
of 700 contributors and because of such
graphs and plots that it produces it's
majorly used for data visualization and
also because it provides an object
oriented API which can be used to embed
those plots into our applications let's
talk about the features of matplotlib
the pi plot module of matplotlab
provides Matlab like interface so
matplotlib is designed to be as usable
as Matlab with an advantage of being
free and open source also it supports
dozens of backends and output types
which means you can use it regardless of
which operating system you are using or
which output format you wish pandas
itself can be used as wrappers around
matpod lips kpi so as to drive Mac
broadly via cleaner and more modern apis
also when you start using this Library
you will realize that it has a very
little memory consumption and a very
good runtime Behavior now let's talk
about the applications of matplotlib
it's important to discover the unknown
relationship between the variables in
your data set so this Library helps to
visualize the correlation analysis of
variables also in machine learning we
can visualize 95 confidence interval of
the model just to communicate how well
our model fits the data then matpatliff
Finds Its application in outlier
detection using scatter plot Etc and to
visualize the distribution of data to
gain instant insights now let's make a
very simple plot to get a basic idea
I've already imported the libraries here
so this function matplotlib inline will
help you show the plots in the Jupiter
notebook this is also called a magic
function I won't be able to display my
plots in the jupyter notebook if I don't
use this function I am using this
function in numpy to fix random state
for reproducibility now I'll take my n
as 30 and will assign random values to
my variables so this function is
generating 30 random numbers here I am
trying to create a scatter plot so I
want to decide the area let's put this
so just multiplying 30 with random
numbers to the power 2 so that we get
the area of the plot which we will see
in just a minute so using the scatter
function and the Alias of Mac plot lip
as PLT I've created this if I don't use
this in a very small circle since my
starter plot it's colorful it's nice so
that's one very easy plot I suggest that
you Explore More of matplotlib and I'm
sure you will enjoy it let's create a
histogram so I'm using my the style is
GG plot and assigning some values to
these variables any random values
now we are assigning bars and colors and
Alignment to the plot and here we get
the graph so we can create different
type of visualizations and plots and
then work upon them using matplotlib and
it's just that simple so that was about
the leading python libraries in the
field of data science but along with
these libraries data scientists are also
leveraging the power of some other
useful libraries for example like
tensorflow Keras is another popular
Library which is extensively used for
deep learning and neural network modules
Keras wraps both tensorflow and theano
back-ends so it is a good option if you
don't want to dive into details of
tensorflow then scikit-learn is a
machine learning library it provides
almost all the machine learning
algorithms that you need and it is
designed to interpolate with numpy and
sci-fi then we have c bond which is
another library for data visualization
we can say that c born is an enhancement
of matplotlib as it introduces
additional plot types if you are an
aspiring data scientist or was looking
out for online training and
certification in data science from the
best universities and Industry experts
than search number simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
let's now start this lesson by defining
what data visualization is data
visualization is the technique to
present the data in a pictorial or
graphical format it enables stakeholders
and decision makers to analyze data
visually the data in graphical format
allows them to identify new trends and
patterns easily well you might think why
data visualization is important let's
explain with an example
you are a sales manager in a leading
Global organization the organization
plans to study the sales details of each
product across all regions and countries
this is to identify the product which
has the highest sales in a particular
region and up to production
This research will enable the
organization to increase the
manufacturing of that product in the
particular region
the data involved for This research
might be huge and complex the research
on this large numeric data is difficult
and time consuming when it is performed
manually
when this numeric data is plotted on a
graph or converted to charts it's easy
to identify the patterns and predict the
result accurately
the main benefits of data visualization
are as follows it simplifies the complex
quantitative information
it helps analyze and explore Big Data
easily
it identifies the areas that need
attention or Improvement
it identifies the relationship between
data points and variables
it explores new patterns and reveals
hidden patterns in the data
there are three major considerations for
data visualization they are Clarity
accuracy and efficiency
first ensure that data set is complete
and relevant this enables the data
scientist to use the new pattern's yield
from the data in the relevant places
second ensure using appropriate
graphical representation to convey the
right message
third use efficient visualization
technique which highlights all the data
points
there are some basic factors that one
would need to be aware of before
visualizing the data visual effect
coordination system data types and scale
informative interpretation
visual effect includes the usage of
appropriate shapes colors and size to
represent the analyzed data
the coordinate system helps to organize
the data points within the provided
coordinates
the data types and scale choose the type
of data such as numeric or categorical
the informative interpretation helps
create visuals in an effective and
easily interpretable manner using labels
title Legends and pointers
so far you have learned what data
visualization is and how it helps
interpret results with large and complex
data with the help of the Python
programming language you can perform
this data visualization
you'll learn more about how to visualize
data using the Python programming
language in the subsequent screens
many new python data visualization
libraries are introduced recently such
as matplot Library visby Boca Seaborn
pygal folium and networks
the matplot library has emerged as the
main data visualization Library
let's now learn about this matplot
library in detail
matplot library is a python
two-dimensional plotting library for
data visualization and creating
interactive Graphics or plots
using Python's matplot Library the data
visualization of large and complex data
becomes easy
there are several advantages of using
matplot library to visualize data they
are as follows
it's a multi-platform data visualization
tool built on the numpy and scipy
framework therefore it's fast and
efficient
it possesses the ability to work well
with many operating systems and graphic
back-ends
it possesses high quality graphics and
plots to print and view for a range of
graphs such as histograms bar charts pie
charts Scatter Plots and heat Maps
with Jupiter notebook integration the
developers have been free to spend their
time implementing features rather than
struggling with cross-platform
compatibility
it has large community support and
cross-platform support as it is an open
source tool it has full control over
graph or plot Styles such as line
properties fonts and axis properties
let's now try to understand a plot
a plot is a graphical representation of
data which shows relationship between
two variables or the distribution of
data look at the example shown on the
screen
this is a two-dimensional line plot of
the random numbers on the y-axis and the
range on the x-axis
the background of the plot is called
grid the text first plot denotes the
title of the plot in text line 1 denotes
the legend
you can create a plot using four simple
steps import the required libraries
Define or import the required data set
set the plot parameters
display the created plot
let's consider the same example plot
used earlier
follow the steps below to obtain this
plot
the first step is to import the required
libraries here we have imported numpy
and Pi plot and style from matplot
library numpy is used to generate the
random numbers and the pi plot which is
built in Python library is used to plot
numbers and style classes used for
setting the grid Style
the matplot library inline is required
to display the plot within jupyter
notebook
the second step is to define or import
the required data set here we have
defined the data set random number using
numpy random method note that the range
is 10. we have used the print method to
view the created random numbers
the third step is to set the plot
parameters in this step we set the style
of the plot labels of the coordinates
title of the plot The Legend and the
line width in this example we have used
ggplot as the plot Style
the plot method is used to plot the
graph against the random numbers
in the plot method the word g denotes
the plot line color as green label
denotes the legend label and it's named
as line one also the line width is set
to 2. note that we have labeled the
x-axis as range and the y-axis as labels
and set the title as first plot
the last step is to display the created
plot use the legend method to plot the
graph based on the set conditions and
the show method to display the created
plot
let's now learn how to create a
two-dimensional plot
consider the following example a Nutri
worldwide firm wants to know how many
people visit its website at a particular
time
this analysis helps it control and
monitor their website traffic
this example involves two variables
namely users and time therefore this is
a two-dimensional or 2D plot
take a look at the program that creates
a 2d plot
object web customers is a list on the
number of users and time hours indicates
the time
from this we understand that there are
123 customers on the website at 7am
645 customers on the website at 8am and
so on
the ggplant is used to set the grid
style and the plot method is used to
plot the website customers Against Time
don't forget to matplot library in line
to display or view the plot on The
jupyter Notebook
the website traffic curve is plotted and
the graph is shown on the screen
it's also possible to change the line
style of the plot to change the line
style of the plot use Define the line
style as dashed in the plot method
observe the output graph changes to a
dashed line also note that the color is
defined as blue using matplot Library
it's also possible to set the desired
axis to interpret the required result
use the axis method to set the axis in
this example shown on the screen the
x-axis is set to range from 6.5 to 17.5
and the y-axis is set to range from 50
to 2000. Let's Now understand how to set
the transparency level of the line and
to annotate a plot
Alpha is an attribute which controls the
transparency of the line lower the alpha
value more transparent than line here
the alpha value is defined as 0.4
the annotate method is used to annotate
the graph
the Syntax for annotate method is shown
on the screen the keyword Max is the
attribute that denotes The annotation
text
h a indicates the horizontal alignment
VA indicates the vertical alignment
X Y text indicates the text position and
X Y indicates the arrow position
the keyword Arrow props indicates the
properties of the arrow in this example
the arrow property is defined as the
green color
the output graph is shown on the screen
so far you've learned how to set line
width title x-axis and y-axis label
title of the plot Legend line color and
annotate the graph for a single plot
the plot we created for website traffic
in the previous screens is for only one
day let's now learn how to create
multiple plots say for three days using
the same example the data set number of
user for Monday Tuesday and Wednesday is
defined with respect to its time
distribution
use different color and line width for
each day to distinguish the plot in this
example we have used red for Monday
green for Tuesday and blue for Wednesday
the output graph is shown on the screen
a subplot is used to display multiple
plots in the same window with a subplot
you can arrange plots in a regular grid
all you need to do is specify the number
of rows columns and plot
the Syntax for a subplot as shown on the
screen it divides the current window
into an M by n grid and creates an axis
for a subplot in the position specified
by P for example subplot 2 1 2 creates
two subplots which are stacked
vertically on a grid
if you want to plot four graphs in one
window then the syntax used should be
subplot 2 1 4 layout and spacing
adjustment are two important factors to
be considered while creating subplots
use PLT subplots adjust method with the
parameters h space and w space to adjust
the distances between the subplot and
move them around on the grid
in this demo you can see how to create
two subplots that will display side by
side in a single frame
two supplies stacked one on top of the
other or vertically split in a single
frame and four subplots displayed in a
single frame
first import matplotlib Pi plot and
style
type percentage matplot live in line to
view the plot in Jupiter notebook
Define the parameters such as
temperature wind humidity precipitation
data and time data
you can see the data being typed here
next to create two subplots to be
displayed side by side in a given frame
for one two one and one two two specify
the figure size subplot space title the
color for time and temperature data
which is blue here and line style and
width
similarly specify the color for wind
which is red its line style and width
you can't see the temperature and wind
subplot charts displayed side by side in
a given frame here
to create subplots for two one one and
two one two specify the parameters
this will create two subplots stacked
one on top of the other or vertically
split in a given frame
let's use humidity and precipitation
data to plot the graphs
specify the title color line style and
line width for both the graphs
you can't see the two subplots stacked
one on top of the other with two
different colors indicating
precipitation and humidity here the two
graphs are separate
finally let's draw four subplots four
two two one
two two two
three and two two four that will display
in a given frame
specify the title subplot data color
line style and line width for all four
subplots
you can see the four subplots displayed
in a single frame
in this demo you learned how to create
subplots displayed side by side a
vertically split subplots and four
subplots displayed in a single frame
using matplotlib
you can create different types of plots
using matplot Library
histogram scatter plot heat map High
chart error bar
histograms
histograms are graphical representations
of a probability distribution in fact a
histogram is a kind of bar chart using
matplot library and its bar chart
function you can create histogram charts
a histogram chart has several advantages
some of them are as follows
it displays the number of values within
a specified interval
it's suitable for large data sets as
they can be grouped within the intervals
Scatter Plots
a scatter plot is used to graphically
display the relationship between
variables
a basic plot can be created using the
plot method however if you need more
control of a plot it's recommended that
you use the scatter method provided by
matplot Library it has several
advantages
it shows the correlation between
variables it's suitable for large data
sets it's easy to find clusters it's
possible to represent each piece of data
as a point on the plot
in this demo you'll learn how to
generate a histogram and scatter plot
using matplotlib
let's import a data set called Boston
dataset which we will use to create the
histogram hand scanner plot from the
scikit learn Library
let's import matplotlib Pi plot
type percentage matplotlib inline to
view the plot in jupyter Notebook
let's use the data in Boston real estate
data set to create the histogram and
scanner plot load this data
you can view this data by using the
print command
Now define the x-axis for the data which
is Boston real estate data
likewise Define the y-axis for the data
which is Boston real estate data with
the target extension
specify the plot style figure style
number of bins and labels of the x-axis
and y-axis use the show method to
display the histogram created by you
specify the style size data sets and
labels of the scatter plot that you want
to create use the show method to display
the scatter plot created by you
heat Maps
a heat map is a better way to visualize
two-dimensional data using heat maps you
can gain deeper and quicker insight into
Data than those afforded by other types
of plots
it has several advantages
it draws attention to the risky prone
area
it uses the entire data set to draw
bigger and more meaningful insights
it's used for cluster analysis and can
deal with large data sets
in this demonstration you'll learn how
to generate a heat map for a data set
using matplotlib
let's import the required libraries
matplotlib Pi plot and Seabourn
type percentage matplotlib inline to
view the plot in Jupiter notebook
let's load the flights data set from the
built-in data sets of Seaborn Library
use head to view the top five records of
the data set
we have to arrange the columns to
generate the heat map let's use the
pivot method to arrange the columns
month year and passengers
let's view the flight data set that's
now ready to generate the heat map
let's use the heat map method and pass
light data as an argument
this will generate the heat map which
you can see here
in this demo you learned how to create
and display a heat map
pie charts
pie charts are typically used to show
percentage or proportional data
note that usually the percentage
represented by each category is provided
next to the corresponding slice of the
pie
matplot Library provides the pi method
to make pie charts
it has several advantages
it summarizes a large data set in visual
form
it displays the relative proportions of
multiple classes of data
the size of the circle is made
proportional to the total quantity
in this demonstration you'll learn how
to create a pie chart and display it
first import matplotlib Pi plot
type percentage matplotlib in line to
view the plot in jupyter Notebook
type the job data within parentheses
using single quotes separated by commas
specify the labels as it Finance
marketing admin HR and operations
specify the slice it to explode
use the show method to display the pie
chart
you can't see the pie chart with the
slices labels and it the largest slice
error bars in error bar is used to show
the graphical representation of the
variability of data it's used mainly to
point out errors
it builds confidence about the data
analysis by unleashing the statistical
differences between the two groups of
data it has several advantages
it shows the variability in data and
indicates the errors
it depicts the Precision in the data
analysis
it demonstrates how well a function and
a model are used in the data analysis
it defines the underlying data
Seabourn is a python visualization
Library based on matplot Library it
provides a high level interface for
drawing attractive statistical Graphics
it was originally developed at Stanford
University and is widely used for
plotting and visualizing data there are
several advantages
it possesses built-in themes for better
visualizations it has tools built-in
statistical functions which reveal
hidden patterns in the data set it has
functions to visualize matrices of data
which become very important when
visualizing large data sets
now in order to learn Tableau the basic
first step is to import a sample data so
in our case what we have done is we have
imported a sample Superstore which is in
Excel format a sample superstore.xl
which has three worksheets in it orders
people and returns so by importing this
data into Tableau first of all we will
create relationships between these
sheets in order to identify who all have
placed orders and how many people have
returned the orders we will do some
analysis on the orders placed by certain
set of people and Order returned by a
certain set of people
now as we have imported uh the sheet we
will make certain joins so the first
step is to drag the orders
table the order sheet on the
relationship canvas here okay and you
can see the data sample data the first
100 rows over here
okay then now we need to create an inner
join with people's table between order
and people okay so if you see
it has automatically detected the field
names on which the inner join has to be
created so on the order side you have
region and on the right hand side which
is the people data you have also a
region okay so both these columns are
common and that's how we have made a
join between orders and people data
so if I close this box and go and check
the people's data
open
so see the region and the person these
two columns from the people table have
now been
joined with the orders table right so it
means that these are the orders in a
particular region which has been placed
by
in this region
let me show you the sample Superstore
Excel file now this is the structure of
the file you have a sample list of
transactions basically the orders which
are placed by customers
across multiple regions
south west of USA South Region west
region then you have a list of order IDs
which have been returned so basically
the order ID in the returns sheet
matches with those orders in the order
table
and then you have the people
sheet in which you have region
and a person associated with that region
the sales person associated with that
region okay so basically when we are
combining joining orders with people we
are joining that
which orders
belongs to which region and who's the
sales person associated with it so what
we have done over here is we have made a
inner join means all the orders should
belong to particular region and that
region is in the people's sheet
and then in the second step
now we will make a
left join between returns and orders
not inner join we'll make a left join
between returns and orders and we will
make a join using the
order ID
okay so just edit this
click on left
and
select order ID as the join column now
what does left join means left join mean
is that consider all the orders from the
orders table and only consider the
orders from the return statement which
have data means which are returned
otherwise show null for the order IDs
which are not returned so if you see
this is the these are the two columns
from the returns table and these are
null because this is relevant to the
order IDs which are not returned okay
which has been accepted by the customer
but these are the orders for which you
see data in the returned and Order ID
column it means that these have been
returned
now with these joins in place please
save your book and now we have our
relations created in the
um
in the Tableau now we are ready to
create certain reports and extract
certain kpis using this relationship
model
now we'll move to sheet1
okay
and first we will place
state
and person on the rose
sheet okay
then
I'll go to my
numbers and
put the profit or D so per state per
person how much profit I am making as a
company okay this is my goal to check
now
sort by highest to lowest
so California is giving me the maximum
profit of 76 381 then New York then
Washington so this is the sorted order
in which I have
listed my profit in descending order
now I can also check what are the number
of orders
placed
and check the distinct count
foreign
out of the total orders of 127 okay so
this is the number of total number of
orders which have been returned for
California is 127.
it's 16 29 so the sorted order is as per
the revenue as per the profit and this
is the details of the orders which have
been returned per state
so if you see for connected for a
cancers there are zero returns
so you can also extract data
table to refresh his orders and identify
new rows using order date so as and when
new data is being added you can
refresh it now say extract
and now you can save this information
profit
by state
and click save so this is the extraction
of this particular report which is
possible in tableau
so this is the first exercise which we
have completed for
reviewing and analyzing the profit per
state highest to lowest and within that
per state what are the number of orders
which have been returned by all the
customers the distinct count of order
IDs which have been returned
now let's start our second exercise on
creating calculated fields in tableau
now in this exercise we will be doing
certain
activities like we will be creating a
set to show the states which have more
than 100 customers then we will be
creating a calculated field to show an
average sales per customer okay then we
will create a calculated field to show
the sales goals and then show emerging
and developing stage so these are the
four kpis which we have to derive
now the first thing we have our sample
Superstore data already imported and the
relationships created in a join with
people and left joined with returns
now we have our sheet 2 in which we will
create the states a list of states which
has more than 100 customers
so what we have to do is we have to
click right click on the customer name
and click create set
okay
now we have to give the name as states
with 100 plus customers
and then go to the condition tab
select by field
and then apply condition as
count of customer name
greater than equal to
100
and click
ok
now we have this set created states with
100 plus customers
now to determine average sales by
customer we have to now create a
calculated field
so go to the analysis and click on
create calculate field
okay
now name it as
average
sales per customer
and now we will say average
we will use a
foreign
that per customer we are using a level
of definition function include which
means that per customer what is my
average sales right we've already used a
function aggregated function called
average so we are saying per customer
give me the total and then give me the
average per customer so we're going to
click ok
now create another calculated field you
can also create from here
and name is as name it as sales goal
now in this we are going to type the
formula if minimum
States
with 100 plus customers equal to true
then
sum of
sales
into 1.3
else
average sales per customer into
100 so me we are saying that
if the customer belongs to the set of
states with 100 plus customers then the
sales Target should be
1.3 times the actual sales as of today
else it should be 100 of the average
sales per customer
now let's create another calculated
field which we call as
emerging
or developing state
if distinct count
of customer name
is greater than equal to 100
then
the state is tagged as developing state
else it is called as
emerging state
okay so we have now
three calculated Fields average sales
per customer emerging or developing
State and sales goals
now we will use this in our reporting
so we will drag sales goal under the
columns
and then I'll drop my state
so now this is the statewise sales goal
depending whether the state has 100 plus
customers or not
then add your customer name
make the measure as count distinct
and make it as discrete
okay so if you see this
we have the count of customers per state
and the
the sales goal
for that particular state
and now
I'll put my sum of sales the total sales
which I want
which is there per state
now go to show me and select
this particular chart
bullet graph
now to bring sales goals to column right
click on the sales access and select
swap reference line fields
now from your left hand panel drag and
drop emerging or developing straight on
the color panel
okay so a merging state is the orange
one and the developing state is the blue
one
and save the sheet as
developing and the emerging States
so if you see this it's an emerging
State because its count is less than the
customer count is less than 100
its sales goal is
57384 but the actual sales is
19511 okay
so now this is a developing State its
count is greater than equal to 100 and
its sales goal and its sales is exactly
the same it matches so that's why you
are saying the bar and the blue bar is
ending exactly where the vertical bar is
foreign
so what we are trying to depict is that
whether the state is going Beyond its
Target sales goal or it's behind it
and you can see that using this
particular vertical bar like for example
Michigan its sales goal is 71 952 but
its actual sales is seven six seven two
seven zero average sales so that's why
it is be above its Target
and it's a developing state
because it has more than 100 customers
so you can even sort
by the count of
the uh customers higher to lower so all
your developing state will grow from at
the top and the emerging States Will
Group at the bottom
or you can sort by
the sales goal
so the orange bar is the sales goal or
the blue bar so depending what sales
goal
has been
derived for each state
let's go ahead and take a look at
building a resume always exciting
putting ourselves out selling ourselves
and if you looked at some of our other
videos dealing with some resumes you'll
see a trend here uh the top part is so
different than what resumes were in the
90s in the 2000 to 2010 they've evolved
they've really evolved it used to be
2000 2010 maybe LinkedIn maybe one other
reference now you're going to see that
we want those references this is a sales
tactic which now has come into resumes
used to be that if you're a real estate
agent every real estate agent I knew I
used to deal with real estate software
for the real estate industry back in the
90s every real estate agent won their
picture on their business card they
wanted them their picture if they could
put on their contracts they want people
to see the face so that's really a big
change is to make sure that it stands
out you stand out here's a picture of
somebody so you're more than just a
couple letters and a name but of course
you need your contact information should
always be at the top you have your
summary what are you focusing on be a
little careful with your summary because
if you have everything in your summary
and then they scroll down to experience
and education and skills they're going
to stop the second you repeat yourself
in your resume that usually means to the
reader hey this person doesn't have
anything more to offer me I'm done so be
a little careful how you word your
summary most companies appreciate when
you come in here and you've adjusted
this summary to be both about you and
how you're going to serve that company
so it's worth researching that company
to find out how those connect and put
that in the summary take some time to do
that that's actually a pretty big deal
and the references are huge also
especially in data science when you're
talking about any of the programming or
data science data analytics having a
place to go where they can look it up
and scroll down and see different things
you're doing whether it's LinkedIn in
this case which is a business profile
most commonly used GitHub Hub we have
stuff published Facebook I'm always
hesitant because that tends to push more
towards uh social media type jobs and
other jobs but certainly there's people
who have Facebook who do marketing and
stuff like that but these links having
these basic links here is important
people are starting to look for that for
some other uh setup maybe you have a
personal website this is a good place to
put that so that they now have a
multitude of links that go back to you
and highlight who you are and then the
next part or next four parts so for the
next four parts we have a combination of
experience education skills
certifications and you can see they're
organized if you have you know a lot of
people like to see what kind of degree
you have they want to know where it came
from and if you just got out of college
you're going to put education at the top
and then maybe you'll put skills after
that and then your experience at the
bottom if you've been in the field for
years you know my degree just to give
you my age goes back to the night early
90s so I usually put education at the
very bottom and then because a lot of
the stuff I'm trying to sell myself on
right now is my skills I actually put
that at the top and I'll put my
education my certifications at the
bottom my skills and then my experience
is since it's a huge part of my resume
goes next you can organize these in
whatever order you want that's going to
work best for you and sell you so
remember you're selling yourself this is
your image probably don't wear an old
tie-dye T-shirt with holes in it you
know something nice because it is
professional and of course your summary
your and then what do you have to offer
the company and again when I put out
resumes I haven't done a resume in a
while you go in there and you can take
this and reorganize this so if the
company is looking for something
specific you might put the experiences
specific to that company you might even
take experience if you have like a long
job history like I do I've gone into a
lot of different things you might leave
out those companies or those experiences
it had nothing to do with data science
because it just becomes overwhelming
resumes should only take about 30
seconds to glance over maybe a minute
tops because after that point you've
lost the person's interest and if they
want to dig deeper they now have links
they have your website they have
LinkedIn and they can now take this and
they come back to it and they go okay
let's look at this person a little
closer so quick overview this is your
sell sheet selling you to the company so
always tie it to the company so that you
have that what am I going to give this
company what are they going to get from
me if you are an aspiring data scientist
who is looking out for online training
and certification in data science from
the best universities and Industry
experts then search no long simply
learns postgraduate program in data
science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below
before we dive in and start going
through the questions one at a time
we're going to start with some of the
logical kind of concept that enters in a
lot of interviews and this one you have
two buckets one of three leaders and the
other are five liters you're expected to
measure exactly four liters how will you
complete the task and note you only have
the two buckets you don't have a third
bucket or anything like that just the
two buckets and the object of the
question like this is to see how well
you are thinking outside the box in this
case you're in a larger box you have two
buckets and also the pattern which you
go on and what that means is if you look
at the two buckets and we'll show you
their answer in just a second you have a
bucket with three liters and a bucket
with five liters and the first thought
is what happens if you go from left to
right so we have a direction and what
happens if you pour the three liters
into the five liter bucket if you pour
the three liters into five liter bucket
you have an empty bucket of three liters
and what's really important here is I
was thinking outside the box you realize
that you have a five liter bucket that
has three liters in it and two empty
liters so you have two additional
leaders you can fill up if we continue
that process we can compare from the
left to right from the small bucket to
the large bucket you can now measure
into additional liters into the five
liter bucket and three minus two is one
and you can keep doing that you can
empty the five liter bucket in pour
those three liters in that one liter in
and then you can pour three liters in
what's cool about these questions is you
explore them is you realize there's
multiple ways usually to solve I went
from small bucket to big bucket the
simply learned team their solution that
they pulled out was you fill the five
liter bucket and emptied into the three
liter bucket now you're left with two
liters in the five liter bucket so
that's great we can empty the three
liter bucket so now we're going from
large to small remember we went from
small to large so you can go both either
way but you have to go one way or the
other it turns out and you can empty the
three liter bucket and pour the contents
of the 5 liter bucket in it so the three
liter bucket now has two liters and if
it has two liters that means it has an
empty one liter and by now you probably
have guessed that if you have an empty
space you can start using that empty
space of one liter as a measuring so we
feel fill the 5 liter bucket again and
we pour the water in the three liter
bucket it already has the two liters and
so we're only pouring one liter in there
and five minus one is four so interview
questions they break up into all kinds
of different patterns we have logic like
this one which is a lot of fun we have
questions that come up that are more
vocabulary list the difference between
supervised and unsupervised learning
probably one of the fundamental
breakdowns in data science and
supervised learning uses known and
labeled data as input supervised
learning has a feedback mechanism most
commonly used supervised learning
algorithms are decision tree logistic
regression support Vector machine and
you should know that those are probably
the most common use right now and there
certainly are so many coming out so
that's a very evolving thing and be
aware of a lot of the different
algorithms that are out there outside of
the deep learning because a lot of these
work faster on Raw data numbers than
they do than a deep neural network would
unsupervised learning uses unlabeled
data as input unsupervised learning has
no feedback making most commonly used
unsupervised learning algorithms are
k-means clustering hierarchical
clustering the Aprilia log algorithm and
there certainly are more I'm going to
say k means definitely is at the top of
the list and the hierarchical clustering
those two are used so many times so
really important to understand what
those are and how they're used and most
important is understand that supervised
learning is you have your data set where
you have training data and you have all
those different pieces moving around but
you you're able to train it you know the
answers and unsupervised we're just
grouping things together that look like
they go together how is logistic
regression done logistic regression
measures the relationship between the
dependent variable our label what we
want to predict and the one or more
independent variables are features by
estimating probability using its
underlying logistic function sigmoid and
whenever I draw these charts I always
end up drawing them the right hand side
first because you want to know what your
output is what is U1 out of here in the
left hand side what do you have going in
so you have your in and out you can see
we have a nice labeled image here to
help you remember this we have our
inputs we have our linear model we have
our probabilities what are the
probabilities of it being a certain way
based on these features coming in the
sigmoid function and it's important to
note that the sigmoid function is maybe
the most commonly used but it's only one
of a number of functions that are out
there and the sigmoid function turns our
probabilities into a value between 0 and
1 or very close to zero very close to
one between 0.1 and 0.009 and based on
that we generate an answer in this case
a zero or one how is logistic regression
done so last time we talked about the
sigmoid function generally depending on
what your interview and level of math
and what expertise you're going in for
the market you'll have to understand
that formula of the probability equals
one over one plus e to the negative Y
and that's e to the base two so you have
your probability function or your
sigmoid function which pushes it as you
can see we have a nice visual of that
and it's that helps a lot to have that
visual on the sigmoid function you
definitely should know your y equals m
times X plus C you're basic lydian
geometry of forming a line and the slope
plus the intercept the y-intercept and
then you have your natural log and the
natural log is to the e as opposed to a
base 2 or base 10. so your natural log
to the E of the probability over one
minus a probability equals your M times
X plus C or your euclidean line that
helps a lot as far as the graphing and
understanding the sigmoid function so
we'll just keep pushing on to question
number three explain the steps in making
a decision tree and I noticed last time
we brought up the decision tree in the
forest a lot of questions came up what
is the difference so let's go through
that when you make a decision tree
you're going to take the entire data set
as input you're going to calculate
entropy of the target variable as well
as the predictor attributes I remembered
entropy is just how chaotic is it so if
you have like you know banana and grapes
and oranges if you're mixing in fruit
and that's your data coming in you have
all these different objects that are so
separate from each other and the more
they become uniform the lower the
entropy when we call that information
gain so we gain information on sorting
different objects from each other so you
have your entropy you have to calculate
your information gain of all attributes
and then you choose the attribute with
the highest Information Gain as the root
node so if you can separate your group
and each group chaos in each group is
lowered whichever split lowers the chaos
the most that's where you split it and
that's your root node at that point you
repeat the same procedure on every
Branch till the decision node of each
branch finalized so understanding that
setup is pretty important as far as
decision trees and you can see here we
have a nice visual of decision tree for
example if you want to build a decision
tree to decide whether we should accept
or decline a job offer since these are
interview questions that's a good one to
ask and just as a tip you should be
pretty aware of the formula for entropy
and Information Gain so you need to look
those up if you don't remember those and
the salary if it's greater than 50 000
no decline the off yes it's got a good
salary the commute is greater than an
hour yes decline the offer no offers
incentives Yes except the offer no
incentives declined the offer so we can
use decision tree pretty much for
everything if you want and if you have a
decision tree then you also should
understand how do you build a random
forest model and remember that a random
Forest is built up of a number of
decision trees so if you split your data
up into a lot of different packages and
you do a decision Tree in each of those
different groups of data the random
Forest is bringing all those trees
together so how do you build a random
forest model randomly select K features
from a total of M features where K is
less than M among the K features
calculate the node D using the best
split Point split the node into daughter
nodes using the best split repeat steps
two and three steps until Leaf nodes are
finalized build for us by repeating
steps one to four for n number times to
create n number of trees so you can see
it's got the same build pattern as the
tree but instead you're building a
number of different trees little small
trees so it all have an in leaf node
random Forest has a vote at the end and
whoever gets the most votes wins that's
the answer how can you avoid overfitting
of your model very important question in
any kind of mathematical scientific data
science setup in any of them there are
three main methods to avoid overfitting
and you should really understand
overfitting overfitting means that your
model is only set for a very small
amount of data and ignores the bigger
picture keep the model simple take into
account fewer variables thereby removing
some of the noise in the training data
good advice for any programming at all
use cross validation techniques such as
k-folds cross validation use
regularization techniques such as lasso
that penalize certain model parameters
if they're likely to cause overfitting
and you should also be well aware that
your cross-validation techniques that's
like a pre-data or your lasso and your
regularization techniques are usually
during the process so when you're
prepping your data that's when you're
going to do a cross validation such as
like splitting your data into three
groups and you train it on two groups
and test it on one and then switch which
two groups you tested on that kind
anything so can you solve another one of
these I love these things there are nine
balls out of which one ball is heavy in
weight and the rest are of the same
weight and how many minimum weighings
will you find the heavier ball and when
we say weighing think of a scale where
you can put objects on one side and the
other and you can see which side is
heavier and you want to minimize that
you want to split the balls up in such a
way that you're going to do as few
measurements as you can you will need to
perform two angs so you can get it down
to just two wings and I always think if
there's nine balls I'm gonna divide them
into three groups of three Place three
balls on each side so you can just
randomly pick six of the balls and three
on one side three on the other and if
they balance out both sides are equal
then you know the heavy weight isn't in
any of those so out of the remaining
three balls from step one take two balls
and place one ball on each side a little
tricky there because I always want to
put all three I want to put two on one
side and one on the other but no just
take randomly pick two of those put one
on each side if they balance out then
left out ball the one he didn't measure
will be the heavier one otherwise you'll
see it in the balance you'll see which
one's heavier because he'll take one of
the balls down now we go to scenario B
where they did not balance out so now we
know which side has a heavier ball in it
and it's just very similar to what we
did before if the balls in Step One do
not balance out then take those three
balls that have the heavier side on them
and reproduce step two to find out the
heavier ball difference between
univariate bivariate and multivariate
Analysis and hopefully if you know a
little Latin it'll kick in there that
you have uni and you have Bine you have
multi because the answer is in the words
themselves so the first one this type of
data contains only one variable so
that's the univariate purpose of the
univariate analysis is to describe the
data and find patterns exist within it
so when you only see one one variable
coming in in this case we're using
height of students you're limited as far
as what you can do with that data so you
can come up and draw different patterns
and conclusions from those patterns
using the means the median the mode
dispersion range minimum maximum so
we're describing the data so all those
words would describe the data and that's
about all you can do with data like that
there's no correlation there's nothing
to go beyond that as far as guessing or
predicting anything so we move into
bivariate you know uni means one by
means two bivariate this type of data
involves two different variables the
analysis of this type of data deals with
causes and relationships and the
analysis is done to find out the
relationship among the two variables and
this is always a favorite one because
everybody loves ice cream in the summer
when it's hot and very few people go for
ice cream in the winter when it's really
cold so it's easy to see the correlation
in the data the temperature and ice
cream cells in summer season and you can
see here where the temperature goes from
28 to 35 and as the temperature goes up
so does the cells of ice cream it goes
from 2000 I'm not sure 2 000 Watts I'm
guessing it's a very large chain because
if they're selling 2 000 ice cream cones
and they have a lot of business good for
them a little vendor on the corner
selling 2 000 ice cream cones a day and
3 100 the next day here the relationship
is visible from the table that
temperature in cells are directly
proportional to each other so the hotter
the temperature we can predict an
increase in sales so the word prediction
should come up so we have description
and prediction when the data involves
three or more variables it is
categorized under multivariate it is
similar to bivariate but contains more
than one dependent variable in this
example another really common one the
data for house price prediction the
patterns can be studied by drawing
conclusions using mean median and mode
dispersion or range minimum maximum Etc
and so you can start describing the data
that's what all that was and then using
that description to guess what the price
is going to be so this is very good if
you're in the market and you have
already looked at the area and you
already know that a two bedroom zero
floor 900 square foot house is usually
runs about forty thousand you can guess
what the next one that looks similar to
it is and I'll just throw in another
word in there I don't see very often
unless you're really a hardcore data
science we talked about describing the
data descriptive we talked about
predictive and there's also
postscriptive postscriptive means we're
going to change the variables to try to
guess what the outcome is if we change
what's going on so that would be the
next step but this usually doesn't show
up unless you're dealing with some
really hardcore data science groups what
are the feature selection methods to
select the right variables there are two
main methods for feature selection
there's filter methods and wrapper
methods and when you're filtering your
before we discuss the two methods real
quick the best analogy for selecting
features is bad data in bad answer out
so when we're limiting or selecting our
features it's all about cleaning up the
data coming in so it's cleaner and is
more representative of what we're trying
to predict filter method filter methods
as they come in we have linear
discrimination analysis a Nova
chi-squared chi-square is probably the
most common one and these are all part
of pre-processing we're taking out all
the the outliers all the things that
have a difference that is very different
from the data we're looking at the odd
ones and sometimes you take the odd ones
out and then you analyze them separately
to see why they're odd but remember your
filter method is you want to pull all
that weird stuff out wrapper methods on
the other hand are forward selection
backward selection recursive feature
elimination and one of the most
important things to remember about
wrapper methods is they're very labor
intensive you have to have some pretty
high-end computers if you're doing a lot
of data analysis with the wrapper method
and just quickly forward selection means
you have all your different features
they're off to the side and we test just
one feature at a time we keep adding
them in until we get a good fit backward
is we have all the features and we start
we run a test on that to see how well it
does and then we start removing features
to see what works and recursive which is
the most processing hungry algorithm out
there goes through and just recursively
looks through all the different features
and how they pair together but again we
have filter method and wrapper method
and it's important to understand that
we're sorting the data out finding out
which features are going to represent
the data the best and which ones are not
going to really add any value to our
models let's jump number eight in your
choice of language write a program that
prints the numbers from 1 to 50. but for
multiples of three print Fizz instead of
the number and for the multiples of five
print a buzz for numbers which are
multiples of both 3 and 5 print Fizz
buzz and this really is testing your
knowledge in iterating over data very
important my sister who runs at the
university the data science team is in
charge of their Department it's the
first question she asks in her interview
of anybody who comes in is how do they
iterate through data
so if you this question comes up a lot
and it's very important you have an
understanding and there's actually a
slight error on this code which I'll
point out in just a second the concept
is we have Fizz buzz in range and you
have range 51 which in this case goes
from 0 to 51 and I'm going to challenge
you to see if you can catch the error
and I'll tell you at the end of the code
where the error is what that means is
that we're going to go through all the
numbers 0 1 2 3 4 and we're going to
process through this Loop if the
remainder of Fizz Buzz divided by 3
equals zero and fizzbuzz divided by 5
also equals zero then print fizzbuzz
continue and else if fizzbuzz divided by
3 equals 0 in print Fizz print Fizz
continue else if is Buzz divided by five
equals zero print Buzz continue in print
Fizz buzz you fit the print the answer
in this case Fizz Buzz is either going
to be the number we generated which is 0
or it'll be the Fizz Buzz Fizz or Buzz
that's a mouthful now if you didn't
catch the error in the code which is
always a fun game find the error it has
to do with the range and it's important
to remember the range here says range to
51 that's 0 to 51 which is is correct we
want to go to 51 because it stops it
gets to 50 and it stops so that's 0 to
50. but if you remember the question
asked from 1 to 50. so the range should
be 1 comma 51 not just 51 which does 0
to 51. in this particular script in
Python you could leave out the continue
but the continue in the script skips the
next lcf so it doesn't keep processing
it going down and in a programming a lot
of scripts you don't need the continuous
this would depend on what script you
chose and there's probably some other
ways to do this it's a lot of fun and
you can see here from the output we end
up with fizz Buzz for zero which
shouldn't be there one two Fizz four
Buzz Fizz seven eight Fizz Buzz 11 Fizz
and so on sounds like a drinking game
for my college days so long ago many
decades ago you are given a data set
consisting of variables having more than
30 percent missing values how will you
deal with them oh the joy of messy data
coming in ways to handle missing data
values data set is huge we can just
simply remove the rows with missing data
values it is the quickest way I.E we use
the rest of the data to predict the
values you just go in there and say any
row of our data that has a n a in it get
rid of it that doesn't work with smaller
data so a smaller data you start running
into problems because you lose a lot of
data and so we can substitute missing
values with the mean or average of the
rest of the data using Panda's data
frame in Python there's different ways
to do this obviously in different
languages and even in Python there's
different ways to do this but in Python
it's real easy you can do the df.me so
you get the mean value so if you set
mean equal to that then you can do a
df.fill in a with the mean value very
easy to do in a python Panda script and
if you're using python you should really
know pandas and numpy number Python and
pandas data frames for the given points
how will you calculate the euclidean
distance in Python so back to our basic
algebra from high school euclidean
distance is the line on the triangle and
so if we're given the points plot one
equals one comma three plot two equals
two comma five we know that from this we
can take the difference of each one of
those points Square them and then take
the square root of everything so the
euclidean distance equals the square
root of plot one zero minus plot two is
zero squared plus plus one of one minus
plot two of one squared mouthful there
and you can remember if you have
multiple Dimensions that go past two
Dimensions you could have plot three can
simply be the distance from plot one you
only need to do one side of that or plot
two you can do either way and square
that and take the square root of that
another mind Bender how to calculate
some how to figure out the solution to
something what is the angle between the
hour and minute hands of a clock when
the time is half past six so you want to
kind of imagine that clock where the
large hand is pointed down to the 30 and
the other half is going to be ready to
between the six and the Seven because
it's half past six there's actually a
couple ways to solve this but let's take
a look and see how they did it note a
clock is a complete circle having 360
degrees in one hour the hour hand covers
360 over 12. so it equals 30 degrees for
each hour in one minute the minute hand
covers 360 degrees over 60 Minutes or 6
degrees per minute the minute hand has
traveled for 30 minutes so it has
covered 30 times 6 which equals 180
degrees so we know that's 180 degrees
from the 12. the hour hand has traveled
for 6.5 hours six and a half 6.5 so it's
covered 6.5 times 30 which equals 195
degrees the difference between the two
will give the angle between the two
hands thus the required angle equals 195
minus 180 equals 15 degrees and this is
nice the way they solved it because you
can now punch in any kind of time within
reason the hard part is on the hours is
you have to be able to convert the hours
into decimals explain dimensionality
reduction and list its benefits
Dimension reduction refers to the
process of converting a set of data
having vast Dimensions into Data with
lesser Dimensions fills to convey
similar information concisely it helps
in data compressing and reducing the
storage space it reduces computation
time as less Dimensions lead to less
computing it removes redundant features
for example there's no point in storing
a value in two different user units
meters and inches and I certainly run to
a lot with this with text analysis I've
been known to run a text analysis over a
series of documents ends up with over
1.4 million different features that's a
lot of different words being used and if
you do what they call buy connect them
you connect two words together now
you're up to 4.8 million different
features and you start having to figure
ways to bring that down what can we get
rid of that kind of thing so you can see
where that can get really high in on
processing and learning how to reduce
the list Dimensions is very important
important how will you calculate
eigenvalues and eigenvectors of a three
by three Matrix and what they're really
looking for here is when you write it
out for the eigen is that you know that
you're going to use the Lambda that's
the most common one obviously you can
use any assembly you want but Lambda is
usually what they use and that you do it
down the middle diagonal and so when you
take that Matrix and you take the
characteristic equation you end up with
a determinant and that's the minus 2
minus Lambda minus 4 2 minus 2 1 minus
Lambda 2 4 2 5 minus Lambda and that's
what they're looking for and you know
that's equal to zero so when you're
doing a matrix in the eigen setup with
the eigenvectors that's all going to
come out equal to zero and then you can
go ahead and write the whole equation
out so we can expand the determinant as
you can see right here the minus 2 minus
Lambda times it's a mouthful I'll leave
it up here for a second so you can look
at it when you break it down into the
algebraic functions you end up with
minus Lambda cubed plus 4 Lambda squared
plus 27 Lambda minus is 90 equals zero
so now we have a nice algebraic equation
built from the eigenvectors and always
remember you can hit the pause button
and you can also send a note send a note
to Simply learn if you have more
questions on vectors or on this
definitely you have that resource
available to you or post down in below
on the YouTube video comments and so
when we calculate the eigenvalues and
eigenvectors of a three by three Matrix
as we continue on down the math of this
and to be honest I really don't like
working with matrixes like this it's
important to understand the math behind
it and it's important to know the code
just enough so that you're not lost when
someone's explaining it or it comes up
and I'm working on different data
science models of course if you're
dealing with a high-end math side of it
then you better know this first is by
hitting trial so you try in different
variables to solve for zero and you can
come in here and you'll find that if we
put in the 3 in there we end up with a
zero at the end and substitute the three
hence we end up with Lambda minus 3 is
one of the factors and you can do them
math going out on that where we have
Lambda cubed minus 4 Lambda squared
minus 27 Lambda Plus 90 equals Lambda
minus 3 times Lambda squared minus
Lambda minus 30. So eigenvalues based on
that one are three minus five and six
and then from there we can calculate the
eigenvector for Lambda equals three and
you can see here where the Matrix as we
write it out is the minus five minus
four two minus two minus two minus two
four two two that's from the beginning
put in the X Y and Z equals zero zero
zero and so when we put in those numbers
and we calculate them out we have for x
equals one we have the minus five minus
four y plus two Z equals zero minus two
minus two y plus two Z equals zero and
subtracting the two equations we just
had we get three plus two y equals zero
y equals minus three over two and Z
equals minus one over two that's going
back to the first equation and similarly
we can calculate the eigenvectors for
minus five and six how should you
maintain your deployed model ooh
distribution time my favorite I spent 10
years in software distribution so first
thing and this is true not just of your
data science model but of any computer
code going out there's this basic setup
can work although usually there's a
little added steps in there first we're
going to monitor it so we have a
constant monitoring of all the model is
needed to determine the performance
accuracy of the models so yeah we want
to just keep an eye on it we want to
make sure they're accurate we want to
make sure that whatever they're supposed
to predict or I threw in that bonus word
PostScript where you change something
you want to figure out how your changes
are going to affect things we need to
monitor it make sure it's doing what
it's supposed to do evaluation metrics
of the current model is calculated to
determine if new algorithm is needed and
then we compare it the new models are
compared against each other to determine
which model performs the best and then
we do a rebuild the best performing
model is rebuilt on the current state of
data this is interesting I found this
out just recently if you're in weather
prediction the really big weather areas
have about seven or eight different
models depending what's going on and so
you actually have almost a little Forest
going on there where they're like which
model is going to fit best and this is
what we're going to use to predict the
weather with so not only do you don't
necessarily get rid of the models but
you figure out which model will fit data
of what's going on or the current state
of data what are recommender systems
most commonly used nowadays in marketing
so very big industry understanding
recommender systems predicts the rating
or preference a user would give to a
product and they they're split into two
different areas one is collaborative
filtering and a good example of that is
the Last.fm recommends tracks that are
often played by other users with similar
interests so people who if you're on
Amazon people who bought this also
bought that this got me a few times and
then there's contact based filtering and
we're looking at content instead of
looking at who else is listening to the
music these example Pandora which uses
the properties of a song to recommend
music with similar properties so you
have collaborative filtering and
content-based filtering how to find rmse
and MSC in linear regression model
hopefully you remember what the two
acronyms mean because that is like half
the answer we have the root mean square
error and the mean square error in
linear regression model so we're looking
for error the rmse and the MSC are the
two of the most common measures of
accuracy for a linear regression model
and you can see here we have the root
mean square error rmse equals and this
is the square root of the sum of the
predicted minus the actual squared over
the total number so we're just looking
for the average mean so we're looking
for the average over the end and the
reason you need to know about the
difference between REM SE versus MSC is
when you're doing a lot of these models
and you're building your own model why
do you need to take the square root of
it it doesn't change the value as far as
the way you're using it because you're
looking as to see whether the error is
greater or less than so why add that
extra computation in so a lot of models
use the MSC which indicates the mean
square error or the average error and
it's the same formula minus the square
root at the end or across the whole
thing another riddle to solve if it
rains on Saturday with a probability of
0.6 and it rains on Sunday with a
probability 0.2 what is the probability
that it rains this weekend and the trick
in probabilities on this case is we're
not we need to know what is the
probability of it not raining what is it
not what's the chance of it not raining
on Saturday and if it doesn't rain on
Saturday we want to take that and
combine that with the chance of it not
raining on Sunday the total probability
which in this case we're just going to
use 1 minus the probability that it will
not rain on Saturday so that's 1 minus
0.6 we're going to take that as a union
which we simply just multiply them
together of the probability they will
not rain on Sunday and it's important to
recognize the union here or the and you
can see by the formula down here we end
up with 0.68 or 68 percent chance that
it will rain on the weekend and there
are a couple other ways to solve this
but this is probably the most
traditional way of doing that how can
you select k for K means so first you
better understand that what K means is
and that K is the number of different
groupings and most commonly we use is
the elbow method to select k for K means
the idea of the elbow method is to run K
means clustering on the data set where K
is the number of clusters within the sum
of squares WSS is defined as the sum of
the squared distance between each member
of the cluster and its centroid and you
should know all the terms for your K
means on there and with the elbow point
and again here's our iteration in our
code we talked about that earlier you
iterate starting with usually you don't
start right at one but at least start
with two three or four let me just see
where it comes out and you can see the
nice elbow there which is easy to see
graphically where the number of K
clusters and the WSS value drops and
then it just kind of flattens out and
there's no reason to take the K means
any further what is the significance of
p-value oh good one especially if you're
dealing with r because that's the first
thing that pops up p-value typically
less than or equal to 0.05 indicates a
strong evidence against the null
hypothesis and you should know what a
difference why we use null hypothesis
instead of the hypothesis so you reject
the null hypothesis very important that
term null hypothesis in any scientific
setup and also in data science it
doesn't mean that it's true it means
that there's a high correlation that
it's true so if your null hypothesis
means it's not true your hypothesis is
has a high correlation that it's
probably true and if the p-value is
typically greater than 0.05 it indicates
a weak evidence against the null
hypothesis so you fail to reject the
whole null hypothesis and if you reject
that then your actual hypothesis is
probably not true the correlation of
your data with what you think it's
saying is is probably Incorrect and if
you're right at the cutoff of 0.05 it's
considered to be marginal could go
either way way and again you can use
that p-value on different features to
decide whether you're going to include
your features as far as something worth
exploring in your data science model how
can outlier values be treated oh good
one you can drop outliers only if it is
a garbage value so sometimes you end up
with like one outlier that just is
probably someone's measurements way off
height of an adult equals ABC feet this
cannot be true as height cannot be a
string value in this case outliers can
be removed if the outliers have extreme
values they can be removed for example
if all the data points are clustered
between 0 to 10 but one point lies at
100 then we can remove this point and
again sometimes you just look for the
outliers you can see what's going on if
there's something unusual there so maybe
the equipment's not calibrated correctly
if you cannot drop outliers you can try
the following try a different model data
detected as outliers by linear model can
be fit by non-linear model so we be sure
you are choosing the right model so if
it has like more of a curved look to it
instead of a straight line you you might
need to use something other than just a
straight line linear model try
normalizing the data this way the
extreme data points are pulled to a
similar range if you can use algorithms
which are less affected by outliers
example random Forest so there's another
solution is you can come up with the
random Force which a lot of times
completely bypasses your outliers how
can you say that a Time series data is
stationary oh that's an interesting term
stationary meaning it's not moving but
it's a Time series we can say that a
Time series is stationary when the
variance and mean of the series is
constant with time and this the graphic
example is very easy to see we have our
the variance is constant with time so we
have our first variable y and x and x
being the time factor and Y being the
variable as you can see goes through the
same values all the time it's not
changing in the long period of time so
that's stationary and then you can see
in the second example the waves get
bigger and bigger so that's
non-stationary here the variance is
changing with time again we have Y which
stays constant so that if you look at
the bigger picture it's the same wave
over and over again and then of course
we have where the wave is growing in
size going up we can also go down so
it'd also be non-stationary how can you
calculate accuracy using confusion
Matrix oh great one confusion matrixes
are so useful when you're taking that
first look at data and also when you're
showing the sharehold holders and you
want to ask them for money how can you
calculate accuracy using confusion
Matrix so you have your total data that
we're looking at is 650 and you have
your predicted values and your actual
values and you have your predicted p and
your actual p and so when you look at
this you'll note that if the predicted p
and the actual P are 262 but our
predicted P also had 15 that weren't
correct so you can see there's a false
positive there 15. and the same thing
with the N you can see where n predicts
in and it has a false negative of 26 out
of the total number of n values in there
and so we can do an accuracy on there
the true positive plus the true negative
is our total observations so you have a
total of 0.93 accuracy or 93 percent and
just a quick note on this this is so
important because it's one thing if
someone is being diagnosed with say
cancer you know this is life death or is
my nuclear reactor going to blow up
Suddenly if the p is the probability of
it blowing up and this could say you
have 15 that's a lot less than say the
26 chances of it blowing up you know so
the actual domain of your data is very
important so if you're non-positive you
don't really care about the predicted
value having non-positive is positive
because they're going to go do a biopsy
on the cancer or whatever anyway but
you're very interested if you have a
positive an actual positive value which
is looked at as negative a false
negative that's really important in that
domain depending on what domain you're
in write the equation and calculate
precision and recall rate and so
continuing with our confusion Matrix I
was just talking about the different
domains we have the Precision equals 262
2 over 277 so your Precision is the true
positive over the true positive plus
false positive and the recall rate is
your true positive over the total
positive plus false negative and you can
see here we have that 262 over 277
equals a 94 and the recall over here is
the 262 over 280 which equals 9.9 or 90
percent and oh good we're going to take
a pause for another brain teaser if a
drawer contains 12 Red Socks 16 blue
socks and 20 white socks how many must
pull out to be sure of having a matching
pair the last time I went through these
kind of brain teaser things was like 20
years ago and I had six people sitting
across the table waiting for my answer
that's kind of mind-numbing when you're
in an interview like that hopefully
you're not stuck in an interview like
that but uh on this you need to ask
yourself how many different colors of
socks are there so they've thrown a lot
of extra data in here that you don't
need to solve the the answer the answer
is four an example your first pick is
white your second pick is red third pick
is blue so no pairs yet and that means
when you get to the fourth pick there's
a hundred percent chance you're gonna
have a match so the most is going to be
four that you ever have to pull out of
your drawer if it was four colors the
answer would be five and so on it
doesn't matter how many white socks you
have or how many red socks or blue socks
different pairs you have it's the
different colors a number of different
colors people who bought this also
bought recommendations seen on Amazon as
a result of which algorithm oh we
covered this earlier recommendation
engine is done with collaborative
filtering collaborative filtering
explosive behavior of other users in
their purchase history in terms of
ratings selection Etc it makes
predictions on what you might interest a
person based on the preference of many
other users and this algorithm features
of the items are not known and we have a
nice example here where they took a
snapshot of a sales page it says for
example suppose X number of people buy a
new phone own and then also by tempered
glass with it next time when a person
buys a phone he'll be recommended to buy
tempered glass along with it and if you
remember the vocabulary words we covered
earlier this is the recommendation this
is collaborative the other word was
content based so looking at things with
similar content versus collaborative
which is similar people I remember you
know you're not going to know every
vocabulary word but it also doesn't hurt
to get your three by five cards out and
make yourself a vocabulary stack of
cards buy an app on your phone for it
SQL query I remember back in the 90s it
was so important to know SQL query and
only a few people got it nowadays it's
just part of your kit you have to know
some basic SQL so write a basic SQL
query to list all orders with customer
information and you can kind of make up
your own name for the database and you
can pause it here if you want to write
that down on a paper and let's go ahead
and look at this we have to list all
orders with customer information and so
usually you have an order table and a
customer table and you have an order ID
d a customer ID order number total
amount and then from your customer table
you have ID first name last name City
County and so if we're going to write in
SQL with this we're going to select
keyword there for SQL selecting order
number total amount first name last name
City Country so that's the columns we're
going to look at we're going to do that
from our order where we're going to join
it with our customer and we're going to
join it on the order customer ID equals
the customer ID so very basic SQL query
that's going to return a table of data
for us you are given a data set on
cancer detection you've built a
classification model and achieved an
accuracy of 96 percent 96 why shouldn't
you be happy with your model performance
what can you do about it that's an
interesting one because this comes up
that's one of the standard data sets on
there is for cancer detection cancer
detection results in imbalanced data in
an imbalanced data set accuracy should
not be based as a Major Performance
because it is important to focus on the
remaining four percent which are the
people who were wrongly diagnosed we
talked to the bill about this earlier
you have to know your domain you know
this is the medical cancer domain versus
weather domain you know whether a
channel they get by with 50 wrong in
cancer you don't want four percent of
the people being wrongly diagnosed wrong
diagnosis is of a major concern because
there can be people who have cancer but
we're not predicted so in an imbalanced
data set accuracy should not be used as
a measurement performance which of the
following machine learning algorithm can
be used for inputting missing values of
both categorical and continuous
variables and so we have a couple
choices here we have k-means clustering
we have linear regression we have the K
Inn nearest neighbor and decision tree
and which of the following machine
learning algorithms can be used for
inputting missing values of both
categorical and continuous variables now
certainly you can use some
pre-processing to do some of that but
you should have gone with the K nearest
neighbor because it can compute the
nearest neighbor and if it doesn't have
the value it just computes in nearest
neighbor based on all the other features
where when you're dealing with k-means
clustering or linear regression you need
to do that in your pre-processing
otherwise it'll crash decision trees
also although there's some variants on
that too can you solve another riddle
always fun ones given a box of matches
and two ropes not necessarily identical
measure a period of 45 minutes and in
this particular setup the ropes are not
uniform in nature and the Rope takes
exactly 60 Minutes to completely burn
out so each rope takes up to 60 Minutes
to burn out and there's actually a
couple different solutions to this but
let me go ahead and one of the things is
they're not uniform in nature so even
though they take 60 minutes anyways
let's go ahead and see what they did to
solve it and then we can also look at
different options we have two ropes A
and B light a from both ends and B from
one end okay when a is finished burning
we know that 30 minutes have elapsed and
B has 30 minutes remaining now like the
other end of B all also so that the
remaining part of B will burn taking 15
minutes to burn this we have gotten 30
plus 15 equals 45 minutes excellent
solution mine which I like was to take
one rope fold it in two so we know it's
a half hour take the other rope fold it
in four places so we know that that
one's 15 minutes and then you can just
connect the two and burn it straight
across I think they're trying to cover
that by saying they're not regular the
ropes are have some irregularities maybe
that's what they meant by that you
couldn't do something like that that's
my solution below are the eight actual
values of Target variable in the train
file so we have a training file and not
to be confused with the Train on the
tracks we have zero zero zero one one
one one one what is the entropy of the
target variable we mentioned earlier
that you should know your entropy and
how to calculate the entropy what is the
entropy of the target variable so we
have a couple options here we have minus
five over eight logarithm of five over
eight plus three over eight logarithm of
three over eight okay okay let's just
see where they got those numbers from we
have one which is going to be five ones
and three zeros and then we have a total
of eight okay and then we have the
option of five which is number of ones
five eight logarithm of 5 8 plus 3 8
logarithm of 3 8 and we also have three
eighths logarithm of 5 8 plus 5 8
logarithm of 3 8 and then we kind of
reverse those numbers around and let's
see what you're gonna get here which one
did you think it was you should have
checked the first one so what is the
entropy of the target variable the key
there is a target variable so we're
looking at the Target in this case is
going to be one usually that's what
you're looking for and so the entropy of
that one we want to subtract out the
entropy of the non-target variable oops
I had that backwards we want to we're
looking at zero so we want to subtract
out the 5 8 from there so 5 8 logarithm
5 8 or negative 5 8 logarithm 5 8 plus 3
8 logarithm three-eighths and they have
the hint on the bottom entropy equals I
of P of n so we have a negative p plus p
and N times the logarithm base two of p
over P plus n minus the N over P plus n
times the logarithm two of n over P plus
n we want to predict the probability of
death from heart disease based on three
risk factors age gender and blood
cholesterol level what is the most
appropriate algorithm for this case so
we have three features and we want to
know the predictability of death okay A
little morbid there choose a right
algorithm do we want to use logistic
regression for this linear regression K
means clustering or the Aprilia
algorithm and if you selected logistic
regression then you've probably got the
right answer linear regression remember
deals with like you take your line and
draw a line through the data and of
course you don't necessarily have to use
a straight line there's other means for
that but you're dealing with a lot of
numbers and K means means we're just
going to Cluster objects together with
the logistic regression though you can
mix those things together in buckets so
really the logistic regression is what
you want to use in that model would be
the most app fit after studying the
behavior of a population you have
identified four specific individual
types who are valuable to your study you
would like to find all users who are
most similar to each individual type
which algorithm is most appropriate for
this study certainly identifying census
in just about a lot of different markets
is common so maybe they have a census or
whatever it means but let's take a look
at some of the different algorithms we
might use on this we have k-means
clustering linear regression Association
rules and decision trees and I'll give
you a hint we're looking for grouping
people together by similarities and by
four different similarities so very
specific they gave you one of the values
specifically the K value so K means
clustering would be great for this
particular problem you have run the
association rules algorithm on your data
set and the two rules banana apple is
associated with grape and apple orange
is associated with grape have been found
to be relevant what else must be true so
this should challenge you to understand
Association rules you could picture in
this particular one you're going
shopping and you almost always see
somebody who has bananas they usually
have grapes in their bag also and
somebody who has apples usually has
grapes in their bags and then apples and
oranges is also associated with grapes
and let's go ahead and take a look at
that and we have a couple different
options here first one is banana apple
and grape orange must be a frequent item
set not so much banana apples oranges
must be relevant rule grape is common
with banana apple must be a relevant
Rule and how about grape apple must be a
frequent item set let's go back and take
a look at that and we notice that we
have bananas apples to grapes we have
apple orange to Grape boy there's a lot
of grapes and a lot of apples in there
and so if you said the last one grape
and apple must be a frequent item set
then you got it correct your
organization has a website where
visitors randomly receive one of two
coupons it is also possible that
visitors to the website will not receive
a coupon you have been asked to
determine if offering a coupon to
visitors to your website has any impact
on their purchase decision which
analysis method should you use and so
let's go ahead and start by giving you
another hint and give you some limiting
your selection we have a one-way Anova K
means clustering Association rules and
student t-test so obviously if you know
what each one of these means but let's
take a look at the question again so you
want to know which method should you use
to see if the coupons valid for their
purchase well we're not clustering and
we're not associating things together we
want to know the end result student
t-test also drawing that little T in
boxes and switch them around there's
really only one answer that works in
here and that's the one-way Anova
wouldn't that be as component of this
session on data science full course if
you have any queries regarding any of
the topics covered in this session or if
you require the resources that we used
in this session like the PPD code
demonstrated over the data sets used
then do let us know in the comment
section Below on our team of experts
will be more than happy to resolve only
queries and the earliest until next time
thank you stay safe and keep learning
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign