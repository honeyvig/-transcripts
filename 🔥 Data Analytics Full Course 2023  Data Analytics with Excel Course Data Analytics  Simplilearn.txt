foreign
data analytics is one of the hottest job
profiles in the industry now the
potential of data analytics is tapped in
new possibilities and lucrative carrier
opportunities in the current industry
the average salary of a data analyst in
India ranges from 25 to 30 lakhs per
annum based on experience and in the
United States of America the salary of a
data analyst can be as high as 250 000
per annum now the current evaluation of
data analytics Technologies 31 billion
dollars and it is projected to rise all
the way above to 40 plus billion dollars
with an estimated CHR growth of 41
percent by the year 2030. the fact
through data analytics is one of the
most promising careers in the current
industry today we present you the data
analytics pull course we will begin the
fundamentals of data analytics and walk
you through the advanced data analytics
and tools used in this process we will
be briefing you with a detailed
introduction to data analytics that
covers the sources of data the tools
involved and the procedures post that we
will dive deep into understanding the
fundamental differences between data
scientists and data engineers and data
analysts as they all have similar roles
and responsibilities and proceeding with
the course we will teach you the data
analytics with python and R next we will
learn the time series analytics with
them and build data analytic projects on
the same which will help you put
together all your learnings up ahead we
will get into big data analytics
proceeding we will work with the data
visualization tools like power bi and
Tableau and build data analytics and
visualization projects on the scene
these projects will serve you as a
finest portfolios for your future
interviews speaking of interviews we
have got you covered along the most
frequently Asked data analytics
interview questions to help you crack
the toughest interviews but before we
begin if an aspiring data analyst
looking for online training and
certifications from prestigious
universities are in collaboration with
leading experts then search no more
similar's professional Third Field
program in data analytics from Purdue
University in collaboration with IBM
should be a right choice for more
details use the course Link in the
description box below below and with
that in mind over to our training expert
I will run you through the top 6 data
analytics job roles so before I dive
deep into the various job rules let's
quickly understand how important a
career in data analytics is and what the
future holds for Professionals in this
domain
let's take a look at the growth of data
so back in the early 2000s there was
relatively less data generated but with
a rapid rise in Technologies and with
the increase in the number of various
social media platforms and multinational
companies across the globe the
generation of data has increased by
Leaps and Bounds
did you know that according to the IDC
the total volume of data is expected to
reach 175 zettabytes in 2025
now that's a lot of data let's take a
look at how organizations leverage all
of this data
as you know there are zillions of
companies across the world these
companies generate loads of data on a
daily basis when I say data here it
simply refers to business information
customer Data customer feedback product
Innovations sales reports and profit
loss reports to name a few
companies utilize all of this data in a
wise way they use all of this
information to make crucial decisions
that can either hamper or boost their
businesses you might have heard of the
term data is the new oil well it
definitely is but only if organizations
analyze all the available data very well
then this oil is definitely valuable
and for that we have data analytics
organizations take the help of data
analytics to convert the available raw
data into meaningful insights
so what is data analytics technically
you can say it is a process wherein data
is collected from various sources then
cleaned which involves removing
irrelevant information and then finally
transformed into some meaningful
information that can be interpreted by
humans
various Technologies tools and
Frameworks are used in the analysis
process
as you might have heard of the term data
never sleeps well it surely doesn't
every millisecond some of the other data
is generated and this is a constant
process this process is only going to
increase in the near future with the
Advent of newer Technologies the data
analytics domain holds Paramount
importance in every sector
companies want to leverage on all the
generated big data and boost their
businesses
they need professionals who can play
with data and convert them into crucial
insights organizations are constantly on
the lookout for such candidates and this
opportunity will only increase as data
is only going to grow every second
so if you want to start your career in
this field or if you want to switch your
job role into a role in the data
analytics domain then we have a set of
job profiles that you can look at
we will look into six job roles in the
data analytics field and learn what each
job role is all about the
responsibilities of a professional
working in that particular role the
skills required to get that particular
job the average annual salary of a
professional working in that role and
finally the company is hiring for that
role so let's start off
first we have the job role of a data
analyst
a data analyst is a person who collects
processes and performs statistical
analysis of large data sets
every business generates and collects
data be it marketing research sales
figures Logistics or Transportation
costs a data analyst will take this data
and figure out a variety of measures
such as how to price new materials how
to reduce Transportation costs or how to
deal with issues that cost the company
money they deal with data handling data
modeling and Reporting
now talking about their responsibilities
data analysts recognize and understand
the organization's goal they collaborate
with different team members such as
programmers business analysts engineers
and data scientists to identify
opportunities for solving business
problems
data analysts write complex SQL queries
scripts and store procedures to gather
and extract information from multiple
databases
they filter and clean data using
different modern tools and techniques
and make it ready for analysis they also
perform data mining from primary and
secondary data sources
data analysts identify analyze and
interpret Trends in complex data sets
this is done using statistical tools
such as R and SAS
another key responsibility of a data
analyst is to create summary reports and
build various data visualizations for
decision making and presenting it to the
stakeholders
next let us discuss the important skills
that you need to know to become a data
analyst
firstly you should have a bachelor's
degree in computer science or
information technology a master's degree
in computer applications or statistics
is also preferable
you must have a good understanding of
programming languages like R python
JavaScript and also understand SQL
in addition to that it is beneficial if
you have hands-on experience with
statistical and data analytics tools
such as SAS Miner Microsoft Excel and
ssas
basic understanding of machine learning
and its algorithms would be an advantage
acquaint yourself with descriptive
predictive prescriptive and inferential
statistics
most importantly you need to have a good
working knowledge of various data
visualization software along with
presentation skills this will help you
pitch in your ideas and viewpoints to
the clients and stakeholders better
now talking about their salaries a data
analyst earns nearly 5 lakhs 23 000
rupees per annum in India while in the
United States they earn around 62 000
453 dollars per annum
let's now look at a few of the companies
hiring data analysts so as you can see
we have the American e-commerce joint
Amazon then we have Microsoft the
American online payments company PayPal
then we have Walmart Bloomberg and
Capital One so that was all about data
analyst
the next job role is of a business
analyst
business analysts help guide businesses
in improving products services and
software through data-driven solutions
they are responsible for Bridging the
Gap between I.T and business using data
analytics to evaluate processes
determine requirements and deliver
data-driven recommendations and reports
to Executives and stakeholders
business analysts are responsible for
creating new models that support
business decisions and come up with
initiatives and strategies to optimize
costs
now let us look at the various
responsibilities of a business analyst
business analysts have a good
understanding of the requirements for
business their vital role is to work in
accordance with relevant project
stakeholders to understand their
requirements and translate them into
details which the developers can
understand
they frequently interact with developers
and come up with a plan to design the
layout of a software application
they also run meetings with stakeholders
and other authorities they engage with
Business Leaders and users to understand
how data-driven changes to products
Services software and Hardware can
improve efficiencies and add value
they ensured that the project is running
smoothly as per the requirements and the
design planned through user acceptance
and validation testing
they make sure all the features are
being incorporated into the application
Bas rely on different software to write
documentation and design visualization
to explain all the findings it is
extremely critical for any ba to
effectively document the findings where
each requirement of the client is
mentioned in detail
now let us look at the skills required
for a ba
a bachelor's degree in the field of
science engineering or statistics or any
related domain will suffice
knowledge of programming languages such
as Python and Java is beneficial you
should be really good at writing complex
SQL queries and you should also have
knowledge of various business process
models
along with knowledge of programming
languages ideas about statistical
analysis and predictive modeling is
necessary
decision making strong analytical and
problem solving skills are necessary to
solve software and business issues you
also need to have excellent presentation
and communication skills both oral and
written
moving on to their salary a business
analyst is expected to earn around 7
lakh rupees per annum in India in the US
they earn nearly 68 000 346 dollars per
annum
iqia Dell Phillips Honeywell the famous
American messaging platform WhatsApp the
UK based company Ernest and Young a few
of the companies hiring for business
analysts
up next we have the job role of a
database administrator
a database administrator is a
specialized computer systems
administrator who maintains a successful
database environment by directing or
performing all related activities to
keep the organization's data secure
they are responsible for storing
organizing and retrieving data from
several databases and data warehouses
their top responsibility is to maintain
data Integrity this means that database
administrator will ensure that the data
is secure from unauthorized taxes
moving on to their responsibilities
a database administrator develops
designs and maintains a database to
ensure that the data in it is properly
stored organized and managed well
they maintain data Integrity by avoiding
unauthorized taxes and they keep
databases up to date
they run tests and modify the existing
databases to ensure that they operate
reliably they also inform end users of
changes in databases and train them to
utilize systems
they need to cooperate with programmers
data analysts and the IT staffs to
ensure smooth running and maintenance of
databases
database administrators are responsible
for taking system backups in case of
power outages and other disasters so
they should have an efficient Disaster
Recovery plan
now let's have a look at their skills
to become a database administrator you
should have a bachelor's degree in
computer science or information
technology
knowledge of programming languages such
as python Java and Scala is important
you need to carry at least three to five
years of experience in data management
you need to have an understanding of
different databases such as Oracle DB
mongodb MySQL server and postgresql also
they should have an idea about database
design and writing SQL queries
finally you need to have a good
understanding of operating systems such
as Windows Mac OS and Linux along with
storage Technologies
talking about their salary a database
administrator in India can earn up to 4
lakh 97 000 rupees per annum in the US
they earn around 78 000 per annum
let's have a look at the companies
hiring for database administrators
so as you see here we have bookmyshow
Oracle the American MNC Intel Amazon
Robert Half and the New York Times to
name a few
fourth in the list of job rules we have
data engineer a data engineer is someone
who's involved in preparing data for
analytical and operational uses a data
engineer transforms data into useful
format for analysis they build and test
scalable Big Data ecosystems for
businesses a data engineer is an
intermediary between a data analyst and
a data scientist
now let's jump into their
responsibilities
data Engineers develop test and maintain
architectures they are responsible for
managing optimizing and monitoring data
retrievals storage and distribution
throughout the organization
they discover opportunities for data
acquisition find Trends in data sets and
develop algorithms to help make raw data
more useful to the Enterprise
data engineers build large data
warehouses using ETL for storing and
retrieving data
they also recommend ways to improve data
quality and efficiency along with
building algorithms to help give easier
access to Raw data
data Engineers often work with big data
and submit their reports to data
scientists for analysis purpose they
need to recommend and sometimes
Implement ways to improve data
reliability efficiency and quality
moving on to the skills of a data
engineer
a data engineer should hold a bachelor's
degree in computer science or
information technology
they should have good hands-on
experience with python R and Java
also data Engineers should be well
versed with big data Technologies such
as Hadoop Apache spark Scala Cassandra
and mongodb
data warehousing and detail experience
are essential to this position along
with in-depth knowledge of SQL and other
database Solutions
basic knowledge of statistical analysis
will be an advantage along with idea
about operating systems
here is what a data engineer can earn so
in India a data engineer can earn up to
8 lakhs 85 000 rupees per annum while
they can earn around 103 000 a year in
the USA
we have capgemini shortest talk the
American provider of stock photography
Spotify Accenture genpact and Facebook
hiring data engineers
the next exciting job role is of a data
scientist
a data scientist is a professional who
uses statistical methods data analysis
techniques machine learning and related
Concepts in order to understand and
analyze data to draw business
conclusions they make sense to messy and
unstructured data and bring value out of
it they employ techniques and theories
drawn from many fields within the
context of mathematics statistics
computer science and information science
a data scientist understands the
challenges in business and comes up with
the best Solutions using modern tools
and techniques to analyze visualize and
build prediction models to make business
decisions
let us now look at their
responsibilities in the industries
data scientists clean process and
manipulate data using several data
analytics tools they perform ad hoc data
mining collect large sets of structured
and unstructured data from disparate
sources
they design and evaluate Advanced
statistical models to work on Big Data
they also create automated anomaly
detection systems and keep constant
track of their performance
data scientists interpret the analysis
of big data to discover Solutions and
opportunities
a data scientist takes input from data
analysts and Engineers to formulate the
results
they use visualization packages and
tools to create reports and dashboards
for Relevant stakeholders they also
adopt new business models and approaches
apart from this they regularly built
predictor models and machine learning
algorithms
now moving on to the skills of a data
scientist
a bachelor's degree in computer science
or information technology will be fine
but a master's degree in the field of
data science will hold a major advantage
you also need to have a good experience
in the analytics domain
you should be proficient in programming
languages such as python Java and C plus
knowledge of Perl will also be an
advantage
familiarity with Apache Hive Pig and
Apache spark is necessary along with the
knowledge of Hadoop
in addition to knowing programming
languages you also need to know SQL
machine learning and deep learning
data visualization and Bs skills are
necessary for creating reports and
dashboards you should also be able to
communicate and present information and
ideas properly
now talking about their salary a data
scientist in India can expect an annual
salary of 10 lakhs 47 000 rupees per
year
meanwhile in the US they can earn up two
hundred and thirteen thousand dollars
per annum that's a lot of money
from the many companies hiring for data
scientists here we have a few companies
named payar yet again Amazon Citibank
Apple Google the Japanese electronic
Commerce and online retailing company
Rakuten and Facebook
and finally we have machine learning
engineer machine learning Engineers are
professionals who develop intelligent
machines that can learn from vast
amounts of data and apply knowledge
without human intervention
they use different algorithms and
statistical modeling to make sense of
data they design and develop machine
learning and deep learning algorithms
their main goal is to create
self-running software
let's have a look at the
responsibilities of a machine learning
engineer
machine learning Engineers research
design and develop machine Learning
Systems they use exceptional
mathematical skills in order to perform
faster computations and work with
algorithms to create sophisticated
models
they perform a b testing and use data
modeling to fine-tune the results
they use data modeling and evaluation
strategy to find hidden patterns and
predict unseen instances
machine learning Engineers work closely
with data Engineers to build data
pipelines and interact with stakeholders
to get a Clarity on the requirements
most importantly they analyze complex
data sets to verify data quality perform
model tests and experiments choose to
implement the right machine learning
algorithm and select the right training
data sets
moving on to their skills
a machine learning engineer should have
a degree in computer science and
information technology they should have
an advanced degree in computer science
or maths
in addition to this they should also
have experience in the same domain
they should be proficient in programming
languages such as python RC plus and
Java
knowledge of Statistics probability and
linear algebra is necessary as all the
machine learning algorithms have been
derived from mathematics also having an
idea of signal processing would be
beneficial
machine learning Engineers need to have
a good understanding of data
manipulation and machine learning
libraries such as numpy Panda
scikit-learn Etc
they should have good oral and written
communication skills
let us now have a look at their salary
structure
a machine learning engineer earns 8 lakh
rupees per annum in India while in the
US they can earn around 114 000 a year
now that's a whopping amount isn't it
let's have a look at the company's
hiring machine learning engineers
so as you see we have Amazon Microsoft
Oracle Salesforce Rapido and Accenture
to name a few
that was all about the job role of a
machine learning engineer
now that we have seen the different job
roles in the field of data analytics
let's also go ahead and see how an ideal
resume of a data analyst should look
like
seen on your screens is a sample
resuming of a data analyst you can grab
some ideas from this and incorporate
them in your resume
nowadays it's quite common to have a
professional photograph of yours on the
resume you can go ahead and have that
then your name in bold followed by your
contact details like email ID and phone
number
then moving on you would have to write a
summary briefly explain your current job
role and what you're looking for in the
future having a LinkedIn profile link
works well these days employers can just
go ahead and look at your profile and
gauge you well
make sure to have an active LinkedIn
profile
in addition to LinkedIn profile it's
also good to have a GitHub profile link
which can show your coding or other
technical skills
if it's impressive enough then a lot of
times the rest of your resume is just
secondary
as I mentioned this is a resume of a
data analyst so as you can see in the
summary here we have just spoken about
the basic responsibilities of a data
analyst
moving on to the experience part you
have to write the job title and below
that you can mention the company and the
tenure accordingly here you would have
to give a brief description of
achievements in the organization any
relevant accomplishments related to the
job you're applying for the tools and
the various Technologies you have worked
with
so in the sample you can see we have
spoken about data visualization using R
in Tableau next we have spoken about how
the candidate has worked with other
teams for a Better Business outcome
most of the data analysts use SQL and
Excel to handle data for reporting and
database maintenance and we have
mentioned that here as well
do make sure that you always specify the
tools you use
then you can also mention if you have
worked on improving data delivery for
example here we have spoken about
developing and optimizing SQL queries
data aggregations and ETL to improve
data delivery
finally you can speak a bit about your
reporting skills and if needed elaborate
on it
usually professionals would have worked
in a similar domain before becoming a
data analyst here we have taken the role
of a statistical assistant as the first
job since it's easier for a candidate
with this job role to shift into the
data analytics field nevertheless you
all can still mention your prior
experience here be it in any domain
under the responsibilities for this job
role we have given Basics such as coding
data prior to compute entry compiling
statistics from various reports
Computing and analyzing data and find me
some visualization and Reporting
moving to the education
here you can mention the name of your
degree and the university name if you
have a post graduation well in good you
can list both the degrees here also if
you have any certifications you can
mention them here under the education
category
now moving to the skills depending on
your skills and your choice you can
either shift this part to the beginning
of the resume or have it here
as you see on your screens this is just
a different way of displaying your skill
sets you can have all the five stars
colored if you are excellent in that
particular tool or language
as you see it's crystal clear as to what
the candidates strong areas are
you can have various categories like
shown
for example under software development
you can list the languages that you know
and how proficient you are in those
particular languages
it's clear that the candidate knows
python better than JavaScript here so
the employer gets a clear idea about the
skills you possess and the depth of it
similarly you can mention the databases
as well the few mentioned here are more
or less a requirement to become a data
analyst at least SQL is a must
not to forget data visualization is also
very important when it comes to the job
role of a data analyst mention the tools
you know here and similarly give
yourself a rating out of five five stars
shaded being the highest
here we have mentioned Tableau and Excel
which are more than sufficient to become
a data analyst
moving to the non-technical skills you
can mention the languages you know here
here we have taken English and German in
addition to the languages you can also
feel free to mention the extracurricular
activities that you are good at
so this is how an ideal resume of a data
analyst should look like you can alter
it according to your achievements skills
and experience
in a world where there is data
generation every millisecond the role of
a data analyst holds Paramount
importance this video will help you
understand what a data analyst does and
the various skills required to back this
position
before understanding the job role of a
data analyst let's understand the
meaning of the term data analytics
so what does the term analyze mean it
merely means to scrutinize something to
derive meaningful conclusions from it
well data analytics also works similarly
it is the process by which useful
insights are extracted from raw data by
studying and examining it carefully
these insights can be related to
business information market trends
product Innovations and profit loss
report to name a few
here's an interesting comparison I'm
sure all of you have played with jigsaw
puzzles at some point in time for that
first you would have to gather all the
pieces together and then fit them
accordingly to bring out a beautiful
picture isn't it we can simply relate
the process of data analytics to how you
make a jigsaw puzzle
as you can see here data refers to the
raw data which can be structured
semi-structured or unstructured in
nature the process of data analytics
incorporates collecting data from
various sources cleaning it and then
finally transforming it into something
meaningful which can be interpreted by
humans
this information can be visually
presented in the form of graphs and
charts which provide precise results of
the analysis
various Technologies tools and
Frameworks are used in the analysis
process organizations take the help of
data analytics to convert the available
raw data into meaningful insights hence
there is a high requirement for
professionals who can play with data and
help organizations with crucial decision
making
there are many job roles in the field of
data analytics if you have watched our
previous video on data analytics Courier
you would have seen a few of these roles
out of all the job roles an important
role is that of a data analyst an
interesting thing about this job role is
that it can be taken up by freshers as
well it can embark on your career in the
field of data analytics it is a
lucrative career as the field of data
analytics is only going to continue to
Blossom in the years to come
so let's see who exactly a data analyst
is
a data analyst is a person who collects
processes and performs analysis on large
data sets here the statistical analysis
is done on various data sets every
business generates and collects data be
it marketing research sales figures
customer feedback Logistics or
Transportation costs a data analyst will
take all of this data and figure out
various measures such as how to price
new materials how to reduce
Transportation costs how to provide
better customer experience or how to
deal with issues that cost the company
money
data analysts also deal with data
handling data modeling and data
reporting
a data analyst has a number of Duties to
perform let's have a look at their
responsibilities now
first and foremost a data analyst is
required to recognize and understand the
organization's goal this helps in
streamlining and planning the analysis
process accordingly data analysts assess
the available resources understand the
business problem and gather the right
data this step is done by collaborating
with different team members such as
programmers business analysts and data
scientists
data analysts need to use queries to
gather information from a database they
write complex SQL queries and scripts to
gather and extract information from
several databases and data warehouses
they are responsible for data mining as
well here data is mined from various
sources and then organized in order to
obtain a new information from it this is
a vital role of a data analyst as they
have to extract data from various
sources in order to work on it with this
data they can build models it can reduce
a complexity and increase the efficiency
of the whole system
another crucial step in data analysis is
data cleaning and data wrangling usually
the data you can collect is often messy
and has a lot of missing values so it's
important to clean this data to make it
ready for analysis
data analysts use a number of
statistical and analytical tools
including programming languages for
performing analysis and logical
examination of data
using different libraries and packages
data analysts discover Trends and
patterns from complex data sets this
will help them find more unseen insights
from the data to make business
predictions
another important role of a data analyst
is to prepare summary reports for the
leadership team so that they can make
timely decisions for this data analysts
use multiple data visualization tools
some of these tools are discussed as
part of skills required which we will
see later
finally data analysts interact with the
development team business and management
team as well as with data scientists to
ensure proper implementation of business
requirements and to figure out
opportunities for better process
Improvement
now let us look at the various skills
required to become a data analyst
so the first skill is more of a
prerequisite you should hold a degree in
any relevant field be it engineering
computer science Information Technology
electrical and mechanical engineering
you can also be a graduate in statistics
or economics also you should have domain
knowledge in the field you are currently
working in or the role you're applying
for
the next important skill is that you
should have good hands-on experience
with programming languages such as R
Python and JavaScript this would help
you write programs to solve complex
problems
then you should have a good experience
working with databases and data analysis
tools such as writing SQL queries and
procedures knowledge of Microsoft Excel
IBM SPSS and Matlab to analyze Trends
forecast data and plan to drive accurate
insights
you must have a strong understanding of
statistics and machine learning
algorithms these include Concepts such
as hypothesis testing probability
distributions regression analysis and
various classification and clustering
techniques
and finally a data analyst should be
able to create different reports with
the help of charts and graphs using
several data visualization tools such as
Tableau and power bi they must have good
presentation skills as well this will
help them convey their ideas to clients
and stakeholders better
now that we have looked at the various
skills required to become a data analyst
let's now see the average annual salary
that a data analyst earns
here we can have a look at the salary
ranges of both in U.S and in India
so a data analyst in the United States
can earn a minimum salary of forty three
thousand dollars to a maximum of 85 000
dollars per year
in India you can earn anywhere between 1
lakh 98 000 Rupees to 9 lakh 24 000
rupees per annum
the data analyst role is in very high
demand with companies looking for
professionals who can handle their data
effectively and efficiently so let's
look at the different companies hiring
for the data analyst role
as you see here we have the American
e-commerce giant Amazon the American
multinational technology company
Microsoft Capital One which is one of
the largest banking companies in the US
then we have the popular retail company
Walmart then we have PayPal next we have
the internet and search engine giant
Google social media firms Facebook and
Twitter as well as apple and Bloomberg
with that let me now tell you how
simplylearn can help you learn data
analytics and guide you to become a data
analyst
so in a new tab I'll search for
simplylearn.com
then here on the search bar I look for
data analyst
let me now click on the first link which
is data analyst I'll open this in
another tab
as you can see on your screens this is
the data analyst Masters program and it
is in collaboration with IBM
on the right hand side you can see the
different courses that will be covered
as a part of the program you will learn
introduction to data analytics business
analytics with Excel then you have
Tableau followed by power bi later on in
the course you will learn programming
Basics and data analytics with python
than our programming and finally you
will get to work on a Capstone project
this is a kind of certificate you would
receive after completing the course it
will have your name along with IBM and
simply learn logo
these are some of the tools that will be
covered in this program you will learn
Excel then numpy Panda scipy IBM Watson
power bi Tableau Python and r
the course advisor for this program is
Ronald Van Loon below you can see the
entire course curriculum and the
different courses that you will learn in
this program
also there are a few electives that you
can choose in this course
there's data science in real life
programming refresher industry master
class data analytics and parasql
training as well
in today's Digital World data is being
generated by companies and individuals
every second so the role of a data
analyst holds supreme importance
so if you're looking for a career in
data analytics this video will help you
learn what a data analyst does and the
various skills you need to possess to
become a data analyst in 2022
before we get started make sure you
subscribe to the simply learned Channel
and hit the Bell icon to never miss an
update from us
let's look at the agenda for this video
first we will understand who a data
analyst is then we will understand the
top 10 data analyst skills for 2022
moving on we will look at the salary of
a data analyst and finally we will look
at the company's hiring data analysts
so now let's understand who is the data
analyst
a data analyst is a professional who
collects business data from various
sources interprets it and uses various
statistical tools and techniques to
extract insights and useful information
from it
they acquire data from primary or
secondary data sources and maintain
databases they also recognize and
understand the organization's goal and
collaborate with different team members
such as programmers business analysts
and data scientists to build an
effective solution to a business problem
now with this basic understanding of who
a data analyst is let's learn the top 10
data analyst skills for 2022.
at number one we have structured query
language or SQL
SQL is a top skill that every data
analyst should have data analysts use
SQL commands and functions to store
process analyze and manipulate
structured data using relational and
nosql databases
they also build data models and write
complex SQL queries and scripts to
gather and extract information from
several databases and data warehouses
some of the popular databases a data
analysts should be familiar with are
Microsoft SQL Server MySQL postgresql
and IBM db2
the second important skill for a data
analyst is Microsoft Excel
Microsoft Excel is one of the most
popular and oldest spreadsheet
applications for creating reports
performing calculations and analyzing
data data analysts need to know how to
handle tabular data in Excel so they
should be aware of features like sorting
filtering conditional formatting pivot
tables what-if analysis and functions
such as sumifs and countifs
the third crucial data analyst skill for
2022 is data cleaning and wrangling
usually the data collected by analysts
from various heterogeneous sources is
often messy and contains a lot of
missing values
so it is always crucial to clean the
data and remove noise missing or
erroneous elements
it is also important to format data
using tools and methods before using it
for analysis
they are responsible for data mining as
well
the data mined from various sources are
organized in order to obtain new
information from it some of the tools
you need to know for data cleaning and
wrangling are Excel power query and open
refine
the fourth skill on our list is
mathematics and statistics
data analysts often work on data for
higher Dimensions that are greater than
three in order to interpret such data
they need to be good at linear algebra
and calculus they also build predictive
models and statistical models such as
linear regression logistic regression
knife base and k-means clustering in
order to understand the working of these
algorithms they must have knowledge
about statistics and probability
coming to the fifth important skill for
a data analyst in 2022 we have
programming
data analysts need to master at least
one programming language preferably
python or Raj in order to work with
complex business problems analysts need
to write scripts and user-defined
functions to automate tedious tasks
Python and R language provide a
collection of different libraries and
packages such as numpy pandas deeply
matplotlib ggplot which data analysts
can use to discover Trends and patterns
from complex data sets
after this we have data visualization as
our sixth skill another data analyst job
role is to visualize large volumes of
data and prepare summary reports and
dashboards for the leadership team and
clients so that they can make timely
business decisions
to do this data analysts use various
data visualization tools such as power
bi Tableau and click View
using these tools data analysts can
integrate various data sets apply joint
conditions sort and filter data as well
create different visualizations using
charts and graphs
7th skill for a data analyst is industry
knowledge
data analysts should have good knowledge
and understanding of the industry or
domain they are working in for example
if you're working in a healthcare domain
you need to know how Healthcare
analytics can be applied to improve
patient care you should have knowledge
about the challenges faced in healthcare
and how you can leverage data and
analytics to solve the issues only if
you have strong industry knowledge can
you try to improve the business
the eighth skill that is important for a
data analyst in 2022 is problem solving
a business deals with several problems
on a daily basis data analysts should be
ready to face those challenges data
analysts are expected to use their
problem solving skills work with the
team troubleshoot what went wrong and
provide an effective solution via data
analysis
a data analyst with good problem solving
skills can help a business identify
current and potential issues and
determine a viable solution based on the
data it collects
the ninth skill on our list is
analytical thinking
data analysts need analytical thinking
ability to break down a complex problem
into simple components and resolve these
components one by one
it is a must-have skill for data
analysts
analytical thinking includes designing
the parameters that need to be
considered for defining data sets
analyzing them from different
perspectives and determining variable
dependencies
coming to the 10th skill among the top
10 skills for a data analyst in 2022 we
have communication
data analysts don't just interact with
computers and programs they also
interact with team members stakeholders
and data suppliers
so good communication skills are
essential
data analysts also present their
findings in front of an audience who
might not be familiar with the
analytical methods and processes so they
need to clearly translate their findings
and insights into non-technical terms
so those were the top 10 skills a data
analyst needs to possess in 2022 do you
think we missed out on any skills then
please put your answers in the comment
section below
now let's look at the salary of a data
analyst
according to Glassdoor the average
annual salary for a data analyst in the
United States is 69
517 dollars while in India you can earn
nearly 7 lakh rupees per random
finally let's look at the top companies
that are hiring data analysts in 2022
here we have the consultancy and big4
giant Deloitte and the pharmaceutical
company Cerner Corporation then we have
the tech giant IBM retail company
Walmart and the e-commerce leader Amazon
before I start off with the top 10 data
analysis tools I'd like to talk a bit
about data analysis
so have you ever wondered why data
analysis is important
there are zillions of companies across
the world all these companies generate a
lot of data they literally work with
this generated data
these companies depend on data to make
crucial decisions which can impact their
businesses
data in its raw format has to be
converted into meaningful information
which can then be used by organizations
this is done by analyzing the generated
data and for this we have data analysis
so what is data analysis
data analysis is not just a single step
but a set of processes it is the process
of collecting data then cleaning it when
I say cleaning it simply means removing
the air 11 data and then this data is
transformed into meaningful information
we can simply relate this process to how
you make a jigsaw puzzle just like how
you gather all the pieces together and
fit them accordingly to bring out a
beautiful picture
data analysis also works on almost the
same grounds
to achieve the goals of data analysis we
use a number of data analysis tools
companies rely on these tools to gather
and transform their data into meaningful
insights so which tool should you choose
to analyze your data which tool should
you learn if you want to make a career
in this field we will answer that in
this session after extensive research we
have come up with these top 10 data
analysis tools here we will look at the
features of each of these tools and the
companies using them so let's start off
at number 10 we have Microsoft Excel all
of us would have used Microsoft Excel at
some point right it is easy to use and
one of the best tools for data analysis
developed by Microsoft Excel is
basically a spreadsheet program using
Excel you can create grids of numbers
text and formulas it is one of the
widely used tools be it in a small or
flat setup
the interface of Microsoft Excel looks
like this
let's now move on to the features of
excel
firstly Excel works with almost every
other piece of software in office we can
easily add Excel spreadsheets to Word
documents and PowerPoint presentations
to create more visually appealing
reports or presentations
the windows version of excel supports
programming through Microsoft's Visual
Basic for applications VBA
programming with VBA allows spreadsheet
manipulation that is difficult with
standard spreadsheet techniques
in addition to this the user can
automate tasks such as formatting or
date organization in VBA
one of the biggest benefits of excel is
its ability to organize large amounts of
data into orderly logical spreadsheets
and charts by doing so it's a lot easier
to analyze data especially while
creating graphs and other visual data
representations the visualization can be
generated from specified group of cells
those were few of the features of
Microsoft Excel
let's now have a look at the companies
using it most of the organizations today
use Excel few of them that use it for
analysis are the UK based company Ernest
and Young then we have Urban Pro Wipro
and Amazon
moving on to our next data analysis tool
at number nine we have rapidminer
a data science software platform
rapidminer provides an integrated
environment for data preparation
analysis machine learning and deep
learning
it is used in almost every business and
Commercial sector rapidminer also
supports all the steps of the machine
learning process
seen on your screens is the interface or
rapidminer
moving on to the features of rapidminer
firstly it offers the ability to drag
and drop it is very convenient to just
drag drop some columns as you are
exploring a data set and working on some
analysis
rapidminer allows the usage of any data
and it also gives an opportunity to
create models which are used as a basis
for decision making and formulation of
strategies
it has data exploration features such as
graphs descriptive statistics and
visualization which allows users to get
valuable insights
it also has more than 1500 operators for
every data transformation and Analysis
task
let's now have a look at the companies
using rapidminer we have the Caribbean
Airline Leeward Islands Air transport
next we have the United Health Group the
American online payment company PayPal
and the Austrian Telecom company
MobileComm so that was all about trap in
minor now let's see which tool we have
at number eight
we have talent at number eight
Talent is an open source software
platform which offers data integration
and management it specializes in Big
Data integration Talent is available
both in open source and premium versions
it is one of the best tools for cloud
computing and Big Data integration
the interface of talent is as seen on
your screens
moving on to the features of talent
firstly automation is one of the great
Boon's Talent offers it even maintains
the tasks for the users this helps with
quick deployment and development
it also offers open source tools Talent
lets you download these tools for free
the development costs reduce
significantly as the process is
gradually speed up
Talent provides a unified platform it
allows you to integrate with many
databases SAS and other Technologies
with the help of the data integration
platform you can build flat files
relational databases and Cloud apps 10
times faster
those were the features of Talon the
companies using Talent are Air France
L'Oreal cab Gemini and the American
multinational Pisa restaurant chain
Domino's
next on the list at 7 we have nine
Constance information Miner on nime is a
free and open source data analytics
reporting and integration platform
it can integrate various components for
machine learning and data mining through
its modular data pipelining concept nime
has been used in pharmaceutical research
and other areas like CRM customer data
analysis business intelligence text
Mining and financial data analysis
here is how the interface of nime
application looks like
now coming to the nine features
9 provides an interactive graphical user
interface to create visual workflows
using the drag and drop feature
use of jdbc allows assembly of nodes
blending different data sources
including pre-processing such as ETL
that is extraction transformation
loading for modeling data analysis and
visualization with minimal programming
it supports multi-threaded in-memory
data processing 9 allows users to
visually create data flows selectively
execute some or all analysis steps and
later inspect the results models and
interactive views
9 server automates workflow execution
and supports team-based collaboration
nime integrates various other open
source projects such as machine learning
algorithms from Becca H2O carers Park
and our project
9 allows analysis of 300 million custom
addresses 20 million cell images and 10
million molecular structures
some of the companies hiring for nime
are United Health Group asml fractal
analytics atos and LEGO Group let's now
move on to the next tool we have SAS at
number six
SAS facilitates analysis reporting and
predictive modeling with the help of
powerful visualizations and dashboards
in SAS data is extracted and categorized
which helps in identifying and analyzing
data patterns
as you can see on your screens this is
how the interface looks like
moving on to the features of SAS
using SAS better analysis of data is
achieved by using automatic code
generation as a SQL
SAS allows you to access through
Microsoft Office by letting you create
reports using it and by Distributing
them through it
SAS helps with an easy understanding of
complex data and allows you to create
interactive dashboards and reports
let's now have a look at the companies
using SAS we have companies like genpact
iqria Accenture and IBM to name a few
that was all about SAS
so for all those who joined in late let
me just quickly repeat our list at
number 10 we have Microsoft Excel then
at number nine we have rapidminer at
number eight we have talent at number
seven we have nine and at number six we
have SAS so far you'll agree with this
list let us know in the comment section
below let's now move on to the next five
Tools in our list
so at number five we have both R and
python yes we have two of them in the
fifth position
R is a programming language which is
used for analysis as well it has
traditionally been used in academics and
research python is a high level
programming language which has a python
data analysis Library it is used for
everything starting from importing data
from Excel spreadsheets to processing
them for analysis
this is the interface of r
next up is the interface of the Python
Jupiter notebook
let's now move on to the features of
both R and python
when it comes to the availability of R
and python it is very easy both are in
Python are completely free hence it can
be used without any license
I used to compute everything in memory
and hence the computations were limited
but now it has changed both are in
Python have options for parallel
computations and good data handling
capabilities
as mentioned earlier as both R and
python are open in nature all the latest
features are available without any delay
moving on to the companies using R we
have Uber Google Facebook to name a few
python is used by many companies again
to name a few we have Amazon Google and
the American photo and video sharing
social networking service Instagram
that was all about R in Python
at number four we have Apache Spark
Apache spark is an open source engine
developed specifically for handling
large-scale data processing and
Analytics
spark offers the ability to access data
in a variety of sources including Hadoop
distributed file system htfs openstack
Swift Amazon S3 and Cassandra
it allows you to store and process data
in real time across various clusters of
computers using simple programming
constructs
Apache spark is designed to accelerate
analytics on Hadoop while providing a
complete Suite of complementary tools
that include a fully featured machine
learning library a graph processing
engine and stream processing
so this is how the interface of Apache
spark looks like
now let's look at the important features
of Apache Spark
Sparks stores data in the ram hence it
can access the data quickly and
accelerate the speed of analytics spark
helps to run an application in a Hadoop
cluster up to 100 times faster in memory
and 10 times faster when running on disk
it supports multiple languages and
allows the developers to write
applications in Java Scala r or python
spark comes up with 80 high-level
operators for interactive querying spark
code for batch processing joint stream
against historical data or run ad hoc
queries on stream state
analytics can be performed better as
spark has a rich set of SQL queries
machine learning algorithms complex
analytics Etc Apache spark provides
fault tolerance through spark rdd spark
resilient distributed data sets are
designed to handle the failure of any
worker node in the cluster thus it
ensures that the loss of data reduces to
zero
Lockheed Martin and eBay are some of the
companies that use Apache spark on a
daily basis
at number 3 we have another important
growing data analysis tool that is Click
View
click view software is a product of
Click for business intelligence and data
visualization click view is a business
Discovery platform that provides
self-service bi for all business users
and organizations
with click view you can analyze data and
use your data discoveries to support
decision making
clickview is a leading business
intelligence and analytics platform in
Gartner magic quadrant
on the screen you can see how the
interface of Click view looks like
now talking about its features
clickview provides interactive guided
analytics with in-memory storage
technology during the process of data
Discovery and interpretation of
collected data the clickview software
helps the user by suggesting possible
interpretations
clickview uses a new patent in memory
architecture for data storage all the
data from the different sources is
loaded in the ram of the system and it
is ready to be retrieved from there
it has the capability of efficient
social and mobile data discovery
social data Discovery offers to share
individual Data Insights within groups
or out of it
a user can add annotations as an
addition to someone else's insights on a
particular data report click view
supports mobile data Discovery within an
HTML5 enabled touch feature which lets
the user search the data and conduct
data Discovery interactively and explore
other server-based applications
clickview performs olap and ETL features
to perform analytical operations extract
data from multiple sources transform it
for usage and load it to a data
warehouse
the companies that can help you start
your career and click view are
Mercedes-Benz cab Gemini Citibank
cognizant and Accenture to name a few
at number 2 we have power bi
power bi is a business analytics
solution that lets you visualize your
data and share insights across your
organization or embed them in your app a
website
it can connect to hundreds of data
sources and bring your data to life with
live dashboards and reports
power bi is the collective name for a
combination of cloud-based apps and
services that help organizations collate
manage and analyze data from a variety
of sources through a user-friendly
interface
power bi is built on the foundation of
Microsoft Excel and has several
components such as Windows desktop
application called Power bi desktop and
online software resist service called
Power bi service mobile power bi apps
available on Windows phones and tablets
as well as for IOS and Android devices
here is how the power bi interface looks
like as you can see there is a visually
interactive sales report with different
charts and graphs
moving on to the features of power bi
it has an easy drag and drop
functionality with features that make
data visually appealing you can create
reports without having the knowledge of
any programming language power bi helps
users see not only what's happened in
the past and what's happening in the
present but also what might happen in
the future
it offers a wide range of detailed and
attractive visualizations to create
reports and dashboards you can select
several charts and graphs from the
visualization pane power bi has machine
learning capabilities with which it can
spot patterns in data and use those
patterns to make informed predictions
and run what-if scenarios
power bi supports multiple data sources
such as Excel Tech CSV Oracle SQL Server
PDF and XML files the platform
integrates with other popular business
management tools like SharePoint Office
365 and Dynamics 365 as well as other
non-microsoft products like spark Hadoop
Google analytics sap Salesforce and
MailChimp
some of the companies using power bi are
Adobe AXA Carlsberg capgemini and Nestle
moving on to the next tool so any
guesses as to what we have at number one
you can comment in the chat section
below
finally on the top of the pyramid we
have tableau
Gartner's magic quadrant of 2020
classified Tableau as a leader in
business intelligence and data analysis
Tableau interactive data visualization
software company was founded in Jan 2003
in Mountain View California
Tableau is a data visualization software
that is used for data science and
business intelligence it can create a
wide range of different visualization to
interactively present the data and
showcase insights
the important products of Tableau are
Tableau desktop Tableau public Tableau
server Tableau online and Tableau reader
this is how the interface of Tableau
desktop looks like
now coming to the features of tableau
data analysis is very fast with Tableau
and the visualizations created are in
the form of dashboards and worksheets
Tableau delivers interactive dashboards
that support insights on the Fly
it can translate queries to
visualizations and import all ranges and
sizes of data writing simple SQL queries
can help join multiple data sets and
then build reports out of it you can
create transparent filters parameters
and highlighters
Tableau allows you to ask questions spot
Trends and identify opportunities with
the help of Tableau online you can
connect with Cloud databases Amazon
redshift and Google bigquery
the company is using Tableau are
Deloitte Adobe Cisco LinkedIn and the
American e-commerce giant Amazon to name
a few
and there you go those are the top 10
data analysis tools
let's now have a question and answer
session please feel free to post your
queries in the comments section and
we'll respond in the chat
before the question answer session let's
recap quickly in the meanwhile you all
can post your questions in the comment
section below
so at number 10 we have Microsoft Excel
then at number nine we have rapidminer
at number eight we have talent at number
seven we have nine at number six we have
SAS R and python at number five Apache
spark at number four
click View at number three power bi at
number two and finally we have Tableau
topping the list at number one we'll
understand the differences between a
business analyst and a data analyst in
terms of their job description the
responsibilities for each of these roles
and the various skills they possess
you will get an idea about the salary
structure of a business analyst and that
of a data analyst and we will also look
at the companies hiring for these roles
finally towards the end of this video
I'll tell you how simply learn can help
you with your career as a business
analyst and as a data analyst
first let's understand the job
description of a business analyst
business analyst is a professional who
Bridges a gap between the it and the
business teams in an organization they
use data analytics and modern
Technologies to assess processes and
deliver data-driven Solutions
they understand and solve a business
problem and validate business
requirements a business analyst
generates reports for executives and
stakeholders
they're part of the business operation
and work closely with the technology
team to improve the quality of the
services being delivered they also
assist in the integration and testing of
new Solutions now let's talk about the
job description of a data analyst
with a rapid increase in data generation
today the term data analyst has found
its prominence
a data analyst collects processes and
performs analysis of large data sets
every business generates data in several
formats this data can be in the form of
customer information and feedback log
files transaction data marketing
research and so on it is the duty of a
data analyst to transform these business
data into valuable insights some of the
problems that can be addressed are how
to improve a business how to provide
good customer experience what would be
the ideal price for a new product how to
reduce Transportation costs and so on
data analysts deal with data handling
data modeling and Reporting with this
brief understanding of the job
description for a business analyst and a
data analyst let's now shift our Focus
towards the various responsibilities of
a business analyst
a business analyst identifies the
business goals understands the problems
faced by an organization and comes up
with a cost-effective solution to tackle
the issues they thoroughly understand
the requirements from the clients and
assign the right resources
Bas communicate and work closely with
the development team to design the
solution for a problem they ensure that
the development team doesn't spend their
time understanding the stakeholders
requirements and often give iterative
feedback on the solution being developed
they check and validate if the project
is running fine with the help of user
acceptance testing they also verify if
the solution being worked on is in line
with the requirements and ensure that
the final product satisfies the user
expectations Bas access the functional
and non-functional requirements a
business analyst documents the project
findings and results they present the
project conclusions to the stakeholders
and clients along with delivering
maintenance reports and building
visualizations to make decisions now
let's take a look at the
responsibilities of a data analyst first
and foremost a data analyst must
identify and understand the
organization's goal and requirements
this helps to plan and streamline the
analysis process data analysts collect
data from various heterogeneous sources
they assess the available resources
comprehend the business problem and
gather the right data for analysis they
work closely with different team members
like programmers business analysts and
data scientists data filtering and data
wrangling are vital jobs of a data
analyst the data collected is often
noisy and it contains missing values
hence it is crucial to clean the
collected data and remove invalid values
to make it ready for analysis they use a
variety of analytical statistical and
business intelligence tools to spot
Trends and patterns in complex data sets
discover hidden insights and prepare
summary reports for the leadership team
they also use programming languages for
data mining and data manipulation
now it's time for us to understand the
difference between a business analyst
and a data analyst based on the skill
set they possess
first let's look at the skills that can
help you become a ba a business analyst
should have a graduation degree in any
relevant field such as business
accounting Information Systems human
resources or engineering you can apply
for entry-level business analyst
positions or with professional
experience Excel is a powerful analytics
and reporting tool for working with data
Bas use Excel to perform various
calculations data analysis plan and
editorial calendar and calculate
customer discounts to derive meaningful
insights and take decisions
BS use SQL to retrieve manipulate and
analyze data stored in relational
databases critical thinking skills are
important to understand customers
business needs it allows them to
distinguish between requirements that
add value to the business and those that
should be given a lower priority Bas
should find different ways to address
each challenge data visualization is a
key skill for Bas to build interactive
dashboards and reports to convey the
outcomes of a project
knowledge of Tableau power bi and click
view is required to make different types
of reports depending on the business
requirements
business analysts should have a good
Hands-On programming experience to solve
complex tasks and perform faster
analysis of data
hence knowledge of programming languages
such as R and python is a prerequisite
finally they should have good
presentation skills they should also be
confident about their findings and
conclusions and communicated in front of
the stakeholders and clients let's Now
understand the skills that a data
analyst should possess you must have a
bachelor's degree in any relevant field
or be a graduate in statistics economics
or science you're eligible to become a
data analyst being a fresher or as an
experienced professional you should have
domain knowledge in the field you are
working in once again knowledge of excel
is another basic requirement for a data
analyst data analysts often work with
structured data so they should be
proficient in writing SQL queries using
data manipulation and data definition
commands they should know how to create
stored procedures
another crucial skill for a data analyst
is to have hands-on experience with
programming languages such as python R
SAS and JavaScript you can analyze and
visualize large data sets and create
predictive models for making business
decisions data analysts create data
visualizations using libraries such as
matplotlib c-born ggplot and plotly this
helps them to perform exploratory data
analysis
knowledge of Tableau and power bi is
required to create different business
reports with the help of graphs and
charts data analysts should have
knowledge of machine learning algorithms
to build sophisticated models and make
future predictions
so they should know about linear
regression logistic regression support
Vector machines k-mean clustering and
other supervised and unsupervised
learning algorithms finally data
analysts should also possess good
communication and presentation skills
now let's discuss a salary structure for
both of these job roles
according to pay scale a business
analyst in the United States earns an
average salary of sixty nine thousand
dollars while in India you can earn
nearly 6 lakh rupees per annum now
talking about the salary of a data
analyst according to pay scale in the US
a data analyst earns an average salary
of sixty thousand seven hundred and ten
dollars per annum and in India you can
earn around 4 lakhs 24 000 rupees per
annum let's now move on and look at the
different companies hiring for business
analyst roles here we have Oracle the
search engine giant Google the American
MNC cognizant and e-commerce company
Amazon in addition to that we have
Ernestine young technology giant IBM
Dell and Cisco hiring business analysts
talking about the companies hiring for
data analysts we have Twitter Google the
social media leader Facebook and Amazon
we also have the American Oil Company
shell the electric vehicle company Tesla
apple and the American Credit Reporting
Agency Equifax
now choosing the right field that is to
become a business analyst or a data
analyst could be a challenging task
the key points that you have to keep in
mind before making a decision is
first review your background and see
what qualifications you have check what
skills you possess and the domain
knowledge you have then gauge your
interest to see what suits you best
and finally consider your long-term
goals and see the job roles that will
help you grow in your career in the long
run now let me tell you how simply learn
can help you grow your career as a
business analyst and a data analyst
simplylearn offers a postgraduate
program in business analysis that is in
collaboration with Purdue University the
endorsed education provider is iiba some
of the skills that will be covered in
this course are strategy analysis
wireframing solution evaluation
dashboarding data visualization agile
scrum methodology scrum artifacts
statistical analysis using Excel and SQL
database
some of the tools covered in this course
are Microsoft Excel Tableau power bi
jira postgresql plan box and others some
of the key features of this business
analysis program are you will receive
Purdue post graduate program
certification master classes from Purdue
faculty you can enroll in simply learns
job assist where you will get IM jobs
Pro membership for six months and obtain
35 iiba PDS cdus and 25 PMI pdus
you will get 170 Plus hours of Blended
learning along with Capstone projects in
three domains
to become a data analyst you can enroll
in the postgraduate program in data
analytics offered by simply learn
this program is in collaboration with
Peugeot University and IBM the skill
that will be covered as a part of the
course are statistical analysis using
Excel data analysis in Python and our
data visualization using Tableau and
power bi linear and logistic regression
modules clustering using k-means
supervised learning and others the tools
that you will learn are numpy pandas
sci-fi scikit-learn Excel and others
some of the key features of this course
are you will get Purdue postgraduate
program certification industry
recognized IBM certificates enrollment
and simply learns job assist and master
classes from Purdue faculty you have 180
Plus hours of Blended learning 14 plus
Hands-On projects on integrated labs and
Capstone projects in three domains
so please go ahead and enroll for these
programs if you want to grow your career
as a business analyst or a data analyst
data analyst versus data engineer versus
data scientist which one to choose
this is one of the most popular
questions asked by Learners looking for
a career in data and Analytics
I'm sure YouTube would have come across
these job roles in the ever-growing data
science landscape though they all deal
with data these jobs are not the same
there are significant differences
between what a data analyst data
engineer and a data scientist does
we will look at these job rules and the
differences in detail
first
let's look at some data analytics and
data science trends
the analytics and data science Market is
thriving
data analytics data engineering and data
science are the key trends in today's
axle rating Market as per statista.com
the global big data analytics Market
Revenue will grow at a Cher of 30
percent with Revenue reaching over 68
billion US Dollars by 2025.
according to techno view the Enterprise
data management Market is expected to
increase by
64.08 billion US Dollars by 2025 as per
markets and markets.com the big data
market size is projected to grow
from 162.6 billion US dollars in 2021 to
273.4 billion US dollars in 2026.
now another report from Research Drive
says that the data science platform
Market is estimated to reach
224.3 billion US Dollars by 2026.
so with so much data available and
companies making huge Investments to
drive business insights the job
opportunities for data analysts data
engineers and data scientists are going
to increase in 2022 and over the coming
years
no
let's learn the major differences
between data analyst versus data
engineer versus data scientist
so who are they
a data analyst analyzes and interprets
vast volumes of data in order to extract
meaningful information out of it
they find solutions to a business
problem and make critical business
decisions
the insights provided by data analysts
are important to companies that want to
understand the needs of their end
customers
for talking about who are data Engineers
a data engineer on the other hand builds
infrastructure and scalable pipelines to
manage the flow of data and prepare it
for analysis
so basically they optimize the systems
that enable data analysts and data
scientists to perform their job
efficiently
data scientists are professionals who
analyze and visualize existing data and
use algorithms to build predictive
models for making future decisions
they also engage with Business Leaders
to understand their needs and present
complex findings
with that let's look at the primary
roles and responsibilities of
these three job roles
data analysts are responsible to collect
clean store and process data the
Discover hidden patterns from data by
performing exploratory data analysis and
visualize data by creating charts and
graphs
acquiring data from primary and
secondary sources is one of their key
tasks
the build reports and dashboards and
also maintain databases
now talking about the roles and
responsibilities of a data engineer
a data engineer performs data
acquisition the design build and test
data as well to develop and maintain
data architecture
data Engineers are tasks with testing
integrating managing and optimizing data
from a variety of sources so they
integrate data into existing data
pipelines prepare data for modeling and
perform various ETL operations
now talking about the roles and
responsibilities of a data scientist
so data scientists develop machine
learning models to identify Trends in
data for making decisions
that develop hypothesis and use the
knowledge of Statistics data
visualization and machine learning to
forecast the future for the business
data scientists visualize data and use
storytelling techniques and also write
programs to automate data collection and
processing
now move on to the skills possessed by
data analysts data engineers and data
scientists
to become a data analyst you need to
have good hands-on experience with
writing SQL queries
you should have excellent Microsoft
Excel skills for analyzing data
data analysts are also good at
programming and they need to know how to
visualize data solve business problems
and possess domain knowledge
data Engineers should have a solid
understanding of SQL mongodb and
programming
they need to have a good command of data
architecture scripting data warehousing
and ETL data Engineers are also good at
Hadoop based Analytics
now talking about the skills for a data
scientist so a data scientist should
have experience with programming in
Python and r
this would have a very good
understanding of mathematics and
statistics as well
data scientists need to possess
analytical thinking and data
visualization skills as well
machine learning deep learning and
decision making are other critical
skills every data scientist should have
now we look at the salaries of a data
scientist a data analyst as well as a
data engineer
so a data analyst in the United States
earns over seventy thousand dollars per
annum while in India a data analyst can
earn nearly 7 lakh 25 000 rupees per
annum
a data engineer in the United States can
earn over 112 500 per year and in India
you can earn over 9 lakh rupees per
annum
talking about the salary of a data
scientist a data scientist in the United
States earns over 117 thousand dollars
per annum and in India a data scientist
can earn over 11 lakh rupees per annum
so coming to the final section of this
video we'll look at the top companies
hiring for data analysts data engineers
and data scientists
so we have the first company as Google
then we have Tesla next we have the
e-commerce giant Amazon
the internet Giant
Facebook or the social media giant
Facebook we have the tech giant Oracle
we also have Verizon and Airbnb so these
are some of the top companies that hire
for the three roles
what is data analytics
data analytics is a process of exploring
and analyzing large data sets to make
predictions and help data-driven
decision making now the definition of
large datasets keeps changing and so
this can range really from just about
anything to anything but usually in
today's world we're talking
significantly larger amounts of data
that you can't just glance at and try to
figure it out yourself and the two steps
are analyze the data and then make
decisions based on the data
applications of data analytics now the
sky's the limit on this in today's world
almost every business Act of Life your
music on your Spotify are driven by data
analytics but some of the big players
when you go in there job hunting are
going to be your fraud analysis if you
want to go make a lot of money and
you're good at it and you like dealing
with numbers go join the banks and track
down the criminals who are stealing
money it's a lot of you know it's a big
thing to protect credit cards predict
sales purchases bad checks any of those
things when you can track them down is
huge
Healthcare exploding there is everything
from trying to find cures for the covet
virus or any of the viruses out there
using your cell phone to diagnose
different ailments that way you don't
have to go and see the doctor you can
actually just go in there and take a
picture of the funky growth on your arm
hopefully it's not too big and then they
send it in there and the data analytics
goes in there looks at it and says oh
this is what this is this is a
professional you need to go see or don't
need to see
and that's just one aspect of healthcare
uh the database is being generated by
Healthcare and getting the right doctors
and helping the doctors analyze whether
something is benign or malignant if it's
cancerous all those things are now part
of the ongoing Health Care growth in
data analytics
Inventory management
think one of those huge warehouses where
they're shipping out all the goods how
do you inventory that in such a way so
that you maximize the stuff that's being
purchased the most near the entrance and
all the other stuff towards the back or
even pre-ship it so it's huge to be able
to inventory the manager inventory and
pretty soon they'll just have a drone
come in there and start picking up some
of those boxes and move them around also
deliver your Logistics again this goes
from getting from point A to point B you
can combine it with our inventory so you
pre-ship stuff if you know a certain
area is more likely to purchase it how
do you get it the delivery to the most
destinations the quickest in the short
amount of time and then they even
pre-stack the trucks going out and
that's all done with data analytics how
do we stack all that stuff so it comes
out in the right order
targeted marketing huge industry any
kind of marketing whether you're
generating the right content for the
marketing who are you targeting with
that marketing researching the people
what they want so you know what products
to Market out there all those things are
huge
and these are just a few examples you
can probably go Way Beyond this from
tracking forest fires
to astrology and studying the stars all
of this is part of data analytics now
and plays a huge role in all these
different areas
City Planning is another one you know
you can see a nice organized City like
this one where you can get in and out of
the neighborhoods if you're a fire truck
police officers need to be able to get
in and out you want your tourists to be
able to come in you still want the place
to look nice and you have the right
commercial development the right
Industrial Development like enough
residents for people to stay all those
things are part of your City Planning
again huge in data analytics
so sky's the limit on what you use it
for let's take a look at types of data
analytics
and this can be broken up in so many
ways but we're going to start with
looking at the most basic questions that
you're going to be asking in data
analytics and the first one is you want
descriptive analytics what has happened
hindsight how many sales per call ratio
coming out of the call center if we have
500 tourists in a forest and you have a
certain temperature how many fires were
started how many times did the police
have to show up to certain houses all
that's descriptive the next one is
predictive Predictive Analytics is what
will happen next we want to predict this
is great if you want to have a ice cream
store and you want to predict how many
people to work at the ice cream store in
a certain day based on the temperature
coming up in the time of the year
and then one of the biggest growing and
most important parts of the industry is
now prescriptive analytics and you can
think of that as combining the first two
we have descriptive and we have
predictive then you get
pre-scriptive Analytics
how can we make it happen foresight what
can we change to make this work better
in all the industries we looked at
before we can start asking questions
especially in City development there's a
good one
if we want to have our city generate
more income and we want that income to
be commercial based what kind of
commercial buildings do we need to build
in that area that are going to bring
people over do we need huge Warehouse
cells Costco sales buildings or do we
need little mom pod joints that are
going to bring in people from the
country to come shop there or do you
want an industrial setup what do you
need to bring that industry in there is
our car industry available in that area
if it's not a car industry what other
Industries are in that area all those
things are prescriptive we're guessing
we're guessing what can we do to fix it
what can we do to fix crime and area
with education what kind of education
are we going to use to help people
understand what's going on so that we
lower the rate of crime and we help our
communities grow better that's all
prescriptive it's all guessing we went
foresight into how can we make it happen
how can we make this better
and we really can't not go into enough
detail on these three because a lot of
people stumble on this when they come in
and are doing analytics whether you're
the manager shareholder or the data
scientist coming in you really need to
understand the descriptive analytics
where you're studying the total units of
furniture sold and the profit that was
made in the past here we go into
Predictive Analytics predicting the
total units that would sell and the
profit we can expect in the future gear
up for how many employees we need how
much money we're going to make and
prescriptive analytics finding ways to
improve the sales and the profit so we
can sell maybe a different kind of
furniture we're going to guess at what
the area is looking for and how that
marketing is going to change
data analytics process steps so let's
take a look at some of the basic
processing and what that looks like when
you're working with this data
so there's five basic steps the five
steps of processing and this changes and
then there's a lot of things that go on
when they talk about agile programming
the whole concept of agile is you take
some kind of framework like this and
then you build on it depending on what
your business needs
so the first step is data collection
and usually with a large company you
might have somebody who is responsible
for the database management you may have
another one where they're pulling apis
they're pulling data off of maybe the
Census Bureau maybe something very very
specific domain specific so if you're
analyzing cancerous growths and how to
understand them then the data collection
is going to be those measurements they
take from the MRI or that might be even
the MRI images they've used those also
so there's a lot of things with data
collection and how to control that and
make sure it has what you need and is
clean and you don't have misinformation
coming in
once you have the data collected there's
a data preparation
so stage two is we take that data and we
format it into something we can use
probably one of the biggest formats that
you see is when you're processing text
how do you process text well you use
what they call a one hot encoder and
each word is represented by a yes no
kind of setup so it'd be like a long
array of bits
that's one way to prepare it and so you
know bit number one is the bit number
two is has or whatever it is
other preparations might be if you're
using neural networks you might be
taking integers or float numbers and
converting them to a value between 0 and
1. that way you don't have one of them
creating a bias in there so there's a
lot of different things that go into
Data preparation that is 80 percent of
data science so we talk about the data
analytics which is a little bit more on
the math side and they usually say talk
about a data scientist kind of being the
overall
prepare this stuff you're going to spend
eighty percent of your data preparation
data exploration that's the fun part
this is where you're exploring things
and it is
maybe 10 to 15 percent of what you do
with the data you spend with the data
exploration it is probably the most
important step because this is where you
got to start asking questions if you ask
your questions wrong you're going to get
some wrong information if you're working
with a company and they want to know the
marketing values then you really got to
focus on hey how do we generate money
for this company or fraud how do we
lower the fraud rate while still
generating a profit for data modeling
this is where we start actually getting
into the data code which model to use
that predicts what's going to happen
uh and then result interpretation we
want to be able to interpret those
results usually see that in your matplot
library where you create nice beautiful
images so it shows up on their dashboard
for the marketing manager or for the CEO
so they can take a quick look and say
hey I can see what's going on there you
want to reduce it to something they can
easily read they don't want to hear the
scientific terms they want to see
something they can use and we'll talk
about that a little bit more when we
start looking at some of this in a demo
since this is data analysis with python
we've got to ask the question why python
for data analytics I mean there's C plus
there's Java there's dot net from
Microsoft why do people go to python for
it
so the number of reasons one it's easy
to learn with simple syntax
you don't have a very high type set like
you do in Java and other coding so it
allows you to kind of be a little lazy
in your programming that doesn't mean
that it can't be set that way and that
you don't have to be careful it just
makes means you can spin up a code much
quicker in Python the same amount of
code to do something in Python A lot of
times is one two or three or four lines
where when I did the same thing say in
Java I found myself a 10 12 13 20 lines
depending on what it was
it's very scalable and flexible so
there's our flexibility because you can
do a lot with it and you can easily
scale it up you can go from something on
your machine to using a pi spark under
the spark environment and spread that
across hundreds if not thousands of
servers across terabytes of data or
petabytes of data so it's very scalable
there's a huge collection of libraries
this one's always interesting because
Java has a huge collection of libraries
C has a huge collection of
libraries.net does and they're always in
competition to get those libraries out
Scala for your spark all those have huge
collection libraries this is always
changing but because Python's open
source you almost always have easy to
access libraries that anybody can use
you don't have to go check your
licensing and have special licensing
like you do in some packages
graphics and visualization they have a
really powerful package for that so it
makes it easy to create nice displays
for people to read
and community support because python is
open source it has a huge community that
supports it you can do a quick Google
and probably find a solution for almost
anything you're working on
python libraries let's bring it together
we have data analytics and we have
python so when we're talking data
analytics we're talking python libraries
for data analytics and the big five
players are numpy pandas matplot Library
scipy which is going to be in the
background so we're not going to talk
too much about the scientific formulas
inside pi and PSY kit
so numpy supports in dimensional arrays
provides numerical Computing tools
useful for linear algebra and Fourier
transform
and you can think of this as just a grid
of numbers and you can even have a grid
inside a grid or data it's not even
numbers because you can also put words
and characters and just about anything
into that array but you can think of a
grid and then you can have a grid inside
a grid and you end up with a nice
three-dimensional array if you want to
talk three-dimensional array you can
think of images you have your three
channels of color four if you have an
alpha and then you have your X Y
coordinates for the image we're looking
at so you can go x y and then what are
the three channels to generate that
color
and numpy isn't restricted to three
dimensions you could imagine watching a
movie well now you have your movie clips
and they each have their X number of
frames and each of those frames have X
number of X Y coordinates for the
pictures in each frame and then you have
your three dimensions for the colors so
numpy is just a great way to work within
dimensional arrays
now closely with numpy is pandas useful
for handling missing data perform
mathematical operations provides
functions to manipulate data
pandas is becoming huge because it is
basically a data frame and if you're
working with big data and you're working
in spark or any of the other major
packages out there you realize that the
data frame is very Central to a lot of
that and you can look at it as a Excel
spreadsheet you have your columns you
have your rows or indexes and you can do
all kinds of different manipulations of
the data within including filling in
missing data which is a big thing when
you're dealing with large pools or lakes
of data where they might be collected
differently from different locations
and matplot Library
we did kick over the sci Pi which is a
lot of mathematical computations which
usually runs in the background of the
for of numpy and pandas although you do
use them they're useful for a lot of
other things in there but the matplot
library that's the final part that's
what you want to show people and this is
your plotting library in Python several
toolkits extend matplot Library
functionality there's like a hundred
different toolkits to extend matplot
Library which range from how to properly
display star constellations from
astronomy there's a very specific one
built just for that all the way to some
very generic ones we'll actually add
Seaborn in when we do the labs in a
minute several tool kits extend matplot
Library functionality and it creates
interactive visualization so there's all
kinds of cool things you can do as far
as just displaying graphs and there's
even some that you can create
interactive graphs we won't do the
interactive graphs but you'll see you'll
get a pretty good grasp of some of the
different things you can do in matplot
library okay
let's jump over to the demo which is my
favorite roll up our sleeves get our
hands in on what we're doing now there's
a lot of options when we're dealing with
python you can use pie charm as a really
popular one
uh and you'll see this all over the
place so it's one of the main ones
that's out there and there's a lot of
other ones I used to use netbeans which
is kind of lost favor I don't even have
it installed on my new computer
but the most popular one right now for
data science Now pycharm is really
popular for python General development
for data science we usually go to
Jupiter notebook or anaconda and we're
going to jump into Anaconda because
that's my favorite one to go to because
it has a lot of external tools for us
we're not going to dig into those but we
will pop in there so you can see what it
looks like so with Anaconda we have our
Jupiter lab we have our notebook these
are identical Jupiter lab is an upgrade
to the notebooks with multiple tabs
that's all it is and we'll be using the
notebook and you can see that pycharm is
so popular with python that we even have
it highlighted here in Anaconda as part
of the setup
Jupiter notebook can also be a
standalone so we're actually going to be
running Jupiter notebook and then you
have your different environments I have
we're going to be under main Pi 36
there's a root one and I usually label
it Pi 36
the reason is is currently as they're
writing this tensorflow only works in 3
6 and not in three seven or three eight
for doing neural networks but you can
actually have multiple environments
which is nice they're they separate the
kernel so it helps protect your computer
when you're doing development and this
is just a great way to do a display or a
demo especially if you're looking for
that job pull up your laptop open it up
or if you're doing a meeting get a
broadcast up to the big screen so that
the CEO can see what you're looking at
and when we launched the notebook it
actually opens up a file browser in
whatever web browser you have this
happens to be Chrome
and then you can just go under new
there's a lot of different options
depending what you have installed Python
3 and this just creates an Untitled
version of this and you can see here I'm
actually in a simply learn folder for
other work I've done for simply learn
and that's where I save all my stuff and
I can browse through other folders
making it really easy to jump from one
project to another
and under here we'll go ahead and change
the name of this and we'll go ahead and
rename it
data analytics data analytics just so I
can remember what I was doing
which is probably about 50 of the
folders in here right or files in here
right now uh so let's go ahead and jump
in there and take a look at some of
these different tools that we were
looking at
and as we go through the demo let's
start with the numpy uh the least
visually exciting and I'm going to zoom
in here so you can see what we're doing
and the first thing we want to do is
import numpy
and we'll import it as NP that is the
most common numpy terminology
and let's go ahead and change the view
so we also have the line numbers I don't
know why we probably won't need them but
I'm looking for easy reference and then
we'll create a one dimensional array we
just call this array one
and it equals NP dot array and you put
your array information in here
in this case we'll spell it out you can
actually do like a range and other ways
there's lots of ways to generate these
arrays but we'll just do one two three
so three integers
and if we print
our array one
we can go ahead and run this
and you can see right here it prints one
two three you can see why this is a
really nice interface to show other
people what you're doing with the
Jupiter notebook
uh so this is the basic we've created an
array this is a one-dimensional array
and then an array is one two three one
of the nice things about the Jupiter
notebook is whatever ran in this first
setup
is still running it's still in the
kernel so it still has the numpy
imported as in p and it still has our
variable
arr1 for array one equal to NP array of
one two three
so we go to the next cell
we can check the type of the array we're
just going to print
we say hey what's what what is this
setup in here and we want type
and then we want what is the type of
array one and let's go ahead and run
that
and it says class numpy ND array so it's
its own class that's all we're doing is
checking to see what that class is
and if you look at the array class
probably the biggest thing you do I
don't know how many times I find myself
doing this because I forget what I'm
working on and I forget I'm working with
a three-dimensional or four-dimensional
array and I have to reformat somehow so
it works with whatever other things I
have and so we do the array shape the
ray shape is just three because it has
three members and it's a one-dimensional
array that's all that is
and with the numpy array we can easily
access stick with the print statement
if you actually put a variable in
Jupiter notebook and it's the last one
in the cell
it will be the same as a print statement
so if I do this where array one of two
it's the same as doing print
array of two that's those are identical
statements in our Jupiter notebook
we'll go and stick with the print on
this one
and it's three so there's our print
space two and we have 0 1 2
equals three we can easily change that
so we have array one of place two
equals five
and then if we print our array one
you can see right down here when it
comes out it's one two and five
and there I left the print statement off
because it's the last variable in the
list it'll always print the variable if
you just put it in like that
that's a Jupiter notebook thing don't do
that in pi charm I've forgotten before
doing a demo
and we talked about multiple Dimensions
so we'll do an array two-dimensional
array
and this is again a numpy array
and in the numpy array
we need our first Dimension we'll do one
two three
and our
second dimension uh three four five and
you can see right here that when we hit
the uh we'll do this we'll just do array
two
and we can run that and there's our
array two one two three three four five
we can also do array two
of 1
and then we can do let's do a zero it
doesn't really matter which One X is two
two there we go and if I run this it'll
print out five because here we are this
is zero zero one two three is under zero
Row three four five is on our one row
now we start with zero and then the two
zero one two goes to the five
and then maybe we forgot what we were
working with so we'll go do array two
dot shape
and if we do array two of shape
uh we'll go and run that we'll see we
have two rows and each row has three
elements a two-dimensional array two
three if you looked up here when we did
it before it just had three comma
nothing when you have a single entity it
always saves it as a tuple with a blank
space
but you can see right here we have two
comma three
and if you remember from up here we just
did this array two of oh let's go what
is it one
comma two
we run that we get the five you can also
count backwards this is kind of fun
and you'll see I just kind of Switched
something on you because you can also do
one comma two to get to the same spot
now two is the last one zero one two
it's the last one in there we can count
backwards and do minus one and if we run
this we get the same answer whether we
count it as uh let's go back up here
whether we count this as 0 1 2
or we count backwards as minus one minus
two minus three and you can see that if
I change this minus one to a minus two
and run that
I get 4 which is going backwards minus
one minus two so there's a lot of
different ways to reference what we're
working on inside the numpy array
it's really a cool tool it's got a lot
of things you can do with it
and we talked about the fact that it can
also hold things that are not values and
we'll call this array s for Strings
equals NP dot array
and put our setup in there brackets and
let's go
China
um
India
USA
Mexico
doesn't matter we can make whatever we
want on here and if we print that out
we run this you can see that we get our
numpy array China India USA Mexico it
even gives us our D type of a U6
a lot of times when you're messing with
data we'll call this array R for range
just to kind of keep it uniform NP dot a
range
so this is a command inside numpy to
create a range of numbers
and if you're testing data Maybe you
want maybe you have equal time
increments that are spaced a certain
point apart but in this case we're just
going to do integers
and we're going to do a setup from 0 20
skipping every other one
and we'll print it out and see what that
looks like
and you can see here we have 0 2 4 6 8
10 12 14 16 18 like you expected it
skips every one
and just a quick note
there's no 20 on here uh why well this
starts at zero and counts up to 20. so
if you're used to another language where
explicitly says less than or less than
equal to 20 like for x equals zero X
plus plus X is less than 20. that's what
this is it just assumes X is less than
20 on here
if we want to create a very uniform uh
set you know zero two four six what
happens if I want to create numbers uh
from 0 to 10 but I need 20 increments in
there we can do that with line space so
we can create
um an R we'll call this l
equals I don't think we'll actually use
any of this again so I don't know why
I'm creating unique identifiers for it
but we'll do NP
Lin space
and we're going to do zero
to 10 or 0 to 9. remember it doesn't it
goes up to 10. and then we want to let's
say we have 20
different
um
increments in there so we're creating a
we have a data set and we know it's over
a certain time period and we need to
divide that time period by 20 and it
happens to just have 10 pieces in it and
here we go you can see right here we
have 20 or has 20 pieces in it but it's
over 10 years we got to divide it in the
middle and you can see it does it goes
0.52 remember yeah there's our 10 on the
end so it goes up to 10.
uh and then we can also do random
there's NP dot random if you're doing
neural networks usually you start it by
seeding it with random numbers
and we'll just do NP dot random and
we'll just call this array
we'll stop giving it unique numbers
we'll print that one out and run it
and you can see we have random numbers
they are zero to one so you'll see that
all these numbers are under one and you
can easily alter that by multiplying
them out or something like that if you
want to do like zero to a hundred you
can also round them up if it's integer 0
to 100 there's all kinds of things you
can do but generates a random float
between 0 and 1.
and you have a couple options you could
reshape that or you can just generate
them in whatever shape you want and so
we can see here we did three and four
and so you can see three rows by four
variables
same thing as doing a reshape of 12
variables to three and four
and if you're going to do that you might
need an empty data set I have had this
come up many times where I need to start
off with zero and I don't know you know
because I'm going to be adding stuff in
there or it might be 0 and 1 or 1 is if
you're removing the background of an
image you might want the background is
zero and then you figure out where the
image is and you set all those boxes to
one and you create a mask so creating
masks over images is really big and
doing that with a numpy array of zero
and we can also uh
give it a space
and we'll just do this all in one shot
this time
and we'll do the same thing like we did
before
zeros and in this case we'll do two
comma three
and so when we run this
I forgot the asterisks around it I knew
I was forgetting something there we go
so when we run this you can see here we
have our 10 zeros in a row and maybe
this is a mask for an image and so it
has uh two rows of three digits in it so
it's a very small image a little tiny
pixel
and maybe you're looking to do something
the opposite way instead of creating a
mask of zeros and filling in with ones
maybe you want to create a mask of ones
and fill them in with zeros and we'll
just do just like we did before with the
three comma four and when we run this
you'll see it's all ones and we could
even do this even we'll do it this way
let's do
10.
10 by 10 icon and then you have your
three colors and you can so creates
quite a large array there for doing
pictures and stuff like that when you
add that third dimension in
if we take that off it's a little bit
easier to see
we'll do 10 again
and you can easily see how we have 10
rows of 10 ones
and you can also do something like
create an array
and we'll do 0 1 2.
and then in this array we actually print
it right out we want a repeat and so you
can actually do a repeat of the array
and maybe you need this array
um let's repeat it three times
so there's our repeat of an array repeat
three times
and if we run this
you'll see we have zero zero zero one
one two two two
and whenever I think of a repeat I don't
really think of repeating being the
first digit three times the second digit
I really always think of it as zero one
two zero one two zero one two it catches
me every time but the actual code for
that one is going to be tile
and again if we do arrange three
and we run this you can see how you can
generate one zero one two zero one two
zero one two
and if you're dealing with an identity
Matrix we can do that also if you're big
on you're doing your matrixes and we'll
just identity
I guess we'll go ahead and spell it out
today any tricks
and the command we're looking for is um
i e y e and we'll do three and then
we'll just go ahead and print this out
there we go there's our identity Matrix
and it comes out by a three by three
array because there's our Matrix
and then it puts the ones down the
middle and for doing a different Matrix
math
and we can manipulate that a little bit
too
we talk about
Matrix is
we might not want ones across the middle
in which case we now have the diagonal
so we can do an NP dot diagonal
and we do a diagonal let's put in the
diagonal
one two three four five and when we run
this
again this generates a value and by just
putting that value in there's the same
as putting print around it or putting
array equals and then print array and
you can see it generates a diagonal one
two three four five and there's your uh
your beginning of your Matrix array for
working with matrixes
and we can actually go in reverse let's
create an array equals remember our
random
random.random and we'll do a five by
five array oops there we go
five five
and just so you can see what that looks
like
helps if I don't mistype the numbers
which in this case I just need to take
out the brackets and there you go you
have your your five by five array set up
in there and we can know because we're
working with Matrix is we might want to
do this in reverse and extract the
diagonals which would be the 0.79 the
0.678 and so on
and we simply type in
P dot diagonal
and we put our array in there
and this will of course print it out
because it returns it as a variable and
you can see here here's our diagonal
going across from our Matrix
and we did talk about shape earlier if
you remember you can do print the shape
out you can also do the dimensions so in
Dimensions very similar to shape it
comes out and just has two dimensions
we can also look at the size so if we do
a size on here we can run that and you
can see it has a size of 25 two
dimensions and of course five by five
and that was from the shape from earlier
that we looked at there's our five by
five shape
and if you remember earlier we did
random well you can also do uh random I
talked a little bit about manipulating
zero to one and how we can get different
answers you can also do straight for the
integer part and we'll do minus 10
210
4
and so we're going to Generate random
integers between minus 10 to 10 we're
going to generate four of those and so
when we run that we have seven minus
three minus six minus three they're all
be at tween minus 10 and 10 and there's
four of them
and now we jump into some of the
functionality of arrays which is really
great because this is where they come in
here's the array and you can add 10 to
it and if I run this there takes my
original array from up here
with the integers and adds 10 to all of
those values so now we have oh this is
the decimal that's right this is a
random decimal I had stored in Array
but this takes a random decimal the
random numbers I had from zero to one
and adds 10 to them and we can just as
easily do uh
minus 10
. we could even do
times two
and we could do divide by two
and it would it'll take that random
number we generated and cut in half so
now all these numbers are under 0.5
another way you can
change the numbers to what you need on
there
and as you dig deeper into numpy we can
also do exponential so as an exponential
function which would generate some
interesting numbers off of the random
so we're taking them to the power I
don't even remember what the original
numbers in the array were because we did
the random numbers up there here's our
original numbers and if you build an
exponential on there this is where you
get e to the X on this and just like you
can do e to the X you can also do the
log so if we're doing logarithmic
functions
that reinforced learning you might be
doing some kind of log setup on there
and you can see the logarithmic of these
different array numbers
and if you're working with a log base 2
you can do you can just change it in
there in P log 2. you have to look it up
because this is not log one two three
four five it is Log and log 2. so just a
quick note that's not a variable going
in that is an actual command there's a
number of them in there and you'll have
to go look and see what the
documentation is but you can also do log
10. so here's log value 10.
some other really cool functions you can
do with this is your sign
so we can take a sine value of all of
our different values in there
and if you have sine you of course have
cosine
we can run that
so here's the cosine of those and if
you're doing activations in your numpy
array you're doing a tangent activation
there's your tangent for that
and the tangent activation is actually
from neural networks that's one of the
ways you can activate it because it
forms a nice curve between from whether
you're generating 1 to negative one with
some discrepancy in the middle
just jumping a little bit in there into
neural networks
and then we get into let me just put the
array back out there so we can see it
when we're doing this
as we're getting into this you can also
sum the values so we have NP sum
and you can do a summation of all the
values in this array and you'll see that
if you added all these together they'd
equal
12.519 and so on I don't know what the
whole setup is in there
but you can see right here the the
summation of this one of the things you
can also do is buy axes so we could do
axes equals zero
and if we run the summation of the axes
equals zero
and you can think of that in numpy as
the rows so that would be or
you can think of that in numpy as being
the columns we're summing these columns
going across
and you can also change this to one
and now we're summing the rows
and so that is the summation of this row
and so forth and so forth going down
and maybe you don't need to um
know the summation maybe what you're
looking for is the minimum
so here's our minimal now you're looking
for and this comes up a lot because you
have like your errors we want to find
the minimal error inside of this array
and just like the other one we can do
axes equals zero
and you can see here
0.0645 is the smallest number in this
First Column is 0.0645 and so on
and if you have a minimum well you might
also want to know the max maybe we're
looking for the maximum profit and here
we go you can see maximum 0.79 is a
maximum on this first column and just
like we did before you can change this
to a one on axes you can take the axes
out of here and just find the max value
for the whole array and the max value in
here was 0.8344 so on so on
and since we're talking data analytics
we want to go ahead and look at the mean
pretty much the same as the average this
is the mean across the whole thing and
just like we did before we could also do
axes equals zero
and then you'll see this is the mean of
this axis and so on
and we have mean we might want to know
the median
and there's our median our most common
numbers if we have median we might want
to know the standard deviation or if we
have the average a lot of times you do
the means in the standard deviation
we can run that and there's our standard
deviations along the axes we can also do
it across the whole array
uh if we're going to do standard
deviations there's also variance
which is your VAR
and there's our variance across the
different levels
and so if we looked at that we looked at
variance we looked at standard deviation
the median and the means there's more
but those are the most common ones used
with data analytics and then going
through your data and figuring out what
you're going to present to the
shareholders
and some other things we can do is we
can actually take slices you'll hear
that terminology and a slice might be
like we have a five by five array but
maybe we don't want the whole array
maybe we want uh from one on we don't
want the zero in there so we got up to
four and maybe on the second part we
just want
two
to row three and see this notation right
here says 1 to the end and if we run
this you can see how that generates a
single row to the end and then row two
and three now remember it doesn't
include three that's why we only get the
one column so if we wanted two and three
you would need to go ahead and go two to
four so it goes up to four
we could also do this in reverse
just like we learned earlier we can go
minus one oops
and when we go to -1
it's the same thing because we have 0 1
2 3 4 this is the same thing as two to
four it goes two to the last one
also very common with arrays is you're
going to want to sort them so we still
have our array up here that we randomly
generated and we might want to um
sort it and we'll go and throw an axis
back in there uh
axis equals one if we run this
you can see from the axes that it sorts
it
the point two being the lowest value to
the highest value by the row we can also
change this of course to axis zero if
you're sorting it by columns so maybe
your values are based on columns
and then of course you can do the whole
array
and we can sort that I don't usually do
that but you know I guess sometimes you
might that might come up
and so you can see right here we have a
nice sorted array
something now let's just go ahead and
reprint our array so we can look at it
again starting to get too many boxes up
there something else you can do with an
array
is we can take and transpose it
this comes up more than you would think
when you transpose it you'll see that
the rows and the column are transposed
so where
0.79.57 0.064 is a column now we've
switched it and we have
0.79.42 as the index
you can see this really more dramatic if
we take a slice
we'll just do a slice of the first
couple
and then we'll just do all the other the
full rows and if we run this you can see
how it comes up a little bit different
and we'll just do the same slice up here
so you can see how those two look next
to each other
there we go there's our slice run
and so you can see the slice comes up
and it has a one two three four five
columns now we have one two three four
five rows and three columns versus three
rows
and the original version when they first
started putting this together was a
function so the original version was
transpose and this still works you can
still see it generates the same value as
just a capital T
so many times we flip this data because
we'll have an XY value or we'll have an
image or something like that and it's
being read one way into the next process
and the next one needs it the opposite
so this actually happens a lot you need
to know how to transpose the data really
quick
and we can go ahead oh let's just take
here's our transpose we'll just stick
with the transpose on here
and instead of doing it this way we
might need to do something called
flattening why would you flatten your
data if this is an array going into a
neural network
you might want to send it in as one set
of values instead of two rows and you
can see here is all the values as a
single array it just flattens it down
into one array
so we covered our scientific means
transpose median some different
variations on here some of the other
things we want to do is what happens if
we want to append to our array so let's
create a new array
I'm getting tired of looking at the same
set of random numbers we generated
earlier so we'll go ahead and create a
new array here something a little
simpler so it's easier to see what we're
doing
and four five six seven eight uh that's
good enough we'll just do four five six
seven eight
and if we print this array
there it is four five six seven eight
and we might want to append something to
the array so we have our array we need
to extend it you got to be very careful
about appending things to your array and
there's a number of reasons for that
one is runtime because of the way the
numpy ray is set up a lot of times you
build your data and then push it into
the numpy array instead of continually
adding on to the array
and then it also usually it
automatically generates a copy for
protecting your data so there's a lot of
reasons to be careful about appending
this way but you can certainly do it and
we can just take our array we're going
to create a new array array one
and if we print array one and we append
8 to it you'll see four five six seven
and then there's our a depended on to
the end
and if you want to append something to
an array you'd probably also want to
whoops
array one let's try that again there we
go now we have the eight appended on to
the end
so you can see four five six seven eight
and then we pinned it another eight on
there
and if you're going to append something
you might want to um
go ahead and insert instead of appending
it might be you need to keep a certain
order and we can do the same thing we do
our array
and we're going to pin or insert at the
beginning and let's go ahead and insert
uh one two three one two three and we go
ahead and print our array two we run it
and you can see one two three a pin is
inserted at the beginning
inserts a lot more powerful and that you
can put it anywhere in the array we can
move it to the one spot and there we go
one two three we can do a minus one
just for fun and you'll see it comes up
one two three and we're counting
backwards by one
I'm imagining do minus zero
and run this and it turns out that minus
zero puts it back at the beginning
because that's why it registers a zero
just takes a minus sign off
and just like we add numbers on we might
want to delete numbers and so let's do
an NP dot delete now let's let's keep it
a little bit make it a little easy here
to watch we'll go ahead and create an
array three
then we'll do NP delete and we're just
working with array 2
and we want to do is delete zero space
so if we look at this here's our array
two array 2 starts with one and when we
delete the space on here
and print that out we deleted the one
right out of there
and we can also do something like this
where we can do it as a slice and we can
do let's do one comma three
and if we run one comma three you'll see
we've deleted the one space
and the three space out which deleted
our two and four
now keep in mind when you're messing
with adding lines and deleting lines
you have to be really careful because
there's a time element involved as far
as where the data is coming from and
it's really easy to delete the wrong
data and corrupt what you're working on
or to insert stuff where you don't want
it so there's always a warning when we
talk about manipulating numpy arrays
and just like anything else we're doing
we'll create an array C which equals
we'll just do our our numpy array that
we just created our numpy array three
and we can do copy so you can make a
copy of it maybe you want to protect
your original data or maybe you're
making a mask and so you copy the array
and then the new array make all these
alterations and change it from values to
zero to one to mask over the first one
and of course we if we do array C since
it equals a copy of array three it's the
same thing one three five six seven
eight
and now we're getting into combine and
split arrays
I end up doing a lot of this
and I don't know how many times I end up
fiddling with this and having a mess
so but but you do it a lot you know you
combine the arrays you split them you
might need one set of data for one thing
another set of data for the other
so let's go ahead and create two arrays
array one array two
and I want you to note
in the terminology we're going to look
for is concatenate what that means is
we're going to take
um
we'll call this a raycat I like a ray
cat there we go our array cat our
concatenated array
we're taking array one and two
and it's very important to really pay
attention to your axes and your accounts
I can't merge two arrays that have like
if their axes are messed up and I'm
merging on axis zero it's going to give
me an error and I'll have to reshape
them so you got to make sure that
whatever you're concatenating together
works
and what that means
as you can see here we have one two
three four one two three four and then
five six seven eight five six seven
eight along the zero axes
these each are four values
um so it's a two by four value and if we
go ahead and switch this to one you can
see how that's that flips it a little
bit so now we have one two three four
five six seven eight
it's interesting that we chose that one
if I did something like this
where this is now
there we go and we concatenate it run
this and it gives me an answer okay
because I have two by two and I'm using
axes one but if I switch this to axis
zero where now it's got three and five
it gives me an error so you got to be
really careful on that to make sure that
your whatever axes you are putting
together that they match so like I said
this one oops X is one axis one has two
entities and since we're going on axis
one or by row you can see that it lets
it merge it right onto the end there
and you could imagine this if this was a
x y plot of value or the x value going
in and the predicted y value coming out
and then you have another prediction and
you want to combine them this works
really easy for that
and we'll go back and let's just put
this back to where we had it
oops I forgot how many changes I made
there we go
um we'll just put it whoops I messed up
in my concatenation order here
[Music]
we go
okay so you can see that we went through
the different concatenation axes is
really important when you're doing your
concatenation values on here
and we'll switch us back to one just
because I like the looks of that better
there we go two rows
now there are other commands in here so
we can do cap V
equals npv v stack
this is nothing more than your
concatenation
but instead we don't have to put the
axes in there because it's v stands for
vertical
and so if we print out cat
V and we run this
you can see we get the one two three
four one two three four and that would
be the same as making this axis zero for
vertical stack and if you're going to
have a vertical stack you can also have
an H stack
so if we change this to from v-stack to
oops here we go H stack and we'll just
change this from cat to cat
and I run this
it's the same as doing axis zero the
process is identical in the background
this is like a legacy setup your v-stack
and your H stack most people just use
concatenate and then put the axes in
there because this much has a lot more
clarity and is more more commonly used
nowadays
the last section in numpy we're going to
cover is
under is kind of data exploration and
that'll make a little bit more sense in
just a moment sometimes it comes set
operations but let's say we have an
array one two three four five six three
whatever it is I think so we generate a
nice little array here and what I want
to go ahead and do is find the unique
values in that array
so maybe I'm generating what they call a
one hot encoder and so these values then
all become I need to know how long my
bit Ray is going to be so each word how
many how many each word is represented
by a number and then I want to know just
how many of those words are in there if
we're doing word count very popular
thing to do
and you can see here when we do unique
uh we have one two three four five six
those are our unique values
uh some of the things we can do with the
unique values is we can also instead of
doing just unique we can do uniques
our unique values and counts of each
unique value
and this is very similar to what we just
did up here where we we're doing NP
unique but we're going to add a little
bit more into there
and it's just part of the arguments in
this and we want to do return
counts equals
true so instead of just returning the
unique values we want to know how many
of those unique values are in each one
and we'll go ahead and print
our uniques
and print
our counts
let me run that you can see here we have
our unique value one two three four five
six just like we had before and then
there's two of the first of two ones two
twos two threes two fours one five two
sixes and so on and you can go through
and actually look at that if you want to
count them but a quick way to find out
your distribution of different values so
you might want to know how often the
word the is used versus the word and if
each word is represented as a unique
number
and along the set variables we might
want to know let me just put a note up
here
we're going to start looking at
uh intersection
and we might want to also know
differentiation
and neither
so when we're whoops neighbor
so what we're looking at now is we want
to know hey where do these two arrays
intersect and we have one two three four
five three four five six seven we might
want to know what is common between the
two arrays
and so when we do that we have NP
intersect
and it's a 1D array one dimensional
array
and then we need to go ahead and put
array 1 array two
and if we run this
we can see they intersect at three four
five that's what they have common
and because we're going to go ahead and
go through these and look at a couple
different options let's change this from
intersect 1D
and we'll do the same thing we'll go
ahead and print this
so we might want to know the
intersection where they have
commonalities
another unique word is Union of 1D so
instead of intersect
we want to know all the values that are
in both of them so here's our Union of
1D when we run that you can see we have
one two three four five six seven so
it's all the different values in there
and the last one of the last words we
have two more to go as we want to know
what the set difference is
uh and so that's where the you'll see
that if you remember set we talked about
that being the what they call these
things so the set difference
have a 1D array we run that you can see
that one is only in one array and two is
only in one array
and if we want to know uh what's in
Array 1 but not in Array two we might
want to know what is in Array one but
not two and what's in two but not one
and this would be the set X or 1D on
here so we have the four different
options here where we can do an
intersection what do they both have in
common we can do a union what are all
the unique values in both arrays we can
see the difference what's in Array one
but not array two so set diff 1D and
then set X or what is not in one but is
in two and what is in not in two but in
one
so we dug a lot in numpy because we're
talking there's a lot of different
little mathematical things going on in
numpy a lot of this can also be done in
pandas although usually the heavy
lifting is left for numpy because that's
what it's designed for let's go ahead
and open up another python 3.
setup in here
and so we want to explore what happens
when you want to display this this is
where it starts getting in my opinion a
little fun because you're actually
playing with it and you have something
to show people and we'll go ahead and
rename this we're going to call this uh
pandas and Pi plot
so pandas pipelot just we can remember
for next time and we want to go ahead
and import the necessary libraries we're
going to import pandas as PD now
remember this is a data frame so we're
talking rows and columns and you'll see
how pandas work so nicely when you're
actually showing data to people and then
we're going to have numpy in the
background numpy works with pandas so a
lot of times you just import them by
default
Seaborn sits on top of the matplot
library so sometimes we use the Seaborn
because it kind of extends it's one of
the 100 packages that extends the
matplot library probably the most common
used because it has a lot of built-in
functionality almost by default I
usually just put Seaborn in there in
case I need it and of course we have
matplot Library as pipelot as PLT and
note we have as PD as NP as SNS as PLT
those are pretty standard so when you're
doing your Imports I would probably keep
those just so other people can read your
code and it makes sense to them that's
pretty much a standard nowadays
and then we have the strange line here
it says ambersign matplot Library inline
that is for Jupiter notebook only so if
you're running this in a different
package it'll have a pop-up when it goes
to display the matplot library you can
with the most current version of Jupiter
usually leave that out and it will still
display it right on the page as we go
and we'll see what that looks like
and then we're going to go ahead and
just do the Seaborn the sns.set and
we're going to set the color codes
equals true let them just keep the
default one so we don't have to think
about it too much
and we of course have to run this the
reason we run this is because these
values are all set if we don't run this
and I access one of these afterward
it'll crash
the cool thing about Jupiter notebooks
is if you forgot to import one of these
you forgot to install it because you do
have to install this under your anaconda
setup or whatever setup you're in you
can flip over to Anaconda and run your
install for these and then just come
back and run it you don't have to close
anything out
and we'll go ahead and paste this one in
here real quick where we have car equals
PD dot read underscore CSV
and then we have the actual path
this path of course will vary depending
on what you are working with so it's
wherever you save the file at and you
can see here I have um like my OneDrive
documents simply Learn Python data
analytics using python slash car CSV
it's quite a long file
when we open that up what we get is we
get a CSV file and we have the make the
model the year the engine fuel type
engine horsepower cylinders and so on
and this is just a comma separated file
so each row is like a row of data think
of it as a spreadsheet
and then each one is a column of data on
here and as you can see right here it
has the make model so it has columns for
a header on here
now your pandas just does an excellent
job of automatically pulling a lot of
this in so when you start seeing the
pandas on here you realize that you are
already like halfway done with getting
your data in I just love pandas for that
reason numpy also has it you can load a
CSV directly into numpy but we're
working with pandas and this is where it
really gets cool is I can come down here
and I can print remember our print
statement we can actually get rid of it
and we're just going to do car head
because it's going to print that out the
head is going to print the top values of
that data file we just ran in
and so you can see right here it does a
nice printout it's all nice and in line
because we're in Jupiter notebook I can
scroll back and forth and look at the
different data and just like we expected
we have our column it brought the header
right in
one thing to note is the index it
automatically created an index 0 1 2 3 4
and so on and we're just looking at the
head so we got zero one two three four
you can change this you might want to
just look at the top two we can run that
there's our top two BMWs another thing
we can do is instead of head we can do
tail
and look at the last three values that
are in that data file and you can see
right here it numbered them all the way
up to
11913 oh my goodness they put a lot of
data in this file I didn't even look to
see how big the file was so you can
really easily get through and view the
different data in here when you're
talking about Big Data
you almost never just print out car in
fact let's see what happens when we do
if we run this and we just run the car
it's huge in fact it's so big that the
pandas automatically truncates it and
just does head plus tail so you can see
the two so we really don't want to look
at the whole thing I'm going to go back
to let's stick with the head
displaying our data there we go so
there's a head of our data it gives us a
quick look to see what's actually in
there I can zoom out if we want so you
can actually get a better View
although we'll keep it zoomed in so you
can see the code I'm working on
and then from the data standpoint we of
course want to look at
um
data types what's going on with our data
what does it look like now this you know
you show your when you're talking to
your shareholders they like to see these
nice easy to read charts they look like
a spreadsheet so it's a nice way of
displaying pieces of the chart
when you talk about the data types now
we're getting into the data science side
of it what are we working with well we
have make model we have an integer 64
for the year engine field type is an
object if we go up here you can see that
there most of them are
like you know it's a set manual a rear
wheel drive so they might be very
limited number of types in there
and so forth and it's either going to be
a float 64 an integer or an object just
the way it's going to read it on here
and the next thing you're going to know
is like your columns
and since it loaded the columns
automatically we have here the make the
model the year the engine the size all
the way up to the MSRP
and um just out of something you'll see
come up a lot is whenever you're in
pandas and you type in dot values it
converts it from a pandas list to a
numpy array
and that's true of any of these so then
you end up in a numpy array so you'll
see a little switch in there in the way
that the data is actually stored and
that's true of any of these in this case
we want car dot columns
you have a total list of your car
columns
and like any good data scientist we want
to start looking at analytical summary
of the data set what's going on with our
data so we can start trying to piecemeal
it together so we can do car
describe
and then we'll do is we'll do include
equals
all
so a nice Panda command is to describe
your data
if you're working with r this should
start looking familiar
and we come down here and you can see
count there's a and make the model of
the year how many of each one how many
unique values of each one the top value
of each one what's most common the
frequency the mean clearly on some of
these it's an object so really can't
tell you what the average is it'd just
be the top ones the average I guess
the year what's the average year on
there all this stuff comes down here
your standard deviation your minimum
value your maximum value what's in the
lower quarter fifty percent Mark where's
that line at and what's in the upper 75
percent the top 25 percent going into
the max
now this next part is just cool this is
what we always wanted computers be back
like in the 90s instead of 5 000 lines
of code to do this maybe not five
thousand all right I built my own plot
Library back in 95 and the amount of
code for doing a simple plot was um
I don't know probably about 100 lines of
code
this is being done in one line of code
we have our car which is our pandas we
generated that it's our data frame and
we have dot hist for histogram that is
the power of Seaborn now it's still
going to generate a numpy graph but
Seaborn sits on top and then we can do
the figure size this is just um so it
fits nicely on the paper on here and we
do something simple like this and you
can see here where it comes up and does
say matplot library and does subplots
and everything
but we're looking at a histogram of all
the different pieces in our database and
we have our engine cylinders that's
always a good one because you can see
like they have some that are they had a
null on there so they came out as zero
maybe a couple maybe one of them had a
two cylinder engine away back when four
is a common uh six a little less common
and then you see the eight cylinder 12
cylinder engines but it's got to be a
Speedster or something
uh but you can see right here just
breaks it down so now you have how many
cars with how many whatever it is
cylinders horsepower and so on and it
does a nice job displaying it
you can see if you're working with your
uh um you're going into your demo it's
really nice just to be able to type that
in and boom there it is it can see it
all the way across
and we might want to zero in and use
like a box plot and this time we'll go
ahead and call the Seaborn SNS box plot
and we're going to go ahead and do
vehicle size in versus engine horsepower
XY plot and the data comes from the car
so if we run this we end up with a nice
box plot
you see our mid-size Compact and large
you can see the variation there's our
outlier showing up there on the compact
that must be a high-end sports car a
large car might have a couple engines
and again we have all these outliers and
then your deviation on them
very powerful and quick way to zero in
on one small piece of data and display
it for people who need to have it
reduced to something they can see and
look at and understand and that's our
Seabourn box plot or sns.box plot
and then if we're going to back out and
we want a quick look at what they call
pair plotting we can run that and you
can see with the Seaborn it just does
all the work for you
uh it takes just a moment for it to pull
the data in and compile it
and once it does it creates a nice Grid
in this grid if you look at this one
space here which is you might not be
able to see the small number it says
engine horsepower this is engine
horsepower uh to the year was built and
it's just flipped so everything to the
right of the middle diagonal is just the
rotation of what's on the left and as
you expect the engine horsepower gets
bigger and bigger and bigger as time
goes on so the the year was built the
further up in the year the more likely
you are to have a heavy horsepower
engine
and you can quickly look at trends
with our pair plot coming up and look
how fast that was that was it took a
couple moment to process but right away
I get a nice view of all these different
information which I can look at visually
in in kind of see how things group and
look
now if I was doing a meeting I probably
wouldn't show all the data
one of the things I've learned over the
years is people myself included love to
show all our work you know we were
taught in school show all your work
prove what you know the CEO doesn't want
to see a huge grid of graphs I guarantee
it so we want to do is we want to go
ahead and drop
um the stuff that might not be
interested in and we're gonna I'm not
really a car person a guy in the back is
obviously so you have your engine fuel
type we're going to drop that we're
going to drop Market category vehicle
style popularity number of doors vehicle
size and we have the axes in here if you
remember from numpy we have to include
that axis to make it clear what we're
working on that's also true with pandas
and then we'll look at just what it
looks like um from the head and you can
see that we dropped out those categories
and now we have the make model year and
so forth and we took out the engine fuel
type Market category Etc
and this should look familiar to you now
when you start working with pandas I
just love pandas for this reason look
how easy it is it just displays it as a
nice spreadsheet for you you can just
look at it and view it very easily it's
also the same kind of view you're going
to get if you're working in spark or Pi
spark which is python for spark across
Big Data this is the kind of thing that
they they come up with this is why
pandas is so powerful
and we may look at this and decide we
don't like these columns and so you can
go in here and we can actually rename
the columns
simple command car equals car rename
columns equals engine horsepower equals
horsepower this is just your standard
python dictionary so it just Maps them
out and you know instead of having like
a lengthy if it here we had engine
horsepower we just won horsepower we
don't need to know it's the engine
horsepower
engine cylinders we don't need to know
that it's for the engine because there's
only one thing we're describing if we're
talking about cars and that cylinders
and we'll go ahead and just run this and
again here's our car head and you can
see how that changed we have model year
and horsepower versus model year engine
horsepower engine cylinders and just
cylinders
again we want to keep reducing this so
it's more and more readable the more
readable you get it the better and of
course we can also adjust the size a
little bit
so that when it prints out instead of
splitting it on two lines we get like a
single line we can do that also that's
just your control Mouse app or plus sign
you use in Chrome that's a chrome
command
and if you remember from numpy we had
shape well pandas works the same way we
can look at the shape of the data so we
now have 11 914 rows in 10 columns so
you'll see some similarities because
pandas is built on numpy
and questions that come up just like you
did in numpy we might want to know
duplicate rows and so we can do car and
look at this switch here
um we're doing a selection this is a
panda selection with the brackets
but we want to select it based on
car.duplicated so how many duplicates on
there
so it's starting to look a little bit
different as far as how we access some
of the data in here this can be a
logical statement and we get the number
of duplicate rows we have 989 rows by 10
columns again
and this is one of those troubleshooting
things that we end up doing a lot more
than we really feel like we should we
might go ahead and do like a car count
just to see how many rows we're dealing
with and then right after that we might
want to go ahead and say hey let's drop
duplicates so remember we did all the
duplicates on there so car equals car
dot drop duplicates and then we can
print the head again we'll just do car
head here and you can see the data on
there looks the same as before
and just note that we did car equals car
draw duplicates there are commands in
here where you can do where it changes
the actual value and it works on some of
them and not on others depending on what
you're doing but by default it always
returns a copy so when we do this we're
reassigning it to car
and you can see it's the same header but
we want to go ahead and do count and see
how the count changes let's go ahead and
run this and you can see here instead of
11 914 we have 10
925. so we've removed about a hundred
cars that were duplicated just slightly
under 100 there
and then as we're prepping our data we
might want to know
um car is null so it's going to count
the values of null and then we want to
sum that up and when we do that we do
the car is no function.sum we end up
with HP the horsepower at 69 have null
values and 30 have cylinders have no
values now if you don't put the sum at
the end it's just going to return a mask
with the true false of is it null or is
it not by zero and one so you're summing
up the ones underneath each column
and this of course then you have to
decide what you're going to do with the
null values there's a lot of different
options it might be that you need to put
in the average or means
maybe you want to put in the median
value there's a lot of different ways to
fill it usually when you first start out
with the data a lot of them you just
drop your null values and you can see
here car dot drop in a
which is equal to all and then we're
going to go ahead and count it and you
can see that we've dropped almost
another 100 values so from 10 1925 to 10
8 27.
yeah maybe 75 or so values
so we've cleaned that this is really a
big part of cleaning data you need to
know how to get rid of your null values
or at least count them and what to do
with them
and of course if we go back to um
counting our null values we should now
have
null null values there we go and you'll
see there's zero null values
I don't know how many times I've been
running a model that doesn't take null
values and it crashes and I just sit
there and look at it trying to get why
did that crash it should have worked
it's because I forgot to remove the null
values
so even jumping around a lot we're going
to go back to finding outliers and let's
go ahead and bring that back into our
Seaborn and if you remember we did a box
plot earlier this time we're going to do
a box plot just on the price and you can
see here our price value
and we have the deviation with the two
thinner bars on each side of the main
value and then as we get up here we have
all these outliers in fact we have one
way out here that's um probably a really
expensive high-end car is what we're
looking at
if you were doing fraud analysis you
would be jumping on all over these
outliers why are these deviation from
the standard what are these people doing
again this is probably like I said a
really high-end expensive car out here
that's what we're looking at and we can
also look at the box plot for the
horsepower
and we'll put that in down here
and run that
and you can see again here's our
horsepower and it just jumps and there's
these really odd huge muscle cars out
here that are outliers
and we're going to jump into making this
a little bit more as we started
displaying your data your information to
your shareholders we're going to look at
plotting a histogram for the number of
cars per brand
and the first thing we want to go ahead
and do is we have with our car go back
over here here we go we have our make
value counts largest plot and we want to
do a kind equals bar
fig size 10-5
and right off the bat we jump up here we
see Chevrolet it's going against what
was it it's um figure resolution the
value counts and we want the largest
value so here's our value counts and
compared to what the different cars are
Chevrolet puts out a lot of different
kinds of cars I didn't realize that they
made that many cars or different types
and then for readability let's go ahead
and add a title number of cars by make
number of cars and make if you had
looked at this the first time you would
have been like well what the heck am I
looking at well we're looking at the
number of cars by make and then you can
see here now we're talking about the
type of cars and the different uh ones
are put out Lotus I guess only had a few
different kinds of cars over there very
high-end cars
and then as uh doing data analytics and
as a data scientist one of the things I
am most interested in is the
relationship between the variables
so this is always a place to start we
want to know what's going on with our
variables and how they connect with each
other
so the first thing we're going to do is
we're going to go ahead and set a figure
size because we want to make sure it
fits our graph we'll just go ahead and
set this one plot Figure Set to figure
size 2010. if you never use the matplot
library which is sitting behind Seaborn
whatever is in the PLT this is what's
loaded it's like a canvas you're
painting on so the second you load that
Pi plot as PLT anything you do to that
is affecting everything on it
and then we want to go ahead since we're
using Seabourn
we'll go ahead and create a variable C
for relationships or correspondence
and car dot c-o-r-r that's a correlation
in Seabourn on top of pandas again one
line and you get the whole correlation
on there
and because we're working with Seabourn
let's put it into a nice heat map if
you're not familiar with heat maps that
means we're just using color as part of
our um
setup so we have a nice visual
and we can see here that the Seaborn
connected to the pandas prints out a
nice chart
we'll talk a little bit about the color
here in a second it prints out a nice
chart this is a chart I look at as a
data scientist these are the numbers I
want to look at and we'll just highlight
one of them here's cylinders versus
horsepower the closer to one the higher
the correlation so 0.788 pretty high
correlation between the number of
cylinders and how heavy the horsepower
is
I'm betting if you looked at the year
versus horsepower we just look at that
one here's year in Horsepower 0.314 not
as so much but if you combine them you
don't actually add them but if you
combine them you'll start to see an
increase in Horsepower per year and
cylinders you could probably get a
correlation there and just like 0.78 is
a positive correlation you might notice
if we look at cylinders
and or let's look at horsepower and
mileage so if we go here to horsepower
to mileage you get a nice negative we'll
do cylinders that's a bigger number
with cylinders to the miles per gallon
it's a minus 0.6 so it's a negative
correlation the closer to -1 the more
the negative correlation is
and then the chart you would actually
show people is a nice heat map this is
all our colors and it's just those
numbers put into a heat map the darker
the color the higher the correlation you
can see straight down the middle
obviously the year correlates strictly
with the year horsepower with horsepower
and so on that's why it's a one the
closer to the one the higher the
correlation between the two pieces of
data
now this is a good introduction pandas
goes Way Beyond this most the
functionality and numpy since pandas
sits on it is also in pandas and then it
even has additional features in it and
we use Seaborn pretty extensively
sitting on top over our pie plot so keep
in mind that our PI plot has a ton of
other features in it that we didn't even
touch on in here we couldn't even if you
had a soul course in it there's just so
many things hidden in there depending on
what your domain you're working on but
you can see here here's our Seaborn and
here's our matplot library that's all
our Graphics that we did and then the
Seaborn works really nicely with the
pandas we really like that
what is time series forecasting
making scientific projection based on
the data with historical timestamps is
known as time series forecasting it
entails creating model through
historical study using them to draw a
conclusion and guide strategic decision
making in the future
the fact that the future result is
wholly unknown at the time of the task
and can only be anticipated through
analysis and evidence-based priors is an
essential decision in forecasting give
yourself a chance to implement
professional certificate program in Ai
and machine learning which comes with
completion certificate and in-depth
knowledge of AI and machine learning
check this course detail from the
description box below
so here is one question for you guys I
will give you exactly one minute for
this you can comment or you can give
your answer in the chat section so I can
see if the answer is given by you are
right or wrong
okay
so the question is which type of
programming does python support
I'm repeating again which type of
programming does python support
oriented programming option b structured
programming option C functional
programming and option D all of the
above
so let us know in your answer in the
chat section or in the comment section
so I am starting a timer of one minute
just type your answer in the comment
section or in the chat section
do let me know your answers please
so I am starting the timer of one minute
this type of programming does python
support object oriented structured
programming functional or all of the
above
do let me know your answers please
you can comment or you can give your
answer in the chat section so I can see
if the answer is given by you are right
or wrong
which type of programming does python
support object oriented structured
functional or all of the above
30 seconds meaning
this type of programming does python
support object oriented structured
function or all of the above
let us know your answers in the chat
section or in the comment section below
and seconds more
which type of programming does python
support
five seconds more
so the allotted time is over we will
give a reply to those who gave the
correct answer and for those who didn't
give the correct answer we will give you
reply with the correct answer okay
now let's move to our programming part
so we will open command prompt to write
a command to open Jupiter notebook so
here I will write
Jupiter
notebook
that's okay Jupiter
notebook press enter
will take time
it's open
so this is the landing page of Jupiter
notebook and here I will select new
python kernel file
so this is how Jupiter kernel look likes
so
here
what we will do we will import some
major libraries of python which will
help us in analyzing the data okay
import numpy
as NP
okay then import
pandas
as PD
then import
C bone
as SNS
the fourth one is from
matplotlib
port
as PLT
okay
then we will import some model libraries
so here I will add from
stats
models
dot TSA
dot API
import
ant shell
muting
and comma then
simple
XP smoothing
and one more
hold
we will write here import sorry from
skill on dot linear
underscore model
import
linear
regression
okay
then import
ance
earnings
then we will already have warnings
Dot
filter warnings
should be ignored
yes there will be no error
oh it's still loading yeah
so numpy numpy is a python Library used
for working with arrays it also has a
function for working with the domain of
linear algebra and matrices
it is an open source project and you can
use it freely numpy stands for numerical
python
second is Panda
pandas is a software Library written for
the Python programming language for data
manipulation and Analysis in particular
it offers data structure and operations
for manipulating numerical data and Time
series
than Seaborn an open source python
Library based on matplotlib is called
c-bone
it is utilized for data exploration and
data visualization
with data frames and the pandas
liability c bond function with ease
matplotlib for Python and its numerical
extension numpy mat problem is a cross
platform data visualization and
graphical charting package
as a result it presents a strong open
source substitute for matpler
the apis format plot lab allow
programmers to incorporate graphs into
GUI applications
linear regression the machine learning
method regression built on linear
supervised learning
analysis regression is done
regression creates a value for the aim
prediction using independent variables
as inputs
its main goal is to investigate the
relationship between factors and
forecasting
exponential smoothing
the exponential Windows function is a
general method for smoothing time series
data known as exponential smoothing
it contrasts to the ordinary moving
average which weights previous data
quality
exponential function use weights that
decrease exponentially with time
and there is one more simple exponential
smoothing
the simple exponential smoothing classes
use Simple exponential smoothing models
to give very simple time serial analysis
a weight average of the most recent
value and the preceding smooth value
constitute the predicted value
the contribution of older value degrade
exponentially as a result of the
smoothing parameter
okay so after importing libraries let's
import data set
for this we I will write here DF
or you can write data frame PD Dot
read
underscore CSV
here I will write
monthly
underscore CSV
dot CSV
okay this is my file name
you can download this file from the
description box below
and
for seeing the data I will write here DF
Dot
head
then press enter
yeah so here PD is for pandas Library
read is used for reading the data set
from the machine and CSV is used for the
type of file which you are using okay or
whichever you want to read
and if you want to see the top 5 rows of
your data set you can use head and if
you want to see the last five rows of
your data set you can use tail instead
of head
okay this one you can write your tail
so moving forward let's see how many
rows and columns are present in our data
set for that I have to write DF dot
shape
okay
then press enter
okay it will give error why because
this yeah
so here you can see 847 rows and two
columns date and price only
so moving forward let's do some Eda
exploratory data analysis okay for that
I have to write here print
data
date range
of
of gold prices
available from
and here I have to give curly brackets
then DF dot location Loc
and colon
comma
date
okay D is capital here so I have to
write
date
and from location 0
to
again same thing DF Dot
hello C
date
to the length of
EF
minus one
but we'll have everything seems good
yeah
then press enter
okay
we have date range of gold prices
available from this 1950 to 2020. okay
in our data set
and here I will write date
equals to PD Dot
date
range
start from
slash one slash 1950.
comma
and it goes to
each slash
one slash 2020.
comma frequency
virtual media
okay then
then here I will write date
then press enter
yeah
so here you can see date time index
okay
from starting to land these dot dots
and here I will write
DF
and
was to
date
we have dot drop
date
comma axis is 1
comma in place
also true
T should be capital
okay then DF equals to DF
dot set
index
and month
and DF
Dot
head
press enter
so instead of
this state
we have adjusted this month
okay for particular value
so moving forward let's see the graph
different different graphs okay
TF Dot Plot
and figure set should be
equals to
20 comma 8.
then plot
dot title
is
like gold
prices
monthly
since
1950
and onwards
okay title should be this
then X level should be PLT dot X label
will be months
then PLT dot y labels
should be
price
and I will write a PLT Dot
grid
okay
press enter
okay title spelling my bad t i t l e
okay then press enter
oh why label it is sorry
here you can see
gold prices monthly since 1950 and
onwards
okay
till from 1950 and from till 2020 okay
this is the price
then moving forward let's see another
graph so I will write here for that
round
DF Dot describe
comma 3.
and here's you can see the count
variable and the average is 416.557
standard deviation that minimum price
value is this 20 this and the maximum
value is this okay
so
so the average gold price in last 70
years is this Force 16.557 okay
only 25 percent of the time the gold
price is above 447.
the highest gold price ever does this
this one 1840 807.
so we will do visual analysis so here I
will write
ax equals to
PLT Dot subplots
sub plots
okay then figure size
pursue
25
comma odd
okay then SNS dot box plot
is X
equals to
DF dot index
dot ear
comma y equals to DF dot values
colon comma 0
comma ax equals to X
okay
then same PLT dot title
gold price
monthly
and in 50.
in words
same graph will come but in the
different format okay
let me remove this
then PLT Dot
X label
table must be here
LT dot y level
ice
and
PLT Dot
X ticks
will be rotation
okay I will give rotation 90.
PLT dot grid
you can write here instead of grid I can
write here direct issue so grid is this
format this box format okay
so press enter
it's loading yeah
so here you can see from 1950 every year
is here till 2020 so how the gold prices
are decreasing and increasing okay
let's see the another graph
I will write here from
ads
models
models
Dot
graphics
Dot
TSA plots
port
okay
then I will write a figure comma ax
equals to
then PLT Dot
subplots
bigger size
it goes to
22 comma 8.
okay
then month plot
TF comma y label
pursue
gold
price
okay comma
then ax equals to ax
and I will give the title PLT Dot
title
I will copy from here
and PLT Dot X label
month
and PLT dot y label
the price
PLT dot grid
so here you can see gold prices monthly
since 1950. like for every month like
January February March April so on till
December okay
we will cover one more graph
okay and many many more graphs
so
so we will
go with the next graph
for that I will add here
let's go
comma ax
equals to
PLT Dot subplots
and figure size
was to
22 comma 8
10 same SNS
dot box plot
x equals to
DF dot index
dot month name
okay
then y equals to
DF dot values
comma 0
comma ax
equals to ax
okay then PLT dot title
will put same title so I can copy from
it here
let's copy from here and paste it here
okay
and PLT Dot
X label
and PLT dot y label
price
and PLT dot grid
okay
dot show
this time let's not use grid
okay
okay something videos month name
yes
you can see
for every month
this is another type of graph
okay box plot graph
so why we are creating so much graphs
because we are doing Eda exploratory
data analysis in this we have to see the
multiple different and different
different types of
graph
okay so moving forward let's see average
gold price per year like Trend since
1950.
so for that I have to write here DF
let's go early
let's go some
question
DF Dot resample
sample
then
a
dot mean
DF underscore
yearly
underscore sum
Dot Plot
PLT Dot title
here I will write average
gold price
yearly
since
1950
or you can write onwards from onwards
1950
so here I will write PLT dot X label
here
and PLT dot y label
ice
okay
then PLT dot grid this time we'll use
grid
so here you can see the average gold
price early since 1950. okay this is the
chart till 2020 like sometimes up
sometimes down okay
and we will see now like average gold
price per quarter
like trends like since 1950
so here I will write DF Dot
not DOT do you have dot quarterly
okay
underscore sum
equals to
DF Dot resample
order queue
dot mean
yeah
oh okay
move this
then DF underscore
quarterly
scores sum
Dot Plot
and the same PLT dot title
from here
here I will write average good price
waterly
quarterly
okay since 1950
then PLT dot X label
label is now here quarter
PLT dot y label
this price
now PLT dot shown
at this time
okay
EF underscore quietly
okay quarterly
yeah
so here you can see the price prediction
okay let me set to the grid only
it's not this
visuals are not good so great
yeah
so here you can see the quarterly
process prediction
okay average gold price quarterly
prediction
so like moving forward
we will see now average gold price per
decade like per 10 years okay
so from 1950 only so here I will write
TF underscore decade
underscore sum
equals to DF Dot resample
every 10 years okay 10 year
dot mean
we are writing mean because we are like
putting out average
okay
so DF underscore decade
underscore sum
Dot Plot
PLT dot title
average
goal
price
per decade
since
1950 okay
it's since 1950 yeah perfect
so here I will get PLT Dot
X label
then
decade decade is off like every 10 year
PLT dot y label
PLT dot grid
so here you can see the average gold
price per decade like from 1950 to 1960
like this straight then against it then
up then sometimes down then up and down
okay every 10 years you can see a 1990
2000 2010 and 2020.
so moving forward let's do like analysis
in coefficient of variation
the coefficient of variation CV is a
statical measure of the relative
dispersion of data points in a data
series around the mean
and in finance the coefficient of
variation allows investor to determine
how much mortality
or risk is assumed in comparison of the
amount
like amount to the return expected from
investors
okay the lower the ratio of the standard
deviation to mean return the better risk
return trade-off
okay let's like let us look now the CV
values for each year in gold prices
so CV means coefficient of variation in
prices okay
so here I will write DF underscore one
equals to DF Dot
Group by
F dot index
dot ER
dot mean
dot name
dot columns
equal to
private price
and
mean
then again DF underscore one equals to
DF underscore one
dot range
or what we can do instead of range V
again right merge
DF dot Group by
School
DF dot index
dot EO
in the deviation
rename
columns
equals to
ice
standard deviation
comma left
index
was so true true
then comma right index
was so true
okay
and here DF underscore one
it's cool
first two
EF underscore one
standard division
slash
DF underscore one
two
hundred
like Dot
round figure should be like 2 after
decimal how much
like numbers you want to see
then TF underscore one
dot hat
press enter yeah
so here you can see for every year I
have mean standard deviation
and this coefficient of variation
okay
for every year
so
like moving forward let's see the
average gold price per year again
so for that figure
underscore
figure Dot
comma ax
goes to PLT
dot subplot
figure size
15 comma
10.
then
I will write here DF underscore one
is
COV underscore
Dot Plot
PLT dot title
average goal
price
yearly
since
1950.
okay then PLT dot X label
is here
or PLT
dot y level
by label
coefficient of variation I am writing CV
in percent
okay
then PLT dot show
okay DF underscore invalid syntax
HTT The Dot Plot
K codf underscore one DF underscore one
right
here
let's syntax
okay
so here you can see average gold price
since 1950.
okay
this is like percentage CV in percent
okay
like
good
you can say the chart
so the CV value reached its highest in
1978 like somewhere here like 1980 1978
okay
like near to 25 percent which could have
made the asset as a highly risky but in
2020 the CV value is closer to five
percent which makes the asset variable
via good investment okay
so
now what we will do we will do time
series forecasting
okay we will train model we will build
model different different model we will
train and test split to build time
series forecasting model
so for that let me do like this first
yeah
so here I will write train
was to DF
TF dot index
dot per year
okay
equals to
2015
and for the testing we will write TF
DF dot index
dot ER
2015.
okay for training we are taking till
2015 for and for the testing we are
taking till 2020 from 2015. okay
then I will write here how many columns
present in train or test so for that I
will add print train
dot shape
print
test dot ship
press enter
792 rows and one column in train
training for training the model and 55
to test the model okay
so now let's see the training data and
testing data
so train
a
like square brackets so
price
Dot Plot
then figure size
equals to
13 comma
5.
and
font size should be
also 15.
ice
Dot Plot
bigger size
bigger size
let me give
same 13 comma 5
and font size
should be
okay then PLT dot let me add grid
LT dot create
then PLT
Dot agent
training
data
comma
test data
okay then PLT
or
so I will tell you what the legend is
okay
so here you can see the training data in
blue and the testing data okay this is
known as the legend this this portion
and
yeah
so here you can see month wise
okay till 2020.
and from 1950
right these are the prices
and this is the chart so moving forward
let's do model formation now okay
we will do two models lineal
linear regression and the name base one
okay so first we are
first we will go from
linear regression
for that I will add train underscore
time
assume I plus 1
.
four
I in range
brain
test underscore time equals to
I Plus
length like train
one for I in
range then length should be test
okay so for length
training time
or my length should be tested
press enter
okay this is the training and this for
the testing 792 rows and here 55 rows in
testing
so
alarm underscore
alarm is linear regression let me make
it capital
brain
equals to
make a copy
underscore
test equals to test copy
dot copy
then LR
okay
so LR
train
time
question train time
okay and LR
underscore test
time
equals to test time
okay
underscore is there
so here I will write LR equals to linear
regression
dot fit to the model
because LR train
and for the time
la
underscore train
ice
values
linear regression is not defined okay my
bad
L should be Capital yeah
so now see the graph
so test
underscore
prediction
let's go model
one
question
Allah Dot
predict
La underscore test
time
test
question
test
underscore predictions
score model
one
okay
let's create the graph figure
size should be
let's do
but in comma 6
.
PLT Dot Plot
brain
price
comma
label
should be
trained
okay then plot
PLT Dot Plot
test
ice
label
equals to this
PLT Dot Plot
then LR train or tests
podcast
label
equals to
regression on time okay regression on
time
then again PLT dot Legend
best
PLT dot get it
let's enter
okay
here I have to write predict
loading
one more error
price
okay
is the price
so here you can see regression on time
test data is this green one and this
training Returns the testing data
OK let's find the map now so for this
we'll write here def
meep
chill comma prediction
then return
Ed
P dot mean
abs
actual
action
ritual
I
100
comma 2.
okay
to give the
yeah
so for forgetting the map you have to
write a map underscore
order
let's go test equals to mape
test
price
Dot values
comma
test underscore
model one
okay
then print
is
percent three
or three f
then head up to percent
model
one underscore test
comma
percentage
press enter
test
okay
jewel is not defined
web test
real model one test
maybe just
values
the test
fictions
model one
okay here's a is small
so
map here you can see 29.76 0.
so you are a bit confused like what is
Me Maybe is a measure of prediction of
accuracy of a forecasting method in
statical model OKAY is a measure of
prediction accuracy of forecasting
method in statical model
and now
results
equals to PD Dot
data frame
test
map
in percent
score
model 1
comma index
plus two
regression
on time
if then I will print the results
okay D should be capital
real pandas has no attribute data frame
okay
model one is not defined
let's go test
when it's in there perhaps you gotta
forget a comma
Colin
model one okay
here you can see the test map regression
on time
so let's do with the name now we have to
perform the same pattern so what I will
do what I will write the code and get
back to you okay
so I'm done with the code so here you
can see I have same pattern train and
test copy
and this is the name forecast on the
test data so this is the line
and this is the training and this orange
one is testing
the same we have got me like 19.380
okay
so and regression is this
of the name model
so what we will do we will create now
final model of ours okay
and we will forecast final forecasting
will do
for that I have to write final
model
question
exponential
smoothing
okay DF comma
Trend equals to
iterative
comma seasonal
pursue additive
comma fit
smoothing
level
goes to 0.4
comma
smoothing
trend
0.3
comma smoothing
seasonal
0.6
okay press enter
exponential exponential
so map
go final underscore model is equal to
map
like DF
dice
dot values
comma final
let's go
model
Dot
fitted values
then print
if
from a map
oh
final underscore model
okay
so map is 17.24 which is quite good for
the final model
okay
so getting the prediction for the same
number of time stamps at the present
time in the test data so I will write
here predictions
equals to final
underscore model Dot
forecast
steps
equals to length DOT test
Center
now we will compute 95 percent of
confidence interval for the predicted
value so
said tkf equals to PD Dot
data frames
did
then
over
CI
prediction
1.96
into
NP dot standard deviation of final
model
Dot
one
okay
and comma
right here prediction
okay then upper CI
action
Plus
1.96
into
dot standard deviation
the same
final model
but
so one
okay
then prediction
underscore DF dot at
so this is lowerci prediction and the
upper CI okay how much it will forecast
or
now
at the end at the final State what we
will do we will plot a graph okay
forecast graph along with the confidence
band
so for that X is equals to DF Dot Plot
label
pursue
actual
comma figure size
16
comma 9
addiction DF
n
Dot Plot
x equals to
this
comma label
underscore DF
it's lower CI
what I will do
instead of this
label equals to
forecast
comma Alpha
equals to 0.5
this
dot l
between
underscore DF
dot index
comma prediction underscore DF
to 1 underscore CI
comma prediction
underscore DF
this CI
then
color
question
um
comma
Alpha
virtual
Point fifteen
okay
then X is
dot set
underscore
X label
year month
axis
dot set
y label
it's
PLT dot Legend
should be location also
best
then PLT dot grid
PLT dot show
okay this is PLT only
then press enter
okay 16 comma 9 position argument
follows keyword argument
15.
oh
okay okay I have to give you
Oppo CI
alright upper CI
so here you can see our final model
forecasting so till 2028 is showing like
normal and after that till 2030
okay this will be the forecast as per
the data
okay
so
here you can see
as you can see we have the map is
17.24
okay then here I did the prediction for
the testing data
and here I've created the data frames
these are the data frames lower CI
prediction and upper CI and then I have
created
C and then I have created the final
graph of the forecasting
Okay so
so I hope you guys must have understood
like how you can do time series
forecasting using machine learning or
python
like if you have any queries you can ask
in the comment section our team will
respond you as soon as possible or if
you want this full code this full code
just comment down for the same
and you can download the data set from
the description box below and don't
forget to check the course link from the
description box below in this video we
are going to perform a really
interesting data analysis using python
on the Spotify music streaming service
platform data set
I'll also be asking you a few questions
related to Spotify during our discussion
please make sure to answer them in the
comment section of the video
so now let's get started
Spotify is a Swedish audio streaming and
media services provider founded in April
2006.
it is the world's largest music
streaming service provider and has over
381 million monthly active users which
also includes 172 million paid
subscribers the total number of
downloads on the Spotify app in the
Android store exceeded 1 billion in May
2021
so millions of people listen to music
all day even I am hooked to music as an
analyst what's better than exploring and
quantifying data about music and drawing
valuable insights before I move ahead I
have a quiz question for you people
the name Spotify comes from a
combination of two words so which are
those two words please let us know your
thoughts in the comment section below I
would like to repeat the question again
the name Spotify comes from a
combination of two words so what are
those two words we would love to hear
from you so please put your answers in
the comment section below
now let's use Python libraries and
functions to analyze and visualize our
data set
first I'll show you the two data sets
that we'll be using
so here is the first data set that we'll
be using for our demo and then I have my
second data set called Spotify features
which is essentially about the journals
of the different soundtracks
now these data sets have been downloaded
from kegel.com now the Links to the data
sets have been provided in the
description box please go ahead and
download them now let me just go ahead
and brief you about the columns that are
present in our first data set which is
about tracks we have a column A which is
ID this is the unique ID for each of the
songs then we have the name column which
is essentially the name of the song
then we have a column for popularity so
the popularity ranges from 0 to 100 then
we have duration in milliseconds this is
the duration of the track in
milliseconds next we have a column
called explicit now we are not bothered
about this column because we are not
going to use it in our analysis
then we have artists so the name of the
artist who has composed or sung the song
then we have ID of the artist then we
have a column for release date which is
basically the date on which the song was
released then we have a column for
danceability so this describes how
suitable a track is for dancing based on
a combination of musical elements such
as Tempo Rhythm stability beat strength
and overall regularity the value ranges
between 0 and 1.
next we have a column for energy so the
energy is a measure between 0.0 to 1.0
and represents a perceptual measure of
intensity and activity typically the
energetic tracks field fast loud and
noisy higher the value the more
energetic is the sound
then we have a column for key so key is
the pitch notes or scale of song that
forms the basis of a song there are 12
Keys ranging from 0 to 11.
moving ahead we have loudness so the
overall loudness of the track in
decibels it ranges from minus 60 to 0
decibels
then we have mode
so songs can be classified as measure
and minor 1.0 represents major or one
represents major and 0 represents minor
next we have
speechiness so speechiness recognizes
the presence of spoken words in a track
more exclusive speech like the recording
example talk show audiobook or poetry
the closer to 1.0 the attribute value
then we have a column for
the cost thickness
so a confidence measure of 0 to 1 of
whether the track is acoustic or not so
1.0 represents high confidence the track
is acoustic then we have other
information about
instrumentalness
then we also have a column for
liveness so liveness detects the
presence of an audience in the recording
then we have a column for valence so
balance is a measure between 0.0 to 1.0
and describes the musical positiveness
conveyed by a track or a song
and finally we have
The Columns for Tempo and time signature
now
even in the second data set you have
almost the same columns just that we
have an additional column that is about
the genre of the songs present in the
data set cool now let's head over to our
Jupiter notebook and we'll start with
our analysis
okay so one more thing to remember our
data has information from
1922 onwards so all the songs from 1922
till 2021
cool okay so I am on my Jupiter notebook
so
you can see I have a few cells that have
already been filled up
so we'll start with our analysis first
of all let's go ahead and import the
necessary libraries so I'm importing
numpy pandas matplotlib and c bond for
my analysis and visualization I'll hit
shift enter to import the libraries all
right and in the next cell I'm going to
load my data set using the pandas read
underscore CSV function I have my
location already
put here let me show you the location
where the data files or the data sets
are located so this is my location on
the Chrome downloads I have a folder
called Spotify datasets
okay
so let's import and
set the first five rows in the data set
so for that I've used the head function
there you go so here you can see I have
my first five rows of information from
the data set and on the top you can see
the different columns you have ID name
popularity
artist
then we have the release date
danceability energy key loudness
liveliness or liveness
balance Tempo and other information
cool
now
let's check for null values in the data
set I'll just give a comment as
null values
every time when you download a data set
from an open repository there are
chances that the data set would contain
null values so it's better to check them
beforehand
so I'm going to use the original
function present in the pandas Library
PD I'm using because I had imported
pandas SPD
so PD dot is null then I'm going to use
the variable name DF underscore tracks
because I imported my data set and
stored it in the variable DF underscore
tracks
so I have my data frame under
DF underscore tracks variable
and then I'll use the sum function to
check the total number of null values
present in the data set for each of the
columns if I run it there you go
so here you can see my name column has
71
missing values or null values and we
don't have any null values for the rest
of the columns
okay
now let's use the info method that will
give us the total number of rows and
columns in the data set and we'll also
check the data types and the memory
usage so I'll see
my data frame name that is DF underscore
tracks dot info
if I run it you can see here
if you mark there are total 5 lakh 86
601 names or
the names of the songs present in the
data set while the rest all have 5 lakhs
86 672 so clearly there are total 71
song names or soundtracks missing from
our data set
and Below you can see the data types
where float integer and object and then
you can see the memory usage
cool
now before I move ahead with our next
analysis I have another question for you
which artist or musician has the most
number of follows on Spotify I repeat
which artist or musician has the most
number of follows on Spotify please put
your answer in the comment section below
we would be happy to hear from you
now let's
move ahead and do our first major
analysis in this demo we are going to
find the 10 least popular
songs present in the Spotify data set so
I'll create a variable called sorted
underscore DF
that will be equal to my data frame name
that is DF underscore tracks dot I am
going to use the sort underscore
values function and say
my column name is popularity
so I am going to sort the values based
on the
popularity and then say
ascending equal to true
since I want
only the least popular songs
and then
I'm going to say
head of 10 which means I want the top 10
least popular songs now let's go ahead
and print sorted underscore dear if I
run it you can see
we have the list of
10 least popular songs on Spotify you
can see that popularity is zero
and you can see the names of the songs
some of them are
songs which are not an English language
and you can see the artist names as well
cool
now moving ahead
let's see some descriptive statistics
for the numerical variables that are
present in our column so I'll say DF
underscore tracks dot describe
which is the function to get some
descriptive statistics and I'm going to
use the transpose function after that
if I run it there you go so we have the
statistics about count mean standard
deviation
minimum value 25th percentile 50 percent
75th percentile and the maximum value
for these columns like popularity
duration in milliseconds then we have
energy key loudness mood
cool now if you see this
popularity column the minimum value is 0
and the maximum value is
100 and you can see the 50th percentile
is 27 which is essentially the median
you have the standard deviation as
18.37 then similarly you can check for
the other features as well
cool
now
we'll see the
10 most popular songs which are greater
than 90. so we are going to check for
the top 10 songs with popularity greater
than 90 let me show you how to do it
so in this cell I am going to create a
new variable called most popular
and I'll say DF underscore tracks dot
this time I'm going to use the
query function that is part of pandas
Library again
I'll use the
column that is popularity and we'll set
the condition popularity should be
greater than 90 I'll give a comma and
say
in place equal to false because I don't
want to change my original data frame
and then I'll say
sort underscore
values
and
I'm going to sort it based on
popularity
in
descending order so I'll say
ascending equal to false
and then let's
take only the
top 10 popular songs so I'll say most
popular use square brackets
use square brackets and
will then pass the slicing operator and
say colon 10
now if I run this there you go
so here you can see the 10 most popular
songs that is present in our Spotify
data set first we have features by
Justin Bieber
Daniel Caesar and given then we also
have a song name called driver's license
astronaut in the ocean
save your tears
we also have the business streets and
heartbreak anniversary so these are the
most popular songs that are present in
our data set based on the
popularity you can see Peaches has the
highest popularity with 100.
all right now moving to the next cell
so here we are going to set the index to
be release date column in the main data
frame so I am setting my index using the
set underscore index function I have
passed in my column name as release date
and I'm saying increase equal to True
which means I want to change it in my
original data frame and then I'm
changing
the value to date time format and let's
print the head of the
data set so you can see here we have
successfully changed our index now here
you can see instead of 0 1 2 3 we have
the release date column and the rest of
the columns are intact
cool
now let's move ahead
so suppose you want to check the artist
at the 18th Row in our data set you can
use the index location method for that
let me show you how to
filter only specific rows of information
from the data set I'll use my data frame
DF underscore tracks and using double
square brackets I'll say
by column name which is artist
and I'll use the index location method
and see let's say I want to check the
artist who is present
in the 18th Row in my data set so I'll
use ilock 18 if I run it the artist's
name is
Victor voucher
cool
now let's move ahead we are going to
convert the duration in milliseconds to
just seconds
so
if you see our data set we have a column
called duration in milliseconds so all
our songs are present in milliseconds
let's convert them into just seconds so
for that I'm using the Lambda function
and dividing the
values in
milliseconds by thousand so that they
get converted into just seconds
I have used in place equal to true so I
want to change it in my original data
frame let's run it and do the necessary
changes all right
now we'll print the head of the data set
just to check the duration column so
I'll say DF underscore tracks
Dot duration
dot head
if I run it there you go
so
you can see the values have now being
changed to just seconds
cool
I have the final quiz question for you
who has the most monthly listeners on
Spotify
please put your answers in the comment
section below we'd be glad to hear from
you
now coming to the next cell
so here we are going to create a first
visualization that is going to be a
correlation map
we are going to drop three unwanted
columns and those are key mode and
explicit and then we are going to apply
Pearson correlation method
now I have set my figure size to 14
comma 6 and then we are using the C bone
heat map function to create our
correlation map
I have put the variable that is
correlation underscore DF you can see
above we had created this and then I'm
setting annotation equal to true
so
this will write the data value in each
cell
I have set fmt equal to dot 1
G so this is a string formatting code to
use when adding annotations then I have
set my V Min and V Max so these are the
values to Anchor the color map otherwise
they are inferred from the data and
other keyword arguments
cmap here stands for color map you can
just search for SNS cmap you will get
the documentation
so you can choose whichever
color palette or the color map you want
here I have used Inferno
and I have set my line bits and line
color finally I am giving a title to my
correlation map and I have set the
acetic labels let's go ahead and
run this together
first visualization
if I scroll down there you go we have
got a
nice
correlation map so here on the right
side you can see the scale it ranges
from -1 to plus 1 minus 1 means the
variables have least or negative
correlation while the values which are
above 0.0
means that the variables have a positive
correlation
now
here you can see there are
values like minus 0.7 for energy and
acousticness which means if the energy
is high the acousticness is daily low
again for loudness and
if the song is loud the speechiness is
low so there is negative correlation but
if you see
for energy and loudness there is really
high correlation between these two
variables you can see the value is 0.8
so if the song is loud this implies the
song has really high energy and vice
versa
now if you see for a few other variables
that is negative correlation between
acousticness and dance ability
there is negative correlation again
between
valence and
acousticness similarly there is positive
correlation between energy and balance
which is 0.4
and even for danceability and balance
that is positive correlation which is
0.5
cool
so from the correlation heat map you can
note that acousticness appears to have a
strong negative correlation with energy
so if you see for acousticness and
energy there is a strong negative
correlation
and there is a moderately strong
positive relation between loudness and
popularity so if you see for popularity
and loudness the color is orange which
means it lies in this positive region
and there is also a moderately strong
positive relation between danceability
and balance so if you check for
danceability and balance here there is
uh moderately strong positive
correlation
all right
now let's move ahead we are going to
sample our data and take just 0.4
percent of the total data and will
create two regression plots using this
data so let me first sample my data so
I'll create a sample data frame
using my original data frame which is DF
underscore tracks and C
ycle
I am going to
use the int function and see 0.004
multiplied by the length of my original
data frame which is DF underscore
tracks
all right
now let's
run this
and will print
the length of my
sample data frame
to run it you can see
point
four percent of
my total data set is
2346 rows
cool
now we are going to create a recreation
plot between loudness and energy now in
our correlation map we saw there was a
positive correlation between loudness
and energy which was 0.8 let's plot it
in the form of a regression line
so I'll use
PLT Dot
figure
I'll set my
figure size
equal to let's say 10 comma 6.
I'll say SNS Dot
regression plot so I'm using the
function called reg plot
and I'll use my data as
sample underscore
TF
give a comma
and say
in my y-axis I'll have my column
loudness
and in the x axis we'll have
energy
I'll give a
color to
my
data variable let's say the color is C
then I'll set my
title to
loudness
versus
energy
let's say correlation
all right
let's make sure everything is
fine
now
we'll run and see a result
there you go
so you can see here clearly
there is a very high positive
correlation between loudness and energy
on the y-axis we have loudness and on
the x axis we have energy and you can
see
all the data points or these songs are
in One Direction so if the energy
increases the loudness of this song also
increases and similarly if the loudness
of the song decreases your energy of the
song or the track also decreases so
there is a very high positive
correlation and you can see the
regression line here it has gone
and is increasing gradually
cool
now similarly I'll just copy this code
and we are going to see another
regression plot this time for two
different features
let's say we have
popularity in the
y-axis so I'll say popularity
and then in the x axis we have let's say
acousticness
I'll change the color to let's say B
which stands for blue
and
will set the title to
popularity
versus
I'll have
across thickness
correlation
just scroll down and we'll run it to see
the result
there you go
so I have the
different points for the songs and here
you can see the regression line is
downwards which means if the
acousticness of the song increases the
popularity decreases and similarly if
the popularity increases acousticness
decreases you can see
the downward trend of the regression
line
all right
now in the next cell we are going to
create a new column called Year from our
release date column
and I have changed this to date time
format let me just run it
cool
now after that
we are going to create a distribution
plot to visualize the total number of
songs in each year since 1922 that is
available on the Spotify streaming app
so I have used my c bond library and
dist plot function now one thing to
remember you need to update your c bond
library to
this version
if you haven't done it so use this
command pip install dash dash user C
burn and
the version
so here in the distribution plot
we are going to plot a histogram so I
have used kind equal to hist which
stands for histogram
let's run and see the result
okay so here you can see I have my
distribution plot so the plot tells us
that the
number of songs for each year in the
data set according to their release date
have increased in the recent years since
music became more accessible to people
globally with technological advancements
so earlier you can see there were very
few songs available in the
1920s later on the
number of songs increased rapidly and
now you can see we have a lot more songs
available for people to listen
cool
now
we are going to see the duration of
songs over the years for that again we
are going to create a bar plot so I'll
first create a
variable called total duration equal to
DF underscore tracks Dot
I'll use the duration column that we
created with seconds
and then
I am going to
set
my
foreign
s so I'll use figure underscore
dims for Dimensions equal to
18 comma
7
after that I'll have my
figure
access defined so I'll use the
matplotlib
sub plots function
and
I'll set the
figure size
equal to
my figure dimensions
and then
I'll say figure equal to sns. Dot I'll
use the
bar plot function
and say
my x axis to be
years
my y-axis will be
total
duration or total underscore Dr that we
created here
I'll set my Axis equal to e x
and then
I'll set error width
equal to
false
let's set the title for my plot as
title equal to
your
bosses duration
and finally I'll say plt.x text
I rotate it by
let's say 90 degrees
all right
let me just recheck once if everything
is fine and then we'll go ahead and run
it
my Axis error width title
okay let me just run it
will see the result in a moment
if I scroll down
you can see we have the bar plot for the
different years and the duration of the
songs in seconds in the y-axis so
earlier in the 1920s you can see the
duration of the
songs with less and literate increased
around late
1930s and this remained consistent until
2010 where the
duration was high but after 2010 you can
see the
duration of the songs have started
decreasing
now in the next cell I am going to
create a line plot to analyze the
average duration of the songs over the
years it is going to be similar to
a bar plot does that now we are going to
visualize it in terms of a line
so I have my code ready
you can see here I've used my C bone
library and the line plot function in
the x-axis I have ears and in the y-axis
I have total duration I've set my title
to year versus duration and I'm rotating
my
X labels by 60 degrees let's run it and
we'll see the output
there you go now if I scroll down you
can see we have a nice
line plot and on the x-axis you have the
ears and on the y-axis we have the
duration you can see that the songs from
1920s to
1960s have comparatively shorter
duration since most of the songs tended
to be more singing Beast rather than
instrument Beast
after 1960s you can see the duration of
the songs started increasing until I
would say 2010 and in the present day
the duration of the songs have started
declining since the attention span of
the average listener is also declining
all right now let's move to our second
data analysis project which is based on
genres of the songs so I am importing my
data set using the pandas read
underscore CSV function I have given my
location and here I have my
data set name followed by the
extension of the
type so this is a CSV data set let's go
ahead and run it
okay now
let's print the first five rows of the
data set
I have
stored my data set in a data frame
called DF underscore genre
I'll use the head function to get the
first five rows of information
I'll hit shift enter to run it there you
go
you can see here I have my journal
column artist name track name track ID
popularity acousticness
duration in milliseconds again and we
have the rest of the other columns that
we saw in our first data set just one
thing to note here key is
present in terms of c d e c minor F
minor
and not in terms of numbers between 0 to
11.
now we'll see
the duration of the songs for different
genres for that I am going to create a
bar plot so I'll start with setting the
title for my plot as
duration of
the
songs
in
front
genres
I'll use the
c bond Library
and
I'll set my
color palette to let's see
rocket
into the comma and say
as cmap equal to true
this would be color palette
now
I'll say SNS Dot
bar plot in the y-axis I'll have
genre
and in the x axis I'll have
my duration column which is in
milliseconds so duration underscore MS
and then I pass my data frame using the
data argument
so I'll say data equal to DF dot Journal
next
will set the x labels so I'll say PLT
dot X label
let's see my X label is
duration in
milliseconds
and then
I'll see
PLT Dot
y label as autonomous
now let's go ahead and
click on this
there you go so here you can see we have
the different genres on the y-axis and
on the x axis you have the duration in
milliseconds
and if you see the graph
for
classical genre and for
songs that belong to World genre the
duration of the songs are
more compared to
other genres now if you check for
children's music genre the duration is
less or the least
cool
and finally
we'll move to our
last demo where we'll see the top five
genres by popularity
so I'll say SNS Dot set underscore
Style
I'll set my
style to
dark
grid which will be my background
and then I'll see PLT Dot
figure I'll set my
figure
size
to
10 comma 5.
then I'll create a variable called
famous
since I want to take only the most
popular songs based on the genre so I
have created a variable called famous
and I'll pass my data frame name that is
DF underscore
Journal and I'll sort the values based
on my popularity column
so I'll see
popularity
and I want to sort it in descending
order so I'll say ascending equal to
false
and I'll take the
first
10 values I'll tell you the reason why
I'm taking the first 10 values are not
5.
then I'll say sns.
bar plot
and in the y axis I'll have
genre
in the x axis will have
popularity
and I'll keep a comma
and using the data argument I will save
my data to be famous which is this
variable that we created
and then I'll set my
title as
top five
genres
by
popularity
all right
now the reason why I took head of 10 is
because there are a few genres which are
repetitive so if you see this we have
children's music appearing twice so
hence we have taken 10 instead of 5 let
me just go ahead and run it
there you go
so here if I scroll down you can see I
have my top 5 genres based on the
popularity so we have dance pop rap
hip-hop at reggaeton so these are the
five genres which are most popular based
on the data that we have collected
from Spotify
so today we will perform some
exploratory data analysis using python
libraries to analyze visualize and draw
insights from 2021 World happiness data
before I begin make sure to subscribe to
the simply launch Channel and hit the
Bell icon to never miss an update
first let's understand what the world
happiness report 2021 is all about
the international happiness day is
celebrated every year since 2013 on 20th
of March to emphasize the importance of
happiness in the daily lives of people
so the United Nations sustainable
development Solutions Network published
the world happiness report on 19th of
March 2021 that ranks the world's 149
countries on how happy the citizens
perceive themselves to be based on
various indicators
the happiness study ranks the countries
on the basis of questions from the
Gallup World Poll
the results are then equated with other
factors such as GDP life expectancy
generosity Etc
this year it focused on the effects of
the covid-19 pandemic and how people all
over the world have managed to survive
and prosper
so using this 2021 data we will answer
critical questions such as the top 10
most corrupt countries
we will plot a graph to understand how
the happiness score is related to the
freedom of making Life Choices will look
at the life expectancy of 10 happiest
and 10 least happy Nations so these are
a few examples but we will explore more
about the data in detail in our demo
session
let's get started so first I'll show you
the data set we'll be using in this demo
so this data has been collected from
Kegel let me show you that
so this is the csb data set that we have
downloaded from kegel.com so you can see
here World happiness report 2021
and
we will share the data set link in the
description of the video you can click
on the link to download the data set
now we have information about
149 countries you can see it here count
is 149 let me go to the top
and I'll run through the columns that
are there in this data set
so the First Column is the country name
so we have 149 different countries and
then we have something called as
Regional indicator we can call this as
just the region so you can see we have
different regions I've applied a filter
we have Central and Eastern Europe then
we have common wealth of independent
states so these include countries such
as Russia then we have East Asia Latin
America and Caribbean we also have South
Asia southeast Asia sub-Saharan Africa
Western Europe and other regions
I'll just cancel this and then we have
the
happiness score column that has been
sorted in descending order
so we have Finland Denmark and
Switzerland who are the top three
happiest Nations now if I scroll down we
have country slide Rwanda Zimbabwe and
Afghanistan which are the
least three happy countries
now
there are a few columns that we won't be
using in our analysis so we will learn
how to exclude those columns and keep
only the relevant ones
so columns such as
standard error of ladder score
then we have
upper whiskers and lower whisker column
so we are going to ignore these columns
we are only concerned about
the GDP column which is this one then we
have the social support or the social
status column then we have the health
life expectancy
freedom to make Life Choices generosity
and perceptions of corruption and
there are a few other
columns you can see to the right and
these columns are not of our interest so
we are going to ignore them now let's
head over to our jupyter notebook and
we'll start by importing all the
necessary libraries for analysis and
data visualization
okay so I am on my Jupiter notebook so
first step I'll just
rename this notebook to let's say
happiness
report
data analysis
I'll click on rename all right
now we'll start by importing our
libraries so first Library I'm going to
import is numpy as NP
then I have
import
pandas as PD
then I'll import to data visualization
libraries C bone and matplotlib so I'll
say input
c bond as SNS and then
import
matplotlib
Dot
Pi plot
which is the module name as PLT
and
I'll say
percentage
matplotlib
inline
okay
now let me just go ahead and run this
all right
well now let's set the parameters that
control the general style of the plots
the style parameters control properties
like the color of the background and
whether a grid is enabled by default or
not
so for that I'll say sns.
set underscore
style I'll
give it as dark grid
next I'll say
PLT Dot
RC
params
which stands for runtime configuration
parameters
I'm going to set my
font size
to
let's say 15.
then I'll say
PLT Dot
RC
params
now I am going to set the
figure size so I'll say figure Dot
fixed size
let's say 10 comma 7
oh
next
just copy this
paste it here
now we are going to set the
face color
so I'll say
figure Dot
face
color
I want to set it to peach color so I'm
going to pass in my RGB values for peach
I'm going to set it in terms of hex code
so for peach the value is f f
e
5 b and 4.
now let me run it okay
now it's time to load our data set so
for that I'll create a variable called
Data and
I'll use the
pandas Library
followed by the read underscore CSV
function because our data set that we
saw
is a CHP data set which is this one now
inside the parenthesis
I'll pass in the location of my
data file so I have my data here World
happiness report 2021 I'll just copy
this location and we'll
paste it here
and make sure the location is within
quotes
and you need to change it to
either forward slash or double backslash
so here I'm using
double backslash so let me just include
one more
backslash
and then I'm going to pass in the
file name which is
world
hyphen
happiness
hyphen
report hyphen 2021
dot CSV which is the extension of the
file
now let me run it all right now to
display the first five rows of
information you can use the head
function
so I am writing data which is my
variable that holds the
data frame so data dot head
there you go
you can see here we have
printed the first five rows from the
data set you have the country name
Regional indicator happiness score
then we have information about the GDP
life expectancy generosity
then we have corruption data and these
are some of the columns that we are not
bothered about so we are going to drop
these columns from our analysis
now
we are going to do that so I'll create a
variable called Data columns which are
of our interest
so I am going to take only specific
columns I need the
country name so I have
taken country name make sure the column
names are within single quotes So I have
my country name next I want the
second column which is
Regional indicator
give a comma
we also need the
happiness score
next I need the
logged GDP per capita data
so I'll take that column
I'll say logged
GDP
per capita
give a comma my next column would be
social support
give a comma here
my next
column would be Health life expectancy
so I'll write
health
life expectancy
let's give another comma and we'll take
the next column as well which is freedom
to make life choices
so I'll write that column name freedom
to make
Life Choices
and finally
we'll take the
next two columns that is generosity and
perceptions of corruption So within
single quotes I'll say
generosity
give a comma and we're going to include
the final column which is of our
interest that is
perceptions of corruption
let me have a recheck
to ensure that I have put the column
names correctly otherwise it will show
an error
now let me go ahead and run this cell
I'll hit shift enter all right so we
have successfully
taken the columns that we'll be using
for an analysis
now I'm going to say data equal to
data I'll pass in my new variable that
is data underscore columns and I'm going
to copy all the data so I'll say dot
copy
let's run it
okay this should be data underscore
columns all right
now
let's rename all these columns we'll
make it more simpler and
easy to understand so I'll say
let's say my new
variable is Happy underscore DF which
stands for data frame equal to I'll say
data Dot we'll use the rename function
and using a dictionary we will
rename our columns so I have used a
curly bracket
I am going to pass in my
First Column which is country name I'll
just paste it here
then
I'm going to give a colon
and again Within single quotes I'll say
country underscore name so this is going
to be my new column name
I'll give a comma
we'll take the next
column which is regional indicator
I'll paste it here and give a colon
and
the new column would be small r
ational underscore
indicator
now similarly we'll do this for all the
remaining columns in the data set
okay now I have renamed all my columns
you can see it here
let's run it
now
we are going to display the head of the
data set again
so I'll say happy underscore DF dot add
there you go so we have
only those columns that are of our
interest
so I have the country name Regional
indicator happiness score GDP social
support life expectancy
freedom to make Life Choices then I have
generosity and
perceptions of corruption
cool
now
we are going to check whether any of the
columns have
any null values so for that I will say
happy underscore DF
Dot
I'm going to use the is null function
I'll give another Dot and we are going
to find the sum
for each of the columns
you can see from the data we do not have
any
null values in any of the columns in the
data set
resolve zero
okay
now let's get started with our first
visualization that is
we'll create a plot between
happiness score and the GDP for
different regions
so for that I'll give a comment as
plot
between
happiness
and GDP
I'll just scroll down
cool
first I am going to set the
RC parameters so I'll say PLT Dot
RC params
within square brackets I'm going to give
my figure size so I'll say
figure Dot
fixed size
equal to let's say my figure size is 15.
comma 7
I'll set the
title that is PLT dot title of my plot
to
let's say plot between
happiness score
and GDP
next
I'm going to say SNS Dot
let's create a scatter plot so I'll say
sns.scatter plot
I'm going to Define my x-axis on the
y-axis for the plot let's say in the x
axis we have
my data frame happy
underscore DF
Dot
this should be an underscore
I'll say
a column name as happiness underscore
score
give a comma
and in the y-axis we'll have
my data frame name that is Happy
underscore DF Dot will have the GDP
column that is
logged underscore GDP
underscore per underscore capital
let's give a comma and we'll pass in Hue
for
the color
let's say for Hue I am going to use the
original
column or the regional indicator column
so I'll say
happy underscore DF
Dot
Regional underscore indicator
and then I am going to give this size of
the dots as let's say 200
then I'll give a semicolon
come to the next line
I'll say PLT dot let me just scroll down
now we are going to define the legend so
I'll say PLT Dot Legend
and in Legend we'll have
the location let's say I want to put the
legend at upper left
corner so I'll say loc which is for
location equal to upper left
give a comma and then say
font size of my Legend let's see B10
make sure this is within quotes
then I'm going to pass my x-axis labels
and the y-axis labels
so I'll say plt.x label
let's see the X label is
happiness score
and my by label is
GDP per capita so I'll say PLT dot my
label
within single quotes I'll say GDP per
capita
there's an error here there should be
plot all right so I have
written my code to create a scatter plot
let's run it and see the result
there is some error here
okay this should be
Regional
indicator and not in the K3 there's a
spelling mistake let's run it again
all right
so here you can see we have a nice
scatter plot on the top you can see
we have the title of the plot that is
plot between happiness score and GDP on
the x-axis we have the happiness score
from 0 to 8. and above and on the y-axis
you have the GDP per capita
and if you see here
in this region we have
countries from
Western Europe which have the
highest happiness score and the GDP per
capita is also the highest
around this region you can see here
which is for green and in the legend you
can see green is for sub-Saharan Africa
so all these countries have
low happiness score and even the GDP per
capita is also low
and if you see the
countries for
Latin America and Caribbean you see a
lot of the values lie here so they are
all within the range of 5.52
7 in happiness score
and even the GDP per capita is more than
nine for most of them
now even the happiness score is high for
the countries that line the North
America and Enz reason and even that GDP
per capita is also the highest I can
name a few countries such as Australia
New Zealand we have Canada and
United States of America which belong to
the North America and ANC region
cool
now there is one country which you see
here this seems to be like an outlier
which means that this is the country
which has the lowest happiness score and
even the GDP per capita is also low but
it is not the lowest because you can see
here there are a few countries up here
from the sub-Saharan Africa which have
the lowest GDP per capita but the
happiness score is higher than this
value or this country so we can assume
that this country is Afghanistan which
has the lowest happiness score as per
the 2021 happiness report data
cool
now we'll plot a pi plot to understand
the GDP by region so by this we can know
which region has the highest percentage
contribution to the world's GDP as per
our data
so for that
I'll create a variable GDP underscore
region
equal to
we'll use our data frame that is Happy
underscore DF
dot I'm going to use the
Group by function
and
after that
I'm going to use my
column that is region
so we have named the column as Regional
underscore indicator so I'm going to
group it by this region column
and I'm going to sum the values of GDP
so I am going to use the logged
underscore GDP underscore per capita
column
and after that I am going to use the sum
function
and let me just print GDP underscore
region
you can see it here we have the
sum of all the countries
for different
regions and their GDP all total
now this data we are going to plot it in
the form of a pi plot
so I'll see
GDP underscore region
Dot plot
Dot pi
we are going to plot it in terms of
percentage so I am going to use a
parameter called
Auto PCT
equal to then I am going to pass in my
format
so I'll say percentage 1.1
f
percentage percentage
then I'll say PLT Dot
title
let's see the title of my Pi plot is
going to be GDP by
region
and I'll say PLT Dot
by label
which is going to be blank
let's run it
okay
so here you can see
we have the each background at the back
because we had
assigned
a peach
face color you can see it here
so for the first
scatter plot also we had the peach color
at the back
and now you can see
we have sub-Saharan Africa contributing
20.7 percent to the world's GDP the
reason being we have
around 34 countries in the sub-Saharan
Africa
and
we have the Western European countries
contributing to 16.2 percent of the GDP
to
check the least we have North American
Eng region because we only have four
countries America Australia Canada and
New Zealand so hence they are
contributing only 3.1 percent to the
world's GDP
okay now moving ahead
let's find the total number of countries
in each region so for this we are going
to use the group by function that is
part of the pan Dash library and we'll
count the total number of countries in
each region
so I'll just give a comment
as
total countries
all right just scroll down
so I'll create a variable called total
underscore country
excuse me
I am going to use my data frame that is
Happy underscore DF dot I'm going to use
the
Group by function
I'll group
the values based on the region column So
within single quotes I'll say
Regional underscore
indicator
and then
I'm going to
find the total count of country name so
I'm using the column country underscore
name
and after that I'll just use dot count
now let's go ahead and
print my variable that is
total underscore country
all right now let me hit shift enter
okay so here you can see
all right so for sub-Saharan Africa
there are total 36 countries and not 34
as I mentioned earlier so hence you can
see because it has the highest number of
countries it is contributing the most to
the world's GDP that is 20.7 percent
then we have the least number of
countries in North America and ANC that
is only four we have six countries in
East Asia and then we have 20 countries
in Latin America and Caribbean 12
countries a commonwealth of independent
states then we have 17 countries in
Central and Eastern Europe
cool now
I'm going to show you how to create a
correlation map so that we can see the
relationship that exists between each of
the variables that are present in our
data set I'll just run through the code
and we'll see the output
okay so here I have my code written for
the correlation map that I want to
create so first of all I am going to
compute the correlation Matrix so I have
used the Corr function which stands for
correlation and the method I am going to
use is Pearson method now here I have a
Wikipedia page opened for Pearson
correlation coefficient so this is a
nice article where you can understand
what the Pearson correlation is all
about
and then I am going to set up the
matplotlib figures so I have used the
subplots function and I have given the
figure size as 10 comma 5. and after
that we are going to draw the heat map
with the mask so I'm using the heat map
function present in the c bond library
and then I have passed in my variable
that is cor which essentially stands for
correlation
and then
I have used mask which is a Boolean
array or it can be a data frame and it
is an optional parameter if it is passed
the data will not be shown in cells
where the mask is true
and the cells with missing values are
automatically masked after that I have
used the
cmap parameter which stands for color
map so you can customize the colors in
your heat map and I'm going to create a
heat map which is in square shape so I
have said Square equal to true and the
axis I am going to pass it as ax which I
have defined here so this is the
matplotlab axis and it is optional so
access in which you want to draw the
plot otherwise you can use the current
active access
now let me just go ahead and run this
and we'll see the heat map
there you go so if I scroll down here we
have the
correlation Matrix and since I had given
my cmap as blue
and here you can see it is mostly blue
and we have the scale here
now
I'll tell you how to read this
correlation Matrix so wherever the cells
are in blue or dark blue color this
means that the variables have very high
correlation and all these cells where
you see light blue color or grayish and
white color
this indicates that the variables have
very low correlation for example there
is very low or
almost negative correlation between
happiness score and perceptions of
corruption so obviously if the citizens
of a country feel that there is a lot of
corruption in the country that happiness
score would obviously be less or low
and if you see there is also low
correlation between happiness score and
generosity there is
then look correlation between
Health life expectancy and perceptions
of corruption and if you see these
places
there is really high correlation between
the happiness score and GDP per capita
again for social support also there is
very high correlation
now if you see even for
social support and healthy life
expectancy there is very high
correlation but there is
low correlation between
making life choices and generosity
and there is
negative correlation between corruption
and freedom of making life choices you
can see it is almost white color which
means it falls around this region so
negative correlation
similarly for
logged GDP per capita and
corruption that is
negative correlation all right
okay now we are going to visualize a bar
plot that will
tell us the corruption in different
regions so I'll just give a comment
corruption in
regions
let me just scroll down okay
so I'll create a variable called
corruption which will be equal to
my
data frame that is Happy underscore DF
now first of all I'm going to use the
group by function to group all my
regions so I'll say Regional underscore
indicator Which is my
column name present in the data set and
after this I'm going to find the average
of the
corruption that is perceptions of
corruption so I am going to pass in my
variable name that is
perceptions of
corruption
and I'm going to use the mean function
to find the average corruption in each
of these regions
now let me just go ahead and print my
variable that is corruption
we run it you can see here
I have the values for the different
regions and from here you can see that
Central and Eastern Europe has the
highest perceptions of corruption as per
the
questions answered in the poll
and if you see the table
we have Western Europe
and the North America region with the
least perceptions of corruption
but we are going to visualize this using
a bar plot now so first of all I'll
set my
parameters by giving the
figure size
I'll say
fig
size
it's rather figure dot fixed size
I'll just add figure here
okay
and now let's say
I'll assign it as 12 comma 8.
now I'll give a title to my plot so I'll
say PLT dot title
and my title of the plot is going to be
perception of
corruption
in
various
regions
all right
then I am going to Define my X label
so I'll say x label will be
regions
and
we'll set the font size of the X label
to let's say 13
or let it be 15
then we are going to set the
y label
in violable I'll have
corruption index
as the label name
again we are going to set the font size
to 15
now I am going to use
X ticks
parameter
since I want to
rotate the axis labels in the x axis by
30 degrees so I'll say rotation equal to
30
and I'll say h a which stands for
horizontal alignment equal to right
make sure this right should be within
single quotes
and finally I'll say PLT dot bar because
I am going to plot a
bar graph
I'll say corruption
dot index
comma
and then I'll say
corruption
Dot
perceptions of
corruption which is my column name
all right so I have my code ready for
the bar plot now let me just go ahead
and
print
the bar plot
make sure everything is correct I'll
just hit shift enter
okay there is one mistake here it says
okay this should be X ticks and not X
tick
let me run it again there you go
so if I scroll down
on the top you have the
title which is perception of corruption
in various regions on the
x-axis you have the regions label on the
y-axis you have the corruption index and
if you see this
as per our table that we created
we have least corruption in North
America and ANZ region then we have the
next least corruption in Western Europe
but we have the highest corruption in
Central and Eastern Europe as per their
citizens perception similarly we have
the
second and third highest corruption in
Latin America and Caribbean as well as
South Asia
cool
now moving ahead
I'm going to show you how you can find
the life expectancy of the top 10
happiest countries and bottom 10 happy
countries so for that I am going to run
you through the code and we'll see the
visualization side by side
okay so I have my code written in these
two cells so first I am going to find
out the top 10
happiest countries and then I am going
to find the bottom 10 happiest countries
so for that I'm using the head function
and I have passed in 10 since I want the
top 10 country names and to get the
bottom 10 countries as per their
happiness score I am using the tail
function so let me just run it so we
have saved the result in two variables
top underscore 10 and bottom underscore
10. and two
create the bar plots we are using
two different
codes so here you can see I have set my
figure size and access and then I have
my X label as
country name
after that I am setting my
title of the bar plot to top 10 happiest
countries life expectancy I've used my
x-stick labels and I am rotating it by
45 degrees and I have my horizontal
alignment as right
you can see it here we have used the bar
plot function I have the x-axis as
country name and by access as healthy
life expectancy column and I have set my
Axis then
um using the X labels and Y labels as
country name and life expectancy and
similarly I have my bar plot for
the bottom 10 least happy countries life
expectancy
let's just run it and we'll see the
result
there you go
and if I scroll down you can see we have
two different bar plots the first one is
for the 10 happiest countries and then
we have the bottom 10 least happy
countries
so if you see
on an average
the top 10 happiest countries life
expectancies above 70 years so if you
are from one of these countries you are
expected to live
for more than 70 years now if you check
the bottom 10 least happy countries you
see here
Lesotho has less than 50
life expectancy age and most of them are
less than
60 years so if you are from one of the
top 10 happiest countries you are
expected to live
10 years more than these countries that
line the bottom 10 region
cool
now moving ahead
all right so now we are going to see the
plot between freedom to make life
choices and the happiness score for this
I am going to use a scatter plot so
I'll first Define my figure size so I'll
say PLT Dot
RC
p-a-r-a-m-s which stands for parameters
I am going to pass in my
figure size so I'll say figure Dot
fig size
equal to
let's say 15 comma 7.
then using the C born library and the
scatter plot function
will pass in the
x-axis let's say the x-axis is
my data frame name happy underscore DF
and
I'll have the freedom to make life
choices in the x-axis
so the column name is freedom underscore
2 underscore make
underscore life
and choices
and give a comma and we'll pass in the
y-axis again I am going to use my
data frame name
Dot and I'll have the happiness score
I am also going to
pass in a hue parameter
to differentiate
the different regions
so I'll say DF Dot
Regional underscore indicator
and I'll give the size of the dots
or the Bubbles as let's say 200
now I'll say
PLT Dot
Legend
I'll place the legend
at upper left corner so I am giving the
location which is loc equal to
upper left
and
font size
Let It Be
12.
I'll say PLT Dot
X label
as
freedom to make
life
choices
and my y label
would be happiness score
all right
now we have the code written for the
scatter plot that I want to create let
me just run it
there you go
so here we have the legend for the
different regions and we have the
different colors for each of these
regions
and you can see here
I have
on the x axis freedom to make life
choices and on the y-axis we have the
happiness core
so you can see it very clearly
for the countries that line the Western
Europe region the blue dots
the freedom to make life's choices is
more and so is the happiness score
then if you see
the values the
green region which is for Middle East
and North Africa the freedom to make
life's choices is
lower and hence the happiness score is
also low and for all these countries
that are part of the sub-Saharan Africa
some of them have
decent
score for freedom to make life choices
but the happiness score is low again
now if you focus on the
pink dots which is for the Southeast
Asian countries the happiness score is
comparatively lower but the
freedom to make life's choices is more
than
0.8
cool
and again we have one data point which
is
lying at the bottom we can assume this
is Afghanistan so in Afghanistan the
freedom to make life's choices is very
low and even the happiness score is
really low
cool
now moving to our next analysis
we are going to see the top 10 most
corrupt countries
so first I am going to sort the
perceptions of corruption column and
find out the top 10 countries in the
list
so for that I'm going to create a
variable called country
I'll have my data frame name that is
Happy underscore DF dot sort underscore
values
by
I am going to sort by
column that is
perceptions
of
corruption
Dot
head
and I want to find the top 10 most
corrupt countries as per the
poll
and then
I'm going to
pass in the figure size so I'll say PLT
dot RC
params
and then I'm going to set the
figure size so I'll say figure Dot
fixed size
make sure there is no spelling mistake
let's say the figure size is 12 comma 6.
I'll pass in the title so I'll say PLT
Dot
title let's say the title of my plot is
going to be
countries
with
most
perception of
corruption
after that I'll say PLT Dot
X label
in the X label I'll have
country and I'll set the
font size to
13
then I'll say PLT Dot
y label
in by label I'll have the
corruption
index
next
I'll
pass in the font size for the by label
again it is going to be 13.
now I'll see PLT dot X
I'm going to
rotate it by 30.
and I'll see horizontal alignment equal
to
right
and then
I'll have my
bar function or the bar plot function so
I'll say PLT dot bar
I'm going to pass in country Dot
country
underscore name
and then we'll have a variable country
Dot
the column name that is
perceptions of
corruption
all right
now if I run it
you can see here
so these are the countries with the
least perceptions of corruption so
Singapore has the
lowest corruption index
and then we have rawanda Denmark Finland
now if you want to see the countries
with the highest perceptions of
corruption you need to change this head
to
deal of 10 since we are sorting it in
ascending order so we would want to know
the bottom 10 countries if I run it
there you go so these are the countries
with the most perception of corruption
you have Slovakia Lesotho Kosovo this
Ukraine Afghanistan Bulgaria Romania and
Croatia all of these countries have a
corruption index of more than
0.85
cool now coming to the final section of
this interesting video on happiness
report data analysis for 2021 I want to
visualize a scatter plot that will tell
us how the corruption varies in terms of
Happiness score
so I'll just say
a comment as corruption versus
happiness
okay
so first of all I'll set my
figure size so I'll say PLT Dot
RC
params
within single quotes I'll give the
figure size
as
15 comma 7.
then I'm going to use the scatter plot
function that is part of the c bond
Library View
in the x-axis I'll have my
column that is
corruption or rather we'll have the
happiness score in the x axis so I'll
say happy underscore DF Dot
column name that is happiness score
let me give a comma and in the y-axis
we'll have
the corruption column so I'll say happy
underscore DF Dot
perceptions of
corruption
foreign
in Hue we'll have the region that is
Regional underscore indicator so I'll
say happy underscore DF Dot
Regional
indicator
and I'm going to give the size of the
dots so I'll say s equal to 200
then I'm going to say PLT Dot
Legend
I'm going to
put it at
lower left corner this time so I am
giving my location as
lower left
and
the font size of the Legend I Want Is
14.
after that I'll just say PLT Dot
X label
as
corruption and
PLT Dot
y label
as
happiness score
you know this should rather be the
opposite since in the x axis we have
taken happiness score
so we'll
put the happiness code label in X and
I'll have
corruption in y
all right
so I have my scatter plot
code ready let me just run it
there you go
so here you can see on the lower left I
have the different regions and on the x
axis I have the happiness score on the y
axis I have the corruption index now you
can see the general Trend as per the
scatter plot that the countries with
greater happiness score have lower
corruption index so you can see these
countries which are from the Western
European regions have
highest happiness score and the
corruption is also really low so all
these blue countries you can name a few
Finland we have Sweden we have Belgium
France Netherland lying in these regions
we also have a few countries from North
America and ENC which are essentially
Australia then we also have Canada U.S
and New Zealand
okay now if you focus on these
green region this region
now these are the countries from the sub
Saharan Africa and most of the countries
from these regions have a very low or
less than five happiness score but
the corruption is also really high you
can see here they have almost more than
0.7 the corruption index
now
you also have a country here which is
from the
southeast Asia we would like to give you
a task it would be really great if you
can tell us in the comments section
which country is this
it has more than six happiness score but
the corruption index is really low Which
is less than
point two
now if you consider
Middle East and North African regions
the
darker green countries here also if you
see the happiness score is less than 5
and the corruption index is also High
now there are a few countries the
Commonwealth of independent states for
example
countries such as Uzbekistan then you
also have Kazakhstan we have Russia
Armenia belonging to these
gray color dots you also have Georgia
and Ukraine as part of Commonwealth of
independent states so here you see the
corruption is below
0.6 for these countries and the
happiness score is more than
five
all right so with that we have come to
the end of this demo session on world
happiness report 2021 data analysis
using python
introduction to R and python let me ask
few queries regarding Python and r
our language is super superficially
related to
python
C
plus plus
Java please leave your answer in the
comment section below and stay tuned to
get the answer
and another question is which one of the
following is a python file extension
dot p
dot python
Dot py
and Dot PYT
please leave your answer in the comment
section below
coming to introduction to Python and r
R is a statistical programming language
and environment that integrates
statistical Computing and Graphics R is
powerful and stable software
python python can also be called as a
general purpose programming language for
data analysis and scientific Computing
python can be considered as the best
player in machine learning python is an
expressive language with many built-in
function
both are open source software and
platform independent
and they are platform neutral and also
compatible with all major operating
systems including Unix Windows and Mac
next we will be covering different
parameters
we will be covering learning
preferability mathematical fundamentals
speed of both languages
visualization and graphics
data handling capacity
demand
community and customer support
employment possibility in both the
languages
let us cover it one by one first one is
learning preferability or ease of
learning
python is renowned for its ease of use
Python's notebooks offer excellent tools
for sharing and documentation despite
the fact that there are currently no
guis for them programmers find are as
difficult language as a beginner
this implies that the programmers must
devote a significant amount of time to
learn and comprehending our coding
coming to mathematical fundamentals
required
coming to python understanding
descriptive analysis is very important
in layman's terms descriptive statistics
often refers to the process of
explaining using certain representative
techniques such as charts tables Excel
files Etc
python statistics is a built-in library
for descriptive statistics
if your data sets are not too big or if
you can't rely on importing other
libraries you can use python
on the other hand R requires basic
statistics from basic status statistics
what I mean is mean mode and median are
the terms used most frequently in basic
statistics it is referred to as measures
of central tendency
probability statistics plays an
important role in handling various types
of probability distribution it includes
binomial and normal distribution
next parameter is speed
python is an interpreted language with
Dynamic typing python always executes
slowly because the code is executed line
by line
compared to Matlab and Python rsr
language is significantly slower
our packages are substantially slower
than those for other languages
now that we have covered speed
coming to data visualization and data
collection in Python
When selecting data analysis tools bit
visualization are crucial and python has
some incredible visualization tool
in Python to large and varied Scatter
Plots using regression lines we can use
GG plot 2 and ggplot tools
compared to Raw values visualized data
is easier to comprehend therefore R has
many packages that offers sophisticated
graphic features
in R we can use
in R we can use tools like matplotlib c
bond Etc
data handling capability in both Python
and r
the new releases in Python have resolved
the issue with the python packages for
data analysis R is useful for analysis
because of the abundance of packages
accessibility of the test and benefit of
employing formulas however simple data
analysis can also be done using it the
need to install many packages
crucial part of parameter that is tools
and libraries in Python and r
as a python developer one needs to be
well versed in the best libraries
because python has a lot of libraries
that have many different uses
libraries like tensorflow scikit-learn
numpy plays an important role in solving
many python related problems
libraries perform a wide variety of
tasks in art that are very beneficial
for data science operations example for
that is deployer bioconductor ETC
community and customer support support
index offered by Python and r
compared to our python has a larger
community
for assistance we can contact
www.python.org
for any queries regarding Python and
help you can support uh you I repeat you
can visit support.realpython.com
for any help and queries are offers you
with our studio community
R provides assistance through its
official website
for queries and Community related issues
we can contact
www.r hyphenproject.org
next is job opportunities in Python and
r
recent survey from indeed.com predicts
that at least 55 000 python jobs in the
USA with exponential pay rates are
available with the companies like Google
Amazon Twitter Facebook requires python
developer to handle massive amount of
data
position provided for a python developer
is software engineer data analyst data
scientist and many more
career in R is an excellent job
opportunity for you as a beginner
big tech companies like Google Twitter
Facebook are using are
position provided by companies as a r
developer is
data scientist data analyst data
visualization analyst Etc
moving on let us wrap up an important
topic which language to be used between
R and python there is no right or wrong
way to study both python or R both are
in demand skills that will enable you to
complete almost any data analytics work
you come across It ultimately depends on
your background interest and career
objectives that which one is better for
you but compared to R python is easy to
learn let's compare its strengths and
weaknesses
it is used to handle large amount of
data python performs non-statistical
functions and it is best suitable for
programming however python is better
when it comes to coding
whereas R is used in data visualization
graphics
R is a widespread language in the
statistical community
it is used to accomplish many
mathematical tasks
so before concluding the topic let me
answer the query that I have asked
regarding R and python do you guys
remember the question
the query was
our language is superficially related to
which language
so the answer for the question is C
language
next question was which one of the
following is a python file extension it
was an easy question
answer for that question is Dot py
so firstly let us understand why data
analytics can somebody here tell me why
data analytics and why is it used in
organizations
so let us understand why data analytics
and what is the use of data analytics
as we all know the data is growing
exponentially year over year
it is collected and it is also available
everywhere
data is no more just available in
structured format but it is also
available in semi-structured and
unstructured format
I'm sure you would have come across this
term called as sentiment analysis pretty
often right what does that mean can we
perform that in data analytics yes right
we use some natural language processing
models try to identify what are the good
reviews what are the bad reviews spoken
by the customer
correct so we try to classify the text
based on the good bad and the neutral so
that is what sentiment analysis about
so for that to perform that activity we
have to also do some data analytics
now that companies have realized the
importance of these informations
not just structured but also
unstructured data format
companies have started utilizing these
data to take some crucial business
decisions which can boost their business
and also which can increase the
efficiency of the business
so now that the raw data is accessible
to the organizations
it becomes very important that the data
is also stored well
I'm sure you all have pretty much heard
about data warehouse in the last few
decades the trend was mostly on the data
warehouses the business intelligence
tools
so the data warehouse used to collect
these data pre-process the data and also
filter the data and make it available in
a structured format for further analysis
however now that is not the scenario
a term coined as data leak is available
and many firms are utilizing data Lake
because it is a central Repository
which stores the raw data in form of
structured and unstructured data
and now let us take
a scenario let us choose one of the
participants here
mock so Mark has recently joined an
organization as a data analyst or in
essence a data scientist
the business connects with him and says
that Mark we have a business problem for
you and we expect you to provide us a
data analytics solution
so Mark sits with the stakeholders and
he listens very carefully to the
business question so the business
question says that
we have couple of products which are
performing really well in the market and
we see higher sales and some of the
products are just not catching up in the
market and we experience lower sales can
you help us identify what are the
factors driving this higher sales and
the lower sales
now Mark has to think with the data
scientist mindset and be prepared to ask
them right questions
some questions such as do you have the
price of all these products available in
the database
and can I also know what is the duration
of data availability
and also Mark can ask some question such
as do you also have some features of
this individual products already
captured in your data base
so these are some of the interesting
questions that makes sense to the
business and the conversation continues
which also means that
data is not only information
data analysis is about unlocking
insightful informations from this raw
data
and hence data analysis plays an
important role in discovering insightful
information
asking questions or answering the right
questions
and also predicting the future are the
unknowns
and to perform all these activities we
use data analytics
so are you all with me so far
are we on the same page
yes great
now the question is what is data
analytics so can somebody tell me what
is data analytics
so let us see what is data analytics
we understood why data analytics and the
importance of it
but perform any activities there has to
be a process right so data analytics is
a process to extract meaningful insights
from data
now let us continue with the scenario of
Mark
so Mark now understands the business
problem
and he has also started asking some
relevant questions to the stakeholders
and he has also got the answers in
return
he may start thinking about what could
be the suitable solution for this
he may have to perform some exploratory
data analysis to unlock some hidden
patterns
to identify some correlation between the
variables and to also know which are the
key variables in the data set and he may
also have to view the market Trend which
means he may have to see that how the
sales has been performing across the
years or across the months
there might be some insightful
information there
he may see that the sales has been
growing exponentially for some of the
products
and some may be volatile some may have
some seasonality pattern or a cyclical
pattern Etc and also he may have to
focus on the customer preferences via
the customer reviews and do some
sentiment analysis
so now let us understand what are those
life cycles of data analytics
we will begin with the discovery phase
this is the first phase
now that Mark has understood the
business problem
he will also start focusing on
identifying the resources that is the
data resources
some may be internal data resources that
is available within the firm could be
some transactional data
and some external data sources may be
via web scrapping identifying and
capturing some competitor price on the
products
after Gathering all of these right data
Mark will focus on data preparation
which is the next phase
now Mark either individually or along
with the team will start focusing on the
data preparation which includes data
wrangling which means cleansing the data
imputing the records
if there are any missing values or you
know he may also go ahead by removing
those records if they are not required
and also doing some exploratory data
analysis which can include some
statistical analysis like looking at the
data distributions understanding the
summary of this data distribution at
individual variable level doing some
bivariate analysis and also trying to
you know figure out which are the
important variables that might be
required for the model building phase
after performing all of these Eda
activities
which also include some visualization
Mark will now sit with his team and try
to identify the suitable models
the suitable models could be simple
statistical techniques or it can also be
some machine learning models so let's
say that Mark and and his team has
identified some five models
that can provide the required result
and out of these five models they will
filter down and they will prioritize
only three models
now there are only three models that
Mark and his team have finalized
after this they start focusing on model
building activity
now for model building activity
they have a data set already in place
so this data set will be split into
training data set and test data set
it's not only training and test we can
also do some validation in between
training and test but here let's focus
on training and test data set
he will separate 75 percent of the data
as training and 25 percent of the data
as test
now if your question is that why perform
this activity of splitting the training
data set and test can't we just go with
the one single data set
what happens if you just utilize one
single data set
let us say that you have used the
original data set
and also executed this data in one of
the selected model
and you will also observe the accuracy
let's say the accuracy return is about
98 percent
98 percent is a very good accuracy
percentage
and you may also be overconfident
because of this that may be a case due
to overfitting
now what will happen when you add some
new records into this data set and you
re-run it
the executed model may not return you
the same accuracy what you had seen
the accuracy might be 72 percentage now
that's not fair right
to avoid these overfitting issues
we ensure that some new records are
tested separately
so hence we locate 25 percent of the
data to the test data set
and then we predict this records the
unknown records which is located in the
test data set we predict them and then
we test the accuracy of training data
set and the test data set and we make a
comparison
now let us say the accuracy result of
training is 98 percent and the test is
97 percent
in this case we can say that the model
is performing really well
however while executing the model there
are certain things that has to be
considered for example inclusion of the
parameters
tuning the parameter which also will
execute the optimal results so this is
very important
now let's say after performing this
model building
the time comes to analyze the results
that is the next phase
now the team will sit and analyze the
result
and they will notice that out of the
three filtered models only two models
are returning excellent accuracies
they will sit with the business team and
they will also explain them the result
and what are the activities that they
have performed to obtain this result
some of the stakeholders may be
technically Savvy some of them may be
non-technical people so it has to be
very important that you also communicate
these results accordingly
all right now that you have the results
you will also gauge them based on the
business objective
which was developed in phase one
looking at the results of the two models
now the business might select one of the
model and say that okay this particular
model seems to be returning some right
information and it also appears valid to
us let's go ahead with this one model
so finally the result and the model
needs to be operationalized
and that is where the team will start
documenting the business problems the
steps that were taken for executing the
models and they will include all the
codes and the findings and finally they
will implement this model so that the
business can view the results and also
utilize them for strategic decision
making at your form
so I'll be good so far with the
understanding of the life cycle
all right great now let us focus on the
types of analytics what are the types of
analytics can somebody tell me which are
the types of analytics you are aware of
okay Predictive Analytics great
descriptive analytics good
all right good enough
now let us focus on this example of
Google Maps
so as we look at this particular Google
Map we understand that the blue color
root is nothing but the root direction
from Sacramento to fluorine and also we
see a display of the
duration estimated as well the distance
to travel from Sacramento to Florin as
well as we see another root here that is
gray colored
this is a substitute root or the
connecting root just to avoid the
traffic which is in Orange within the
blue color root so the Gray colored root
as well shows us the estimated duration
to travel as well the distance
now let us understand what is
descriptive Analytics
and why do we focus on this particular
map example
as we understand the root map the
estimated duration as well the distance
to travel
wire the blue color root as well the
gray color root
this is one way of understanding
descriptive analytics as in what is
happening
but there is another way of
understanding descriptive analytics that
is by focusing on summarized past data
and this is a descriptive analytics we
see that what had happened
in the previous year now let us focus on
Predictive Analytics
what is Predictive Analytics
this type of analytics looks into the
historical and present data to make
predictions of the future
what does this mean
so Google has already suggested the best
route
which is the blue color root to travel
from Sacramento to Florin and the
duration is 18 minutes
and distance is 9.7 miles
let's assume that Google map has already
collected historical data of this
particular route
and based on the available data their
model has predicted the best route
and also the duration that will be taken
to travel from Sacramento to Florin
now let us focus on prescriptive
Analytics
prescriptive analytics describes the
solution to a particular problem
what was the problem in this case on the
predicted best route
some predictions on the traffic
congestions and that's when Google Map
recommends The Substitute Roots correct
now these substitute roots are also
prescribed by Google Map as a
recommendation
so prescriptive analytics is nothing but
a solution and a recommendation provided
for a problem
so in this case we have the best route
and we also have other substitute troops
so let's quickly summarize
descriptive analytics is about
summarizing the past data or to see what
is happening for example in the Google
Maps scenario
Predictive Analytics is about what would
happen
and prescriptive analytics is about
prescribing the solution the best
solution and the recommended Solutions
now let us refer back to Mark's earlier
scenario
Mark along with this team identify the
best model they also did some testing
and they got identified the best results
and they also finalized on one
particular model
based on that particular model now the
results have to be provided in such a
way
that they are the best results and also
the recommendations correct
so it may not be just one single
solution it may be a solution with
couple of other recommendations as well
so that is exactly what happens in the
entire process of data analytics
I hope this has been clear so far
yes okay
and now let us focus on benefits of
using R why do companies extensively use
R for data analysis and why is it chosen
firstly R is an open source programming
language which means that there is no
license required to work with r
and R does not require you to have a
coding experience which means that a
non-technical person in your team can
also learn R very easily and start
coding or building models in few lines
of codes
R can also be used with other
programming languages such as Java C
plus plus and pythons and integration of
r with other programming tools or bi
tools is very simple and easy
and various statistical models are
readily available in R which also means
that there are plenty of embed libraries
and packages already available
and Reporting the results of an analysis
becomes easier by using this inbuilt
packages and for creation of these
models in just simple few lines code
with this understanding of the benefits
of using R let us quickly hop on to our
studio and start performing the Hands-On
exercise for data analysis
for this exercise we will use a data set
named as demographics which is in a DOT
CSV file tab firstly let us load the
data set to our studio and we will
locate this in an variable named as demo
we also refer to this as a data frame
and now you will notice that a variable
is created in an environment section
which is in the bottom right hand side
of the rstudio window
and this particular variable comprises
of 510 observations or records with 8
variables
let us simply expand this particular
data frame and have a quick check on the
data structure and understand the data
types
this particular data frame includes
variables such as h
marital
income the unit of income is dollar per
day
education levels the car price car
category with several levels gender and
retired status
now let us view the top six records of
this particular data set
for this let's simply type head of demo
and the result is now visible in the
console section
if you are interested to view all the
records then simply type view of demo
and this new window will show you every
single record that is being loaded to
our studio
you may also simply apply the filters
and the filters section here on
individual categorical variables
now that we have loaded the data set and
also viewed individual records let us
focus on creating subsets of Records by
applying filters on individual variables
or multiple variables
so firstly let us apply filter on gender
we will only retrieve the records with
gender is equal to female and we will
locate these records in a variable named
as demo 2.
as you notice now in the environment
section the second variable is also
created that is demo and this now is
comprising of 250 observation which
means the records are filtered down to
only gender female
next let us see how to apply a filter on
income variable let us only retrieve the
records where income is greater than
100.
let's view the result
as we see here all the records include
income greater than 100.
now let us modify this query
and we will ensure that the retrieve
records includes income greater than 100
and also specific variables are returned
let's say we only want to have
the first variable third variable and
the seventh variable returned as the
result
let's have a quick check
so we only have the first third and the
seventh variable returned
how about we only exclude
the variable 6 to 8
for this we include a prefix of minus
sign
and now let us see what is the result we
have the variables from first to the
fifth variable however we don't have 6 7
and 8 variable
I hope it's clear so far
yes all right
now let us see how can we apply
condition by including both the
variables that is gender and income and
then we will filter the record and
create a subset of data
now let's view the result
income is greater than 100
and the gender is only female
this is one way of creating subsets
however now let us see how to use the
subset command and create the subset
let's create a subset of Records by
applying filter on marital status and
age
we'll only retrieve records where
marital status is equal to married
an age is greater than 35.
okay
let's now view the result
so here we have the age greater than 35
and marital status is married
let's use the same code
and this time we will retrieve selected
variables let's say variables ranging
from 1 to 3.
let's have a quick check so there are
three variables age is greater than 35
and marital status is married
now let us see how to structure the data
by sorting the data frame in ascending
and in descending order
we will apply this order function on the
variable income
firstly let us see how to order income
variable in ascending order
let's do a quick check
and here we have income in ascending
order
now let's see how to modify the same
code
and view the records
with income in descending order
so we have now the income in descending
order
how about include two variables and sort
the variables accordingly
firstly we will sort the records
by ordering income and age in ascending
order
let's quickly view the result we have
income in ascending and age as well in
ascending
let's now modify this code
this time we will order income and
descending
and age in ascending
let us view the result so the income is
in descending and age is in ascending
order
so hope this is clear on how to solve
the data frame by ascending and
descending order per variable or by
using multiple variables with this we
will focus on learning statistical
analysis
how to perform statistical analysis on
individual variable or multiple variable
let's start by understanding the data
distribution of variable income
so that we identify what is the minimum
value of income what is a maximum what
is the range what is median what is the
mean and we will also focus on the
quantile distribution
which is also analyzed in a box plot
what is the minimum value in the
variable income
it's nine
and what is the maximum
so we have the maximum value
now let us see what's the range
so the range shows you the result with
minimum and the maximum value
how about the difference of Maximum and
the minimum now let us focus on other
summaries of data distribution for this
variable income let's identify what is
the mean value of income
the mean is 78.
let's also understand what is the
standard deviation
all right so the standard deviation is
one one two dollar
let us see what is the variance the
variance should be larger than the
standard deviation
now let's say what is the median
absolute deviation
as you notice here the median absolute
deviation value is lower than standard
deviation why do we make this comparison
from this it is evident that median
absolute deviation is robust outliers
and standard deviation is
sensitive to outliers and also to the
change in the mean value
now let us understand the quantile
distribution this is the same analysis
that is visualized in a box plot ranging
from zero percent to hundred percent
identifying the individual data points
and we can also refer and compare this
to the Min the Max and the median values
let us quickly see what is the median
value of income
as you notice here the median is 45 as
well the 50th percent of quantile is 45
which means zero percent is minimum and
hundred percent is the maximum value
now if your question is what is 25 and
75 percent this is again used for
identifying the range of interquartile
the interquartile range is nothing but
the difference of 75 percent minus the
25 percent
let's quickly see what is the IQR of
income
the iqrf income is 58. let us do a quick
check
75 percent of quantile is 86 and in five
percent of quantile is 28.
and the value is 58 which is equal to
the IQR result
now that we have focused on the
statistical analysis of the individual
variables data distribution let us focus
on the data visualization
in this we will have a pictorial
representation of analysis to identify
the outliers to see what is the minimum
and where do we see the data densely
populated and how is it scattered Etc
we will begin with creating a histogram
now histogram can be used for univariate
analysis which means in this scenario we
will consider income variable and we
will see how the count of income ranges
gets distributed in a histogram
for this we will have to install a
package called as jgplot2 and also call
this Library ggplot
let us install the package
and now let us call the library
all right and we are ready now to begin
with visualization
for this we will use the geometric
object histogram
on the data demo data frame
let me expand this window so that the
code is visible
and also use an aesthetic mapping
for variable income this will be helpful
for filling colors or filtrations
Etc
and only include 30 bins
with individual bin size width
of 100 which means there will be 100
incomes in individual bins
let's quickly look at the distribution
of this histogram
as you notice there are couple of
outliers
the counts of these income range are
very limited
however we see the densely populated
income ranges with higher counts
between 0 to 200. dollars per day
this is also a way to identify and
segment the customers based on their
income ranges
now let us see how to change the color
of this histogram and also the border of
the histogram
for this we will include some additional
options
such as fill
Ed with blue color
and the Border color
is black
now as you notice here the executed code
provides us the histogram with blue
color bars and black color Border Lines
now we will focus on creating a facet
grid facing grid is also an aesthetic
mapping object
we will see how to enable the multiple
histograms across the marital status and
the genders so that we identify how the
income is distributed for individual
maratha status as well the genders
let's Zoom this View and have a look at
it
as you notice here there are some
interesting outliers here in the data
distribution
female unmarried drawing higher income
and male unmarried and married also
drawing higher income as compared to the
females
whereas if you notice that the female
unmarried is drawing much higher income
than the male
this may also be very much related to
the age
now let us see how to create a stacked
histogram when I say a stacked histogram
I mean instead of feeling the color we
will fill the gender
so that there is a stack within the
histogram
so as you notice here I have made couple
of changes I have included fill equal to
gender within the aesthetic mapping now
let us look at this histogram as you see
here the gender is fair in the histogram
hence we have stacked distribution of
female and the male
now let us focus on creating a bar chart
with education versus income where we
can identify the education levels and
the income ranges for these education
levels
thank you
as you notice here we are going to
create a visualization where we have the
aggregation in form of mean and the
geometric object used here is bar plot
now let's Zoom this View and understand
which education level
have higher average income
so as we see here the blue color bar is
the post undergraduate degree which
means this education level draws higher
average income as compared to other
education levels
now let's create a histogram where we
will see car price and the number of
cars for individual category
let's look at this visualization
this visualization provides with some
interesting Insight just by looking at
the distribution of the car prices and
the counts of the cars at the car
category economy and even the luxury
luxury car category or car prize is
pretty much distributed whereas Economy
Car category is densed which means that
we could also look back into the income
and age variables and try to figure out
further more insights and then segment
the customers for further targeting of
these customers
now what happens if we simply change
this bin width to 30.
As You observe here changing the bin
width or increasing the bin width will
also reduce the number of bins
now we only have four bins here
and the car category is filled that is
what we have enabled within the
aesthetic mapping
and we see some more interesting insight
as you look at the standard and the
luxury car category
the car prices are pretty much
overlapping
for the car category luxury and standard
this could be the starting car price of
the luxury brands
now let us create
a clustered bar chart
let's look at this visualization
in this visualization As You observe
though we have enabled fill equal to
gender in the aesthetic mapping we do
not have the view in stack form but we
have the bars one besides the other it
is also because we have enabled a
position called as position equal to
Dodge in the code
now what is the inside that we can draw
from this visualization as you see post
graduate degree with female gender is
drawing higher average income as
compared to any other education level
now let us see how to create a box plot
for variable income across the genders
so the box plot can be enabled if there
is a bivariate analysis to be performed
on a continuous variable and a
categorical variable or multiple
categorical variables with a continuous
variable
now let's look at this visualization
what does this say
we have data distribution of income
for individual genders that is for
female and the male and we also notice
outliers here
anything about this whisker is
considered to be outliers
it might make more sense if we also
include some coloring for these outliers
maybe also enable shape
foreign
the outliers and it's colored Orange
let's see if we can also enable the
shapes
and now we have here the outlier color
as well the shape enabled
now let us see how to enable a violin
plot
what is the utility of violin plot with
a box plot we understand the analysis
and the distribution of the data points
is to identify the outliers to know what
is the minimum value what is the max
what is the median and what are the
outliers
but what is the purpose of a violin plot
let us have a quick check
foreign
there is some concentration of data
points in the bottom of every car
category
however the concentration is higher for
standard car category as compared to
economy and the luxury
now this is an interesting Insight that
you wouldn't have come across in box
plot the box plot is a very good
representation for identifying outliers
however while in plot will help you
focus on the nuances which is not
captured by the box plot
we can also simply combine the box plot
and the violin plot together
simply include this jio object
let's Zoom this now you have a
representation of box plot and the
violin plot both combined in a single
visualization
interestingly you notice the outliers as
well the concentration in the bottom of
this violin plot so this could be some
interesting insights that you draw and
focus on these data points and
understand what exactly is happening
there
now let's focus on the density plot that
is density estimate of the histograms
rather than just viewing the frequencies
now we see the frequency in the y-axis
across the income distributions
how about enabling the probability as
true
so that we enable the density instead of
the frequency
so now we have the density in the y-axis
and in the x-axis we still have the
income distribution
foreign
this is the way of also adding a line
plot which is a density plot on the
histogram now as You observe here the
density plot is not in the same level as
the bar so let us adjust this line
for this we will include
adjust
let's say equal to 3 and now let us see
how the visualization appears now the
density plot is on the same level as the
bar
now let us see how to create a cross
table
for car category
and gender
for this let us call the library d e s c
r
now let us create the visualization
enabling cross table for car category
and
gender
let's look at the result in console as
you see here now we see the counts of
the gender for Individual Car category
the values over here represents that
there are 67 females
falling within the car category economy
and 80 males within the car category
economy
and for luxury we see that the count of
female is higher than the male as well
the proportions now how do you
understand what proportions are
presented here
we may simply turn off some of the
proportions like the t-test the
chi-square Etc let us see how to enable
that
now let's look at the result this looks
better
now that we have the counts the female
counts and the male counts across
Individual Car category we also see the
percentages rather than just looking at
the absolute value so there are
45.6 percentage of female within the car
category economy and 54.4 percentage of
mail within the car category economy
similarly across rest of the car
categories this kind of cross table or a
contingency table is also helpful when
you want to analyze the different
categorical variables and identify the
counts or the proportions now let us see
how to use a scatter plot of age versus
income
scatter plot is a visualization used for
bivariate analysis when you want to
perform some analysis between two
continuous variable at a data point
level rather than performing the
analysis at an aggregated level such as
sum or mean
and now we have a scatter plot of age
versus the income age in the x-axis and
income in the y-axis
though we did not see any kind of a
positive correlation or a negative
correlation but we still see some
interesting insights over here some of
the data points are pretty much
scattered and much away from densely
populated data points
I hope the learning has been informative
and interesting so far we have covered
the concepts of data analytics as well
we have performed some Hands-On doing
some statistical analysis and also
creating interesting visualization so
welcome to this session where we will
learn on time series analysis using our
programming language so this is
basically a mini project where we will
look at time series data and how we can
analyze it visualize it to basically
find some important information or
gather insights from the data now when
you talk about time series analysis time
series is basically any data set where
your values are measured at different
points in time so when you talk about
time series data data is usually
uniformly spaced at a specific frequency
for example hourly weather measurements
you have daily counts of website visits
monthly sales total and so on so when
you talk about time series that can also
be irregularly spaced and sporadic for
example timestamp data and computer
systems event log or history of 911
emergency calls now when we work with
time series data for example here I am
taking a energy data set we can see how
techniques such as time based indexing
resampling rolling windows can help us
explore variations in electricity demand
and renewable energy Supply over time
now here we will look at some aspects of
this data set which I am considering so
there is this is open Power Systems data
set and here is the data set I have we
can look at the data set now this is in
a simple format it has time it basically
has values for consumption and then you
have data for wind and solar and wind
plus solar so in certain cases you have
only the date and the consumption but
then if we scroll down we will also find
data for wind solar wind plus solar and
so on so this is a Time series data set
which we would want to work on sometimes
you may also have the data collected
which just does not have the time but it
may also have time stamp that is it
would have say hour minutes and seconds
and that can also be worked upon so
let's consider this data set and let's
work on this project where we will
Analyze This Time series data set now
here we can work on this time series
data we can basically create some data
structures out of it such as data frames
we can do some time based indexing we
can visualize the data we can look at
the seasonality in the data look at some
frequencies and also do some Trend
detection now when you talk about this
data set it has electricity production
and consumption which is reported as
daily totals in gigawatt hours and here
are The Columns of the data which I was
just showing you so you have data you
have consumption you have wind you have
solar and wind plus solar so this is the
data we have and we will basically
explore say electricity consumption and
production in Germany which has varied
over time so some of the questions which
we can answer here is when is
electricity consumption typically
highest and lowest how do wind and solar
power production vary with seasons of
the year what are the long term Trends
in electricity consumption solar power
and wind power how blue wind and solar
power production compared with
electricity consumption and how has this
ratio changed over time we can also do
wrangling or cleaning of this data or
pre-processing of data and create a data
frame and then we can visualize this now
let's see how do we do that so I will
open up my R studio and let's look at
the data set so here is the data set now
I'm picking it up from my machine you
can also pick it up from GitHub so all
the data sets or similar data sets can
be find in my GitHub repository and here
I can look in the data sets you will
find a lot of different data sets here
there are some time series data sets
such as power I can search for power or
you have basically coal
or you have this
opsd Germany daily data set and there
are many other data sets which you can
work on now to
get the documentation on this project
you can also look in my GitHub
repository and you can search for
repositories and then basically you can
look in data science and R and here
there is a project folder where I have
given the documentation sample data set
and also your time series analysis
related document this is also the code
which you can directly Import in your R
studio and you can practice or work on
this project so let's see how does that
work
so first thing is we will create a data
frame
from this data set now here if you see I
am using header as true so that it
understands the heading of each column
I'm also giving row dot names and I'm
specifying date so there is this date
column in the data set as I showed you
earlier let's look at it again so you
update consumption wind solar wind plus
solar so you can suggest that date
should become the index column which can
be useful so you can do this now let's
just create this let's look at what does
this data frame contain and here if you
see it shows me some data which has been
now as a part of this data frame
structure it starts with consumption
wind solar wind plus solar and if you
see this one is becoming my index column
so I can always do a head and look at
part of the data frame using head or
tail so look at the first records so
let's see this now that shows me the
head data I can also do a tail and look
at the
ending values so if you closely see here
we have wind solar wind dot solar and
that basically has n a values so there
are missing values but let's look at the
tail and that tells me that there is
some data available for wind and solar
and wind solar now we can always look in
a tabular format using View
and we can look at the data so this
shows me that there are values in these
columns we see any values but if I
really scroll down
I can see some values which would be
available for wind and solar and wind
solar so I can just use view now I can
look at the dimensions of this
particular object
and that tells me there are 400
4384 rows and four columns you can
always look at the structure that is
check the data type of each column which
can be very useful so if I see here I
don't see the date column because date
column was considered as an index which
can be useful but I also look at my
other columns so they are of the num
types so that's the data type for each
attribute or each column here now we
would be interested in looking at this
date column so let's look at the data
type of this date column
now if I try to do this this will show
me that this is null because date as a
column does not exist because we created
it as an index so if I look at row names
and then I search for my data show me
the index column or row dot names it
tells me these are the values that's the
date column which we are seeing here now
we can access a specific row by just
doing a my data and give the index value
or row name value so let's look at that
and that shows me based on this index
you are looking at the value you can
obviously search for a different date
something like this you can also pass in
a vector and you can give range of
values so that is 0 1 2006 to 4 of
January and we can look at this one so
it shows me
these are the values so here actually
I'm not giving a range but I'm just
selecting multiple values from row dot
names now we already know that in R you
have a summary function so you can
always do a summary and that gives you
for each column it gives you minimum
first quartile median mean third
quartile and maximum values so we are
looking at consumption we are looking at
wind solar and wind dot solar now this
is good but then if I would want to
really visualize the data access the
data do some analysis then it would be
good to take all the columns and then we
can later decide to change the data type
of say date column if you want to use it
so earlier I was using date as row dot
names or the name of the rows or index
what you call in any other programming
language so here I will just use my data
set and I'll say header as true I'm
calling it my data 2. let's look at the
data and this one shows me
five columns wherein My First Column is
the date consumption wind solar and so
on now looking at the structure so let's
look at the data type
so it tells me that if now I'm
interested in looking at the date column
from my data to data frame it tells me
it is a factor with 4 384 levels and
these are the values so it is not in a
date time format it's a factor now what
we can do is we can convert this into a
date format how do we do that so let's
have a variable X and I'm going to use
as dot date function and I'm going to
pass in my date column so that's
assigned to X now let's look at the head
of X and it shows me the values we will
also see what kind of class it is and we
will look at the structure of X so class
already says it is date type and look at
the structure so it shows me the format
now we have converted this column or
column related value into X now how do I
basically extract values out of it or
make it a part of data frame so first I
will use so all once it has been
converted in date format I will go for
as dot numeric and here I will create a
variable called year and I will just do
a format on X which is basically of date
type and there I'm saying percentage y
so that will get me the ear component
out of this let's look at the values
that shows me year component
now similarly we can get the month out
of this and then basically look at the
month values we can get the day out of
it and we can get the day component now
if I look at my data 2 which we had
created earlier this basically had date
consumption wind solar wind solar so
what I can do is I can add these
extracted columns such as year month day
to my data frame using a c byte that is
column bind and I will assign it to my
data 2 again so let's do this and now if
you look at head it shows me date so
that should be date format consumption
now this one might not be date format
but we'll see you have consumption wind
solar and we have extracted the year
month and day which can help us for
group by we can do some aggregations we
can do a plotting and we can do various
things by these additional columns now
let's look at first three rows here so
I'll say one is to 3 for my data 2 and
that shows me some data here you can
always do a head and look at the sample
of data so that basically shows me month
day your columns and then you have your
date now what we can do is we would want
to visualize this data we would want to
basically understand the consumption now
as I said if we want to visualize the
data say for example I want this which
is consumption of data over years and
this one is in terms of gigawatts per
hour as we were mentioning here gigawatt
hours so if I would want to create this
visual to basically understand the
pattern of the data how do we do it so
we can you create a line plot of
full-time series of Germany's
electricity consumption using the plot
method now how do we do that so here
one of the option is I can straight away
use the plot method I can then say what
would be in my x-axis what would be on
my y-axis what would be the type of
graph I would want to plot what is my
name on x-axis y-axis and this is the
simplest way so I am saying my data 2
I'm extracting the year column and here
I am taking the consumption so let's
create a plot and here if you see we are
looking at a plot we do see some tick
times and we see that the data has been
divided with every two years so from
2006 onwards to 2016 but then really
this data does not give me uh you know a
very useful way of looking at the rate
or understanding it might be what I can
do is I can use the same way but I can
give apart from x axis and y axis I can
say that limits that is X limit is 2006
to 2018 and Y limit is from 800 to 1700
so we can do this and let's look at this
again this is a plot but it really does
not help me in visualizing and
understanding the data so what are the
better options
I can go for multiple plots in a window
as of now we are just sticking to one
plot in window so if you would want to
have multiple plots you can always
change the value here and make it two or
three that will say how many rows and
how many columns so as of now we will
just keep it as it is par
MF row now
if I would want to plot I can straight
away give the column name so I am
interested in getting the consumption
now I can just do a plot I'll say my
data 2 and I will choose the second
column which is consumption which we saw
here
from our data so consumption was the
second column so I can just do a plot in
a straight away way without mentioning
your x-axis y-axis limits and so on and
if you look at this this one is giving
me a pattern now here I am looking at
x-axis y-axis which is not really named
we do not have a name to this graph and
we are looking at the data it does show
me some kind of pattern but might be we
can make it more meaningful so I can do
it this way where I say my data second
column let's give access as here x axis
y-axis is consumption now that has
changed the x-axis and y-axis now I can
also give some more details I can say
type should be line I have the line
width I'm saying color is blue and let's
do this so this looks more meaningful
maybe shows a wavering pattern of
consumption over years I can also give a
limit of x that is 0 to 2018 and that
basically shows me the range now we can
change that and we can be more specific
and saying X limit should be 2006 to
2018 and let's look at this now this one
once you have given a proper limit it
shows the line graph and it shows what
was the consumption in 2006 and over a
period till 2018. I can then
use any of these options are fine but it
depends on what and whom you are
presenting the data or what kind of
analysis you are doing so I can do a
plot I can choose column second X lab
which is x-axis y-axis type is line
width giving X limit y limit and then
I'm giving a title to this which is
consumption graph and then basically you
are looking at the line graph now those
are the options which you can do either
you could be very specific or you could
just give
your column which you want to plot or
obviously make it more meaningful by
giving all the details now what we can
do is if we would want to look at this
data and understand it better rather
than just looking at a simple line I can
take the log values so here I am saying
log of my data to Second column so I'm
taking log values of consumption and I'm
taking the difference of logs so I can
say difference and then you can
basically increase or decrease this by
multiplying it by some number so rest
Remains the Same I am changing the color
and let's look at this plot and you see
this basically is giving me a better
pattern which makes meaning here we see
the log values so this is you are using
a simple plot function
in R you can also use ggplot now for
that we can install the ggplot package
it's already there in my machine so I'll
say no I will access this by using the
library GGG plot 2.
and now I can use ggplot to plot so
the way you specify here you can say my
data 2 that's the data frame I am saying
type as o and when I'm saying line I am
basically going to use x axis which is
here Y is consumption and let's look at
this plot so again we are back to the
one which we were doing earlier really
does not make any sense gives us some
data but then really does not give me
enough information I can
in my Aesthetics I can say x as year Ys
consumption I can do a grouping and then
I can give line and plot so again we
have some information but really does
not help me right now let's look at
other example so I am just doing the
same thing here and I'm looking at line
type being test I'm using the GG plots
other methods such as geomline and
geompoint to give me more information
and if I look at the
plot it does give me data it tells me
what are the different values it gives
me some kind of pattern but I would
still prefer the way we were doing with
plot
now we can change the color and
obviously add details to it so what we
see is when you use the plot method
which I did earlier it was choosing
pretty good tick locations that is every
two years and labels the years for the
x-axis which was helpful right but with
these data points which we were seeing
here
or say for example this one
or say this one or say this one we are
looking at some data but then that
really is quite crowded and it is hard
to read you can look at the values but
then it really does not give you enough
information so we can go for plot method
but then we will see how we can consider
different data now if I would want to
plot the solar and wind time series so
let's see how do we do that
so wind column is what I'm interested in
so first thing is it is always good to
find out the minimum and the maximum
values in every column so I'm saying
minimum I am saying let's put in here my
data to
and then let's look at the values so we
are looking at the columns we know
consumption is the second column when
does the third column and you have solar
as the fourth and this one is the fifth
so let's say let's find out the minimum
of each of these columns which we would
want to plot so let's say minimum of
data third column and here I'm also
saying remove the any values because we
do not want to consider the any values
so let's let look at the minimum that
shows me
5.7757 what is the maximum value it is
826 so that also helps me in giving a
limit if I want to plot wind on y-axis I
can give a y limit from 5 to 850
consumption wise let's find out the
minimum from second column and maximum
and similarly for solar find the minimum
and maximum and wind plus solar minimum
and maximum so this will be helpful when
you would want to plot multiple graphs
or give some limits so that's fine now
for multiple plots as I said instead of
having one plot let's plot consumption
and wind and solar and try to see a
pattern so I can say par function and I
will say three rows and one column so
now when I start plotting you will see
you will have multiple plots in one
single window so let's see how we do it
so here
let's look at plot one so this one is
consumption as we did earlier and let's
look at the data so that gives me some
data you can always do a zoom and you
can look at the data you can basically
expand this graph or you can reduce this
graph to see what kind of pattern we
have in consumption similarly we can
basically choose date being x-axis my
consumption being y-axis right so this
is being more specific because here we
have a range but it really does not give
me enough information so I will
basically give x-axis Y axis I will give
the name that is daily totals and then I
will basically give consumption color
and Y limit based on my minimum and
maximum limits so let's do this and now
we can look at the data here so let's
see this data makes a little more
meaning because we are looking at the
dates
and let me do a zoom so it shows me all
the dates it shows me the data points it
shows me how the data pattern is
changing for consumption now this is for
consumption so what we can do is we can
also extract specific data so if you see
here I have done some testing where I am
saying okay I would want to get a date
specifically
I would want to extract some value so we
are looking at the date column but if
you remember we did not change the data
type we just change the data type of
date column we extracted year month out
of it it would be good if we can convert
a column into date time format and put
that in our data frame now let's look at
the plot 2 this is mainly for your
column which should be consumption and
wind and solar so here I see it is solar
data and I can plot this one to see how
it looks like
and that tells me from 2006 onwards we
have some pattern I can be more specific
where I say I would be giving date and
then
the column for solar x axis Y axis what
is the type what is the Y limit and what
is the color it is always good to
specify your X and Y axis give a name
rather than let it automatically pick up
now this makes more meaning because it
shows me some dates similarly we can do
for wind so either you do it just by
giving the column
or you give your X and Y axis so let's
look at this one
and this shows me the data so we can
choose plot 3 this one we can choose
plot 2 we can choose plot one and we can
put all that data in one graph so that's
when you are putting in multi plots in
one particular graph you can always do a
zoom you can always look at the data
right and this is usually useful to look
at the pattern what kind of pattern we
see what data we have and so on now
moving forward so we have seen how you
are creating these plots all in one
window let me reset this back to one
plot per window and let's basically plot
time series in a single year so what we
have seen is that when you look at the
plot method it was quite crowded then we
looked at solar and wind and if you
compare that you will see your
consumption pattern your solar pattern
your wind pattern and basically we can
see from this particular data some kind
of pattern so electricity consumption is
highest in the winter
where we will see what is the
consumption
is it highest in Winter or is it in
summer we can see that by breaking a
year further into months we can see that
but we see a pattern which goes for
every year or every two years being
highest at a particular point of time
and then it drops down so electricity
consumption is highest in Winter and
that might be due to electrical Heating
and increased lighting usage and lowest
in summer now when you look at
electricity consumption appears to split
into two clusters we can always look at
the consumption one with oscillation
centered roundly around 1400 gigawatts
so you can always look at 1400 gigawatts
and you see all the values here which
are in that particular consumption
another with fewer and more scattered
data points entries roughed around 1150
so if you really expand this you can see
you will have lot of data points at this
point
now we might guess that these clusters
correspond with weekdays and weekends
which we can see if you break that data
into yearly monthly weekly and so on now
if you look at solar production that is
highest in summer when sunlight is most
abundant and lowest in winter so
obviously when you are making or
gathering some insights when you're
looking at the data you are also using
your domain knowledge your business
knowledge you are you know knowledge of
business to understand how this goes if
you look at wind power production that's
again highest in Winters and drops down
in summer so due to stronger winds and
more frequent storms and lowest in
summer so there is some kind of
increasing Trend in wind power
production over years which we can see
here
over the years
and all the time series data what we are
looking at is referring or showing us
some kind of seasonality that is we are
looking at seasonality in which a
pattern is repeating again and again at
regular times
at regular intervals so if you look at
consumption solar and wind time series
that oscillates between high and low
values on a yearly time scale which we
can break down and see I'll show you
that it corresponds with the seasonal
changes in weather over the year so
seasonality does not have to correspond
with meteorological reasons for example
if you look at retail stale sales data
uh that will show you yearly seasonality
with increased sales in particular
months so seasonality when we say can
occur on other time scales so the plots
what we are seeing here they are fine
but if you look at those plots they
might show some kind of weekly
seasonality also so in your consumption
corresponding to weekdays and weekends
so let's plot for one single year now
how do I do that so first is I will look
at my data too
that shows me the structure it shows me
date which is Factor other columns which
are all numerics now like we did earlier
I'll repeat this step where I'm going to
convert the date column into date type
look at head of it look at class of it
look at the structure of it right and
then what I want to do is I want to add
this
and to my data frame so I will create a
variable called mod data and this one
will have as data and I'm formatting the
value of x which is date time into month
day and year so let's do that and now
you look at the mod data which I created
like modified data so this is the format
I have it is in date type if you
carefully see here and then I can look
at the head of it so it saves me mod
data now we are what we did here is when
I said my data 3
so my data 3 we did a c bind and I did a
mod data which is going to add this
column to my other Columns of my data
too so my new data frame is my data 3.
let's look at the structure of it and
you see there is this date column I can
delete it I can remove it I can let it
be right so that depends on our choice
might be we want to once our analysis
done we want to remove the mod data
right so we can keep both of them now
let's basically extract data for a
particular year now how do you do that
so this is some wrangling so I will say
my data 4 let's call it my data 4 and I
will use subset function so subset will
work on my data 3 that's the data and
what I'll do is I will do a subset how
do how is the subset found so I'll say
take the mod data column the value
should be greater than or equal to 2017
and should be less than
2017 December 31st so I'm getting data
for one year and I'm storing it as my
data 4.
let's get the head of it and you see we
are specifically looking at 2017 related
data now let's do a plotting of this
where I will only
create a plot for one year so I'm saying
my data 4 that's my new
data what we got
so
here I am going to take the first column
which is mod data
I am going to take the third column
which is consumption so I'm looking at
the date format for one year consumption
values for it and then rest of the
things as we have done earlier let's
look at the plot and this makes more
meaning right so when you look at this
plot it tells me Jan to Jan it shows me
some kind of pattern where I have
divided the year into months
right and it is broken down into say two
months Jan and March and May and July
and so on but we still see a pattern and
that gives me good understanding of
pattern where I've broken it down into
months so this is where you have taken
time series in a single year to
investigate further and this is what we
see right now we can clearly see there
are some weekly oscillations what one
more interesting feature is that at this
level of granularity that is when you're
looking at yearly data there is a
drastic decrease in electricity
consumption in early January and late
December during the holidays or probably
we can assume that this is holidays now
I can zoom in further and look at just
Jan and Feb data
let's see how we do that and let's see
how we work by zooming in the data
further so to zoom in the data further
let's see how we do it now here we have
this my data 4 which is basically having
a subset right so let's work on this one
so I will say my data 4 which earlier I
was taking data 3 I was doing a subset
and I was giving the date but this time
I will make it more narrower so I'll say
my data 4 I will say subset from my data
3
and I will choose mod data column which
we have modified with the date format I
will choose the starting date as
1701 that is Jan and then let's go till
Feb and let's create this
now let's look at the head of this so it
shows me we have the data which is Jan
and then you you can basically look at
more on this now again as I said earlier
let's find out the minimum of this from
the First Column so that is basically
your mod data so let's look into this
one and that basically
will give me minimum and maximum let's
look at the value so this one tells me
Jan 17 January 1 and maximum is
your
Feb 28th second month 2017 so we are
actually looking at two months data here
let's look at the Y minimum so this is I
will look at column three now what is
column three consumption so let's look
at the minimum value for consumption
maximum value of consumption let's look
at the values which can be given as our
limits now this is the minimum and
maximum now let's do a plotting for this
data which has been narrowed down for
consumption based on my data so I am
saying My First Column which is mod data
and then third column which is
consumption I am giving some naming
convention for sorry namings for your
x-axis y-axis what is my consumption
or what is my title here what is the
color and then you see I'm using X limit
to give the minimum and maximum limit
and Y limit so let's look at this data
and if you look at this data it is
specifically for two months and again I
can look at the pattern here what I can
also do is I can add some grid here
so I can basically look at this data and
make more meaning out of it so it is
bi-weekly data you can see now I can add
a line here using AB line and then I can
basically choose what lines I would want
to add horizontally so that basically
allows me to dissect the data and look
at data in a more meaningful way I can
also add vertical lines so vertical
lines as I'm saying sequence will be
minimum maximum and I'm saying an
interval of seven
so let's do this and this basically has
added some lines every week and you can
see at the end of week it is dropping
and then it is starting again it Peaks
somewhere in the mid of the week and
again it drops down so this is you're
looking at your consumption data right
now what we can also do is we can create
some box plots so when we looked at
zooming in data for Jan and Feb you can
add some data points like this so
consumption is highest on the weekdays
as I showed you here and lowest on the
weekends so this is what we are seeing
when we are breaking the data or zooming
it further for a couple of months so we
have vertical grid lines and we have
nicely formatted tick labels that is Jan
first and 15th Feb first and so on so we
can easily tell which days are weekdays
and weekends with use of these grid
lines and basically breaking it down so
there are many other ways to actually
visualize your time series data
depending on what patterns you're trying
to explore you can use Scatter Plots you
can use heat maps you can use histograms
and so on now moving further we would
want to explore the seasonality right so
when you further explore the seasonality
of our data we can use box plots
basically to group the data by different
time periods and display the
distribution for each group now how do
we do that
let's come here and let's see how box
plot works so I can just do a simple box
plot and I can choose my consumption
column and that gives me just the
consumption data but this really does
not give me any meaning I can look at
solar data I can look at the wind data
and we can also see some outliers here
so we can create box plots but if we
would want to do a box plot what is box
plot it is basically a visual display of
your Phi number summary that is you want
to look at your mean median you want to
look at your 25th percentile 50
percentile or 75th percentile so we can
use a quantile function use the
consumption column and then you
basically give a vector which shows you
Phi number summary so that's your
quantile and then let's do a box plot so
if you are looking at quantile it tells
me what is the minimum what is 25th
percentile 50 75th and 100 that's from
my consumption column so let's create a
box plot for consumption let's give it a
name as consumption let's give y-axis as
consumption and a limit for y-axis now
that's my consumption graph so I can
look at yearly data now that will make
more meaning rather than just looking at
the complete consumption data so how do
we do it yearly so we will say
consumption and then I will say the year
column so it is consumption but grouped
based on year
so here I can give x axis y axis and I
can give y limit so let's create this
and this makes more meaning we can give
some coloring scheme here but now I'm
looking at 2006 2007 8 9 and so on and
we can look at the data what is the
range right it gives me 5 percentile or
sorry five number summary of the data
per year and it basically allows me to
look at the seasonality of this
similarly we can create box plot by just
giving consumption yearly grouped and
here I am giving the title as
consumption y-axis x axis and y limit
wherein I can also use less so this is
one more feature which you can do and
that basically will give me the tick
points if you compare this one to the
previous graph so when I created this
previous graph I had two 2006 2008 and I
had from 600 to 1800 and if I go for the
next one I am basically seeing more
useful information now let's look at
monthly data so I would want to group it
based on months
and let's create that so this gives me
the monthly data where I'm looking at
months
and I could select a particular year or
I can just do a grouping based on months
so I can have multiple plots to see a
difference here so let's do this now
let's create a box plot for consumption
which is monthly data and let's give it
a color let's look at the wind data
which is again grouped monthly and let's
look at the solar data which is grouped
monthly now if I zoom in it basically
gives me the seasonality of the data for
your wind for your consumption for your
solar so what we are doing is we are
creating these box plots which are
giving us values now what I can also do
is I could look at the day wise also but
before we look into this how do I infer
some information from these box plots
which are being created so this is what
we have done where we are looking at the
data for month and these box plots give
me ear seasonality which we were seeing
in earlier plots but give some
additional insights so if I look at the
data here it tells me the electricity
consumption is generally higher in
winter
now this is based on months so we can
see consumption is higher in Winters and
lower in summer so we can obviously look
at our plot we can see where it is lower
where it is higher and then we can look
at the median and lower two quartiles
are lower in December and January
compared to November and February so
that is you look at the quartiles and
you will see that
the median and lower two quartiles are
lower in December and January
here Jan and December so you can look at
from my plot now this is giving you some
idea on seasonality
now that might be due to business being
closed over holidays now this one we
were also seeing when we looked at time
series for 2017 only and box plot
basically confirms that there is this
consistent pattern throughout the years
now when you look at your solar and wind
power production both will give you a
year seasonality what we are seeing here
and
if basically I look at the data so it
depends on what parameters you are
choosing but if you look at solar it
will reflect the effect of occasional
extreme wind speeds associated with
storms and other transient and since we
are grouping it based on months we can
see this pattern is quite evident every
year
now what we can do is we can group the
data day wise so here let me again reset
this to one plot per graph now I'll say
box plot I'll say consumption which is
group based on day now we know that
there is a day column and let's give a y
limit and let's look at the data so this
is where I am grouping the data day wise
so you look at 31 days and you look at
the box plot so this is where you are
plotting it on a daily basis
so you can look at the data you can
break it down to a particular week so
here I have given day and I have chosen
all the 31 days but I can break it down
to a week and I can look at the data so
if we look at the data per week or per
day we can basically infer that
electricity consumption where I'm doing
a consumption group by day is higher on
weekdays than on weekends
so time series with strong seasonality
can often be represented with models
that can decompose signal into
seasonality and long Trend now this is
an easy way now how do we look at the
frequency of the data that could be
interesting to see so let me look at
say the yearly data
which we were seeing here
now let's go further and here we have
looked at data so what we will do is we
look at the frequency now when you look
at the frequency when you talk about
frequency in your data so we have the
modified date column which gives me a
frequency and if we really look into the
data that will tell me that the data is
on a daily basis so for that let's look
at my data 3 again which gives me data
and you can just see all the data's data
or dates are in sequence so your 22 23
24 25 26 and so on I can look at I can
access a d plier package
that is basically allowing me to work in
a better way now I can look at the
summary of this and for all my columns I
am seeing what is the minimum Phi number
summary date and consumption so date
does not show me anything because this
is not in a date format it is just a
factor but other things have the fine
number summary so we are looking at wind
plus solar we are looking at year and
month and day and all these columns now
what we will do is we will want to find
out the sum of each
column how many entries does it have and
we will say the value should any value
should not be considered so let's look
at this one so it tells me for my
particular columns so let me run this
again
and that shows me for each column how
many values you have and these counts do
not include the N A values now similarly
I can find out specifically for
consumption I can find out is there any
n a value so I'm saying is Dot N A and
let's find out if there is any n a value
or missing value in consumption it says
0 okay that's good if you look in wind
it tells me there are
1463 entries which are n a
similarly solar similarly wind dot solar
or wind plus solar so it gives me a
count of n a values that is missing
values and also values which are not
missing so to understand frequency what
we can do is we can find out the minimum
on the date that is the First Column and
I am saying RM
n a DOT RM is true that is get rid of n
a values and find out the minimum and
let's look at the minimum value this is
the minimum from my modified date
now if I would want to get the frequency
I can basically use sequence function so
I can say from X minimum that is the
minimum value I want to look at the
frequency that is day wise and let's
just look at five entries and see if
there is a day by day frequency
so let's look at the value of this and
obviously it tells me there is daywise
frequency so that allows me to look at
the frequency look at the type of it it
is an integer class is a date so
similarly we can say from X minimum we
can basically look at the frequency
month wise and I can again look at five
records so that shows me monthly data
right so I can extract the data for
frequency similarly yearly data and
that's also very useful now we can
select data which has any values for
wind so how do I do it I would want to
find out the wind column and I want to
find out where the values are in a so I
will create a variable and here I will
say my data 3 and then I give a
conditional where I say is n a
in the column so let's do this now once
I have done this
once I have done this I have said that
my selected wind data from my data 3
where we said any values and I will give
the names to this so name should be in
my data 3 I'm interested in mod data
consumption wind and solar so these are
the four columns I'm interested in let's
look at first 10 records here or first
10 rows so that tells me these are the
values where wind has n a or missing
values
I can always do a view and that gives me
the complete data so it basically shows
me
1463 entries
and here it shows me all n a values so
you can look at all the way to the end
and it shows me wind has any solar does
have some value here
in the last row but then also if you see
the numbers have
a difference so you have one four six
one and then you have two one seven four
so there is a difference so there is
some data in between where wind has some
values so we have found out any values
now what we will do is we will select
data which does not have any values so I
will call it cell selected wind 2 I'll
again use my data 3 I will say which but
now I'm saying not n a from this column
and I will select the data for the
columns so I am interested in looking at
10 records and this shows me not any
value so no more missing values so if I
really look at this data as I saw
earlier which has n a and if I look at
these values which are not any for the
wind column so looking at these two
result we will know that in year 2011
wind column
has some missing values
so let's focus on year 2011. so how do I
do that let's call it a different
variable I'll say my data 3 I will say
here when I say which where we were
saying n a here I will say the year
should have a value of 2011 and I want
all these columns
let's look at the data here and this is
showing me 2011 but
V
are not seeing all the values so there
are some values but then there are some
missing values also for 2011 based on
whatever analysis we have done so let's
look at the class of this it is
basically a data frame to a view and
this one will help me in finding out
where are the N A values so if you just
scroll down
looking at all the data let's search if
wind column has a n a or a missing value
and I will see
if there is any missing value in which
column or which row it is for the wind
column so we have all the values which
are existing
I could select and search for one
specific value and I'll show you how we
can do that so here let's scroll all the
way down so it's like you're exploring
your data and seeing is
wind column having n a or missing value
for a particular row and let's scroll
here and here you see there is a missing
value for one particular row so 13th
December 2011 has been value 15 December
has wind value but your 14th December
does not have right similarly we can
search so there was only one entry which
was missing now that could be for some
reason might be it was not calculated
might be it was not tabulated so we have
a missing value and that can affect my
plotting that can affect my analysis so
let's look at the number of rows in this
which will tell me how many rows we have
for 2011 so it tells me 365 so that is
basically the number of days in a year
now we will find out if there were any
values so we earlier checked total
number of any values per column
that is in your row number 265 to 269
we can see here 265 to 269
so this is where we were seeing are
there any n a values right so let's go
back here
and
we want to find out the number of any
values for a particular year how do I do
it so I can just do a sum I will say is
n a now I am interested in my data 3
wind column and I am saying my ear has
to be 2011 but I'm finding out the N A
values
so let's do this and it tells me 1 and
that's right that's what we saw when we
did a view let's see how many non-na
values you have and that is 364 so that
basically
satisfies my logic so it's 364 plus 1
missing so there are 365 let's look at
the structure of this it tells me you
have modified date and date format you
have consumption wind and solar now
let's create a variable
selected wind 4 I will say win 3 that is
which was having all my n a n non-na
values for 2011 I will say let's find
out the n a value
because I am interested in finding out
that particular role so I'm saying find
out where the value is n a and I want
all the columns
let's look at this one and this is my
specific
row which has a n a value
now we know that data follows a device
frequency which we have clearly seen now
let's select data which has n a and
non-na values so let's say let's call it
test one I will use wind 3 which has n a
non-na values but now I will say I want
the modified date which should be
greater than 12 12 2001 now remember we
had when we were doing a view we saw
that one particular day or what we see
here 14th of December there is no date
so I will select a subset of data which
includes this n a and non n a that is
might be I can take 13th of December and
15th of December so let's start from 12
12
so the date should be greater than 1212
that means 13th and it should be less
than 16 so that is 15th and the columns
right so now we have some data let's
look at this so I have a I've selected a
subset of data I could have done this
using subset also so I have any and
non-na values now why are we doing this
so sometimes you might have some data
for a particular column and you may want
to find out if there are any missing
values might be you want to fill them up
or replace them with something so that
is usually useful when you are doing a
trend detection so say for example you
have data for every month and might be
in one one of the months you have missed
or might be you have data for every year
collected monthly and then in one of the
years for a couple of months you don't
have the data like I can say 2016 I have
data for all 12 months 2017 all 12
months 2018 might be a don't have data
for March and June 2019 I don't have
data for same months so I can forward
fill or backward fill them using the
previous years same month data so we can
do that so here I have test data where I
have extracted a subset of data I can
look at the class of this it is a data
frame structure of this it has the
columns now let's use the library and
function and use the Tidy r package
and what we will do is we will fill it
up so I will use test one I will fill
the wind column which has a missing
value now once you do this if you notice
it has done a forward fill so it has
taken the previous value and it has just
filled up that so you can fill up the
data using different directions such as
up and down left and right and so on so
we can take care of missing values in
our frequency data which allows us to
basically
analyze the data in a better way now
here we will want to also look at some
more data so this is to deal with
frequencies of fill column
wherein you can take care of missing
values forward fill so filling values
can be done in different directions as I
said and you may want to First convert
your time series to specified frequency
if your data does not have a frequency
but we had now if you do not have a
frequency might be you can convert it
into a frequency such as weekly daily
monthly as I showed you and then
basically you can do a forward fill
for the values so for example if I have
my data I can break it down into weekly
and then look at the values and if there
are any values missing for weekly data I
can use a forward fill so that can take
care of my frequency data then
let's look at the trends of the data
which is the last part of this project
so basically let's look at the trend So
when you say Trend what does that mean
so in Time series data
you always have some kind of trend so
that will exhibit some slow gradual
variability in addition to higher
frequency variability such as
seasonality and noise now to visualize
these Trends what we do is we use what
we call as rolling means so we know how
our data is spread over year or month or
day but how about looking at a rolling
average and see what is the difference
so a rolling mean will tend to smooth a
Time series by averaging out the
variations and frequencies
so this can be higher than the window
size so there is something called those
windowing where you can choose a set of
time frame you can also average out any
seasonality on a time scale equal to
window size so this will allow you to
look at lower frequency variation in the
data
so when we are looking at electricity
consumption time series we already saw
there is a weekly pattern there is a
yearly seasonality which we saw using
box plots so we can also look at the
rolling means of the time scales how do
we do that so for this you can use some
package like zoo and then you can
basically use a rolling mean using the
zoo package and you can say what is the
frequency with which you want to
calculate the rolling mean now how do we
do this let's look at this data so here
I am going to my look at my data 3 which
we have been using so far now let's call
it a three day test you can give it any
name I'm going to use my data 3 I'm
using the piping function
now I will use D plier and I will
arrange the data descending in here now
you can always break it down step by
step and you can see the result of this
so I'm going to arrange this data in
descending order of year
so obviously my last one 2017 or 2018
will be on the top you want to group the
data by year so it depends on how many
years we have we'll see so you can group
the data by year now this data is then
used to basically mutate so mutate
function is going to allow me to use
this rolling mean so I'll call it as
says 0 3 day so I'm going to calculate a
rolling mean every three days for my
consumption column and
basically let's ungroup this so let's
see how this works
sorry yeah let's look at this and here
when I'm doing a three day test
let's look at the result of this and
then I'll explain this so if you see
here we have the test three day column
now this has the rolling average now
what does that mean so first value here
what we see is one three six seven is
the average consumption in 2017 from the
first date with the data point on either
side of it that is you can look at this
date so one one three zero then you look
at
you are looking at the value one three
six seven here so you look at one one
three zero one four four one one five
three zero if I take a mean of these so
for example if I would just do this part
and that
is giving me mean okay because I have a
comment so let's basically add anything
as comment and then let's do this so it
saves me one three six seven that's what
we are seeing here right so you've got
getting a rolling average every three
days similarly if you want every five
days it takes the five values and it
gets the mid value right so you can
always find out the mean rolling mean
for a particular frequency now let's do
that for seven days that is weekly data
and yearly data that is 365 days so how
do I do it same logic my data test
now I'm using my data 3 I'm arranging it
in a descending order I am grouping by
year so when you do a group by year so
earlier when we did a grouping by and
when we looked at the data it was
telling me how many rows we had right so
let's do a grouping by year and let's
say test zero seven so that's a rolling
average every seven days and I'm also
saying take care of the N A values
similarly I'm getting rolling average
every 365 days might be you can do
quarterly might be you can do half
yearly and let's do this so let's create
this my data test and let's look at the
result of this so I will use my data
test I will say arrange based on
modified date now we know there is a
column called modified date I want to
just look at 2017 data so I'm doing a
filter
right and then I will choose what are
the columns I'm interested in so I will
look at the 7 and 365 day and let's look
at say first seven records so let's do
this
and that basically gives me the
consumption value modified date year and
my rolling seven day average order of
seven day mean which is for first seven
days and then 365 you will not see the
data here but if I do a view on this I
can basically see the values so you can
always select a particular column to see
the values
these are the values for every seven day
rolling average this is for 365 days
every 365 days so you see all the values
are missing but every 365 entry you will
have basically some data
now let's do a plotting of this and
basically visualize this data which we
are seeing rolling average so let me
first do a plotting one plot per graph
and let's do a plotting I will take
consumption data
x-axis y-axis
color and give a title to this so let's
create this and that's my consumption
data which is spread over a period of
time and that's fair enough but now
let's add some more plot to this so I
will add the seven day rolling average
to this so for second plot to be added
in the same one in R you can use points
so I will say points I will choose 7
data column type is line width X limit y
limit and color so let's do this and
that's my pattern seven day rolling
average which basically gives me some
kind of trend
similarly I can add one more here and
this time I will choose the 365 day
and look at the pattern lines so now you
see some Dots here well you could do it
in a different way so I can just add
Legend to this and I can say Legend will
be aware in x axis and y axis so I'm
saying it will be 2500 and Y is 1800 so
my Legend will come in some in here I am
saying my Legend will have consumption
test and this one I can give some names
I can give what is the color I can say
what kind of
Legend it explains what is
for each color and then basically a
vector so let's add a legend to this and
I've added a legend now you can do a
zoom and look at the data
and
here I see that my x-axis is fine but
y-axis is going a little about of my
plotting area so I can actually change
that so here I have 1800 how about
making it 1600 and let's look at this
one so we can basically uh go for this
one and start again here plot end points
and line and then add a legend right and
you can basically place your Legend
anywhere in the plot so this basically
is giving me the trend what I'm looking
at my rolling average so similarly you
can look at the trend for wind and solar
data so what we are seeing here is when
you look at Trend this is one more way
of looking at it you can always create
plots in different ways so seven day
rolling mean has smoothed out all week
Lee season personality which we were
seeing Here in My Graph where you look
at every seven day preserving the yearly
seasonality so seven day will tell that
electricity consumption is typically
higher in Winter and lower in summer so
better is you break it down uh yearly so
here if you look at every year you can
see when is winter when it's summer what
is the seasonality what your Trend what
you are seeing here and if there is a
decrease or increase for few weeks every
winter so similarly if you look at 365
now as you said as I said rolling
average basically
reduces the variation so if I look at
360 by Rolling mean we can see long term
Trend in electricity consumption is
pretty flat now that's what we are
seeing it's kind of pretty flat there is
not much variation over years if you
really join these dots so
we can basically see some highs and lows
and that gives me a trend now this is
how you can do a trend detection and
similarly we can do plotting for wind
and solar so this is a small project
which I demonstrated using R now all
this code
which you have here in the form of a
project.r file you can find here in my
GitHub page this is a document which
explains some things feel free to
download this and you can add details to
it this is the sample data set which you
can also find in my repository in the
data sets folder so continue learning
and continue practicing r
everyone knows there is an enormous
amount of data generated every second it
has become crucial to analyze those data
as they can be very useful everyone is
aware of the importance of big data
analytics from Big retail markets to
education from the banking sector to
clothing brands it is everywhere how do
you handle them how do you process them
at all the answer to all these questions
is big data analytics hi this is sahana
from simplener today we will learn
interesting terms about big data
analytics which has become a buzzword
everywhere error before we start please
make sure to subscribe to our Channel
and press the Bell icon to never miss an
update
so let's go through the topics to be
covered in today's video
foreign
tics
next we will learn about importance of
quick data analytics next types of big
data analytics followed by life cycle of
big data analytics
tools and advantages of big data
analytics and finally we'll go through
use cases of big data analytics
first we must understand what do you
mean by Big Data Analytics
the data analytics is the technique to
analyze valuable data sets having any
format like structured unstructured or
semi-structured for example music
Industries like Spotify the company has
nearly 96 million users that generate
tremendous amount of data every day
using this information the application
generates suggested songs Through smart
recommendation techniques based on likes
shares search history and money mode
playlists are created automatically and
many more automated processes can happen
What enables this is the techniques
tools and Frameworks that are a result
of big data analytics if you are a
Spotify user then you must have come
across the top recommendation sections
which is based on your light past
history and other things utilizing a
recommendation engine that leverages
data filtering tools that collect data
and then filter it using algorithm this
is how Spotify works
by collecting and analyzing proper data
and incorporating it into business plans
which will further help them in decision
making this is how big data analytics
helps big business organizations
72 percent of e-commerce companies rely
on data produced from big data analytics
and this data is increasing day by day
any YouTube channel can use data
analytics to analyze user interest and
develop his next project based on that
what is the importance of the topic why
do we need to study big data analytics
is the biggest question it is observed
that every organization adopts this
technique for a better understanding of
data and its use
cloud based analytics can store a large
amount of data with minimal cost for
example tools like Zoho analytics
Microsoft power bi and many more thus
big data analytics can result as cost
efficient
the way of analyzing it is very sweet
which will help to analyze all the
sources this will contribute to Quick
decision making in any organization
the result of such analysis will help to
develop new products as due to these
products analysis of the companies will
know about their customer likings and
behavior due to all these efficient
strategies big data analytics is very
crucial
let's move forward and understand what
are the types in big data analytics
types of big data analytics
first let's try and understand
descriptive analysis it includes
processing past and present data set
which will lead to help in print
analysis
in predictive analysis it makes future
predictions based on past or historical
data sets which will help in decision
making
the diagnostic analysis is an advanced
analysis system in which introspections
are made based on why that has happened
and further decisions are taken based on
the available data sets
prescriptive analytics is the process of
using data to determine an optimal
course of action the next topic we are
covering is the life cycle of data
analytics
first is business case evaluation will
help decision makers to understand the
source of business in this case the
learner or the team will learn about the
business domain which presents the
motivation and goals for carrying out
the analysis in this point the problems
get identified and assumptions are made
based on the available data sets
in the identification of data once the
business case is identified now it's
time to find the appropriate date assets
to work with in this stage analysis is
done to see what other companies have
done for a similar data sets
in data filtration once the source of
data is identified now it's time to
gather data from such resources this
kind of data is mostly unstructured then
it is subjected to filtration such as
removal of corrupt data or irrelevant
data which is of no scope further now
the data is filtered but there might be
a possibility that some of the entries
of such data is incompatible to rectify
this issue a separate phase is created
called as extraction phase in this phase
the data which don't match the
underlining scope of the analysis are
extracted and transformed
in data aggregations data sets are
validated and combined with common field
for instance we can take data set of a
student one data set can be named as
student academic section and the other
can be named as student personal details
then both can be joined together via
common feed we can take it as roll
number
depending on the nature of big data
problem analysis is carried out data
analysis can be classified as
confirmatory analysis and exploratory
analysis in Conformity analysis the
cause of the phenomenon is analyzed
before the assumption is called
hypothesis the data is analyzed to
approve or disapprove the hypothesis
such kind of analysis will provide
definitive answer to some specific
questions and confirms whether an
assumption was true or not in
exploratory analysis the data is
explored to obtain information that why
a phenomenon has occurred this type of
analysis answer why a phenomenon
occurred this kind of analysis doesn't
provide definitive information meanwhile
it can provide discovery of certain
patterns
visualization or data visualization is
said to influence the interpretation of
the results moreover it will allow the
user to discover answer to certain
questions that are yet to be formulated
the analysis is done the results are
visualized now it's time for the
business user to make decisions using
the result the result can be used for
optimization to Define business
processes it can be used as an input for
the system to enhance performance
this is all about life cycle of data
analytics
if getting your learning started is half
the battle what if you could do that for
free visit scale up by simply learn
click on the link in the description to
know more
there are multiple tools available on
the market which can help companies in
big data analytics let us go through
some of these tools
mongodb Hadoop tableau Cassandra these
are the main tools which are used in big
data analytics
let's go for mongodb
mongodb is an open source tool that
support data storage it is a no SQL
document oriented database mongodb is
used by Facebook and Google for data
store mongodb is best suited for Big
Data where resulting data and further
manipulation for the desired output some
of the powerful resources are thread
operations aggregation framework text
research and the map reduce features
foreign
it is an open source software utility to
store and process gigabytes and
terabytes of data set it uses the map
reduce programming module to solve
problems of analyzing massive data
mapreduce is a framework that helps
programs do the parallel computation on
data the math task takes input data and
converts it into a data set that can be
computed in key value page the output of
the map task is consumed by reducing
tasks to aggregate output and provide
the desired result
Tableau is a software company that
offers collaborative data visualization
software for organizing working with
business information analytics
organizations use Tableau to visualize
data and reveal pattern for analysis in
business intelligence
Apache Cassandra is highly scalable High
performed distributed database designed
to handle large amount of data across
many Commodities like servers providing
High availability with no single point
of failure it is the type of nosql
database
now we have covered the important tools
used by big data analytics let us go and
cover distinct advantage of Big Data
technology
advantages of big data analytics
innovation of new ideas leading to
product development which means
developing new products provides a means
to Target new markets increase market
share sell more and increase revenue
streams meanwhile redesigning existing
products enables cost to be cut margins
to be decreased and ultimately more
profits to be made
risk management insights about customers
likings and behavior and market trends
will help decision maker to take their
position on top of that it will help to
get rid of Financial Risk it will also
assist to detect potential cyber risk
one benefit from Big Data and business
analytics can help improve decision
making by identifying patterns
identifying problems and providing data
to backup the solution is beneficial
whether the solution is solving the
problem improving the situation or it
has an insignificant effect
here customer engagement specifically
how your customers View and interact
with your brand is a key factor big data
analytics provides the business
intelligence you need to bring about
positive changes like improving existing
products or increasing Revenue per
customer
next
improving data quality will improve
operational efficiency and valued
feedback from customer which will help
businesses to handle vast amounts of
data it will also help in enabling data
driven decisions
but how do these advantages related to
the real world let us cover some real
life use cases empowered by Big Data
Analytics
use cases of big data analytics
Amazon is a well-known name to all of us
it is among the leading e-commerce
platforms apart from offering online
shopping Amazon serves us with different
services like Amazon pay Amazon web
services and many more for a company
like Amazon the amount of data collected
on a regular basis is very big to manage
such vast amount of data companies
leverage Big Data technology
thank you
for any company like Netflix one of the
most valuable asset is the customer base
because it is the customer who turns the
company into a brand and if a company
fails to meet the expectations of the
customer that probably leads to its
decline big data is a technology that
helps in the management of large amount
of data
big data is like a heart for American
Express decision making
their main goal is to detect fraudulent
transaction as soon as possible for
reducing loss and in this big data plays
a very important role they use big data
for anticipating and analyzing customers
Behavior by looking at recorded
transactions and incorporating more than
a hundred variables the company assigns
modern predictive models instead of
customary businesses
next is very important that is Google
Google uses big data to understand what
we want from it based on several
parameters such as search history
locations Trends and many more
after that it goes through an algorithm
where complex estimations are done and
afterwards Google easily shows the
arranged or positioned index list
as far as significance and Authority
intended to coordinate the users
this is all about use cases of big data
analytics
let's now start this lesson by defining
what data visualization is data
visualization is the technique to
present the data in a pictorial or
graphical format it enables stakeholders
and decision makers to analyze data
visually
the data in graphical format allows them
to identify new trends and patterns
easily well you might think why data
visualization is important let's explain
with an example
you are a sales manager in a leading
Global organization the organization
plans to study the sales details of each
product across all regions and countries
this is to identify the product which
has the highest sales in a particular
region and up to production
This research will enable the
organization to increase the
manufacturing of that product in the
particular region
the data involved for This research
might be huge and complex the research
on this large numeric data is difficult
and time consuming when it is performed
manually
when this numeric data is plotted on a
graph or converted to charts it's easy
to identify the patterns and predict the
result accurately
the main benefits of data visualization
are as follows it simplifies the complex
quantitative information
it helps analyze and explore Big Data
easily
it identifies the areas that need
attention or Improvement
it identifies the relationship between
data points and variables
it explores new patterns and reveals
hidden patterns in the data
there are three major considerations for
data visualization they are Clarity
accuracy and efficiency
first ensure that data set is complete
and relevant this enables the data
scientist to use the new pattern's yield
from the data in the relevant places
second ensure using appropriate
graphical representation to convey the
right message
third use efficient visualization
technique which highlights all the data
points
there are some basic factors that one
would need to be aware of before
visualizing the data visual effect
coordination system data types and scale
informative interpretation
visual effect includes the usage of
appropriate shapes colors and size to
represent the analyzed data
the coordinate system helps to organize
the data points within the provided
coordinates
the data types and scale choose the type
of data such as numeric or categorical
the informative interpretation helps
create visuals in an effective and
easily interpretable manner using labels
title Legends and pointers
so far you have learned what data
visualization is and how it helps
interpret results with large and complex
data with the help of the Python
programming language you can perform
this data visualization
you'll learn more about how to visualize
data using the Python programming
language in the subsequent screens
many new python data visualization
libraries are introduced recently such
as matplot Library visby Boca Seabourn
piegel folium and networks
the matplot library has emerged as the
main data visualization Library
let's now learn about this matplot
library in detail
matplot library is a python
two-dimensional plotting library for
data visualization and creating
interactive Graphics or plots
using Python's matplot Library the data
visualization of large and complex data
becomes easy
there are several advantages of using
matplot library to visualize data they
are as follows
it's a multi-platform data visualization
tool built on the numpy and scipy
framework therefore it's fast and
efficient
it possesses the ability to work well
with many operating systems and graphic
back-ends
it possesses high quality graphics and
plots to print and view for a range of
graphs such as histograms bar charts pie
charts Scatter Plots and heat Maps
with Jupiter notebook integration the
developers have been freed to spend
their time implementing features rather
than struggling with cross-platform
compatibility
it has large community support and
cross-platform support as it is an open
source tool it has full control over
graph or plot Styles such as line
properties fonts and axis properties
let's now try to understand a plot
a plot is a graphical representation of
data which shows relationship between
two variables or the distribution of
data look at the example shown on the
screen
this is a two-dimensional line plot of
the random numbers on the y-axis and the
range on the x-axis
the background of the plot is called
grid the text first plot denotes the
title of the plot in text line 1 denotes
the legend
you can create a plot using four simple
steps import the required libraries
Define or import the required data set
the plot parameters
display the created plot
let's consider the same example plot
used earlier
follow the steps below to obtain this
plot
the first step is to import the required
libraries here we have imported numpy
and Pi plot and style from matplot
library numpy is used to generate the
random numbers and the pi plot which is
built in Python library is used to plot
numbers in style class is used for
setting the grid Style
matplot Library inline is required to
display the plot within jupyter notebook
the second step is to define or import
the required data set
here we have defined the data set random
number using numpy random method
note that the range is 10. we have used
the print method to view the created
random numbers
the third step is to set the plot
parameters in this step we set the style
of the plot labels of the coordinates
title of the plot The Legend and the
line width
in this example we have used ggplot as
the plot Style
the plot method is used to plot the
graph against the random numbers
in the plot method the word g denotes
the plot line color as green label
denotes the legend label and it's named
as line one also the line width is set
to 2. note that we have labeled the
x-axis as range and the y-axis as labels
and set the title as first plot
the last step is to display the created
plot use the legend method to plot the
graph based on the set conditions and
the show method to display the created
plot
let's now learn how to create a
two-dimensional plot
consider the following example a Nutri
worldwide firm wants to know how many
people visit its website at a particular
time
this analysis helps it control and
monitor their website traffic
this example involves two variables
namely users and time therefore this is
a two-dimensional or 2D plot
take a look at the program that creates
a 2d plot
object web customers is a list on the
number of users and time hours indicates
the time
from this we understand that there are
123 customers on the website at 7am
645 customers on the website at 8 AM and
so on
the ggplant is used to set the grid
style and the plot method is used to
plot the website customers Against Time
don't forget to matplot library in line
to display or view the plot on the
Jupiter notebook
the website traffic curve is plotted and
the graph is shown on the screen
it's also possible to change the line
style of the plot to change the line
style of the plot use Define the line
style as dashed in the plot method
observe the output graph changes to a
dashed line also note that the color is
defined as blue using matplot Library
it's also possible to set the desired
axis to interpret the required result
use the axis method to set the axis in
this example shown on the screen the
x-axis is set to range from 6.5 to 17.5
and the y-axis is set to range from 50
to 2000. let's Now understand how to set
the transparency level of the line and
to annotate a plot
Alpha is an attribute which controls the
transparency of the line lower the alpha
value more transparent than line here
the alpha value is defined as 0.4
the annotate method is used to annotate
the graph
the Syntax for annotate method is shown
on the screen
the keyword Max is the attribute that
denotes The annotation text
h a indicates the horizontal alignment
VA indicates the vertical alignment
X Y text indicates the text position and
X Y indicates the arrow position
the keyword Arrow props indicates the
properties of the arrow in this example
the arrow property is defined as the
green color
the output graph is shown on the screen
so far you've learned how to set line
width title x-axis and y-axis label
title of the plot Legend line color and
annotate the graph for a single plot
the plot we created for website traffic
in the previous screens is for only one
day let's now learn how to create
multiple plots say for three days using
the same example the data set number of
user for Monday Tuesday and Wednesday is
defined with respect to its time
distribution
use different color and line width for
each day to distinguish the plot in this
example we have used red for Monday
green for Tuesday and blue for Wednesday
the output graph is shown on the screen
a subplot is used to display multiple
plots in the same window with a subplot
you can arrange plots in a regular grid
all you need to do is specify the number
of rows columns and plot
the Syntax for a subplot as shown on the
screen it divides the current window
into an M by n grid and creates an axis
for a subplot in the position specified
by P for example subplot 2 1 2 create
two subplots which are stacked
vertically on a grid
if you want to plot four graphs in one
window then the syntax used should be
subplot 2 1 4 layout and spacing
adjustment are two important factors to
be considered while creating subplots
use PLT subplots adjust method with the
parameters h space and w space to adjust
the distances between the subplot and
move them around on the grid
in this demo you can see how to create
two subplots that will display side by
side in a single frame
two subplots stacked one on top of the
other or vertically split in a single
frame and four subplots displayed in a
single frame
first import matplotlib Pi plot and
style
type percentage matplot lid in line to
view the plot in Jupiter notebook
Define the parameters such as
temperature wind humidity precipitation
data and the time data
you can't see the data being typed here
next to create two subplots to be
displayed side by side in a given frame
for one two one and one two two specify
the figure size subplot space title the
color for time and temperature data
which is blue here and line style and
width
similarly specify the color for wind
which is red its line style and width
you can't see the temperature and wind
subplot charts displayed side by side in
a given frame here
to create subplots for two one one and
two one two specify the parameters
this will create two subplots stacked
one on top of the other or vertically
split in a given frame
let's use humidity and precipitation
data to plot the graphs
specify the title color line style and
line width for both the graphs
you can see the two subplots stacked one
on top of the other with two different
colors indicating precipitation and
humidity here the two graphs are
separate
finally let's draw four subplots four
two two one two two two
three and two two four that will display
in a given frame
specify the title subplot data color
line style and line width for all four
subplots
you can see the four subplots displayed
in a single frame
in this demo you learned how to create
subplots displayed side by side a
vertically split subplots and four
subplots displayed in a single frame
using matplotlib
you can create different types of plots
using matplot Library
histogram scatter plot heat map High
chart error bar
histograms
histograms are graphical representations
of a probability distribution in fact a
histogram is a kind of bar chart using
matplot library and its barge heart
function you can create histogram charts
a histogram chart has several advantages
some of them are as follows
it displays the number of values within
a specified interval
it's suitable for large data sets as
they can be grouped within the intervals
Scatter Plots
a scatter plot is used to graphically
display the relationship between
variables
a basic plot can be created using the
plot method however if you need more
control of a plot it's recommended that
you use the scatter method provided by
matplot Library it has several
advantages
it shows the correlation between
variables it's suitable for large data
sets it's easy to find clusters it's
possible to represent each piece of data
as a point on the plot
in this demo you'll learn how to
generate a histogram and scanner plot
using matplotlib
let's import a data set called Boston
dataset which we will use to create the
histogram hand scanner plot from the
scikit learn Library
let's import matplotlib Pi plot
tight percentage matplotlib inline to
view the plot in jupyter Notebook
let's use the data in Boston real estate
data set to create the histogram and
scanner plot load this data
you can view this data by using the
print command
Now define the x-axis for the data which
is Boston real estate data
likewise Define the y-axis for the data
which is Boston real estate data with
the target extension
specify the plot style figure style
number of bins and labels of the x-axis
and y-axis use the show method to
display the histogram created by you
specify the style size data sets and
labels of the scatter plot that you want
to create use the show method to display
the scatter plot created by you
heat Maps
a heat map is a better way to visualize
two-dimensional data using heat maps you
can gain deeper and quicker insight into
Data than those afforded by other types
of plots
it has several advantages
it draws attention to the risky prone
area
it uses the entire data set to draw
bigger and more meaningful insights
it's used for cluster analysis and can
deal with large data sets
in this demonstration you'll learn how
to generate a heat map for a data set
using matplotlib
let's import the required libraries
matplotlib
Pi plot and Seaborn
type percentage matplotlib inline to
view the plot in jupyter Notebook
let's load the flights data set from the
built-in data sets of Seaborn Library
use head to view the top five records of
the data set
we have to arrange the columns to
generate the heat map let's use the
pivot method to arrange the columns
month year and passengers
let's view the flight data set that's
now ready to generate the heat map
let's use the heat map method and pass
light data as an argument
this will generate the heat map which
you can see here
in this demo you learned how to create
and display a heat map
pie charts
pie charts are typically used to show
percentage or proportional data
note that usually the percentage
represented by each category is provided
next to the corresponding slice of the
pie
matplot Library provides the pi method
to make pie charts
it has several advantages
it summarizes a large data set in visual
form
it displays the relative proportions of
multiple classes of data
the size of the circle is made
proportional to the total quantity
in this demonstration you'll learn how
to create a pie chart and display it
first import matplotlib Pi plot
type percentage matplotlib in line to
view the plot in jupyter Notebook
type the job data within parentheses
using single quotes separated by commas
specify the labels as it Finance
marketing admin HR and operations
specify the slice it to explode
use the show method to display the pie
chart
you can see the pie chart with the
slices labels and it the largest slice
error bars an error bar is used to show
the graphical representation of the
variability of data it's used mainly to
point out errors
it builds confidence about the data
analysis by unleashing the statistical
differences between the two groups of
data it has several advantages
it shows the variability in data and
indicates the errors
it depicts the Precision in the data
analysis
it demonstrates how well a function and
a model are used in the data analysis
it defines the underlying data
Seabourn is a python visualization
Library based on matplot Library it
provides a high level interface for
drawing attractive statistical Graphics
it was originally developed at Stanford
University and is widely used for
plotting and visualizing data there are
several advantages
it possesses built-in themes for better
visualizations it has tools built-in
statistical functions which reveal
hidden patterns in the data set it has
functions to visualize matrices of data
which become very important when
visualizing large data sets
now why power bi so generally you know
visualization tools reporting tools are
required in order to create and prepare
and analyze meaningful data it could be
a data for an organization it could be a
social media platform data it could be a
data from iot devices but something
which needs to be analyzed and some
intelligent inferences and data mining
has to be done on top of it now imagine
there is today we are in a world where
terabytes of data and information is
getting generated on an instantaneous
basis on minute-by-minute basis so it
becomes very essential to churn out
something meaningful something
intelligent out of it in the market
there are a lot of other tools which are
available like clicks all tricks Tableau
and power bi so power bi is a Microsoft
product which is one of the most popular
products and it comes as a free to
download product Microsoft power bi
desktop which is available and I'll show
you a couple of ways how you can install
it on your machine but why power bi is
popular is because it provides a lot of
out of the box features drag and drop
features which we will talk about in our
subsequent sessions and you know classes
but today's session is primarily focused
on giving you guys an introduction on
what what is the purpose of power bi and
what all problems it solve uh in the in
the real world so power bi allows you to
view analyze and visualize huge
quantities of data and the data could be
in any format Excel CSV text or it could
be a direct connection to a database
like SQL MySQL Azure Oracle anyone IBM
db2 so it supports n number of uh you
know data types or data sets and it's
very powerful in terms of data
connectivity
so it uses powerful compression
algorithms to import and cache the data
within the dot pbix file
so it's as convenient as a simple
software if suppose you import a data
and then you prepare a report and then
you can easily share the reports with
your peers or someone who's
co-developing with you either through
Power bi cloud services or even you can
share the pbix file in an email
or through any other means and you can
share the data set with the concern and
they can then work on the report
independently so there are different
ways there is no uh kind of a limitation
for you know working on power bi there
are multiple ways and it is very fast it
is the most fast uh tool to work with
Excel because definitely Excel is also a
Microsoft Technology so it works very
fast on Excel based data and gives you
numbers and Reporting at a very high
speed
so now once you have imported the data
power bi allows you to model the data
allows you to work intelligently on the
data it allows you to model the data in
a way that if you are importing data
from multiple Excel sheets importing
data from multiple tables you can easily
create a relationship between those
tables or data sets in power bi and then
create visually appealing reports
meaningful reports as I've been
emphasizing and make sense out of that
data no data in silos is of any use data
in Silo means a single worksheet or a
single data set will not churn out any
meaningful information until unless you
basically join it clubbit merge it Union
it append it with some other data sets
because a single data set will never be
able to hold that much amount of
information which is generally required
for a you know important report
so it has easy drag and drop
functionality with features that allow
you to copy all formatting across
similar visualizations so just like in
Microsoft Excel we use format painter to
copy the format of one cell to another
similar features very very similar to
excel products or Microsoft products
they have provided that if you have
applied uh a particular thing on a
report you can easily replicate that on
any other report the font uh the header
size the background color you don't need
to do it again and again so there's a
lot of reusable features which are also
available
okay now
as I said Excel is a Microsoft product
power bi is a Microsoft product so they
have intercompatibility you can publish
data from Excel to power bi now with the
latest developments and enhancements
as of today pixel has also plugged in a
new feature called Power pivot which
I'll uh show you later down the line but
that also allows you to do a quick
analysis no you can't create of course
complex reports or uh fancy reports like
power bi but Power pivot allows you to
create you know quick measures quick
functions quick calculations on your
data quickly only in Excel so it's an
Excel plugin but whereas you know you
can power bi is also compatible with
Excel so when you create a report in
power bi it gives you inbuilt feature to
export your power bi report into Excel
format directly you don't need to do any
programming for it also
you can easily publish when you publish
your power bi reports it allows you to
give some inbuilt intelligence of
analyzing your reports in Excel and it
gives you all those of all those
features of exporting your power bi
reports into Excel which is not
available in any other tool or otherwise
those tools have to create plugins
create add-ins and probably they might
charge for it but Power bi comes with
lot of out of the box features which are
very very helpful for analyzing data in
Excel and vice versa
Azure Cloud now Azure itself again is a
Microsoft cloud Tech stack so using
power bi with Azure allows you to
analyze and share large volumes of data
so Azure basically Azure database or
Azure Cloud servers are meant to hold
huge amount of data and power bi allows
you to have seamless connection you can
easily connect to Azure data Lake
you can reduce the time it takes to get
insights and increase collaboration
between business analysts data engineers
and data scientists so azure
data Lake becomes the central focal
point where all your analysts Engineers
can keep working on on the centralized
piece of data and churn out their
reports data scientists primarily job is
to keep the data in a structured way
optimized way optimize the input output
operations disk operations and memory
utilization so that the reports also get
churned out in a faster manner so every
uh you know every person has their own
role in order to give a quick
refreshable report a quick rendered
report any report which is taking huge
amount of time to get rendered will not
eventually be used by the business users
because then it does not solve the
purpose the report should be fast
reports should have uh you know
appropriate filters slices and dices you
should be able to you know create the
reports or dynamically or you should be
able to analyze visualize the data
dynamically so all those visualization
features are available in power bi and
it works seamlessly either it is small
data or huge data it allows that you to
work on those kind of data in a seamless
fashion
right so this is just a very uh quick uh
example of how our typical dashboard
looks like dashboard is nothing but you
know you have clubbed couple of multiple
reports on a single page and you know if
you change a filter uh a single filter
on the page all the reports will honor
that filter and the numbers will change
accordingly so if you see the in this
example there is a filter or a drop down
or product ID product name employee name
or supervisor or a date range so
whatever date range or filter you will
apply all the reports on this dashboard
will get changed based on the filter you
have selected so power bi allows you to
get insights from data and turn insights
into action to take data driven business
decision and that is the ultimate goal
of any visualization tool that is the
purpose for which visualization tools
are bought
uh and purchased by the organizations
and data is fed into them
Now power bi fetches data from Factory
sensors social media sources to get
access to real-time Analytics
so that you are always ready to make
timely business decisions so so
basically there is a a feature of live
connection or cut off data connection so
either you can work on data which is
deciding on a machine and you can just
work on the cutoff data like for example
it's there's a data which is available
for sales 2017 2018 and you're just
working on a historical data it's a cut
off data or it could be possible that
you want to be connected live uh to a
real-time iot based sensor based data or
social media data like Twitter Facebook
feeds or you know you're connected to
live Google worksheets that is also
possible you just need to publish your
Google sheet for a public domain embed
that you are into Power bi and then
whoever updates that Google sheet
automatically the power bi report will
also start honoring and consuming the
new data which is added in the Google
sheet so all those kind of real-time
streaming analytics is also possible and
that is one big feature and very
important feature of power bi which is
widely used and has a very huge uh you
know Market acceptance and Market
utilization
now what is power bi
power bi is a business analytics service
provided by Microsoft that lets you
visualize your data and share insights
right so earlier
you know uh
Microsoft used to have a technology
called ssas now they have replaced
actually ssas and SSRS with power bi so
basically you can use power bi on the
data which is there in your Excel or any
other data source and the power bi
service or power bi desktop basically
creates a connection to those data sets
and import it cache it and give you a
handle to it in order to work with it so
you can create these fancy meaningful
visualizations like for example there's
a geographical map if you are importing
data for a country or a continent or a
region power bi will automatically
detect that it's a geographical
information and give you a map with
latitude longitude information and you
just need to plot your uh your numbers
on the map either you can use bubbles or
either you can use triangles or whatever
data structure you want to use but all
the mapping will be available
geographically then you can create pie
charts which is shown in this
visualization you can create tree maps
you can create cards where you know you
can highlight the most important numbers
like sales total sales of your company
across all the regions or the growth
chart or the month on month uh you know
sales of your organization or number of
total number of products or units sold
so whatever is important and to be
highlighted for the management to take
any meaningful decision or any insights
you want to share
power bi visualization tool the power bi
visualization uh
uh chart allows you to drag and drop and
create wonderful reports okay
so what are the features of power bi so
power bi desktop is something a
standalone tool which you need to
install on your machine it allows you to
build reports by accessing data easily
you do not need Advanced report
designing or query skills to build a
report though yes it is beneficial that
if you know some SQL programming
analytical programming or you are aware
of advanced features of any analytical
tool that might help you but that's not
a showstopper you can easily build
reports in quick turnaround time without
needing any technical background you
just need to have some analytical uh
mindset and you can create uh Savvy
visualization and you know analytical
reports
stream analytics as I mentioned you can
create a live connection uh with any
kind of data it could be iot it could be
media social media it could be Google
Docs it could be uh you know any other
kind of uh you know live connection it
could be a live database connection
itself so any insertions or updations or
deletions happening will automatically
reflect in your report
yes multiple data sources and that has
to be the primary criteria for any tool
to be popular if any visualization tool
is limited to certain data sets then you
know it will not be highly acceptable in
the market
and custom visualizations right so as I
showed you certain uh examples in the
past in the previous presentation uh
that feature is very important because
someone might want to look the kpis look
at the kpis from a different perspective
some management might want to look at
the kpis from a different perspective so
you need to you need to have that
capability to create different
visualization from the same data set now
let's take a look at how
to install power bi desktop on your
machine
so basically what you need to do is
you need to go to this URL
power bi dot microsoft.com en US desktop
okay
and you need to just
enter this
now you can download it for free so just
click over here and
it will open Microsoft store so
basically now what Microsoft have done
in the latest operating systems is that
when you are trying to download you can
actually directly go to the Microsoft
store
and search for Power bi
so let's wait for a couple of seconds
right here so power bi desktop in
Microsoft store for me it's already
installed so it's asking me to open it
I'll open it in a while but for you for
anyone who is not installed he will see
the button of install over here and it
will automatically install in your
machine and then you can easily go and
open power bi desktop now if I click
open over here
now this is the
uh UI of the power bi desktop
I'm not going to go right now in
creating reports right away we will talk
about that in a subsequent session with
sample data sets and we will cover the
features of power bi desktop one by one
but this is what it is this is the whole
tool of power bi which is having the
visualization pane all the different
visualizations are you know can be
created from this pane
then this is the pane which allows you
to select the data data fields then
there is a report view data View and
relationship view the data model view
where multiple relationships you can
create you can view the data in the grid
of the tables which you will create and
the reports so you can create multiple
reports on multiple Pages you can keep
adding pages either you can drag create
multiple reports on a single page and it
will become a dashboard or you can
create separate independent reports on
the single page
and these are the menu options which we
will talk about how you can change color
scheming you can do data modeling you
can create new reports and you can also
transform data which is the biggest
feature extract transform and load the
data apply different logic changing data
types massaging the information creating
new joins appending the data you know
adding new columns etc etc uh we that is
what you can do in transform data so
this itself is a whole different world
it's a dedicated topic so we will talk
about that in our subsequent sessions so
what's in it for us today we will be
learning how to connect to data
different data types data files like
Excel PDF then what are the different
data importing modes and then I will
also show you practically different sets
how to import them in power bi and use
it for your visualization purpose now
what are the steps to connect to data so
now we will go directly into Power bi
and try to import one by one few most
commonly and popularly used data sets
which are most commonly used in a
day-to-day activity rest of course there
are power bi supports n number of data
sources but we will do something
practical on the most popular ones so
let's let's open our power bi
now this is my power bi and first I want
to show you that how can I import data
directly from a web page and import the
data now it is asking for a URL in order
to import data so what I have done is I
have created a Google Excel sheet with
simple data with rows and columns and
what I have done is I have shared this
sheet as published to web
okay so you just need to say publish to
the web the link as web page and
say done it's it's automatically
published and say link so copy the link
which you have published on the web copy
this link
and then go back to your tableau
paste that link over here and click ok
Now power bi will try to establish a
connection with this Google doc sheet
because it's published on the web
you need to wait for a while while it is
reading
okay now it has read one of the HTML
tables I'll select this one now you can
see it has it is showing me a preview of
the table which is there on my
Google sheet right it has 11 rows so it
has all showed all the 11 rows so now I
can go and transform this data because I
can see my headers are there starting
from the second row so there is an
opportunity for me to transform the data
so I'll go and transform it so that it
looks clean
okay so first is I need to remove the
first row which is the
null row remove the top rows
okay and then I need to use the first
row now as a header so you just click
this option use first row as headers
set so now if you see my row ID order ID
order date ship date all my data is now
ready so I can say close and apply
click apply changes
now this is an example of web data
import you can go and preview your data
right now the biggest advantage of this
data connection is that it's a live data
so for example I insert another row
let me change the order ID
some some are some strange some basic
stuff and I
it's Auto saved
Ctrl s now I'll go to my w
and I'll refresh
now you can see as I refreshed my
power query editor I click refresh all
and I got my new row which is there in
the live data I got that fetched from my
okay I got that row the row number ID
number 12 so I have to say close and
apply
now you can see the new row the row
number 12 is now available in my new
data set in the data set because it's a
live connection it's a live connection
with the web based Google sheet okay so
this is one
important way in which you can import
data
now let's try to import data from a text
file
now I have already prepared our text
file called
sub categories Dot txt
now let me just open it in a notepad now
it's a very plain simple file tab
separated file in which you have product
subcategory ID subcategory name and
product category key so basically to
which product category this particular
sub product belongs to right so what I'm
going to do is I'm going to go back to
my get data option
and I'm going to select text slash CSV
option
and I'm gonna select option mod product
subcategories Dot txt
okay so now power bi has identified that
it's a tab delimited file it is
recognized the headers
Etc right and I can now directly load
this file
okay
so now once the data is imported in
power bi it is like
irrelevant to me it's a composite data
in input I am doing right so in my
presentation when I am talking about
importing data there are different
importing modes right import our data
import can happen through different ways
okay
one is direct query mode in which I
create a live connection to the database
which I'll also show you uh using MySQL
and Ms SQL server and also you can do a
composite mode in which you can have
data imported from Excel plus you can
have direct query modes so you can have
multiple uh modes to connect and create
a composite data model and that's what
we are doing right now in our practical
so what we are doing over here is one we
have imported data from the web second
we have to imported data from a text
table
now after doing text now our next task
is to import from CSV let's try another
one so now I have imported product
subcategory now I'll import
a CSV file so again I'll choose the
option Text slash CSV and now in this
CSV file let me open this CSV file and
show you what it is it
so this is a list of all my products
product key products sub category Key
Products stock keeping unit Etc a simple
CSV file and I'm gonna import that
okay so now it is identified the
delimiter is comma rather than a tab and
it has already recognized the headers
correctly so I load it
okay so now my products are there
product subcategories are there for
product categories now what I have done
is I have created a Excel mode now so
now Excel I am using to import my
product category
so now I have to click on the option of
import data from Excel and I'll say
product categories
select the sheet load
and now so my products product
categories product subcategories do with
different uh data storage types but
still now the data is imported into
Power bi it is a composite data model
now another very important data type
which you can import is the PDF also
right so what I have done is I have
created a PDF called customers my
customers data is lying in a PDF so what
I've done is I've created a PDF
which has data for some columns are
there like you know customer key prefix
first name last name birth date marital
status gender email address annual
income total children etc etc so this is
the data set
which I have created in PDF so what I'm
going to do is
I'm going to select PDF now
and import customers.pdf
and see it has recognized my table on
page one which I am going to load
okay
you can rename this as PDF
table
so this basically these are the
different type of
data
types we have imported PDF Excel text
CSV and web page
now let's take a look at another
interesting data set which you want to
import is the my SQL Server data set
so what I have done is I've already
installed MySQL server on my local
instance and there's already a schema of
SQL live tutorial over there and I have
certain tables already
prepared over there like Department
employee Etc so my goal is now to import
this data or create a live connection
with this data set
now in order to import
my SQL database Connection in power bi
you need to First
download a
connector MySQL power bi connector so
you need to go to this link
and then click on download and install
the MySQL connector based on the
operating system you have you click on
download and install it
after you have done this go back to
Power bi
and then give
the IP address of the database in my
case it's there in this local machine
and the schema which I want to import is
SQL live tutorial so I'll give the name
click connect
okay now it's connected so now it is
asking me which particular tables you
want to create a connection with I am
choosing department and employee
and I'm just loading them
okay so now this is the exact data which
is there in the employee and Department
in MySQL okay so this is one example of
how to create connectivity between power
bi and MySQL
now I want to do the same thing using
SQL Server Microsoft SQL Server so I
have also installed Microsoft SQL server
on my machine
and I have used the SQL Express so this
is the name of my server so which I'll
copy the server name and
go to get data
select SQL Server
and for now database is optional I can
say direct query click ok
now it is showing me what all tables I
can import so in my SQL Server tutorial
in my SQL Server I have
I have these three tables customers
employee attrition Olympic events so I
can use probably the customers one which
is
now you can see this is the data the
customer's data which is lying in my SQL
Server okay so I can preview it and load
it
so now you can you can preview the data
in uh Power bi that is this is the data
so I can rename his customers from
mssql
and this is from
MySQL
and
okay so now
this is not the only data
sets you can import now if you take a
look at the options which power bi gave
what different type and variations of
data it can it has compatibility to
import from
okay so we can just take a look at the
categorization on the left hand side
first there are file based like Excel
text XML Json is also possible you can
evenly directly import an entire folder
and within the folder whatever uh data
types of files are there it will detect
it PDF power key or even SharePoint
folder which is itself Microsoft uh
technology then different kind of
databases SQL server and MySQL we just
saw but it's not only limited to this
you can connect to Microsoft Access
ssas Oracle database IBM db2 postgres
cybase teradata and then sap
databases Amazon redshift Impala vertica
Snowflake and N number of databases
which are there in the market today
Amazon
Etc
then it also allows you to connect with
its own power platforms power bi
platforms data Marts power bi data flows
dataverse Etc Azure there are different
kind of storage
mechanisms in Azure and Azure itself is
a Microsoft Technology so it has a
compatibility of a lot of azure uh based
data storages like Azure SQL database
blob storage uh Azure data breaks right
Azure HD insights path so if you have
those kind of services running on your
observed cloud services you can even
import them over here
now online services like you know you
have erps running uh or some data which
is shared on the internet if you want to
import it uh that is also possible
through certain products uh Dynamics 365
Microsoft Exchange online Salesforce
Google analytics Adobe analytics GitHub
LinkedIn sales if you want to do some
analysis of some social networking uh
you know feeds that also you can import
and then other miscellaneous are also
their web-based hive R script python
script if there's something to import
get data from uh Google Sheets like we
saw one example in our video right now
so there are multiple options available
now once you have imported the data
which is relevant to you
um in our subsequent sessions we will
see how to create relationships but just
giving you a glimpse that whatever data
you are importing power bi Auto detect
certain relationships and it will create
for you but then you can go and manually
also change so this is the composite
data model which is getting created in
the back end while you are importing the
data you can easily go and manage these
relationships either keep them as is you
can delete and create new ones manually
so there is no limitation in that
so this is what we have witnessed we
have imported data from different files
types data types and then you know we
have tried once it is imported into uh
Power bi then there is no limitation of
how you use it you can create
visualization across different data sets
and then create your
standard reports
so this is the example of importing data
from web importing data from a database
from a PDF
and then once you have data you can
shape and combine data you can basically
do what whatever transformation you want
to do you want to uh make joins merge
the data so for example if we go back to
our power bi and if I go back to my
transform data section
now as I have now different data sets
available with me I have I can do any
kind of you know operation
transformation on the data right uh
so like I showed you I uh upgraded the
header row because one of the imported
data was not showing the header
correctly uh or disk columns like this
exact one column is extra I can remove
the column
right all those Transformations whatever
I do in the back end gets captured in
the applied steps section right
this is the customer data you can create
uh you can merge it you can append it uh
you know with other data sets right
let's for example I want to create a
merge data set of my categories and sub
categories so I can say more selectors
two data sets and say merge queries as
new
and
I can select product categories and
products are categories select
product category key on both the sides
and then they do a left Auto Zone so
whatever product categories are there
I'll get the subcategories associated
with it and I'll create a new table
which will have
now I have the table which has the
category and the subcategory and
subcategory in one table itself so I can
rename it now to as
category
sub category
table it's a it's a merge basically it's
a join between category and subcategory
now I have a common table right and I
can close and apply
so imagine I have created a new table
which is imported created from one data
set is which is Excel page another data
set which is text based
see this category subcategory table so
now I can use it
the way I want in my visualization
reports so that's what the presentation
says right that once you have uh the
imported data you can shape you can
combine you can adjust you can do
whatever transformation you want to do
and create your visualization okay so
what topics we are going to cover today
we are going to talk about different
types of data modeling and the most
important part
and aspect of data modeling is the
cardinality the cardinality which you
basically decide after reviewing the
nature of data and after you've imported
it what kind of cardinality you have to
basically highlight right and there are
different type of cardinalities which
you might have heard earlier also if you
are from PL SQL background like one is
to one one is too many Etc we will we
will talk about that
now what are the different types of data
modeling now dimensional data modeling
is one of the most popular and most uh
you know widely used uh modeling in
dimensional data modeling you have
Master data uh like for example customer
data date store data product data so
these are like you know uh less
frequently changing data sets so there
is an organization right and you have
set of customers their email ID phone
numbers
Etc that will change less frequently as
compared to the sales transactions
because transactions are happening every
day every minute so sales is a more fast
changing data set in dimensional
modeling which is in the terminology of
data is also called as a fact and
customer store product which are like
more of static data and less changeable
data is sometimes called a diamond
ancient so this is a typical dimensional
data model which is typically used uh
sometimes right and then there is
another model which is relational model
this is a typical model which we have
been using in database design like you
know primary key foreign key
relationships so for example you have a
customer who has purchased a product so
probably he might have the customer
might have the details of the product
which is processed and you will make a
join between customer and product table
and even you can make a join between
product or product type or customer or
product types customer table will also
have a key to the product type so this
is less conducive for reporting but it
is more of a transactional relational
model but of course this is also
feasible but from the power bi
perspective when we talk about reporting
and visualization this is the most
extensively used dimensional data model
and this is what we are going to see in
our example now so what I'm going to do
is I'm gonna show you certain data sets
first we will prepare and create certain
offer data sets and then we will Import
in our sample power bi file and then
slowly slowly we will create the
relationships
now one important thing which you need
to understand that in power bi if you go
to Power bi there is an option that that
power bi Auto detect new relationships
after data is loaded and import
relations from the data source on first
load so for example if you are importing
the data from a database where you have
already defined the primary keys and the
foreign key relationships so uh that is
the first option which power bi will
Auto detect and secondly if suppose you
are importing two different kind of data
sets one is Excel one is csb and if
power bi detects a common column key
columns it will auto detect a
relationship which you can go and later
change modify manage in your
relationship
menu manage relationship menu in power
bi which I'm going to show you okay so
if I open a power bi and this is where
the option lies go to file
go to options and setting options
data load
and these are the two options which are
by default check you can uncheck it and
auto and manually prepare relationships
there's no limitation to that but if you
uh keep it checked then power bi will do
its job to detect the relationships okay
now coming to the next important factor
cardinality
now before I start playing around with
my data and start showing you certain
relationships it's very important to
understand these four types of
cardinalities one is many to one right
so basically
many to one means that many orders
contain data of One customer so per
order one customer is there so from
customer to order or product or delivery
address it's a one-to-many relationship
and from the other side from order to
customer perspective it's a many-to-one
relationship
okay
second other cardinality is one is to
one one is to one relationship is only
applicable when you are saying it's an
extension of the current table so for
example in one table you have employee
details and you are extending the
details of the employee in another table
like employee address employee ID so
that is like one is to one there is no
multiple records of a single employee in
the address table only one employee ID
exists right
now one is too many as I said is the
reverse side of many is to one okay so
in customer table only one customer
record exists per customer and one
customer can place many orders for
multiple products and can also have
multiple delivery addresses so that way
this is a typical one is too many
relationship we will be seeing this
example also in our sample data set
and last is the many-to-many
relationship now many to many is a very
typical example so which I'm going to
show you practice practically and in our
case we will see that like for example
you have placed an order for a
particular product uh you know but there
are multiple fulfillments which has
happened so suppose you made order for
10 products but at the back end when the
company is fulfilling it is first
fulfilling the first two products then
the rest three so basically you the
Fulfillment is happening in batches so
one order
ID might have a multiple fulfillments
for the same order ID so there will be a
multi many to many relationship which
I'll show you practically so now with
this background let's
start importing our data now the first
important thing which we need to import
is the
Master data so first I'll import all my
master Dimensions which are uh which I'm
going to you know use in my example so
first is the customers table customers
data
so this is the customer details the
customer key prefix first name last name
birth date marital status and gender
some redundant columns are also present
but we'll remove it
so my customer data is loaded now
today's session is all about this
section of modeling so we will keep our
Focus over here
okay now some columns probably some
blank columns are there I can select
them and say delete from model
yes
okay so now this is my customers data
with the relevant columns and the key
per customer customer key
now there's no relationship in this
model right now right because only
single table is there and the associate
data is only imported now let me also
import my another important master table
is the products
select the products data product key
product sub category product SKU product
name model name product description
color size so just see all the relevant
information only specific to the product
is available so I imported
okay now see there's no relationship
between product and customers directly
because until unless a customer makes an
order places an order for a particular
product there is no join right so now
between these two tables the most
important now another table which will
now make sense is the sales order table
sales table
now I I'm assuming that power bi have
Auto detected the relationship now you
can see that because I've already uh
ticked that check box now let's see what
power bi what relations power bi has
Auto detected let's first say check the
relation between customer and sales I'll
double click this
join now what it has done is it has
created a join of many to one
between sales and customer so what does
that mean is that one customer has can
place many orders right and that is that
it is detected by the quality of the
data and the data sampling which power
bi has done you can also reverse this
relationship here I can select customers
and I can select sales now it has become
one too many so that you can also do
manually so that is what I said whatever
power bi is detected it is up to the
description of power bi internal uh
configuration and algorithm but you can
go and change it so this is now you can
this is by default active so we want to
keep it active One customer many sales
orders cross filter Direction means that
only from customers to sales is the
filter applicable not reverse I'll come
to this with my another example but
first let's
change the relationship so one is too
many means from One customer and many
sales orders
similarly let's see what has happened at
the product side
of the relationship
similarly power bi many sales orders for
one product you can for simplicity's
sake you can say products
sales
product key is the join now just focus
one more thing please also see the the
column on which the join is is the grade
column grade out column product key is
also here and product key is also there
and it is what we wanted so one is too
many relationship from product to sales
table and act
now looks fine this is something which
is looking logical and probably now we
can
proceed further to create a report
now let me explain the cross filtering
with an example
now for example I want to check in a
report
that what is the count of products which
uh which a particular customer has
ordered
okay so what I'll do is I'll select the
product count of product name
now if you see
and for each customer in front of each
customer name the count is coming as 293
293 it is getting repetitive because
because there is a one-way filter
Direction filter between customers and
sales and sales and products right so
this join
is single sided it means that from
customer to product you can't find a
relationship because it's a single side
cross filter right what does this if I
change it to both it means that it is
equal to a join between product and
sales and every product detail now is
appended to the sales table so if I want
to make you visualize this you need to
go here
I'll first open my sales table
we can also open it here let's make
click it says okay now if I click OK you
can see the single arrow is changed to
Double Arrow it means it's a it's a both
site filter
So when you say a both side filter it
means that implicitly within power bi
you can imagine that all the product
columns now will get appended because of
both ways filter you have applied and if
you go to your report now see the change
of the numbers now for T20 so the total
count of products across all my
customers come out to be 293. now the
report looks uh correct if I change the
relationship from back to single between
product and sales then you can't make a
join between customers and product
basically you can't derive the product
count from the product table see this
if you have to live with it then you
would have to go to the sales table get
the product key and get the value of
count of product key but that is not
correct okay
so if you want a report in which you
want the count of product name
and even if you want a count of distinct
product name so this will not come
correctly you would have to go and
change the
direction of the filter which is from
single to both so this is a typical
example they where you want to use a
two directional filter
now let's proceed further and import
other data set in order to give show you
another example
now I want to show you an example of
one is to
1.
so I have another table which is called
customer details so the key in this
table is again customer key but only
email address annual income total
children education level Etc other
details of the customer is there
so I'm loading the customer details now
you see it has Auto detected a one is to
one relationship
what is the meaning of one is to one
means One customer key only has one
entry in customer details there is no
multiple entry so if you click this
button
it's a one is to one and the cross
filter can be both or single doesn't
matter because one customer will have
only one value you can make this as
active okay and if you go to the
customer report table you can now easily
associate a
email address with the first name you
will get one is to one
record
so now you can see that with one is to
one relationship
with the first name I have Associated
the email ID and for each email ID
there is a Associated first name method
so this is an example of
one is to one relationship so in this
example what we have explained is that
for each customer there is an Associated
customer detail right uh so you have the
first name email address education level
homeowner occupation and total children
count
so in this report what we have done is
uh if you click over here so the first
name and the email address okay so
there's a one is to one relationship
and then
and if you drag
the customer key
report takes time to render and even if
you can
so this is the reporting output you have
the customer key first name associated
email address and the count of product
names uh which the customer has ordered
now this is an example of one is to 1.
now I want to show you an example of
many too many now for that I'll import
my
fulfillment data set
huh
okay now in my fulfillment data set
there is a column for
order number so basically what I'll do
is I'll drag order number from here to
here
okay so now what has
um
uh Power bi detected
I will do one thing I'll select sales
over here
fulfillment over here and order number
to odd number
okay so it's a many-to-many relationship
so it means that per order I have
created multiple batches to fulfill that
particular order now
many-to-many relationship is a
definitely a candidate for both ways
cross filter detection uh a Direction
but you can you can check that but
definitely uh Power bi shows a warning
that this relationship has cardinality
to many to many and this should only be
used if it is expected that neither
column contains unique values okay so we
know that fact that's why we are
accepting this relationship as many to
many because we know there are multiple
order numbers over here in the sales
table which are mapped to the multiple
order numbers in the Fulfillment table
we'll click ok
now you want to keep the uh
uh direction as both ways or One
Direction that is up to you the way you
want to map the report so I can double
click over here
and you can even click so now you can
select from which way single filter you
want from fulfillment to sales or sales
to fulfillment I'll prefer sales to
fulfillment and click ok
okay now we have our all our different
kind of relationships over here uh which
we have tried to shortlist one to many
many to one one to one which is uh this
example and many too many
now if I show you further relationships
which you can keep on adding like for
example I have uh the example of
territories
now in which particular territory the
sales was done
a bit over here
okay so now it's a typical one is too
many relationship because territory is
my master table uh where I have a static
list of continent country region and it
is mapped to the uh territories which
are for in which my orders have been
placed so it's a typical one is too many
so that way you know you can keep on
adding data then you have
uh details of returns
now this is another transactional table
which is about the orders which have
been returned rather than being you know
returned by the customers so you have a
product key
and so automatically power bi has
detected a relationship between the
product key and the product uh table
right and even if you can join the
territory key in which territory the
return has happened
right so mostly the most common
relationship which you will observe is
the one is too many because
as I told earlier the most common
relational model is the dimensional
model uh the static data the slow
changing Dimensions the scds are the
master tables and the most frequent
changing are the fact tables so if I
talk about a typical dimensional model
the Fulfillment table sales table and
the territory stable sorry the
Fulfillment table sales table and my
returns table are the fact tables of my
data model now so far what we have done
as per our last session is that we did
data modeling on the different data sets
which we had imported in power bi like
products sales data returns fulfillment
customer details uh and customer Master
data calendar details Etc so in the last
session we prepared a data model and
established the relationships between
these different data sets like one is to
one one is too many many to one one is
to one Etc and we saw the examples now
once our relational model is prepared
our data model is prepared now our next
activity is to create certain additional
columns which we want to derive basis
the data which we have imported
so for example I'll start with my
product data set now in my product data
set I want to introduce a column which
basically
categorizes that if any product which
has a color uh you know red black or
gray I am going to tag it as a colored
product rest I'm gonna say not a colored
product right so all these are like
example of byte type product skus so for
that now in order to introduce a new
column you just need to do what you need
to select the table in the data grid go
to the table tools and say new column
okay so column will get appended to the
rightmost part and you will start seeing
a uh formula section typical to like you
get in your Excel now I'm gonna say that
the name of my column is going to be
byte type color okay and I'm just
creating a if condition if
product
color
is equal to
black
okay
or
or if it is equal to red
or if it is equal to
Gray
then
say yes it's a colored product I'll say
no
okay so now you can see this is the
product color red and black they are
saying by type color Yes blue is no
multi is no etc etc so this is a classic
example of an if and else condition
based conditional column
okay so you can create such columns now
second column custom column which I want
to create is I'm going to call as
discount
now this is the pricing of my products I
want to associate certain discounts
which I am ready to give to my customers
this is the product category like what
is the pricing of the category again I'm
going to use make use of if else but in
a nested way so if I am saying if my
product
price is less than 100
then I will give
zero percent of
uh zero percentage of discount so 0 into
product price just to keep it consistent
now I'm seeing
else
if less than 100 and 0 else I'll check
again that
if
the price is
less than
500 then I'm ready ready to give one
percent discount
on the product price
else
I'll move further so like this I have
created a formula
so what I'm saying is if product price
is less than 100 give zero percent if it
is less than 500 then give one percent
less than 2 000 then 1.5 percent less
than three thousand then two percent and
otherwise else less than three thousand
if a two percent else three percent
right now after this column is created
now you can check right so this C the
product price for this particular
product it is less than 100 so that's
why there is no discount it is uh
between 100 to 200 then this has been
given a one percent discount so like
this all the discount column is now
calculated now this column is available
just like a regular column in my product
table
now after this I'll go to my sales table
now in sales table I want to identify uh
create a column called as cost
there is no product cost column over
here so that will be derived
so let's create a column
called as cost
and it is derived by order quantity
into now the cost of the product
is in the product table and I know I
have already created a relationship
between product and sales table
so I just need to select the product
cost column now only keyword which I
have to use in power bi is the related
keyword so this will pick up the
relation
and now for this particular sale order
the cost has already been derived so
this order number this is the cost for
which uh the product has is the costing
of the product for this particular order
foreign
now I'm gonna create another conditional
column over here called as
order status
I'm saying if
any order whose order quantity is
greater than 2
then for my organization it's an urgent
order
else it is a
normal order
oh sorry
positive
so this is my
order status column and I have my order
quantity
urgent or normal
so any order which has order quantity
one is normal any order which is having
order quantity as greater than
2 is
urgent you can see this
so there's a this whole power bi
uh tabs and sheets allow you to also
review the data what you're doing so
it's very convenient
now
I have my sales data now what I want to
bring within the sales is my discount
column so here also I want to bring the
discount which I have created
so I'll say discount
[Music]
will be order quantity
into
related
product discount right so the discount
calculated column which I had created
under products I'll bring over here
now I am creating the order level
discount so if you see for this
particular order there's a 25 uh
uh you know for 25 rupee discount at the
cost is 100 000 rupees okay
and what is the order price now so I
have taken the order cost the discount
now I have to create a column call as
price order price so that will be
again order quantity
into related
price which is per product
price and
okay so now I have the cost the discount
and the price right available with me
now I want to
calculate the total
uh
total revenue total profit and loss
right
per order how much
so first I'll calculate per order how
much revenue I am generating so now I
have to generate a column called as
Revenue
revenue is price
minus discount
so 1700 minus 25
1700 minus 25 2071 minus 42 and if I
want to calculate the profit per order
then it is
Revenue
Minus cost
okay so now you can see you know typical
custom columns calculated columns which
we have created are all playing around
with the number numeric values numeric
data primarily and trying to give
inferences into per order cost discount
per order price revenue and profit so
typical calculation columns which I have
prepared in front of you
now let's take a look at other different
variations of custom columns
I'll create certain columns for text
based custom columns calculated columns
using text pix data so I'm now moving
towards my customer table in which I
have customer key prefix first name last
name birth date marital status and
gender
now I want to create a new column in
which I want to derive the age of each
customer as of today right so I'll use
another function a date function called
as
date div
now date div so I want the difference
between the birth date of the customer
and as of today
in years
okay so this customer as of today 68
year old one is 74 68 57 Etc so this is
one derivation of a calculated column of
age
let's take another example now this is a
text based column where I want to
derive the full name of the customer
now here I'll say first
lowercase in lower case I'll concatenate
the prefix
then ampersand
space
ampersand
first name
ampersand
space
ampersand
last name
and closing brackets
Etc
full name
okay so this is an example of full name
in low cases
now another
calculated column conditional column at
the customer level
I want to identify a flag which says who
is my Target customer base is the
demographics shared over here Target
customer
so I'll say
if
the marital status is equal to
m
and
total children
hurry
and total children annual income
so okay so let me change the logic a bit
so marital status is M and
age
is
less than
50
these customers are my Target customers
okay so I would say
yes
else no
see this
he is a marital status is married Logan
Diaz and age is less than 50 else
everyone so if I try to filter
so these are the my Target customers
69 out of 1178 so this is just a
conditional column but a logical
condition an example which I am trying
to highlight over here
okay now
let's look at certain calendar
date oriented columns calculated columns
very typical like now I have a simple
date column now I'll keep adding certain
columns which are you know which help
you which will help you understand how
we can uh
you know do some calculations on the
dates so like for example I want a date
which is 12 days after the current date
the date in the column so just simple 12
Days After
select the date and add 12.
now if you see the date format you can
go and change the format at the top
and if whatever you feel like like this
now see 12 days after first Jan 2015 is
13 Jan 2005. you can go and change the
format and other details
let me also show you if I go to my cost
and other columns I can go and change
the format this is like a currency cost
is currency so I can go and
select the
change the currency type and you can
even show the dollar value or whatever
currency type it is
so for numbers you can do currency or
text or dates you can select the format
so this is available at the column tools
level
now in customers
like we had our
column of full name so now what all
things are available format as text okay
data type text so very minimal options
are there with date you have options of
the date format
now next
I want a column which defines the expiry
date okay so eight months prior
to the
expiry date within which is coming up in
eight months so I'll create a column
called as eight months expiry
and then there is a e date function I'll
use that I'll use my
date in the data set
comma I'll say eight so now this date
column will append eight months to my
actual date and again I can go and
change the format
correct
now another important column like I want
to know the date name
so I'll use a function called as
format
and I'll select my calendar date column
and I'll say give me the DDD
format of it so it will give me the day
name the day the day name of on that
particular date
next
years in between
so I want the years in between the
today's date and the date of my calendar
so equal to
D Def
the calendar date
comma today
comma year
end
seven years 2015 to 2022
then last date of the month
so if I want what is the last date of
this particular calendar month
I will use a function EO month which is
there available in the
power bi so I'll say
last date of the month
equal to
e o month
then
just select the calendar CSV date
comma
0 in months and enter
change the format
then similarly start of the month
so I'll use a function called start of
month
so for for all January dates end of the
month is 31st Jan and start of the month
will remain 1st of Jan
change the format
next I want to know what is the weak
number of that particular date so now
there is an inbuilt function called
week number week num and just pass the
date and you will get
the weak number first week of the year
second week of the year Etc
now another very
good example is whether the weak
day is a weekday or a weekend right so
what is it's a weak type okay
so I'll check I'll put a if condition
and there is a function called weekday
and I'll pass the calendar
if it is less than 6
it means it's a weekday else it's a
weekend
so all Saturday
and Sunday D names will come as weekends
or else everything else will come as
weekly
so these are very different variations
of different column types calculated
columns which is a very important
utility and uh and any Bravo bi project
you will definitely be ending up
creating n number of calculated columns
to derive your numbers to prepare your
reports but it's important to understand
what all things we can do that yes there
are n number of functions available in
power bi uh but uh what I have tried to
Showcase over here is some important
functions but basis your utility basis
your problem statement you can look up
for a relevant function in the power bi
dictionary
now with this
introduction to calculated column this
is the base for us to now get into our
next session where we'll be talk we will
be talking about creating Dax measures
and Dax functions we will be using power
bi Dax functions which is much more
powerful than simple uh calculated
columns where you can do more uh complex
calculations uh you can calculate totals
and then use them in the reports
now
we have imported uh the sheet we will
make certain joins so the first step is
to drag the orders
table the order sheet on the
relationship canvas here okay and you
can see the data sample data the first
100 rows over here
okay then now we need to create an inner
join with people's table between order
and people okay so if you see
it has automatically detected the field
names on which the inner join has to be
created so on the order side you have
region and on the right hand side which
is the people data you have also a
region okay so both these columns are
common and that's how we have made a
join between orders and people data
so if I close this box and go and check
the people's data
open
so see the region and the person these
two columns from the people table have
now been
joined with the orders table right so it
means that these are the orders in a
particular region which has been placed
by
in this region
let me show you the sample Superstore
Excel file now this is the structure of
the file you have a sample list of
transactions basically the orders which
are placed by customers
across multiple regions
south west of USA South Region west
region then you have a list of order IDs
which have been returned so basically
the order ID in the returns sheet
matches with those orders in the order
stable
and then you have the people
sheet in which you have region
and a person associated with that region
the sales person associated with that
region okay so basically when we are
combining joining orders with people we
are joining that
which orders
belongs to which region and who's the
sales person associated with it so what
we have done over here is we have made a
inner join means all the orders should
belong to particular region and that
region is in the people's sheet
and then in the second step
now we will make a
left join between returns and orders
not inner join we'll make a left join
between returns and orders and we will
make a join using the order ID
okay so just edit this
click on left
and
select order ID as the join column now
what does left join means left join mean
is that consider all the orders from the
orders table and only consider the
orders from the returns table which have
data means which are returned otherwise
show null for the order IDs which are
not returned so if you see this is the
these are the two columns from the
returns table and these are null because
this is relevant to the order IDs which
are not returned okay which has been
accepted by the customer but these are
the orders for which you see data in the
returned and Order ID column it means
that these have been returned
now with these joins in place please
save your book and now we have our
relations created in the
um
in the Tableau now we are ready to
create certain reports and extract
certain kpis using this relationship
model
now we'll move to sheet1
okay
and first we will place
state
and person on the rose
sheet okay
then
I'll go to my
numbers and
put the profit or D so per state per
person how much profit I am making as a
company okay this is my goal to check
now
sort by highest to lowest
so California is giving me the maximum
profit of 76 381 then New York then
Washington so this is the sorted order
in which I have
listed my profit in descending order
now I can also check what are the number
of orders
placed
and check the distinct count
foreign
out of the total orders of 127 okay so
this is the number of total number of
orders which have been returned for
California is 127.
it's 16 29 so the sorted order is as per
the revenue as per the profit and this
is the details of the orders which have
been returned per state
so if you see for connected for a
cancers there are zero returns
so you can also extract data
table to refresh his orders and identify
new rows using order date so as and when
new data is being added you can
refresh it now say extract
and now you can save this information
profit
by state
and click save so this is the extraction
of this particular report which is
possible in tableau
so this is the first exercise which we
have completed for
reviewing and analyzing the profit per
state highest to lowest and within that
per state what are the number of orders
which have been returned by all the
customers the distinct count of order
IDs which have been returned
now let's start our second exercise on
creating calculated fields in tableau
now in this exercise we will be doing
certain
activities like we will be creating a
set to show the states which have more
than 100 customers then we will be
creating a calculated field to show an
average sales per customer okay then we
will create a calculated field to show
the sales goals and then show emerging
and developing stage so these are the
four kpis which we have to derive
now the first thing we have our sample
Superstore data already imported and the
relationships created in a join with
people and left join with returns
now we have our sheet 2 in which we will
create the states a list of states which
has more than 100 customers so what we
have to do is we have to click right
click on the customer name and click
create set
okay
now we have to give the name as states
with 100 plus customers
and then go to the condition tab
select by field
and then apply condition as
count of customer name
greater than equal to
100
and click
ok
now we have this set created states with
100 plus customers
now to determine average sales by
customer
we have to now create a calculated field
so go to the analysis and click on
create calculate field
okay
now name it as
average
sales per customer
and now we will say average
we will use a
thank you
okay
so we are saying that per customer we
are using a level of definition function
include which means that per customer
what is my average sales right we've
already used a function aggregated
function called average so we are saying
per customer give me the total and then
give me the average per customer so
we're going to click ok
now create another calculated field you
can also create from here
and name is as name it as sales goal
now in this we are going to type the
formula if
minimum
States
with 100 plus customers equal to true
then
sum of
sales
into 1.3
else
average sales per customer into
100 so me we are saying that
if the customer belongs to the set of
states with 100 plus customers then the
sales Target should be
1.3 times the actual sales as of today
else it should be 100 of the average
sales per customer
now let's create another calculated
field which we call as
emerging
or developing state
if distinct count
of customer name
is greater than equal to 100
then
the state is tagged as developing state
foreign
state
okay so we have now three calculated
Fields average sales per customer
emerging or developing State and sales
goals
now we will use this in our reporting
so we will drag sales goal under the
columns
and then I'll drop my state
so now this is the statewise sales goal
depending whether the state has 100 plus
customers or not
then add your customer name
make the measure as count distinct
and make it as discrete
okay so if you see this
we have the count of customers per state
and the
the sales goal
for that particular state
and now
I'll put my sum of sales the total sales
which I want
which is there per state
now go to show me and select
this particular chart
bullet graph
now to bring sales goals to column right
click on the sales access and select
swap reference line fields
now from your left hand panel drag and
drop emerging or developing State on the
color panel
okay so a merging state is the orange
one and the developing state is the blue
one
and save the sheet as
developing and the emerging States
so if you see this it's an emerging
State because its count is less than the
customer count is less than 100
its sales goal is
57384 but the actual sales is 19511 okay
so now this is a developing State its
count is greater than equal to 100 and
its sales goal and its sales is exactly
the same it matches so that's why you
are saying the bar and the blue bar is
ending exactly where the vertical bar is
thank you
so what we are trying to depict is that
whether the state is going Beyond its
Target sales goal or it's behind it
and you can see that using this
particular vertical bar like for example
Michigan its sales goal is 71 952 but
its actual sales is seven six seven two
seven zero average sales so that's why
it is be above its Target
and it's a developing state
because it has more than 100 customers
so you can even sort
by the count of
the uh customers higher to lower so all
your developing state will group from at
the top and the emerging States Will
Group at the bottom
or you can sort by
the sales goal
so the orange bar is the sales goal or
the blue bar so depending what sales
goal
has been
derived for each state
So currently on my screen what you can
see is the Tableau the Netflix data
sheets the data sources where you have
the Netflix shows movies their
Associated duration uh what kind of
shows or movies are there the release
here are the associated rating
description and a key which is the show
ID now this is the unique identifier for
each show in this Netflix title sheet
and with this show ID all other sheets
are related like you know who are the
directors of the show in which country
the show was released
what is the cost of each show all that
information is in this sheet and to
which particular category basically the
listed in category is being listed in
this particular sheets so what we're
going to do is first we are going to
import this sheet into tableau
okay and then create relationship
between them using the Tableau
relationship canvas
so first our the primary transaction
table the sheet in which all the
information related to movies is there
or shows is there is in Netflix titles
and then we will drag
sheet like Netflix cast titles cast now
W has automatically identified the
relationship between the show ID of
titles and show ID of titles cast and it
has made a join
so if you double click this
you can see this relationship
cardinality and the related fields
similarly I'll drag
titles category
countries and directors
Now by virtue of this all the sheets
have now been joined with Netflix titles
and with this relationship ready we can
start preparing our reports
now let's create a basic report where we
can just glance the data like you know
whatever we were seeing in the Excel how
it looks in tableau so you have all the
types of movies then
for example I drag the release Here
now first I do not want to consider it
as a
Dimension so I'll just
release Here per category per type there
is a release Here
and then I'll drop the listed in
so now these are the categories so per
year the the details of the categories
right and then you can
drag the title and the associated rating
so this is just a view of your data
we can name it as shows listing report
now let's try to create some report for
some
measures some numbers
Etc
so now let's check in which country how
many movies or titles were released you
know what is the count
so first let's drag the
country
so as soon as we dragged the country
field it is identified that it has the
geographical names and identified the
latitude and longitude details so
Tableau internally does that
automatically and it has identified the
spots across the globe of the relevant
country
now let's drag the listed in on the
color section
now what it is doing is it is showing in
which country what different kind of
uh category wise movies are or shows
have been released like in Sri Lanka
documentaries have been released in
India action and adventure United States
action and adventure like this
now let's
put
a count of
the titles right so if you see 247
action adventure movies or shows have
been released in United States
okay so this is one inference by this
particular report you can
identify
so let's save this report as
listed
in
by control
okay now let's create another report
we will call it as the per year
statistical report in this first I'll
put the release years in the column
now
count of Netflix titles so this is the
count of Netflix titles per year 2017
2016 14. and then count of Netflix
titles in the countries
this is the second bar chart okay now
what I'll do is I'll combine it into one
so one report
we are
moving in the bar creating the bar and
one in the line now I just have to
so we will click on the count of Netflix
titles by country right click Market
dual access
so now as soon as you click click this
both the charts have been combined and
right click over here and say
synchronize access
okay
okay so now if you can see see right in
2017 you had uh 2 303 Netflix releases
across all categories and one one five
nine
titles right one zero six three titles
in 2017. so this is a descending
representation of the count of titles
release per year across category and
across titles
so let's call this as
per year
stats
now let's create another report
number of shows
per
title okay
so or or sorry per rating so we will
drag the rating column
and
we will
count of titles
unique count of titles and unique count
of show IDs
okay
one is bar and one is line
okay
so this is a report which shows rating
wise titles and the show IDs
so in the tooltip you can see the
listing count of Titus and show ID per
rating
okay so let's call it
shows for rating
now let's create another sheet call it
as
shows
by cast
so in this
what we're going to do is first drag the
cast column on the rows section you have
name all of all the cast and then we
drag the
show ID on the column section and put a
count d
and sort it okay
you can remove the null cast
and this is your
sorted order and on the labels section
you can say show Mark labels and you
will see the count that don't open care
has done the maximum then on Shahrukh
Khan om Puri nasruddin Shah so this is
the details on this sorted order that
who has done how many shows okay
then next is
shows by director similar to shows by
cast
drag
director into rows
or differently if you want to prepare
into columns and then count off
show ID
okay sort it
and you can actually filter out the null
director value
and this is the account put the label
so you can see Jan's Twitter has the
maximum shows
okay
then create another report
shows by category
now similarly drag the listed in
in the rows and show ID count
remove the null category
and sort
labels show Mark labels so International
movies are the highest category dramas
the next comedy International TV shows
this way
you have your category
now let's create a dashboard in which we
want to bring combine all these reports
and
take a common view so first let's drag
listed in by country
then
shows by director
also please let's set the size as
automatic
then let's drag shows by category
and shows by cast
now these four reports are on a single
dashboard we will link them to each
other so go to worksheet actions
add action
filter
select dashboard one
only select shows by category here
and in Target dashboard one
except shows by category keep everything
else
and in the source sheet just select
click select
click ok
now whatever category you will select
over here
that related category data will
automatically be shown in other reports
like see International movies across the
globe across countries directors of only
International movies who has done the
maximum
Johnny Tow and uh shows by cast right
that who has done the maximum
International movies or dramas
or comedies
travel has a maximum number of comedies
and then if you see the comedy movies uh
you know these are the count of comedy
movies released across the countries and
the who is the director
another a very interesting report which
you can prepare is that for example you
want to check in which country maximum
duration of
uh your
maximum duration of
movies have been released so
first let's create uh to measure
remove this
okay
so if you see
maximum duration of the movies or the
entire Netflix content is maximum United
States then in India these many minutes
next is United Kingdom and you can also
change the colors
whatever you feel is as per your
standards or as per the convention you
can change the color combination
so there are multiple ways you can
generate reports and uh hope you have
understood how you can leverage such a
data to create your reports hello and
welcome to data analytics interview
questions
simplylearn.com get certified get ahead
today we're going to jump into some
common questions you might see on numpy
arrays and pandas data frames in the
python along with some Excel Tableau and
SQL
let's start with our first question what
is the difference between Data Mining
and data profiling
it's real important to note that data
mining is a process of finding relevant
information which has not been found
before it is a way in which raw data is
turned into valuable information you can
think of this as anything from the sales
stats and from their SQL Server all the
way to web scraping and Census Bureau
information where the heck do you mine
it from where do you get all this data
and information
then we look at data profiling is
usually done to assess a data set for
its uniqueness consistency and logic it
cannot identify incorrect or inaccurate
data values so if somebody has a
statistical analysis on one side and
they're doing their you might in the
wrong data to then program your data
setup so you got to be aware that when
you're talking about data mining you
need to look at the Integrity of what
you're bringing in where it's coming
from data profiling is looking at it and
saying hey how is this going to work
what's the logic what's the consistency
is it related to what I'm working with
find the term data wrangling and data
analytics data wrangling is a process of
cleaning structuring and enriching the
raw data into a desired usable format
for better decision making
and you can see a nice chart here with
our Discover it we structure the data
how we want it we clean it up get rid of
all those null values we enrich it so we
might take and reformat some of the
settings instead of having five
different terms for height of somebody
you know in American English or whatever
clean some of that up and we might do a
calculation and bring some of them
together
and validate I was just talking about
that in the last one need to validate
your data make sure you have a solid
data source and then of course it goes
into the analysis very important to
notice here in data wrangling 80 percent
of data analytics is usually in this
whole part of wrangling the data getting
it to fit correctly and don't confuse
that with data cooking which is actually
when you're going into neural networks
cooking the data so it's all between
zero and one values
what are common problems that data
analysts encounter during analysis
handling duplicate and missing values
collecting the meaningful write data the
right time making data secure and
dealing with compliance issues handling
data purging and storage problems again
we're talking about data wrangling here
eighty percent of most jobs are in
wrangling that data and getting it in
the right format and making sure it's
good data to use
number four what are the various steps
involved in any analytics project
understand the problem we may spend 80
percent doing wrangling but you better
be ready to understand the problem
because if you can't you're going to
spend all your time in the wrong
direction this is probably uh the most
important part of the process everything
after it falls in and then you can come
back to it two data collection data
cleaning number three four data
exploration analysis and five interpret
the results
number five is a close second for being
the most important if you can't
interpret what you bring to the table to
your clients you're in trouble
so when this question comes up you
probably want to focus on those two
noting that the rest of it does eighty
percent of the work is in two three and
four well one and five are the most
important parts
which technical tools have you used for
analysis and presentation purposes
being a data analyst you are expected to
have knowledge of the below tools for
analysis and presentation purposes
there's a wide variety out there SQL
Server MySQL you have your Excel your
SPSS which is the IBM platform tab blue
python you have all these different
Tools in here now certainly a lot of
jobs are going to be narrowed in on just
a few of these tools like you're not
going to have a Microsoft SQL Server
MySQL server but you better understand
how to do basic SQL polls and also
understanding Excel and how the
different formats from column and how to
get those set up
number six what are the best practices
for data cleaning this is really
important to remember to go through this
in detail these always come up because
80 percent of most data analysis is in
cleaning the data make a data cleaning
plan by understanding where the common
errors take place and keep
Communications open identify and remove
duplicates before working with the data
this will lead to an effective data
analysis process focus on the accuracy
of the data maintain the value types of
data provide mandatory constraints and
set Cross Field validation
standardize the data at the point of
entry so that it is less chaotic and you
will be able to ensure that all the
information is standardized leading to
fewer errors on Entry
number seven how can you handle missing
values in a data set list wise deletion
in listwise deletion method entire
record is excluded from analysis if any
single value is missing sometimes we're
talking about records remember this
could be a single line in a database so
if you have your SQL comes back and you
have 15 different columns every one of
those has a missing value you might just
drop it just to make it easy because you
already have enough data to do the
processing average imputation use the
average value of the responses from the
other participants to fill in the
missing value this is really useful and
they'll ask you why these are useful I
guarantee it if you have a whole group
of data that's collected and it doesn't
have that information in it at that
point you might average it in there
regression substitution you can use
multiple regression analysis to estimate
a missing value that kind of goes with
the average imputation input regression
model means you're just going to get
you're going to actually generate
generate a prediction as to what you
think that value should be for those
people based on the ones you do have
multiple amputation so we talk about
multiple inputs it creates plausible
values based on the correlations for the
missing data and then average the
simulated data sets by incorporating
random errors in your predictions
what do you understand by the term
normal distribution and the second you
hear the word normal distribution should
be you think in a bell curve like we see
here normal distribution is a type of
continuous probability distribution that
is symmetric about the mean and in the
graph normal distribution will appear as
a bell curve the mean median and mode
are equal that's a quick way to know if
you have normal distribution is you can
compute mean median and mode all of them
are located at the center of the
distribution 68 of the data lies within
one standard deviation of the mean 95
percent of the data Falls within two
standard deviations of the mean 99.7 of
the data lies within three standard
deviations of the mean
what is time series analysis time series
analysis is a statistical method that
deals with ordered sequence of values of
a variable of equally spaced time
intervals time series data on a covid-19
cases and you can see we're looking at
by day so our spaces of days and each
day goes by if we take a graph it you
can see the time series graph always
looks really nice if you have like two
different in this case we have what the
United States going over there I have to
look at the other setup in there but
they picked a couple different countries
and it is it's time sensitive you know
the next result is based on what the
last one was Cove is an excellent
example of this anytime you do any word
analytics where you're figuring out what
someone's saying what they said before
makes a huge difference is what they're
going to say next another form of Time
series analysis
10. how is joining different from
blending in tableau
so now we're going to jump into the
table blue package data joining data
joining can only be done when the data
comes from the same Source combining two
tables from the same database or two or
more worksheets from the same Excel file
all the combined tables or sheets
contains common set of dimensions and
measures
data blending data blending is used when
the data is from two or more different
sources combining the Oracle table with
the SQL server or two sheets from Excel
or combining Excel sheet and Oracle
table in data blending each data source
contains its own set of dimensions and
measures
how is overfitting different from
underfitting
how he's a good one overfitting probably
the biggest danger in data analytics
today is overfitting model trains from
the data too well using the training set
the performance drops significantly over
the test set happens when the model
learns the noise and random fluctuations
in the training data set in detail and
again the performance drops way below
what the test set has
the model neither trains the data well
nor can generalize to new data performs
poorly both on train and the test set
happens when there is less data to build
and an accurate model and also when we
try to build a linear model with a
non-linear data
in Microsoft Excel a numeric value can
be treated as a text value if it
proceeds with an apostrophe definitely
not an exclamation if you're used to
programming in Python you'll look for
that hash code and not an Amber sign
and we can see here if you enter the
value 10 into a fill but you put the
apostrophe in front of it it will read
that as a text not as a number
what is the difference between count
count a count blank and count if in
Excel
we can see here when we run in just
count D1 through D 23 we get 19 and
you'll notice that there is 19 numbers
coming down here
so it doesn't count the cost of each
which is a top bracket it doesn't count
the blank spaces either with the
straight count
when you do a count a you'll get the
answer is 20. so now when you do count a
it counts all of them even the title
cost of each
when you do count blank we'll get three
why there's three blank fields
and finally the count if if we do count
F of e 1 to e23 is greater than 10
there's 11 values in there basic
counting of whatever is in your column
pretty solid on the table there
explain how vlookup Works in Excel
vlookup is used when you need to find
things in a table or a range by row
the syntax has four different parts to
it we have our lookup value that's the
value you want to look up we have our
table array
the range where the lookup value is
located
column index number the column number
and range that contains the return value
and the range lookup specify true if you
want an approximate match or false if
you want an exact match of the return
value
so here we see vlookup F3 A2 to C8 2
comma zero for prints now they don't
show the F3 F3 is the actual cell that
prints is in that's what we're looking
at is F3 so there's your prints he pulls
in from F3 A2 to C8 is the the data
we're looking into and then number two
is a column in that data so in this case
we're looking for uh age and we count
name as one age is two keep in mind this
is Excel versus a lot of your Python and
programming languages where you start at
zero in Excel we always look at the
cells as one two three so two represents
the age
0 is false for having an exact match up
versus one we don't actually need to
worry about that too much in this 0 or 1
would work with this example and you can
see with the Angela lookup again her
name would be in the F column number
four that's what the F4 stands for is
where they pulled Angela from and then
you have A1 to C8 and then we're looking
at number three so number three is
height name being one H2 and then height
three and you'll see here pulls inner
height 5.8
so we're going to run jump over to SQL
how do you subset or filter data in SQL
to subset or filter data in SQL we use
where and having clause and you can see
we have a nice table on the left where
we have the title the director the year
the duration we want to filter the table
for movies that were directed by Brad
Bird why just because we want to know
who what Brad Bird did so we're going to
do select star you should know that the
star refers to all in this case we're
what are we going to return we're going
to return all title directory year and
duration that's what you mean by all
movies movies being our table where
director equals Brad Bird and you can
see he comes back and he did the
incredible on Ratatouille
two subsetter filter data SQL we can
also use the where and having Clause so
we're going to take a closer look at the
different ways we can filter here filter
the table for directors whose movies
have an average duration greater than
115 minutes so there's a lot of really
cool things into this SQL query and
these SQL queries can get pretty crazy
select director sum duration as total
duration average duration as average
duration from movies Group by director
having average duration greater than
115.
uh so again what are we going to return
we're going to return whenever we put in
our select which in this case is
director we're going to have total
duration and that's going to be the sum
of the duration we're going to have the
average duration average underscore
duration which is going to be the
average duration on there and then we of
course go ahead and group by director
and we want to make sure we group them
by anyone that has an having an average
duration greater than 115. these SQL
queries are so important I don't know
how many times you the SQL comes up and
there's so many different other
languages not just MySQL and not
Microsoft SQL but addition to that where
the SQL language comes in especially
with Hadoop in other areas so you really
should know your basic SQL doesn't hurt
to get that little cheat sheet and
glance over it and double check some of
the different features in SQL
what is the difference between where and
having clause in SQL where where Clause
works on row data in where Clause the
filter occurs before any groupings are
made aggregate conscience cannot be used
so the syntax is select your columns
from table where what the condition is
having Clause works on aggregated data
having is used to filter values from a
group aggregate functions can be used in
the syntaxes select column names from
table where the condition is grouped by
having a condition ordered by column
names
what is the correct Syntax for reshape
function in numpy so we're going to jump
to the numpy array program
and what you come up with is you have in
this case be numpy.reshape a lot of
times you do an import numpy as NP
reshape and then your array and the new
shape
and you can see here as we as the actual
example comes in the reshape is a and
we're going to reshape it in two comma
five setups and you can see the printout
in there that prints in two rows with
five values in each one
what are the different ways to create a
data frame in pandas
well we can do it by initializing a list
so you can Port your pandas as PD very
common data equals Tom 30 jerry20 Angela
35 we'll go ahead and create the data
frame and we'll say
pd.dataframe is the data columns equals
name and age so you can designate your
columns you can also it is a index in
there you should always remember that
the index in this case maybe you want
the index instead of one two to be the
date they signed up or who knows you
know whatever and you can see right
there it just generates a nice pandas
data frame with Tom Jerry and Angela
another way you can initialize a data
frame is from dictionary you can see
here we have a dictionary where the date
equals name Tom Jerry Angela Mary ages
20 21 1918 and if we do a DF PD dot data
frame on the data you'll get a nice the
same kind of setup you get your name age
Tom Jerry Angela and Mary
write the python code to create an
employee's data frame from the
emp.csv file and display the head and
summary of it to create a data frame in
Python you need to import the pandas
library and use the read CSV function to
load the CSV file
and here you can see we have import
pandas is PD employees or the data frame
employees equals pd.read CSV and then
you have your path to that CSV file
there's a number of settings in the read
CSV where you can tell it how many rows
are the top index you can set the
columns in there
you can have skip rows there's all kinds
of things you can also go in there and
double check with your read CSV but the
most basic one is just to read a basic
CSV
how will you select the department and
age columns from an employee's data
frame
so we have import pandas is PD you can
see we have created our data we will go
ahead and create our employees PD data
frame on the left
and then on the right to select
department and age from the data frame
we just do employees you put the
brackets around it now if you're just
doing one column you could do just
department but if you're doing multiple
columns you've got to have those in a
second set of brackets it's got to be a
reference with a list within the
reference
what is the criteria to say whether a
developed data model is good or not a
good model should be intuitive
insightful and self-explanatory follow
the old saying kiss keep it simple
the model develops should be able to
easily consumed by the clients for
actionable and profitable results
so if they can't read it what good is it
a good model should easily adapt to
changes according to business
requirements we live in quite a dynamic
world nowadays so it's pretty
self-evident and if the data gets
updated the model should be able to
scale accordingly to the new data so you
have a nice data pipeline going where
when something when you get new data
coming in you don't have to go and
rewrite the whole code
what is the significance of exploratory
data analysis
exploratory data analysis is an
important step in any data analysis
process
exploratory data analysis Eda helps to
understand the data better it helps you
obtain confidence in your data to a
point where you're ready to engage a
machine learning algorithm it allows you
to refine your selection of feature
variables that will be used later for
model building you can discover hidden
Trends and insights from the data
how do you treat outliers in a data set
an outlier is a data point that is
distant from the other similar points
they may be due to variability in the
measurement or may indicate experimental
errors
uh one you can drop the outlier records
pretty straightforward you can cap your
outliers data so it doesn't go past a
certain value you can assign it a new
value you can also try a new
transformation to see if those outliers
come in if you transform it slightly
differently
explain descriptive predictive and
prescriptive analytics descriptive
provides insights into the past to
answer what has happened uses data
aggregation and data mining techniques
examples an ice cream company can
analyze how much ice cream was sold
which flavors were sold and whether more
or less ice cream was sold than before
predictive understands the future to the
answer what could happen use the
statistical models and forecasting
techniques
an example predicts a sale of ice creams
during the summer spring and rainy days
so this is always interesting because
you have your descriptive which comes in
and your businesses are always looking
to know what happened hey did we have
good sales last uh quarter what are we
expecting next quarter in sales and we
have a huge jump when we do uh
prescriptive suggest various courses of
action to answer what should you do uses
optimization and simulation algorithms
to advise possible outcomes example
lower prices to increase sell of ice
creams produce more or less quantities
of certain flavor of ice cream and we
can certainly uh today's world with the
covid virus we had that on an earlier
graph you could see that as a
descriptive what's happened how many
people have been infected how many
people have died in an area predictive
where do we predict that to go
um do we see it going to get worse is it
going to get better what do we predict
that we're going to need in hospital
beds and prescriptive what can we change
in our setup do you have a better
outcome maybe if we did more social
distancing if we tracked the virus
how do these different things directly
affect the end and can we create a
better ending by changing some
underlying criteria
what are the different types of sampling
techniques used by data analysis
sampling is a statistical method to
select a subset of data from an entire
data set population to estimate the
characteristics of the whole population
one we can do a simple random sampling
so we can just pick out 500 random
people in the United States to sample
them
they call it a population in regular
data we also call that a population just
because that's where it came from was
mainly from doing census
systematic sampling
cluster sampling
stratified sampling
and judgment or purposive sampling
then we have our systematic sampling
that's where you're doing like using one
five ten fifteen twenty use a very
systematic approach for pulling samples
from the setup cluster sampling that's
where we look at it we say hey some of
these things just naturally group
together if you were talking about
population which is the really a nice
way of looking at this cluster sampling
would be maybe by a zip code we're going
to do everybody's zip code and just
naturally cluster it that way
stratified sampling would be more
looking for shared things a group has
like income so if you're studying
something on poverty you might look at
their naturally group People based on
income to begin with and then study
those individuals in the income to find
out what kind of traits they have
and then judgmental that is where the
researcher very carefully selects each
member of their own group
so it's very much based on their
personal knowledge
jumping on the 26 what are the different
types of hypothesis testing
hypothesis testing is a procedure used
by estheticians and scientists to accept
or reject statistical hypothesis we
start with a hypothesis testing we have
null hypothesis and alternative
hypothesis
on the null hypothesis it states that
there is no relation between the
predictor and the outcome variables in
the population it is denoted by H naught
example there is no association between
patients BMI and diabetes
alternative hypothesis it states there
is some relation between the predictor
and outcome variables in the population
it is denoted by H1 example there could
be an association between patients BMI
and diabetes
and that's the body mass index if you
didn't catch the BMI and you're not in
medical
describe univariate bivariate and
multivariate Analysis
a unit variate analysis it is the
simplest form of data analysis where the
data being analyzed contains only one
variable an example is studying the
heights of players in the NBA because
it's so simple it can be described using
Central Tendencies dispersion quartiles
bar charts histograms pie charts
frequency distribution tables
the bivariate analysis it involves
analysis of two variables to find causes
relationships and correlations between
the variables example analyzing sale of
ice creams based on the temperature
outside
bivariate analysis can be explained
using correlation coefficients linear
regression logistic regression Scatter
Plots and box plots
and multivariate Analysis it involves
analysis of three or more variables to
understand the relationship of each
variable with the other variables
example analyzing Revenue based on
expenditure so if we have our TV ads we
have our newspaper ads our social media
ads and our Revenue we can now compare
all those together
the multiverted analysis can be
performed using multiple regression
factor analysis classification and
regression trees cluster analysis
principle component analysis clustering
bar chart dual axis chart
what function would you use to get the
current date and time in Excel
in Excel you can use the today and now
function to get the current date and
time and you can see down here with the
two examples are just equals today or
equals now
using the sum ifs function in Excel find
the total quantity sold by cells
Representatives whose names start with a
and the cost of each item they have sold
is greater than 10.
and you can see here on the left we have
our actual table
and then we want to go ahead and sumifs
so we want the E2 through E20
B2 through B20 greater than 10. and this
basically is just saying hey we're going
to take everything in the E column
and we're going to sum it up but only
those objects where the D column is
greater than 10 that's what that means
there
is the below query correct if not how
will you Rectify it
select customer ID year order date as
order Year from order where order year
is greater than or equal to 2016.
and hopefully you caught it right there
uh it's in the devils in the details we
can't not use the Alias name while
filtering data using the where Clause so
the correct format is all the same
except for where it says where the year
order date is greater than or equal to
16 versus using the order year which we
assign under the select setup
how are union intersect and accept used
in SQL the union operator is used to
combine the results of two or more
select statements
and you can see here we have select star
from region one and we're going to make
a union with select star from region two
and it basically takes both these SQL
tables and combines them to form a full
new table on there so that's your union
as we bring everything together
we look at the intersect operator
Returns the common records that are the
result of the two or more select
statements
so you can see here we select star from
region one intersect select star from
region two
and we come up with only those records
that are shared that have the same data
in them and hopefully you jumped ahead
to the accept the accept operator
returns The Uncommon records that are
the result of two or more select
statements so these are the two records
or the records that are not shared
between the two databases
using the product price table write an
SQL query to find the record with the
fourth highest market price
so here we have a little bit of a brain
teaser
they're always fun
and the first thing we want to do is
we're going to go ahead and I'm going to
if you look at the script on the left we
really want the fourth one down so we're
going to select the top four from
product price but we're going to order
it by Marketplace descending SP order by
market price ascending
so we do is we take the top four of the
market price ascending and that's going
to give us the four greatest values
and then we're going to reverse that
order and do descending and we're going
to take the top one of that which is
going to give us the lowest value which
will be the fourth greatest one in the
list
from the product price table find the
total and average market price for each
currency where the average market price
is greater than 100 and currency is in
the INR or the AUD
so INR or AUD India Rupal or Australia
dollar
you can see over here the SQL query if
you had trouble putting this together
you might actually do some of it in
reverse and you can see right here where
the average market price is greater than
50. remember we use having not where at
the end because it's part of the group
so Group by currency because we want
those two currencies and we want the
currency India the INR or the AUD
and as you keep going backwards we're
actually going to be selecting the
currency the sum of the market price as
total price and the average Marketplace
as average price so there's our select
it's going to come from the product
price which is just our table over there
and then we have where our currency is
in uh and like I said you can put
together however you want but hopefully
you got to the end there
so this question will test your
knowledge in Tableau exploring the
different features of Tableau and
creating a suitable graph to solve a
business problem
and of course Tableau is very visual in
its use so it's very hard to test it
without actually just getting your hands
on and if you can't visualize some of
this and how to do it then you should go
back and refresh yourself
using the sample Superstore data set
Create A View to analyze the cells
profits and quantities sold across
different subcategories of items present
under each category so the first step is
to go ahead and load the sample
Superstore data set
so make sure you know how to load the
sample the superstore data set that's
underneath either the connect button in
the upper left or the Tableau icon up
there and be able to pull in the data
set and then once you've done that you
just drag the category and subcategory
on rows and salaries onto columns it
will result in a horizontal bar chart
so in this one we're just going to drag
profit onto color and quantity onto
label sort the sales axes in descending
order of sum and cells within each sub
category
and if you're at home doing this you'll
see the chairs in their Furniture
category have the highest sales and
profit while tables had the lowest
profit for office supplies subcategory
binders made the highest profit even
though storage had the highest sales
under technology category copiers made
the highest profit though it was the
least amount of sales
let's work to create a dual axis chart
in Tableau to present cells and profits
across different years using sample
Superstore data set
load the orders sheet from the sample
Superstore data set
drag the ordered data field from the
dimensions onto columns and converted
into continuous months
drag cells onto rows and profits to the
right corner of the view until you see a
light green rectangle one of those
things if you haven't done this Hands-On
you don't know what you're doing you're
right into a buying so you can be just
kind of dropping it and wondering what
happened synchronize the right axes by
right-clicking on the profit axes
and then let's finalize it by going
under the marks card change some cells
to bar and some profit to line and
adjust the size
and then we have a nice display that we
can either print out or save and send
off to the shareholders
let's go and do one more Tableau design
a view in Tableau to show Statewide
cells and profits using the sample
Superstore data set
and here you go ahead and drag the
country field onto the view section and
expand it to see the states drag the
states field onto size and profit onto
color
increase the size of the Bubbles at a
border and a Halo color States like
Washington California and New York have
the highest sales and profits while
Texas Pennsylvania and Ohio have a good
amount of sales but the least amount of
profits
we'll go ahead and Skip back to python
numpy Suppose there is an array number
equals NP or numpy if you're using numpy
depending on how you set it up dot array
and we just have one to nine broken up
into three groups extract the value 8
using 2D indexing so you can see on the
left we have our import numpy is in p
number equals our NP array if we print
the number we have one two three four
five six seven eight nine
since the value 8 is present in the
second row and First Column we use the
same index position and pass it to the
array and you just have number two comma
1 and you get eight and remember we're
in Python so you start at zero not one
like you do in Excel
always gets me if I'm working between
Excel and python where I just kind of
flip and usually it's the Excel that
messes up because I do a lot more
programming
Suppose there is an array that has value
0 1 all the way up to nine how will you
display the following values from the
array one three five seven nine
uh so first of all we go ahead and
create the array NP dot a range of 10
which goes from 0 to 9 because there's
ten numbers in it but we don't include
the 10. we print it out the first thing
you want to do is what's going on here
with one three five seven nine well if
we divide by two there's going to be a
remainder equal to one and then from
python we remember if you use the
percentage sign you get the remainder on
there so the remainder is 1 and then you
have the your numpy array and then we
just want to do a logical statement of
all values that have a remainder of 1
and that generates our nice one three
five seven nine
there are two arrays A and B stack the
arrays A and B horizontally boy these
horizontal vertical questions will get
you every time
and in numpy we go ahead and we've
created two different arrays over here A
and B uh the first one is your
concatenate NP dot concatenate
A and B on axes equal one
that is the same as H stack and in the
back end they're still identical they
run the same that's all each stack is a
concatenate and axes equals one
how can you add a column to a panda's
data frame
suppose there's an imp data frame that
has information about few employees
let's add address column to that data
frame and you can see on the left we
have our basic data frame you should
know your data frames very well
basically looks like an Excel
spreadsheet as you come over here it's
really simple you just do DF of address
equals the address once you've assigned
values to the address
using the below given data create a
pivot table to find the total cells made
by each cells represented for each item
display the cells as a percentage of the
grand total
so we're back in Tableau select the
entire table range click on insert Tab
and choose pivot table
select the table range and the worksheet
where you want to place the pivot table
it'll return a pivot table where you can
analyze your data
drag the cell total on the values and
sales rep and item onto row labels it'll
give the sum of the cells made by each
representative for each item they have
sold
and finally right click on sum of cell
total and expand show values as to
select percentage of grand total
real important just understand what a
pivot table is we're just pivoting it
from rows and columns and switching this
direction on there
and finally we have our final pivot
table and you can see the values roles
and sum of total sale
so we're going to go ahead and take a
product table this is off of an SQL so
we're going to do some SQL here
and we're going to use the product and
sales order detail table find the
products that have total units sold
greater than 1.5 million and here's our
sales order detail table so we have a
product table and a sales order detail
table two separate tables in the
database
and we're going to do is put together
the SQL query we want to select PP name
sum sod unit price as cells and then we
have our p p dot product ID from
production product as PP inner join
cells.sales order detail as sod on PP
product ID equals sod.product ID Group
by pp.name comma pp.product ID having a
sum of sod.uniprice greater than the 150
million there that's a mouthful and
again these SQL queries they start
looking really crazy until you just
break them apart and do them step by
step
and what we're looking for is the inner
join and how did you do the group by
this really wanted to know how do you do
this inner join this comes up so much in
SQL how do you pull in the ID from one
chart and the information from another
chart and the sum totals on that chart
how do you write a stored procedure in
SQL let's create a storage procedure to
find the sum the squares or the first n
natural numbers so here we have our
formula n times n plus 1 times 2N plus 1
over 6. and you can see from the command
prompt uh or the setup you have which
depending on what your login is the
command is create procedure Square sum1
declare our variable at n of integer as
begin and then we're going to declare
the sum of integer set sum equals n
times n plus 1 plus 2 times n plus 1
over 6.
of course we can go ahead and print
those out print First cast
member sign in or our variable as a
variable character 20 natural numbers
parenthesis sum of the square is cast
the at sum as a variable character 40
and then we do the output display the
sum of the square for first four natural
numbers we have execute Square sum 1 and
then we're going to put in 4 and you can
see here where it brings up the first
four natural numbers sum of square is
30.
write a store procedure to find the
total even number between two user given
numbers
couple things to note here first we go
and create our procedure you have your
two different variables the N1 N2 and we
go ahead and begin we're going to
declare our variable count as an integer
we're going to set count equal to zero
and then we have while n is less than N2
we're going to begin and if N1 remainder
2 equals 0 so we're going to divide it
by two even number begin we're going to
set the count equal to count plus one
we're going to print even number plus
cast n as a variable character 10 for
printing count is plus cast variable
count as variable character 10 end else
print odd number plus cast variable
number one as variable character ten and
then we go ahead and set the increment
our variable one up one so it goes from
n 1 all the way to N2 and I'll print the
total number of even numbers
and you can see here we went ahead and
executed it we're going to count the
even numbers between 30 and 45 and you
can see it goes all the way down to
eight
what is the difference between tree maps
and heat maps in tableau
now if you've worked in Python other
programmings you should automatically
know what a heat map is but a tree map
are used to display data in nested
rectangles use Dimensions to define the
structure of the tree map and measure to
define the size or color of individual
rectangles
tree maps are relatively simple data
visualization that can provide Insight
in a visually attractive format
and again you can see the squares over
here this is our tree map over here with
the each block also has this information
inside of its different blocks
a heat map helps to visualize measures
against Dimensions with the help of
colors and size to compare one or more
dimensions and up to two measures the
layout is similar to a text table with
variations in values encoded as colors
in heat map you can quickly see a wide
array of information
and in this one you can see they use the
colors to denote one thing and the size
of the little square to denote something
else
a lot of times you can even graph this
into a three-dimensional graph with
other data so it pops out but again a
heat map is the color and the size
using the sample Superstore data set
display the top five and bottom five
customers based on their profit so you
start by dragging the customer name
field onto rows and profit on columns
right click on the customer name column
to create a set
give a name to the set and select top
tab to choose top 5 customers by some
profit similarly create a set for the
bottom five customers by some profit
select both the sets right click to
create a combined set give a name to the
set and choose all members in both sets
and then you can drag top and bottom
customer sets onto the filters and
profit field onto color to get the
desired results
as we get down to the end of our list
we're going to try to keep you on your
toes we're going to skip back to numpy
how to print four random integers
between 1 and 15 using numpy to Generate
random numbers using numpy we use a
random random integer function and you
see here we did the import numpy as in p
random Arrangement equals
np.random.random integer 1 through 15 of
4.
from the below data frame I'm going to
jump again on you now we're into pandas
how will you find the unique values for
each column and subset the data for age
less than 35 and height greater than 6.
to find the unique values and the number
of unique elements use the unique and
the in unique function
you can see here we just did DF Heights
we're selecting just the height column
and we want to look for the unique and
that returns an array where in unique if
we do that on the height or the age
we'll return just the number of unique
values
and then we can do a subset the data for
ages less than 35 and height greater
than 6. so if we look over here we have
a new DF remember this is going to be
taking slices of our original data frame
it doesn't actually change the data
frame so our new DF equals the data
frame or DF the data frame where age is
less than 35
and the height is greater than 6.
and in case you're not using uh tab blue
which has a lot of its own different
mapping programs in there make sure you
understand how to use the basics of
matplot Library plot a sine graph using
numpy and matplot library in Python and
the way we did this is we went ahead and
generated an X we know our y equals NP
dot sine of x if you print out X you'll
see a whole value here our matplot
library Pi plot as PLT if you are
working in Jupiter notebook make sure
you understand the matplot library
inline that little percentage sign
matplot Library Online that prints it on
the page in the jupyter notebook the
newer version of jupyter notebook or
Jupiter Labs automatically does that for
you but I usually put it in there just
in case I end up on an older version
if you print y you can see here we have
our different y values and our different
X values
you simply put in plt.plot x y and do a
plot show
and before we go let's get one more in
we're going to do a pandas using the
below pandas data frame find the company
with the highest average cells derive
the summary statistics for the cells
column and transpose these statistics
that's a mouthful and just like any of
these computer problems break it apart
so first of all we're looking for the
highest average cells so group the
company column and use the mean function
to find the average cells you see here
by company equals df.group by company
once we've done that using the describe
function we can now go ahead and look at
the summary of statistics on here use
the describe function to find the
summary so by company those are groups
we're just going to describe them and
you could actually bundle those together
if you wanted and just do a Mullen one
line so here we go by company dot
describe you can see we have a nice
breakout always good to remember whether
you're using any of the packages whether
it's tab blue or pandas in python or
even r or some other package being able
to quick look and describe your data is
very important and then we go ahead and
just do a basic apply a transpose
function over the describe method to
transpose the statistics
all we've done here is flip the index
with the column names but if you're
following the numbers a lot of times
it's easier to follow across one line or
maybe you want to average out the count
or it's all kinds of different reasons
to do that
we have reached the end of this session
on full course data analytics course
should you need any assistance on PPT
project code or any other resources used
in this session please let us know in
the comment section below and our team
of experts will be more than happy to
help you as soon as possible until next
time thank you and keep learning stay
tuned for more from Simply learn
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here