1
00:00:01,020 --> 00:00:05,640
[Music]

2
00:00:14,599 --> 00:00:16,880
so hello there and welcome to another

3
00:00:16,880 --> 00:00:19,760
tutorial my name is Tim Baki and this

4
00:00:19,760 --> 00:00:21,600
time we're going to be going over how

5
00:00:21,600 --> 00:00:23,439
you can use convolutional neural

6
00:00:23,439 --> 00:00:27,160
networks with caras in Python in order

7
00:00:27,160 --> 00:00:29,960
to create an mnist digit class

8
00:00:29,960 --> 00:00:32,439
classifier and now let's get into how

9
00:00:32,439 --> 00:00:34,440
we're going to be doing that all right

10
00:00:34,440 --> 00:00:36,399
so to begin today what we're going to be

11
00:00:36,399 --> 00:00:38,680
doing is we're going to be uh first of

12
00:00:38,680 --> 00:00:40,320
all I assume that you're going to that

13
00:00:40,320 --> 00:00:43,079
you have carass and tensorflow installed

14
00:00:43,079 --> 00:00:44,800
on your system and today we're going to

15
00:00:44,800 --> 00:00:49,120
be using a 14.04 to use Python uh and

16
00:00:49,120 --> 00:00:51,559
specifically the carass library in order

17
00:00:51,559 --> 00:00:53,640
to train convolutional neural networks

18
00:00:53,640 --> 00:00:55,079
and in this case we are going to be

19
00:00:55,079 --> 00:00:57,840
training a CNN on the

20
00:00:57,840 --> 00:01:01,960
mnist data that is available online all

21
00:01:01,960 --> 00:01:05,880
right so let's begin now most mnist

22
00:01:05,880 --> 00:01:07,600
digit classifiers built with

23
00:01:07,600 --> 00:01:10,240
convolutional neural networks use uh you

24
00:01:10,240 --> 00:01:12,159
know standard architectures like uh

25
00:01:12,159 --> 00:01:14,799
layet created by Yan Lun uh there will

26
00:01:14,799 --> 00:01:16,200
be a link to that in the description who

27
00:01:16,200 --> 00:01:17,799
is actually the creator of the data as

28
00:01:17,799 --> 00:01:20,799
well uh but the only thing is though uh

29
00:01:20,799 --> 00:01:22,880
I didn't want to use one of those

30
00:01:22,880 --> 00:01:25,040
architectures I wanted to use a custom

31
00:01:25,040 --> 00:01:27,560
architecture and so I built this uh this

32
00:01:27,560 --> 00:01:29,520
very uh this uh I guess you could say

33
00:01:29,520 --> 00:01:31,680
sort of mini convolutional neural

34
00:01:31,680 --> 00:01:34,479
network that can actually be trained uh

35
00:01:34,479 --> 00:01:36,560
you can actually train this at home on

36
00:01:36,560 --> 00:01:39,520
the mnist data and now let's get into

37
00:01:39,520 --> 00:01:42,360
how this works uh so to begin just in

38
00:01:42,360 --> 00:01:44,520
case you're not familiar with the mnist

39
00:01:44,520 --> 00:01:47,479
data well here's a quick overview so the

40
00:01:47,479 --> 00:01:50,119
mnist data is actually this huge data

41
00:01:50,119 --> 00:01:52,680
set of images uh and these images

42
00:01:52,680 --> 00:01:55,240
actually represent uh digits that have

43
00:01:55,240 --> 00:01:57,960
been drawn out and handwritten by people

44
00:01:57,960 --> 00:02:00,640
now these are all 28x 28 GR black and

45
00:02:00,640 --> 00:02:03,119
white photos and so like for example

46
00:02:03,119 --> 00:02:06,119
we've got classes uh zero uh they're

47
00:02:06,119 --> 00:02:12,080
handwritten zeros ones twos threes fours

48
00:02:12,080 --> 00:02:18,319
fives six sevens eight and nines so

49
00:02:18,319 --> 00:02:20,080
we've got 10 different digits that have

50
00:02:20,080 --> 00:02:22,400
been handwritten out a lot of different

51
00:02:22,400 --> 00:02:24,920
times by people actually uh and those

52
00:02:24,920 --> 00:02:26,959
have been put into our data set

53
00:02:26,959 --> 00:02:29,519
specifically each number on average has

54
00:02:29,519 --> 00:02:31,599
been written or we have handwritten

55
00:02:31,599 --> 00:02:36,200
images of 6,000 of each so we've got

56
00:02:36,200 --> 00:02:38,280
6,000 and these will actually be for our

57
00:02:38,280 --> 00:02:39,879
training

58
00:02:39,879 --> 00:02:42,640
set but for our test set we've got an

59
00:02:42,640 --> 00:02:46,720
additional 1,000 images that we can then

60
00:02:46,720 --> 00:02:49,200
go ahead and test and validate with to

61
00:02:49,200 --> 00:02:51,200
ensure that our training is going nicely

62
00:02:51,200 --> 00:02:53,159
and get a good measure of accuracy

63
00:02:53,159 --> 00:02:55,400
without having accuracy on an overfitted

64
00:02:55,400 --> 00:02:56,879
neural

65
00:02:56,879 --> 00:02:59,239
network all right so now since we've

66
00:02:59,239 --> 00:03:01,159
gotten 10 different classes each one

67
00:03:01,159 --> 00:03:03,959
containing 6,000 train 1,000 test in

68
00:03:03,959 --> 00:03:06,519
total we've got

69
00:03:06,519 --> 00:03:10,840
60,000 different train

70
00:03:10,840 --> 00:03:13,920
images and then of course we've got

71
00:03:13,920 --> 00:03:17,120
10,000 different test

72
00:03:17,120 --> 00:03:19,360
images and that is what we're going to

73
00:03:19,360 --> 00:03:21,959
be training our convolutional neural

74
00:03:21,959 --> 00:03:24,920
network on but before we actually get

75
00:03:24,920 --> 00:03:26,799
into the code of this and how it really

76
00:03:26,799 --> 00:03:29,200
works I'd like to go over the actual

77
00:03:29,200 --> 00:03:31,720
convolution neural network architecture

78
00:03:31,720 --> 00:03:33,720
a little bit beforehand so let's

79
00:03:33,720 --> 00:03:35,360
actually draw out what this

80
00:03:35,360 --> 00:03:37,159
convolutional neural network is going to

81
00:03:37,159 --> 00:03:39,280
look like now I assume that you already

82
00:03:39,280 --> 00:03:41,000
know what convolutional neural networks

83
00:03:41,000 --> 00:03:42,799
are and the different types of layers

84
00:03:42,799 --> 00:03:44,599
they provide and the activation

85
00:03:44,599 --> 00:03:47,040
functions Etc but in case or not there

86
00:03:47,040 --> 00:03:48,760
is going to be another video coming out

87
00:03:48,760 --> 00:03:50,400
very soon describing all the different

88
00:03:50,400 --> 00:03:52,000
types of layers with convolutional

89
00:03:52,000 --> 00:03:53,599
neural networks how they work in the

90
00:03:53,599 --> 00:03:55,920
logic behind them again coming out soon

91
00:03:55,920 --> 00:03:58,159
but for now let's talk a little bit

92
00:03:58,159 --> 00:04:00,480
about what's going behind it so it

93
00:04:00,480 --> 00:04:04,879
actually begins with one convolution

94
00:04:05,239 --> 00:04:07,920
layer so we begin with this convolution

95
00:04:07,920 --> 00:04:09,720
layer and this convolution layer

96
00:04:09,720 --> 00:04:15,639
actually has 16 filters and a 5x5 filter

97
00:04:15,639 --> 00:04:18,880
size now after this convolution we are

98
00:04:18,880 --> 00:04:21,320
going to have an activation function and

99
00:04:21,320 --> 00:04:23,160
this activation function in this case is

100
00:04:23,160 --> 00:04:25,960
going to be in the rectified linear unit

101
00:04:25,960 --> 00:04:28,040
activation function again in the second

102
00:04:28,040 --> 00:04:29,800
part that's going to be coming out soon

103
00:04:29,800 --> 00:04:31,720
I will discuss why exactly Rectify

104
00:04:31,720 --> 00:04:33,400
linear units are used so much with

105
00:04:33,400 --> 00:04:35,440
convolutional neural networks and why

106
00:04:35,440 --> 00:04:37,080
you don't usually see them in other

107
00:04:37,080 --> 00:04:39,160
neural network types or regular

108
00:04:39,160 --> 00:04:40,680
activation functions that are used more

109
00:04:40,680 --> 00:04:43,800
often with convolutional neural

110
00:04:43,800 --> 00:04:45,800
networks all right so after our

111
00:04:45,800 --> 00:04:47,479
activation function we're going to make

112
00:04:47,479 --> 00:04:49,280
our image much smaller by doing

113
00:04:49,280 --> 00:04:51,840
something called Max pooling so of

114
00:04:51,840 --> 00:04:55,280
course we're going to have a Max

115
00:04:55,280 --> 00:04:58,039
pool pooling

116
00:04:58,039 --> 00:05:03,759
layer uh and so here we've got a 5x five

117
00:05:03,759 --> 00:05:06,479
size and after that we practically just

118
00:05:06,479 --> 00:05:08,800
repeat the cycle we have one more

119
00:05:08,800 --> 00:05:11,520
convolution

120
00:05:13,000 --> 00:05:16,680
layer and this now has 32 filters again

121
00:05:16,680 --> 00:05:18,199
what happens is as you go deeper in the

122
00:05:18,199 --> 00:05:20,560
neural network you usually start to see

123
00:05:20,560 --> 00:05:22,520
more and more filters coming along so we

124
00:05:22,520 --> 00:05:25,120
can learn more and more sophisticated

125
00:05:25,120 --> 00:05:26,680
different types of

126
00:05:26,680 --> 00:05:30,000
features after that again this is a 5X

127
00:05:30,000 --> 00:05:33,120
five filter size and once we've got that

128
00:05:33,120 --> 00:05:34,800
done we of course going send it through

129
00:05:34,800 --> 00:05:37,000
an activation function such as Rectify

130
00:05:37,000 --> 00:05:39,400
linear unit once more and then of course

131
00:05:39,400 --> 00:05:43,240
we do one more round of Max

132
00:05:44,440 --> 00:05:48,120
pooling once more 5x five and once that

133
00:05:48,120 --> 00:05:50,840
is done we are ready to

134
00:05:50,840 --> 00:05:52,680
flatten the

135
00:05:52,680 --> 00:05:56,199
data meaning it goes from 2D to 1D oh

136
00:05:56,199 --> 00:05:59,199
flattening

137
00:06:00,600 --> 00:06:03,199
it goes from a 2d array two dimensional

138
00:06:03,199 --> 00:06:05,120
uh two-dimensional vectors to just

139
00:06:05,120 --> 00:06:07,599
onedimensional array uh and that

140
00:06:07,599 --> 00:06:09,960
onedimensional array is then used by a

141
00:06:09,960 --> 00:06:11,840
pretty much classic feedforward

142
00:06:11,840 --> 00:06:14,160
multi-layer perceptron neural network

143
00:06:14,160 --> 00:06:16,560
which will then be able to classify what

144
00:06:16,560 --> 00:06:18,280
this image is so like now what we're

145
00:06:18,280 --> 00:06:19,960
going to do is we're going to have

146
00:06:19,960 --> 00:06:21,639
something that neural convolutional

147
00:06:21,639 --> 00:06:24,280
neural networks like to call a fully

148
00:06:24,280 --> 00:06:27,840
connected layer

149
00:06:30,160 --> 00:06:31,840
and so this fully connected layer will

150
00:06:31,840 --> 00:06:33,919
have 1,000 output

151
00:06:33,919 --> 00:06:36,240
neurons and what's going to happen is

152
00:06:36,240 --> 00:06:38,680
we're going to use the rectified linear

153
00:06:38,680 --> 00:06:42,000
unit activation function once more once

154
00:06:42,000 --> 00:06:43,759
that's done we're going to have one last

155
00:06:43,759 --> 00:06:46,759
fully connected

156
00:06:48,680 --> 00:06:51,919
layer and this will finally have just 10

157
00:06:51,919 --> 00:06:54,039
outputs and this is going to be our

158
00:06:54,039 --> 00:06:55,879
final I guess you could say probability

159
00:06:55,879 --> 00:06:58,599
Matrix in fact that is what it's called

160
00:06:58,599 --> 00:07:00,280
and that's why we're using using for

161
00:07:00,280 --> 00:07:03,199
this the soft

162
00:07:03,199 --> 00:07:06,639
Max activation function the softmax

163
00:07:06,639 --> 00:07:08,599
activation function is then going to

164
00:07:08,599 --> 00:07:11,039
basically take these 10 outputs and

165
00:07:11,039 --> 00:07:13,960
create this probability Matrix for each

166
00:07:13,960 --> 00:07:16,280
digit individually so what's going to

167
00:07:16,280 --> 00:07:18,319
happen is let's just say you feed in a

168
00:07:18,319 --> 00:07:22,960
two the 0 one2 if element of the array

169
00:07:22,960 --> 00:07:24,440
meaning technically if you were to start

170
00:07:24,440 --> 00:07:26,080
from one to third element if you were

171
00:07:26,080 --> 00:07:28,120
start from zero to Second element of the

172
00:07:28,120 --> 00:07:30,840
array at the output would have a higher

173
00:07:30,840 --> 00:07:33,080
number than the rest and that would

174
00:07:33,080 --> 00:07:35,680
indicate the probability percentage uh

175
00:07:35,680 --> 00:07:37,680
uh that the percentage of likelihood

176
00:07:37,680 --> 00:07:39,759
basically that the convolutional neural

177
00:07:39,759 --> 00:07:42,199
network believes that this digit is in

178
00:07:42,199 --> 00:07:45,280
fact this output uh and so I will

179
00:07:45,280 --> 00:07:46,759
explain a little bit more and make that

180
00:07:46,759 --> 00:07:49,120
a little bit more clear in the coding

181
00:07:49,120 --> 00:07:51,720
part but just before we go one more

182
00:07:51,720 --> 00:07:53,680
thing I'd like to mention here is that I

183
00:07:53,680 --> 00:07:55,960
was actually able to train this neural

184
00:07:55,960 --> 00:07:58,879
network and the great news is that I was

185
00:07:58,879 --> 00:08:03,720
able to aeve achieve an absolutely great

186
00:08:04,879 --> 00:08:06,560
99.66%

187
00:08:06,560 --> 00:08:09,400
accuracy so that's actually a great uh

188
00:08:09,400 --> 00:08:11,280
percentage for such a small neural

189
00:08:11,280 --> 00:08:13,360
network usually you've got much deeper

190
00:08:13,360 --> 00:08:16,199
neural networks with more filters uh and

191
00:08:16,199 --> 00:08:18,120
and of course then they're able to

192
00:08:18,120 --> 00:08:20,639
achieve such accuracy but with just four

193
00:08:20,639 --> 00:08:22,639
Epoch and only around 7 Minutes of

194
00:08:22,639 --> 00:08:25,720
training we were able to achieve

195
00:08:25,720 --> 00:08:28,199
99.66% accuracy so if we were to make

196
00:08:28,199 --> 00:08:31,120
this deeper or G Al optimize this

197
00:08:31,120 --> 00:08:32,919
architecture and if we were to train it

198
00:08:32,919 --> 00:08:34,599
for a bit longer we could of course

199
00:08:34,599 --> 00:08:36,760
cranked out some more accuracy and get

200
00:08:36,760 --> 00:08:39,399
it as accurate as possible but I found

201
00:08:39,399 --> 00:08:42,120
that this is still very very accurate

202
00:08:42,120 --> 00:08:43,959
and I was able to test it out on many

203
00:08:43,959 --> 00:08:46,399
test images when getting almost every

204
00:08:46,399 --> 00:08:48,760
single one of them correct in fact it

205
00:08:48,760 --> 00:08:50,800
believes that each each of these you

206
00:08:50,800 --> 00:08:52,560
know whenever I give it an image it

207
00:08:52,560 --> 00:08:54,839
believes so much in its answer that it

208
00:08:54,839 --> 00:08:57,200
has so many nines that it just rounds it

209
00:08:57,200 --> 00:09:01,040
to 100% confidence most of the time and

210
00:09:01,040 --> 00:09:02,200
that's what you're going to be seeing

211
00:09:02,200 --> 00:09:05,399
now how we can use this in carass and

212
00:09:05,399 --> 00:09:08,760
python on an abun 2 machine now one

213
00:09:08,760 --> 00:09:11,440
thing I'd like you to note uh is that

214
00:09:11,440 --> 00:09:14,519
this example is using Cuda for GPU

215
00:09:14,519 --> 00:09:17,399
acceleration as well as CNN for more

216
00:09:17,399 --> 00:09:19,920
further acceleration with tensorflow and

217
00:09:19,920 --> 00:09:22,120
carass uh so if you'd like to train this

218
00:09:22,120 --> 00:09:24,360
on your CPU since this isn't too much of

219
00:09:24,360 --> 00:09:26,720
an intensive neural network it would

220
00:09:26,720 --> 00:09:29,480
still work on a CPU uh or an open c

221
00:09:29,480 --> 00:09:32,600
supporting uh GPU it's just that with

222
00:09:32,600 --> 00:09:35,399
Nvidia gpus you get that really fast

223
00:09:35,399 --> 00:09:37,480
performance because of Cuda and all the

224
00:09:37,480 --> 00:09:39,600
optimization provides so I'd highly

225
00:09:39,600 --> 00:09:42,040
recommend uh doing this sort of training

226
00:09:42,040 --> 00:09:44,760
on an Nvidia GPU due to the number uh

227
00:09:44,760 --> 00:09:46,880
the massive number of like the millions

228
00:09:46,880 --> 00:09:49,720
of things that you can do at once with a

229
00:09:49,720 --> 00:09:52,279
GPU but now let's get over to the

230
00:09:52,279 --> 00:09:53,680
machine where I'm going to be showing

231
00:09:53,680 --> 00:09:55,839
you now how you can actually train this

232
00:09:55,839 --> 00:09:58,120
convolutional neural network let's get

233
00:09:58,120 --> 00:10:00,680
to it now all all right so welcome back

234
00:10:00,680 --> 00:10:02,399
to the coding part and now I'm going to

235
00:10:02,399 --> 00:10:04,040
be showing you how you can actually

236
00:10:04,040 --> 00:10:07,720
implement this mnist digit classifier uh

237
00:10:07,720 --> 00:10:10,760
using caras in Python all right so let's

238
00:10:10,760 --> 00:10:13,279
head over to an SSH session I've got

239
00:10:13,279 --> 00:10:15,079
running here as you can see I basically

240
00:10:15,079 --> 00:10:18,920
ssed into a server uh W that will allow

241
00:10:18,920 --> 00:10:21,560
me to do all of this uh deep learning

242
00:10:21,560 --> 00:10:25,880
stuff uh so now let's go ahead and begin

243
00:10:25,880 --> 00:10:27,519
uh now let's actually start off with the

244
00:10:27,519 --> 00:10:29,560
script that is going to be training all

245
00:10:29,560 --> 00:10:32,160
of training the neural network but just

246
00:10:32,160 --> 00:10:34,000
before we even get to that let's take a

247
00:10:34,000 --> 00:10:36,480
quick look at our data now if I go into

248
00:10:36,480 --> 00:10:37,800
the data and I go into the train

249
00:10:37,800 --> 00:10:39,560
directory as you can see we've got 10

250
00:10:39,560 --> 00:10:42,600
directories under that 0 through n if I

251
00:10:42,600 --> 00:10:44,440
go into three for example as you can see

252
00:10:44,440 --> 00:10:48,240
we've got a good 6,000 images that LS

253
00:10:48,240 --> 00:10:50,120
has printed out here I'm not going to go

254
00:10:50,120 --> 00:10:52,600
through all of them though uh and if we

255
00:10:52,600 --> 00:10:54,320
go back as you can see if we go to

256
00:10:54,320 --> 00:10:57,279
something like eight we've got uh we've

257
00:10:57,279 --> 00:10:59,320
got similarly 6,000 images for eight as

258
00:10:59,320 --> 00:11:01,639
as well uh then we can go back and

259
00:11:01,639 --> 00:11:02,959
really do that for whichever number we

260
00:11:02,959 --> 00:11:04,720
want but then again we should we can go

261
00:11:04,720 --> 00:11:06,800
over to our validation set or our

262
00:11:06,800 --> 00:11:09,600
testing Set uh and again here as well

263
00:11:09,600 --> 00:11:11,760
we've got 10 directories uh and we can

264
00:11:11,760 --> 00:11:14,040
go into one of them like six and here

265
00:11:14,040 --> 00:11:15,720
we've got only a thousand of those

266
00:11:15,720 --> 00:11:17,399
images but still more than enough to

267
00:11:17,399 --> 00:11:20,680
test with for now uh and so from here we

268
00:11:20,680 --> 00:11:22,600
should be able to actually train our

269
00:11:22,600 --> 00:11:24,440
convolutional neural network and then

270
00:11:24,440 --> 00:11:26,959
actually use it using these validation

271
00:11:26,959 --> 00:11:30,760
images now if we go back so as you can

272
00:11:30,760 --> 00:11:34,399
see if I Nano this python file this is

273
00:11:34,399 --> 00:11:36,000
the file that's actually going to be

274
00:11:36,000 --> 00:11:38,440
helping us train our convolutional

275
00:11:38,440 --> 00:11:40,639
neural network now we've only got a few

276
00:11:40,639 --> 00:11:42,320
inputs here we're importing the image

277
00:11:42,320 --> 00:11:44,120
data generated from carass the

278
00:11:44,120 --> 00:11:47,120
sequential model uh the convolution 2D

279
00:11:47,120 --> 00:11:50,000
and Max pooling 2D layers from carass

280
00:11:50,000 --> 00:11:51,839
and of course the flatten and dense

281
00:11:51,839 --> 00:11:54,680
layers from carass as well once the

282
00:11:54,680 --> 00:11:56,480
Imports are done we are then setting our

283
00:11:56,480 --> 00:11:58,160
image width and height telling it where

284
00:11:58,160 --> 00:12:00,000
to find the train d data and where to

285
00:12:00,000 --> 00:12:02,399
find the validation data and finally how

286
00:12:02,399 --> 00:12:04,000
many training samples to use and how

287
00:12:04,000 --> 00:12:06,920
many validation samples to use after

288
00:12:06,920 --> 00:12:08,120
that I tell it that we're going to be

289
00:12:08,120 --> 00:12:10,839
training for four APO and once I tell it

290
00:12:10,839 --> 00:12:13,199
that we start actually defining our

291
00:12:13,199 --> 00:12:16,079
model now the model is in this case a

292
00:12:16,079 --> 00:12:18,079
sequential model and what we're going to

293
00:12:18,079 --> 00:12:19,880
do is we're going to add our first layer

294
00:12:19,880 --> 00:12:22,320
which is a convolution 2d layer with 16

295
00:12:22,320 --> 00:12:25,320
filters of 555 filter size a Rectify

296
00:12:25,320 --> 00:12:27,839
linear unit activation function and a

297
00:12:27,839 --> 00:12:30,360
very very very simple input shape which

298
00:12:30,360 --> 00:12:32,160
is the image width the image height and

299
00:12:32,160 --> 00:12:33,120
three

300
00:12:33,120 --> 00:12:36,160
channels once that is done we have a 2x2

301
00:12:36,160 --> 00:12:39,079
Max pooling 2D layer and after that

302
00:12:39,079 --> 00:12:42,160
we've got another 5x5 filter size 32

303
00:12:42,160 --> 00:12:44,959
filter convolution 2D layer once again

304
00:12:44,959 --> 00:12:46,880
with the rectified linear unit

305
00:12:46,880 --> 00:12:48,920
activation function which is Then

306
00:12:48,920 --> 00:12:51,720
followed up by a 2X two Max pooling 2D

307
00:12:51,720 --> 00:12:52,920
and that's really all there is to the

308
00:12:52,920 --> 00:12:54,959
convolutional part of the network then

309
00:12:54,959 --> 00:12:57,120
we've got our you know regular uh fully

310
00:12:57,120 --> 00:13:00,120
connected or in this case dense layers

311
00:13:00,120 --> 00:13:01,399
uh and so basically first of all we're

312
00:13:01,399 --> 00:13:02,920
adding a flattening layer which will

313
00:13:02,920 --> 00:13:04,519
bring it from two Dimensions to one

314
00:13:04,519 --> 00:13:06,600
dimension and once we've gotten that

315
00:13:06,600 --> 00:13:09,560
data in one dimension we are ready to

316
00:13:09,560 --> 00:13:11,160
actually use our fully connected layers

317
00:13:11,160 --> 00:13:13,560
or again in this case our dense layers

318
00:13:13,560 --> 00:13:15,240
now our dense layers will really take

319
00:13:15,240 --> 00:13:17,399
whatever input size has been given to

320
00:13:17,399 --> 00:13:19,720
them uh and they will output though a

321
00:13:19,720 --> 00:13:22,480
thousand neurons and again uh Rectify

322
00:13:22,480 --> 00:13:25,199
linear unit activation function will of

323
00:13:25,199 --> 00:13:28,040
course activate all of that data for us

324
00:13:28,040 --> 00:13:30,639
and then finally once we've got all of

325
00:13:30,639 --> 00:13:33,160
that data we can then converge down onto

326
00:13:33,160 --> 00:13:36,120
10 different classes with the softmax

327
00:13:36,120 --> 00:13:37,959
activation function order to get that

328
00:13:37,959 --> 00:13:40,279
really great probability Matrix which

329
00:13:40,279 --> 00:13:42,480
will give us a percentage of confidence

330
00:13:42,480 --> 00:13:45,079
with every classification it

331
00:13:45,079 --> 00:13:48,240
creates and once it is done we then

332
00:13:48,240 --> 00:13:50,759
print out a model summary uh which will

333
00:13:50,759 --> 00:13:52,639
basically just show us what the model

334
00:13:52,639 --> 00:13:54,720
looks like so we're able to confirm that

335
00:13:54,720 --> 00:13:57,160
this is exactly what we want uh we don't

336
00:13:57,160 --> 00:13:59,160
really use it for much though uh we just

337
00:13:59,160 --> 00:14:01,120
see uh we just use it to see what our

338
00:14:01,120 --> 00:14:02,560
model looks

339
00:14:02,560 --> 00:14:05,800
like after that then we compile this

340
00:14:05,800 --> 00:14:07,880
model and basically what this is doing

341
00:14:07,880 --> 00:14:10,240
is it's getting it ready for training by

342
00:14:10,240 --> 00:14:12,279
giving it some parameters for example

343
00:14:12,279 --> 00:14:14,279
for loss we're using the binary cross

344
00:14:14,279 --> 00:14:16,000
entripy algorithm we are using the

345
00:14:16,000 --> 00:14:19,120
optimizer RMS prop and for the metrics

346
00:14:19,120 --> 00:14:20,839
we are keeping

347
00:14:20,839 --> 00:14:23,480
accuracy but then after we have compiled

348
00:14:23,480 --> 00:14:25,480
our model we're creating a data

349
00:14:25,480 --> 00:14:27,880
generator for the training Set uh and

350
00:14:27,880 --> 00:14:29,440
this is where we do a little bit of data

351
00:14:29,440 --> 00:14:31,320
augmentation we tell it how much we can

352
00:14:31,320 --> 00:14:33,320
rescale our image how much to do the

353
00:14:33,320 --> 00:14:35,360
sheer range with and how much to do the

354
00:14:35,360 --> 00:14:37,440
zoom range with uh of course we're

355
00:14:37,440 --> 00:14:39,199
telling it that it can do horizontal

356
00:14:39,199 --> 00:14:41,079
flips that's why we're giving it true

357
00:14:41,079 --> 00:14:43,160
but with the testing data generator all

358
00:14:43,160 --> 00:14:44,360
we're doing is we're telling it that it

359
00:14:44,360 --> 00:14:46,880
can rescale uh again we're giving it how

360
00:14:46,880 --> 00:14:49,279
much we believe it should be

361
00:14:49,279 --> 00:14:52,000
rescaling once that is done though we

362
00:14:52,000 --> 00:14:55,160
are creating a train generator uh and in

363
00:14:55,160 --> 00:14:57,360
this generator we generator we are

364
00:14:57,360 --> 00:14:59,120
basically going to be passing it the

365
00:14:59,120 --> 00:15:01,759
data directory uh we're going to be

366
00:15:01,759 --> 00:15:04,199
passing it the target size how how big

367
00:15:04,199 --> 00:15:06,000
these images are going to be we're going

368
00:15:06,000 --> 00:15:09,000
to patch it pass it a batch size for the

369
00:15:09,000 --> 00:15:10,800
training and we're going to tell it that

370
00:15:10,800 --> 00:15:12,759
we're not using binary class mode we're

371
00:15:12,759 --> 00:15:14,240
not using any of that we're using the

372
00:15:14,240 --> 00:15:17,320
categorical class mode in order to allow

373
00:15:17,320 --> 00:15:19,079
us to have all of these different

374
00:15:19,079 --> 00:15:21,959
classes from 0 to 9 we're doing

375
00:15:21,959 --> 00:15:23,440
practically the exact same thing with

376
00:15:23,440 --> 00:15:25,480
the validation generator and once we've

377
00:15:25,480 --> 00:15:27,839
got all of those generators down then we

378
00:15:27,839 --> 00:15:29,560
can actually tell our model to fit

379
00:15:29,560 --> 00:15:31,959
towards a generator we then pass it our

380
00:15:31,959 --> 00:15:33,759
train generator tell it how many samples

381
00:15:33,759 --> 00:15:35,680
we're going to be using per aoch we're

382
00:15:35,680 --> 00:15:37,720
going to be telling it uh which EPO to

383
00:15:37,720 --> 00:15:39,959
use here uh we're going to be giving it

384
00:15:39,959 --> 00:15:43,440
the validation generator as well and

385
00:15:43,440 --> 00:15:45,480
then finally we're going to be giving it

386
00:15:45,480 --> 00:15:47,519
the validation samples as well to give

387
00:15:47,519 --> 00:15:49,600
it all the parameters that it needs in

388
00:15:49,600 --> 00:15:53,240
order to actually run the training once

389
00:15:53,240 --> 00:15:55,000
it is done the training goes this is a

390
00:15:55,000 --> 00:15:57,040
synchronous operation so once it's done

391
00:15:57,040 --> 00:15:59,199
it will save its weights to a five

392
00:15:59,199 --> 00:16:02,120
called mnist neuron net.

393
00:16:02,120 --> 00:16:04,600
H5 now what we can do is we can get out

394
00:16:04,600 --> 00:16:07,519
of Nano and actually run this custom

395
00:16:07,519 --> 00:16:10,759
train script using python as you can see

396
00:16:10,759 --> 00:16:12,279
uh what's happening now is it is

397
00:16:12,279 --> 00:16:13,920
training our neural network and that

398
00:16:13,920 --> 00:16:16,560
accuracy is going up uh so what's

399
00:16:16,560 --> 00:16:18,279
happening here is uh it gives us our

400
00:16:18,279 --> 00:16:20,560
estimated time of arrival and this will

401
00:16:20,560 --> 00:16:22,639
basically allow us to train our neural

402
00:16:22,639 --> 00:16:23,880
network and remember this is only going

403
00:16:23,880 --> 00:16:25,759
to do four of these sorts of

404
00:16:25,759 --> 00:16:28,959
iterations uh and so of course uh you

405
00:16:28,959 --> 00:16:31,399
want to do as many as you can uh then

406
00:16:31,399 --> 00:16:32,720
again you don't want to overfit so it

407
00:16:32,720 --> 00:16:34,480
really depends on how deep your neural

408
00:16:34,480 --> 00:16:36,800
network is how much data you have uh how

409
00:16:36,800 --> 00:16:39,360
and and what sort of the ratio of split

410
00:16:39,360 --> 00:16:41,560
is between uh your training and testing

411
00:16:41,560 --> 00:16:43,079
set it depends on a lot of different

412
00:16:43,079 --> 00:16:45,680
parameters but again uh you always want

413
00:16:45,680 --> 00:16:47,519
to pick and choose you always want to

414
00:16:47,519 --> 00:16:50,000
experiment until you find what works

415
00:16:50,000 --> 00:16:52,959
best for you and your specific neural

416
00:16:52,959 --> 00:16:55,120
network all right so since I've already

417
00:16:55,120 --> 00:16:56,639
got this neural network trained and it

418
00:16:56,639 --> 00:16:59,040
does take a little bit of time uh again

419
00:16:59,040 --> 00:17:01,399
because it's not this this data set

420
00:17:01,399 --> 00:17:03,480
isn't massive but it's not small either

421
00:17:03,480 --> 00:17:06,400
so it wouldn't be very quick uh so this

422
00:17:06,400 --> 00:17:07,799
is why I always recommend training on a

423
00:17:07,799 --> 00:17:10,120
GPU instead of uh especially with

424
00:17:10,120 --> 00:17:12,439
convolutional and deep neural networks

425
00:17:12,439 --> 00:17:14,360
uh but now what I'm going to do is I'm

426
00:17:14,360 --> 00:17:17,000
going to stop this training script and

427
00:17:17,000 --> 00:17:18,480
now that that's done let's just pretend

428
00:17:18,480 --> 00:17:20,679
you know it's trained and we've got our

429
00:17:20,679 --> 00:17:23,559
neural network file over here now we're

430
00:17:23,559 --> 00:17:25,280
ready to actually use this neural

431
00:17:25,280 --> 00:17:28,240
network and run predictions against it

432
00:17:28,240 --> 00:17:30,120
using our images and the way we're going

433
00:17:30,120 --> 00:17:31,799
to be doing this is through another file

434
00:17:31,799 --> 00:17:33,679
called predict.

435
00:17:33,679 --> 00:17:36,520
py now inside of predict. py we are

436
00:17:36,520 --> 00:17:38,799
importing numpy we are importing open

437
00:17:38,799 --> 00:17:41,360
CV2 we're importing all the same carass

438
00:17:41,360 --> 00:17:43,360
modules that you were just seeing and of

439
00:17:43,360 --> 00:17:45,480
course we're importing the CIS or system

440
00:17:45,480 --> 00:17:48,160
module I'll tell you why in just a

441
00:17:48,160 --> 00:17:50,679
moment then of course we are setting the

442
00:17:50,679 --> 00:17:53,080
image width and height to 28 and 28 once

443
00:17:53,080 --> 00:17:56,000
more just as we do for all of our mnist

444
00:17:56,000 --> 00:17:58,159
amnest data because mnist images are

445
00:17:58,159 --> 00:18:01,960
always 20 8 x 28 pixels wide and high

446
00:18:01,960 --> 00:18:03,520
then what we do is we create a function

447
00:18:03,520 --> 00:18:05,919
called create model we Define this

448
00:18:05,919 --> 00:18:07,760
function and what we're going to do is

449
00:18:07,760 --> 00:18:09,360
inside of this function we are going to

450
00:18:09,360 --> 00:18:11,360
create a new variable called model which

451
00:18:11,360 --> 00:18:13,400
is equal to a new sequential model and

452
00:18:13,400 --> 00:18:15,120
then create the exact same model that we

453
00:18:15,120 --> 00:18:17,120
did in the training script except that

454
00:18:17,120 --> 00:18:19,080
we're not going to tell it which weights

455
00:18:19,080 --> 00:18:21,159
to use just yet we're going to print out

456
00:18:21,159 --> 00:18:23,240
another summary and we're going to turn

457
00:18:23,240 --> 00:18:25,919
this model then what we will do is we

458
00:18:25,919 --> 00:18:27,679
will actually open up an image using

459
00:18:27,679 --> 00:18:30,480
open CV we will do image is equal to CV2

460
00:18:30,480 --> 00:18:32,720
IM read meaning image read and what

461
00:18:32,720 --> 00:18:34,400
we're going to do is instead of passing

462
00:18:34,400 --> 00:18:37,480
a direct file pass we'll be uh sending

463
00:18:37,480 --> 00:18:40,320
to it the first argument sent to the

464
00:18:40,320 --> 00:18:42,919
python script uh when when you were

465
00:18:42,919 --> 00:18:45,000
running the script and basically in the

466
00:18:45,000 --> 00:18:46,760
first parameter you are going to put a

467
00:18:46,760 --> 00:18:48,760
file path to an image that you would

468
00:18:48,760 --> 00:18:50,720
like to predict using this

469
00:18:50,720 --> 00:18:52,760
classifier once you pass that then it

470
00:18:52,760 --> 00:18:55,120
will be opened and CB2 will then resize

471
00:18:55,120 --> 00:18:56,039
it to

472
00:18:56,039 --> 00:18:58,960
2828 uh this is not required because all

473
00:18:58,960 --> 00:19:02,600
this digits are 28x 28 by default but if

474
00:19:02,600 --> 00:19:04,720
you would like to repurpose this to work

475
00:19:04,720 --> 00:19:07,120
with your own model or to work with uh

476
00:19:07,120 --> 00:19:09,240
you know your own data then it it's

477
00:19:09,240 --> 00:19:10,840
really easy to have this because then

478
00:19:10,840 --> 00:19:12,600
you just set your uh image width and

479
00:19:12,600 --> 00:19:14,480
height up here and it'll automatically

480
00:19:14,480 --> 00:19:16,799
change everywhere else to ensure that

481
00:19:16,799 --> 00:19:20,559
you are having uh your model work

482
00:19:20,559 --> 00:19:23,120
correctly all right but after that we

483
00:19:23,120 --> 00:19:25,320
are then crawling that create model

484
00:19:25,320 --> 00:19:27,320
function that we have just created and

485
00:19:27,320 --> 00:19:29,640
once we've got a model return from there

486
00:19:29,640 --> 00:19:32,080
we tell that model to load its weights

487
00:19:32,080 --> 00:19:34,919
from a file in current directory called

488
00:19:34,919 --> 00:19:39,360
mnist neuronet H5 and once we're done

489
00:19:39,360 --> 00:19:41,440
that we are then ready to convert the

490
00:19:41,440 --> 00:19:43,720
image that we just imported using open

491
00:19:43,720 --> 00:19:47,120
CV2 and convert it to a numpy array and

492
00:19:47,120 --> 00:19:49,559
once we convert that to a numpy array we

493
00:19:49,559 --> 00:19:52,120
then reshape it to fit the image with

494
00:19:52,120 --> 00:19:54,440
height and then finally the dimensions

495
00:19:54,440 --> 00:19:56,559
or not Dimensions the channels in the

496
00:19:56,559 --> 00:19:58,320
image which in this case we are setting

497
00:19:58,320 --> 00:20:00,120
to three because our neural network was

498
00:20:00,120 --> 00:20:02,320
trained on cheap three channels and with

499
00:20:02,320 --> 00:20:04,480
three channels you can always use RGB

500
00:20:04,480 --> 00:20:07,080
images as well as black and white they

501
00:20:07,080 --> 00:20:09,600
both work although technically since

502
00:20:09,600 --> 00:20:11,760
we're us using black and white images

503
00:20:11,760 --> 00:20:13,400
just one channel would work for the

504
00:20:13,400 --> 00:20:15,200
training and the testing and the you

505
00:20:15,200 --> 00:20:17,280
know predictions but the thing is if you

506
00:20:17,280 --> 00:20:19,039
again if you want to repurpose this

507
00:20:19,039 --> 00:20:20,880
immediately just get it working on your

508
00:20:20,880 --> 00:20:23,600
own data which is RGB then having three

509
00:20:23,600 --> 00:20:26,480
channels already there is really

510
00:20:26,480 --> 00:20:28,919
convenient all right after that we just

511
00:20:28,919 --> 00:20:31,039
add one more Dimension to make this a

512
00:20:31,039 --> 00:20:34,159
fourth rank tensor uh with our numpy

513
00:20:34,159 --> 00:20:37,159
array and then finally we tell our model

514
00:20:37,159 --> 00:20:39,880
to predict what it believes uh this

515
00:20:39,880 --> 00:20:42,400
image is by giving it the numpy array

516
00:20:42,400 --> 00:20:44,720
and getting the zeroe element from it

517
00:20:44,720 --> 00:20:46,400
now remember this is a two-dimensional

518
00:20:46,400 --> 00:20:49,159
array that numpy is giving us like for

519
00:20:49,159 --> 00:20:52,200
example uh if the if we gave it an image

520
00:20:52,200 --> 00:20:55,000
of a one this would be our array it

521
00:20:55,000 --> 00:21:02,159
would be uh 0 1 0 0 0 0 0 0 0 0 uh and

522
00:21:02,159 --> 00:21:03,799
so what's happening here is this is

523
00:21:03,799 --> 00:21:06,080
before the prediction so what happens is

524
00:21:06,080 --> 00:21:08,200
this is a two-dimensional array but we

525
00:21:08,200 --> 00:21:10,320
get this first element which is this

526
00:21:10,320 --> 00:21:12,799
first array which in turn has 10

527
00:21:12,799 --> 00:21:15,679
elements inside of it uh and so what

528
00:21:15,679 --> 00:21:18,039
happens is we are able to get that zeroe

529
00:21:18,039 --> 00:21:20,320
element and then what happens we have

530
00:21:20,320 --> 00:21:22,640
some logic to see which one has the

531
00:21:22,640 --> 00:21:25,120
highest probability and so in order to

532
00:21:25,120 --> 00:21:26,840
do this we find the best class and best

533
00:21:26,840 --> 00:21:28,200
confidence then we just do a really

534
00:21:28,200 --> 00:21:31,320
quick script here which will try and see

535
00:21:31,320 --> 00:21:34,360
which prediction is the greatest once it

536
00:21:34,360 --> 00:21:36,159
is done that though then it will print

537
00:21:36,159 --> 00:21:38,760
out I think this digit is a then the

538
00:21:38,760 --> 00:21:41,440
best class with and then the percentage

539
00:21:41,440 --> 00:21:45,039
of confidence and and then confidence so

540
00:21:45,039 --> 00:21:46,960
what this is going to do is if I just

541
00:21:46,960 --> 00:21:50,679
exit out of this script here not exactly

542
00:21:50,679 --> 00:21:53,799
sure what I changed uh but all right

543
00:21:53,799 --> 00:21:56,880
let's um yeah let's go uh and then what

544
00:21:56,880 --> 00:21:59,320
happens is basically we can run this

545
00:21:59,320 --> 00:22:02,200
python script python predict and then we

546
00:22:02,200 --> 00:22:04,720
can give it images that we want it to

547
00:22:04,720 --> 00:22:07,159
test on now the only thing is though

548
00:22:07,159 --> 00:22:08,279
we're not going to be passing it

549
00:22:08,279 --> 00:22:10,000
training images because those images

550
00:22:10,000 --> 00:22:12,200
have already been well trained on we're

551
00:22:12,200 --> 00:22:13,919
going to be giving images from the

552
00:22:13,919 --> 00:22:16,080
validation set so that we are able to

553
00:22:16,080 --> 00:22:18,400
give images that it's never seen before

554
00:22:18,400 --> 00:22:20,559
therefore testing its true

555
00:22:20,559 --> 00:22:22,360
capability the way we're going to do

556
00:22:22,360 --> 00:22:24,640
this is by going to the data directory

557
00:22:24,640 --> 00:22:26,120
and inside of that we're going to go to

558
00:22:26,120 --> 00:22:27,919
the validation directory now these are

559
00:22:27,919 --> 00:22:30,279
all images that it has never seen before

560
00:22:30,279 --> 00:22:31,840
and let's try out zero first we're going

561
00:22:31,840 --> 00:22:34,320
to try out all0 to nine and see how well

562
00:22:34,320 --> 00:22:37,159
it does now for the zeroth we can give

563
00:22:37,159 --> 00:22:39,720
it something like 71.png again just

564
00:22:39,720 --> 00:22:41,679
random number just throwing that out

565
00:22:41,679 --> 00:22:44,159
there uh click in or out and as you can

566
00:22:44,159 --> 00:22:46,960
see it says I think this digit is a zero

567
00:22:46,960 --> 00:22:50,440
with 100% confidence again that is

568
00:22:50,440 --> 00:22:52,600
absolutely great but let's just see if

569
00:22:52,600 --> 00:22:55,520
it can it can classify a one with a good

570
00:22:55,520 --> 00:23:00,400
amount of accuracy let's just say 7 14

571
00:23:00,400 --> 00:23:03,400
PNG all right so as you can see this has

572
00:23:03,400 --> 00:23:06,159
correctly classified this digit as a one

573
00:23:06,159 --> 00:23:09,640
with a 100% confidence and so we are on

574
00:23:09,640 --> 00:23:11,400
the right track so far we're getting

575
00:23:11,400 --> 00:23:13,600
great accuracy let's hope we can keep

576
00:23:13,600 --> 00:23:16,200
that up so now again we're going to try

577
00:23:16,200 --> 00:23:17,640
this out with a two and let's just say

578
00:23:17,640 --> 00:23:20,400
two uh

579
00:23:20,400 --> 00:23:22,400
390

580
00:23:22,400 --> 00:23:24,400
PNG all right so as you can see it's

581
00:23:24,400 --> 00:23:27,000
using our GPU to to do this here and as

582
00:23:27,000 --> 00:23:28,640
you can see it says this did I think

583
00:23:28,640 --> 00:23:32,640
this digit is a two with 100% confidence

584
00:23:32,640 --> 00:23:36,120
which again is the correct answer now

585
00:23:36,120 --> 00:23:37,840
let's try out what happens if we give it

586
00:23:37,840 --> 00:23:39,799
something like a three so let's give it

587
00:23:39,799 --> 00:23:42,039
a three this time let's just say you

588
00:23:42,039 --> 00:23:46,279
know 5,000 Happ andry let's see what it

589
00:23:46,279 --> 00:23:49,279
says as you can see it says that I think

590
00:23:49,279 --> 00:23:52,320
this digit is a three with 100%

591
00:23:52,320 --> 00:23:55,480
confidence again this is so close to

592
00:23:55,480 --> 00:23:58,080
100% that it's just rounding up to 100%

593
00:23:58,080 --> 00:24:00,039
due to the fact that it is so sure that

594
00:24:00,039 --> 00:24:01,799
it's a three it's seen so many patterns

595
00:24:01,799 --> 00:24:03,559
in it that make it believe that it's a

596
00:24:03,559 --> 00:24:06,080
three that it's just not even going to

597
00:24:06,080 --> 00:24:08,760
give other classes of

598
00:24:08,760 --> 00:24:11,120
probability uh and then what happens is

599
00:24:11,120 --> 00:24:12,880
again we can run this let's just try

600
00:24:12,880 --> 00:24:16,640
four now uh for four we can use 5,000

601
00:24:16,640 --> 00:24:18,279
four

602
00:24:18,279 --> 00:24:21,120
um and as you can see it says I think

603
00:24:21,120 --> 00:24:25,039
this is a four with a 100% confidence

604
00:24:25,039 --> 00:24:28,080
again absolutely great uh we can then go

605
00:24:28,080 --> 00:24:31,679
ahead and continue to save five uh where

606
00:24:31,679 --> 00:24:34,440
we can do let's do

607
00:24:34,440 --> 00:24:38,720
8,360 PNG or

608
00:24:38,919 --> 00:24:41,799
60.png all right so here we should be

609
00:24:41,799 --> 00:24:44,799
able to see that oh um apparently that

610
00:24:44,799 --> 00:24:47,840
image does not exist let's see yeah

611
00:24:47,840 --> 00:24:52,080
sometimes oh 66 Works let's see all

612
00:24:52,080 --> 00:24:55,520
right so I think this is a five with

613
00:24:55,520 --> 00:24:58,559
100% confidence again absolutely great

614
00:24:58,559 --> 00:25:01,120
that is the correct answer with a very

615
00:25:01,120 --> 00:25:03,559
good amount of confidence as well uh we

616
00:25:03,559 --> 00:25:06,760
can go to six then we can say

617
00:25:06,760 --> 00:25:10,880
,000 148 maybe uh let's see if this

618
00:25:10,880 --> 00:25:13,000
exists yeah it exists and as you can see

619
00:25:13,000 --> 00:25:15,480
I believe this digit is a six with 100%

620
00:25:15,480 --> 00:25:18,000
confidence once more we can then go

621
00:25:18,000 --> 00:25:20,960
ahead and give it a seven and let's just

622
00:25:20,960 --> 00:25:24,240
say our seven is 3,000 this time um 50

623
00:25:24,240 --> 00:25:25,200
and

624
00:25:25,200 --> 00:25:27,640
four uh hopefully that image exists and

625
00:25:27,640 --> 00:25:29,600
it does and it's you can see it says

626
00:25:29,600 --> 00:25:32,200
that seven is the correct answer with

627
00:25:32,200 --> 00:25:33,679
100%

628
00:25:33,679 --> 00:25:36,320
confidence now once we're done seven we

629
00:25:36,320 --> 00:25:38,279
finally have just two characters left to

630
00:25:38,279 --> 00:25:40,200
test our second last character is eight

631
00:25:40,200 --> 00:25:41,679
and let's try

632
00:25:41,679 --> 00:25:45,080
6024 PNG why

633
00:25:45,080 --> 00:25:47,559
not all right so as you can see again it

634
00:25:47,559 --> 00:25:49,840
has classified the digit correctly with

635
00:25:49,840 --> 00:25:52,679
100% confidence uh and now we can go

636
00:25:52,679 --> 00:25:54,960
ahead and give it something like nine

637
00:25:54,960 --> 00:25:56,279
and let's just change this up a bit

638
00:25:56,279 --> 00:25:59,919
let's just say something like um d29 all

639
00:25:59,919 --> 00:26:03,799
right uh and oh once more it says that

640
00:26:03,799 --> 00:26:06,360
this is the correct it says that this

641
00:26:06,360 --> 00:26:08,720
digit is nine with 100% confidence and

642
00:26:08,720 --> 00:26:12,399
that is once more the correct answer and

643
00:26:12,399 --> 00:26:14,919
the and our users uh and then again I

644
00:26:14,919 --> 00:26:17,559
mean users could uh have this built into

645
00:26:17,559 --> 00:26:18,399
their

646
00:26:18,399 --> 00:26:20,399
applications uh because like let's just

647
00:26:20,399 --> 00:26:22,919
say you're creating uh a no JS

648
00:26:22,919 --> 00:26:25,279
application or a website you can use

649
00:26:25,279 --> 00:26:27,360
libraries that allow you to use carass

650
00:26:27,360 --> 00:26:30,039
models uh in those languages pre-trained

651
00:26:30,039 --> 00:26:32,320
crass models in those languages uh and

652
00:26:32,320 --> 00:26:33,919
you can have this convolutional neural

653
00:26:33,919 --> 00:26:36,080
network run in browsers run on phones

654
00:26:36,080 --> 00:26:39,080
run on uh a desktop applications really

655
00:26:39,080 --> 00:26:41,880
anywhere where a language that supports

656
00:26:41,880 --> 00:26:43,960
the device also has a library that

657
00:26:43,960 --> 00:26:46,080
supports pre-trained carass models as

658
00:26:46,080 --> 00:26:48,640
long as those prerequisites are met you

659
00:26:48,640 --> 00:26:50,799
should technically be able to use this

660
00:26:50,799 --> 00:26:53,279
model anywhere and again in the worst

661
00:26:53,279 --> 00:26:55,080
case scenario if you're unable to do

662
00:26:55,080 --> 00:26:57,520
that then you can always just create uh

663
00:26:57,520 --> 00:26:59,919
a web AP Pi where you can send an image

664
00:26:59,919 --> 00:27:01,640
it would send it back to the

665
00:27:01,640 --> 00:27:02,919
convolutional neural network and the

666
00:27:02,919 --> 00:27:04,360
convolutional neural network would then

667
00:27:04,360 --> 00:27:07,360
go ahead and further return uh the

668
00:27:07,360 --> 00:27:09,279
answer and then that would be replied to

669
00:27:09,279 --> 00:27:12,559
you by the rest API once more again

670
00:27:12,559 --> 00:27:14,799
though in part two there's going to be a

671
00:27:14,799 --> 00:27:17,840
lot more coming out in those terms uh

672
00:27:17,840 --> 00:27:18,919
and so in part two you're going to be

673
00:27:18,919 --> 00:27:20,880
seeing a lot of how you can use these

674
00:27:20,880 --> 00:27:23,279
pre-trained models in many different

675
00:27:23,279 --> 00:27:26,320
languages so that's all for this

676
00:27:26,320 --> 00:27:28,760
tutorial I really do hope you enjoyed

677
00:27:28,760 --> 00:27:30,360
and you're able to learn how exactly you

678
00:27:30,360 --> 00:27:32,120
can use carass in Python with a

679
00:27:32,120 --> 00:27:35,159
tensorflow back end uh and of course use

680
00:27:35,159 --> 00:27:37,960
it to train on your own data really and

681
00:27:37,960 --> 00:27:39,159
that's the best part we're not using

682
00:27:39,159 --> 00:27:41,760
pre-built data sets we're not using like

683
00:27:41,760 --> 00:27:43,519
uh cryptic data sets that are in random

684
00:27:43,519 --> 00:27:46,320
formats like lmdb we're using actual

685
00:27:46,320 --> 00:27:48,360
image data which sometimes I guess you

686
00:27:48,360 --> 00:27:49,840
could say isn't the best per for

687
00:27:49,840 --> 00:27:51,440
performance but then again if you just

688
00:27:51,440 --> 00:27:52,880
want to get a convolutional neural

689
00:27:52,880 --> 00:27:54,960
network running quickly and running fast

690
00:27:54,960 --> 00:27:57,320
and running nicely the then of course

691
00:27:57,320 --> 00:28:00,440
images are always another option as well

692
00:28:00,440 --> 00:28:01,799
all right so I really do hope you

693
00:28:01,799 --> 00:28:03,519
enjoyed thank you very much for watching

694
00:28:03,519 --> 00:28:05,399
today that's going to be all for the

695
00:28:05,399 --> 00:28:07,720
video and I really do hope you were able

696
00:28:07,720 --> 00:28:09,519
to learn from it again if you did please

697
00:28:09,519 --> 00:28:11,159
do make sure to leave a like down below

698
00:28:11,159 --> 00:28:12,320
and of course share the video If you

699
00:28:12,320 --> 00:28:13,559
believe it could help anybody else so

700
00:28:13,559 --> 00:28:15,799
you know like your friends or family uh

701
00:28:15,799 --> 00:28:17,440
and of course if you have any questions

702
00:28:17,440 --> 00:28:19,360
or suggestions or feedback please do

703
00:28:19,360 --> 00:28:20,399
make sure to leave it down in the

704
00:28:20,399 --> 00:28:22,240
comment section below email to me at

705
00:28:22,240 --> 00:28:24,760
tajim Manny gmail.com or tweet it to me

706
00:28:24,760 --> 00:28:26,399
at tajim Manny again all contact

707
00:28:26,399 --> 00:28:27,360
information will be down in the

708
00:28:27,360 --> 00:28:29,360
description as well as has all the code

709
00:28:29,360 --> 00:28:32,320
for this video all right so if you

710
00:28:32,320 --> 00:28:33,640
really like my content though and you

711
00:28:33,640 --> 00:28:34,840
want to see a lot more of it please do

712
00:28:34,840 --> 00:28:36,159
make sure to subscribe to my YouTube

713
00:28:36,159 --> 00:28:37,840
channel as well as I really as it really

714
00:28:37,840 --> 00:28:40,200
does help out a lot and of course

715
00:28:40,200 --> 00:28:42,159
finally if you want to be notified

716
00:28:42,159 --> 00:28:43,640
through Google notification and email

717
00:28:43,640 --> 00:28:45,600
whenever I release new content please do

718
00:28:45,600 --> 00:28:48,039
make sure to click the not the Bell icon

719
00:28:48,039 --> 00:28:49,880
beside the Subscribe Button as well in

720
00:28:49,880 --> 00:28:52,120
order to be notified thank you very much

721
00:28:52,120 --> 00:28:53,559
for watching today that's going to be

722
00:28:53,559 --> 00:28:57,200
all goodbye
