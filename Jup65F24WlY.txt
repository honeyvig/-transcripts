1
00:00:00,240 --> 00:00:02,560
so hello there my name is tammy bakshi

2
00:00:02,560 --> 00:00:03,360
and today

3
00:00:03,360 --> 00:00:05,040
we're going to be talking about why

4
00:00:05,040 --> 00:00:06,799
exactly bert performs

5
00:00:06,799 --> 00:00:09,679
better on downstream tasks if it's

6
00:00:09,679 --> 00:00:10,400
trained

7
00:00:10,400 --> 00:00:12,960
twice in the pre-training stage once on

8
00:00:12,960 --> 00:00:15,200
a very large corpus of text data

9
00:00:15,200 --> 00:00:17,760
and then once on domain specific text

10
00:00:17,760 --> 00:00:18,400
data

11
00:00:18,400 --> 00:00:20,400
now one thing i do want to say before we

12
00:00:20,400 --> 00:00:22,160
actually start today's tutorial

13
00:00:22,160 --> 00:00:23,600
is that if you do enjoy this kind of

14
00:00:23,600 --> 00:00:25,199
content you want to see me make more of

15
00:00:25,199 --> 00:00:25,519
it

16
00:00:25,519 --> 00:00:27,199
please do make sure to subscribe to the

17
00:00:27,199 --> 00:00:28,640
channel as it really does help out a lot

18
00:00:28,640 --> 00:00:30,160
and turn on notifications

19
00:00:30,160 --> 00:00:31,679
that way you're actually notified when i

20
00:00:31,679 --> 00:00:33,840
release videos like this one

21
00:00:33,840 --> 00:00:35,120
apart from that if you do enjoy this

22
00:00:35,120 --> 00:00:36,800
video please do also make sure to hit a

23
00:00:36,800 --> 00:00:37,840
hit the like button

24
00:00:37,840 --> 00:00:39,440
and if you have any questions feel free

25
00:00:39,440 --> 00:00:41,840
to leave them down in the comments below

26
00:00:41,840 --> 00:00:43,280
now personally this is something that

27
00:00:43,280 --> 00:00:44,640
i'm really interested in because i've

28
00:00:44,640 --> 00:00:46,480
started to see this shift

29
00:00:46,480 --> 00:00:48,480
from training architectures and training

30
00:00:48,480 --> 00:00:50,719
regimes really for neural networks

31
00:00:50,719 --> 00:00:52,559
that are mostly hey let's train a

32
00:00:52,559 --> 00:00:54,079
network on our task

33
00:00:54,079 --> 00:00:55,680
um and then we saw this evolution

34
00:00:55,680 --> 00:00:58,399
towards well maybe we can fine-tune some

35
00:00:58,399 --> 00:01:00,239
neural networks and get them to do other

36
00:01:00,239 --> 00:01:02,079
things like for example if we train a

37
00:01:02,079 --> 00:01:04,000
network on imagenet then maybe we can

38
00:01:04,000 --> 00:01:06,000
you know specialize it to cats and dogs

39
00:01:06,000 --> 00:01:07,280
or something of that sort

40
00:01:07,280 --> 00:01:09,119
and now we're seeing this full-on sort

41
00:01:09,119 --> 00:01:10,560
of transfer learning approach

42
00:01:10,560 --> 00:01:12,320
where we're trying to say how much

43
00:01:12,320 --> 00:01:14,080
insight can we extract

44
00:01:14,080 --> 00:01:16,560
from just data right from just as much

45
00:01:16,560 --> 00:01:17,840
data as we can get

46
00:01:17,840 --> 00:01:18,960
and then how can we go ahead and

47
00:01:18,960 --> 00:01:21,360
specialize these neural networks to work

48
00:01:21,360 --> 00:01:22,799
really really well

49
00:01:22,799 --> 00:01:24,880
on other kinds of sort of domain

50
00:01:24,880 --> 00:01:26,479
specific data

51
00:01:26,479 --> 00:01:27,759
now personally i find this really

52
00:01:27,759 --> 00:01:30,000
interesting and sort of the field that's

53
00:01:30,000 --> 00:01:31,759
been leading this evolution i believe

54
00:01:31,759 --> 00:01:32,960
is the field of natural language

55
00:01:32,960 --> 00:01:35,119
processing now depending on how many of

56
00:01:35,119 --> 00:01:36,479
my videos you've watched before

57
00:01:36,479 --> 00:01:38,159
you probably know that a i'm really

58
00:01:38,159 --> 00:01:39,360
passionate about machine learning

59
00:01:39,360 --> 00:01:40,400
technology

60
00:01:40,400 --> 00:01:42,720
and that b natural language processing

61
00:01:42,720 --> 00:01:44,079
and natural language understanding and

62
00:01:44,079 --> 00:01:44,960
specific

63
00:01:44,960 --> 00:01:46,880
is my favorite sort of subfield within

64
00:01:46,880 --> 00:01:48,159
machine learning technology

65
00:01:48,159 --> 00:01:49,840
reasons for that are you know many

66
00:01:49,840 --> 00:01:51,040
different reasons actually but really

67
00:01:51,040 --> 00:01:52,000
it's because

68
00:01:52,000 --> 00:01:55,200
natural language is so complex not only

69
00:01:55,200 --> 00:01:56,320
do we need people to

70
00:01:56,320 --> 00:01:58,880
sort of encode within machines generic

71
00:01:58,880 --> 00:01:59,360
and

72
00:01:59,360 --> 00:02:01,360
sort of imaginative thoughts that we

73
00:02:01,360 --> 00:02:02,479
have as humans

74
00:02:02,479 --> 00:02:04,479
but we also somehow have to make it work

75
00:02:04,479 --> 00:02:06,479
across domains right no matter what

76
00:02:06,479 --> 00:02:07,200
exactly

77
00:02:07,200 --> 00:02:09,039
that language is about and that can be

78
00:02:09,039 --> 00:02:10,560
really difficult right it's like

79
00:02:10,560 --> 00:02:12,319
for example bert was trained on just

80
00:02:12,319 --> 00:02:14,239
hundreds of gigabytes of just text from

81
00:02:14,239 --> 00:02:15,520
the internet

82
00:02:15,520 --> 00:02:17,520
and that enables bert to be a really

83
00:02:17,520 --> 00:02:19,200
good natural language understanding

84
00:02:19,200 --> 00:02:19,920
engine

85
00:02:19,920 --> 00:02:22,239
which we can then fine-tune on what are

86
00:02:22,239 --> 00:02:23,599
known as downstream

87
00:02:23,599 --> 00:02:25,599
tasks things like question answering or

88
00:02:25,599 --> 00:02:27,680
natural language classification or

89
00:02:27,680 --> 00:02:30,239
aggression or things like this

90
00:02:30,239 --> 00:02:34,080
but then in between there's a sort of

91
00:02:34,080 --> 00:02:36,239
i would say rarer step that not as many

92
00:02:36,239 --> 00:02:37,360
people do

93
00:02:37,360 --> 00:02:39,760
but can be incredibly helpful and that

94
00:02:39,760 --> 00:02:42,319
is the same sort of pre-training stage

95
00:02:42,319 --> 00:02:43,840
but once again for your own

96
00:02:43,840 --> 00:02:46,000
domain-specific data so

97
00:02:46,000 --> 00:02:47,599
before we get into that let's take a

98
00:02:47,599 --> 00:02:49,200
little bit of a step back

99
00:02:49,200 --> 00:02:50,720
what is bert now if you're watching this

100
00:02:50,720 --> 00:02:52,239
video i assume you already know a little

101
00:02:52,239 --> 00:02:53,599
bit about the bert network

102
00:02:53,599 --> 00:02:56,480
but just as a quick recap burt is a

103
00:02:56,480 --> 00:02:57,120
natural

104
00:02:57,120 --> 00:03:00,319
language understanding or a transformer

105
00:03:00,319 --> 00:03:01,599
built with the goal of being able to

106
00:03:01,599 --> 00:03:03,440
understand natural language

107
00:03:03,440 --> 00:03:05,200
this was developed by google a couple

108
00:03:05,200 --> 00:03:06,720
months ago

109
00:03:06,720 --> 00:03:08,560
and it's really brought a revolution

110
00:03:08,560 --> 00:03:10,239
into the world of natural language

111
00:03:10,239 --> 00:03:12,000
processing technology right now

112
00:03:12,000 --> 00:03:13,760
pretty much every nlu solution out there

113
00:03:13,760 --> 00:03:16,000
is powered in some way by

114
00:03:16,000 --> 00:03:17,519
this bert neural network or its

115
00:03:17,519 --> 00:03:19,040
derivatives

116
00:03:19,040 --> 00:03:20,959
now the thing about bert is that it's

117
00:03:20,959 --> 00:03:22,800
trained with a really really interesting

118
00:03:22,800 --> 00:03:23,440
scheme

119
00:03:23,440 --> 00:03:24,879
well there are technically two of them

120
00:03:24,879 --> 00:03:26,959
but the main one is called masked

121
00:03:26,959 --> 00:03:28,000
language modeling

122
00:03:28,000 --> 00:03:30,159
now you may recognize the term language

123
00:03:30,159 --> 00:03:31,120
modeling from

124
00:03:31,120 --> 00:03:32,560
neural networks that can generate

125
00:03:32,560 --> 00:03:34,159
language so for example being able to

126
00:03:34,159 --> 00:03:34,959
take a word

127
00:03:34,959 --> 00:03:36,400
and then predict the next word and then

128
00:03:36,400 --> 00:03:37,440
take those two and then predict the

129
00:03:37,440 --> 00:03:39,200
third word and so on and so forth

130
00:03:39,200 --> 00:03:41,200
until you reach the end of a sequence

131
00:03:41,200 --> 00:03:43,680
well masked language modeling is similar

132
00:03:43,680 --> 00:03:46,239
but except in instead of going from left

133
00:03:46,239 --> 00:03:47,840
to right and continuously generating

134
00:03:47,840 --> 00:03:48,879
more words

135
00:03:48,879 --> 00:03:52,239
what's fed into bert is a whole sentence

136
00:03:52,239 --> 00:03:54,799
with a certain number of tokens or words

137
00:03:54,799 --> 00:03:56,560
just randomly removed

138
00:03:56,560 --> 00:03:58,400
and then what they actually train bert

139
00:03:58,400 --> 00:04:00,799
to do is fill in the blanks

140
00:04:00,799 --> 00:04:02,640
so bert is going to learn the sort of

141
00:04:02,640 --> 00:04:04,879
internal representation of natural

142
00:04:04,879 --> 00:04:06,560
language

143
00:04:06,560 --> 00:04:09,439
just based off of filling the blanks

144
00:04:09,439 --> 00:04:10,640
that is incredible

145
00:04:10,640 --> 00:04:12,560
who would have thought that we can train

146
00:04:12,560 --> 00:04:14,720
neural networks to not only understand

147
00:04:14,720 --> 00:04:15,680
natural language

148
00:04:15,680 --> 00:04:17,519
but even structure it internally

149
00:04:17,519 --> 00:04:19,680
mathematically similarly to how we as

150
00:04:19,680 --> 00:04:20,959
humans do

151
00:04:20,959 --> 00:04:22,560
just by training them to fill in the

152
00:04:22,560 --> 00:04:24,400
blanks i think that's really interesting

153
00:04:24,400 --> 00:04:26,240
it's kind of like how we teach kids to

154
00:04:26,240 --> 00:04:27,600
to read and write now

155
00:04:27,600 --> 00:04:29,360
bert is also trained on what's known as

156
00:04:29,360 --> 00:04:31,840
the next sentence prediction task

157
00:04:31,840 --> 00:04:33,360
uh which enables it to take two

158
00:04:33,360 --> 00:04:35,360
sentences and basically predict uh is

159
00:04:35,360 --> 00:04:36,240
sentence b

160
00:04:36,240 --> 00:04:38,400
what comes immediately after sentence a

161
00:04:38,400 --> 00:04:40,320
or is just some completely random

162
00:04:40,320 --> 00:04:41,600
sentence

163
00:04:41,600 --> 00:04:43,520
however the the researchers over at

164
00:04:43,520 --> 00:04:44,960
facebook sort of determined that wasn't

165
00:04:44,960 --> 00:04:46,479
really necessary didn't really help out

166
00:04:46,479 --> 00:04:47,840
the training process at all

167
00:04:47,840 --> 00:04:49,440
and so with the roberta network they

168
00:04:49,440 --> 00:04:51,040
kind of just didn't do

169
00:04:51,040 --> 00:04:53,280
the next sentence prediction um and so

170
00:04:53,280 --> 00:04:54,880
whether or not that's needed as a whole

171
00:04:54,880 --> 00:04:56,960
sort of debate it sort of helps out some

172
00:04:56,960 --> 00:04:58,639
downstream tasks but the vast majority

173
00:04:58,639 --> 00:04:59,520
of them it doesn't

174
00:04:59,520 --> 00:05:00,800
and there are other kinds of training

175
00:05:00,800 --> 00:05:02,400
objectives that are better but

176
00:05:02,400 --> 00:05:03,199
regardless

177
00:05:03,199 --> 00:05:05,520
what we're focusing on today is masked

178
00:05:05,520 --> 00:05:06,240
language

179
00:05:06,240 --> 00:05:08,960
modeling now after the mass language

180
00:05:08,960 --> 00:05:10,080
modeling objective

181
00:05:10,080 --> 00:05:12,160
usually what pretty much anyone would do

182
00:05:12,160 --> 00:05:13,600
is they would go to the google website

183
00:05:13,600 --> 00:05:13,919
or

184
00:05:13,919 --> 00:05:16,240
github repo for bert they would download

185
00:05:16,240 --> 00:05:17,280
the bert model

186
00:05:17,280 --> 00:05:19,120
they would use their own data and they

187
00:05:19,120 --> 00:05:20,560
would just fine-tune

188
00:05:20,560 --> 00:05:22,639
bert and its generic natural language

189
00:05:22,639 --> 00:05:24,000
understanding engine

190
00:05:24,000 --> 00:05:25,919
on their data and that's what i've been

191
00:05:25,919 --> 00:05:28,160
doing that's what everyone's been doing

192
00:05:28,160 --> 00:05:30,960
but if you'd like to go the extra mile

193
00:05:30,960 --> 00:05:32,720
if you have for example

194
00:05:32,720 --> 00:05:35,440
lots of unlabeled text data that is

195
00:05:35,440 --> 00:05:37,120
specific to your domain

196
00:05:37,120 --> 00:05:39,840
but only a little bit of labeled data

197
00:05:39,840 --> 00:05:41,759
then you can do what's known as a

198
00:05:41,759 --> 00:05:44,960
second step of pre-training

199
00:05:44,960 --> 00:05:47,520
you can take the pre-trained bird and

200
00:05:47,520 --> 00:05:48,320
run mass

201
00:05:48,320 --> 00:05:51,360
language modeling once more on your own

202
00:05:51,360 --> 00:05:53,360
data before you run

203
00:05:53,360 --> 00:05:56,160
your downstream task now the reason for

204
00:05:56,160 --> 00:05:58,240
this is not because you want bert to

205
00:05:58,240 --> 00:06:00,560
play film blanks with you but because

206
00:06:00,560 --> 00:06:02,319
what what's going to happen is

207
00:06:02,319 --> 00:06:04,319
burt is going to adjust its data

208
00:06:04,319 --> 00:06:06,319
distribution or the way that it expects

209
00:06:06,319 --> 00:06:08,560
the data to be distributed as input

210
00:06:08,560 --> 00:06:12,000
to match your domain specific language

211
00:06:12,000 --> 00:06:13,600
now i've been working on a couple of

212
00:06:13,600 --> 00:06:15,280
different you know projects that are

213
00:06:15,280 --> 00:06:17,120
based on this foundation of very

214
00:06:17,120 --> 00:06:18,560
powerful nlp

215
00:06:18,560 --> 00:06:20,240
one of them is in the field of music

216
00:06:20,240 --> 00:06:21,520
it's something that i haven't really

217
00:06:21,520 --> 00:06:22,960
talked about just yet

218
00:06:22,960 --> 00:06:24,960
but one of the components for this

219
00:06:24,960 --> 00:06:27,919
project requires utilizing birth

220
00:06:27,919 --> 00:06:29,840
as you can imagine it's really difficult

221
00:06:29,840 --> 00:06:31,520
to get access to large amounts of

222
00:06:31,520 --> 00:06:32,400
labeled data

223
00:06:32,400 --> 00:06:34,000
when i'm working with this sort of the

224
00:06:34,000 --> 00:06:36,720
sort of field and it's not very generic

225
00:06:36,720 --> 00:06:38,479
language either it has a very specific

226
00:06:38,479 --> 00:06:40,240
theme to it sometimes it you know

227
00:06:40,240 --> 00:06:41,680
ignores grammatical rules and

228
00:06:41,680 --> 00:06:43,759
punctuation and spacing

229
00:06:43,759 --> 00:06:46,080
and therefore i need a very custom

230
00:06:46,080 --> 00:06:48,479
version of bert to work here

231
00:06:48,479 --> 00:06:50,479
so what i've gone ahead and done is i

232
00:06:50,479 --> 00:06:52,080
have actually put together a real

233
00:06:52,080 --> 00:06:52,880
example

234
00:06:52,880 --> 00:06:54,720
of how that secondary stage of

235
00:06:54,720 --> 00:06:56,240
pre-training helps bert

236
00:06:56,240 --> 00:06:58,240
and we'll talk about why exactly this

237
00:06:58,240 --> 00:07:00,240
all works as we take a look at the code

238
00:07:00,240 --> 00:07:00,639
as well

239
00:07:00,639 --> 00:07:03,759
so let's quickly dive into this now to

240
00:07:03,759 --> 00:07:05,759
start off i'm going to tell you what i

241
00:07:05,759 --> 00:07:06,319
did

242
00:07:06,319 --> 00:07:08,240
as i said bert's already been trained

243
00:07:08,240 --> 00:07:10,080
with masked language modeling

244
00:07:10,080 --> 00:07:12,479
what i've done is i've gone ahead and i

245
00:07:12,479 --> 00:07:13,440
have downloaded

246
00:07:13,440 --> 00:07:16,639
a couple ten thousand song lyrics

247
00:07:16,639 --> 00:07:19,120
from the genius website now what i've

248
00:07:19,120 --> 00:07:20,080
done is i've taken

249
00:07:20,080 --> 00:07:22,800
each set of lyrics and i've split them

250
00:07:22,800 --> 00:07:25,280
by the individual line so just line by

251
00:07:25,280 --> 00:07:27,199
line not section by section just line by

252
00:07:27,199 --> 00:07:28,000
line

253
00:07:28,000 --> 00:07:29,680
then what i did is i put together a

254
00:07:29,680 --> 00:07:31,440
little script that would automatically

255
00:07:31,440 --> 00:07:32,560
go through each line

256
00:07:32,560 --> 00:07:35,520
and make two versions for each one where

257
00:07:35,520 --> 00:07:36,240
15

258
00:07:36,240 --> 00:07:37,680
of the tokens were just masked out

259
00:07:37,680 --> 00:07:39,680
randomly and removed and the other one's

260
00:07:39,680 --> 00:07:41,039
just the original

261
00:07:41,039 --> 00:07:44,240
then i retrained bert to do that fill in

262
00:07:44,240 --> 00:07:45,440
the blank task

263
00:07:45,440 --> 00:07:47,520
and before we take a look at why exactly

264
00:07:47,520 --> 00:07:49,360
this was so useful for me

265
00:07:49,360 --> 00:07:51,280
let's just take a look at some of the

266
00:07:51,280 --> 00:07:53,840
outputs from the network to begin with

267
00:07:53,840 --> 00:07:55,680
now we're going to start off with what i

268
00:07:55,680 --> 00:07:58,319
think is a really interesting example

269
00:07:58,319 --> 00:08:01,599
it's an example where the network it's

270
00:08:01,599 --> 00:08:04,720
competent but you can see that the

271
00:08:04,720 --> 00:08:06,160
second step of pre-training didn't

272
00:08:06,160 --> 00:08:08,080
really help out that much so

273
00:08:08,080 --> 00:08:09,919
i'm gonna go ahead and take this line

274
00:08:09,919 --> 00:08:12,319
over here from genius and i'm going to

275
00:08:12,319 --> 00:08:14,319
feed it into both neural networks

276
00:08:14,319 --> 00:08:16,800
you can see both pieces of output on

277
00:08:16,800 --> 00:08:18,080
screen right now

278
00:08:18,080 --> 00:08:20,160
on the left you can see the top 10

279
00:08:20,160 --> 00:08:22,160
predictions from the network that i

280
00:08:22,160 --> 00:08:25,120
fine-tuned to be really good at filling

281
00:08:25,120 --> 00:08:27,440
the blanks for lyrics specifically

282
00:08:27,440 --> 00:08:28,879
on the right you can see the output from

283
00:08:28,879 --> 00:08:30,720
the network that wasn't fine-tuned this

284
00:08:30,720 --> 00:08:31,440
was just

285
00:08:31,440 --> 00:08:33,279
directly downloaded from the hugging

286
00:08:33,279 --> 00:08:35,680
face repository for neural networks or

287
00:08:35,680 --> 00:08:38,000
transformers to be specific as you can

288
00:08:38,000 --> 00:08:40,240
see looking which is indeed the gold

289
00:08:40,240 --> 00:08:41,360
standard label

290
00:08:41,360 --> 00:08:44,320
is the top prediction for both models to

291
00:08:44,320 --> 00:08:45,760
give you a bit more context this is the

292
00:08:45,760 --> 00:08:47,200
base model for bert

293
00:08:47,200 --> 00:08:48,640
i'm not really using the large model

294
00:08:48,640 --> 00:08:50,560
here so looking which is the gold

295
00:08:50,560 --> 00:08:51,360
standard label

296
00:08:51,360 --> 00:08:54,880
is indeed the top prediction

297
00:08:54,880 --> 00:08:56,959
now this is a bit more subjective but if

298
00:08:56,959 --> 00:08:58,399
you were to go down in the

299
00:08:58,399 --> 00:09:01,360
you know top two three etc you might

300
00:09:01,360 --> 00:09:02,560
notice that the network that was

301
00:09:02,560 --> 00:09:04,399
fine-tuned for music lyrics

302
00:09:04,399 --> 00:09:06,399
comes up with words that would still be

303
00:09:06,399 --> 00:09:09,040
more plausible to be part of a song

304
00:09:09,040 --> 00:09:11,279
rather than the non-fine-tuned bert

305
00:09:11,279 --> 00:09:12,399
which is just trying to fill out the

306
00:09:12,399 --> 00:09:13,760
sentence as if it's just

307
00:09:13,760 --> 00:09:16,080
somewhere random on the internet but

308
00:09:16,080 --> 00:09:17,519
then again that is a bit more subjective

309
00:09:17,519 --> 00:09:19,200
so i'm not going to

310
00:09:19,200 --> 00:09:20,880
try to do an objective analysis of that

311
00:09:20,880 --> 00:09:22,480
right now instead

312
00:09:22,480 --> 00:09:24,080
i want to move on so we've seen an

313
00:09:24,080 --> 00:09:26,320
example where both networks achieve

314
00:09:26,320 --> 00:09:28,240
pretty much the same outcome and i'm

315
00:09:28,240 --> 00:09:29,200
showing you this because i don't

316
00:09:29,200 --> 00:09:30,240
necessarily want to

317
00:09:30,240 --> 00:09:32,080
cherry pick the results that i show you

318
00:09:32,080 --> 00:09:34,720
here now i want to show you an example

319
00:09:34,720 --> 00:09:35,200
where

320
00:09:35,200 --> 00:09:39,120
both completely fail all right and

321
00:09:39,120 --> 00:09:40,320
it's it's not because the neural

322
00:09:40,320 --> 00:09:42,320
networks aren't competent once again

323
00:09:42,320 --> 00:09:45,120
it's because the context is just too

324
00:09:45,120 --> 00:09:45,839
varied

325
00:09:45,839 --> 00:09:47,040
all right so what i've done is i've gone

326
00:09:47,040 --> 00:09:49,279
ahead and taken this specific

327
00:09:49,279 --> 00:09:50,800
line from this song all right and i

328
00:09:50,800 --> 00:09:53,120
masked out the word lining

329
00:09:53,120 --> 00:09:55,279
and i fed this into my neural network as

330
00:09:55,279 --> 00:09:57,600
well as the original bert network

331
00:09:57,600 --> 00:10:00,399
and both of them had absolutely terrible

332
00:10:00,399 --> 00:10:01,920
output

333
00:10:01,920 --> 00:10:03,680
now again you might say that the one

334
00:10:03,680 --> 00:10:05,680
that's fine-tuned has

335
00:10:05,680 --> 00:10:07,920
slightly better output as in you know it

336
00:10:07,920 --> 00:10:09,920
would make a little bit more sense for

337
00:10:09,920 --> 00:10:12,880
uh picking to be here instead of brought

338
00:10:12,880 --> 00:10:14,959
which is the second top prediction um

339
00:10:14,959 --> 00:10:17,600
respectively for both models but then

340
00:10:17,600 --> 00:10:18,079
the word

341
00:10:18,079 --> 00:10:20,160
lining isn't there at all however i will

342
00:10:20,160 --> 00:10:22,160
say the fine-tuned model

343
00:10:22,160 --> 00:10:25,519
did get a lot closer as you can see

344
00:10:25,519 --> 00:10:27,839
down at rank number seven we do have the

345
00:10:27,839 --> 00:10:28,640
word

346
00:10:28,640 --> 00:10:32,240
lined which is close to lining

347
00:10:32,240 --> 00:10:33,360
again it doesn't really mean the same

348
00:10:33,360 --> 00:10:36,079
thing in context here but you can argue

349
00:10:36,079 --> 00:10:39,200
that the fine-tuned network was in a

350
00:10:39,200 --> 00:10:42,480
sort of closer ballpark or radius

351
00:10:42,480 --> 00:10:44,399
versus the model that wasn't fine-tuned

352
00:10:44,399 --> 00:10:46,240
at all so you've currently seen an

353
00:10:46,240 --> 00:10:47,040
example

354
00:10:47,040 --> 00:10:48,959
where both networks are pretty much the

355
00:10:48,959 --> 00:10:51,600
same both networks pretty much fail

356
00:10:51,600 --> 00:10:53,200
so you're probably saying well what's

357
00:10:53,200 --> 00:10:54,720
the value here why would i go through

358
00:10:54,720 --> 00:10:55,680
all that extra

359
00:10:55,680 --> 00:10:58,399
extra effort to actually do the extra

360
00:10:58,399 --> 00:11:00,560
parade training step

361
00:11:00,560 --> 00:11:03,040
now remember you haven't seen actual

362
00:11:03,040 --> 00:11:05,200
downstream performance just yet

363
00:11:05,200 --> 00:11:06,640
but i will show that to you in just a

364
00:11:06,640 --> 00:11:09,519
moment now i want to show you some truly

365
00:11:09,519 --> 00:11:10,880
mind-boggling examples

366
00:11:10,880 --> 00:11:13,360
examples that i think really prove why

367
00:11:13,360 --> 00:11:14,000
this is such

368
00:11:14,000 --> 00:11:16,079
an important step let's take a look at

369
00:11:16,079 --> 00:11:18,640
them on my computer right now

370
00:11:18,640 --> 00:11:20,240
now as you can see what i've gone ahead

371
00:11:20,240 --> 00:11:22,240
and done is i've gone ahead and taken

372
00:11:22,240 --> 00:11:25,519
this line from this song now as you can

373
00:11:25,519 --> 00:11:26,240
tell

374
00:11:26,240 --> 00:11:28,959
in in sort of general context on the

375
00:11:28,959 --> 00:11:30,720
internet you more than likely wouldn't

376
00:11:30,720 --> 00:11:32,240
see this kind of line if you were just

377
00:11:32,240 --> 00:11:33,839
browsing reddit or if you were just

378
00:11:33,839 --> 00:11:35,120
browsing facebook or twitter you

379
00:11:35,120 --> 00:11:36,959
probably wouldn't see this line anywhere

380
00:11:36,959 --> 00:11:38,720
but in a song you wouldn't bat an eye at

381
00:11:38,720 --> 00:11:40,160
it it's just there

382
00:11:40,160 --> 00:11:42,160
of course it wouldn't be it's it's it's

383
00:11:42,160 --> 00:11:43,200
music

384
00:11:43,200 --> 00:11:46,560
so i'm gonna go ahead and feed this into

385
00:11:46,560 --> 00:11:48,480
uh both my fine-tuned and the original

386
00:11:48,480 --> 00:11:50,480
network and i'm gonna mask out the word

387
00:11:50,480 --> 00:11:53,600
sheets now as you can see

388
00:11:53,600 --> 00:11:55,600
the fine-tuned network just blows the

389
00:11:55,600 --> 00:11:57,360
original one out of the

390
00:11:57,360 --> 00:12:00,639
water now the gold standard label is the

391
00:12:00,639 --> 00:12:01,519
number one

392
00:12:01,519 --> 00:12:03,920
prediction for the fine-tuned model and

393
00:12:03,920 --> 00:12:04,639
the original

394
00:12:04,639 --> 00:12:06,240
doesn't even get the gold standard in

395
00:12:06,240 --> 00:12:08,639
the top ten i think that is

396
00:12:08,639 --> 00:12:11,519
absolutely incredible now you can see

397
00:12:11,519 --> 00:12:13,360
that bird is trying its best

398
00:12:13,360 --> 00:12:15,200
it's like where could a stain be well

399
00:12:15,200 --> 00:12:16,800
maybe there would be a stir stain on the

400
00:12:16,800 --> 00:12:18,800
floor maybe the walls maybe a

401
00:12:18,800 --> 00:12:21,120
counter or a carpet but it's unable to

402
00:12:21,120 --> 00:12:22,320
get the word sheets

403
00:12:22,320 --> 00:12:24,959
because well that's very you know song

404
00:12:24,959 --> 00:12:28,320
domain specific context extension

405
00:12:28,320 --> 00:12:31,040
but what i think is an even more sort of

406
00:12:31,040 --> 00:12:33,680
extraordinary application of this

407
00:12:33,680 --> 00:12:36,399
is when we move the mask to the very

408
00:12:36,399 --> 00:12:37,440
beginning

409
00:12:37,440 --> 00:12:40,480
of the sentence now it's going to be the

410
00:12:40,480 --> 00:12:41,600
exact same sentence we're going to

411
00:12:41,600 --> 00:12:43,120
remove the mask from the word sheet so

412
00:12:43,120 --> 00:12:44,959
we're going to put that back there

413
00:12:44,959 --> 00:12:46,399
but then i'm actually going to go ahead

414
00:12:46,399 --> 00:12:48,000
and have the models

415
00:12:48,000 --> 00:12:50,720
uh predict on the first word masked

416
00:12:50,720 --> 00:12:51,360
instead

417
00:12:51,360 --> 00:12:54,240
and as you can see once again the

418
00:12:54,240 --> 00:12:55,519
fine-tuned model just

419
00:12:55,519 --> 00:12:58,959
absolutely shreds the original model's

420
00:12:58,959 --> 00:12:59,680
performance

421
00:12:59,680 --> 00:13:02,079
the original model isn't even close in

422
00:13:02,079 --> 00:13:03,360
what it's trying to

423
00:13:03,360 --> 00:13:05,839
fill whereas the fine-tuned model

424
00:13:05,839 --> 00:13:06,959
instantly right there

425
00:13:06,959 --> 00:13:08,880
it's got the word lipstick first

426
00:13:08,880 --> 00:13:10,800
prediction it's god

427
00:13:10,800 --> 00:13:13,519
now what i personally find so incredible

428
00:13:13,519 --> 00:13:14,800
about this

429
00:13:14,800 --> 00:13:18,160
is is think about it the word

430
00:13:18,160 --> 00:13:20,480
lipstick stains and the word sheets

431
00:13:20,480 --> 00:13:21,360
didn't appear

432
00:13:21,360 --> 00:13:23,040
in the same context that this neural

433
00:13:23,040 --> 00:13:25,200
network was trained on at all

434
00:13:25,200 --> 00:13:27,360
it was actually taking these different

435
00:13:27,360 --> 00:13:28,720
pieces of data that referred to

436
00:13:28,720 --> 00:13:31,200
different concepts in different contexts

437
00:13:31,200 --> 00:13:33,920
and it's able to sort of generalize them

438
00:13:33,920 --> 00:13:35,680
in its internal structure

439
00:13:35,680 --> 00:13:37,440
in order to make it so when we refer to

440
00:13:37,440 --> 00:13:39,440
something as a brand new imaginative

441
00:13:39,440 --> 00:13:40,240
concept

442
00:13:40,240 --> 00:13:42,880
just like that it's able to predict what

443
00:13:42,880 --> 00:13:45,199
the masked word should be

444
00:13:45,199 --> 00:13:46,639
now while the fill in the blank

445
00:13:46,639 --> 00:13:48,720
performance may be amazing

446
00:13:48,720 --> 00:13:50,160
you're probably wondering well does it

447
00:13:50,160 --> 00:13:52,639
really help you in a downstream task

448
00:13:52,639 --> 00:13:55,600
and well yes it does because think about

449
00:13:55,600 --> 00:13:56,160
it

450
00:13:56,160 --> 00:13:58,160
if you're able to better predict what

451
00:13:58,160 --> 00:13:59,199
words should be

452
00:13:59,199 --> 00:14:00,720
that doesn't mean you have a better word

453
00:14:00,720 --> 00:14:02,880
predictor that means you have a better

454
00:14:02,880 --> 00:14:03,839
and better

455
00:14:03,839 --> 00:14:06,480
you have a better every single layer in

456
00:14:06,480 --> 00:14:07,920
burke before the last one

457
00:14:07,920 --> 00:14:10,000
that is working towards embedding your

458
00:14:10,000 --> 00:14:11,360
entire sequence

459
00:14:11,360 --> 00:14:14,160
into a space in which it makes sense in

460
00:14:14,160 --> 00:14:15,440
your domain

461
00:14:15,440 --> 00:14:17,360
and the networks that are responsible

462
00:14:17,360 --> 00:14:18,560
for taking the output of those

463
00:14:18,560 --> 00:14:19,360
embeddings

464
00:14:19,360 --> 00:14:21,040
and actually doing something with them

465
00:14:21,040 --> 00:14:22,560
in a downstream task

466
00:14:22,560 --> 00:14:24,680
will then have better more

467
00:14:24,680 --> 00:14:26,959
contextualized embeddings to use

468
00:14:26,959 --> 00:14:28,320
i mean think about why brit is so

469
00:14:28,320 --> 00:14:30,360
powerful in the first place it's massive

470
00:14:30,360 --> 00:14:31,680
contextualization

471
00:14:31,680 --> 00:14:33,519
and with this technology we're getting

472
00:14:33,519 --> 00:14:35,320
further and further

473
00:14:35,320 --> 00:14:37,920
contextualization now what's absolutely

474
00:14:37,920 --> 00:14:39,040
amazing about this

475
00:14:39,040 --> 00:14:41,680
is just how simple it is to fine-tune

476
00:14:41,680 --> 00:14:42,959
networks themselves

477
00:14:42,959 --> 00:14:44,240
i mean take a look at this in order to

478
00:14:44,240 --> 00:14:45,839
add this pre-training stage they're

479
00:14:45,839 --> 00:14:46,639
really just

480
00:14:46,639 --> 00:14:48,800
two main steps i have to do first of all

481
00:14:48,800 --> 00:14:49,680
i put together

482
00:14:49,680 --> 00:14:52,959
the following data python file what this

483
00:14:52,959 --> 00:14:53,440
does

484
00:14:53,440 --> 00:14:55,600
is it goes ahead and takes of course the

485
00:14:55,600 --> 00:14:57,760
hugging face libraries uh the the

486
00:14:57,760 --> 00:14:59,279
transformers library specifically from

487
00:14:59,279 --> 00:15:00,320
hugging face

488
00:15:00,320 --> 00:15:02,880
um and it goes ahead and imports simple

489
00:15:02,880 --> 00:15:05,760
bass bert model as well as its tokenizer

490
00:15:05,760 --> 00:15:07,279
i define a couple of functions like for

491
00:15:07,279 --> 00:15:08,639
example function that's responsible for

492
00:15:08,639 --> 00:15:09,920
tokenizing input

493
00:15:09,920 --> 00:15:11,360
a function that's responsible for

494
00:15:11,360 --> 00:15:13,360
actually masking out some of those uh

495
00:15:13,360 --> 00:15:15,120
some of those values at random

496
00:15:15,120 --> 00:15:16,800
another function that's responsible for

497
00:15:16,800 --> 00:15:18,560
sorting and batching a bunch of

498
00:15:18,560 --> 00:15:19,519
different inputs

499
00:15:19,519 --> 00:15:21,279
into batches where burke can actually go

500
00:15:21,279 --> 00:15:23,040
ahead and and use them efficiently on

501
00:15:23,040 --> 00:15:24,000
the gpu

502
00:15:24,000 --> 00:15:26,000
and then all i got to do is go ahead and

503
00:15:26,000 --> 00:15:28,160
save those outputs to my disk

504
00:15:28,160 --> 00:15:30,320
and then what i do is i've got a little

505
00:15:30,320 --> 00:15:32,480
bit of a training script here

506
00:15:32,480 --> 00:15:35,440
and what this training script does is it

507
00:15:35,440 --> 00:15:37,440
uses the power of the library known as

508
00:15:37,440 --> 00:15:38,320
pytorch

509
00:15:38,320 --> 00:15:41,120
lightning in order to do what it is that

510
00:15:41,120 --> 00:15:42,160
i'm doing

511
00:15:42,160 --> 00:15:44,720
which is training this burp model in a

512
00:15:44,720 --> 00:15:46,399
way easier way than i could ever

513
00:15:46,399 --> 00:15:48,160
implement from scratch using pi torch

514
00:15:48,160 --> 00:15:50,000
and hogging face library

515
00:15:50,000 --> 00:15:52,320
um and specifically what i mean by that

516
00:15:52,320 --> 00:15:54,160
is if we go to the end of the code i'm

517
00:15:54,160 --> 00:15:55,519
using what's known as a

518
00:15:55,519 --> 00:15:58,000
pi torch lightning trainer module in

519
00:15:58,000 --> 00:15:59,440
order to automatically just say you know

520
00:15:59,440 --> 00:15:59,839
what

521
00:15:59,839 --> 00:16:02,160
i want to use distributed parallel back

522
00:16:02,160 --> 00:16:03,920
end i want to run four epochs and i want

523
00:16:03,920 --> 00:16:06,399
to run on four gpus on this machine

524
00:16:06,399 --> 00:16:08,480
just like that pie torch lightning is

525
00:16:08,480 --> 00:16:09,920
handling it and

526
00:16:09,920 --> 00:16:12,240
i've got my model training on four gpus

527
00:16:12,240 --> 00:16:14,000
and it's using distributed data parallel

528
00:16:14,000 --> 00:16:15,360
which is

529
00:16:15,360 --> 00:16:17,120
way more difficult to implement than the

530
00:16:17,120 --> 00:16:19,279
regular data parallel in pi torch

531
00:16:19,279 --> 00:16:21,120
but it's so much more efficient so it

532
00:16:21,120 --> 00:16:23,360
makes it worth it and pi torch lightning

533
00:16:23,360 --> 00:16:25,519
makes it that easy for me now if you

534
00:16:25,519 --> 00:16:27,199
take a look at the actual code that goes

535
00:16:27,199 --> 00:16:29,040
behind training the basic concepts are

536
00:16:29,040 --> 00:16:30,240
pretty simple of course

537
00:16:30,240 --> 00:16:32,079
i've got the data set module which is

538
00:16:32,079 --> 00:16:33,600
basically just over here to enable me to

539
00:16:33,600 --> 00:16:34,880
load in my data

540
00:16:34,880 --> 00:16:36,320
which is essentially just loading in a

541
00:16:36,320 --> 00:16:38,720
pickle that i had already created for my

542
00:16:38,720 --> 00:16:40,320
data processing script because of course

543
00:16:40,320 --> 00:16:41,519
every time i run the training i don't

544
00:16:41,519 --> 00:16:42,360
want to

545
00:16:42,360 --> 00:16:44,320
re-reprocess my data because that takes

546
00:16:44,320 --> 00:16:45,600
some time

547
00:16:45,600 --> 00:16:46,959
and then i've got this little pie torch

548
00:16:46,959 --> 00:16:48,399
lightning module over here that's

549
00:16:48,399 --> 00:16:50,560
responsible for opening up a pre-trained

550
00:16:50,560 --> 00:16:52,560
burp model from honking face

551
00:16:52,560 --> 00:16:54,480
going ahead and feeding my data into it

552
00:16:54,480 --> 00:16:56,639
along with the mask that prevents me

553
00:16:56,639 --> 00:16:57,440
from

554
00:16:57,440 --> 00:17:00,399
actually attending to padding tokens and

555
00:17:00,399 --> 00:17:01,759
then from there i'm just going ahead and

556
00:17:01,759 --> 00:17:03,519
running a training step which is

557
00:17:03,519 --> 00:17:05,760
feeding data into the network getting

558
00:17:05,760 --> 00:17:07,520
the loss based off of the mass language

559
00:17:07,520 --> 00:17:08,559
modeling objective

560
00:17:08,559 --> 00:17:10,079
and then returning that loss over the pi

561
00:17:10,079 --> 00:17:11,439
torch lightning where it can determine

562
00:17:11,439 --> 00:17:12,799
what exactly my new neural network

563
00:17:12,799 --> 00:17:13,679
weights need to be

564
00:17:13,679 --> 00:17:15,439
and it does that using the atom

565
00:17:15,439 --> 00:17:17,280
optimizer that i define

566
00:17:17,280 --> 00:17:20,000
over here so the basic code for fine

567
00:17:20,000 --> 00:17:21,439
tuning is incredibly simple this is

568
00:17:21,439 --> 00:17:22,480
already on

569
00:17:22,480 --> 00:17:24,160
github and there will be a link to it

570
00:17:24,160 --> 00:17:25,919
down in the description unfortunately

571
00:17:25,919 --> 00:17:28,079
i cannot provide you with a data set for

572
00:17:28,079 --> 00:17:29,840
this but if you do have your own data

573
00:17:29,840 --> 00:17:32,160
set you can easily repurpose this code

574
00:17:32,160 --> 00:17:34,720
and make it work for your own data and

575
00:17:34,720 --> 00:17:36,640
for your own tasks

576
00:17:36,640 --> 00:17:39,200
and of course just to show you once more

577
00:17:39,200 --> 00:17:39,760
you're probably

578
00:17:39,760 --> 00:17:41,679
finally wondering well i clicked on the

579
00:17:41,679 --> 00:17:43,120
video to figure out what the performance

580
00:17:43,120 --> 00:17:44,240
difference is

581
00:17:44,240 --> 00:17:46,000
and here it is the performance

582
00:17:46,000 --> 00:17:47,440
difference for me here

583
00:17:47,440 --> 00:17:50,400
uh is is usually a difference in loss

584
00:17:50,400 --> 00:17:51,880
from about

585
00:17:51,880 --> 00:17:55,039
0.52 for my usual downstream tasks

586
00:17:55,039 --> 00:17:58,400
to 0.49 when i'm training

587
00:17:58,400 --> 00:18:01,440
on uh with a with a fine-tuned

588
00:18:01,440 --> 00:18:03,919
pre-training objective uh now you're

589
00:18:03,919 --> 00:18:04,799
probably wondering

590
00:18:04,799 --> 00:18:06,400
what do those numbers mean well

591
00:18:06,400 --> 00:18:07,919
unfortunately i can't tell you what the

592
00:18:07,919 --> 00:18:08,400
task

593
00:18:08,400 --> 00:18:10,080
is that i'm training on because once

594
00:18:10,080 --> 00:18:12,080
again this is a uh this is um

595
00:18:12,080 --> 00:18:13,679
an objective that i can't i can't share

596
00:18:13,679 --> 00:18:15,280
with you just yet but

597
00:18:15,280 --> 00:18:18,000
the idea is that i'm using in general a

598
00:18:18,000 --> 00:18:18,559
triplet

599
00:18:18,559 --> 00:18:20,160
loss function customized version

600
00:18:20,160 --> 00:18:22,080
nonetheless um but it is

601
00:18:22,080 --> 00:18:24,799
a it is a triplet loss function and this

602
00:18:24,799 --> 00:18:26,480
triplet loss function again i train it

603
00:18:26,480 --> 00:18:27,760
on the pre-trained bert

604
00:18:27,760 --> 00:18:30,080
and the fine-tuned bert when i train on

605
00:18:30,080 --> 00:18:31,120
pre-trained burke

606
00:18:31,120 --> 00:18:34,320
loss comes out to usually about 0.52 and

607
00:18:34,320 --> 00:18:37,840
on the fine-tuned burke comes up to 0.49

608
00:18:37,840 --> 00:18:39,360
now you're probably wondering well great

609
00:18:39,360 --> 00:18:41,679
it's about a 0.03 difference

610
00:18:41,679 --> 00:18:43,039
why exactly does that matter well it

611
00:18:43,039 --> 00:18:45,039
matters because of two things first of

612
00:18:45,039 --> 00:18:45,360
all

613
00:18:45,360 --> 00:18:46,799
when you're dealing with limited amounts

614
00:18:46,799 --> 00:18:49,440
of data as much generalization and as

615
00:18:49,440 --> 00:18:49,760
much

616
00:18:49,760 --> 00:18:51,280
shaving off the losses you can get the

617
00:18:51,280 --> 00:18:53,039
better second

618
00:18:53,039 --> 00:18:55,520
i only trained this for two epochs and

619
00:18:55,520 --> 00:18:57,600
we're already seeing a difference

620
00:18:57,600 --> 00:19:00,480
and remember one more thing my burp when

621
00:19:00,480 --> 00:19:00,880
it was

622
00:19:00,880 --> 00:19:03,840
fine-tuned was only trained on line by

623
00:19:03,840 --> 00:19:04,559
line

624
00:19:04,559 --> 00:19:07,200
whereas this next task that i'm training

625
00:19:07,200 --> 00:19:08,480
on downstream

626
00:19:08,480 --> 00:19:11,120
is trained section by section so lines

627
00:19:11,120 --> 00:19:12,960
separated by separator tokens within

628
00:19:12,960 --> 00:19:14,000
birth

629
00:19:14,000 --> 00:19:16,799
and so keep in mind that my task isn't

630
00:19:16,799 --> 00:19:19,280
particularly ideal for pre-training and

631
00:19:19,280 --> 00:19:19,840
yet

632
00:19:19,840 --> 00:19:21,600
i'm getting such a great performance

633
00:19:21,600 --> 00:19:24,160
boost so with your domain specific tasks

634
00:19:24,160 --> 00:19:26,000
where you're actually pre-training on

635
00:19:26,000 --> 00:19:28,240
data that makes sense in your domain

636
00:19:28,240 --> 00:19:29,360
you can only imagine the kinds of

637
00:19:29,360 --> 00:19:30,960
performance boosts you're going to get

638
00:19:30,960 --> 00:19:32,000
and so

639
00:19:32,000 --> 00:19:34,799
this was a demo of why exactly it makes

640
00:19:34,799 --> 00:19:36,559
sense and how you can go ahead

641
00:19:36,559 --> 00:19:39,120
and sort of fine tune bert on your own

642
00:19:39,120 --> 00:19:41,280
data in a mass language modeling sense

643
00:19:41,280 --> 00:19:43,679
all the code will be in the description

644
00:19:43,679 --> 00:19:46,640
uh and hopefully that helped you realize

645
00:19:46,640 --> 00:19:48,240
what sort of training regimes you should

646
00:19:48,240 --> 00:19:49,600
be using for your neural networks of

647
00:19:49,600 --> 00:19:50,160
course

648
00:19:50,160 --> 00:19:52,000
every task is different going to require

649
00:19:52,000 --> 00:19:53,440
a completely different regime

650
00:19:53,440 --> 00:19:55,919
but this gives you a pretty good idea of

651
00:19:55,919 --> 00:19:57,120
what direction to look at

652
00:19:57,120 --> 00:19:58,320
i really do hope you enjoyed that

653
00:19:58,320 --> 00:20:00,000
tutorial once more if you did please do

654
00:20:00,000 --> 00:20:01,440
go ahead and subscribe to the channel it

655
00:20:01,440 --> 00:20:02,720
really does help out a lot and make sure

656
00:20:02,720 --> 00:20:04,000
to like the video as well

657
00:20:04,000 --> 00:20:06,240
if you did enjoy it now of course feel

658
00:20:06,240 --> 00:20:08,080
free to leave any questions down in the

659
00:20:08,080 --> 00:20:09,440
comments below i'd love to go ahead and

660
00:20:09,440 --> 00:20:11,520
answer them or as a github issue on the

661
00:20:11,520 --> 00:20:12,240
repo

662
00:20:12,240 --> 00:20:14,080
in the link in the description apart

663
00:20:14,080 --> 00:20:15,280
from that thank you very much everyone

664
00:20:15,280 --> 00:20:16,559
for joining in today i really do

665
00:20:16,559 --> 00:20:17,600
appreciate it and

666
00:20:17,600 --> 00:20:20,480
goodbye
