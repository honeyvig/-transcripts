foreign
hey there and Welcome to our YouTube
video on data analytics bootcamp in
today's Digital World data is King with
companies collecting more and more
information about their customers their
huge demand for professionals who can
analyze their data and turn it into
valuable insights and that's where a
career as a data analyst comes in did
you know that the employment of data
analysts is expected to group by 21 from
2020 to 2030 which is much faster than
the average for all occupations and that
the average base salary for a data
analyst in the United States is 70 000
per year with some data analysts earning
more than hundred thousand dollars per
year with stats like these it's easy to
see why a career as a data analyst can
be so lucrative so if you are interested
in working with data and making a real
impact then keep watching a data analyst
bootcamp course everything you need to
know to get started in this exciting
field and before that if you're working
professional looking to upskill and take
your career to the next level then look
no further than the Caltech data
analytics bootcamp our comprehensive
program is designed for individuals from
all backgrounds who want to develop core
skills in data analysis and become
expert in this field at the Caltech data
analytics bootcamp you will learn
everything you need to know to succeed
as a data analyst for from mastering
Excel to creating data driven
presentations manipulating data with SQL
analyzing data with python and
visualizing data with w and with the
added bonus of learning to use data
analysis tools such as AWS you will be
well equipped to tackle any data related
challenge that comes your way whether
you are just starting out in your career
or looking to upskill the Caltech data
analytics bootcamp is the perfect choice
so why wait enroll now and take the
first step towards becoming a data
analysis expert coming back to our data
analytics bootcamp we cover a variety of
topics to help you get started in this
exciting field we begin with an
introduction to data analysis and what
skills you will need to succeed as a
data analyst we then dive into the
different Industries where data analysts
are needed as well as the various career
paths you can expect next we cover the
top 10 skills and tools every data
analyst should know we also explore the
key differences between a data scientist
and a data analyst and which career
might be the best fit for you
additionally we provide tutorials on how
to use data analytics with Excel SQL and
python that includes pivot tables and
creating dashboards as well as an
introduction to using R and python for
data analysis finally we wrap up with
common data analyst interview questions
and how to prepare for them with all of
these topics covered you will be well on
your way to a successful career as a
data analyst before understanding the
job role of a data analyst let's
understand the meaning of the term data
analytics
so what does the term analyze mean it
merely means to scrutinize something to
derive meaningful conclusions from it
well data analytics also works similarly
it is the process by which useful
insights are extracted from raw data by
studying and examining it carefully
these insights can be related to
business information market trends
product Innovations and profit loss
report to name a few
here's an interesting comparison I'm
sure all of you have played with jigsaw
puzzles at some point in time for that
first you would have to gather all the
pieces together and then fit them
accordingly to bring out a beautiful
picture isn't it we can simply relate
the process of data analytics to how you
make a jigsaw puzzle
as you can see here data refers to the
raw data which can be structured
semi-structured or unstructured in
nature the process of data analytics
incorporates collecting data from
various sources cleaning it and then
finally transforming it into something
meaningful which can be interpreted by
humans
this information can be visually
presented in the form of graphs and
charts which provide precise results of
the analysis
various Technologies tools and
Frameworks are used in the analysis
process organizations take the help of
data analytics to convert the available
raw data into meaningful insights hence
there is a high requirement for
professionals who can play with data and
help organizations with crucial decision
making
there are many job roles in the field of
data analytics if you have watched our
previous video on data analytics Courier
you would have seen a few of these roles
out of all the job rules an important
role is that of a data analyst an
interesting thing about this job role is
that it can be taken up by freshers as
well it can embark on your career in the
field of data analytics it is a
lucrative career as the field of data
analytics is only going to continue to
Blossom in the years to come
so let's see who exactly a data analyst
is
a data analyst is a person who collects
processes and performs analysis on large
data sets here the statistical analysis
is done on various data sets every
business generates and collects data be
it marketing research sales figures
customer feedback Logistics or
Transportation costs a data analyst will
take all of this data and figure out
various measures such as how to price
new materials how to reduce
Transportation costs how to provide
better customer experience or how to
deal with issues that cost the company
money
data analysts also deal with data
handling data modeling and data
reporting
a data analyst has a number of Duties to
perform let's have a look at their
responsibilities now
first and foremost a data analyst is
required to recognize and understand the
organization's goal this helps in
streamlining and planning the analysis
process accordingly data analysts assess
the available resources understand the
business problem and gather the right
data this step is done by collaborating
with different team members such as
programmers business analysts and data
scientists
data analysts need to use queries to
gather information from a database they
write complex SQL queries and scripts to
gather and extract information from
several databases and data warehouses
they are responsible for data mining as
well here data is mined from various
sources and then organized in order to
obtain a new information from it this is
a vital role of a data analyst as they
have to extract data from various
sources in order to work on it with this
data they can build models it can reduce
a complexity and increase the efficiency
of the whole system
another crucial step in data analysis is
data cleaning and data wrangling usually
the data you can collect is often messy
and has a lot of missing values so it's
important to clean this data to make it
ready for analysis
data analysts use a number of
statistical and analytical tools
including programming languages for
performing analysis and logical
examination of data
using different libraries and packages
data analysts discover Trends and
patterns from complex data sets this
will help them find more unseen insights
from the data to make business
predictions
another important role of a data analyst
is to prepare summary reports for the
leadership team so that they can make
timely decisions for this data analysts
use multiple data visualization tools
some of these tools are discussed as
part of skills required which we will
see later
finally data analysts interact with the
development team business and management
team as well as with data scientists to
ensure proper implementation of business
requirements and to figure out
opportunities for better process
Improvement
now let us look at the various skills
required to become a data analyst
so the first skill is more of a
prerequisite you should hold a degree in
any relevant field be it engineering
computer science Information Technology
electrical and mechanical engineering
you can also be a graduate in statistics
or economics also you should have domain
knowledge in the field you are currently
working in or the role you're applying
for
the next important skill is that you
should have good hands-on experience
with programming languages such as R
Python and JavaScript this would help
you write programs to solve complex
problems
then you should have a good experience
working with databases and data analysis
tools such as writing SQL queries and
procedures knowledge of Microsoft Excel
IBM SPSS and Matlab to analyze Trends
forecast data and plan to drive accurate
insights
you must have a strong understanding of
statistics and machine learning
algorithms these include Concepts such
as hypothesis testing probability
distributions regression analysis and
various classification and clustering
techniques
and finally a data analyst should be
able to create different reports with
the help of charts and graphs using
several data visualization tools such as
Tableau and power bi they must have good
presentation skills as well this will
help them convey their ideas to clients
and stakeholders better
now that we have looked at the various
skills required to become a data analyst
let's now see the average annual salary
that a data analyst earns
here we can have a look at the salary
ranges of both in U.S and in India
so a data analyst in the United States
can earn a minimum salary of forty three
thousand dollars to a maximum of 85 000
dollars per year
in India you can earn anywhere between 1
lakh 98 000 Rupees to 9 lakh 24 000
rupees per annum
the data analyst role is in very high
demand with companies looking for
professionals who can handle their data
effectively and efficiently so let's
look at the different companies hiring
for the data analyst role
as you see here we have the American
e-commerce giant Amazon the American
multinational technology company
Microsoft Capital One which is one of
the largest banking companies in the US
then we have the popular retail company
Walmart then we have PayPal next we have
the internet and search engine giant
Google social media firms Facebook and
Twitter as well as apple and Bloomberg
with that let me now tell you how
simplylearn can help you learn data
analytics and guide you to become a data
analyst
so in a new tab I'll search for
simplylearn.com
then here on the search bar I look for
data analyst
let me now click on the first link which
is data analyst I'll open this in
another tab
as you can see on your screens this is
the data analyst Masters program and it
is in collaboration with IBM
on the right hand side you can see the
different courses that will be covered
as a part of the program you will learn
introduction to data analytics business
analytics with Excel then you have
Tableau followed by power bi later on in
the course you will learn programming
Basics and data analytics with python
then our programming and finally you
will get to work on a Capstone project
this is a kind of certificate you would
receive after completing the course it
will have your name along with IBM and
simply learn logo
these are some of the tools that we
covered in this program you will learn
Excel then numpy Panda scipy IBM Watson
power bi Tableau Python and r
the course advisor for this program is
Ronald Van Loon below you can see the
entire course curriculum and the
different courses that you will learn in
this program
also there are a few electives that you
can choose in this course
there's data science in real life
programming refresher industry master
class data analytics and there is SQL
training as well
let's quickly understand how important a
career in data analytics is and what the
future holds for Professionals in this
domain
let's take a look at the growth of data
so back in the early 2000s there was
relatively less data generated but with
a rapid rise in Technologies and with
the increase in the number of various
social media platforms and multinational
companies across the globe the
generation of data has increased by
Leaps and Bounds did you know that
according to the IDC the total volume of
data is expected to reach 175 zettabytes
in 2025
now that's a lot of data let's take a
look at how organizations leverage all
of this data
as you know there are zillions of
companies across the world these
companies generate loads of data on a
daily basis when I say data here it
simply refers to business information
customer Data customer feedback product
Innovations sales reports and profit
loss reports to name a few
companies utilize all of this data in a
wise way they use all of this
information to make crucial decisions
that can either hamper or boost their
businesses you might have heard of the
term data is the new oil well it
definitely is but only for organizations
analyze all the available data very well
then this oil is definitely valuable
and for that we have data analytics
organizations take the help of data
analytics to convert the available raw
data into meaningful insights
so what is data analytics technically
you can say it is a process wherein data
is collected from various sources then
cleaned which involves removing
irrelevant information and then finally
transformed into some meaningful
information that can be interpreted by
humans
various Technologies tools and
Frameworks are used in the analysis
process
as you might have heard of the term data
never sleeps well it surely doesn't
every millisecond some of the other data
is generated and this is a constant
process this process is only going to
increase in the near future with the
Advent of newer Technologies the data
analytics domain holds Paramount
importance in every sector
companies want to leverage on all the
generated big data and boost their
businesses
they need professionals who can play
with data and convert them into crucial
insights organizations are constantly on
the lookout for such candidates and this
opportunity will only increase as data
is only going to grow every second
so if you want to start your career in
this field or if you want to switch your
job role into a role in the data
analytics domain then we have a set of
job profiles that you can look at
we will look into six job roles in the
data analytics fields and learn what
each job role is all about the
responsibilities of a professional
working in that particular role the
skills required to get that particular
job the average annual salary of a
professional working in that role and
finally the company is hiring for that
role so let's start off
first we have the job role of a data
analyst
a data analyst is a person who collects
processes and performs statistical
analysis of large data sets
every business generates and collects
data be it marketing research sales
figures Logistics or Transportation
costs a data analyst will take this data
and figure out a variety of measures
such as how to price new materials how
to reduce Transportation costs or how to
deal with issues that cost the company
money they deal with data handling data
modeling and Reporting
now talking about their responsibilities
data analysts recognize and understand
the organization's goal they collaborate
with different team members such as
programmers business analysts engineers
and data scientists to identify
opportunities for solving business
problems
data analysts write complex SQL queries
scripts and store procedures to gather
and extract information from multiple
databases
they filter and clean data using
different modern tools and techniques
and make it ready for analysis they also
perform data mining from primary and
secondary data sources
data analysts identify analyze and
interpret Trends in complex data sets
this is done using statistical tools
such as R and SAS
another key responsibility of a data
analyst is to create summary reports and
build various data visualizations for
decision making and presenting it to the
stakeholders
next let us discuss the important skills
that you need to know to become a data
analyst
firstly you should have a bachelor's
degree in computer science or
information technology a master's degree
in computer applications or statistics
is also preferable
you must have a good understanding of
programming languages like R python
JavaScript and also understand SQL
in addition to that it is beneficial if
you have hands-on experience with
statistical and data analytics tools
such as SAS Miner Microsoft Excel and
sses
basic understanding of machine learning
and its algorithms would be an advantage
acquaint yourself with descriptive
predictive prescriptive and inferential
statistics
most importantly you need to have a good
working knowledge of various data
visualization software along with
presentation skills this will help you
pitch in your ideas and viewpoints to
the clients and stakeholders better
now talking about their salaries a data
analyst earns nearly 5 lakhs 23 000
rupees per annum in India while in the
United States they earn around 62 000
453 dollars per annum
let's now look at a few of the companies
hiring data analysts so as you can see
we have the American e-commerce giant
Amazon then we have Microsoft the
American online payment company PayPal
then we have Walmart Bloomberg and
Capital One so that was all about data
analyst
the next job role is of a business
analyst
business analysts help guide businesses
in improving products services and
software through data-driven solutions
they are responsible for Bridging the
Gap between I.T and business using data
analytics to evaluate processes
determine requirements and deliver
data-driven recommendations and reports
to Executives and stakeholders
business analysts are responsible for
creating new models that support
business decisions and come up with
initiatives and strategies to optimize
costs
now let us look at the various
responsibilities of a business analyst
business analysts have a good
understanding of the requirements for
business their vital role is to work in
accordance with relevant project
stakeholders to understand their
requirements and translate them into
details which the developers can
understand
they frequently interact with developers
and come up with a plan to design the
layout of a software application
they also run meetings with stakeholders
and other authorities they engage with
Business Leaders and users to understand
how data driven changes to products
Services software and Hardware can
improve efficiencies and add value
they ensured that the project is running
smoothly as per the requirements and the
design planned through user acceptance
and validation testing
they make sure all the features are
being incorporated into the application
findings where each requirement of the
client is mentioned in detail
now let us look at the skills required
for a ba
a bachelor's degree in the field of
science engineering or statistics or any
related domain will suffice
knowledge of programming languages such
as Python and Java is beneficial you
should be really good at writing complex
SQL queries and you should also have
knowledge of various business process
models
along with knowledge of programming
languages ideas about statistical
analysis and predictive modeling is
necessary
decision making strong analytical and
problem solving skills are necessary to
solve software and business issues you
also need to have excellent presentation
and communication skills both oral and
written
moving on to their salary a business
analyst is expected to earn around 7
lakh rupees per annum in India in the US
they earn nearly 68 346 dollars per
annum
iqia Del Phillips Honeywell the famous
American messaging platform WhatsApp the
UK based company Ernest and Young a few
of the companies hiring for business
analysts
up next we have the job role of a
database administrator
a database administrator is a
specialized computer systems
administrator who maintains a successful
database environment by directing or
performing all related activities to
keep the organization's data secure
they are responsible for storing
organizing and retrieving data from
several databases and data warehouses
their top responsibility is to maintain
data Integrity this means that database
administrator will ensure that the data
is secure from unauthorized access
moving on to their responsibilities
a database administrator develops
designs and maintains a database to
ensure that the data in it is properly
stored organized and managed well
they maintain data Integrity by avoiding
unauthorized taxes and they keep
databases up to date
they run tests and modify the existing
databases to ensure that they operate
reliably they also inform end users of
changes in databases and train them to
utilize systems
they need to cooperate with programmers
data analysts and the IT staffs to
ensure smooth running and maintenance of
databases
database administrators are responsible
for taking system backups in case of
power outages and other disasters so
they should have an efficient Disaster
Recovery plan
now let's have a look at their skills
to become a database administrator you
should have a bachelor's degree in
computer science or information
technology
knowledge of programming languages such
as python Java and Scala is important
you need to carry at least three to five
years of experience in data management
you need to have an understanding of
different databases such as Oracle DB
mongodb MySQL server and postgres SQL
also they should have an idea about
database design and writing SQL queries
finally you need to have a good
understanding of operating systems such
as Windows Mac OS and Linux along with
storage Technologies
talking about their salary a database
administrator in India can earn up to 4
lakh 97 000 rupees per annum in the US
they earn around 78 000 per annum
let's have a look at the companies
hiring for database administrators
so as you see here we have bookmyshow
Oracle the American MNC Intel Amazon
Robert Half and the New York Times to
name a few
fourth in the list of job rules we have
data engineer a data engineer is someone
who's involved in preparing data for
analytical and operational uses a data
engineer transforms data into useful
format for analysis they build and test
scalable Big Data ecosystems for
businesses a data engineer is an
intermediary between a data analyst and
a data scientist
now let's jump into their
responsibilities
data Engineers develop test and maintain
architectures they are responsible for
managing optimizing and monitoring data
retrievals storage and distribution
throughout the organization
they discover opportunities for data
acquisition fine Trends in data sets and
develop algorithms to help make raw data
more useful to the Enterprise
data engineers build large data
warehouses using ETL for storing and
retrieving data
they also recommend ways to improve data
quality and efficiency along with
building algorithms to help give easier
access to Raw data
data Engineers often work with big data
and submit their reports to data
scientists for analysis purpose they
need to recommend and sometimes
Implement ways to improve data
reliability efficiency and quality
moving on to the skills of a data
engineer
a data engineer should hold a bachelor's
degree in computer science or
information technology
they should have good hands-on
experience with python R and Java
also data Engineers should be well
versed with big data Technologies such
as Hadoop Apache spark Scala Cassandra
and mongodb
data warehousing and detail experience
are essential to this position along
with in-depth knowledge of SQL and other
database Solutions
basic knowledge of statistical analysis
will be an advantage along with idea
about operating systems
here is what a data engineer can earn so
in India a data engineer can earn up to
8 lakhs 85 000 rupees per annum while
they can earn around 103 000 dollars a
year in the USA
we have cab Gemini shot his dog the
American provider of stock photography
Spotify Accenture genpact and Facebook
hiring data engineers
the next exciting job role is of a data
scientist
a data scientist is a professional who
uses statistical methods data analysis
techniques machine learning and related
Concepts in order to understand and
analyze data to draw business
conclusions they make sense to messy and
unstructured data and bring value out of
it they employ techniques and theories
drawn from many fields within the
context of mathematics statistics
computer science and information science
a data scientist understands the
challenges in business and comes up with
the best Solutions using modern tools
and techniques to analyze visualize and
build prediction models to make business
decisions
let us now look at their
responsibilities in the industries
data scientists clean process and
manipulate data using several data
analytics tools they perform ad hoc data
mining collect large sets of structured
and unstructured data from disparate
sources
they design and evaluate Advanced
statistical models to work on Big Data
they also create automated anomaly
detection systems and keep constant
track of their performance
data scientists interpret the analysis
of big data to discover Solutions and
opportunities
a data scientist takes input from data
analysts and Engineers to formulate the
results
they use visualization packages and
tools to create reports and dashboards
for Relevant stakeholders they also
adopt new business models and approaches
apart from this they regularly built
predictor models and machine learning
algorithms
now moving on to the skills of a data
scientist
a bachelor's degree in computer science
or information technology will be fine
but a master's degree in the field of
data science will hold a major advantage
you also need to have a good experience
in the analytics domain
you should be proficient in programming
languages such as python Java and C plus
knowledge of Perl will also be an
advantage
familiarity with Apache Hive big and
Apache spark is necessary along with the
knowledge of Hadoop
in addition to knowing programming
languages you also need to know SQL
machine learning and deep learning
data visualization and bi skills are
necessary for creating reports and
dashboards you should also be able to
communicate and present information and
ideas properly
now talking about their salary a data
scientist in India can expect an annual
salary of 10 lakhs 47 000 rupees per
year
meanwhile in the US they can earn up two
hundred and thirteen thousand dollars
per annum that's a lot of money
from the many companies hiring for data
scientists here we have a few companies
named they are yet again Amazon Citibank
Apple Google the Japanese electronic
Commerce and online retailing company
Rakuten and Facebook
and finally we have machine learning
engineer machine learning Engineers are
professionals who develop intelligent
machines that can learn from vast
amounts of data and apply knowledge
without human intervention
they use different algorithms and
statistical modeling to make sense of
data they design and develop machine
learning and deep learning algorithms
their main goal is to create
self-running software
let's have a look at the
responsibilities of a machine learning
engineer
machine learning Engineers research
design and develop machine Learning
Systems they use exceptional
mathematical skills in order to perform
faster computations and work with
algorithms to create sophisticated
models
they perform a b testing and use data
modeling to fine-tune the results
they use data modeling and evaluation
strategy to find hidden patterns and
predict unseen instances
machine learning Engineers work closely
with data Engineers to build data
pipelines and interact with stakeholders
to get a Clarity on the requirements
most importantly they analyze complex
data sets to verify data quality perform
model tests and experiments choose to
implement the right machine learning
algorithm and select the right training
data sets
moving on to their skills
a machine learning engineer should have
a degree in computer science and
information technology they should have
an advanced degree in computer science
or maths
in addition to this they should also
have experience in the same domain
they should be proficient in programming
languages such as python RC plus and
Java
knowledge of Statistics probability and
linear algebra is necessary as all the
machine learning algorithms have been
derived from mathematics also having an
idea of signal processing would be
beneficial
machine learning Engineers need to have
a good understanding of data
manipulation and machine learning
libraries such as numpy Panda
scikit-learn Etc
they should have good oral and written
communication skills
let us now have a look at their salary
structure a machine learning engineer
earns 8 lakh rupees per annum in India
while in the US they can earn around 114
000 dollars a year now that's a whopping
amount isn't it
let's have a look at the company's
hiring machine learning engineers
so as you see we have Amazon Microsoft
Oracle Salesforce Rapido and Accenture
to name a few
that was all about the job role of a
machine learning engineer
now that we have seen the different job
roles in the field of data analytics
let's also go ahead and see how an ideal
resume of a data analyst should look
like
seen on your screens is a sample
resuming of a data analyst you can grab
some ideas from this and incorporate
them in your resume
nowadays it's quite common to have a
professional photograph of yours on the
resume you can go ahead and have that
then your name in bold followed by your
contact details like email ID and phone
number
then moving on you would have to write a
summary briefly explain your current job
role and what you're looking for in the
future having a LinkedIn profile link
works well these days employers can just
go ahead and look at your profile and
gauge you well
make sure to have an active LinkedIn
profile
in addition to LinkedIn profile it's
also good to have a GitHub profile link
which can show your coding or other
technical skills
if it's impressive enough then a lot of
times the rest of your resume is just
secondary
as I mentioned this is a resume of a
data analyst so as you can see in the
summary here we have just spoken about
the basic responsibilities of a data
analyst
moving on to the experience part you
have to write the job title and below
that you can mention the company and the
tenure accordingly here you would have
to give a brief description of
achievements in the organization any
relevant accomplishments related to the
job you're applying for the tools and
the various Technologies you have worked
with
so in the sample you can see we have
spoken about data visualization using R
in Tableau next we have spoken about how
the candidate has worked with other
teams for a Better Business outcome
most of the data analysts use SQL and
Excel to handle data for reporting and
database maintenance and we have
mentioned that here as well
do make sure that you always specify the
tools you use
then you can also mention if you have
worked on improving data delivery for
example here we have spoken about
developing and optimizing SQL queries
data aggregations and ETL to improve
data delivery
finally you can speak a bit about your
reporting skills and if needed elaborate
on it
usually professionals would have worked
in a similar domain before becoming a
data analyst here we have taken the role
of a statistical assistant as the first
job since it's easier for a candidate
with this job role to shift into the
data analytics field nevertheless you
all can still mention your prior
experience here be it in any domain
under the responsibilities for this job
role we have given Basics such as coding
data prior to computer entry compiling
statistics from various reports
Computing and analyzing data and find me
some visualization and Reporting
moving to the education here you can
mention the name of your degree and the
university name if you have a post
graduation well in good you can list
both the degrees here also if you have
any certifications you can mention them
here under the education category
now moving to the skills depending on
your skills and your choice you can
either shift this part to the beginning
of the resume or have it here
as you see on your screens this is just
a different way of displaying your skill
sets you can have all the five stars
colored if you are excellent in that
particular tool or language
as you see it's crystal clear as to what
the candidates strong areas are
you can have various categories like
shown
for example under software development
you can list the languages that you know
and how proficient you are in those
particular languages
it's clear that the candidate knows
python better than JavaScript here so
the employer gets a clearer idea about
the skills you possess and the depth of
it
similarly you can mention the databases
as well the few mentioned here are more
or less a requirement to become a data
analyst at least SQL is a must
not to forget data visualization is also
very important when it comes to the job
role of a data analyst mention the tools
you know here and similarly give
yourself a rating out of five five stars
shaded being the highest
here we have mentioned Tableau and Excel
which are more than sufficient to become
a data analyst
moving to the non-technical skills you
can mention the languages you know here
here we have taken English and German in
addition to the languages you can also
feel free to mention the extracurricular
activities that you are good at
so this is how an ideal resume of a data
analyst should look like you can alter
it according to your achievements skills
and experience welcome to this session
on top 10 skills to become a data
analyst before diving into our topic
let's quickly speak about the job role
of a data analyst in this 21st century
data analytics is used in every sector
Beach and organizations where meaningful
insights are drawn pertaining to the
growth of the company or be it in
fighting the ongoing pandemic covet 19.
data Finds Its importance everywhere
speaking of the role of a data analyst
he or she is a skills professional who
is responsible for collecting and
processing data they perform analysis on
large data sets they also deal with data
handling data modeling and reporting a
data analyst understands the trends and
insights that are revealed in massive
data sets so if you want to become a
data analyst then there are a few skills
that you need to possess let's have a
look at the top 10 skills that can help
you back the position of a data analyst
here we will look into both Technical
and non-technical skills at number 10 we
have mathematics
data analysts work with a lot of
structured and unstructured data in
order to analyze and understand all the
acquired data a strong foundation in
mathematics is essential
most of the data analysis will use
linear algebra statistics probability
and calculus for performing analysis and
for The Logical examination of data
hypothesis testing such as the null
hypothesis and Alternate hypothesis
analysis is another crucial task that
data analysts perform to ensure that the
data they have collected is relevant for
analysis
they need to perform z-test t-test and
chi-square test to make sure the sample
data is good for analysis also data
analysts build machine learning models
for solving business problems using
classification regression and clustering
algorithms so to understand the working
of these algorithms knowledge of
mathematics is compulsory
moving to number nine we have the big
data tools and Frameworks
data analysts deal with complex and
inaccurate data that is really huge in
volume now to handle this data they need
to possess Big Data technology skills
such as Hadoop and the tools that are
part of its ecosystem
Hadoop provides the Hadoop distribution
file system to store data in several
chunks scoop is popularly used as a data
ingestion tool for extracting data from
hdfs onto relational databases
data analysts use hbase which is a
column oriented database for processing
semi-structured data
there are other Frameworks such as
Apache big and high for processing and
analyzing data using Pig Latin scripts
and Hive query language it would be an
advantage for a data analyst to have an
idea about these tools and Frameworks
at number eight we have data cleaning
and data wrangling
in this modern era of internet and
social media data is being generated
every second and often this data is
noisy and messy containing missing
values data is also often unstructured
and this could be a problem for data
analysts to perform analysis on such
data so they need to pre-process the
data and clean it using various tools
and techniques to make it fit for
analysis data analysts must transform
the data into the right format for
carrying out Analytics
they should also have data manipulation
and data mining skills to find out
unseen Trends and patterns from the data
some of the tools they should have
knowledge of are open refine and
Trifecta Wrangler they need to have
hands-on experience in certain numerical
computation and data manipulation
libraries such as numpy pandas declare
sci-fi anti-der
at number 7 we have bi tools for data
visualization
in order to understand the complexities
of business and derive the desired
solution data analysts should have an
idea about business intelligence tools
business intelligence is a process to
analyze and visualize vast volumes of
data it helps in creating reports and
dashboards to better understand the
trends in data
VI tools help data analysts to sort and
filter the data perform data
manipulation by joining multiple data
sets and build different charts and
graphs to present the data in a
pictorial format it also helps them to
focus the data to make future
predictions the reports and dashboards
created using bi tools can help data
analysts convey their ideas to clients
and stakeholders
some of the popular bi tools used in
analytics are power bi Tableau click
View and SAS bi all these tools feature
in the gutner magic quadrant for 2020
for business intelligence and Analytics
at number 6 we have Microsoft Excel and
ETL tools
every data analyst should possess a good
working knowledge of Microsoft Excel
Excel is the most preferred tool for
analytics that is commonly used by
managers across the globe Microsoft
Excel has really good features to
manipulate and analyze structured data
that is in the form of rows and columns
it provides a lot of inbuils numerical
and text functions you also have the
advantage of creating pivot tables and
pivot shots along with creating
different charts and graphs for building
a report you can explore Advanced
features such as Excel macros good
knowledge of data warehousing and ETL
tools is important data analysts often
gather data from several data sources
then they manipulate and transform data
using different techniques and finally
they load the data to a data warehouse
for easy access some of the popular ETL
and data warehousing tools are
Informatica and talent
at number five we have programming
languages
data analysts should have excellent
Hands-On programming knowledge for
solving complex business problems
they need to know programming languages
such as python R SAS and Java Python and
R are the most widely used languages in
the field of data analytics and machine
learning both Python and R are open
source programming languages they are
easy to learn and Implement python has
built a mathematical functions regular
expressions and libraries like pandas
numpy matplotlib and c bond for data
analysis
R supports packages such as plier deeply
tidier tidy verse GG plot and lattice
for manipulating and visualizing the
data SAS is another preferred
programming software for statistical
analysis and model building while Java
is mainly suitable for writing
user-defined methods and object-oriented
programming
at number 4 we have the most important
skill for any data analyst which is
database and SQL
the database is a storage container
where companies store huge volumes of
data organizations deal with vast
volumes of structured and
semi-structured data on a daily basis
this data is stored in relational and
non-relational databases
in order to retrieve process and
manipulate the data from such databases
data analysts should use rdbms and nosql
databases such as Microsoft SQL Server
MySQL IBM db2 mongodb and postgres SQL
they should know how to write SQL
queries using commands such as select
insert update delete drop and truncate
data analysts must have advanced
querying skills like implementing where
and having Clauses to filter the data
using built-in SQL functions joining
tables and writing stored procedures to
automate complex tasks
those were all the technical skills that
are required to be possessed by a data
analyst now that you had a look at all
the technical skills you must note that
the role of a data analyst is a blend of
both Technical and non-technical you
need to focus on certain non-technical
skills as well to become a full-fledged
data analyst so let's now move on and
look at what non-technical skills are
required to become a data analyst if you
enjoy watching informative Tech videos
like this one consider subscribing to
Simply learns channel to stay up to date
on the trending Technologies and hit the
Bell icon to never miss an update in the
future
at number 3 we have problem solving
data analysts should be prepared to face
several barriers on a daily basis being
able to problem solve a way out of
obstructions is an essential skill
there can be multiple issues like budget
constraints short deadlines and so on
these problems would require you to come
up with innovative solutions hence no
matter what the circumstances having
strong problem solving skills will
always be a virtue
being a data analyst also requires you
to think like an analyst analytical
skills also known as logical thinking
refers to breaking down problems
logically
having strong analytical skills will
help you arrive at a buy solution in any
situation based on information and facts
complex problems can be solved this way
critical thinking also goes hand in hand
with analytical skills critical thinking
is a self-guided and self-disciplined
way of thinking which attempts to reason
in a fair-minded way
as a data analyst critical thinking will
help you stay grounded when you are
searching for a solution to a tricky
problem you should also be capable of
making well-thought independent
decisions there are a number of tips
that can help you improve your critical
thinking skills
moving on to number two we have business
knowledge
business knowledge or domain knowledge
refers to holding a sound understanding
of the domain you are working in this
knowledge is different for different
organizations for example if you're
working in the automobile industry you
might need to understand how systems
work and how its output can be
potentially influenced irrespective of
where you work you need to have good
business knowledge and understand what
you're analyzing
you should be in a position to
understand the various business problems
and how to solve them
only if you have a strong industry
knowledge can you try to improve the
business if you keep yourself updated
with market trends you can understand
where your company stands and
accordingly build a business model this
will also help you assist your business
in exploring greener pastures
so now let's have a look at what's at
number one here we have communication
and presentation
this might seem like a very common skill
but it's not as easy as it sounds data
analysts interact with various teams to
ensure proper implementation of business
requirements and for this collaboration
to run smoothly communication is very
important
the ability to communicate in numerous
ways is a key data analyst skill this
includes writing speaking presenting and
listening
written communication is crucial you
will be required to write up your
analysis and provide regular
documentations of it
data visualization and presentation
skills go hand in hand presenting your
analysis results to various team members
and stakeholders holds Paramount
importance you might also need to be in
a state to explain a complex topic to
non-technical teammates
there is no point of a great analysis if
you are unable to explain it through
your presentation
you can Master the presentation skill
with regular practice until you are
comfortable to explain in front of a
bunch of people
having said that you should also be very
crisp with your presentations you need
to be clear direct and focus on the
results rather than deviating from your
topic like they say you need to hit the
bullseye
before I start off with the top 10 data
analysis tools I'd like to talk a bit
about data analysis
so have you ever wondered why data
analysis is important
there are zillions of companies across
the world all these companies generate a
lot of data they literally work with
this generated data
these companies depend on data to make
crucial decisions which can impact their
businesses
data in its raw format has to be
converted into meaningful information
which can then be used by organizations
this is done by analyzing the generated
data and for this we have data analysis
so what is data analysis
data analysis is not just a single step
but a set of processes it is the process
of collecting data then cleaning it when
I say cleaning it simply means removing
the air 11 data and then this data is
transformed into meaningful information
we can simply relate this process to how
you make a jigsaw puzzle just like how
you gather all the pieces together and
fit them accordingly to bring out a
beautiful picture
data analysis also works on almost the
same grounds
to achieve the goals of data analysis we
use a number of data analysis tools
companies rely on these tools to gather
and transform their data into meaningful
insights so which tool should you choose
to analyze your data which tool should
you learn if you want to make a career
in this field we will answer that in
this session after extensor research we
have come up with these top 10 data
analysis tools here we will look at the
features of each of these tools and the
companies using them so let's start off
at number 10 we have Microsoft Excel all
of us would have used Microsoft Excel at
some point right it is easy to use and
one of the best tools for data analysis
developed by Microsoft Excel is
basically a spreadsheet program using
Excel you can create grids of numbers
text and formulas it is one of the
widely used tools be it in a small or
flat setup
the interface of Microsoft Excel looks
like this
let's now move on to the features of
excel
firstly Excel works with almost every
other piece of software in office we can
easily add Excel spreadsheets to Word
documents and PowerPoint presentations
to create more visually appealing
reports or presentations
the windows version of excel supports
programming through Microsoft's Visual
Basic for applications VBA
programming with VBA allows spreadsheet
manipulation that is difficult with
standard spreadsheet techniques
in addition to this the user can
automate tasks such as formatting or
date organization in VBA
one of the biggest benefits of excel is
its ability to organize large amounts of
data into orderly logical spreadsheets
and charts by doing so it's a lot easier
to analyze data especially while
creating graphs and other visual data
representations the visualization can be
generated from specified group of cells
those were few of the features of
Microsoft Excel
let's now have a look at the companies
using it most of the organizations today
use Excel few of them that use it for
analysis are the UK based company Ernest
and Young then we have Urban Pro Wipro
and Amazon
moving on to our next data analysis tool
at number nine we have rapidminer
a data science software platform
rapidminer provides an integrated
environment for data preparation
analysis machine learning and deep
learning
it is used in almost every business and
Commercial sector rapidminer also
supports all the steps of the machine
learning process
seen on your screens is the interface or
rapidminer
moving on to the features of rapidminer
firstly it offers the ability to drag
and drop it is very convenient to just
drag drop some columns as you are
exploring a data set and working on some
analysis
rapidminer allows the usage of any data
and it also gives an opportunity to
create models which are used as a basis
for decision making and formulation of
strategies
it has data exploration features such as
graphs descriptive statistics and
visualization which allows users to get
valuable insights
it also has more than 1500 operators for
every data transformation and Analysis
task
let's now have a look at the companies
using rapidminer we have the Caribbean
Airline lever Islands Air transport next
we have the United Health Group the
American online payment company PayPal
and the Austrian Telecom company
MobileComm so that was all about
rapidminer now let's see which tool we
have at number eight
we have talent at number eight
Talent is an open source software
platform which offers data integration
and management it specializes in Big
Data integration Talent is available
both in open source and premium versions
it is one of the best tools for cloud
computing and Big Data integration
the interface of talent is as seen on
your screens
moving on to the features of talent
firstly automation is one of the great
bones Talent offers it even maintains
the tasks for the users this helps with
quick deployment and development
it also offers open source tools Stalin
lets you download these tools for free
the development costs reduce
significantly as the process is
gradually speed up
Talent provides a unified platform it
allows you to integrate with many
databases SAS and other Technologies
with the help of the data integration
platform you can build flat files
relational databases and Cloud apps 10
times faster
those were the features of Talon the
companies using Talent are Air France
L'Oreal cab Gemini and the American
multinational Pisa restaurant chain
Domino's
next on the list at 7 we have nine
Constance information Miner on nime is a
free and open source data analytics
reporting and integration platform
it can integrate various components for
machine learning and data mining through
its modular data pipelining concept nime
has been used in pharmaceutical research
and other areas like CRM customer data
analysis business intelligence text
Mining and financial data analysis
here is how the interface of nime
application looks like
now coming to the nine features
nine provides an interactive graphical
user interface to create visual
workflows using the drag and drop
feature
use of jdbc allows assembly of nodes
blending different data sources
including pre-processing such as ETL
that is extraction transformation
loading for modeling data analysis and
visualization with minimal programming
it supports multi-threaded in-memory
data processing 9 allows users to
visually create data flows selectively
execute some or all analysis steps and
later inspect the results models and
interactive views
9 server automates workflow execution
and supports team-based collaboration
nime integrates various other open
source projects such as machine learning
algorithms from Becca H2O Caris Park and
our project
9 allows analysis of 300 million custom
addresses 20 million cell images and 10
million molecular structures
some of the companies hiring for nime
are United Health Group asml fractal
analytics atos and LEGO Group
let's now move on to the next tool we
have SAS at number six
SARS facilitates analysis reporting and
predictive modeling with the help of
powerful visualizations and dashboards
in SAS data is extracted and categorized
which helps in identifying and analyzing
data patterns
as you can see on your screens this is
how the interface looks like
moving on to the features of SAS
using SAS better analysis of data is
achieved by using automatic code
generation as a SQL
SAS allows you to access through
Microsoft Office by letting you create
reports using it and by Distributing
them through it
SAS helps with an easy understanding of
complex data and allows you to create
interactive dashboards and reports
let's now have a look at the companies
using SAS we have companies like genpact
iqia Accenture and IBM to name a few
that was all about SAS
so for all those who joined in late let
me just quickly repeat our list at
number 10 we have Microsoft Excel then
at number nine we have rapidminer at
number eight we have talent at number
seven we have nine and at number six we
have SAS so far do you all agree with
this list let us know in the comment
section below let's now move on to the
next five Tools in our list
so at number five we have both R and
python yes we have two of them in the
fifth position
R is a programming language which is
used for analysis as well it has
traditionally been used in academics and
research python is a high level
programming language which has a python
data analysis Library it is used for
everything starting from importing data
from Excel spreadsheets to processing
them for analysis
this is the interface of r
next up is the interface of the Python
Jupiter notebook
let's now move on to the features of
both R and python
when it comes to the availability of R
and python it is very easy both are in
Python are completely free hence it can
be used without any license
I used to compute everything in memory
and hence the computations were limited
but now it has changed both are in
Python have options for parallel
computations and good data handling
capabilities
as mentioned earlier as both R and
python are open in nature all the latest
features are available without any delay
moving on to the companies using R we
have Uber Google Facebook to name a few
python is used by many companies again
to name a few we have Amazon Google and
the American photo and video sharing
social networking service Instagram
that was all about R in Python
at number 4 we have Apache Spark
Apache spark is an open source engine
developed specifically for handling
large-scale data processing and
Analytics
spark offers the ability to access data
in a variety of sources including Hadoop
distributed file system htfs openstack
Swift Amazon S3 and Cassandra
it allows you to store and process data
in real time across various clusters of
computers using simple programming
constructs
Apache spark is designed to accelerate
analytics on Hadoop while providing a
complete Suite of complementary tools
that include a fully featured machine
learning library a graph processing
engine and stream processing
so this is how the interface of Apache
spark looks like
now let's look at the important features
of Apache Spark
spark stores data in the ram hence it
can access the data quickly and
accelerate the speed of analytics spark
helps to run an application in a Hadoop
cluster up to 100 times faster in memory
and 10 times faster when running on disk
it supports multiple languages and
allows the developers to write
applications in Java Scala r or python
Spar comes up with 80 high-level
operators for interactive querying spark
code for batch processing join stream
against historical data or run ad hoc
queries on stream state
analytics can be performed better as
spark has a rich set of SQL queries
machine learning algorithms complex
analytics Etc Apache spark provides fall
tolerance through spark rdd spark
resilient distributed data sets are
designed to handle the failure of any
worker node in the cluster thus it
ensures that the loss of data reduces to
zero
can Viva Netflix IQ via Lockheed Martin
and eBay are some of the companies that
use Apache spark on a daily basis
at number 3 we have another important
growing data analysis tool that is Click
View
click view software is a product of
Click for business intelligence and data
visualization click view is a business
Discovery platform that provides
self-service bi for all business users
and organizations
with click view you can analyze data and
use your data discoveries to support
decision making
clickview is a leading business
intelligence and analytics platform in
Gartner magic quadrant
on the screen you can see how the
interface of Click view looks like
now talking about its features
clickview provides interactive guided
analytics with in-memory storage
technology during the process of data
Discovery and interpretation of
collected data the clickview software
helps the user by suggesting possible
interpretations
clickview uses a new patent in-memory
architecture for data storage all the
data from the different sources is
loaded in the ram of the system and it
is ready to be retrieved from there
it has the capability of efficient
social and mobile data discovery
social data Discovery offers to share
individual Data Insights within groups
or out of it
a user can add annotations as an
addition to someone else's insights on a
particular data report clickview
supports mobile data Discovery within an
HTML file enabled touch feature which
lets the user search the data and
conduct data Discovery interactively and
explore other server-based applications
clickview performs olap and ETL features
to perform analytical operations extract
data from multiple sources transform it
for usage and load it to a data
warehouse
the companies that can help you start
your career and click view are
Mercedes-Benz capgemini Citibank
cognizant and Accenture to name a few
at number two we have power bi
power bi is a business analytics
solution that lets you visualize your
data and share insights across your
organization or embed them in your app a
website
it can connect to hundreds of data
sources and bring your data to life with
live dashboards and reports
power bi is the collective name for a
combination of cloud-based apps and
services that help organizations collate
manage and analyze data from a variety
of sources through a user-friendly
interface
power bi is built on the foundation of
Microsoft Excel and has several
components such as Windows desktop
application called Power bi desktop and
online software resist service called
Power bi service mobile power bi apps
available on Windows phones and tablets
as well as for IOS and Android devices
here is how the power bi interface looks
like as you can see there is a visually
interactive sales report with different
charts and graphs
moving on to the features of power bi
it has an easy drag and drop
functionality with features that make
data visually appealing you can create
reports without having the knowledge of
any programming language power bi helps
users see not only what's happened in
the past and what's happening in the
present but also what might happen in
the future
it offers a wide range of detailed and
attractive visualizations to create
reports and dashboards you can select
several charts and graphs from the
visualization pane power bi has machine
learning capabilities with which it can
spot patterns in data and use those
patterns to make informed predictions
and run what-if scenarios
power bi supports multiple data sources
such as Excel Tech CSV Oracle SQL Server
PDF and XML files the platform
integrates with other popular business
management tools like SharePoint Office
365 and Dynamics 365 as well as other
non-microsoft products like spark Hadoop
Google analytics sap Salesforce and
MailChimp
some of the companies using power bir
Adobe AXA Carlsberg capgemini and Nestle
moving on to the next tool so any
guesses as to what we have at number one
you can comment in the chat section
below
finally on the top of the pyramid we
have tableau
Gartner's magic quadrant of 2020
classified Tableau as a leader in
business intelligence and data analysis
Tableau interactive data visualization
software company was founded in Jan 2003
in Mountain View California
Tableau is a data visualization software
that is used for data science and
business intelligence it can create a
wide range of different visualization to
interactively present the data and
showcase insights
the important products of Tableau are
Tableau desktop Tableau public Tableau
server Tableau online and Tableau reader
this is how the interface of Tableau
desktop looks like
now coming to the features of tableau
data analysis is very fast with Tableau
and the visualizations created are in
the form of dashboards and worksheets
Tableau delivers interactive dashboards
that support insights on the Fly
it can translate queries to
visualizations and import all ranges and
sizes of data writing simple SQL queries
can help join multiple data sets and
then build reports out of it you can
create transparent filters parameters
and highlighters
Tableau allows you to ask questions spot
Trends and identify opportunities with
the help of Tableau online you can
connect with Cloud databases Amazon
redshift and Google bigquery
the company is using Tableau are
Deloitte Adobe Cisco LinkedIn and the
American e-commerce giant Amazon to name
a few
and there you go those are the top 10
data analysis tools
let's now have a question and answer
session please feel free to post your
queries in the comments section and
we'll respond in the chat
before the question answer session let's
recap quickly in the meanwhile you all
can post your questions in the comment
section below
so at number 10 we have Microsoft Excel
then at number nine we have rapidminer
at number eight we have talent at number
seven we have nine at number six we have
SAS R and python at number five Apache
spark at number four
click View at number three power bi at
number two and finally we have Tableau
topping the list at number one
currently all of us are living in an
information driven world and
organizations rely on data for various
decision makings this in turn provides a
lot of job opportunities for candidates
who can play with data out of the many
job roles in the field of data science
the two popular ones are that of a data
scientist and a data analyst
haven't we all wondered at some point as
to what the exact differences between
these two job roles oh wait are they the
same well they define various ways and
you will see how in this video
we will start off by looking at the job
descriptions of both the data scientist
and the data analyst then we will look
at their responsibilities and skill set
we will also have a look at their salary
structure and the various companies
hiring for these professionals so
without further Ado let's get started
let's have a look at the job description
now
a data scientist is a professional who
uses different statistical methods data
analysis techniques and machine learning
in order to understand and analyze data
in order to arrive at business
conclusions
they proactively fetch information from
a plethora of sources and analyze it for
better understanding about how the
business performs and they also build AI
tools that automate certain processes
within the company
they derive meaning out of messy and
unstructured data a data scientist is
usually a senior most member in the team
moving to the description of data
analyst a data analyst is responsible to
collect process and perform analysis on
large data sets they deal with data
handling data modeling and Reporting
they are sometimes the entry-level
members into the data analytics team
they bring technical expertise to ensure
the quality and accuracy of the data
then process design and present it in
ways to help people businesses and
organizations make better decisions
after a few years of experience data
analysts can move into the roles of a
data engineer and a data scientist
now that we have understood the job
descriptions let's go ahead and
understand the various roles and
responsibilities of a data scientist and
a data analyst
firstly data scientists are responsible
for performing cleaning processing and
manipulation of data using several data
analytics tools they also perform ad hoc
Data Mining and collect large sets of
structure and unstructured data from a
number of sources
secondly data scientists interpret the
data using various statistical methods
they design and evaluate Advanced
statistical models to work on Big Data
thirdly data scientists regularly build
predictive models and machine learning
algorithms to work on past volumes of
data
lastly data scientists use visualization
packages and tools to create reports and
dashboards for Relevant stakeholders
they also work with data analysts and
data Engineers to formulate the analysis
results
let's now have a look at the various
responsibilities of a data analyst
the first responsibility of a data
analyst is to recognize and understand
the company's goal this in turn helps in
streamlining the whole analysis process
they are required to assess the
available resources comprehend the
business problem and gather the right
set of data this step is done by
collaborating with different team
members such as data scientists business
analysts and programmers
they gather data from various databases
and warehouses through querying they
write complex SQL queries and scripts to
gather and extract information
data analysts also filter and clean data
to get the required information they are
responsible for data mining as well data
is mined from various sources and then
organized in order to obtain new
information from it data analysts
identify and analyze Trends in complex
data sets using various statistical
tools
a data analyst is also responsible for
creating summary reports for the
leadership team so that they can make
timely decisions data analysts use
multiple data visualization tools for
achieving this
in order to achieve all the about
mentioned responsibilities data
scientists and data analysts are
required to possess a rich skill set
let's now have a look at few of the most
important skills required to back the
position of a data scientist
the basic requirement to become a data
scientist is that you must have a
bachelor's degree in computer science or
information technology but a master's
degree in the field of data science will
be a lot more beneficial
you also need to have a good experience
in the analytics domain as I mentioned
before this role is a senior role and to
get here the right amount of experiences
a must
let's have a look at the tools you need
to know
ledge of Microsoft Excel is good it is
one of the most basic requirements
speaking of programming languages you
should be good at python C plus plus and
Java knowledge of pearl is a brony point
you should also be proficient in SQL
as we discussed earlier data scientists
work on building machine learning
algorithms hence you need to have a good
knowledge of machine learning and deep
learning
familiarity with Apache spark Apache
Hive and Apache pig is necessary along
with the knowledge of Hadoop
data visualization and bi skills are
necessary for creating reports and
dashboards you should also be able to
communicate and present information and
ideas clearly
so these are the skills required to
become a data scientist
if you want to explore the role of a
data analyst then you should hold a
degree in any relevant field be it
engineering and computer science
information technology or electrical
engineering you can also be a graduate
in statistics or economics
moving on to the tools once again you
should be familiar with Microsoft Excel
the next important skill is that you
should have good hands-on experience
with programming languages such as
python R and JavaScript this would help
you write programs to solve complex
problems
you should also have a good knowledge of
statistical and data analytics tools
such as SAS Miner and sses
you must be able to write various SQL
queries and procedures
in addition to these you must have a
strong understanding of statistics and
machine learning algorithms these
include Concepts such as hypothesis
testing probability distributions and
various classification and clustering
techniques
most importantly a data analyst should
be able to create visually appealing
reports with the help of charts and
graphs using several data visualization
tools such as power bi and Tableau they
must possess good presentation skills as
well in order to convey their ideas to
the clients and stakeholders in a better
way
so these are the skills that are
required to become a data analyst
let's now have a look at the annual
salary range of a data scientist and a
data analyst both in the US and in India
in the United States a data scientist
can earn a minimum salary of sixty one
thousand dollars to a maximum of 136
thousand dollars per year
meanwhile in India a data scientist can
earn a minimum salary of 3 lakh 47 000
Rupees to a maximum of 2 million rupees
per annum
a data analyst in the United States can
earn a minimum salary of forty three
thousand dollars to a maximum of 85 000
per year
in India you can earn anywhere between 1
lakh 98 000 Rupees to 9 lakh 24 000
rupees per annum
let's now take a look at the various
companies hiring data scientists and
data analysts
here we have Amazon the internet and
search engine giant Google Deloitte the
American multinational technology
company Microsoft then we have apple and
the American social media web and mobile
application company Pinterest hiring
data scientists
from the many companies hiring data
analysts here we once again have Amazon
then we have the popular retail company
Walmart Robert huff and atnt next we
have social media firms Facebook and
Twitter
as you might have heard of the term data
never sleeps well it surely doesn't and
not only that but it also brings in a
number of job opportunities with it the
job group in this domain is Limitless
and data will only continue to grow
let's have a look at few of the stats
now
well IBM had predicted that by this year
that is 2020 the number of job listings
in the field of data science and
analytics will increase by 3 lakh 64
000.
and we had a U.S Bureau of Labor
Statistics predicting that there will be
a rise in the data science needs and
this in turn will create 11.5 million
job openings by 2026. now that's a big
number isn't it
and if you are aspiring to become a data
scientist then you are on the right path
as there is a huge demand for this rule
according to Deloitte the United States
is projected to face a shortfall of 2
lakh 50 000 data scientists by 2024.
that's barely just in four more years
all these stats prove that the job
demand in the field of data science and
data analytics is here to stay
professionals belonging to this domain
are in high demand all across the globe
now let's have a look at the Google
search trends for data scientists and
data analysts
as you see the blue color depex data
scientist and the Red Data analysts
both the search trends go hand in hand
and over time the search term data
analyst is higher but the search term of
that of a data scientist is experiencing
a steady rise
let's have a look at the worldwide
YouTube search Trend as well
as you can see on your screens it looks
like people are more keen on exploring
the job role of a data scientist and
looks like they want to learn more about
the job role of a data scientist
compared to that of a data analyst
nevertheless the search term data
analyst is also right there in the
competition
we're going to look at three very
important data related roles in the
field of data science and then we're
going to pit them against each other so
welcome to data scientist versus data
analyst versus data engineer now let's
have a look at what's in store for you
firstly we'll talk about the job
descriptions the skill sets required for
each role the salary the roles and
responsibilities and the companies
hiring for these positions so now let's
have a look at each of these roles in
detail first off let's have a look at
data scientists now a data scientist is
able to create machine learning based
tools or processes within the company
now they use Advanced Data techniques
such as clustering division trees neural
networks and so on so that they can
derive business conclusions they are the
senior most member in the team which
involves a data engineer as well as a
data analyst now they need to have
in-depth knowledge of Statistics data
handling and machine learning they also
take inputs from data Engineers as well
as analysts so that they can formulate
actionable insights for the business now
data scientist also needs to have the
same skills as a data analyst and an
engineer but needs to have a lot more
in-depth knowledge and expertise with
these skills next up we have data
analyst now a data analyst is someone
who's able to translate numeric data
into a form that everyone in the
organization can understand now this is
an entry level position in the data
analytics team he or she needs to have
technical skills in programming
languages such as Python and have
knowledge of tools like Excel and
understand the basics of data handling
modeling and Reporting now in due time
they can move up the ranks by taking up
roles of data engineer and data
scientists with some experience that
they can accumulate over the years and
finally we have data engineer now data
engineer is someone who's involved with
pairing data Who's involved with
preparing data for analytical or
operational purposes now they are the
intermediary between the data analyst
and the data scientist he or she needs
to have a lot of experience when it
comes to developing constructing and
maintaining architectures now they do
generally work on big data and submit
their reports to the data scientists so
that they can can be analyzed now let's
have a look at the skill sets required
for each of these roles first off we
have data scientists now since this role
is a little more coding oriented you
need to know a great deal when it comes
to programming languages programming
languages such as python R SQL SAS Java
and so on now you also need to be well
versed with Frameworks in relation to
Big Data such as Pig spark and Hadoop
peeking of Hadoop if you want to learn
more about how it works I suggest you
click on the top right corner and watch
our video on what to subdue coming back
data scientists also need to be well
versed with machine learning deep
learning and other similar Technologies
next up we have data analyst now this
role is much less technical as compared
to a data scientist as well as a data
engineer considering how it's entry
level here knowing programming languages
is a great bonus so an idea about
programming languages such as python R
SQL JavaScript ssas and so on is a great
benefit at the same time you do need to
be well versed with tools such as SAS
Miner Microsoft Excel ssas SPSS and so
on and finally we have data engineer now
being a data engineer requires you to be
well versed with a bunch of programming
languages as well as Frameworks now you
need to know about programming languages
such as python R SQL SAS Java and so on
while having expertise in Frameworks
such as Hadoop mapreduce Hive big Apache
spark data streaming nosql and so on now
let's talk about money or the salary
each of these roles get firstly we have
the data scientists who earns a whopping
137 000 US dollars per annum then we
have the data analyst who earns 67 000
per annum which is a pretty high salary
when you consider that it's only an
entry level job and a data engineer
which is in the median with 116 000 US
dollars per annum now let's talk about
rules and responsibilities firstly we
have the data scientist now a data
scientist gets to work with a lot of
unstructured data so they need to mine
and clean the data so that it's usable
they need to be able to design machine
learning models to work on the big data
they need to infer and interpret the
analysis and data to be able to lead an
entire team to achieve the goals of the
organization and deliver conclusions
that have a direct business impact now
let's have a look at the roles and
responsibilities of a data analyst they
need to use queries to gather
information from a database they need to
process the data and provide summary
reports they need to use basic
algorithms for their work such as linear
regression logistic regression and so on
and have core skills in statistics data
munging data visualization and
exploratory data analysis and finally we
have data engineer now they need to mine
through the data so that they can gain
insights from it they need to convert
erroneous data into a usable form so
that they can be further analyzed they
need to write queries on data they need
to maintain the design as well as
architecture of the data and create
large data warehouses using ETL or
extract transform load now let's have a
look at some of the companies hiring for
this role firstly for data scientists
you have Citibank Facebook Schneider
Intel Amazon and so on for data analysts
you have Infosys Oracle so Visa Capital
One Walmart and so on and for data
engineer you have Google Cisco
flocaskend Apple Spotify and much much
more so if all this has inspired you to
get started with data science I suggest
you take simply learn certification now
I'm choosing this certification because
it acts as a great entry point for
starting your career as a data analyst
or a data engineer now with this
certification it goes through all the
important Concepts when it comes to data
science and has 68 hours of in-depth
learning for real life industry based
projects Interactive Learning with
Jupiter notebooks and dedicated
mentoring sessions from our faculty of
Industry experts now some of the
concepts are you will be going through
statistical analysis and business
applications python environment setup
mathematical Computing with python which
is numpy sci-fi data manipulation with
pandas machine learning with
scikit-learn and so much more so then
you can start up on your step to
becoming a data analyst a data engineer
and then eventually a data scientist now
if you're already working as a data
analyst or a data engineer you can
become a certified data scientist which
simply learns data scientist Masters
program this goes through all the
important Concepts that you need to know
so that you can become a successful as
well as certified data scientist we'll
compare the two most popular job roles
in the field of Information Technology
that is business analyst versus data
analyst
first let's understand the job
description of a business analyst
business analyst is a professional who
Bridges a gap between the I.T and the
business teams in an organization they
use data analytics and modern
Technologies to assess processes and
deliver data-driven solutions they
understand and solve a business problem
and validate business requirements
a business analyst generates reports for
executives and stakeholders
they are part of the business operation
and work closely with the technology
team to improve the quality of the
services being delivered they also
assist in the integration and testing of
new Solutions now let's talk about the
job description of a data analyst
with a rapid increase in data generation
today the term data analyst has found
its prominence
a data analyst collects processes and
performs analysis of large data sets
every business generates data in several
formats this data can be in the form of
customer information and feedback log
files transaction data marketing
research and so on it is the duty of a
data analyst to transform these business
data into valuable insights some of the
problems that can be addressed are how
to improve a business how to provide
good customer experience what would be
the ideal price for a new product how to
reduce Transportation costs and so on
data analysts deal with data handling
data modeling and Reporting with this
brief understanding of the job
description for a business analyst and a
data analyst let's now shift our Focus
towards the various responsibilities of
a business analyst
a business analyst identifies the
business goals understands the problems
faced by an organization and comes up
with a cost-effective solution to tackle
the issues they thoroughly understand
the requirements from the clients and
assign the right resources
BS communicate and work closely with the
development team to design the solution
for a problem they ensure that the
development team doesn't spend their
time understanding the stakeholders
requirements and often give iterative
feedback on the solution being developed
they check and validate if the project
is running fine with the help of user
acceptance testing they also verify if
the solution being worked on is in line
with the requirements and ensure that
the final product satisfies the user
expectations Bas access the functional
and non-functional requirements a
business analyst documents the project
findings and results they present the
project conclusions to the stakeholders
and clients along with delivering
maintenance reports and building
visualizations to make decisions now
let's take a look at the
responsibilities of a data analyst first
and foremost a data analyst must
identify and understand the
organization's goal and requirements
this helps to plan and streamline the
analysis process data analysts collect
data from various heterogeneous sources
they assist the available resources
comprehend the business problem and
gather the right data for analysis they
work closely with different team members
like programmers business analysts and
data scientists
data filtering and data wrangling are
vital jobs of a data analyst the data
collected is often noisy and it contains
missing values hence it is crucial to
clean the collected data and remove
invalid values to make it ready for
analysis they use a variety of
analytical statistical and business
intelligence tools to spot Trends and
patterns in complex data sets discover
hidden insights and prepare summary
reports for the leadership team
they also use programming languages for
data mining and data manipulation
now it's time for us to understand the
difference between a business analyst
and a data analyst based on the skill
set they possess
first let's look at the skills that can
help you become a ba a business analyst
should have a graduation degree in any
relevant field such as business
accounting Information Systems human
resources or engineering you can apply
for entry-level business analyst
positions or with professional
experience Excel is a powerful analytics
and reporting tool for working with data
Bas use Excel to perform various
calculations data analysis plan and
editorial calendar and calculate
customer discounts to derive meaningful
insights and take decisions
BS use SQL to retrieve manipulate and
analyze data stored in relational
databases critical thinking skills are
important to understand customers
business needs it allows them to
distinguish between requirements that
add value to the business and those that
should be given a lower priority Bas
should find different ways to address
each challenge data visualization is a
key skill for Bas to build interactive
dashboards and reports to convey the
outcomes of a project
knowledge of Tableau power bi and click
view is required to make different types
of reports depending on the business
requirements
business analysts should have a good
Hands-On programming experience to solve
complex tasks and perform faster
analysis of data
hence knowledge of programming languages
such as R and python is a prerequisite
finally they should have good
presentation skills they should also be
confident about their findings and
conclusions and communicated in front of
the stakeholders and clients
let's Now understand the skills that a
data analyst should possess you must
have a bachelor's degree in any relevant
field or be a graduate in statistics
economics or science you're eligible to
become a data analyst being a fresher or
as an experienced professional you
should have domain knowledge in the
field you are working in once again
knowledge of excel is another basic
requirement for a data analyst data
analysts often work with structured data
so they should be proficient in writing
SQL queries using data manipulation and
data definition commands they should
know how to create stored procedures
another crucial skill for a data analyst
is to have hands-on experience with
programming languages such as python R
SAS and JavaScript you can analyze and
visualize large data sets and create
predictive models for making business
decisions data analysts create data
visualizations using libraries such as
matplotlib c-born ggplot and plotly this
helps them to perform exploratory data
analysis
knowledge of Tableau and power bi is
required to create different business
reports with the help of graphs and
charts data analysts should have
knowledge of machine learning algorithms
to build sophisticated models and make
future predictions so they should know
about linear regression logistic
regression support Vector machines
k-mean clustering and other supervised
and unsupervised learning algorithms
finally data analysts should also
possess good communication and
presentation skills now let's discuss a
salary structure for both of these job
roles
according to pay scale a business
analyst in the United States earns an
average salary of 69 000 while in India
you can earn nearly 6 lakh rupees per
annum now talking about the salary of a
data analyst according to pay scale in
the US a data analyst earns an average
salary of sixty thousand seven hundred
and ten dollars per annum and in India
you can earn around 4 lakhs 24 000
rupees per annum let's now move on and
look at the different companies hiring
for business analyst roles here we have
Oracle the search engine giant Google
the American MNC cognizant and
e-commerce company Amazon in addition to
that we have Ernest and Young technology
giant IBM Dell and Cisco hiring business
analysts
talking about the companies hiring for
data analysts we have Twitter Google the
social media leader Facebook and Amazon
we also have the American Oil Company
shell the electric vehicle company Tesla
apple and the American Credit Reporting
Agency Equifax
now choosing the right field that is to
become a business analyst or a data
analyst could be a challenging task
the key points that you have to keep in
mind before making a decision is
first review your background and see
what qualifications you have check what
skills you possess and the domain
knowledge you have then gauge your
interest to see what suits you best
and finally consider your long-term
goals and see the job roles that will
help you grow in your career in the long
run now let me tell you how simplylearn
can help you grow your career as a
business analyst and a data analyst
simplylearn offers a postgraduate
program in business analysis that is in
collaboration with Purdue University the
endorsed education provider is iiba some
of the skills that will be covered in
this course are strategy analysis
wireframing solution evaluation
dashboarding data visualization agile
scrum methodology scrum artifacts
statistical analysis using Excel and SQL
database
some of the tools covered in this course
are Microsoft Excel Tableau power bi
jira postgresql plan box and others some
of the key features of this business
analysis program are you will receive
Purdue post graduate program
certification master classes from Purdue
faculty you can enroll in simply learns
job assist where you will get IM jobs
Pro membership for six months and obtain
35 iiba PDS cdus and 25 PMI pdus
you will get 170 Plus hours of Blended
learning along with Capstone projects in
three domains
to become a data analyst you can enroll
in the postgraduate program in data
analytics offered by simply learn
this program is in collaboration with
Peugeot University and IBM the skills
that will be covered as a part of the
course are statistical analysis using
Excel data analysis in Python and our
data visualization using Tableau and
power bi linear and logistic regression
modules clustering using k-means
supervised learning and others the tools
that you will learn are numpy pandas
sci-fi scikit-learn Excel and others
some of the key features of this course
are you will get Purdue postgraduate
program certification industry
recognized IBM certificates enrollment
and simply learns job assist and master
classes from Purdue faculty you have 180
Plus hours of Blended learning 14 plus
Hands-On projects on integrated labs and
Capstone projects in three domains
so please go ahead and enroll for these
programs if you want to grow your career
as a business analyst or a data analyst
applications of data analytics now the
sky's the limit on this in today's world
almost every business Act of Life your
music on your Spotify are driven by data
analytics but some of the big players
when you go in there job hunting are
going to be your fraud analysis if you
want to go make a lot of money and
you're good at it and you like dealing
with numbers go join the banks and track
down the criminals who are stealing
money it's a lot of you know it's a big
thing to protect credit cards predict
sales purchases bad checks any of those
things when you can track them down is
huge
Healthcare exploding there is everything
from trying to find cures for the covet
virus or any of the viruses out there
using your cell phone to diagnose
different ailments that way you don't
have to go and see the doctor you can
actually just go in there and take a
picture of the funky growth on your arm
hopefully it's not too big and then they
send it in there and the data analytics
goes in there looks at it and says oh
this is what this is this is a
professional you need to go see or don't
need to see
and that's just one aspect of healthcare
the database is being generated by
Healthcare and getting the right doctors
and helping the doctors analyze whether
something is benign or malignant if it's
cancerous all those things are now part
of the ongoing Health Care growth in
data analytics
Inventory management
think one of those huge warehouses where
they're shipping out all the goods how
do you inventory that in such a way so
that you maximize the stuff that's being
purchased the most near the entrance and
all the other stuff towards the back or
even pre-ship it so it's huge to be able
to inventory the manager inventory and
pretty soon they'll just have a drone
come in there and start picking up some
of those boxes and move them around also
deliver your Logistics again this goes
from getting from point A to point B you
can combine it with our inventory so you
pre-ship stuff if you know a certain
area is more likely to purchase it how
do you get it the delivery to the most
destinations the quickest in the short
amount of time and then they even
pre-stack the trucks going out and
that's all done with data analytics how
do we stack all that stuff so it comes
out in the right order
targeted marketing huge industry any
kind of marketing whether you're
generating the right content for the
marketing who are you targeting with
that marketing researching the people
what they want so you know what products
to Market out there all those things are
huge
and these are just a few examples you
can probably go Way Beyond this from
tracking forest fires
to astrology and studying the stars all
of this is part of data analytics now
and plays a huge role in all these
different areas
City Planning is another one you know
you can see a nice organized City like
this one where you can get in and out of
the neighborhoods if you're a fire truck
police officers need to be able to get
in and out you want your tourists to be
able to come in you still want the place
to look nice and you have the right
commercial development the right
Industrial Development like enough
residence for people to stay all those
things are part of your City Planning
again huge in data analytics
so sky's the limit on what you use it
for let's take a look at types of data
analytics
and this can be broken up in so many
ways but we're going to start with
looking at the most basic questions that
you're going to be asking in data
analytics and the first one is you want
descriptive analytics what has happened
hindsight how many sales per call ratio
coming out of the call center if we have
500 tourists in a forest and you have a
certain temperature how many fires were
started how many times did the police
have to show up to certain houses all
that's descriptive the next one is
predictive Predictive Analytics is what
will happen next we want to predict this
is great if you want to have a ice cream
store and you want to predict how many
people to work at the ice cream store in
a certain day based on the temperature
coming up in the time of the year
and then one of the biggest growing and
most important parts of the industry is
now prescriptive analytics and you can
think of that as combining the first two
we have descriptive and we have
predictive then you get
pre-scriptive Analytics
how can we make it happen foresight what
can we change to make this work better
in all the industries we looked at
before we can start asking questions
especially in City development there's a
good one
if we want to have our city generate
more income and we want that income to
be commercial based what kind of
commercial buildings do we need to build
in that area that are going to bring
people over do we need huge Warehouse
cells Costco sales buildings or do we
need little mom pod joints that are
going to bring in people from the
country to come shop there or do you
want an industrial setup what do you
need to bring that industry in there is
our car industry available in that area
if it's not a car industry what other
Industries are in that area all those
things are prescriptive we're guessing
we're guessing what can we do to fix it
what can we do to fix crime and area
with education what kind of education
are we going to use to help people
understand what's going on so that we
lower the rate of crime and we help our
communities grow better that's all
prescriptive it's all guessing we went
foresight into how can we make it happen
how can we make this better
and we really can't not go into enough
detail on these three because a lot of
people stumble on this when they come in
and are doing analytics whether you're
the manager shareholder or the data
scientist coming in you really need to
understand the descriptive analytics
where you're studying the total units of
furniture sold and the profit that was
made in the past here we go into
Predictive Analytics predicting the
total units that would sell and the
profit we can expect in the future and
gear up for how many employees we need
how much money we're going to make and
prescriptive analytics finding ways to
improve the sales and the profit so we
can sell maybe a different kind of
furniture we're going to guess at what
the area is looking for and how that
marketing is going to change
data analytics process steps so let's
take a look at some of the basic
processing and what that looks like when
you're working with this data
so there's five basic steps the five
steps of processing and this changes and
then there's a lot of things that go on
when they talk about agile programming
the whole concept of agile is you take
some kind of framework like this and
then you build on it depending on what
your business needs
so the first step is data collection
and usually with a large company you
might have somebody who is responsible
for the database management you may have
another one where they're pulling apis
they're pulling data off of maybe the
Census Bureau maybe something very very
specific domain specific so if you're
analyzing cancerous growths and how to
understand them then the data collection
is going to be those measurements they
take from the MRI or it might be even
the MRI images they've used those also
so there's a lot of things with data
collection and how to control that and
make sure it has what you need and is
clean and you don't have misinformation
coming in
once you have the data collected there's
a data preparation
so stage two is we take that data and we
format it into something we can use
probably one of the biggest formats that
you see is when you're processing text
how do you process text we'll use what
they call a one hot encoder and each
word is represented by a yes no kind of
setup so it'd be like a long array of
bits that's one way to prepare it and so
you know bit number one is the bit
number two is has or whatever it is
other preparations might be if you're
using neural networks you might be
taking integers or float numbers and
converting them to a value between 0 and
1. that way you don't have one of them
creating a bias in there so there's a
lot of different things that go into
Data preparation that is 80 percent of
data science so when we talk about the
data analytics which is a little bit
more on the mass side and they usually
say talk about a data scientist kind of
being the overall
prepare this stuff you're going to spend
80 percent of your data preparation
data exploration that's the fun part
this is where you're exploring things
and it is
maybe 10 to 15 percent of what you do
with the data you spend with the data
exploration it is probably the most
important step because this is where you
got to start asking questions if you ask
your questions wrong you're going to get
some wrong information if you're working
with a company and they want to know the
marketing values then you really got to
focus on hey how do we generate money
for this company or fraud how do we
lower the fraud rate while still
generating a profit for data modeling
this is where we start actually getting
into the data code which model to use
that predicts what's going to happen
uh and then result interpretation we
want to be able to interpret those
results usually see that in your matplot
library where you create nice beautiful
images so it shows up on their dashboard
for the marketing manager or for the CEO
so they can take a quick look and say
hey I can see what's going on there you
want to reduce it to something they can
easily read they don't want to hear the
scientific terms they want to see
something they can use and we'll talk
about that a little bit more and we
start looking at some of this in a demo
let's look at one more interesting
feature of Excel and that's your
conditional formatting now as you see on
the screen conditional formatting has
different rules which can be applied on
your data and that allows you to
basically differentiate or easily
identify data values which are
based on certain criterias or rules so
when you talk about conditional
formatting you have different options
such as you can highlight sell rules you
can get top and bottom values you can
apply different rules apply different
color scales and you you can easily
manage these rules so conditional
formatting is very useful for people who
would want to work on huge amount of
data and easily perform some data
analysis
it's easy to use as it is shown here and
with your conditional formatting you can
format cells based on a preset condition
you can perform conditional formatting
to identify cells you can highlight a
few significant cells and you can easily
perform conditional formatting
as shown on the left side
now how do we work with conditional
formatting let's have a quick look so
say for example we have our Excel sheet
and if you see here I am highlighting
the sales person who have generated
Revenue greater than 10 000. so we can
be looking at the values where the
revenue generated by a particular sales
person is greater than 10 000 it has a
particular
color and how do we get here so for
example let's select this data
and what I could do is
I could go into conditional formatting
now I could basically highlight cell
rules and we could just say greater than
that's an easier way I could also go
ahead and create a new rule but then I
can use one of this option I can say
greater than and let's give some value
might be we would be interested in
looking at any value greater than 12 000
so let's choose 12.
1000
and here it is what color would you want
to select so for example I would say
something like
yellow fill with dark yellow text
and let's say okay so right now what I'm
doing is I have all the values where the
revenue generated was greater than 10
000 but then I have also selected all
the sales people who have made or who
have generated Revenue greater than 12
000. so I can just wait Ctrl Z to see
the previous result now here I had the
values which were greater than ten
thousand and the one which we did just
now basically
highlighted the values which are greater
than twelve thousand so this is one
simple example now we can look at some
other examples say for example you want
to format cells using three color scale
so if you look at the values here I have
a three color scale mainly in green
yellow and red and how do you do this so
for example I can go in here and I can
go to conditional formatting so I would
want to go for color scales and here you
can create different rules so we can set
up a two color scale so we can say
format only values that are above and
below average I can format only cells
that contain something I can get the top
and bottom value so these are different
ways in which I can have a three color
based scale now what I will do is I will
select this and let me show you the rule
which which I have so for example I can
go into manage rules and if you see here
there are certain rules which have been
specified now what does that mean so you
would want to specify a three grade
scale so for example if I would want to
look at my first rule it tells me that I
am choosing three color scale I can
choose lowest value percentile and
highest value and that basically will
select the cells based on their values
so what we could have done is I can
basically
use one of these values I can delete
these rules which I have created so for
example I have all these rules but you
should always carefully remember that
the rules will be applied in the order
shown so for example if I just delete
these rules
and then say apply and say okay my data
is back now it does not have any
highlighting now I can go in here I can
say conditional sorry conditional
formatting I could go for color scale so
I could basically go into
new rule so I would want
the
cells to be using three color scale so
let's
choose three color scale now when you
say three color scale it says what will
be the color of lowest value
and we could choose might be any one of
this let's choose red I can say midpoint
is percentile 50 and then the highest
value is green and if that looks good
let's say okay and now if you see the
lowest values have been highlighted as
red you have mid values and then you
have the positive value so this is the
three color scale and that easily helps
me in identifying the data based on the
cell values
now in conditional formatting what you
can also do is you can basically color
the cells based on their value so what
we are seeing here is if the revenue
generated is greater than average then
that shows in green and if the revenue
generated is lesser than average that's
shows in Orange now how do we do that so
we can basically again manage some rules
so I can basically create a new rule now
here I can select one of the options
which says format only values that are
above or below average and that's the
option I would want to select now I can
select this and it says format values
that are above average so in our case we
had it in green so for example I'll say
above average and then here I can go for
a particular color
so you can go for a particular size so
let's go and look into
the formatting so for example let's
choose yellow
say okay now I am saying wherever the
cell values are above average it would
be yellow instead of green
and let's go in here let's go and look
into manage rules so this is basically
the rule which we are applying now we
can also add a new rule
and I need to select the values so for
example I will say
here so we had gone for above now we'll
go for below we'll go for format we will
choose red we'll say okay we'll say
now these are basically
the rules which we have created and here
it says applies to your data so right
now it has not been applied so for
example if I select this and then I
could basically choose my area just hit
on enter and similarly you can go in
here and then select your area hit or
enter and say apply say okay and now if
you see I have really chosen bright
colors but then I have said wherever my
revenue generated is above average it
should be in yellow and
below average should be in Red so we
wanted above
average to be in green and below average
to be in Orange so
that's what we have here right so you
can always
color code your cell values based on
some rules which you are setting up now
similarly you can also find the top 10
and bottom 10 values and that's pretty
easy so you can just select this and
then you can go into conditional
formatting you can go for top and bottom
values top 10 items bottom 10 items or
you can go in for more rules so you can
say format only top or bottom ranked
values so you have top 10 now you can
choose the color and for example I'll go
for blue
and
I'll say okay so now if you see my top
10 values are blue now similarly I can
add one more rule so I can say new rule
and I can say let's go for top or bottom
let's go for bottom let's go for format
let's say orange
say okay say okay and that's it so now
you have your values which are top or
bottom 10 values so you are using
conditional formatting where you are
basically highlighting your cell values
based on different colors and here
easy conditional formatting based on
different rules helps us to do that now
similarly you can also have the values
which is basically
showing you how the values are
increasing so what we can do is we can
select our columns either you could
apply this to all the columns now here I
have applied this only to Jan and April
now I could apply this for June so let's
say June so you can go for gradient fill
you can go for solid fill you can
obviously just select the color and that
takes care of the things you can say for
example select this and now this is
selected what I would want to might be
format this so I can go in here I can go
into manage rules now that will tell me
what rule has been applied
in the order so I can just do a edit
Rule and that basically says this is a
solid fill which is color you have no
border this is basically color as black
now I can go for something like gradient
fill
and I can say okay and now if I say
apply and okay so this basically is Like
Your First Column so you can use
conditional formatting for various use
cases and you can highlight the values
so anyone who would look at the value
would automatically notice which are the
higher values which are lower values
might be here the revenue is getting
generated or was getting generated but
did not grow Beyond a particular value
and so on now similarly you can also go
in for different options say for example
here we would want to
see if the revenue was dropping
or if the revenue was
if the revenue decreased or say for
example if the revenue was going up for
this particular sales person so here we
are looking at Carol so in Jan the
revenue Generation by sales was very
high then in Feb it was falling down
in March it was kind of stable then in
April it went way below so we can
obviously work on this wherein we can
grade our cell values so what we can do
is we can go in for
highlighting the cell values now you can
go for color scales you can go for Icon
sets and this is where you can choose
your different shapes
so you could choose one of these shapes
so for example I would be interested in
looking at the indicators like
directional I could go
using this three arrows I can go in for
this color I can choose directional and
then my values are
automatically using directional now what
we can also do is we can then go into
manage rules and that basically tells me
what rules have been applied so for
example the latest one is the icon set
which I have chosen it shows the
selected columns I can obviously do a
edit Rule and then I can choose so I'm
saying the format style is icon sets I'm
not using a data bar I'm not using color
scale now here
I have chosen the style of icons
and then here you can basically give
some values so you have icon which is
green when the value is greater than or
equal to 67 percentage
when I say
hyphen or minus it is less than 67. it's
way below 33 percentage then you give
this value so you can obviously edit and
easily highlight your cell values based
on this icon set so I can apply this and
that's how I use conditional formatting
so conditional formatting can be very
useful if you would want to use icon set
if you want to use your data bars if you
would want to highlight particular
values if you would want to color code
based on some calculation if you would
want to
use a three color or a two color scale
or if you would want to just find out
values based on some simple calculation
so conditional formatting is used
extensively by data analysts or people
who are working in business intelligence
teams or people who would want to use
Excel to easily identify the data
easily identify the cells which contain
particular value or finding out less
significant or more significant cells to
then pull out values and carry out your
computations calculations or analysis
let's learn one more feature of Excel
and that's basically your data
validation now this can be very useful
when you would want to work on
validating the data which is being fed
in the cells so you could limit it to
basically a number between a particular
value you could also add some messages
to it if you would want or you could
even
Circle invalid data or clear validation
circles so data validation really helps
us in validating the data which is being
fed in to particular fields
now it's a feature in Excel which is
mainly used to control what a user can
fill in a Cell you can decide what type
of values must be entered you can also
restrict user to enter only valid data
and if any invalid data is entered an
error message will be displayed now
that's where you can use your data
validations so let's see how that can be
done so for data validation let's see
some exercises here so for example you
have a name column and you would want to
restrict that the name should accept
only 15 characters now how do you do
that so you can basically select the
cells or you can just select a
particular cell and then we can later
drag the property to other fields now
here once the cell is selected so for
example let's try this out and let's see
if that works so for example sample I
will say Peter Johnson
okay and that is basically fan nine
and 12 characters so let's say
Junior
and if I do this it says input length is
greater than 15 do you want to continue
if I say yes right so basically
it is allowing me to add the value here
but then it has basically violated the
rule now this is giving a message to the
user that the username should be entered
less than 15 characters now how do we do
that so for this we can basically select
the column and then we can search for
data Tab and get into Data validation so
this is where you can create or select
different kind of rules so for example I
can go into Data validation I can go
into settings and I can say the text
length and that should be less than 15
now this is the maximum I'm giving and
it says apply these changes to all other
cells within the same settings so I can
do this or I can just drag and drop so I
can basically
apply this formula and now you can in
fact randomly check in any particular
cell is the rule applied so it says text
length less than 15. so we can basically
Control Data validation in this
particular column and that will allow
only 15 characters it will pop up a
message if the user really wants to go
beyond the particular limit now you also
have similarly date of birth so the
Restriction is date of birth should be
between 10 Jan 1990 to 30 December 1998
so this is what we want to restrict how
do we do that so we can select the field
we can click on data validation and if
you see here I have selected date and
then date is between and then obviously
you can give a range that is 10 Jan date
1990 12 30 1998 so that's the start and
the end filter which has been applied
and once this is done you can also check
your input messages which says
when cell is selected show this input
message enter a valid date
so if you see here on the left there is
a pop-up which is coming up which says
enter a valid date now I can also say
when user enters invalid data show this
error alert so I can say stop I can say
invalid entered
and this is how I have set a rule so for
example if I just do something like this
and that says invalidate entered I can
do a retry it will take me back here but
unless and until I do not give the right
format the date will not be accepted and
again the same thing applies to all the
cells
similarly email so we are saying the
email should have at the rate present in
the value provided now for this we can
use a formula and we can select well I
would want to apply this rule to all
these rows starting from C2 to c14 so
let's get into Data validation let's
look in settings and here we are
choosing custom now within custom I am
choosing what is the formula so I am
saying is number
and then I'm saying find
at the rate for
the rows C2 to c14 so the only thing we
are concerned about here is the value
should have an email
icon
you can input a particular message you
can say what has to be done for error
alert so for example we can basically go
in here and we can say
invalid email
that's the title I'm giving and we can
say email should contain
at the rate so if I do this and if I say
ok now you can test it so you can say A
B C D and that says email should contain
at the rate and basically that will not
allow me to add the values so now you
have the field called salary it says
salary should be greater than 50 000 now
we can limit the values by choosing a
whole number so for example for salary I
can go into Data validation I can go
into settings so here I can say
something like decimal or I can go for
whole number so both of the things are
fine it depends on what kind of values
get into this particular field so if I
say whole number and if I say it has to
be greater than 50 000 so I'm saying the
minimum is 50 000 or I could have given
a decimal and then I can say greater
than less than equal or anything or even
between so I can select greater than and
then I can just say okay
now for rank the rule is rank should be
between 100 and 200 so again we can use
a whole number so ranks will generally
not be decimal so this would be whole
number salary is can be a whole number
or it could be decimals so we have
chosen decimal here and in rank if you
click on data validation I have chosen
whole number
I have set data between 100 and 200
input message nothing error alert
nothing but that depends I can give this
so this is how you can just do a simple
data validation and control the values
which land in the cell okay so now let's
also understand how we can
restrict the values in a particular cell
which might be based on a list of items
now for example here if you see I have
two columns so one column basically has
the values of city names and then you
have places within that particular City
so if I would want to implement a data
validation based on this so for example
if you see here City and that basically
shows me only the four values which can
be entered and if I go to place then it
tells me for Maharashtra I can only
enter Pune Mumbai nashik now how do I do
this so say for example you take an
empty field and you want to restrict the
values of city names so I could select
data validation I could basically select
list and then it tells me you need to
enter the list values that is the source
so you click here and then I can just
select these fields now if I do this and
if I say okay it has implemented a data
validation but if you look into this it
will show me the same thing but then it
shows me some empty cells which did not
have any value so this is fine but it
would be better if we do it in a
different way so I can select this and I
have already given my city names here so
I can just do a data validation and here
in the source let me get rid of this now
I can just have my cursor here I can
select these values and then if I say ok
so now if you see my city names have
been restricted to these values and
that's how you can Implement your data
validation so I have this data
validation here but I will get rid of
this one by just doing a control Z now
I'll come here and say for example I
would want to implement the same data
validation now the easiest way would be
e in doing this for all my four cities
now if you see here I have data
validation I could choose Bangalore and
if you come and check here there is no
data validation but we have implemented
data validation here now how is that
done so I could select this or for
example I can go in here I can select
data validation I can select list and
then here in the source I will say for
Bangalore the value should be these and
then if I basically say okay
now if we see these are the values which
are fed into Bangalore so we could do
this or like what we have done here so
if you can check the data validation
rule I have used list and then I have
said indirect F2 so basically I'm giving
in a formula which relates to the value
which is in for the city Maharashtra so
we could do this or in a simpler way we
could do this and then just drag and
drop here so we could check for
Maharashtra what are the values let's
choose a different city so for example
Kolkata and here I will choose now since
we did a drag and drop it has basically
taken the values of Bangalore so that's
not right so we select this we go into
Data validation and here I can either
feed in the values that is such as
Bangalore I could basically say
something like this Kolkata and I could
say okay so now if you see it shows me
the places in Kolkata now here we have
let's choose a different city so for
example let's go for Delhi and here I
can go to data validation and I can just
say Delhi so this is an easier way of
doing it or if you remember the formula
then what you can do is you can just
give in
indirect and then basically
give the
value of the cell for which you would
want to keep in the values so this is
how you can do list validation so you
can provide a list of values and then
you can restrict the values in a Cell
which should be belonging to a
particular list so this is how you do a
simple data validation by restricting
the data in the form of a list
Excel is a really powerful tool for data
analytics and Reporting and pivot tables
are one of the features that Excel
offers for creating tabular reports to
summarize our data
let's begin by understanding what is a
pivot table
a pivot table is a tool that summarizes
and reorganizes selected columns and
rows of data in a spreadsheet to obtain
a desired report
it does not actually change the
spreadsheet data it simply pivots or
turns the data to view it in different
perspectives pivot tables are specially
useful with large amounts of data that
would be time consuming to calculate
manually
now let's understand the different
components of a pivot table
so there are four main components first
we have rows when a field is chosen for
the row area it populates as the first
column in the pivot table
similar to The Columns all row labels
are unique values and duplicates are
removed
columns
is the second component
when a field is chosen for the column
area only the unique values of the field
are listed across the top
then we have values each value is kept
in a pivot table cell and displace the
summarized information the most common
values are sum average minimum and
maximum finally we have filters filters
apply a calculation or restriction to
the entire table
so let's jump over to Microsoft Excel
and let me show you the data set that we
will use in this demo so with India
being ready for its 16 census in 2021
that is next year it is a good time for
us to analyze India's last census data
from 2011 and see where different states
and cities across India stood in terms
of population literacy and other
socio-economic factors we will analyze
this data by creating different pivot
tables in Excel and explore some of its
features so let's begin
first I'll show you one of the features
that Excel offers us so suppose I click
on any cell and hit Control Plus Q
you can see our entire table is selected
and at the right bottom there's an
option of quick analysis
now you can see by default Excel has
prompted certain features such as
formatting we have charts totals and
this one more called tables now Excel by
default has created some pivot tables
for us now the first one you say sum of
district code by state names
next we have sum of sex ratio by state
name
then we have some of child sex ratio
some of male graduates and some of
female graduates by state name and there
are others before creating our pivot
table so let's have a final look at our
data set
so First Column you see is the city
column so there are different cities
from different parts of India then we
have the state code we have the state
name district code we have the total
population followed by male and female
population
next you can see we have the total
literates from each City
then we have the male and female
literates next we have the sex ratio
then we have the child sex ratio
next we have total number of graduates
and finally you can see we have male and
female graduates so using this table
we'll create several pivot tables
first of all let's create a pivot table
to find the total population for each
state and sort it in descending order so
you can see here we have the problem
statement
so our first pivot table will have the
total population for each of the states
in descending order
so to create a pivot table you can click
any cell in your data go to the insert
Tab and here
left you can see we have the option to
create a preview table so let me select
pivot table
now my range is already selected the
entire table
and here I'll choose existing worksheet
because I want to place my pivot table
in the same worksheet
and I'll give my location I'll point to
cell Q5
now let me click ok
you can see the pivot table Fields
appears here on the right
now since we want to find the total
population for each state so what I'll
do is
I'll drag my state name onto rows
so here in our pivot table you can see
we have the different state names listed
now
we want the total population for each of
these states
so
in the field list I'll search for total
population which is this one and drag it
under values
you can see
we have a sum of total population for
each of these states by default
Excel built sum any numeric column you
can always change it to average minimum
maximum anything you want now we want to
sort this column in descending order so
I right click go to sort option and
choose Z to A
that is largest to smallest
you can see here in 2011 Maharashtra had
the highest number of population
or the total population in Maharashtra
was the highest then it was uttar
Pradesh we had Andhra Pradesh
and if I come down we have nagaland and
Andaman and nicobar Islands towards
the end
so this is a simple pivot table that we
created now the next problem we have is
we want to find the total sum of
literates in each City belonging to a
certain state so let's see how to do it
I'll click on any cell go to insert and
here I can click on pivot table
my range is selected I'll choose
existing worksheet and give my location
which is Q5
I click on OK
now here we want to find the total sum
of literates so what I'll do is first
let me drag
total literates column two values you
have the total sum of literates from all
the states
next I want to see the sum of total
literates based on states and cities so
let me first drag state name onto rows
and then
will drag City
on two rows
you can see here we have our pivot table
ready
to the left of the pivot table you can
see we have the state names and
the cities per state and on the right
you can see the total number of
literates from each City if I scroll
down we have Assam then you can see we
have Bihar
and if I keep scrolling we have all the
states haryana himanchal Pradesh that's
Jammu and Kashmir which has now become a
union territory
as well moving on
okay so the next thing we want to see is
what is the average sex ratio and the
child sex ratio for each state
with that we also want to find the
states that had the highest and lowest
sex ratio in 2011.
so let's create a pivot table for this
I'll click on any cell go to insert
choose pivot table
click on existing worksheet I'll select
cell Q5
and click on OK
now since we want the average sex ratio
and the child sex ratio so first I'll
drag those columns either you can
manually scroll and
drag it or here you have the option to
search for it so if I look for child
you can see we have
the same
column listed you can just drag it from
there let me delete this
and I also want the sex ratio
so I'll place it on top of child sex
ratio
next
we want to see it based on
different states so what I'll do is
I'll take
state name and put it under rows
so here you can see we have our pivot
table ready
on the left you can see we have the
different state names listed and on the
right we have the values
now we want to find the average Now by
default Excel will sum the numeric
columns you can see it tells you sum of
sex ratio and child sex ratio
so what you can do
you can click on this drop down and go
to Value field settings
and here summarize values by you can
choose average
you can see the custom name it says
average of 6 ratio
click on OK
our entire column is now
giving us the average sex ratio
similarly for this column let me convert
it into average I'll again click on the
drop down
go to Value field settings
click on average
and click ok
and you can see here we have the average
of child sex ratio for each of the
states now the next question says which
states had the highest and lowest sex
ratio so we'll consider this column
so we'll sort it in any order you want
you can do it either ascending or
descending let me short it in descending
order
you can see we have our
column sorted now
so in 2011 Kerala had the highest sex
ratio
and if I scroll down to the bottom you
can see himanchal Pradesh at the lowest
which is around 818.
up next let's explore one more feature
of pivot table so suppose you want to
see the top or bottom few rows of a
pivot table you can do that as well so
here we have a question at hand
we want to find the top three cities
with the highest number of female
graduates so let's see from the entire
pivot table how we can filter the top
three cities
so I'll go to insert
click on the pivot table option
go to existing worksheet
click on Q5
and hit OK
now since we want to find the top three
cities I'll drag
City column onto rows and then we want
the female graduates so in the search
bar I'll look for female
and I'll choose this column that is
female graduates and ragged
here on to values
so I have
the sum of female graduates for each of
the Cities now since we want to find the
highest number of female graduates in
the top three cities so let me first
sort this column
I'll sort it in descending order
now we have it sorted now from this you
can see that Delhi greater Mumbai and
Bengaluru are the top three cities
but it's displaying all the cities for
us
so let's filter
only the top three so what you can do is
right click
and go to filter
under filter you have the option of top
10 I'll select this
here I only want the top three
so either you can
go down like this or you can directly
type 3
your column is already selected let me
just click on OK
there you go we have the required pivot
table ready and it only displays us the
top three cities with the highest number
of female graduates
now the next thing we want to see is how
to use a Slicer in a pivot table so we
have a question here
What's the total population for all the
cities in Rajasthan and Karnataka
so let's create a pivot table for this
and see how you can use a slicer to
filter the table
click on existing I'll click on a
location this time q6
click ok
now since I want the total population so
I'll drag total population onto values
and then I'll select
the city
onto rows and then the state name also
I'll place it on top of City so you have
in the pivot table all the states and
their cities
and on the right you can see the total
population for each of these cities
but our question is we want to find only
for Rajasthan and Karnataka
now for that what you can do is
go to insert
and create a slicer either you can
create from this option or you can go to
pivot table analyze option and here you
have the option to create or insert a
slicer I'll click on this
and since we want to slice the table
based on
state that is Rajasthan and Karnataka
I'll choose state name
as my slicer field
you can see this is my slicer here
now
you only want
the data for Rajasthan and Karnataka so
I'll search for
these two so here we have Karnataka so
let me select Karnataka first
and I also want for Rajasthan
so let me select Rajasthan also
you can see in our pivot table we only
have
data for Rajasthan and Karnataka
so this pivot table
shows you different cities from
Karnataka and the total sum of
population from each of the cities and
similarly we also have for Rajasthan
moving ahead now we will see another
very interesting feature of pivot that
is how you can
create percentage contribution of a
table
for example we have a question here
what's the percentage contribution of
male and female literates from each
state
now we want to see in terms of
percentage and not as sum or average
let's do that
I'll create my pivot table
click on existing
and I'll select an empty cell
okay
now here since we want to find the
percentage contribution of male and
female literates so first I'll drag
male literatures onto values followed by
female literates on values by default it
has summed up the male literacy and
female literates value
and also
I want to drag
State column
two rows
so here you can see the sum of male
literates and female literates per state
I want to convert this as
percentage contribution so
what we can do is I'll select any cell
and
I'll right click
and I'll go to so value as and here I
have the option to select percentage of
grand total so I'll select this
you can see we have the percentage
contribution of military to the total
now if I sort this you will get to know
which state contributed or has the
highest percentage contribution
so we have Maharashtra for male
literatures
then we had uttar Pradesh in 2011
if I come down
we had meghalaya nagaland and Andaman
and nicobar Islands
as two states which had little or
minimal contribution to male literates
similarly let's do it for female
literates
I'll go to show value as and select
percentage of grand total
so you can see here also Maharashtra
uttar Pradesh then Gujarat and all had
the
highest percentage contribution to
female literates
so this is another good feature
to convert your data and C10 terms of
percentage
now moving ahead let's say we want to
find the bottom three cities from each
state that had the lowest female
graduates
we can do that as well I'll go to insert
click on pivot
go to existing worksheet select an empty
worksheet and click on OK
now since I want to see based on States
as well as cities so let me drag the
state name first onto rows
and let's drag the city column onto ruse
next
we want female graduates so let me look
for female graduates in the field list
I'll drag it onto values
now we have the list of states and the
respective cities
and to the right of the pivot table you
can see the sum of female graduates from
each City
now
first I'll
sort this column
I'll right click go to sort and click on
shortest to largest
now we have sorted our female graduates
from
shortest or smallest to largest
now since I want to find the bottom
three cities from each state I'll come
to this cell
right click go to filter and select top
10.
now I'll
replace top 10 with bottom and I want
the bottom three cities from each state
I have my
column selected that is sum of female
graduates if I click on OK
you can see here some of the states
don't have three cities so you can see
Andaman and Uber islands has only one
city that is Sport player while the
remaining you can find
the bottom three cities with the lowest
number of female graduates so Andhra
Pradesh had these three in Assam we had
nagao then there was dibrugar and silcha
similarly if I come down in haryana we
have palwal kathal and zind
if I come further here you can see for
Karnataka this gangavati this Rani
bennur and the scholar similarly you can
see for Kerala as well
now moving ahead
now in the next example I'll tell you
how you can create a calculated field or
a calculated column in Excel with the
help of a pivot table so in a pivot
table you can create or use custom
formulas to create calculated fields or
items calculated fields of formulas that
can refer to other fields in the pivot
table
calculated Fields appear with other
value fields in the pivot table like
other value Fields a calculated Fields
name May proceed with sum of followed by
the field name
so here we have a sales table that has
columns like the items which has
different fruits and vegetables and
those have been categorized as fruits
and vegetables we have the price per kg
and this is in terms of rupees
and we have the quantity that was sold
now let's see if you want to find the
sales for each item in the table you can
create a calculated field
so your sales column is going to be the
product of price per kg
and quantity so let me show you how you
can do that with the help of a pivot
table
I'll create a pivot table first
click on an empty cell
hit OK
now
if you see on the top under pivot table
analyze and under calculations we have
the option Fields items and sets if I
click on this drop down I get the option
to create a calculated field or insert a
calculated field I click on this
I'll give my field name as sales
and
I'll select my formula
I'll first click on price per kg and hit
insert field
I'll give a space hit shift 8 to give
the product symbol and then I'll double
click on quantity
now this is my formula for sales that is
price per kg multiplied by quantity
I'll click on ADD
and I click on OK
if you see here
there's a calculated field that is
present in the pivot table fields which
is sales but it did not add it to our
original table or original table is the
same but here we have added a calculated
field which is present only in the pivot
table list now we can use this it has
already taken it under values
now let's say I want to find the sum of
sales for
each item
under each category
you can see it here we have our category
fruit and we have our category vegetable
and under that we have different items
like apple apricot banana similarly in
vegetables we have broccoli this carrots
corn eggplant and others
so this is how you can create a
calculated field in a pivot table
now there's one more good feature that
Excel offers us in pivot table is to
create a pivot chart so you can use your
pivot table and create different charts
so I'll show you how to do that
if I go to insert
here I have the option of recommended
charts if I click on this Excel gives me
some default charts which you can use
let's say I'll select this
let me drag it a bit to the right
here you can see I'll close this pivot
field list
this is a nice bar chart that Excel has
created this is called a pivot chart now
here you can see the category fruits and
vegetables and the different fruits and
vegetables or the items
in the y-axis you can see the total
seals if you see from the graph guava
meet the highest amount of sales now if
I sort this let's sort this first
you can see it here fruit guava meet the
highest amount of sales now since I
sorted and changed my pivot table the
pivot chart also automatically gets
updated
similarly there are other charts also
that you can create let's go to the
insert tab and let's click on
recommended charts again
let's look for a pie chart so this is a
pie chart that you can create let me
click on OK
so here is our pie chart and each Pi
represents a certain item
and the pi that has the highest area
represents it had the highest amount of
sales in this case you can see
it is guava and similarly we have other
items as well this is fruit banana
that's corn
and we have spinach and others
let's explore a few other charts so
first I'll
click on my pivot table go to insert
and under recommended charts let's now
select a line chart
if I hit OK
move it to the right so this is a line
chart
you can see it starts
from guava
which had the highest amount of seals
then it drops
and in the x axis you can see the
different items similarly when it starts
with the vegetables broccoli made the
highest amount of sales width 2800
rupees and our lowest was eggplant at
900 rupees for fruits papaya sold the
least at 700 rupees
let's take another chart I'll go to
insert
under recommended charts let's see
this time we'll see a bar chart now this
is a
horizontal bar chart and not a vertical
one we just saw a vertical
column chart like this
this is an horizontal bar chart now you
can always increase and decrease the
size of these charts
let's explore
a last start let's take the area chart
for now
so this is an area chart again it looks
similar to the line chart it starts with
guava which are the highest amount of
sales similarly papaya under fruits had
the lowest amount of sales and the
vegetables it was
broccoli and finally eggplant metal
lowest amount of sales under vegetable
now let's go to our first sheet and
summarize what we have done in this demo
tables in Excel so we had our data
this is a 2011 Census Data from India we
had the different cities the state names
and we had the total population total
literals female literates male literates
we have the sex ratio total graduates
and other information
so we began by understanding how to
create a simple pivot table where we
calculated the total population for each
state and sorted it in descending order
we found that Maharashtra uttar Pradesh
had the total population in 2011.
then
we saw
another preview table where we
calculated the total sum of literates in
each City belonging to a certain state
so you can see we had the
different state names and the cities
under each state
then
we saw another feature where you could
calculate the average of a certain
numerical column so here we calculated
the average succession and the child sex
ratio for each state and found out which
one had the highest and lowest sex ratio
after that we saw
how you could find or filter tables
we saw how to find the top three cities
with the highest number of female
graduates we found out that Delhi
greater Mumbai and Bengaluru were the
top cities with highest number of female
graduates
next we saw how to use Slicer in a pivot
table so we sliced our table based on
Rajasthan Karnataka State and saw the
total population for all the cities in
Rajasthan and Karnataka
in the next sheet
we explored another feature that was to
find the percentage contribution of male
and female literatures from each state
then
here we saw how to find out
the bottom three cities for each state
having lowest female graduates and one
thing marked that some of the states did
not have three cities for example
Andaman had only one city that was sport
player but the others we found out the
bottom three cities that had the lowest
female graduates
finally we looked at how to create a
calculated field in a pivot table so we
saw how to create a calculated field
called sales
and then we explored how to create
different charts and graphs so this was
an area chart that we saw is a column
chart we also saw or looked at
a bar chart that was a horizontal bar
chart similarly we saw how to create a
pie chart as well in this video we'll be
creating two dashboards using a sample
sales data set
so if you want to get the data and the
dashboard file that we'll be creating in
this demo then please put your email IDs
in the comment section of the video a
team will share the files via email
now let's begin by understanding what is
a dashboard in Excel
a dashboard is a visual interface that
provides an overview of key measures
relevant to a particular objective with
the help of charts and graphs
dashboard reports allow managers to get
a high level overview of the business
and help them make quick decisions
there are different types of dashboards
such as strategic dashboards analytical
dashboards and operational dashboards an
advantage of dashboards is the quick
detection of outliers and correlations
with Comprehensive data visualization it
is time saving as compared to running
multiple reports
with this understanding let's jump into
our demo for creating our dashboards
we'll be using a sample sales data set
let me show you the data set first
so here is the sales data set that we'll
be using for our demo
so this data set was actually generated
using a simulator and is completely
random it was not validated though we
have applied certain transformations to
the data using power query features
so this data as you can see has
1000 rows so using the simulator we had
generated thousand rows of data
similarly if I go on top you can see
this data set has 17 columns
now let me give you a brief about each
of the columns so first we have the
region column so we have Middle East and
North Africa this North America Asia
sub-Saharan Africa and others
similarly we have the country names from
which the item was ordered
the third column is the item type so we
have different items Cosmetics
vegetables there's baby food cereal
fruits
Etc
then we have the representative's name
or you can see this as the customer name
who ordered the product
then we have a sales Channel column so
there are basically two channels whether
the item was sold offline or online next
we have the order priority column
now here C stands for critical
then we have H which is for high
priority orders then we have M for
medium priority orders and finally we
have L which is for low priority orders
you can see the order date column
then we have the order ID the ship date
next we have
units sold
which is basically the total number of
units sold for each item
then we have the unit price column this
is the price at which each product was
sold
then we have the unit cost column which
is basically the production cost for
each of the items
next we have the total revenue the total
revenue is actually the product of unit
sold and unit price
then we have the total cost column now
the total cost column is actually the
product of units sold and the unit cost
similarly we have the total profit
column so total profit is the difference
between total revenue and the total cost
and finally we have created two more
columns that is order year and then we
have order month now these two columns
were actually generated using the power
query features so we use the order date
column which is this column and
extracted order year and Order month
so first we are going to create a
revenue dashboard where we'll focus on
generating reports for Revenue by order
year Revenue by year and region Revenue
by order priority and much more
will create separate po tables and po
charts and format them to make them look
more interesting and presentable
we'll add slices and timeline to attach
posts in order to filter it based on
specific fields
now let's create our first report to see
the total revenue generated each year
so we need to create a pivot table for
this I'll click on a cell in my data set
and then I'll go to the insert tab here
we have the option to select the pivot
table
I click on this
you can see
my table range is selected
next I want to place my pivot table in a
new worksheet
and let's just click on OK
there you go so we have a new sheet
where I can place my pivot table
so first I need to find the total
revenue generated by each year so what
I'll do is
I'll drag my order here column
under rows and then
I'll select the total revenue column
under values
you can see I have my Pivot chart ready
now if you want you can sort this
so from the data you can see we have
order Year from 2010 to 2017
now based on this data let's create our
pivot chart so I'll click on any cell
go to insert and here you have the
option to select recommended charts I
click on this
now actually I want a line chart so I'll
click on
line here and select ok
there you go so we have successfully
created our first pivot chart
now let me show you how you can format
this chart to make it more readable
so first let me delete
these so I'll right click and select
hide all field buttons on the chart so
this will delete the buttons present on
the chart now let me go ahead and edit
the chart title
so the title I want is total revenue
I'll type it down
by year
all right
next
let's do a few more transformations
so if I click on this plus sign
which is actually for chart elements we
have some options like to add access
access titles chart title data labels
this grid lines Legend and others
Okay so
let's remove the legend now
you can see the total Legend is gone
now let me
add access titles so we label our x-axis
and y axis so here under x-axis
I can
write it as
year
similarly on the y-axis I'll put Revenue
okay now you can move a bit
all right
now
let me select this chart Style option
and go to Colors first
here I'll select yellow color okay
and then let me go back to style
let's select a new style from this list
I want
this die
okay
now you can also add
data labels so I'll just click on data
labels
you can see we have
the revenue for each of the years now
this is not readable at all so we'll
format this a bit
if I click on this Arrow here I have
more options
if I scroll down you can see we have
something called as number here
I'll expand this
and under category I'll select custom
now here
will give a format code
which is a bit different
so this is actually a kind of a formula
so I'll write if my Revenue value
is greater than
let's say
9 lakhs
99
000.
let me make sure there are six nines
here so one two three four five and six
okay we are good to go
I'll close the bracket
I'll give a hash
give two commas
so if the revenue is greater than 9
lakhs 99
000 I'll put it in the format of
millions So within double quotes I'll
write m
I'll give a semicolon
followed by
another hash
and if the value is less than
the desired number
it should be 0 million let me click on
ADD
all right you can see how nicely we have
formatted our data and you see here we
have
added the new format which is in
millions
all right now if you want you can go
ahead and adjust the
boxes let me move this a bit up I'll
delete this
now if you notice this line chart you
can make a few conclusions for example
if you see here in 2010 the total
revenue generated was nearly 175 million
now this came down to 150 million in
2011 then the revenue constantly grew
from 2011 till 2014 it reached 195
million
and after 2014 it again came down to 180
million and the revenue dropped
significantly between 2016 and 2017. in
2017 the revenue was just 96 million
now before moving ahead to my next chart
let me just rename this sheet so I'll
write it as
Revenue
by year
all right
now
let's analyze the revenue generated each
year in different regions so for this
we'll create another pivot table
let me close this
I'll click on any cell go to insert and
select pivot table I just click ok so
that my PO table is placed on a new
sheet
all right
now this time we want the revenue by
each year and region so first of all
let's drag region to
columns
then
let's drag the order year column under
rows and then
I'll select total revenue onto values
so here you can see we have the pivot
table ready so for 2010 you can see in
Asia
this was the revenue generated
similarly if you see for 2013 this was
the total revenue generated in Europe
and
we have for other years as well
now let's create a line chart based on
this pivot table so I'll select any cell
in the pivot table I'll go to insert and
I'll click on recommended charts from
this list I'll select my line chart and
click on OK there you go so we have our
next pivot chart ready
so
on the right you see the different
regions that are present in different
colors let me just expand it so that you
can see all the regions we have
so in total we have
seven regions
and each of the regions have been
represented in different colors
so if you notice this graph
for the
sub-Saharan African region in 2012
sub-Saharan Africa made the highest
amount of sales
now from the sample data you can also
tell
that the revenue for North America has
been significantly low compared to other
regions
similarly if you see
for Europe
this was the revenue Trend between 2010
and 2017 so if you see here in 2011 the
sales
where at this level then it
significantly dropped in 2012 then in
2013
there was a huge Spike and then in again
came down in 2015 and so on so you can
make your own conclusions by looking at
these line charts now let's
format this chart so first of all let's
delete the field buttons present on the
chart and we'll also
delete the Legend all right now let me
just reduce the size of the chart
next we'll add a chart title so
will
give the title as Revenue
by
year end
region
okay
you can also format the
y-axis in terms of Millions
so I'll right click on this axis and
I'll select format access
I'll scroll down and here we have
the number drop down
let me scroll again
under category I'll select custom
and
we will use this format that we created
for our previous chart there you go you
can see our access labels have been
changed in terms of millions now
so let's close this and let me save it
now you can reduce the font size or
increase the font size let me just show
you suppose you want to increase the
font size of the chart title so you just
select it and from here you can either
reduce or you can increase you can see
now it's 12 if you want you can make it
16. similarly you can also edit the
access labels also
by selecting the chart title you can
also move it to left or right or you can
place it in the center as well for the
time being let me just keep it to the
left
all right
now
we'll see the revenue and total cost
by each region and will create a combo
chart for this so let me show you how to
do it
I'll go to my data sheet
I have my cell selected
go to insert and click on pivot table
let me just click on OK all right
so for this
I'll select
my region onto rows
and then
I'll have
two columns under values the first one
is going to be total revenue
and the next column will be the total
cost column
all right so here we have the pivot
table ready now based on this pivot
table let's create our pivot chart so
I'll go to recommended charts and if you
see below
at the bottom we have combo chart so
this is the preview of the combo chart
all right now let me just click on OK
there you go so we have a nice combo
chart ready here
now the way to look at it is the bars
represent the total revenue which is
this column
now
the line represents
the total cost
so let me go ahead and edit this chart a
bit so first of all let's delete the
field buttons all right and let's also
remove the legend from here
next we'll add data labels so I'll click
on data labels here
okay so these are the data labels for
the bars or the revenue column
now
let's format the detail labels in terms
of millions so I'll click on this Arrow
go to more options
if I scroll down
I have number
from here I'll select custom and
I'll choose my type that is in millions
all right so you can see we have
formatted our data
next thing we'll add a chart title
so here I'll write as
Revenue
by
region
it's actually revenue and total cost
by region
before moving ahead let me
rename the sheets as well I'll write
revenue and
total cost
similarly
sheet 3 also I am going to rename it as
Revenue
by
year and region
so this makes your sheet more readable
all right
now moving ahead
next we are interested to get the
revenue generated by order priority and
for this we are going to create a pie
chart
so let's go to the data sheet and create
our pivot table first
I click on OK
now
I'll select order priority under rows
and under values I'll select total
revenue
so this is a very simple pivot table so
you have your order priority so C is for
critical H is for high L is for low and
M is for medium now based on this let's
create a pie chart
so I'll go to recommended charts and
here you have pie chart
I want to select this 3D type of pie
chart and I'll click on OK all right so
we have our pie chart ready let me just
resize it and from here I'll remove the
field buttons
and also don't need the Legends I'll
delete this as well
all right now let's give a chart title
so this is going to be
Revenue by
order
priority
now let's add
our data labels
I'll check this option
okay now let's again format this in
terms of Millions
so here
I'll click on the last option I'll go to
numbers
under category I'll select custom and my
type is going to be in terms of millions
there you go
let me close this
I will just
move this
to the center
all right
now if you want you can change the color
of the text as well so let's
have it in white color and see how it
looks okay so this looks pretty decent
cool
now
moving to our next report
so this time
we are going to find the total revenue
by countries so we have multiple
countries present in our data set we
want to visualize the revenue generated
in each country
so for this we are going to create a
horizontal bar chart so let me show you
how to do it but before moving ahead let
me just rename this sheet so I'll write
Revenue
by
I'll just put op which stands for order
priority all right
now
let's create our horizontal bar chart
I'll go to insert
click on pivot table
and select ok
so I want my Revenue based on different
countries so I'll select country and put
it under rows
and then I'll choose total revenue and
place it under values
so here you have the different country
names we have Afghanistan this Albania
let me scroll down you have Bangladesh
there are
a number of countries you have Czech
Republic there's Estonia France Gabon
similarly if you scroll down
we have India there's Jamaica Italy and
all the way to the bottom if you go we
have New Zealand the Netherlands
Philippines Portugal
we also have
Singapore
lots and lots of countries
we have the UAE United States of America
Zimbabwe and others
all right let me go up
So based on this pivot table let's
create our people chart
so I'll go to insert and select
recommended charts
from here
I'm going to select the column chart you
can see the preview here and let me
click on OK
all right
so here you can see
the different country names at the
bottom and the revenue for each of the
countries
let's go ahead and edit this chart so
first of all I'll delete
the field buttons okay and let me also
remove the legend
here I'll write
Revenue
by
countries
this is going to be my
chart title
okay
let's format this chart a little more
so I'll click on this option
and we'll select a new style let's say
I'll select
style 6 okay
and
let me now go under colors and we'll
select the color of the bars so let's
choose this color okay
so you have Revenue by items so we'll
visualize
our revenue for different items present
in the table
so if you see this we have Cosmetics
vegetables cereal fruits this cloths
snacks households and other products as
well so let's check the revenue for each
of these items
so we'll continue the same drill I'll
create my pivot table on a new worksheet
and this time I'm going to drag
item type under rows and
will have the total revenue under values
so here on the left of the table you can
see we have the different item names and
then we have the
total revenue
so let me just
sort this
total revenue
from largest to smallest so you can see
here office supplies meet the highest
amount of Revenue
followed by household then cosmetics and
fruits made the lowest amount of Revenue
I'll click on this go to insert and
select recommended charts this time I am
going to create a bar chart so this is
how my bar chart is going to look like
I'll select OK all right now let's
format this chart a bit
I'll delete the field buttons and I'll
delete the legend as well
and
let's edit the chart title so this is
going to be
Revenue
by items
cool
we also want to change the
color of the bars so I have selected all
the bars I'll go to my home Tab and here
let's say I want to select green color
all right
I've edited my chart a bit now
let's
make it
14
and I'll remove the Bold
okay
here if you want you can change the font
Also let's keep it in blue color all
right
finally
let's rename this sheet so I'll write
Revenue by
items
cool
finally
now it's time for us to merge
all the charts that we have created
to our dashboard
so let me show you how you can create
the dashboard
I'll create a new sheet
and first thing I'm going to do is
I'll click on The View Tab and uncheck
grid lines so this will remove the grid
lines present in the worksheet
next
I am going to insert an image so we'll
have a background image on our dashboard
so the way to do is I'll go to the
insert Tab and under illustrations I
have the option to select pictures or
insert pictures so I'm going to insert
picture
present on the device that is my
computer
I'll go to desktop and here I have a
folder called Excel dashboard files
and I'll select this dashboard
background and hit insert
so this is going to insert an image now
let me just
drag this image so it covers a fair
enough portion
so I'll hit shift and I'll drag it
all right
so you can see I have successfully added
a background image if you want you can
still
expand this background image
a bit to the right
cool
now the next thing
is going to be
the title of the dashboard so I'll click
on insert and here
I have the option to select a text box
so
I'll click on a text box and
I'm going to place text box in the
middle
and I'm going to name this text box as
Excel
Revenue dashboard
on sales
I'll
Center Line it
let's do some more formatting so I'll
select this text box on the top you can
see ship format
here I'm going to expand this shape fill
and I'll select no fill
so my text box is transparent now and
I'll also remove the outline
all right
now let me just double click on the
title of my dashboard and I'm going to
select
a font you can select whichever font you
want
let me stick to Britannica bold
and I'll increase the size to let's say
30
all right I'll just
drag the text box
I'll make the text as white instead of
black
all right so we have our
title of the dashboard ready
now if you want you can also insert some
icons to this dashboard so I'll go to
insert and I'll click on illustrations
again and select pictures
I'm going to add
these two pictures which is of a store
and a cart
to make it look
visually appealing so I'll
place the icons
here and similarly let me just copy it
and I'll place the
cart and the store
to the right as well
all right
next
the idea is to
bring in all the charts that we have
created and place it on the dashboard
so let me copy
each of the charts and
place it
on the dashboard so I'll hit
Ctrl V to paste it
and we'll resize this as well
all right
similarly let me bring in all the other
charts as well
all right so now you can see I have
added
all my charts and graphs to this
dashboard
so you can see here we have
a line charts are
column charts the combo charts the Spy
chart
and others
now let me go ahead and format these
charts a little more so you can see this
looks a bit cluttered so let's adjust
the labels let me bring this down
similarly I'll bring
190 million
a little below
all right this looks fine now
one more thing we are going to do is
we'll remove the white background from
each of the charts and make it
transparent so let me show you how to do
it so I'll select this chart
then I'll right click and go to format
chart area
here on the right you see we have an
option called No fill so if I select no
fill you can see the white background
it's gone now similarly let me also
remove the grid lines so I'll select the
grid lines and hit delete so I've also
removed the grid lines from here now
let's also remove the white outline that
we have so I'll select this chart go to
format and here
I'll go to shape outline and I'll select
no outline
you see this so we have our total
revenue by year which is a line chart
and this is completely transparent now
now what I'm going to do is
I'll place this chart over a box
so I'll go to insert and in insert we
have the option to create a shape
so I'll click on illustrations and here
I'll choose a shape and let me select a
rectangle
so I'll just
create a rectangle here
all right and now
what I'll do is
I'll select this and bring this to front
I'll right click
and choose
bring to front
and I'll place this shape
below it
all right now the next thing is to edit
the shape
so first I'll change the color of this
box so let me select this blue color
and and let me increase the transparency
so I'll
right click and go to format shape here
I'll increase the transparency let's
keep it to
25 percent
or let's say
20 percent all right
next thing
we'll just convert all the
font to white color
including the access labels
the chart title
will also
convert all the
access labels to white color so it looks
better now we'll just adjust our
chart over here
next thing let's just
remove the outline so I'll go to shape
outline and I'll select no outline you
see
we have now formatted our chart let's
just
pull this a little up
all right now we'll add this blue
background to all the other charts so
we'll first add the background make it
transparent and then we'll convert the
font text to white color to make it more
readable and visible so
for the time being I'll just pause the
video and come back again
all right so now you can see on your
screens we have nicely formatted our
dashboard so I've added a few logos for
each of the charts you can see the logos
here so for
Revenue by countries we have a globe
then if you see here this is kind of a
map or a location
similarly we have all formatted the
color of the bars
then we have also formatted the labels
in terms of Millions
if you look on the y-axis even the
revenue for year-end region are all
formatted in terms of Millions
if you want this you can also format the
total year by Revenue in terms of
millions so the way to do is you can
select this graph
right click and go to format access
here if I scroll down you have numbers
and under category I'll select custom
then I'll select my type as
this format which is in millions
and you see here we have successfully
formatted our
y-axis labels
all right
so the next thing
is to add slicers and timelines to a
dashboard now slicers are used to format
your data based on a particular column
suppose if you want to see Revenue by
certain items you can add item as a
slicer and you can view the entire
dashboard similarly for timelines you
can add date columns so if you want to
see what was the amount of sales or
revenue generated on a particular year
or a particular month you can do that
using a timeline
so I'll select one of the charts
and then either you can go to the insert
Tab and here you can see under filters
we have slices and timeline or if you go
to the pivot chart analyze tab here also
you have insert slicer and timeline
option so I'll select insert slicer here
first now it's giving me the list of
Fields present in the data set so I'll
select
country
region and let's say we want to
know by
item type and sales channel so these are
going to be my four
slicers I'll click on OK
you can see it here we have our
four slicers here and these are the list
of values under region we have Asia
this Europe North America and others
similarly we have the different country
names for Country slicers and then for
item type also we have all the items
that were present in our data set
now
moving ahead we need to connect
all the slicers to a dashboard so what
I'll do is
I'll right click
on this option
and I'll go to report connections
okay so under report connections you
have
all the pivot tables that we created
you currently see only one of the pivot
table is selected so we need to select
all the pivot tables so let me check
all the pivot tables present in this
workbook and click on OK
all right now that we have connected one
of our slices we'll now connect the
other remaining slicers so I'll right
click on this go to report connections
and I'll check
all the pivot tables present in this
worksheet
click on OK
similarly let's do it for the country
slicer
I'll go to report connections and let me
select
all the pivot tables
and finally we have the item type so
I'll right click go to report
connections and then I'll select all my
Pivot tables
and let's hit OK
all right now let me just organize this
a bit so I'll
place my Pivot tables
to the right
I'll just reduce the size
let me scroll down
I'll add
my region slicer here
similarly
I'll add my final slicer that is sales
Channel
now in our next dashboard which is going
to be the profit dashboard I'll show you
how to add a timeline
all right
now I have arranged all my slicers
so let's see you want to find the
revenue that was generated for an item
type let's say
beverages so you can just select
beverages here
and
all your charts
show the respective revenues so you have
the total revenue by year for beverages
only
similarly here you can see the revenue
by year and region only for beverages
item type
if I scroll down
this chart represents the revenue that
was generated in each of the countries
only for item type beverages
let me just uncheck it
all right
let's say you want to see
the revenue generated for a country like
India
so I have selected India here
and now you can see
My Graph has changed only for
country India
you can see here it is showing only for
India now
similarly you can also filter your
revenues based on the different regions
let's say you want to know the revenue
generated based on sales channel so we
have two sales channel that is offline
and online suppose we want to know the
revenue generated offline so I'll just
select
offline you can see the values have
changed so these were the revenues
generated for each of the items only for
offline
if you
see here now these were all the offline
seals for the different regions
so
this is
our entire Excel Revenue dashboard on
sales
we created multiple charts and graphs
then we applied different formatting we
added different icons then we formatted
the labels also next we added slicers
and
finally we saw how we could filter our
data based on these slicers
likewise now we are going to create a
profit dashboard based on the same data
so before moving ahead let me rename
this sheet as
Revenue dashboard I'll write Rev
dashboard okay
Now we move to our data sheet and
start creating a PO tables and pivot
charts for the profit dashboard
all right so let me go ahead and create
my first pivot table so
I'll create a new worksheet
this time I am going to create a line
chart
to visualize the profit for each year
so I'll drag my
total profit column two values and my
order here to rows
so here you can see we have our
pivot table ready
now you can sort this data
to get an idea as to which
year had the highest profit and which
year had the lowest profit so from this
pivot table you can see since I've
sorted this data in descending order so
2014
had the maximum amount of profit and
2017 had the least amount of profit
I'll just do Ctrl Z to undo it all right
now based on this pivot table let me go
ahead and create my PO chart so I'll go
to recommended charts and click on a
line chart so this is the preview of the
chart
I'll click on OK
let me close this
similarly
we are going to
edit this chart now so first I'll hide
all the field buttons present on the
chart and I'll
rename the chart title as total
profit
by year
next I am going to remove the legend so
I'll delete this
let's do some more formatting
so I'll go to style
and
this time
I am going to select
my style type okay and if you want you
can
choose the colors as well for the timing
let's have this yellow color
next let me add the data labels so again
if you see here this is not formatted
properly so let's go ahead and format
the
data labels so I'll click on number
and I'll select custom here
and the type I'm going to select is in
millions and I'll click on
close so here you can see we have our
line chart ready which shows total
profit by year
let's
rename this sheet as
profit
by
year
all right
now let's move back to our data sheet
again
next
we are going to
show the total profit by countries for
this I am going to create a map so let
me first create my pivot table so I'll
go to insert and I'll click on pivot
table
let me click on OK
since I want the country names so I'll
select country under rows and then I
have
my total profit under values
the next thing I'm going to do is I'll
just rename the row labels as
countries
and then
I'm going to delete the
grand total which you can see at the
bottom so here we have the grand total
let me just delete the grand total so
I'm going to select this pivot table go
to the design tab here we have subtotals
in Grand totals I'll
switch off the grand total let me just
verify it again I'll scroll down you see
the grand total has gone now
all right now we want to create a map
out of this the way to do is
I'm going to
select my data
copy it
'll go on top
and I'll paste it here
using this data I can create my field
map now so I'll go to insert here we
have the option to create a field map
there you go
you can see we have our map ready
I can expand this
now as you can see our map has a color
scale which comes from light gray color
to dark blue color
so the countries that are in Gray or you
can see light blue
have the lowest amount of profit while
the regions or the countries that have
been shaded in
dark color a dark blue color have
highest amount of profit
I will go ahead and delete this scale
okay
next we need to connect this map to the
original data source so what I do is
I'll right click on this map and I'll go
to select data here instead of the
previous range I'll give my new Range
now so my new range
will be my original pivot table that I
had created
I'll go on top and click on OK
so we have our
map ready now now if you want you can
change the color of the shade so I'll
just go to colors and let's see we'll
keep green color
so the countries that are shaded in dark
green have the highest amount of profit
while those which are highlighted in
light green color are
the countries that made least amount of
profit
okay
now moving on
next
we want to create a pivot table
that will show us the profit by
year and sales Channel
so for this we are going to create
another line chart
so I'll go to insert and click on pivot
table so I'll select new worksheet here
since I want to know the profit by year
first of all I'll drag my order here
column to rows
and then
I'll choose my total profit column under
values
next I'm going to select my sales
Channel under columns there you go so we
have our pivot table here based on this
pivot table let me create my PO chart so
I'll go to recommended charts and I'm
going to create a line chart
I close this
you see here
based on this chart you can tell
The Profit generated with
online sales were actually lower than
that of offline so here the Blue Line
represents offline profit and the Orange
Line represents Online Profit
if you Mark Clearly
in year 2012 the Online Profit was
actually higher than the offline profit
so let me go ahead and edit this chart a
bit so we'll delete the field buttons
I'll also delete the legend for now
let me go ahead and add
a chart title so I'll write
profit
by
year and
sales Channel
okay
so this is my
second report before moving ahead let me
just
rename this sheet so I'll write
profit
by
countries
similarly
let me rename this sheet as
profit
by year and let's say SC for sales
Channel
okay
moving ahead
now I want to create a pie chart
based on a pivot table that will show
The Profit by sales Channel only so this
is going to be a simple pie chart so
I'll first go to insert click on pivot
table and click on OK
so I'll drag my sales Channel under rows
and then we'll have the total profit
column under values
so this is my simple pivot table now
let's create our pivot chart which is
going to be a pie chart let me explore
the other types of pie charts we have
okay so I'm going to select a donut
chart here I'll click on OK
let's
edit this chart I'll remove the field
buttons let me now remove the legend as
well
I'll just resize it
and this is going to be
profit
by
sales Channel
okay
let's also add
data labels
and here again I am going to
format this label
I'll select the category as custom and
by type
will be in millions okay
let me just
move this
to the left
and this to the right
okay
let's also delete the lines cool
now let me just rename this sheet so
I'll write
profit
by
let's say C which stands for sales
Channel
cool
finally
I am going to create a report that will
show the revenue and profit by items
so I'll go ahead and create my pivot
table first
this time I'll choose my
total profit under values
and we'll also have the revenue column
so I'll put my Revenue at the top
then I'm going to select
item type under rows
so here is my pivot table
based on this pivot table let me now
create a combo chart
so you can see the preview of the chart
the blue bars represent the total
revenue and the Orange Line represents
the total profit
I'll click on OK
let me close this
first
let's remove the field buttons
let's also remove
the legend here
then we'll add a chart title I'll name
it as
Revenue
and
profit
by
items
okay
if you want you can also go ahead and
change the color of the bars
so let me just select
one of these colors okay
all right
so we have our five reports ready that
we are going to use for our profit
dashboard
next let's create a new sheet and we'll
get started with building our dashboard
so I'll click on a new sheet let me just
rename this as
profit dashboard
all right
we'll continue with the previous drill
so first of all let's go to the view Tab
and remove the grid lines
now we'll insert a background image
like we did for our Revenue dashboard so
I'll go to insert under illustrations
I'll click on pictures and select this
device
I am going to have the same background
I'll click on insert
all right so you can see we have a
picture of a company or you can say an
organization let's just drag this
a bit to the right
will adjust the size also
all right
now let's copy the title of my profit
dashboard so here you can see I have
brought my Revenue dashboard
and I'll copy the title and the logos
that we used
for the revenue dashboard
I'll paste it on my new dashboard
let's just align it
in the center
all right the next step
let me now go ahead and edit the title
so this is actually going to be
Excel
profit dashboard instead of Revenue
now we'll copy each of the charts that
we just created for example
the revenue and profit by items then we
had
profit by sales Channel
all this we are going to copy one by one
and put it on the profit dashboard
so let me just copy a few now
we'll paste it here
and later on we can make the adjustment
and copy this as well
similarly I'll bring the other three
charts onto my sales dashboard
okay so here on my
profit dashboard I have added all the
charts
and have
aligned and reshaped it so that it looks
good
I've also made some formatting for
example I have reduced the size of the
chart title
now let me go ahead and
show you a few more formatting that we
also did for the revenue dashboard first
let's remove the white background from
all the charts so I'll
select the first chart I'll right click
and I'll
click on
format chart area
here under fill I'll select no fill
next I am going to remove the grid lines
so I'll just delete it
let me close this
now we also have a
outline so I'll go to design
actually format and I'll remove the
outline
next I am going to add
a blue box at the back like how we did
for the revenue dashboard so let me
select a blue box from here and I'm
going to paste it here
okay
let me just select
the chart and I'll bring this to front
and I'll move this
to the back
next I'm going to change the font color
all to White
so that it's clearly visible and it's
more readable
I'll do it for the x-axis as well okay
so here I have my first
chart ready the same I'm going to do for
the rest of the charts
okay so now you can see here I have
formatted all my charts I have also
added a blue background
you see here I have also formatted the
y labels in terms of millions which is
actually the profit similarly here I
have added the data labels this is for
Revenue
some of the charts also have the data
Legend so here you can see the blue
color represents offline
and the red represents online similarly
here
you have the legends
have also formatted the map as well
okay now the next thing
is to make this dashboard more
interactive so we'll add our slicers as
well as timeline first let me show you
how to add a timeline so I'll select one
of the charts and I'll go to insert
under insert I have the option to create
a timeline
so I'll just
click on timelines
so timeline is actually based on date
columns so since in our data set we only
have two date columns one is order date
and one is ship date so Excel has only
shown us two columns so I am going to
create my timeline based on order date
so I'll select my order date column and
I'll click on OK
you can see here this is called a
timeline I can expand this now this
timeline is based on months now and if I
scroll this timeline you can see here I
have my order year 2010 and I have all
the 12 months similarly we have for 2011
then we have for 2012 all the way till
2017. now you can filter this in terms
of years quarters months or days let me
just select year now so I have
years from 2010 till 2017. let me just
squeeze this and I'll place it somewhere
here on the right
now let me go ahead and create a few
slicers for my profit dashboard so I've
selected one of the charts
under insert I'll click on slicer
you can see it gives me the list of
columns from which I want to create
slicers so I'll create region
let me also select country
let's say I want
the representative's name or the
customer's name
and I'll click on OK
so here I have created three slicers
let me
first resize it and
I'll place it
on the right
similarly I'll place the
country column also
then we have
the region slicer
I'll resize this and I'll bring it here
okay
the next thing we need to do is
I have to connect all the slicers and
the timeline
to
the pivot tables
for the profit dashboard so I'm going to
click on
the multiple select option and go to
report connections here
I'm going to select
all the pivot tables that are related to
profit
so here I have selected four and I need
one more which is view table number 10
if I click on OK
similarly let's create or connect my
region filter to all the pivot tables so
I right click go to report connections
here I'll
choose all my Pivot tables which are
based on profit
I'll click on OK
let's do it for the country slicer as
well
click on OK and similarly I'll connect
my timeline as well I'll go to report
connections
and I'll select
all the pivot tables related to profit
then I'll click on OK
let me now go ahead and create another
slicer based on sales channel so I am
selecting one of the pivot charts I'll
go to insert click on slicer and I'll
select
sales Channel and hit OK
so I have my sales Channel
slicer now let me connect it to all the
respective pivot tables
that are based on profit
click on OK
now
let me just bring it here
all right
the next thing I want to show is
how are we going to use the timeline
first
so you see we have all the
years here from 2010 till 2017 or
suppose we want to know the profit that
was generated in the year 2012 so I'll
just click on this range and now you can
see our
charts only so information for 2012 so
this dot represents there was 51 million
profit in the year 2012. similarly you
can see here
The Profit by sales channel for 2012
from the map you can see the different
countries and the
profit each of these countries made in
2012
if I scroll down you can see the revenue
and profit by items
now if I select
another year let's say 2013 I can just
drag this to the right and
now you can see
our profit by year and sales channel for
offline and online you can see the map
or the line chart for total profit by
year so in 2012 it was 51 million and
then it went up to 54 million in 2013.
similarly
our map has also changed now this is a
sort of an information that we have you
can click on this and
check the information that
Excel has prompted all right so this is
how you can use a timeline now as I said
we checked by
years you can also see it for months and
quarters as well
let me just
uncheck it
I'll
send it back to the place where it was
and I'll reduce the size
okay
now suppose you want to check the profit
made by different Representatives you
can select them one by one let's say
Adam Churchill
this is the
profit generated by a Adam Churchill
similarly
you can select multiple persons as well
now suppose you want to see the profit
by different countries so you can use
the country slicer let me just bring
this to the middle and
let's expand our chart a bit
okay so here you have the
profit by different countries chart I'll
just
bring this to the front so that you can
see it clearly okay
now here suppose you want to see the
profit generated
in let's say United Kingdom you can
select United Kingdom so this is the map
of United Kingdom and it tells you the
total profit that was generated in
United Kingdom and
below you can see
the revenue and profit for all the items
that was sold in
United Kingdom so you had beverages
clothes household office supplies so you
can see clearly
office supplies item meet the highest
amount of profit in
United Kingdom
now you can also select multiple
countries let's say I want to know for
France as well
so my
map will change accordingly so now I
have
United Kingdom and France selected and
the other charts present in my dashboard
change accordingly now I have my
country selected as India you can see
the map of India here
and these were the respective profit
values now one thing to note here is
this is actually
not millions that should be in K that is
Thousands so please mark this as
1000 and not in millions
even for this this is actually K and
naught million
all right
so we have successfully
created our second dashboard that is on
profit
let me just resize this a bit and we'll
place it
where it was earlier
cool
so we saw
how to create different pivot tables and
pivot charts and then we formatted our
po charge based on our requirement we
saw how to edit the colors now let me
show you one more thing you can also
change the
look and feel of the dashboard by going
to the page layout tab under page layout
you have themes so here you can select
different themes currently we are
with the office theme now let me just
select another theme let's say face it
you see the colors have changed and it
looks
really beautiful
similarly let me try out
another
theme let's say organic you see our
chart has changed let me just delete
this okay so now once you change the
theme the text also change a bit you can
see
the slicers are in a different font
let me explore One More theme let's say
this time I'm going to choose
depth
and this is more
of a
green type of color
you can play around and select
whatever theme suits the best
all right
now let me just move back to my Revenue
dashboard and see how it looks there you
go so since we changed our theme even
our Revenue dashboard is also impacted
so
this is how it looks now
you can always go ahead and play with
different themes colors fonts and
effects all right
so in this demo we saw how to create a
revenue dashboard so we created line
charts this combo chart pie chart
horizontal and vertical
bar charge and then we learned how to
add slicers and connected to different
pivot tables and we filtered our data to
see
Revenue as well as profit by items by
countries by different regions
sales Channel
we learned how to create
a map
and lots more
let's quickly see some more examples of
doing data analysis using Excel and for
that we can use some inbuilt add-ins
which can be added to our Excel sheet so
for example if you would want to do a
descriptive analytics or descriptive
analysis on your data say for example
getting your descriptive statistics such
as your mean median mode and so on so we
can do that and we can use Excel for it
so for example you can if you are given
some data say I have temperature price
of ice cream units sold and I would want
to have descriptive statistics on this
what I can do is I can click on file and
here in file you can click on options
and within options click on add-ins now
within add-ins you have Excel add-ins
which is selected here so click on say
go for example that shows what add-ins
are available and you can choose which
ones are you interested in so for
example I have chosen analysis tool pack
and solver add in and click on ok now
that basically should add more options
to your Excel so if you click on data so
here you see data analysis and solver
and this is what we would want to use to
get our descriptive statistics for these
three columns so for example let's say
temperature
or you can even give the names later
once you get your descriptive statistics
so for example let's go for data
analysis and here it says what are you
interested in there is a two factor with
replication you have correlation
covariance descriptive statistics you
have histograms so let's click on
descriptive statistics click on ok now
this one basically asks your input range
so while your cursor is blinking here
also it is said grouped by so let's give
it a range so for example I will say say
temperature
now if I do this and I have selected The
Heading just look at that and now you
need an output range so let's just
select this and then you can have your
cursor blinking in here let's select
save Fields here and this is where I
would want the output now it also said
what options do you want so it has
output range we can then select summary
statistics confidence level so I will
say summary statistics is what I'm
interested in say okay and this says
input input range contains non-numeric
data now why is that because we chose
temperature The Heading also so click on
OK and here we will alter the range so
this one is our range should be only
the values numeric values on which we
would want the descriptive statistics we
have output range already selected we
have summary statistics and now you can
click on OK
and that basically gives your
descriptive statistics for temperature
so here I could basically
given a value for this so I can say
temperature and that's my descriptive
statistics for temperature might be I
can just do some formatting and
that's it so that gives me descriptive
statistics for the values here now
similarly we can do it for price of ice
cream so what we need is we need to
basically go for data
data analysis descriptive statistics say
okay now you need to give a range so
here I will change my range to these
values output range is already selected
now we are interested in summary
statistics click on OK and this says
output range will overwrite existing
data press ok to override data in range
I will say cancel no that's not what we
want to do we need to give a new Range
so let's select our new Range which is
here and now click on OK so now we get
the values which is for your price of
ice cream so again we can basically
select this and say price of ice cream
and we got our descriptive statistics
for price of ice cream and like we did
earlier I can select this I can
basically do a merge and center and that
gives me descriptive statistics for
price of ice cream so we could also
basically change this now I can go into
data and I can go into Data analysis
descriptive statistics so we know that
we had selected this B2 to B8 and this
one which is H6 to h19 we would want to
shift it might be two columns up so
maybe I can just say H5 and I can
manually change it to a 17 and let's say
okay
and
we will basically get this and I can get
rid of this so I can have it in the same
range so similarly so this one will have
to be renamed and I can basically say
price of ice cream
and that's basically my descriptive
statistics for my price of ice cream
and similarly we can do it for the third
column which is units sold
so we would want to have this now let's
see we can click on data we can click on
data analysis descriptive statistics so
we need to give the range correctly so
this time our range changes to units
sold
now we can also say labels in first row
okay if we were selecting The Heading so
let's do it in this way so in my range
in my range let me empty this I can
basically select this which we know has
non-numeric data in the first row say
for example I'll say labels in first row
I am interested in summary statistics
and this range will now have to be
changed from H to basically something
like J so let's say J and let's select
these values so that should take care of
things and now you see you have your
unit sold you did not have to manually
rename it
and you have basically got the
descriptive statistic so this is how you
can simply perform analysis using data
analysis here you can basically get your
descriptive statistics for your columns
and then you can do whatever needed
formatting you need to basically make
your data look
in a
good way now let's look at one more
example of data analysis where we may
want to look at the frequency of values
or frequency of values occurring in a
range of values so for example if you
have been given temperatures you have
been given some pins where you would
want to identify how many values fall
into the range of 0 to 20 20 to 30 30 to
40 40 to 50 and the easiest way to do
that would be creating histogram now
histogram is usually used for data
analysis where you would want to look at
different variables or say features for
example temperature is one such feature
might be there might be one more
variable or feature such as sale of ice
cream and you would want to see if the
increase or decrease in temperature
affects the increase or decrease in
sale of ice cream might be sale of ice
cream is a is a response based on
temperature so it depends so sometimes
you may want to find the relationship
between two variables whether they are
positively or negatively related or you
would want to do different kind of
analysis and in certain cases we may
want to First do analysis on one single
variable look at the frequency of values
might we also look at the defects and
for which we can use something like
parrot or chart so we can go for
histogram and that basically gives us
the frequency of values now how do we do
that so we have already added the
add in which is data analysis earlier so
we can just use the same thing again
here we would want to create a histogram
so let's say okay now I have already
selected input range so if you see here
my input range is temperature which is
also with the headings and I have bin
range which is basically the range of
values so for example I can select this
and that's my bin range I am selecting
or the option labels because I'm using
the first
row which has the heading such as
temperature and pins now we need to give
a output range so for example let's say
I would want my data here and that
becomes my output range so you can have
a sorted histogram or basically a Pareto
chart so if that's what you are
interested in looking at the frequencies
for your different ranges and here I am
also selecting chart output because I
would want to have a visual histogram
which gives us the frequency and it's as
simple as this just click on OK and now
you get your pins so it basically tells
you frequency of values which is
basically 20 but that does not mean it
is only talking about the values 20 it
is basically talking as a range of 0 to
20 so we have 0 to 20 that is 2 so we
can basically say there is 120 here with
the 20 being the maximum value and then
there is one more 20 so that's your 0 to
20 then you have 20 to 30 which shows
three values so might be in that case I
can say 26
is one thing then I can say 30 that's
the second one and then basically I can
look at 22 so basically this one does
not select 20 as the lower range but it
basically selects 30 as the higher range
so I do see 20 to 30 there are three
entries similarly we can see values for
40 and 50 and since we have selected
Pareto or sorted histogram that shows in
a descending order what is the highest
frequency of values within a particular
range
so that shows me highest frequency is 5
and then you have 3 and 3 and then two
so this is how we can create a histogram
and we can perform analysis on a single
variable
now
as discussed earlier as I said sometimes
we may be interested in finding out the
correlation between different variables
such as say here we have temperature
price of ice cream and units sold and we
may want to
find out the correlation between one
variable to another variable or we would
want to find out the relationship
between variables are they linearly
related are they positively related
negatively related and so on and for
that we can use the correlation of your
data analysis add-in so for example you
want to find out correlation of
temperature and units sold and what we
can do is we can find out that using a
formula so for example if I search for
something like Co relation
and let's search so there is a function
called correlation which we can use and
we can use this to calculate the
correlation of temperature and units
sold so for example let's select this
and that's the function so it says give
me an first array and a second array so
we are interested in finding out
correlation of temperature in units sold
so let's select the range of values for
temperature and then I am interested in
finding out the correlation of
temperature in units sold so let's
select this and that basically gives me
a range of values it gives me the
correlation value which is 0.2859 say
okay and that's your value
so similarly we can do it for
temperature and price ice cream so let's
go for
correlation so that's the function we
are interested in you need to give a
range of values so here we are
interested in temperature and price of
ice cream so let's select temperature
and then the second array or list of
values is price of ice cream let's
select that let's close our bracket and
here we have the correlation value of
temperature and the price of ice cream
similarly you may be interested in
finding out temperature and units sold
like what we have done earlier so we can
do the same thing based on function so
this is same as correlation of
temperature in units sold so I can get
rid of this one now how do I do it using
the data analysis add in
so for that what we need is we need to
go into Data we need to go click on data
analysis and here you have the option
called correlation let's select this now
that basically needs an input range so
we need the range now I might be
interested in finding out the
correlation between temperature and
price of ice cream and units sold so
I've selected all the columns here we
will say Group by columns obviously we
need to select labels in first row
because that is basically taking care of
the first row as heading now output
range you can just give one simple cell
and that's where your data will start
from or you can give a new workbook so
click on OK and that basically gives you
that basically gives you a correlation
of your different variables and what are
the values and we can check these values
based on the values what we have here so
we have basically temperature and price
of ice cream
and that basically shows me
0.96149 you have temperature in units
sold so you have 0.2859 now you can also
look at units sold and say for example
price of ice cream you can look at these
particular values so if I would be
interested in finding out what is the
relationship between these variables I
can easily find using correlation so I
could be basically writing in a formula
here and selecting what are the cells so
here we were selecting A2 and C here we
were selecting A and B now might be I'm
interested in price of ice cream and
units sold and if that's what I'm
interested in then I will give a range
of B2 to B8 C2 to C8 and similarly you
can get your analysis or correlation
values so it's very simple table in
Excel and you can use either the data
analysis tab
and get your correlation or you can use
formulas and do that
now one more important part of data
analysis is doing your sampling now
sampling could be periodic sampling or
random sampling so sometimes you may
want to look at a variable and you may
want to get some values based on
periodic data that means might be I am
interested in range of values I am
interested in seeing a sample of values
for
a particular period which could be
basically
a range of values or you could just do a
random sampling so for example if I go
for periodic sampling so out of these
values which I see here might be I want
to see say periodic sampling that is a
frequency of two values how many times
we have these values occurring here or I
would go for random sampling so
basically randomly I would want to pick
up say three temperature values now how
do I do it so for example here I have
seven values now if I go for periodic
sampling
the sample or periodic sample value
which I need to give has to be lesser
than the total input values
so for example we can do this let's go
in here and let's go for data analysis
so we can go for sampling here click on
OK and that needs a range of values so
we will select a22 A8 now I could have
selected all the values for this one
temperature and in that case I can give
labels which is going to take care of
the first
row now here we can go for
number of samples which you are
interested in or giving a period so
let's go for period and say for example
I have seven values so what if I select
five so for example if I say 5 that
means
I could just get one value so basically
when I'm saying 5 out of 7 so that's
just giving me out of five I want one
value so I can then just give an output
range so here I can basically select
this cell I'll say okay and now you see
it just shows me
one value
so out of the first range that is I have
said 5 it has given me the fifth value
that's your periodic sampling so for
example we want more values so let's
reduce this period to might be 2 which
basically gives me every second value so
I can basically say for example 2 and
say OK and then say okay so that shows
me 26 then you have 35 then you have 40
and then well this one does not have any
more values so that's your periodic
sampling now if you go for random
sampling that's basically randomly
picking up values and you can choose how
many values you want so go for data
analysis go for sampling I'll go for
number of samples how many you want so
for example out of seven values randomly
I want three values
and I can just give this say okay and
then
we will do a cancel because we need to
change that range so let's select this
and say okay and that gives me random
three values from this values of
temperature so we can use Excel to do a
simple sampling and we can choose
whether we would want to go for periodic
sampling or random sampling since this
is data analysis with python we got to
ask the question why python for data
analytics I mean there's C plus plus
there's Java there's dot net from
Microsoft why do people go to python for
it
so the number of reasons one it's easy
to learn with simple syntax you don't
have a very high type set like you do in
Java and other coding so it allows you
to kind of be a little lazy in your
programming that doesn't mean that it
can't be set that way and that you don't
have to be careful it just makes means
you can spin up a code much quicker in
Python the same amount of code to do
something in Python A lot of times is
one two or three or four lines where
when I did the same thing say in Java I
found myself with 10 12 13 20 lines
depending on what it was
it's very scalable and flexible so
there's our flexibility because you can
do a lot with it and you can easily
scale it up you can go from something on
your machine to using uh Pi spark under
the spark environment and spread that
across hundreds if not thousands of
servers across terabytes of data or
petabytes of data so it's very scalable
there's a huge collection of libraries
this one's always interesting because
Java has a huge collection of libraries
C has a huge collection of
libraries.net does and they're always in
competition to get those libraries out a
scale up for your spark all those have
huge collection of libraries this is
always changing but because Python's
open source you almost always have easy
to access libraries that anybody can use
you don't have to go check your
licensing and have special licensing
like you do in some packages
graphics and visualization they have a
really powerful package for that so it
makes it easy to create nice displays
for people to read
and community support because python is
open source it has a huge community that
supports it you can do a quick Google
and probably find a solution for almost
anything you're working on
python libraries let's bring it together
we have data analytics and we have
python so when we're talking data
analytics we're talking python libraries
for data analytics and the big five
players are numpy pandas matplot Library
scipy which is going to be in the
background so we're not going to talk
too much about the scientific formulas
inside pi and PSY kit
so numpy supports in dimensional arrays
provides numerical Computing tools
useful for linear algebra and Fourier
transform
um and you can think of this as just a
grid of numbers and you can even have a
grid inside a grid or data it's not even
numbers because you can also put words
and characters and just about anything
into that array but you can think of a
grid and then you can have a grid inside
a grid and you end up with a nice
three-dimensional array if you want to
talk three-dimensional array you can
think of images you have your three
channels of color four if you have an
alpha and then you have your X Y
coordinates for the image we're looking
at so you can go x y and then what are
the three channels to generate that
color
and numpy isn't restricted to three
dimensions you could imagine watching a
movie well now you have your movie clips
and they each have their X number of
frames and each of those frames have X
number of X Y coordinates for the
pictures in each frame and then you have
your three dimensions for the colors so
numpy is just a great way to work within
dimensional arrays
now closely with numpy is pandas useful
for handling missing data perform
mathematical operations provides
functions to manipulate data
pandas is becoming huge because it is
basically a data frame and if you're
working with big data and you're working
in spark or any of the other major
packages out there you realize that the
data frame is very Central to a lot of
that and you can look at it as a Excel
spreadsheet you have your columns you
have your rows or indexes and you can do
all kinds of different manipulations of
the data within including filling in
missing data which is a big thing when
you're dealing with large pools or lakes
of data where they might be collected
differently from different locations
and matplot Library
we did kick over the sci Pi which is a
lot of mathematical computations which
usually runs in the background of the
for of numpy and pandas although you do
use them they're useful for a lot of
other things in there but the matplot
library that's the final part that's
what you want to show people and this is
your plotting library in Python several
toolkits extend matplot Library
functionality there's like a hundred
different toolkits to extend matplot
Library which range from how to properly
display star constellations from
astronomy there's a very specific one
built just for that all the way to some
very generic ones we'll actually add
Seaborn in when we do the labs in a
minute several tool kits extend matplot
Library functionality and it creates
interactive visualization so there's all
kinds of cool things you can do as far
as just displaying graphs and there's
even some that you can create
interactive graphs we won't do the
interactive graphs but you'll see you'll
get a pretty good grasp of some of the
different things you can do in matplot
library okay
python module you need for doing data
science and data processing there's so
many other modules that come off of it
there's actually sits kind of on numpy
so if you already had our numpy array
hopefully you've already gone through
the numpy tutorial one and two So today
we're going to cover what is pandas
we'll discuss series we'll discuss basic
operations on series and then we'll get
into a data frame itself basic
operations on the data frame file
related operations on a data frame
visualization and then some practice
examples roll up our sleeves and get
some coding underneath there and let's
start with just some real general what
is pandas pandas is a tool for data
processing which helps in data analysis
and provides functions and methods to
officially manipulate large data sets
now this is a step down from say using
spark or Hadoop in Big Data so we're not
talking about Big Data here but we are
talking about pandas I mean there is
some connections there's like an
interface going on with that so there is
availability but you really should know
your pandas because if you're working in
Big Data you'll know there's data frames
well pandas is a data frame primarily it
has a couple different pieces we'll look
at here and if you've never worked with
data frames before a data frame is
basically like an Excel spreadsheet you
have rows and columns you can access
your data either by the row or the
column and you have an index in
different that kind of setup and we'll
dig more into that as we get deeper into
pandas but think of it as like a giant
Excel spreadsheet that's optimized to
run a larger data on your computer and
then I said it that it's a data frame so
the data structures in pandas are series
one-dimensional arrays and then we have
data frame two-dimensional array and it
really centers around the data frame the
series just happens to be part of that
data frame and here's a closer look at a
pandas series series is a
one-dimensional array with labels it can
contain any data type including integers
strings floats python objects and more
so it's very diverse if you remember
from numpy we studied they had to be all
uniform not in pandas and pandas we can
do a lot more and pandas actually kind
of sits on numpy so you really need to
know both of those if you haven't done
the numpy tutorials and you can see here
we have our index one two three four
five and then our data a b c d and e
very straightforward it's just two
columns and we have a nice index label
and a column label for the data and then
a data frame is a two-dimensional data
structure with labels we can use labels
to locate data and you can see here we
had if we go back one we had our index
one two three four five so in each one
of these series they would share the
same index over there the row index so
you have your row index DF dot index and
then you have a column index DF dot
columns and this would look like I said
this would be really familiar if you've
done any work with spreadsheets Excel so
it kind of resembles that this does make
it a lot easier to manipulate data and
add columns delete columns move them
around same thing with the row so you
have a lot of control over all of this
now we're of course going to do this
inner Jupiter notebook you can use any
of your python editors but I highly
suggest if you haven't installed Jupiter
and haven't worked with it it is
probably one of the best ways for easily
displaying a project you're working on I
skip between a lot of different user
interfaces or Ides for editing my Python
and it's just simply jupiter.org
j-u-p-y-t-e-r dot org and then I always
let mine sit on anaconda anaconda.com
and just real quick we'll open that up
for you oops offline mode don't show me
that again but you can see here that I
have different tools that I can actually
install in my anaconda including the
Jupiter notebook which comes by default
and then I have access to the
environments and again that's
anaconda.com named after the very large
one of the largest world's largest
snakes and then Jupiter notebook in this
case jupiter.org and when we're in our
I'm going to go in here to our jupyter
notebook and we're going to go ahead
just do new and a Python 3 and this will
open up a Python 3 Untitled folder so
diving right in let's go ahead and give
this a title pandas tutorial and we'll
go up to cell and we'll change the cell
type to mark down so it doesn't execute
it as actual code one of those wonderful
tools when you have jupyter notebooks
you can do demos with this and let's go
ahead and import pandas and usually
people just call it PD that's become
such a standard in the industry so we'll
go ahead and run that now we have our
pandas has been imported into our
jupyter notebook
and then oh we can go ahead and let me
do the control plus it's Internet
Explorer I can enlarge it very easily so
you have a nice pretty view oops too big
there we go and whenever you're working
with the new modules good to check your
version of the module in pandas you just
use the in this case PD dot underscore
underscore version underscore underscore
that's actually pretty common in most of
our python modules there's different
ways to look up the version but that's
one of the more common ones and we'll go
ahead and run that we get
0.23.4 and if we go to the pandas site
we see
0.23.4 is the latest release and of
course a reminder that if you're going
to environment you need to install it so
you'll need to do pip install pandas if
you're using the PIP installer we'll go
and close out of that
and the first thing we want to do is
we're going to work with series a lot of
the stuff you do in series you can then
do on the whole data set we need to do
what create one we need to manipulate it
take pieces of it so query query it
delete so you can delete different parts
of it so we want to do all those things
with the series and we'll start with the
series and then almost all the code in
fact all the code does transfer right
into
the actual data table so we go from a
series of a single list of one column
and then we'll take that and we'll
transfer that over to the whole table
and we'll start by creating let's put up
there we go creating a series from
list
and let's just call this ARR equals and
we'll do 0 1 2 3 4. if you remember from
our last one we could usually do R
equals range of five which would be zero
to four what we'll do R equals zero to
four and we'll call this S1 and we'll go
PD and series is capitalized this one
always throws me is which letters do you
capitalize on these modules they're
getting more and more uniform but you
got to watch that with python and we're
just going to go ahead and do ARR so
we're just going to take this python
list and we're going to turn it into a
series and then because we're in Jupiter
we don't have to put the print statement
we can just put S1 and it'll print out
this series for us now let's go ahead
and run that and take a look
and you'll see we have two rows of
numbers so the first one is the index
now it automatically creates the index
starting with zero unless you tell it to
do differently so we get 0 index row 0
is 0 1 1 2 2 3 3 4 4. and because it's a
series it doesn't need a title for the
column there's only one column so why
title it
and this also lets you know that it's a
data type of integer 64. so we print
this out this is our series our basic
series we've just created
now let's do a second series
PD and we'll use the same
data list and let's go ahead and do
order we'll give it an order equals oh
let's do it this way let's go index
equals order
and it helps if we actually give it an
order so we'll do order equals and let's
do one two three four five so instead of
starting with zero we're going to give
it an order starting with one we're
going to run that and we'll go ahead and
print it out down here S2
and we'll see that we now have an index
of one two three four five and that
represents 0 1 2 3 4 in the series and
we're still data type integer 64. and
very common as you're missing with numpy
arrays is we can import our numpy as NP
remember that from our numpy tutorials
we can go ahead and create a numpy at a
random with the random numbers of five
and let's just see what that end looks
like so we can see what our numpy looks
like so we have some nice random float
values here 2.33 so on and that's from
our last tutorial the numpy tutorial one
and two and instead of calling it order
let's call it index and we're going to
set our index equal to a b c d and e I
want to show you that the index doesn't
have to be an integer so it can be
something very different here and then
let's go ahead and create our we'll just
use S2 again and here's our NP for numpy
series capital s
and N is our NP for numpy
PD for pandas there we go switching my
anachronisms so we have pd.series of N
and we go and do our index equals our
index we just created
and then let's go ahead and see what
that looks like S2 is a printed and
let's run that and we can see here we
have a nice Series going on a b c d and
e for our indexes so instead of being 0
1 2 3 or 4 we can make this index
whatever we want and you can see the
numbers here going down that we randomly
generated from the number array so we
use numpy to create our Panda Series
right here and so continuing on with
creating our Series this one I use so
often we create a series from a
dictionary so we have our dictionary in
this case we went ahead and did a of one
B is 2 C of three D4 e of five so each
one of those is a key and then a value
and then we're going to use oh let's use
S3 equals PD for pandas
series and then we want to go ahead and
just do D in here
out S3 here and let's go ahead and run
this and you can see we got a is one B
is 2 C is three D is four e is 5. and
it's still of integer 64 Because the
actual data is one two three four five
and it's all integers 64 type 64. and
the last thing we want to do in the
creating section of our series is to go
ahead and modify the index because we're
going to start modifying all this data
so let's start with modifying the index
of the series and if you remember let's
do a print this time S1
I'll go ahead and run this and the
reason I did print is because it only
prints out the last variable so if I put
S1 up here and we're going to do another
variable back down lower it won't print
the first one just the last one and
we're going to go ahead and take S1
the index and we're just going to set it
equal to a new index and obviously the
number of objects in our index has to
equal the number of objects in our data
and then because it's a last variable we
can go ahead and just do an S1 and let's
run that and you can see how we went
from 0 to 0 0 1 2 3 4 as our index we've
now altered it to a b c d and e so this
would be much more readable or might be
representational of a larger database
you're working with
So Cool Tools we've covered creating
database based on my basic array python
array we've showed you how to do reset
the index
that we showed you how to use a numpy
array so you can put a numpy array in
there it's all the same you know
pd.series numpy array and then we can
set the index on there and the same
thing with the dictionary so it's very
versatile how it pulls in data and you
can pull in data from different sources
and different setups and create a new
series very easily in the pandas and
then we looked on changing your index so
now we have a new index on here
and then we want to go ahead and do some
selection let's do some basic
slicing most common thing you'll
probably do on here and we'll just do S1
this notation should start to look
really familiar again this is going to
put an output so I'd usually it doesn't
change S1 this just selects it so we
might do a equals S1 and then print a
and you'll see that it just looks at the
first three zero one two and we can do
the same thing by not having the a in
there I'll go ahead and take that out
but it's just a reminder that it's not
actually changing S1 it's just viewing
S1 so simple slicing on here and we can
likewise do an append oops before we do
a pin let's just do a quick kind of fun
one we'll do two minus one and you'll
see it covers everything but the E of
course you can do minus two on this side
so one another way to select it is to go
how far from the end and likewise we can
do a 2 here a c d e to the end so it
starts at the second one and another way
we can do this is we can do a minus 2
over here
and that looks at just the last two in
the slice so you can see how easy it is
to slice the data and of course there's
no reason to do this but you could
select all of them
if you want to view all of them on there
UPS 32 there's not 32 so it's just going
to show the first three there we go and
then we can also append so I can take
and oh let's create another series and
append one to it and if you remember we
had S3 there's our S3 and we have our S1
I'm going to do S1
and let's go ahead and do oh let's call
it S4
equals S1
a pin S3
so we're just going to combine those two
into S4
and if we go ahead and print S4 on here
you'll now see that we have a b c d e a
b c d e zero one two three four one two
three four five because we started the
data at one so very easy to compend one
series to the next and if we're going to
append one series to the next we need to
go ahead and drop or delete one and drop
is a keyword for that and let's just do
e or index e and so if I run this you'll
see that it'll print it out and a b c d
there's no e and remember all these
changes if I type in S4 again you'll see
that S4 still has e in it so this change
does not affect the series unless you
tell it to so I'd have to do like x S4
equals s4.drop e and there's another way
to do that which we'll show you later on
let me just cut this one out
there we go all right so we've covered
all kinds of Cool Tools here we have
appending we have slicing we did all the
creating stuff earlier as you can see
here on the setup how easy it is to
manipulate the series
so next what we want to get into is we
want to get into
operations that happen on the series so
let me go ahead and change this cell to
mark down there we go and run that so
series operations what can we do with
the series and let's start by creating a
couple arrays we'll call it array one
and we'll do 0 through 7 and array two
six through six seven eight nine five I
don't know let me threw the five on the
end but let's go ahead and run those so
those load up into Jupiter and we'll do
this a little backwards we're going to
do S5 equals a panda series of array two
so I'm doing this in reverse and then
when we do S5 you'll see that we have
zero to four it automatically assign the
index
67895 for our series and let's go ahead
and do the same and we'll call this S6
and we'll set this equal to PD series
for our first array and if we do an S6
down here to print it out
we'll see something similar I got zero
through six zero one two three four five
seven for the data so those are two
series we just created series six five
and six
and one of the first things we can do is
we can add one series to the next so I
can do S5 dot add S6 and let's see what
that generates and just a quick thing if
you've never used pandas what do you
think is going to happen with the fact
that this only has five different values
in it and this one has seven values
so let's see what that does and we end
up with 6 8 10 12 9 and it goes oh I
can't add this there's nothing there so
it gives us a null return very different
than the numpy that would have given you
an error this instead tells you there's
no value here because we couldn't
generate one so we can easily add S5 dot
add S6 and likewise we can do S5 Dot
sub for subtract S6 and we'll run that
and on the add the subtract and you
guessed it we're going to do multiply
and divide next again you can see
there's the null values where it can't
subtract the two because there's no
values there to subtract we can also do
S5 multiply mul they're all three
letters on these that's one of the ways
to remember how they figured out the
code for this so remember these are all
three letters mole we'll go ahead and
run this and you can again you can see
how they're multiplied together and then
we can also do the S5 div three letters
again S6 and run that
and you'll see here this goes to
Infinity because we have 0 in the wrong
position so it actually gives you a
whole different answer here that's
important to notice and then in the null
values because there's no data and it
can't actually produce an answer off of
null off of missing data and since we're
in data science let's do S6
median so let's look at the median data
which is simply median sorry for those
who are following the three letters
because median is not three letters and
you can see an S6 is 3.0 and let's do a
print here and we'll do median
or average S6
and let's print Max
my S6 and just like median there's max
value and if we're going to have a max
value we should also have a minimum
value so let's pop in minimum
we'll go ahead and run this and you're
starting to see something that would be
generated like say an R where you're
starting to get your different
statistics we have a medium value of
three max value of 7 and a minimum value
of zero and what it does when it hits
these null values if there is no values
in there because we could still do that
we could actually you know what let's go
up here and do
let's pick this one where we multiplied
let's go s seven equals
I'm going to print the S7 just so I keep
it nice and uniform so I still have my
S7 down there and run it and then I want
to take the S7
because S7 now has null values and an
Infinity value and let's see what
happens
this is going to be interesting because
I want to see what it does with infinity
and we end up with a median of six
maximum of 27 and minimum of zero which
is correct it drops those values so when
it gets to there and it doesn't know
what to do with them it just drops those
values and then it computes it on the
remaining data on there so that's
important to know when you're making
these computations you're looking at Min
and Max and median you're not going to
know that there's no values unless you
double check your data for the null
values it's a very important thing to
note on there so just a real quick
review on there we've done our created
our PD series and we've gone ahead and
done addition subtraction multiplication
division all those are three letters so
sub Min div add and then we looked at
median maximum and minimum so we're
going to go ahead and jump into the next
big topic which is to create a data
frame so now we're going to go from
series and we're going to create a
number of series and bundle them
together to make a data frame
there we go cell type markdown and run
that so we have a nice title on there
it's always good to have a good title
all right so our first data frame we'll
jump in with some stuff that looks a
little complicated we'll break it down
first I'm going to create some dates and
you know what let's just go ahead and do
this I want you to see what that looks
like what I'm creating here I've created
a series of dates PD date range and
we're going to use these for the index
okay so when you look at this you'll see
that it's just basically it comes out
kind of like a basic python list or
numpy array however you want to look at
it with our different dates going down
and we've generated six of them and it's
going to have whatever time it is right
now on your on the thing for the date
for the time that's that time stamp
right there and then you'll see we have
11 19 2008 11 20 11 19 and looking into
the future there so that's all this is
is generating a series of dates that
we're going to use as our index and this
is a pandas command so we have a date
range which is nice that's one of the
tools hidden in the the pandas that you
can use and next we're going to use
numpy to go ahead and generate some
random numbers in this case we'll do the
np.random.random and six comma four you
can look at this as rows and columns as
we move it into the pandas and of course
you could reshape this if you had those
backwards on your data but we want the
six to match the rows and we have six
periods so our indexes should match
along with the rows on there and then
you know before we do the next one let's
go ahead and just print out our numpy
arrays and see what that looks like here
we have it one two three four by one two
three four five six four by six so it's
a nice little setup on there and since
working with data frames can be very
visual let's give our columns we have
four columns and we're going to give
them names A B C and D so now we have
columns on there also and then let's put
this all together in a data frame and we
can actually you know what let's do this
since I did it with everything else
let's go ahead and do columns and you
can see there's our columns on there
and we'll go ahead and do df1 equals
pandas Dot data frame and note that the
D and the f are capitalized series it
was just the S and I always highlight
this because you don't know how many
times these things get retyped when you
forget what's capitalized on there it's
a minor thing you'll pick it up right
away if you do a lot of it and the first
thing we want to do is we want to go
ahead and take our numpy array because
we're going to create our data frame off
of is the numpy array and then we want
our index equal to our dates so there's
our index in there and then we also have
columns equals columns and then finally
let's see what that looks like now
remember we had all that different data
that just looked like a jumble of data
we have our column names and everything
else our numpy array kind of just a
jumble array over there four by six you
could sort of read it but look how nice
this looks I mean this is you come into
a board meeting you're working with your
shareholders this is pretty readable
this is you know this is our date this
is our a b c d whatever it is maybe it's
one of these dates has your leads
closures lost leads total dollar made
you know whatever it is if it's in a
business maybe it's measurements on some
scientific equipment whether searching
material you know where this is like a
high of the temperature low of the day
humidity of the day whatever it is so
you can see that we can really create a
nice clear chart and it looks just like
a spreadsheet you know we have our rows
and we have our columns and we have our
data in there now this one I use all the
time if we're going to create we can
create it like you saw here with our
numpy array very easy to do that and
reshape it you can also create it with a
dictionary array so here we have some
data and let me just go down a notch so
you can see all the data on there we
have an animal in this case cat cat
snake dog dog cat snake cat dog we have
the age so we have an array of Ages we
have the number of visits and the
priority was it a high priority yes no
and then we're going to take that we're
going to create some labels we have a b
c d e f g h i and what I want you to
notice on this is we have a title animal
and then we have basically a python list
and these lists they don't necessarily
have to be equal because we can have
non-data you know np.net numpy array
null value but we want to go ahead and
create labels that are equal to the
number in the list so a the first cat B
the second cat C the snake D the dog and
so on so we'll go ahead and create our
labels which we're going to use as an
index and we'll call this DF let's do it
this way we'll call this df2
equals PD for pandas data frame and then
we have our data just like we did before
and then we have our index equals
labels
and if we're going to go from there
let's go ahead and print it out so we
can see what that looks like df2 so
let's go ahead and run that and another
again you have a nice very clean chart
to look at we've gone from this mess of
data here to what looks like a very
organized spreadsheet very Visual and
easy to read animal age visits priority
and then a through J cats and all the
different animals so on and so on and
then when you do programming a lot of
times it's important to know what the
data types are so we can simply do df2
D types and if we run that we can see
that our animal
is an object because it's just a string
but it comes in as an object age is a
float 64 integer 64 and then priority
again is just an object
and exploring this this one's very
popular let's go df2 head and if we
print that out the df2 head Returns the
first five and we can change this you
don't have to do five you might want to
just look at the top two maybe you want
to look at let's see now let's do six so
maybe we'll look at just the top six in
the database in your data frame and you
can actually this creates another data
frame so I could have a DF 3 equal to
df2 and this now takes the df2 and just
the first six values let's if we do df3
run get the same answer
and if we do it the head of the data we
can also do the tell it's the same thing
DFT you can look at the last we'll just
do the tell which by default does five
the last five and of course you can just
look at the last three of those real
quick just to see what's at the end of
the data and this is the tell I love
doing the tell of one because I'll have
like the index or something like that
and it will just show me the last
whatever the last entry was looking at
stock values and I might want to look at
just the last five days of the stock
values I can do that with the data frame
tail
and some other key things to look up are
the index so we can do df2 dot index
and I want you to notice that this isn't
a call function so if I put the brackets
on the end it'll give me an error
because index is not callable it's just
an object in there so we do df2 dot
index there's also columns
so we can go ahead and let's do uh let's
print this remember the first one is not
going to show unless I print it and then
df2 columns so now we can see we have
our indexes and we have our columns
listed here df2. columns animal age
visits priority that tells you what kind
of object it is or what kind of data
type it is and they're both object
and then finally df2 dot values and
again there's no brackets on the end of
df2.values because this is an actual
object it's not a callable function so
we'll go ahead and run that and it
creates this displays a nice array a
very easy way to convert this back to a
numpy array basically so before I go
into the next section let's just take a
quick look at what we covered so far
with the data frame we came up here we
created our data frame we did it from a
numpy array first setting the columns
and the index the index is setting it up
as the same as when we set up the series
so that should look very familiar so is
the whole format the numpy array the
index dates and the columns columns and
remember in our numpy array we're
looking at row comma column so six rows
four columns is how that reads in the
data frame
and we went ahead and also did that from
a dictionary in this case animal was the
column name with all the date data
underneath that column and then age with
that data visits that data priority that
data and then of course we added our
labels in there for our index so there's
no difference in there but it
automatically pulled the column names
important to know when you're dealing
with the data frame and importing a data
frame this way
and then we did looking up D type we
looked at head and tail looking at your
data really quick we also did index and
columns and values and note these don't
have the brackets on the end
so the next thing we want to do is go
ahead since we're dealing with data
science is we want to go ahead and
describe the data so we have def2 dot
describe to do that and we're going to
manipulate it in just a minute but let's
just see what this generates
and you can see right here we have age
and visits so looking at our data from
up above let me just go all the way up
here animal age business priority
and it does a nice job generating your
age versus visits which has all the data
you have your account your means your
standard deviation your minimum value 25
or in this group 50 75 and your maximum
value so this will look familiar as a
data science setup with your describe
for a quick look at your data Frame data
so let's start manipulating this data
frame and moving stuff around and we'll
start with transposing and it is simply
capital T for transpose and when we run
that it flips The Columns and the
indexes so now the indexes are all
column names and the columns are all
indexes animal age visits priority so if
we had come in here with our data shaped
wrong up above where we had a four by
six we can quickly just swap it if we
had it backwards not a big deal and we
can also sort our data something that
you can't deal which is more difficult
to do with a lot of other packages and
the data frame is really easy to do take
our our data frame df2 and we're going
to sort underscore values y equals H and
so when we run this you'll see the
default is ascending so we have 0.5 to
2.53 and everything else is organized so
if you look at your indexes they've been
moved around because each index it moves
a whole row not just the one piece of
data is not being sorted so a very quick
way to sort by age are different data in
the data frame and in addition to
sorting it we can also slice the data
frame so I could do df2 and this should
look familiar from earlier we'll just do
one to three so we're going to pull out
oops it does help if I use the DF
instead of just D and we're going to
pull up just between one and three so we
have not zero which is a we have B which
is 2 or B which is one and C which is
two so one two and then it does not
include 3 which is the standard in
Python and we can even do something like
this we can combine them which is always
fun because remember this returns a data
frame so if I take df2 dot sort
values and we'll do by equals age this
is just kind of fun and then I'm going
to slice it there we go double check my
typing and run it and now you can see fa
because F A are now 1 and 2 on there
I'm saying very quickly create a whole
string on here which narrows it you know
that you can sort it then slice it and
do all kinds of fun things with your
data frame let's go back to the original
one run there we go and if we can slice
it by row we can also query the data
frame so we can do df2 and this is a
little different because I'm going to
create an array within an array and in
this case we're going to look at all
let's do H comma
visits so look at the different format
in here we have one to three so we've
done this by slicing by an integer value
and then on here I've done df2 H comma
visits in an array and when I run this
you can see that we get just these two
columns on here we get age and visits so
it's a quick way to select just two
columns or select number of columns
you're working with
and if you stop there we did the slicing
almost identical to slice is I location
which uses the integer location one
comma three there's a push in pandas to
move to this particular setup instead of
doing just a regular slice and that's
because this can be confusing when we
slice one to three and then we select
age and visits so there is a push to go
ahead and move to an eye location which
does the same thing you can see here BC
it's the same as up above there's also a
copy command so we can do df3 equals df2
copy we're just going to create a
straight copy of it and of course if we
do df3
it'll be the same as a df2 on there so
df3 equals df2. copy and then let's do
df3 dot is null so we're looking for
null values
and this will return a nice map and
you'll see that everything is false
except when you go up here under the cat
or H they had a null there and so if we
go to have a couple up here also
underneath of let's see the dog okay
there's a bunch of nulls in here there's
D up here so let's look at D down here
and you'll see false true there it is
there's our null value so we can create
a quick chart of null values you can use
this to do other things we can leverage
that null value to maybe take an average
or something and fill those null spaces
with data and we can also modify the
location so here's our df3 location
and notice this is location not I
location I location has I for integer
location uses the in this case the
variables on the left and what we can do
on here and we're going to set this
equal to one five and
some I'll pick a spot let's go back up
here where we had let's do F A just
let's see where we're looking at oh here
we go let's do F and age and up here f
is set to age of 2.0 and we find out
that that's incorrect data so we go
ahead and switch to df3 equal and then
we go and print out our df3
and if we go to F and H it is now 1.5 so
we're just changing the value in the df3
and this is changing the actual data
frame remember a lot of our stuff we do
a slice and like it returns another data
frame this changes the actual data frame
and that value in the data frame
so we've covered uh location and I
location is null making a copy here's
our I location which is equivalent of a
slice and also selecting columns so now
we want to dive just to take a little
detour here and let's look at df3 means
and this is kind of nice because you can
do this you can either do this by as you
can select a single column here by the
way you can just add the column
selection right here like we did before
so we could have age
look up the mean that just creates a
series if I run that there's our age
but if I take that out instead of
selecting it we can do the whole setup
and it has age and visits so why doesn't
it have priority or animal well those
are not integers so it's really hard
they're non-numerical values so what is
the average I guess you could do a
histogram which probably will look at
that later on but the only two things we
can really look at is age and visits and
we have the average or the mean on the
age is 3.375 and the mean on visits is
1.9 and let's do df3
visits we'll go ahead and steal the
visits again
and remember all those different
functions we looked at for a series well
we can do those here we can do the sum
so if we run that we'll see that these
sum up to 19. you can also look up
minimum if you remember that from before
the minimum is 1 Max so all that
functionality is here I'll just go back
to summing it up and adding it all
together so real quick we've shown you
how to take the series operations and
put them into the data frame and then we
can actually this is interesting one we
can just do df3 sum run and you'll see
the different summations on there it
just combines them I like the way it
just combines the strings on there for
priority and animal we've looked at is
null we've also looked at copying along
with the different slices which we
talked about earlier so let's talk about
strings let's dive into the string setup
on there and let's go ahead and create a
string series string equals PD series
and we just put it right in there we
have a c d a a b a c a popped in a null
value cow and Al I don't know why they
picked Cal and Al in the background
someone must like those animals and of
course we can just do string if we run
that you'll see leave the r out we'll
get an error but if we put it in there
you'll see that we have a simple series
0 a 1C 2D and it automatically indexes
it zero to eight and then we can go
string dot lower so when we're talking
about our data frame in this case or our
data series string in this case we use
the string function Str and we're going
to make it lower and we go ahead and put
the brackets on there and you'll see
that we've gone from capital a Capital C
so on to ABC and Baka CBA cow Al they
were all lowercase already and of course
if you want to go lower you can also do
upper and we'll go ahead and run that
and you can see we now have ACD AAA Baca
everything's capitalized except for the
null value which is still null all right
so we looked at a few basic string you
can see that string functions upper and
lower we're going to jump into a very
important topic I'm even going to give
it its own header on here because it's
such an important topic what do you do
with missing values Panda has some great
tools for that so we'll dive into those
we'll call we'll work with df4 and if
you remember the DF copy from above
we're just going to make a copy of df3
and let's just take a quick look at the
data we're working with oops df3 forgot
the three on there there we go so here
we have our cats snakes and dogs
hopefully not all in the same container
because that would be just probably mean
to all of them so we made a copy we're
going to be working with df4 and the
reason we made a copy is we want to go
ahead and fill the data and we just
simply do fill in a and then we're going
to give it the value we want to put in
there we'll give it the value 4. so I
can run in here and you'll see now that
df4 now has where the N A was is filled
with the value of four same thing down
here
a lot of times we'll compute the mean
first so I might do a mean page equals
df4 and then we want to go ahead and do
age
and dot mean
and then I'll do something like this df4
I only want to select the age and I want
to fill that
with the mean h and I run in there and
you'll see that our df4h now has the
means in there just a quick way of
showing you how you can combine these
let me go back to our original one there
we go and run that and keeping with good
practices df5 equals df3 dot copy
a little printer df5 which should be the
original one
and then on the df5 we can now drop our
missing data
I'm going to Simply drop in a and we're
going to use how equals any so I'm going
to drop any row that has missing data in
it and you'll see we had D here with
missing data and H and then let's go
ahead and see what df5 looks like when
we do that
there we go and there it is D is gone
and so is H so we create a new data
frame off of this missing those values
now if you have a lot of data dropping
values is a good way to take care of it
because you don't miss some data if you
have not a whole lot of data you're
working with like the iris data set or
something like that or something small
you want to start trying to find a way
to fill that data in so you don't lose
your computational power of the data you
got so just a quick look at processing
null values or missing values you can
fill them usually with the means some
people use medium or the mode there's
different ways you can fill it one way
is means and we can also just drop those
rows those are the two main things we do
with missing data
here we go uh we're going to cover next
this is I so love data frames for this
file operations it saved me so much time
because they have so many different
tools for bringing data in and saving
data so we're looking at the data frame
file operations it's really streamlined
I don't know how many times I'll go on
to different data downloads and they'll
have Panda download standard on there
just because it's so widely used so
let's start with the most common file is
a CSV so we have df3 to CSV or animal
and let me just show you the folders
going into right now I have some
Untitled a few things in here but
nothing labeled animal so we go ahead
and run this and this is now save the
animal to my hard drive and you can now
see the animal folder up here and if I
let's do edit with a notepad oh let's
open up with just a regular notepad
there we go or wordpad if I open that up
you can see it's comma separate did our
titles they don't have an index on the
categories on the top in the index comma
then all the different data is separated
by commas
standard CSP file on there and if we're
going to send it to CSV and notice the
format is dot 2 underscore CSV and it's
just the name of the file we're sending
it to you can also put the complete path
by default it's going to go whatever the
active directory this program is running
on that's why those other folders are in
there so we have our df3 to CSV and then
if we're going to put it in there we
want to also get it back out and we'll
call this one DF underscore animal
equals PD read underscore CSV I always
have to remember is two underscore CSV
and read underscore CSV I always want to
do like a capital in there and not the
underscore so we're going in here again
it's the active directory so if I now do
print out my DF animal
let's just do the head we only want to
look at the first three lines so if I go
ahead and run this we'll see the first
three lines and they should match up
here what we saved to our CSV so very
easy to save and import from our CSV
files on here
and it turns out DF 3 also has a 2 XL
they actually have a lot of different
formats but you know old school Excel
was real popular for so long still is we
can go ahead and save it as animal dot
xlsx we're going to call the sheet named
sheet1 and then I can also do DF we'll
call it animal 2.
two and this one's going to come from
and the same format on here there we go
so we still have our animal xlsx
the sheet 1 that's where it's coming
from index columns equals none so we're
not going to we're going to suppress the
indexing on the columns n a values and
it'll just assign that zero one up on
your indexes so if it says index columns
equals none that's what it does and then
we've added null values because there's
no values in here and we want to just
make sure that they're marked as n a and
we'll go ahead and just print out the
animal animal to there we go and let's
run that let's make this let's just do
the whole thing so we'll go ahead and
run that and it probably doesn't help
that I completely forgot the read so
animal 2 equals PD dot read
Excel there we go Excel so now we go
ahead and run it
and what we expect is happening here we
have the same data frame on here and if
I flick back to my folder you can now
see that we have the animal one of these
is in Excel and one of these is a CSV on
here and so there's our two file Types
on there and they have other formats
these are just the two most common ones
used and I don't know how many times
I've had stuff from Excel I need to pull
out if you've ever played with Excel
it's a nightmare on the back end because
of the way they do the indexing
so this just makes it quick and easy to
pull in an Excel spreadsheet so we
looked at two different ways to bring
data in and save it to files we've
looked at all kinds of different ways of
manipulating our data set and slicing it
and creating it for our data frame let's
get in there put your visualization
always a big thing at the end because
one it lets you check to see what you
did make sure it looks right and then
also if you're going to show somebody
else it makes it very clear what's going
on if they see something visual so this
is where a really important part of data
science is so let's go ahead and bring
in our tools we're going to do import
numpy as NP we want to make sure we have
our Amber sign matplot library in line
this just lets Jupiter know that we're
going to print it on this page if you're
using a different IDE you don't really
necessarily need that but this does help
it displays correctly in Jupiter
notebook and if you remember for earlier
we could create a we're going to call it
TS we're going to create a pandas which
are cute cuddly creatures versus a
pandem short for pandemonium no so we
have TS equals PD series and we're just
going to create a random setup of 50.
we'll do an index we'll set it equal to
the pandas date range today periods
equals 50 so the 50s should match and I
want you to notice something here I did
not import the matplot library why
because it's already in there pandas
already has its built-in connection and
interface with matplot Library so you
don't have to import it and we'll go
ahead and do TS equals TS dot cumulative
sum we're going to do the cumulative sum
it's a little reformatting there and
we'll go ahead and plot it and let's
take a look at what that looks like so
we have a nice graph here we have the
dates on the bottom we set this up so we
have a nice range between in this case
minus four to looks like about two maybe
or one minus four and one so what we've
done here we've plotted a basic series
just a single row of data and we've set
indexes on there but we can also do the
whole data frame on there and let's see
what that looks like so first let's go
ahead and create the data frame we have
here random number so we're going to do
50 by 4 and then we'll go ahead and
create columns a b X and Y just because
we can index is a ts.index on there so
we're going to use the same index as
before just to keep it nice and uniform
we've already generated the dates to go
with it and then we can do just like we
did with the series we can also do with
the data frame
DF equals DF cumulative sum so we're
going to sum the whole data frame and
then we'll do simply DF plot and let's
put that in and let's go ahead and run
this and look how easy and quick that
was to generate a nice graph with all
the different data on there so we have
our shared index we have the shared
columns and then we have the different
data from each one that we can easily
look at and compare so very quick way of
displaying data you can imagine if you
were working in oh I think I mentioned
stock earlier because I've been doing
some analysis of stock lately so you'd
have your date down here and then you
would have stock a Stock B stock X Y
whatever it is and you can put them all
in one chart and see how they what they
look like next to each other and this
isn't too far off from what some of
those graphs looks like and this is just
randomly generated so stock has a lot of
Randomness in it which is one of the
reasons I actually play with it for
doing some of my models on for testing
them out now there are a lot of features
in pandas so we're going to show you one
more thing on here there's some of the
things like I didn't go too deep we
looked at the top two for importing data
from a CSV and from an Excel spreadsheet
showed you how to quickly plot the data
there's more settings in there you can
do we're going to do one more thing down
here and this is kind of a fun one
changes to a mark down and run that so
how would you remove repeated data using
pandas
and this is where you have a data set
that comes in and maybe it's a feeding
from one location and in instead of
noting that it's repeated the date like
oh let's go back to stocks that's a good
visual we have the stocks from the 23rd
and it adds another row and it's the
same row it's importing the 23rd again
and again so now you have that data
repeated three times and you need to go
back and figure out how to get rid of it
how do you track that down so let's
start by creating a quick database our
data frame not a database I keep saying
databases the data frame and we'll just
make this data frame has using our
dictionary going in this data frame only
has one data Series in it which is fine
so if we do DF to print it out you'll
see a one two two two two two four five
six seven and so on and so how would you
remove that well there is a neat feature
in data frames called shift
along with another feature that lets us
select just certain information and
we'll go with the location function put
that in Brackets remember that from
above location and then in the location
let me just spread this out a little bit
so it's really easy to read in fact I'm
going to go upscale on that since we're
doing some a little bit more complicated
here
what you can see on this on the location
is I have DFA dot shift so this is going
to shift up one by default you can
actually change this to two or three you
can even do a minus one and it shifts
the other way but it's going to shift up
by one by default that's going to say if
that does not equal DF of a then we want
that and if you look down here we had
one two two two two when we run this
Logic on here and we do the shift it now
gets rid of all the duplicates so we
went from one two two two two four four
five whatever it was here it is one two
two two four four four five five five
six six six two one two four five six
seven eight and you'll see on the index
it just deletes them out of there so the
index stays the same obviously you don't
want the dates to change if you're
working with an index dated setup so it
just deletes those duplicates out of
there this is just a quick way to
introduce you to one the fact that you
can add logic gates into here and two
the I location allows you to use shift
so there's the shift function and then
the I allocation selects that based on
true or false
wow so we've actually covered a lot
today in pandas we've really covered
into the basics of selecting your
different series out of your column out
of your data frame how to index rows how
to slice how to plot hopefully you'll
take this beyond that and start
combining these different things and you
can create long strings and really
explore your data generate some nice
graphs if you're in Jupiter notebook
it's a great demo to show others and I
didn't notice about Jupiter notebook you
can do this in Jupiter notebook and then
you can download and I always I never
really look too closely at all the
downloads but you can download as an
HTML and post it to your blog so it's
got a neat feature in there but any of
this is really powerful tool all of this
is really powerful tools for doing your
data science if getting your learning
started it's half the battle what if you
could do that for free visit skillup by
simply learn click on the link in the
description to know more
at the Caltech data analytics bootcamp
you will learn everything you need to
know to succeed as a data analyst from
mastering Excel to creating data driven
presentations manipulating data with SQL
analyzing data with python and
visualizing data with W today we're
going to study the plot library and the
python code so what's in it for you what
is matplot Library types of plots
plotting graphs and sub graphs adding a
graph inside a graph graph parameters
title label Legend line graphs line
types color and transparency canvas grid
and axes range 2D plots scatter step bar
fill between radar chart histogram
Contour image 3D surface image and then
we'll hit a practice example pie chart
so let's start with what is matplot
library matplot library is an open
source drawing Library which supports
Rich drawing types it is used to draw 2D
and 3D graphics and there are so many
packages in the matplot library we're
going to cover the basics and there are
so many packages that sit on top of the
matplight library that we can't even
cover them all today but we'll hit the
main one so you have a good
understanding of what the matplot
library is and what the basics can do
you can understand your data easily by
visualizing it with the help of matplot
Library you can generate plots
histograms bar charts and many other
charts with just a few lines of code and
here we have some basic types of plots
you can see here that we'll go into we
have the bar chart the histogram boy I
use a lot of histograms in my stuff a
scatter plot line chart pie chart and
area graph let's start plotting them and
to do this I'm going to be using Jupiter
notebook you can use any of your python
interfaces for programming or scripting
and running it of course we here really
like the Jupiter notebook for doing
basic a lot of basic stuff because it's
so visual and in our Jupiter notebook
which opens up in this case I'm using
Google Chrome you can go up here to New
and we'll create a new Python 3 and set
that up
if you're not familiar with Jupiter
notebook we do have a tutorial that
covers some of the basics of that you'll
look at any of our tutorials they
usually cover a number of them showing
how to set up Jupiter and Anaconda I
myself use Jupiter through anaconda in
fact let's go ahead and open that up and
just take a look at what they see what
that looks like you can see your
anaconda Navigator if you install it it
will automatically install the Jupiter
notebook but it also installs a lot of
other things I know some people like the
QT console for doing python or spider
I've never used them I actually use
notepad plus plus as one of my editors
and then I use the jupyter notebook a
lot because it's so easy to have a
visual while I'm programming and even
simple script in Python I'll take it
from The jupyter Notebook and then do a
save as you always go under file and you
can download as a Python program so
that'll download it as an actual python
versus the IPython that this saves it as
so let's go ahead and dive in and see we
got going here and
put matplot Library tutorial and I'm
going to turn this cell into a markdown
so it doesn't actually run it you can
see it has a nice little title there
that's all Jupiter notebook and then
from Matt plot Library let's import
Pi lab
back one and then let's go ahead and
just print
we'll go pylab and the version let's go
ahead and run this so we're going to
import our pylab module from the matplot
library and we find out that we're in
version
1.15.1 always important to note the
version you're in probably I was reading
an article that said the number one
thing that python programmers struggle
with is remembering what version they're
working in and making sure that they're
going from one platform to the other
with the same version and if we're going
to graph things I think we need some
data to graph so we're going to import
numpy as NP now if you're not familiar
with numpy definitely go back and check
out our numpy tutorial there's so many
different things you can do with it
dealing with reshaping the data and
creating the data we're just going to
use it to create some data for us and
there is a lot of ways to create data
but we're going to use the np.line space
so we're going to create a numpy array
and the way you read this is we're going
to create numbers between 0 and 10 and
we're going to create 25 of these
numbers so we're just going to divide
that equally up between 0 and 10. and if
we have x coordinates we should probably
have some Y coordinates and we'll do
something simple like x times X plus two
and let's just take a look we're going
to print X and print
why let me go ahead and run this and
let's see we got going on here so we
have our x coordinates which is 0 0.4.83
Etc and you can look at this as an X Y
plot so we have 0 we have two and we
have 0.416 we have 2.17 and just as a
quick reminder we're going to do print
in pra X comma y dot reshape 25 comma
two and the reason I want to do this is
I want to show you something here a lot
of times a program returns X comma Y and
it's an array of X comma y x comma y x
comma Y and so when you're working with
the pi plot
you have to separate it out and reshape
it so if I start off with pairs like
this I can reshape them if I know
there's 25 pairs in there I can switch
the 2 and the 25 and this is kind of
goofy but we'll do it anyways reshape so
I'm going to reshape my 25 by 2 back to
2 by 25.
and if I run that you'll see I end up
with the same output as the X Y the two
different arrays in here
and this is important that we want X and
Y separate again that's all numpy stuff
but it's important to understand that
this is a format that matplot Library
works with it works with an array of x's
and they should match your array of y's
so each one has 25 different entities in
it and then for our basic plotting of
this data it only takes one command to
draw a graph of this data and so we use
our from up here we imported pylab we
take our PI lab and the keynet lender
there is plot for plotting a line and
then we want our x coordinates and our Y
coordinates and we'll throw in R and the
r simply means red so we're going to
draw the line in red let me go and run
that
you can actually switch this around if
you wanted to do different there's B for
blue we have a lot of fun yellow hard to
see yellow there we go but we'll go
ahead and stick with red run and when
you're doing presentations with these
try to be consistent you know if the
business and the shareholders send you a
spreadsheet and they have losses in red
you use red for losses in your graph try
to be consistent use green for profit
for money you don't have to necessarily
use green but it's whatever they're
using whatever the company is using try
to mirror that that way people aren't
going to be confused if you switch your
data around and every time one graph has
red for loss and one graph has blue for
loss it gets really confusing so make
sure you're consistent in your graphs
and your coloring and something to know
because we're going to cover this in a
minute this is your canvas size so we
have a canvas here and what we're going
to do next is we're going to look at sub
graphs okay so let's take our PI lab and
create a sub plot
okay one of the things also don't know
when we're working with the matplot
library I'm not setting when I do this
this is my drawing canvas the pi lab so
once I've imported the pylab I'm drawing
my images on there very important to
know and with the subplot we're going to
give it some different values and we're
going to represent by rows columns and
indexes and let's do one two one so it's
going to be the first row second column
and the index is I think you can stack
your graphs and things like that we
don't worry too much about indexes but
rows and columns we want to go ahead and
use Row one and column two and if we're
going to have one object we should
probably have two but before we do that
we have to plot data onto the subplot so
the order is very important and we're
going to stick with our X comma Y and
let's do this we're going to add in a
third parameter here remember we did Red
we're going to add shorthand dash dash
for dashed lines so the this plots the
data into Row one column two and if
we're going to do that let's do another
one pilab dot subplot and if we're going
to do Row one let's do column two and
index two
and this time we're going to add G for
green and this denotes a style and if
we're going to set up our pylab subplot
there we go my lab we've got to go ahead
and plot that pi
lab plot
and instead of X Y we want y comma X
Oops I messed up this is in the wrong
spot there we go we'll move that down
here real quick because that goes in the
plot part so the subplot tells that the
row column and index and the pi plot
tells it what data in this case we
switch them and the color and then the
style shorthand and let's go ahead and
run that
and you'll see it takes this canvas
splits it in two and now we have two
different graphs and we have the red one
with dashed lines and we have the green
one which is has little stars going up
and if we take this and let's just um
just for fun let's change this and run
that with an index of one it puts them
both on the same index it also gives me
a warning because it's a strange way of
doing two subplots there's this
depreciated there's another way to do it
but most people just ignore that warning
because it's not going to go away
anytime soon now that's using the same
setup what happens if we do instead of
this let's change the column on here and
find out what happens
and if we do the column
I didn't really like that on the setup
it just disappears so let's keep our
column as 2 and let's change the row on
the second one to two and run that
and you'll see again it kind of squishes
everything together it can cause us some
issues so let's take the index so these
need a unique index and you can see here
where I made some changes I said row two
and look what happens when I change to
column two so I now have Row 2 column
two index two I squished it up here so
you could put another graph underneath
is what that does and there's all kinds
of different things you really have to
just play with these numbers so you get
a handle on them because
you know we get repeated 164 times
according to Cambridge University if
it's completely new to you and you can
see right here where we go three run
there we go but you can see it takes a
little bit sometimes to play with these
and get the numbers right oops I hit the
wrong one that's why let's go three
there three there run
there we go now it's overlapping so I
have this doubled over here on the right
for now we'll just go ahead and leave
this with the where we have column in
row two and the two different indexes so
they appear nice and neatly side by side
and then as we just saw as we were
flashing through them we can put them on
top of each other
and let me just highlight that and copy
it down here
paste it down there and here we have one
two one and then we'll do one two one
also for this one and that puts the two
subplots directly on top of each other
it gives us that warning and you can see
we now have two different sets of data
graphed on top of each other and you can
also see how it did the indexes since
one of them is from 0 to 10 that's the
green one on the x-axis and the other
one is from 0 to 10 on the y axis so it
took the greatest value of either one
and then used those as a shared value
so let's next look at operator
description and we'll go ahead and turn
this cell into a markdown and run that
so it looks nice so fig and remember I
talked about the canvas earlier I
briefly mentioned it we're going to look
a little bit more at the canvas later on
but that's what the figure is a fit
we're going to add axes so we're going
to initialize a subplot add the subplot
in rows and columns and all kinds of
different things with this that you can
do let's look at that code and see
exactly what's going on and I want you
to notice that there's fig which is the
actual canvas in the matplot library and
ax is commonly used to refer to the
subplots so we're creating subplots
you'll see ax equals PLT subplot
earlier we did the pi lab so let's go
ahead and import Pi plot from matplot
library and we're going to do it as PLT
you'll see that a lot that's really the
standard in the industry is to call it
PLT just like pandas's PD and numpy
array is NP certainly you can import it
as whatever you want but I would stick
to the standards and we're going to do
the same graph as we did above with the
pi lab but with the PLT so if it looks
familiar there's reason we're doing this
because we want to show you how the
figure part works and working with the
canvas goes but we're going to do the
same plot as we did before and we'll
call it fig and we're going to set that
equal to plot figure so there's our
figure or our canvas on there and let's
create a variable called axes and we're
going to set that equal to Fig dot add
axes
and in this we're going to control the
Left Right the width the height of the
canvas from zero to one and so we can go
ahead and I'm just going to put some
stuff in there I got 0.5
0.1.8.8 so when you're looking at this
this is a zero to one or you could say
50 10 80 80 percent but it's a control
it's going to control your left and your
right along with the width and the
height so the width and the height we're
going to use 80 percent and we're going
to have like a little indent on the left
and the right and this would look
familiar from above xcs.plot X comma Y
and then let's give it a color how about
red since we're recreating the same
graph let's keep it uniform oops it
helps if I use a ax i s instead of axes
I don't know where that came from but
this looks identical to the one we had
up above so here's our axis plot X comma
y of red same graph same setup but this
time we've added a variable equal to the
figure dot add axes so our plot figures
our canvas our axes is what we're
working in and then our axis.plot X
comma Y and again we can draw Sub graphs
we put that down here
[Music]
just like we did before and a little
different totation here we have fig
comma axes equal
plt.subplots
and in here it's going to be the number
of rows we're going to do one row
in columns equals two so if you remember
before that's what we did we had one row
with two different graphs on it we're
going to do the same thing but know how
we did this here's our figure our canvas
and our axes we're going to create
actually two different axes we're going
to create a row one column two and so
axes is an array of information so we
can simply do
four well let's do X in axes
this will now look familiar X Dot Plot
we're going to do X comma y we'll go
ahead and make it red keep everything
looking the same remember nice uniform
graphs everything looks the same and if
we go ahead and run this
you'll see we get two nice side by side
graphs so just as we had before the same
look the same setup
and just for fun let's change in columns
to three we'll run that and now you'll
see we'll have three on there and let's
see if we make it a little bit more
interesting we'll do in rows equals to
two and you can see down here we're
going to get an attribute there because
it's trying to scrunch everything
together so it does have a limit how
much stuff you can put in one small
space that's important to know you can
fix that by changing the canvas size
which we'll look at in just a minute and
there's other ways to change it on here
but here we go we can do in rows two and
columns equals one you can see two nice
images right above each other we'll go
back to the original one row two columns
side by side left to right and we can
also draw a picture or graph inside
another graph
and that's kind of a fun thing to do
it's important to note that we can layer
our stuff on top of each other which
makes for a really nice presentation so
let's start by uh fig create another
figure so we're going to start over
again with our canvas we set that equal
to plt.figure
so there's our new canvas and let's do
axes we'll call it axis one and two axis
one equals fig dot add axes remember
this from earlier
and this here similar numbers we used
before saying how big this axis is this
figure in the axis is so this is going
to be the big
axes and let's do axes two equals
another figure at axes and then
0.2.5.4.3 and if we're going to do this
they need data on them so let's go ahead
and plot some data on our axis so axis1
Dot Plot
and we'll make this simply X comma y
comma make it red and then let's go axis
true Dot Plot and let's reverse them y
comma X comma green there we go doing
what I told you not to do you shouldn't
be swapping axes around and plotting
your data in five different directions
because it's confusing let's go ahead
and run this and see what this looks
like and then let's talk a little bit
about this we talked about the
0.2.5.4.3 let me just grab The
annotation for that that's left right
width and height so we have in here that
this is going to be left right so here's
our left is point one n 0.5 and we look
you know what let's just play with this
a little bit what happens when I change
this to 0.1 it moves it way over to the
left so there's our point one so we can
make this point four run that there we
go so you can see how you can move it
around the branches on here 0.2 0.5 is
the left so that's the right so see what
happens when we do point oh let's make
this point one that actually is they had
it down a left right I thought this was
wrong it's actually how far from the
bottom I switched that on here bottom
there we go so we had here on this we
can go ahead and put that back to 0.5
and run that and this is and this is 0.3
let's make this point three also and
that is the width and then
a height we can make that really tiny
actually let's do 0.2 and let's run that
and you can see it changes the height on
there let me make it even smaller 0.2 by
0.2 and as you can see you can get stuck
playing with this to make it look just
right it could sometimes take a little
bit certainly once you have the settings
if you're doing a presentation you try
to keep it uniform unless it doesn't
make sense for the graph you're working
on try to keep the same color as the
same position and the same look and feel
and I mentioned earlier we can adjust
the canvas sides so this is from earlier
I just copied it down below we're going
to re-plot the same data we've been
looking at and what we can do is we can
change the figure size to 16 by nine and
let me run that and show you what that
looks like so it fills the whole screen
and then if you are normally when you're
working on the screen you don't worry
too much about this but we can set the
DPI to 300 run that
there it goes this is your dots per inch
and if you are doing an output of this
and you're printing a hard copy you want
the higher quality I would suggest
nothing under 300 if it's a professional
print you might get a little less than
that but whenever I'm doing professional
graphics and printing them out on
something 300 dots per inch is kind of
the minimal on there
you can go a lot higher too but keep in
mind the higher you get the more memory
it takes the more lag time and the more
resources you use so usually 300 is a
good solid number to use your dots per
inch and you can see a drill These are
nice it draws a nice large canvas here
which is 16 by nine and then the DPI is
300 on here so it's a little higher
quality and just out of curiosity I
wonder how long it takes to draw
something double that size 600 and you
can see here we're at 600 DPI it's going
to take a while there it goes just
because it's utilizing a lot more
graphics on there and let me just go
back to the 300 now we'll actually do
let's do a 100 you're not going to see a
difference on this because it is web
based Graphics are pretty low
and up here you saw I did this with the
plot figure this works the same if I do
figure axis subplot figure size and then
we'll go ahead and do actually
plot
X comma y comma we'll stick to red
let's go ahead and run this and you
should get almost the same thing here
here's our axes on the subplot on here
with the fixed size and the DPI let me
take this all out let me just read
remove all that real quick run it again
there we go now we're back to our
original figure and let's look at other
some of the other things you can do with
this
one of the things we do is we can set a
title for the axis so axis set title
you'll see right here since I put this
on the axis it's the main title for the
whole graph
and if you're going to have a title you
should also label so we can label our X
label and we can set our y label in this
case we're just going to call it X and Y
keep it nice and uniform and if we run
this you'll see that we've added a nice
X label and Y label whoops where'd they
go and it turns out in this environment
that you have to put it before the title
so let me go ahead and put it before the
title and there's our x y and we run
that and of course we can also do upper
size a little bit so you can see what's
going on a little better so here we have
X label X and if you come down here
you'll see our X label and our y label
we can of course change this to X
label you can change this to y
and be whatever you want on here of
course and our title graph there we go
run so here we have a title of graph our
y label and our X label all set up on
our nice little plot and then before we
move on to the next section let's do
more thing on here we have a thing
called The Legend and we're going to do
we're going to set our ax Legend label
one label two up here it's a format for
it but let's go down here and actually
use it I'm going to do two different
plots we're going to have axes plot X by
x times x squared and X cubed and if I
run this you'll see it puts two nice
graphs on the setup on there but it's
nice to have a legend telling you what's
going on so for the legend we can
actually do axes since we have the two
plots Legend and on here we've created
an array and we have y equals x squared
Y equals X cubed you can actually put
this as whatever you want those are just
strings and then location two and let's
go ahead and run this and see what that
looks like and you can see it puts a
nice Legend on the upper left hand
corner location two we can do location
three and run it and it drops it down to
the bottom
one I can't remember where that's at
there we go upper right so each one of
these is a number that refers to the
different locations on the screen zero
kind of have to play with them or look
them up to remember where they're at but
they do work it just kind of moves
around depending on where you want your
Legend data on there so on this section
we cover the title the graph the Y
labels and legends this is we're getting
into some starting to look really fancy
here so we now have something we can
actually put out you'll see the title of
the graph looks a little fuzzy so I
might in a web setup put the DPI up a
couple notches maybe put it at 200 100
might work fine just so you know
something to notice on here when you're
playing with these different things we
had our subplots DPI equals oh let's do
200 and see what that looks like so you
can see it now is a lot clearer it's
also larger so it's a nice little
feature you can throw in there with your
DPI dots per inch so the next section is
let's look at some graph features we're
going to look at line color transparency
size and a few more things on here and
oops I forgot the main title so we have
our figure in our axes equals our plot
and subplots and I'll go ahead and do a
DPI equals 150 so the graph comes out
nice and large and easy for you to see
and let's go ahead and do three plots on
here we'll do X by X plus one so it's
going to be a straight line plot
X Plus X plus two
and axes Dot Plot
x x
plus three this looks like we're doing
nearest neighbor setup we're showing how
it located data putting your lines on
there between the nearest neighbors
there we go so it draws a nice little
graph with three lines on it one of the
things we can do is we can control the
alpha on this oops and you can actually
see the when they did these lines it
automatically pulls in different colors
for your setup so some automatic
automatic things going on in there and a
lot of times we do that comma R but
we're going to do color equals red
another notation on here let's go ahead
and run this now we have a bright red
line down there and with the matplot
library you're not limited to Red you
can also use the one of many different
color references as you see here with
the pound sign
1155dd which just is just blue and we
could do the same thing with another
color on here which it turns out to be
green I can just as easily do this green
blue oops there we go blue and run that
and you'll see here we have red blue and
green and what I want to do is I want to
make this we're going to say What's
called the alpha on this and we're going
to set this equal to 0.5 so this is
halfway see-through when I run this and
it's almost going to look pink because
you can see through it and let's change
this just a little bit just to make this
kind of fun let's Square it there we go
run it so now we have this nice square
that comes up and you can see when it
crosses it because I plotted these two
lines after it and they have no Alpha
the red is behind those lines or in this
case pink because we did the alpha
halfway through so let's go ahead and do
this Alpha equals 0.5
and oh you know what instead of squaring
it let's take it to the 0.5 power
that'll be kind of interesting to see
what that does we'll just go to keep it
squared there we go and run that and
let's go back and look at this where it
crosses over and the first thing you see
right here is on the blue it's kind of
light blue now you can see how the two
colors add together you get almost a
purple on there so I can clearly see
where the red crosses the blue line and
then the green just blanks it over
because I didn't do any opaqueness no
Alpha on there so this is great if you
have lots of data that crosses over and
you need to be able to track those lines
better and we'll go ahead and do this
0.5 and we'll run that oops I Did equals
0.5 let me go ahead and run that and so
you can see right here now you can
easily see the red line how it crosses
the green and the blue down here and if
we want to we can do this as the default
is one is solid so we can change this
all to 0.8 let me just do that oops 58 8
there we go run up so it must have hit a
wrong button there let me try that again
I actually get rid of a bracket and
let's go ahead and run that and we come
down here and look at this you can still
see where it passes behind them but the
green dominates and the blue dominates
because we're now at eighty percent
instead of 50 percent and you can do
less that's kind of fun although at some
point the lanes kind of fade
so 0.5 is usually the best setting on
there we have a nice pastel here at 0.3
and you can easily see where they cross
over and just like you can play with the
colors we can play with line width and
you know Let's do let's try DPI 100 and
see what that looks like on my screen
equals 100 and we'll go ahead and just
take our ax plot
let's do four of these lines just so you
can see how they look next to each other
click here there we go
and if I run this they should all appear
the same it automatically does different
colors on there so let's do color equals
blue
I forgot my quotation marks there we go
and we'll go ahead and just make these
all blue just for purposes of being nice
and uniform and then what I want to do
is I want to do the line width
width equals
0.25 and let's just copy and paste that
down here
let's do equals one
about 1.5 and let's do one let's make
this equal to two let's see what that
looks like and we do that you can see it
goes from a very thin line a 0.5 a 1 or
1.5 and 2 which is twice the width of
the one and if we're going to do
different sizes we had different colors
we had our Alpha scheme let's take this
whole thing here let's paste it down
here and do another one but instead of
line width let's look at Styles and
something to note here you can actually
abbreviate this with LW so line width
can also be Point let's just do
everything point two and let's set up a
line style we do the first one dashes
and let me just paste that down here so
I'm not doing a lot of extra typing
there we go
this out so we have our dashed
we can do a dash dot we'll just do the
dash dot here and a colon here there we
go and there's a lot of different
options we'll look at a few more as we
go down for different ways of
highlighting data but when you look at
this we have everything as a line width
of two and now we have a straight line
we have a dashed line or a dot dash and
a dot dot dot line
and then another thing we can add on
here is we're going to do here's our ax
plot and we did X plus two X plus four
so it goes right on the top and do color
black line width 1.5 so it's a smaller
line and we're going to take the line
and we're going to set dashes so look
I've changed some of the notation here
for my line and my X plot so I can set
my line comma equal to X plot and then I
can change the line settings this way
and when I run this let me run that on
here you'll see the 5 10 15 10 creates a
series of dashes that are varied in
length Link in this case they alternate
between a short Dash and a long Dash and
we can play with these numbers curiosity
always has me what happens when you play
with the numbers just to see what they
look like let's do this and let's paste
this down here I'll do two of these just
because they're kind of fun to play with
and let's change this from 10 to 3 and
we're going to change this one from 15
to to four and let's run that and you
can see the differences in the lines
oops very a little bit confusing on
there because I forgot to change the
lines are all on top of each other so
let me change that really quick here and
let's run that and now you can see
here's our original dashed line
alternating when I change these numbers
on the second one the very end value to
3 you can see now we have the dashes of
five let's see I'm going to guess this
is a dash is a five skip ten dashes of
15 skip 3 and then it goes back to the
beginning dashes five dashes skip 10 15
dashes skip three and of course the last
one we just switched up a little bit it
looks a lot more uniform because I'm
using two sets of ten or if I did
something like this and changed it to 30
it really becomes pronounced as far as
the distances between them and instead
of four let's go let's put 30 here also
30 by 30 there we go really pronounced
on that one and let's look at one more
important group for plotting our data
and in this we're gonna here's our plot
we started with with the X Plus One X
plus two X plus three and did it in Blue
on this one's three or four different
blue lines and this property we want to
add the actual plots so you can see
where the plots are on the graph and for
that we might have marker equals o and
if we run this you'll see it puts a DOT
for each of these and there's 25 dots
because we have 25 x values so we
actually have 0 in each of the different
values of X Y are then plotted here with
the dots and we don't want to just limit
ourselves to dots
we can also do plus sign that's another
option dots is most common I'll actually
like the dots the best if we do the plus
signings here puts a nice crosshairs or
plus sign on there and we can do a
marker there's a number of different
markers you can use
I think this one was it s is another one
which is a nice square and that's
actually a good one s for square o for
period okay that's just kind of weird so
you can see that probably on these
markers another one is the number one so
if we run that you'll see we now have
these little hatch marks and let's take
all this just go with the o on this one
by the way this works with square really
nicely some of the stuff we're going to
do here on just a second let's do Marker
size equals two and change that to five
and run that and you can see here it
puts a nice little tiny dot versus uh
the size die here this is interesting
because it said two I thought it would
be bigger
but if you do 0.5 it gets even smaller
and let's just do 10 to see what that
looks like run that looks huge so marker
size a lot of these are dependent on the
DPI and the setups there's things that
switch around as far as the way the size
shows up you got to be a little careful
when you change one setting it can
change all the other markers and then
let's take our Square on here
and we'll do we have marker size we also
have marker face we'll set that equal to
Red of course we let me take change the
so it's up one notch we'll run that
whoops must have missed type something
on here and I did is marker face color
equals red and so when I run that you
can now see I have the squares on there
with the marker face color of course we
can mix and match these
come down here and we'll make this
instead of this let's make this plus
seven and we'll make this
15
marker face color equals and we'll do
what green just because there we go run
very hard to actually see what's going
on there still 25 dots they kind of
overlap as you can see they print them
over each other and of course if we
really wanted to make it look horrible
we could just make that really huge
generally though you'd want something a
little bit smaller and cuter we'll just
try doing it this way there we go that's
too small to even see the face so four
you start to see the face on there
around four maybe on eight eight might
be a good number for this there we go
eight again that all just depends on
what you're trying to show and display
so we've covered a lot of stuff here as
far as our lines we've covered opaque
with our Alpha setting on there give us
some nice pastels you can see how they
overlap and how they cross over we
covered the line width different size on
there different formats for the line
itself and these are all you can combine
all these so you can have our line width
equals two line style equals you can
bring this down here also to the markers
and then we added markers in just enter
the circle a plus sign the square a
little tick which uses a one then we had
a marker size and a marker color face
and we combine those you see we get a
nice different series of representations
we also briefly mentioned color where
you didn't have to use like in here we
used color black someplace up here I'd
have to find it we use the actual number
for the color as opposed to I changed it
to red and blue so you get very precise
on the color if you have very specific
color set that you need to match your
website or whatever you're working on
all of those are tools in the matplot
library so we have one more piece to
formatting the graph so we want to show
you and then we have two big sections
we're going to go over the different
graphs that they have along with a
challenge problem so let's go in the
last section we're going to look at is
limits we're going to limit our data so
this first parameter is going to paste
in there we're going to create our
subplots one two so one row two columns
we're going to do a figure size a 10
comma five does it all look familiar now
since we've done a number of them and
we're going to go ahead and plot and
this is an interesting notation you
should notice here our axis zero so one
we've used instead of you can just
iterate through them but they're just an
array so it's an array of zero is still
the axes of the first axis out of two
and we're going to plot x x squared x x
cubed line with two so we're going to go
ahead and just plot two graphs right on
top of each other without doing multiple
plots on here and we'll set the grid
equal to true on here let's go ahead and
run that and you can see here are two
plots with the x value going across and
I'm going to do something similar and by
the ways you can just if you look at it
you can see the Grid on there that's all
that is easier to spot the data going
across we're going to take the same data
for axes one so we have our plot of x x
squared x and x cubed line with two and
this time we're going to take our axes
one and do y limit that's actually set
underscore y limit this is the y axis so
it's going to be an array of two two
values and we'll do zero comma 60. I'm
just making these numbers up the guys in
the back actually made them up I'm just
using their numbers and we're going to
set the x limit
and let's set the x limit as um don't
forget our brackets there 2 comma five
so it's the same data going in and but
we're setting a limit on it let's go
ahead and run that and let's see what it
comes out of and here we have the Y
limit 0 to 60. so we're looking at just
the lower part of this curve here up to
here and we have the X limit two to five
so that starts right here at 2 and you
can see very different graphs this is
kind of nice because you could actually
put one of these on top of the other if
you wanted to draw Focus to one part of
a graph remember how we did that earlier
one inside the other but just a quick
note you can easily limit your graph and
re kind of reshape the way it looks
quite easily and we can also add that
grid down there if you want a grid
run that and add the Grid in there oops
I guess you have to do the grid
beforehand
switch that there we go sometimes the
order on this is really important so you
may double check your order when you're
printing these things out and also if I
change it to one so in this case it
might not be the order I wonder if I'll
go back here as one there we go so it
doesn't matter the order and grid but
you can set the grid for easy viewing in
here nice setup on there but you can see
how we can limit the data so let's start
looking at some other 2D graphs and make
this cell a markdown so we run it as a
nice pretty title to it and let's go
ahead and create some data with an NP
array we'll do 0 to 5 on here there we
go and let's look at four common graphs
we'll put them side by side so we'll do
a figure our axis equals plot subplots
one four columns and then figure size
hopefully will fit nicely on here it
seems to do a pretty good on here and
I'll go and just run that since we're in
there run you'll see I have my four
blank plots on here
and we'll start with axes of zero the
set title
and we want this to be a scatter plot
a scatter plot just means it has a bunch
of dots on it so here's our axes of zero
dot scatter easy to remember scatter a
bunch of plots on there we'll do our in
we can do X or nope there and let's go
ahead and do axis set title scatter up
already did that we're just going to do
scatter
that's how you do it on there notice how
you create a scatter plot with simply
with the scattered control and we'll do
let's do the variable X X Plus let's
throw some Randomness in here use these
Scatter Plots are have a lot of random
numbers connected to them that's why
they do them on there and so the bigger
the X gets the bigger the randomness so
0.25 times the randomness and what we
should end up down here is with the
scatter plot and you can see as you go
up it just kind of has some random
numbers and moves up and down the line
but plus just the points so if you
remember from back up here where we did
marker this is plotting basically just
the marker so it's a scatter plot
probably less used is a step plot so for
X is one we'll go ahead and do a step
plot so you can see what that looks like
and this time I'll use our n value
instead of X we generated that n value
up here and so for this we have n n
times two or N squared n times 2 N
squared line width equals two and if we
run that it creates a nice Step Up
let's see so we've got a scatter plot
we've got a step plot let's do a bar
plot
and we'll use the same formula N N
squared alignment centered because you
can have them left or right with 0.5 and
Alpha if you remember correctly that's
how opaque it is let's see what that
looks like on there so we have some nice
you can see here a nice bar plot it
should look very similar to the step
plot but color it in
and we can change the width let's see
what happens we do 0.9 run and if we
take width out completely
run that you can see it starts coming
together on there and we can change the
alpha we can take the alpha out too and
run that so you know you have the solid
colors and if we take out the center
and run that everything you really can't
see the shift on here because that's
actually the default on this but these
are common settings for the bar graph
let me just put them back in there there
we go alignment center and Alpha now I
can't say I have Houston step craft very
much there's certain other certain I
guess domains of expertise that require
a step graph but the scatter plot and
the bar graph very common especially the
bar graph and we'll look at histograms
here in just a minute so I use
histograms a lot especially in data
science but this is nice if you have
very concrete objects somebody how many
people wearing yellow hats that kind of
thing but if we're going to do that
let's go ahead and do the last one which
I see a lot more in the Sciences
certainly using the data science but
more like for mapping I saw publication
on solar flares and they were discussing
the energy and so filling in the graph
gives it a very different look so we're
going to do the fill between and it's
just like you think it'd be it's filled
between but with a underscore between
and we'll do x and x squared and x and x
cubed and we'll do color green and Alpha
again in case you had other data you
want to plot on there you can see it
forms a nice squared coming up here and
also if you look at the bottom one is
your squared value the upper line is
your cubed value and then it fills in
everything in between
if you remember from calculus this would
be if you had like a car a motor an
efficiency they would talk about the
efficiency going up and the loss and
you're looking for the space or the area
between the two lines so it gives you a
nice visual of that now let's look at a
few more basic two-dimensionals so we
have our figure figure size on here
we're going to do a radar chart to be
honest I've never used a radar chart in
business or in data science I can't
Define a reason to use one now so the
first line for doing a radar chart we
have to add axes and the figure and with
this this actually creates our oh this
let's run it so you can see what it
creates it creates a nice looks like
you're on a submarine and you're
tracking The Hunt for Red October or
something like that and it needs all of
these the polar is the fact that we're
doing polar coordinates 0 0.6.6 has to
do with the size if you take out any of
these things and run them you get just a
box if you take out the other half you
pretty much get nothing in there and if
you change these numbers and change them
a little bit you can see it gets bigger
they had 0.6 on here I'll go ahead and
leave it as one because that's just kind
of fun but that's all about the size on
here the height and the width and then
let's create some data T equals NP line
space and this is 0 to 2 times NP times
pi so if you remember that is the
distance across and we're going to
generate a hundred points so this is
just a thing of data we're putting
together let me simply do an ax Dot Plot
and in this case let's do T comma T
which would be a diagonal line and a
regular chart and we'll give it a nice
color equals blue
and line width equals three let's see
what that looks like and we can see here
a spiral coming out remember this would
be just a diagonal line on a regular
chart what happens if we take this and
instead of T time 0.5 there we go and
you can see it slightly Alters the way
it spirals out we could do T times two
it spirals out a little quicker so it's
kind of just a fun like I said I've
never used a radar chart it's a column
but you can always think of radar under
submarine kind of looks like one of
those or on an airplane and none of this
would be complete if we didn't discuss
histograms oh my gosh do I use a
histogram so much and we'll use our
numpy that we have set as NP to generate
oh it looks like we have a hundred
thousand variables we're going to set
equal to n and of course we create our
figure and our axes from subplots one
two figure size 12 14. so we're going to
look at two different variations of the
histogram and we'll set a title default
histogram set our title there and then
this is simply hit for histogram and
we'll just go ahead and put in our n in
there and let me run this and see what
that looks like
and let's talk about what is going on
here so we generated an array here of
data 1 000 random arrays it looks like
they're mostly between minus four and
four and then it adds up each one it
says I'm zero you have 35
000 that are zero so that's what's most
common on here and we have 20 000 that
are somewhere in this range right here
between the minus two and well it looks
like one and minus two and somewhere
between zero and one there's thirty
thousand numbers so all this is saying
is this is how common these variables
are and this gives you this is a point
in so many directions when you're
looking at data science to go ahead and
run your histogram so you should always
have your histogram and you can always
put limits and all the other different
things on your rate just like you did on
the other graphs on there and then we're
going to do a cumulative detailed
histogram and all it is is a histogram
let me just do that
and we set cumulative equal to true
and bins equal 50. and I really want to
highlight the the cumulative equals true
is important but we can now choose how
many bins we have in the first one it
kind of selected them for us in this
case let me go ahead and run this and
you'll see it has the printed data out
for us and here's our whoops I must have
missed oh there we go it doesn't help
that I put it over the old one there we
go okay so now you have your default
histogram and now we have a cumulative
histogram and we should have 50 steps in
there and let's just find out if that's
true not So Much by counting them I'm
not going to count them if you want to
you can count them let's just change it
to 10 and see what happens and we see
here we have now 10 counts of that
set that for five
and run that
and then we have our five on there and
we go ahead and take the cumulative
equals true out just so you can see what
that looks like and let me run that on
here too
that looks just like it did before I
think there's what one two three four
five six seven eight they have eight
different bins on here is what the
default came out of
put that back in there run and so now it
should look almost identical and it does
and then we can put the cumulative back
in see what that looks like with the
cumulative
and run that
and we can see how that shifts
everything over and has a slightly
different look wait it shifts it all to
the right no it doesn't actually shift
it to the right it's cumulative so it's
the total of the different currencies
and so what that means is like if you
consider this like for the year of
rainfall we have like day one you had a
little bit of rain day two we have more
rain and so if you look at the number
this is a hundred thousand thirty five
thousand so it's a cumulative detail the
histogram the currents as it grows and
rainfall is a good one because that
would be a cumulative histogram of how
much rain occurred throughout the year
and we're going to look at two more
graphs we've already looked at a bunch
of them we looked at our radar graph
we've looked at scatter step bar fill in
basic plots we've looked at different
ways of showing the data and we can
increase the size of the line the look
the color the alpha setting
so let's look at contour maps just put
that in there there we go draw a contour
map and before we draw a contour map we
need to go ahead and create data for it
and if you have Contours your data is
all going to have three different values
so let's go ahead and create the data
here we have our you'd import your
matplot library your numpy so we have
our numbers array and we'll import
matplot.cm and that's your color Maps so
you have all these different color maps
you can look at there's like hundreds of
color Maps so if you don't want to do
your own color you need to do your own
color map they're pretty diverse and of
course our PLT we're going to our PI
plot and to generate our different data
we're going to create a Delta 0.025 and
we'll start with X and we're going to
create an array between -3 and 3 and
Delta increments of 0.025
and we'll have our y we'll do something
similar and then we'll create our X Y
into a mesh grid again these are all
numpy commands so if you're not familiar
with these you'll want to go back and
review our numpy tutorial and we'll do
an exponential line here minus x squared
minus y squared for our Z1 we'll do a Z2
so we have two different areas and Z
equals z of one minus Z2 times 2. so
we've created a number of values here
and let me go ahead and run this and
let's plug that in so you can see where
those values are going so once we've set
these we're going to create our figure
and our X from our PLT subplots we're
going to create the variable Cs and this
is going to be our Contour so right here
CS is our Contour surface and we're
feeding it X Y and Z if you remember X Y
we created as our X and Y components
using our mesh grid and you know what
let's do this just because it's kind of
good to see this let's go ahead and
print X and let's print Y and I always
like to do this when I'm working with
something this either is really
complicated in this case is what we're
looking at or you don't understand yet
so we've created a mesh grid and we have
x y and when we're done with this we end
up with here's our X
and this set of values in our y so those
are X and Y coordinates and then we've
also created Z based on our X and Y so
we have x capital x capital Y and
capital Z is our three components X and
Y being the coordinates well Z is going
to be our actual height since we're
doing a contour map so we created our
contour map from our X Y and Z
coordinates we want to go ahead and put
in a c label maybe we want to go ahead
and do an title on here
we'll put that in our set title and this
is a contour there we go contour map and
let's go ahead and run this and see what
that looks like and you'll see we
generated a nice little contour map
there's different settings you can play
with on this but you can picture this
being you're on a mountain climb and
here we have a line that's represents
zero maybe at sea level and then moving
on up you have your Contours of 0.5 and
then minus one and different setups
little Hills I guess if it's minus
that's like a pit so I guess you're
going down into a pit at minus five and
minus one but on the other side you can
see you're going up in levels so here's
a mountain top and here's like a basin
of some kind and in data science this
could represent a lot of things this
could also be representing two different
values and maybe profits and loss I
don't know if I'd ever really do that as
a contour map but I'm sure you could be
creative and find something fun to do
with a contour map and then we're going
to look at one last map which is the 3D
map and those are can be really
important as a final product because
they can show so much of additional
information that you can't fit on
two-dimensional graphs
there we go draw a 3D image and so we're
going to import from our MPL toolkits
the implant 3D and the axis 3D we're
going to import axis 3D this is what's
going to let us work with the 3D image
and this should look familiar we're
going to create another figure just like
we did before figure size 14 by 6 that's
a good fit on the screen we'll go ahead
and run that so we have our figure and
let's go ahead and take our X and we're
going to set that equal to Fig dot add
subplot that should also be familiar
from earlier and we're going to work
with this sets the settings for the
projection we're going to use one two
one projection 3D and we'll see what
that looks like in just a minute and we
just created some three-dimensional data
here before where we had X Y and Z
capital X Y and Z so we're going to
reuse that data we're just going to use
that since this also this is also a
three-dimensional image so let's use
that for a three-dimensional graph and
we simply do ax plot underscore
surface and a capital x capital y
capital Z so there's our data coming in
and we're going to add some settings in
here we're going to do R stride 4 C
stride four and line width 0. I'll show
you what that is here in just a minute
let's go ahead and run that so we can
see our graph and of course it helps if
I don't add an extra comma in there and
you can see it generates this really
beautiful three-dimensional graph
so let's take a little bit time to
explore some of these numbers we have
going in here
we have the r stride 4 the C stride four
and the projection 3D projection 3D is
the important one because that's telling
us that this is a 3D graph here
so what are these first numbers one two
one let's just change one of these I'm
going to change this to five
and it's going to give me an error let's
change it to 1.
and oop that didn't work let's change
this middle one to three instead and
you're going to see how it starts
reshaping the size and how it fits on
the screen and we'll change the first
one to two and we'll run that one and
again it's changed the dimensions and
the size and how it fits on here play
with these numbers to get a nice look
and feel for it part of it is the Tilt
and the angle
I'll do seven on this one
there we go you can see it really
shifted it there
but again that changes the size and
outfits on the canvas but we'll leave it
at the one
two and just so you get a good look at
what that we're talking about here this
is column width and index from before
if we do one one one you can see that it
now spreads it out all the way across
uses the whole setup on there so this
has to do with the size and how big you
want it to be now there's one term that
we didn't cover in this yet but we've
used it throughout the whole setup
and I'm just going to type that down
here even though we're not going to go
into detail and that's the term heat map
you might see that is kind of starting
to lose ground as far as a common
reference but there sure are a lot of
people still talk about heat Maps what
is a heat map well it is simply a color
map that's all it is so if you ever see
the term heat map
that refers to the fact this is in
different colors representing different
heights
that one is in the heat map but you can
see up here we switched into let me go
back up here here we go this one has
different colors for the different
values a lot of times you'll use like
instead of X and Y you might do a heat
map
where you have a fourth value and the
fourth value represents the color and so
you'll see this 3D image into nice
colors represented by heat map that's
all it is so if you see the term heat
map that only means we're plotting some
of the data in color to make it stand
out or to give it a fourth dimension in
this case
so we've covered a lot of things on
matplot and that brings us covered all
the basics so that brings us to practice
example and this is going to be the
challenge for you and let me go ahead
and change our cell cell type mark down
and run that so it looks pretty
practice example write a Python program
to create a pie chart of the popularity
of programming languages
okay excellent
and if you're going to have a challenge
we need some data and I'll just throw in
our import our matplot library at the
beginning you should do that
automatically and so for our data to
plot we're going to have our languages
we're going to have python we're going
to have Java PHP JavaScript c-sharp C
plus plus so those are six categories
and then we have our popularity oops
misspelling there popularity we'll give
the first one 22.2 percent Java 17.6 and
I don't know if these are real numbers
they pulled my guess is that they might
have just been made up because I don't
know if Python's really that much more
popular than the other ones maybe
specific to data science because
Python's very popular in data science
right now because it has so many options
the only other program that's highly
used and exclusively for data science is
R so Python's big and python also does a
lot more it's a full programming
language where R is primarily for data
science they didn't put R in here so we
have python we have Java we have our PHP
see the different values they've given
it or different percentages
and I did add these up does not add up
to 100 it adds up to 71 percent or
something like that
and then we're going to give colors and
we've chosen these guys in the back
brought in these colors I'm not sure
what these colors are we'll find out in
a minute so I'll be exciting but you can
see they're using the actual color
values you can pull off of a color wheel
or something like that it could have
just as easily done blue red green if
you're too lazy to pick the exact colors
and then let's go ahead and solve this
and see we got here we're going to do
something a little fancy just because we
can the first thing we're going to do is
we're going to use a variable called
explode and you'll notice that there's
six variables in here so that matches
our six different categories and the
first one we've done is 0.1 and then
zero zero zero zero zero point one when
we put this in here under the explode in
the plot it will actually push that
square out so it's a really cool feature
to highlight certain information on a
pie chart
and this is simply PLT dot pi and we're
plotting
popularity there we go and before we add
in all the really cool settings for this
let's go ahead and run it and you'll see
we generate a nice flat pie not too
exciting there and then we'll go ahead
and put in all the extras I talked about
explode or we can explode one of the
values out so here's our explode equals
explode labels as languages because we
want to know what the different colors
mean here's our colors equals colors our
Auto picture and this is standard print
format so that's a python setup on there
and that's just going to put the value
on the pi slice and then we're going to
add Shadow because it just looks cooler
with a shadow gives a little 3D look and
we'll do a start angle of 140. let's go
ahead and run this and take a look and
see what comes out of that
and look how that changes the whole
setup so here's our labels there's our
value we put on there there's our slice
that's pushed out there's our shadow a
3d effect and then we started at 140. we
could also rotate this let's just do
this angle 90.
and if we run it
you'll see the blue pie slices moved up
a little bit we could actually do
actually let's just take the whole
starting triangle out and run it'll
default to zero
this is what it looks like if it
defaulted to zero so depending on where
you want the highlighted slice to appear
usually you want that to appear on the
left because people read left to right
and so it draws a focus onto in this
case Python and how great python is I'm
a little biased we're teaching a Python
tutorial so it should be understandable
that we're looking at Python and one
last reference before we close you can
go over to the matplot library.pi plot
setup and if you go underneath there the
different functions on there you can
look this up on their website you'll see
a full list and this is why it's so
important to go through a tutorial like
this because this list is just so
massive I'm trying to figure out like
here's our bar plot there's a bar H you
can add barbs there's a box plot we
didn't cover C labels a totally
different kind of for your Contour plot
you can set up in there if you go down
here we have our figures we used on
there we showed you the basics how to do
the figure you'll see some closer
references on those there's a histogram
down here h-i-s-t there's also the hist
2D makes a 2d histogram plot H lines all
of this these are all the different
commands that are underneath of here and
you can see it's pretty extensive we've
covered all the basic ones so that you
know have a solid ground to look at
these different options so when you come
to these functions some of them are
going to look a little off or not off
will look unfamiliar but you'll still
have the availability to probably
understand most of this and have a basic
understanding of your matplot library
let's go ahead and open up another
Python 3
setup in here
and so we want to explore what happens
when you want to display this this is
where it starts getting in my opinion a
little fun because you're actually
playing with it and you have something
to show people and we'll go ahead and
rename this we're going to call this uh
pandas and Pi plot
so pandas pipelot this we can remember
for next time and we want to go ahead
and import the necessary libraries we're
going to import pandas as PD now
remember this is a data frame so we're
talking rows and columns and you'll see
how pandas work so nicely when you're
actually showing data to people and then
we're going to have numpy in the
background numpy works with pandas so a
lot of times you just import them by
default
Seaborn sits on top of the matplot
library so sometimes we use the Seaborn
because it kind of extends it's one of
the 100 packages that extends the
matplot library probably the most common
used because it has a lot of built-in
functionality almost by default I
usually just put Seaborn in there in
case I need it and of course we have
matplot Library as pipelot as PLT and
note we have as PD as NP as SNS as PLT
those are pretty standard so when you're
doing your Imports I would probably keep
those just so other people can read your
code and it makes sense to them that's
pretty much a standard nowadays
and then we have the strange line here
it says Amber sign matplot Library
inline
that is for Jupiter notebook only so if
you're running this in a different
package you'll have a pop-up when it
goes to display the matplot library you
can with the most current version of
Jupiter usually leave that out and it
will still display it right on the page
as we go and we'll see what that looks
like
and then we're going to go ahead and
just do the Seabourn the sns.set and
we're going to set the color codes
equals true let them just keep the
default one so we don't have to think
about it too much
and we of course have to run this the
reason we run this is because these
values are all set if we don't run this
and I access one of these afterward
it'll crash
the cool thing about Jupiter notebooks
is if you forgot to import one of these
you forgot to install it because you do
have to install this under your anaconda
setup or whatever setup you're in you
can flip over to Anaconda and run your
install for these and then just come
back and run it you don't have to close
anything out
and we'll go ahead and paste this one in
here real quick where we have car equals
PD dot read underscore CSV
and then we have the actual path
this path of course will vary depending
on what you are working with so it's
wherever you saved the file at and you
can see here I have like my OneDrive
documents simply Learn Python data
analytics using python slash car CSV
it's quite a long file
when we've opened that up what we get is
we get a CSV file and we have the make
the model the year the engine fuel type
engine horsepower cylinders and so on
um and this is just a comma separated
file so each row is like a row of data
think of it as a spreadsheet
and then each one is a column of data on
here and as you can see right here it
has the make model so it has columns for
a header on here
now your pandas just does an excellent
job of automatically pulling a lot of
this in so when you start seeing the
pandas on here you realize that you are
already like halfway done with getting
your data in I just love pandas for that
reason numpy also has it you can load a
CSV directly into numpy but we're
working with pandas and this is where it
really gets cool is I can come down here
and I can print remember our print
statement we can actually get rid of it
and we're just going to do car head
because it's going to print that out the
head is going to print the top values of
that data file we just ran in
and so you can see right here it does a
nice printout it's all nice and in line
because we're in Jupiter notebook I can
scroll back and forth and look at the
different data and just like we expected
we have our column and it brought the
header right in
one thing to note is the index it
automatically created an index 0 1 2 3 4
and so on and we're just looking at the
head so we got zero one two three four
you can change this you might want to
just look at the top two we can run that
there's our top two BMWs another thing
we can do is instead of head we can do
tail
and look at the last three values that
are in that data file and you can see
right here numbered them all the way up
to
11913 oh my goodness they put a lot of
data in this file I didn't even look to
see how big the file was so you can
really easily get through and view the
different data in here when you're
talking about Big Data
you almost never just print out car in
fact let's see what happens when we do
if we run this and we just run the car
it's huge in fact it's so big that the
pandas automatically truncates it and
just does head plus tail so you can see
the two so we really don't want to look
at the whole thing I'm going to go back
to we'll stick with the head
displaying our data there we go so
there's ahead of our data it gives us a
quick look to see what's actually in
there I can zoom out if we want so you
can actually get a better View
although we'll keep it zoomed in so you
can see the code I'm working on
and then from the data standpoint we of
course want to look at
data types what's going on with our data
what does it look like now this you know
you show your when you're talking to
your shareholders they like to see these
nice easy to read charts they look like
a spreadsheet so it's a nice way of
displaying pieces of the chart
we talk about the data types now we're
getting into the data science side of it
what are we working with well we have
make model we have an integer 64 for the
year engine fuel type is an object if we
go up here you can see that there most
of them are
like you know it's a set manual rear
wheel drive so they might be very
limited number of types in there
and so forth and you'll it's either
going to be a float 64 an integer or an
object it's the way it's going to read
it on here
and the next thing you're going to know
is like your columns
and since it loaded the columns
automatically we have here the make the
model the year the engine the size all
the way up to the MSRP
and um just out of something you'll see
come up a lot is whenever you're in
pandas and you type in dot values it
converts it from a pandas list to a
numpy array
and that's true of any of these so then
you end up in a numpy array so you'll
see a little switch in there in the way
that the data is actually stored and
that's true of any of these
in this case we want car dot columns
you have a total list of your car
columns
and like any good data scientist we want
to start looking at analytical summary
of the data set what's going on with our
data so we can start trying to piecemeal
it together so we can do car
describe
and then we'll do is we'll do include
equals
all
so a nice Panda command is to describe
your data
if you're working with r this should
start looking familiar
and we come down here and you can see
count there's a and make the model of
the year how many of each one how many
unique values of each one the top value
of each one what's most common the
frequency the mean clearly on some of
these it's an object so really can't
tell you what the average is it'd just
be the top ones the average I guess
the year what's the average year on
there all this stuff comes down here
your standard deviation your minimum
value your maximum value what's in the
lower quarter fifty percent Mark where's
that line at and what's in the upper 75
percent the top 25 percent going into
the max
now this next part is just cool this is
what we always wanted computers be back
like in the 90s instead of 5 000 lines
of code to do this maybe not five
thousand all right I built my own plot
Library back in 95 and the amount of
code for doing a simple plot was um
I don't know probably about 100 lines of
code
this is being done in one line of code
we have our car which is our pandas we
generated that it's our data frame and
we have dot hist for histogram that is
the power of Seaborn now it's still
going to generate a numpy graph but
Seaborn sits on top and then we can do
the figure size this is just um so it
fits nicely on the paper on here and we
do something simple like this and you
can see here where it comes up and does
say matplot library and does subplots
and everything
but we're looking at a histogram of all
the different pieces in our database and
we have our engine cylinders that's
always a good one because you can see
like they have some that are they had a
null on there so they came out as zero
maybe a couple maybe one of them had a
two cylinder engine away back when four
is a common uh six a little less common
and then you see the eight cylinder 12
cylinder engines whether it's got to be
a Speedster or something
uh but you can see right here just
breaks it down so now you have how many
cars with how many whatever it is
cylinders horsepower and so on and it
does a nice job displaying it
you can see if you're working with your
uh um you're going into your demo it's
really nice just to be able to type that
in and boom there it is I can see it all
the way across
and we might want to zero in and use
like a box plot and this time we'll go
ahead and call the Seaborn SNS box plot
and we're going to go ahead and do
vehicle size in versus engine horsepower
XY plot and the data comes from the car
so if we run this we end up with a nice
box plot
you see our mid-size Compact and large
you can see the variation there's our
outlier showing up there on the compact
that must be a high-end sports car a
large car might have a couple engines
and again we have all these outliers and
then your deviation on them
very powerful and quick way to zero in
on one small piece of data and display
it for people who need to have it
reduced to something they can see and
look at and understand and that's our
Seabourn box plot or sns.box plot
and then if we're going to back out and
we want a quick look at what they call
pair plotting we can run that and you
can see with the Seaborn it just does
all the work for you
uh it takes just a moment for it to pull
the data in and compile it
and once it does it creates a nice Grid
in this grid if you look at this one
space here which is you might not be
able to see the small number it says
engine horsepower this is engine
horsepower uh to the year was built and
it's just flipped so everything to the
right of the middle diagonal is just the
rotation of what's on the left and as
you expect the engine horsepower gets
bigger and bigger and bigger as time
goes on so the the year was built the
further up in the year the more likely
you are to have a heavy horsepower
engine
and you can quickly look at trends
with our pair plot coming up and look
how fast that was that was it took a
couple moment to process but right away
I get a nice view of all these different
information which I can look at visually
in in kind of see how things group and
look
now if I was doing a meeting I probably
wouldn't show all the data
one of the things I've learned over the
years is people myself included love to
show all our work you know we were
taught in school show all your work
prove what you know the CEO doesn't want
to see a huge grid of graphs I guarantee
it so we want to do is we want to go
ahead and drop
um the stuff that might not be
interested in and we're gonna I'm not
really a car person a guy in the back is
obviously so you have your engine fuel
type we're gonna drop that we're gonna
drop Market category vehicle style
popularity number of doors vehicle size
and we have the axes in here if you
remember from numpy we have to include
that axis to make it clear what we're
working on that's also true with pandas
and then we'll look at just the what it
looks like um from the head and you can
see that we dropped out those categories
and now we have the make model year and
so forth and we took out the engine fuel
type Market category Etc
and this should look familiar to you now
when you start working with pandas I
just love pandas for this reason look
how easy it is it just displays it as a
nice spreadsheet for you you can just
look at it and view it very easily it's
also the same kind of view you're going
to get if you're working in spark or Pi
spark which is python for spark across
Big Data this is the kind of thing that
they they come up with this is why
pandas is so powerful
and we may look at this and decide we
don't like these columns and so you can
go in here and we can actually rename
the columns
simple command car equals car rename
columns equals engine horsepower equals
horsepower this is just your standard
python dictionary
so it just Maps them out and you know
instead of having like a lengthy here we
had engine horsepower we just won
horsepower we don't need to know it's
the engine horsepower
engine cylinders we don't need to know
that it's for the engine because there's
only one thing we're describing if we're
talking about cars and that cylinders
and we'll go ahead and just run this and
again here's our car head and you can
see how that changed we have model year
and horsepower versus model year engine
horsepower engine cylinders just
cylinders
again we want to keep reducing this so
it's more and more readable the more
readable you get it the better and of
course we can also adjust the size a
little bit
so that when it prints out instead of
splitting it on two lines we get like a
single line we can do that also that's
just your control Mouse app or plus sign
you use in Chrome that's a chrome
command
and if you remember from numpy we had
shape well pandas works the same way we
can look at the shape of the data so we
now have 11 914 rows and 10 columns so
you'll see some similarities because
pandas is built on numpy
and questions that come up just like you
did in numpy we might want to know
duplicate rows and so we can do car and
look at this switch here
um we're doing a selection this is a
panda selection with the brackets
but we want to select it based on
car.duplicated so how many duplicates on
there
so it's starting to look a little bit
different as far as how we access some
of the data in here this can be a
logical statement and we get the number
of duplicate rows we have 989 rows by 10
columns again
and this is one of those troubleshooting
things that we end up doing a lot more
than we really feel like we should we
might go ahead and do like a car count
just to see how many rows we're dealing
with and then right after that we might
want to go ahead and say hey let's drop
duplicates so remember we did all the
duplicates on there so car equals car
dot drop duplicates and then we can
print the head again we'll just do car
head here and you can see the data on
there looks the same as before
and just note that we did car equals car
draw duplicates there are commands in
here where you can do where it changes
the actual value and it works on some of
them and not on others depending on what
you're doing but by default it always
returns a copy so when we do this we're
reassigning it to car
and you can see it's the same header but
we want to go ahead and do count and see
how the count changes let's go ahead and
run this and you can see here instead of
11 914 we have 10
925. so we've removed about a hundred
cars that were duplicated just slightly
under 100 there
and then as we're prepping our data we
might want to know
um car is null so it's going to count
the values of null and then we want to
sum that up and when we do that we do
the car is null function.sum we end up
with HP the horsepower at 69 have null
values and 30 have cylinders have no
values now if you don't put the sum at
the end it's just going to return a mask
with the true false of is it null or is
it not by the zero and one so you're
summing up the ones underneath each
column
and this of course then you have to
decide what you're going to do with the
null values there's a lot of different
options it might be that you need to put
in the average or means
maybe you want to put in the median
value there's a lot of different ways to
fill it usually when you first start out
with the data a lot of let me just drop
your null values and you can see here
car dot drop in a
which is equal to all and then we're
going to go ahead and count it and you
can see that we've dropped almost
another 100 values so from 10 1925 to 10
8 27.
and maybe 75 or so values
so we cleaned that this is really a big
part of cleaning data you need to know
how to get rid of your null values or at
least count them and what to do with
them
and of course if we go back to um
counting our null values we should now
have no null values there we go and
you'll see there's zero null values
I don't know how many times I've been
running a model that doesn't take null
values and it crashes and I just sit
there and look at it trying to get why
did that crash it should have worked
it's because I forgot to remove the null
values
so even jumping around a lot we're going
to go back to finding outliers and let's
go ahead and bring that back into our
Seaborn and if you remember we did a box
plot earlier this time we're going to do
a box plot just on the price and you can
see here our price value
and we have the deviation with the two
thinner bars on each side of the main
value and then as we get up here we have
all these outliers in fact we have one
way out here that's probably a really
expensive high-end car is what we're
looking at
if you were doing fraud analysis you
would be jumping on all over these
outliers why are these deviation from
the standard what are these people doing
again this is probably like I said a
really high-end expensive car out here
that's what we're looking at and we can
also look at the box plot for the
horsepower
and we'll put that in down here
and run that
and you can see again here's our
horsepower and it just jumps and there's
these really odd huge muscle cars out
here that are outliers
and we're going to jump into making this
a little bit more as you started
displaying your data or your information
to your shareholders we're going to look
at plotting a histogram for the number
of cars per brand
and the first thing we want to go ahead
and do is we have with our car and go
back over here here we go we have our
make value counts largest plot and we
want to do a kind equals bar
fig size 10-5
and right off the bat we jump up here we
see Chevrolet it's going against what
was it it's um figure resolution the
value counts and we want the largest
value so here's our value counts and
compared to what the different cars are
Chevrolet puts out a lot of different
kinds of cars I didn't realize that they
made that many cars or different types
and then for readability let's go ahead
and add a title number of cars by make
number of cars and make if you looked at
this the first time you would have been
like well what the heck am I looking at
well we're looking at the number of cars
by make and then you can see here now
we're talking about the type of cars and
the different ones are put out Lotus I
guess only had a few different kinds of
cars over there very high-end cars
and then as uh doing data analytics and
as a data scientist one of the things I
am most interested in is the
relationship between the variables
so this is always a place to start we
want to know what's going on with our
variables and how they connect with each
other
so the first thing we're going to do is
we're going to go ahead and set a figure
size because we want to make sure it
fits our graph we'll just go ahead and
set this one plot Figure Set to figure
size 2010. if you never use the matplot
library which is sitting behind Seaborn
whatever is in the PLT this is what's
loaded it's like a canvas you're
painting on so the second you load that
Pi plot as PLT anything you do to that
is affecting everything on it
and then we want to go ahead since we're
using Seabourn
we'll go ahead and create a variable C
for relationships or correspondence
and car dot c-o-r-r that's a correlation
in Seabourn on top of pandas again one
line and you get the whole correlation
on there
and because we're working with Seabourn
let's put it into a nice heat map if
you're not familiar with heat maps that
means we're just using color as part of
our
setup so we have a nice visual
and we can see here that the Seaborn
connected to the pandas prints out a
nice chart we'll talk a little bit about
the color here in a second it prints out
a nice chart this is a chart I look at
as a data scientist these are the
numbers I want to look at and we'll just
highlight one of them here's cylinders
versus horsepower the closer to one the
higher the correlation so 0.788 pretty
high correlation between the number of
cylinders and how heavy the horsepower
is
I'm betting if you looked at the year
versus horsepower we just look at that
one here's year and horsepower 0.314 not
as so much but if you combine them you
don't actually add them but if you
combine them you'll start to see an
increase in Horsepower per year and
cylinders you could probably get a
correlation there and just like 0.78 is
a positive correlation you might notice
if we look at cylinders
and or let's look at horsepower and
mileage so if we go here to horsepower
to mileage you get a nice negative we'll
do cylinders that's a bigger number
with cylinders to the miles per gallon
it's a minus 0.6 so it's a negative
correlation the closer to -1 the more
the negative correlation is
and then the chart you would actually
show people is a nice heat map this is
all our colors and it's just those
numbers put into a heat map the darker
the color the higher the correlation you
can see straight down the middle
obviously the year correlates strictly
with the year horsepower with horsepower
and so on that's why it's a one the
closer to the one the higher the
correlation between the two pieces of
data
now this is a good introduction pandas
goes Way Beyond this most the
functionality in numpy since pandas sits
on it is also in pandas and then it even
has additional features in it and we use
Seaborn pretty extensively sitting on
top over our pie plot so keep in mind
that our PI plot has a ton of other
features in it that we didn't even touch
on in here we couldn't even if you had a
sole course in it there's just so many
things hidden in there depending on what
your domain you're working on but you
can see here here's our Seaborn and
here's our matplot library that's all
our Graphics that we did and then the
Seaborn works really nicely with the
pandas we really like that
in the next project we'll understand how
to perform data analysis using Uber data
set from New York city so let me show
you the data set first
so this is the data set that we'll be
using for our analysis you can see
this data set
has around 5 lakhs 64 516 rows
it has information about Uber bookings
in the New York City
you have the date and time column when
the booking was done
or you can see when the pickup or the
trip was made next
we have the latitude and longitude
information basically defining the
location from where the Uber was booked
the last column is the base column it
has data regarding TLC based company
code affiliated with the Uber pickup
that is the base that dispatched Uber
now let me take you straight to my
jupyter notebook where I have
implemented this project
so I have my code ready I'll run through
each cell of code and tell you what it
does
in the process we'll also learn some of
the functionalities of python 2
first
let's start by importing the libraries
so first we are importing numpy Library
as NP pandas SPD now numpy is used for
numerical computation pandas is used for
pre-processing and data manipulation
finally we have matchplotlib and c bond
for data visualization and exploratory
data analysis so let me hit shift enter
to run this cell
okay
now
let's go ahead and import our Uber data
set so it's a CSV file for that I am
using the pandas library and read
underscore CSV function and inside this
function I provided the location where
the data file is you can see this is my
location
followed by the data set name this is
the name of my data set
and the extension as well that is dot
CSV
let me hit shift enter to run this cell
now we have successfully imported a data
set
now to display the head of the data set
I'll write Uber dot head
this is the function which is used to
display the head of the data set now the
dot head function will display the top 5
rows from this data set
there you go so we have our first
five rows from the data set
now
if you want to display some additional
information about the data set
you can use info function so let me hit
shift enter to run this cell you can see
the info function gives you the total
number of rows and columns in the data
set
you can see we have 5 lakhs 64 516
entries that is rows and now we have a
total of four columns and you can see
all the columns listed here date time
latitude longitude base and we also have
the data type for each of these columns
now moving ahead
in the following cell
we'll check if there are any missing
values or null values in all the columns
for that I have defined a function
called num missing
and it has one argument that is X
I'm using the is null function to return
the total sum of null values
in the final print statement I have used
pandas library and apply method now this
apply method takes a function and
applies it to all the values of pandas
series
let's run this cell
you can see none of the columns have any
missing values it says 0 0 0 and 0 which
means there are no missing values in any
of the columns
next
there's another method to find null
values
where you can use is any function so let
me show you how to do that
I'll write Uber which is my data frame
dot is any is my function and I'll use
the sum function
let me run this
there you go it has written the same
results indicating there are no null
values or missing values
now
we are interested to extract some
additional information from the data so
I am converting the date time column
into a data frame using two underscore
date time function and I have my format
specified you can see it here
next
I am finding the day of week num which
means I want to know a particular d
false on which week number so 0 for
Monday 1 for Tuesday two for Wednesday
and so on
you can see I have used DT dot d of week
method similarly
you can also find the weekday name that
is if a particular day is a Monday or a
Tuesday or a Sunday or anything
and for that I have used DT dot weekday
underscore name
now let me extract two more information
that is the D number and the r of the
day at which the trip was made
so I'll write Uber which is my data
frame name I'll give square brackets and
inside that I pass in my column names
say t num
and that will be equal to
my data frame that is Uber
followed by
a column name that is date slash time
and I'll use the method DT dot day
similarly
to find the r
I'll write Uber
within square brackets I give my column
name r of day
that will be equal to
Uber
followed by the original column name
date slash time
and
the method I'll use is
DT dot r
let's run this
okay
now let me display the head of the data
set once again
so I'll write Uber dot head
you can see
towards the right we have our new
columns added
that did not exist in our original data
set
now you can also display the shape of
the data set now the shape will return
the total number of rows and columns
present so I'll use Uber dot shape
you can see we have 5 lakhs 64 516 rows
and eight columns now earlier if I go on
top we had four columns now there are
eight columns in total
you can also find out all the unique
base code names from where the Uber was
dispatched so let me show you how to do
that I'll write Uber
and inside square brackets I'll give my
column as base and give the unique
function
if I run this
you can see we have five different base
code names
now coming to our first visualization of
the project
here
we'll see the total count of uber trips
from each base
for that I've used the c bond library
and Cat plot function
in the x-axis I have my base column
defined
I have passed my data frame name that is
Uber
and kind I have given as count that is I
want to count the number of bookings at
each base
so let me run it
this might take some time
there you go
you can see on the screen we have a nice
bar graph here
the x-axis shows the count of trips for
each base code
so base
0 to 682 had the highest number of trips
or bookings in April 2014
and base 0 to 764 had the lowest number
of bookings
in the next cell let's explore another
feature of pandas library that is to
create a pivot table
for that I have used pivot underscore
table function
for index I am using the column names D
of week num and D of week values I have
passed my base column and the aggregate
function I have used is count
now the cell of code will create a table
to understand the total number of trips
for each day of the week
let me run this
you can see in April 2014 Wednesdays had
the highest number of trips which is
around 1 lakh 8 000 while Sundays had
the lowest 51 251
now that makes sense because Sunday is a
holiday in New York and people would
like to spend time at home
now
let's visualize the pivot table in the
form of a bar plot so
I'll write Uber underscore week
underscore data dot I'll use the plot
function
inside plot function I'll give kind
is bar so I want a bar plot and I'll
Define my figure size
and I'll give
equal to
let's say
8 comma 6.
run this cell there you go
as we saw above Wednesday had the
highest number of uber pickups while
Sunday had the lowest
similarly
let's visualize the bookings based on R
of the day so I have created a pivot
table and then I am trying to plot a
line graph
let's run this
okay
you can see the trend
at midnight the number of trips are less
it slowly increases early in the morning
and during the evening time
Uber bookings are higher so you can see
around four in the afternoon to 7 in the
evening Uber trips attain a peak and
trips gradually decrease after 8 PM in
the evening
moving ahead
let's now analyze the number of uber
bookings based on each day in the April
month of 2014
so I have created my pivot table and
then I'm trying to plot a bar graph you
can see I have kind equal to bar
let's run this
okay
we have our bar graph ready
now this bar graph depicts 30th of April
had the highest number of uber bookings
followed by 4th of April that is this
one
now I can
show you in the next two cells of code
and the current cell you are basically
counting the number of trips for each
day in April month I have grouped each
day number using the group by function
and used the apply function to count
rows let's run it
so I have all the day numbers listed and
the total number of trips that were made
for each day of the month
now let me sort the bookings for each
day in ascending order
so I'll write
by underscore date
underscore sorted
equal to
by underscore date
Dot
I'll use the function sort underscore
values
now to display I'll again use buy
underscore
date
underscore sorted
let me run it now we have arranged this
in ascending order
the results tell us that 20th of April
had the lowest number of trips while
30th of April had the highest followed
by
4th of April now you can also create a
histogram to analyze the pickups by ours
let's run this cell
I've used the hist function for this
okay
so we have our histogram ready
now this histogram is similar to
the line graph that we plotted above
this one
now let's create a cross table to
analyze Uber bookings based on day of
week num and R of T so here I have used
Group by function split function apply
as well as unstack function
let me run this
there you go so this is my cross table
ready on the top we have day of week num
that is 0 for Monday one for Tuesday
two for Wednesday
five for Saturday six for Sunday and
then here we have the r of d
now let me tell you how to read this
table for example
at 11 am
on Tuesday there were
2949 trips
if I scroll down
let's consider this one at 2 pm on
Sundays there were
2934 trips
let's move on to our final section of
the demo
here we'll create a heat map to analyze
bookings in terms of hour of day and day
of week number for this I am using
sns.heat map function
the brightest spot will tell the D or
the r with the highest frequency of
trips
let's run this
there you go
we have a nice heat map ready
the dark regions depict the frequency of
trips were less while the bright regions
saw the highest number of trips
so if I come down these are the bright
regions
so if you look at this heat map we can
conclude that
5 PM on Wednesdays at the maximum number
of trips because this region is the
brightest
and now let us focus on benefits of
fusing R why do companies extensively
use R for data analysis and why is it
chosen
firstly R is an open source programming
language which means that there is no
license required to work with r
and R does not require you to have a
coding experience which means that a
non-technical person in your team can
also learn R very easily and start
coding or building models in few lines
of codes
R can also be used with other
programming languages such as Java C
plus plus and pythons and integration of
r with other programming tools or bi
tools is very simple and easy
and various statistical models are
readily available in R which also means
that there are plenty of in-bit
libraries and packages already available
and Reporting the results of an analysis
becomes easier by using this inbuilt
packages and for creation of these
models in just simple few lines code
with this understanding of the benefits
of fusing are let us quickly hop on to
our studio and start performing the
Hands-On exercise for data analysis
for this exercise we will use a data set
named as demographics which is in a DOT
CSV file tab and firstly let us load the
data set to rstudio and we will locate
this in an variable named as demo we
also refer to this as a data frame
and now you will notice that a variable
is created in an environment section
which is in the bottom right hand side
of the rstudio window
and this particular variable comprises
of 510 observations or records with 8
variables
let us simply expand this particular
data frame and have a quick check on the
data structure and understand the data
types
this particular data frame includes
variables such as age
marital
income the unit of income is dollar per
day
education levels the car price car
category with several levels gender and
retired status
now let us view the top six records of
this particular data set
for this let's simply type head of demo
and the result is now visible in the
console section
if you are interested to view all the
records then simply type view of demo
and this new window will show you every
single record that is being loaded to
rstudio
you may also simply apply the filters
and the filters section here on
individual categorical variables
now that we have loaded the data set and
also viewed individual records let us
focus on creating subsets of Records by
applying filters on individual variables
or multiple variables
so firstly let us apply filter on gender
we will only retrieve the records with
gender is equal to female and we will
locate these records in a variable named
as demo 2.
as you notice now in the environment
section the second variable is also
created that is demo and this now is
comprising of 250 observation which
means the records are filtered down to
only gender female
next let us see how to apply a filter on
income variable let us only retrieve the
records where income is greater than
100.
let's view the result
as we see here all the records include
income greater than 100.
now let us modify this query
and we will ensure that the retrieve
records includes income greater than 100
and also specific variables are returned
let's say we only want to have
the first variable third variable and
the seventh variable returned as the
result
let's have a quick check
so we only have the first third and the
seventh variable returned
how about we only exclude
the variable 6 to 8.
for this we include a prefix of minus
sign
and now let us see what is the result we
have the variables from first to the
fifth variable however we don't have 6 7
and 8 variable
I hope it's clear so far
yes all right
now let us see how can we apply
condition by including both the
variables that is gender and income and
then we will filter the record and
create a subset of data
foreign
now let's view the result
income is greater than 100
and the gender is only female
this is one way of creating subsets
however now let us see how to use the
subset command and create the subset
let's create a subset of Records by
applying filter on marital status and
age
will only retrieve records where marital
status is equal to married and age is
greater than 35.
let's now view the result
so here we have the age greater than 35
and marital status is married
let's use the same code and this term we
will retrieve selected variables let's
say variables ranging from 1 to 3.
let's have a quick check so there are
three variables age is greater than 35
and marital status is married
now let us see how to structure the data
by sorting the data frame in ascending
and in descending order
we will apply this order function on the
variable income
firstly let us see how to order income
variable in ascending order
let's do a quick check
and here we have income in ascending
order
now let's see how to modify the same
code
and view the records
with income in descending order
so we have now the income in descending
order
how about include two variables and sort
the variables accordingly
firstly we will sort the records
by ordering income and age in ascending
order
let's quickly view the result we have
income in ascending and age as well in
ascending
let's now modify this code
this time we will order income and
descending
and age in ascending
let us view the result so the income
isn't descending and age is in ascending
order
so hope this is clear on how to solve
the data frame by ascending and
descending order per variable or by
using multiple variables with this we
will focus on learning statistical
analysis
how to perform statistical analysis on
individual variable or multiple variable
let's start by understanding the data
distribution of variable income
so that we identify what is the minimum
value of income what is the maximum what
is the range what is median what is the
mean and we will also focus on the
quantile distribution
which is also analyzed in a box plot
what is the minimum
value
in the variable income
it's nine
and what is the maximum
so we have the maximum value
now let us see what's the range
so the range shows you the result with
minimum and the maximum value
how about the difference of Maximum and
the minimum now let us focus on other
summaries of data distribution for this
variable income let's identify what is
the mean value of income
the mean is 78.
let's also understand what is the
standard deviation
all right so the standard deviation is
one one two dollar
let us see what is the variance the
variance should be larger than the
standard deviation
now let's say what is the median
absolute deviation
as you notice here the median absolute
deviation value is lower than standard
deviation why do we make this comparison
from this it is evident that median
absolute deviation is robust outliers
and standard deviation is
sensitive to outliers and also to the
change in the mean value
now let us understand the quantile
distribution this is the same analysis
that is visualized in a box plot ranging
from zero percent to hundred percent
identifying the individual data points
and we can also refer and compare this
to the Min the Max and the median values
let us quickly see what is the median
value of income
as you notice here the median is 45 as
well the 50th percent of quantile is 45
which means zero percent is minimum and
hundred percent is the maximum value
now if your question is what is 25 and
75 percent this is again used for
identifying the range of interquartile
interquartile range is nothing but the
difference of 75 percent minus the 25
percent
let's quickly see what is the IQR of
income
the iqrf income is 58. let us do a quick
check 75 percent of content is 86 and 25
percent of quantile is 28.
and the value is 58 which is equal to
the IQR result
now that we have focused on the
statistical analysis of the individual
variables data distribution let us focus
on the data visualization
in this we will have a pictorial
representation of analysis to identify
the outliers to see what is the minimum
and where do we see the data densely
populated and how is it scattered Etc
we will begin with creating a histogram
now histogram can be used for univariate
analysis which means in this scenario we
will consider income variable and we
will see how the count of income ranges
gets distributed in a histogram
for this we will have to install a
package called as ggplot2 and also call
this Library ggplot
let us install the package
now let us call the library
all right and we are ready now to begin
with visualization
for this we will use the geometric
object histogram
on the data demo data frame
let me expand this window so that the
code is visible
and also use an aesthetic mapping
for variable income this will be helpful
for filling colors or filtrations
Etc
and only include 30 bins
with individual bin size width
of 100 which means there will be 100
incomes in individual bins
let's quickly look at the distribution
of this histogram
as you notice there are couple of
outliers
the counts of these income range are
very limited
however we see the densely populated
income ranges with higher counts
between 0 to 200. dollars per day
this is also a way to identify and
segment the customers based on their
income ranges
now let us see how to change the color
of this histogram and also the border of
the histogram
for this we will include some additional
options
such as fill
filled with blue color
and the Border color is black
now as you notice here the executed code
provides us the histogram with blue
color bars and black color Border Lines
now we will focus on creating a faceted
grid facing grid is also an aesthetic
mapping object
we will see how to enable the multiple
histograms across the marital status and
the genders so that we identify how the
income is distributed for individual
maratha status as well the genders
thank you
let's Zoom this View and have a look at
it
as you notice here there are some
interesting outliers here in the data
distribution
female unmarried drawing higher income
and male unmarried and married also
drawing higher income as compared to the
females
whereas if you notice that the female
unmarried is drawing much higher income
than the male
this may also be very much related to
the age
now let us see how to create a stacked
histogram when I say a stacked histogram
I mean instead of feeling the color we
will feel the gender
so that there is a stack within the
histogram
so as you notice here I have made couple
of changes I have included fill equal to
gender within the aesthetic mapping now
let us look at this histogram as you see
here the gender is fair in the histogram
hence we have stacked distribution of
female and the male
now let us focus on creating a bar chart
with education versus income where we
can identify the education levels and
the income ranges for these education
levels
as you notice here we are going to
create a visualization where we have the
aggregation in form of mean and the
geometric object used here is bar plot
now let's Zoom this View and understand
which education level
have higher average income
so as we see here the blue color bar is
the post undergraduate degree which
means this education level draws higher
average income as compared to other
education levels
now let's create a histogram where we
will see car price and the number of
cars for individual category
let's look at this visualization
this visualization provides with some
interesting Insight just by looking at
the distribution of the car prices and
the counts of the cars at the car
category economy and even the luxury
luxury car category or car prize is
pretty much distributed whereas Economy
Car category is densed which means that
we could also look back into the income
which variables and try to figure out
further more insights and then segment
the customers for further targeting of
these customers
now what happens if we simply change
this bin width to 30.
As You observe here changing the bin
width or increasing the bin width will
also reduce the number of bins
now we only have four bins here
and the car category is filled that is
what we have enabled within the
aesthetic mapping
and we see some more interesting insight
as you look at the standard and the
luxury car category
the car prices are pretty much
overlapping
for the car category luxury and standard
this could be the starting car price of
the luxury brands
now let us create
a clustered bar chart
foreign
let's look at this visualization
in this visualization As You observe
though we have enabled fill equal to
gender in the aesthetic mapping we do
not have the view in stack form but we
have the bars one besides the other it
is also because we have enabled a
position called as position equal to
Dodge in the code
now what is the inside that we can draw
from this visualization as you see post
graduate degree with female gender is
drawing higher average income as
compared to any other education level
now let us see how to create a box plot
for variable income across the genders
so the box plot can be enabled if there
is a bivariate analysis to be performed
on a continuous variable and a
categorical variable or multiple
categorical variables with a continuous
variable
now let's look at this visualization
what does this say
we have data distribution of income
for individual genders that is for
female and the male and we also notice
outliers here
anything about this whisker is
considered to be outliers
it might make more sense if we also
include some coloring for these outliers
maybe also enable shape
thank you
so now we have colored the outliers and
its colored Orange
let's see if we can also enable the
shapes
and now we have here the outlier color
as well the shape enabled
now let us see how to enable a violin
plot
what is the utility of violin plot with
the box plot we understand the analysis
and the distribution of the data points
is to identify the outliers to know what
is the minimum value what is the max
what is the median and what are the
outliers
but what is the purpose of a violin plot
let us have a quick check
As You observe there is some
concentration of data points in the
bottom of every car category
however the concentration is higher for
standard car category as compared to
economy and the luxury
now this is an interesting Insight that
you wouldn't have come across in box
plot the box plot is a very good
representation for identifying outliers
however violin plot will help you focus
on the nuances which is not captured by
the box plot we can also simply combine
the box plot and the violin plot
together
simply include this jaw object
let's Zoom this now you have a
representation of box plot and the
violin plot both combined in a single
visualization
interestingly you notice the outliers as
well the concentration in the bottom of
this violin plot so this could be some
interesting insights that you draw and
focus on these data points and
understand what exactly is happening
there
now let's focus on the density plot that
is density estimate of the histograms
rather than just viewing the frequencies
now we see the frequency in the y-axis
across the income distributions
powerboard enabling the probability as
true so that we enable the density
instead of the frequency
so now we have the density in the y-axis
and in the x-axis we still have the
income distribution
this is the way of also adding a line
plot which is a density plot on the
histogram now as You observe here the
density plot is not in the same level as
the bar so let us adjust this line
for this we will include
adjust
let's say equal to 3 and now let us see
how the visualization appears now the
density plot is on the same level as the
bar
now let us see how to create a cross
table
for car category
and gender
for this let us call the library
d-e-s-cr
now let us create the visualization
enabling cross table for car category
and
gender
let's look at the result in console as
you see here now we see the counts of
the gender for Individual Car category
the values over here represents that
they're of 67 females
falling within the car category economy
and 80 males within the car category
economy
and for luxury we see that the count of
female is higher than the male as well
the proportions now how do you
understand what proportions are
presented here
we may simply turn off some of the
proportions like the t-test the
chi-square Etc let us see how to enable
that
now let's look at the result this looks
better
now that we have the counts the female
counts and the male counts across
Individual Car category we also see the
percentages rather than just looking at
the absolute value so there are
45.6 percentage of female within the car
category economy and 54.4 percentage of
mail within the car category economy
similarly across rest of the car
categories this kind of cross table or a
contingency table is also helpful when
you want to analyze the different
categorical variables and identify the
counts or the proportions now let us see
how to use a scatter plot of age versus
income scatter plot is a visualization
used for bivariate analysis when you
want to perform some analysis between
two continuous variable at a data point
level rather than performing the
analysis at an aggregated level such as
sum or mean
and now we have a scatter plot of age
versus the income age in the x-axis and
income in the y-axis
though we did not see any kind of a
positive correlation or a negative
correlation but we still see some
interesting insights over here some of
the data points are pretty much
scattered and much away from densely
populated data points
I hope the learning has been informative
and interesting so far we have covered
the concepts of data analytics as well
we have performed some Hands-On doing
some statistical analysis and also
creating interesting visualization
hi everyone
welcome to this section of our
programming let's learn about data
manipulation in R and here we will learn
about d-plier package and when we talk
about this D player package it is much
faster and much easier to read than base
R so D player package is used to
transform and summarize tabular data
with rows and columns you might be
working on a data frame or you might be
getting in a inbuilt or data set which
can then be converted into a data frame
so we can get this package deployer by
just calling in library function and
this can be used for grouping by data
summarizing the data adding new
variables selecting different set of
columns filtering our data sets sorting
it selecting it arranging it or even
mutating that is basically creating new
columns using functions on existing
variables so let's see how we work with
d player now here I can basically get
the package here so I can just say
install dot packagesdp player now we
already see the the package here which
is showing up so I will just select this
one I can do a control enter and that
will basically set up the package
package deployer successfully unpacked
so that is done now you can start using
this package by just doing a library D
player and this was built it shows me my
version of R so let's also use a inbuilt
data set that is New York flights 13 so
we can do install.packages and that will
search and get that relevant data set
I can again Call It by using Library
function now once that is done we can
look at some sample data here by just
doing view flights and that shows me the
data in a neat and a tabular format
which shows me year month day departure
time schedule departure time and so on
now we can also do a head to look at
some initial data which can help us in
understanding the data better
so what is this data about how many
columns we have what are the data types
or object types here it shows me how
many variables we have so this is fine
now we can start using deployer and in
that we can use say filter function if
we would want to look in for specific
value now here we have the column as
month so I will do a filter now I'm
creating a variable F1 I'm using the
filter function on flights which we
already have
and then what we can do is we can
basically look at the month where the
month value is 0 7 so let's look at that
and this one you can do a view on F1
which shows me the data wherein you have
filtered out all the data based on month
being seven
so this is a simple usage of filter we
can take some other example we may want
to include multiple columns so we can
say F2 filter flights and here we will
say month is equal to 7 day is 3 and
then look at the value of F2 if you are
interested in seeing this
and that tells you the month 7 and days
three you could also look into a more
readable format by using view on F2 and
that gives me my selected result so we
are just extracting in some specific
value we can keep extending this so here
we can say flights is what we would want
to work on I'm using the filter function
so I can straight away
instead of creating a variable then then
doing a view I can also do a view in
this way I can just pass in my filter
within the view and within this I am
saying filter I would want to look at
the flights month being 0 9 day being 2
and origin being LGA and then that shows
me the value here and obviously you can
scroll and look at all the columns and
if you see the origin column it shows
the selected value so now we have
filtered out our data based on values in
three different columns now what we can
also do is we can use and or we can use
or operators so I could have done this
in a a little different way so I would
have said head which shows me initial
result I will do a flight So within my
head function I am passing in this
and what does that contain so you are
saying flights and in this flight's data
set you would want to pick up the month
being the column so we use the dollar
symbol here we're given a value and I'll
say and and I'll again say flights
wherein I will select the day being two
and and remember when you talk about and
it is going to check if all the values
are met true so then you say flights
origin LGA and you look at the value so
in this way I can filter out
specifically multiple values by
specifying columns now we could have
done it in this way we could have
created a view or we could have assigned
this to a variable and then done a view
on that where we could have selected
month being day and origin or you can be
more
specific in specifying all the columns
it makes the code more readable so let's
look at the values and here you are
looking at head which shows me based on
month day and then you can look for
further columns for other variables that
is origin being LGA now what we can also
do is we can do some slicing here to
select rows by particular position so I
can say slice and I would want to look
at rows 1 to 5 and I can do this so you
can always assign or look at the view of
this I can just do here so when I did a
slide one is to 5 it shows me my entries
for one two five now similarly we can do
is slice 5 to 10 and now you are looking
at 5 to 10 values
so you can always look at the complete
data and then you can slice out
particular data now mutate is usually a
function which is used when you would
want to apply some variable on a
particular data set and then you would
want to add it to
your existing data frame or you would
want to add a new column so this is
where you use mutate which is mainly
used to add new variables so let's see
how you work on mutate so it's pretty
simple so you create a variable over
delay now I would want to do a mutate so
that it adds a new column so I'm
selecting my data which is flights I
will call the new column as overall
delay and then basically
I can look at
overall delay being arrival delay minus
departure DeLay So let's create this and
let's look at view of this which shows
me
or which should show me my new column
which is overall delay which was not in
my original data set so you can anytime
do a head on this one to compare the
value so this one shows me arrival delay
and then there are many other variables
what you can also do is you can do a
view and you could have just look at
flights if you would want to compare so
you can look at the flights and this one
would not have any
overall delay column so it basically
shows me 19 columns only
what we see here and if you
do a view on overall delay then that
basically shows me 20 columns so we know
that the new column has been added
to
this overall DeLay So if you would want
to work with 20 columns you will use
overall delay if you would want to work
with your original data set you will use
flights now you can also use a transmute
function which is used to show only the
new column so we can do an overall delay
and at this time we will say transmute
we will say flights overall delay the
computation remains same but at this
time if I look at view on overall delay
it only shows me the new column so
sometimes we may want to compute result
based on two variables or two columns
and just look at the new value and then
we can decide if we would want to add it
to our existing structure now you can
also use summarize and summarize
basically helps us in getting a summary
based on certain criteria so we can
always do a summarize
and what we can do is we can look at our
data and we can say on what basis we
would want to summarize this particular
data so we can do a summarize function
now summarize on flights I will say
average airtime and I would want to
calculate an average so for that I'm
using inbuilt function called mean I
will do that on airtime column
so
let's look at flights once again and
here we can see there is
arrival time not a time sorry arrival
time and we would want to do some
average on this particular data we would
want to summarize this so what I'll do
is I will use the summarize function I
will say average air time and this one I
will look at mean of a time so let's see
if there is a airtime column I might be
let's look at this one and I will delay
and yes we have an airtime so we were
actually looking at summarizing based on
airtime not the arrival time so air time
is how much time it takes in air for
this particular flight and we will want
to use the trans summarize function not
the transmute so summarize flights
average airtime and this one we will
calculate the mean of average a time and
I will also do a n a removal which is I
am saying true so let's do this and that
basically shows me the average airtime
is 151 I can also do a total a time
where I am doing a summation of values
or I can get the standard deviation or I
can basically get multiple values such
as mean
I can say total air time where I am
doing a summation and then I can look at
other values which is if you would want
to put in standard deviation here you
could do that so let's look at the
result of this summarize and this
basically allows me to get some useful
information which is summarized based on
a particular function such as mean sum
standard deviation or
all three of them now
let's look at grouping by so sometimes
we may be interested in summarizing the
data by groups and that's where we use
the group by function so we can always
use the group by Clause now here we are
taking a different data set so we will
say for example let's look at head of Mt
cars and that is basically my data set
on empty cars now that shows me the
model of the car it shows me mileage
cylinder power this and your horsepower
and various other characteristics or
variables in this particular data set so
here we can say let's do a grouping by
gear so there is a column called gear so
I will call it by gear I will look at my
data set and then what I'm using here
which you see with these percentage and
greater symbol is called
piping so that basically feeds your
previous data frame into next one so
this is sometimes useful and you can get
this by just saying Ctrl shift and M and
you can then use this so we are going to
have piping so I am saying empty cars
now this is my original data set where I
did a head or I could have done a view
on this one if you would want to see it
in a more readable format and that
basically shows me the data so we are
using a different data set so I want to
group it by the gear column so I'm going
to call it by gear and this one takes my
data that is empty cars I'm using the
piping and then I am saying group the
data based on gear column that's done
now let's look at the value of by gear
or you can always do a view so remember
whenever you're doing a group by it is
giving you a internal object where your
data is grouped based on a particular
column
so we can look at the values here you
can do a view that shows you your data
grouped based on a particular column now
I can again use the summarize function
where I would want to Now work on the
new one where it was grouped based on
gear so I'm doing a summarize and here I
am going to say Gear 1 which will be
having the value of summation on the
gear column and then I'm saying Gear 2
which is mean well you could give some
meaningful names to this and let's look
at the value of this one
where we are basically now looking at
the values which is sum and mean values
based on the gear
similarly we can use look at different
example so we can say by gear and I'm
again using piping
but earlier we had taken gear we had
grouped the data and we called it by
gear so we took our original data set Mt
cars but now within this particular data
which was grouped by gear I will take
this data set I will use the piping and
I will summarize it where I am saying
within this particular data set I would
want to get the sum or I would want to
get the mean and then you can look at
the values so what you are doing is you
are either looking at your original data
set or you are looking at the data which
was already grouped and then you can
look at the values now here what we can
do is we can Group by cylinder say might
be you are interested in looking at data
which is summarized based on the
cylinder column you can do that and then
for this buy cylinder I am doing a
piping where I am using the summarize
function and summarizing will then be
done based on the mean values of the
gear column or the horsepower so let's
do this
and then you can basically look at the
value at any point you may want to look
at the data set again so just go ahead
and you can look at what does the value
contain
and
by cylinder or by gear and do a head and
it gives you the value so you can always
do some summarizing or grouping in these
ways now here we are going to use sample
underscore n function and Sample
underscore
fraction for creating samples so for
this let's take the flights data set
again and we would want to get 15 random
values now that is done and it shows me
15 rows with some random values from the
data what you can also do is you can do
a portion of data by using sample
underscore fraction and here I'll say
flights I'll say 0.4 which will return
40 percent of the total data so this can
be useful when you are building your
machine learning where you would want to
split your data into training and test
might be you are interested in some
portion of the data so you can do this
which is very useful function and then
you can look at the value of that now
what we can also do is we can use a
range function so like we were doing a
grouping by or we were trying to pull
out a particular column so in the same
way we can use a range which is a
convenient way of sorting then your base
are sorting so for range function let's
do a view based on a range so we will
work on the flights data set which we
have and here what we would want to do
is we would want to arrange the flights
data set which is based on year and
departure time and we are doing a view
out of it so that basically gives me the
data which is arranged based on your
year and departure time now I can do a
head to give me some highlighting of
that data now the piping operator what
we are using can be used in these ways
also so here I will say DF I will just
assign the data set empty cars to it
let's look at the DF which has basically
your different models you can obviously
look at the head or view of it to look
at useful information we can also go for
nesting options which can be useful so
we are creating a variable called result
here now that has the arrange function
so what does this arrange function do so
when we would want to use arrange to
sort the data so I would want to sort
the data but what data would I sort so I
will use sample n which will give me
some portion of the data or some sample
data now what is that sample data so
here we are using nestling that is
earlier when we did a sample we just
said data and how many random samples we
want but instead of giving that what we
are going to do is we are going to use
filter here now this filter will work on
DF
so filtering will happen based on the
mileage which is greater than 20 I will
say size is 5 and I would want to
basically arrange this in a descending
order so I am using the Des on this
particular mileage column by default it
is always ascending so let's get the
result out of this
which will basically show me the mileage
details in a descending order so this is
my data frame and now we can look at the
result what we have created
so just do a view or do a head and look
at the view so here you see mileage
where the highest value is on the top
and we were only interested in five
values in a random sample so that's why
when you did a view it shows your five
values and it shows in a descending
order based on mileage so we have not
only used an inbuilt function we have
not only arranged the data that is we
have sorted the data but we have sorted
the data based on a descending order on
a particular column we have said the
value should be greater than 20 and we
have also said we just need five random
samples
now let's look at some other examples so
you can always do a multi assignment so
I can say filter
wherein I'm going to use DF which was
assigned Mt cars I am going to say
mileage should be greater than 20 then I
say B which is going to get a sample out
of a and I just want five random values
so let's look at that so we have B which
is
going to get a
set of five values from a now I will
create a result variable which will
arrange B which is sample data in a
descending order now let's look at the
result of this and that basically shows
me what we were seeing earlier so you
can do a multi assignment where you can
create a variable get a sample out of it
and then basically whatever is that
result you can arrange that or sort that
in a descending or by default ascending
order
so same thing we can do it using pipe
operator so piping so here I will say
result I am passing in my DF that's the
data set I'm using piping and which
basically tells what you need to do on
this particular data set so I'm going to
filter out the data based on mileage 50
sorry mileage 20 then I'm going to push
that or forward it to
get the random sample and whatever is
this random sample is going to be pushed
so you are arranging this in a
descending order so this is one more way
of doing it and then basically you can
look at the result so these are some
simple examples where you can use your D
plier with multiple assignments or using
your nesting to filter out the data you
can also do a arrange which is to sort
the data you can get some random samples
out of it you can summarize the data you
can also summarize the data based on one
or two or multiple columns and you can
use some inbuilt functions to summarize
the data based on some functions which
are applied on the variables or on the
columns
you can transmute it where you would be
interested in only looking at one column
you can mutate it where you want to add
a new column
you can slice it
and you can give the conditions where
you can say and on or to filter out the
data so what we can also do is on this
particular data set which we have say
for example DF where I have my data
let's look at this one and if I just do
a DF at this point it shows me my data
set and if you would be interested only
in particular column then your D player
also allows you to either we can do a
filter or we can simply do a select
now for selecting we can choose
our data so for example I'll say DF
underscore I'm interested in mileage I'm
interested in Horsepower might be I am
interested in your cylinders in this and
for this one what I can do is when I
would want to do a select I can
basically say
selected DF let's call it some name I
can say control shift M which is for
piping and then basically what you can
do is you can do a select and you can
choose your columns so I was interested
in mileage I was interested in
horsepower
I was interested in cylinder and here
what I'm doing is I'm using a select
where I can look at the new data frame
so let's do this
and I'm sorry here we will have to give
it DF this is where you are passing in
your data
yeah now this one is done and we can
look at the value of this one by just
doing a DF or head on DF
underscore mileage horsepower cylinder
and look at the selected result so you
can be looking at selective columns I
could have done this filter but filter
will always look for
a condition say your mileage is greater
than 20 or might be your cylinders are
more than four or something else but
when you do a select you are selecting
specific columns so view always gives
you all the columns head gives you
highlight but then select can be useful
when we are interested in looking at
only specific data so this is how you
can use the player for manipulation
for your data transformation for
basically filtering out the data by
selecting particular data and then
working on it so similarly there is one
more package called tider and we'll see
how we can use data manipulation done
using your tider package let's learn
about that IDR package which makes it
easy to tidy your data and this
basically helps you creating a more
cleaner data so which is easy to
visualize and model now this comes with
mainly four functions so you have gather
which makes your data wide or it makes
wide data longer so that is basically
used to stack up multiple columns you
have spread function which makes long
data wider that is stacking the data
together or stack if you would want to
unstack the data to data
and you are talking about data which has
same attributes
and then your spread can spread the data
across multiple columns you have
separate which is function with splits
single column into multiple columns and
to complement that you have one more
function which is unite and that
combines multiple columns into single
columns so these are four main functions
which are used in your tider package so
let's look how we work with this so let
me bring up my R Studio here now for
this first is let me just clean up my
screen here doing a control l so I will
install the package it is already
installed but we can just do a control
enter and then
I can say do you want to restart R prior
to reinstall Store install I'll say okay
and it is basically going to get the
package
now it says package did
tidyr that is idrs has been successfully
unpacked let's use that package using
our library function
and that was built under R version 3.6
now I can basically start using these
functions so for example here we are
creating a data frame so let's say n is
10
and then we basically would say we will
call it white now that's the variable
name I'm using the data dot frame
function I am saying ID which will be 1
to n so that will take the values from 1
to 10 and then these are the values
which have
10 entries so this is a vector Phase 1
phase two phase three let's create a
data frame out of it now that's done we
can have a look at our data frame by
just doing a view wide and that shows me
the ID column and it has phase dot 1
phase.2 and phase.3 now we can use our
function so for example we can work with
gather that is reshaping the data from
wide format to Long format and basically
you can say stacking up multiple columns
so let's see how we do that here I'll
call it long I am working on White I am
using the piping
functionality and then I'm using gather
so this one I will say what will be the
data which I will use so we are using
wide as a data frame
then I am saying response time so that
will be basically one more column and
then you have your columns which you
would want to
basically stack so I'm saying from Phase
1 to phase three so let's do this and
once this is done let's have a look at
our variable long so this one shows me
that I have an ID column
I have the response time column and I
have the face column which we mentioned
and that basically has all the values
stacked in so you have phase.1 phase.2
and phase.3 so I have all the columns
are being stacked here so all my data so
now I have totally 30 entries in this
one so this is basically using your
gather function now sometimes we may
want to use a separate function now
separate function is basically splitting
a single column into multiple columns so
which we would want to use when multiple
variables are captured in a single
variable column okay so let's look at an
example of this one so let's say long
separate that's what we will call we
will work on this long which has all the
data stacked in as the calls selected
then I am saying separate I want the
face column and then I would say when I
separate the columns what are my column
names now I could also also give a
separator by giving a comma and then
mentioning the separator if that is
required so let's do this
now once this is done let's have a look
at our long separate so what we see here
is the column which we used so we were
doing a face column and that was to be
split and we wanted to split it into
Target and number so that's what we see
here so you have face being split into
Target and number and then you have the
response time so this is how you use a
separate function now there is also
something called as unite function which
is basically a complementing of separate
function so it takes multiple columns
and combines the elements to a single
column so for example here we will call
it long Unite and we will take long
separate which was separating the data
we want to unite so we will take phase
Target number and we want to have a
separator between them so let's
basically do this and now let's look at
the result of this unite so you see you
have the face and Target merge together
so you have phase dot one the separator
is dot as we have mentioned and we have
United multiple columns so this is one
more function of your tider which helps
you basically
uh tidy up your data or put it in a
particular way now then you have your
spread function and this is basically
for unstacking if you have if you would
want to convert a stack to data or if
you would want to unstack the data which
is of same attributes spread can be used
so that you can spread the data across
multiple columns so it will take two
columns say key and value and spread it
into multiple columns so it makes long
data wider so we can look at this one we
will say long unite I'm using the piping
I will use the spread function I'll work
on the face column and response time and
let's do this and then let's do a view
on this so it tells me our data is back
in the shape as it was in the beginning
so these are four functions which are
very helpful when we work with tider
package welcome to this video tutorial
on introduction which SQL by simply loan
in this session we are going to learn
about databases how data is stored in
relational databases and we'll also look
at some of the popular databases finally
we'll understand various SQL commands on
MySQL server now let's get started with
what is the database
so according to Oracle a database is an
organized collection of structured
information or data that is typically
stored electronically in a computer
system
a database is usually controlled by a
database management system or dbms
so it is a storage system that has a
collection of data relational databases
stored data in the form of tables that
can be easily retrieved managed and
updated
you can organize data into tables rows
columns and index it to make it easier
to find relevant information now talking
about some of the popular databases
we have mySQL database we also have
Oracle database then we have mongodb
which is a nosql database
next we have Microsoft SQL Server next
we have Apache Cassandra which is a free
and open source nosql database and
finally we have postgres SQL now let's
learn
what is SQL so SQL is a domain specific
language to communicate with databases
SQL was initially developed by IBM
most databases use structured query
language or SQL for writing and querying
data SQL commands help you to store
process analyze and manipulate databases
with this let's look at what a table is
so this is how a table in a database
looks like
so here you can see
the name of the table is players
on the top you can see the column names
so we have the player ID the player name
the country to which the player belongs
to and we also have the goals scored by
each of the players so these are also
known as fields in a database
here each row represents a record or a
couple
so if you have the player ID which is
one zero three here the name of the
player is Daniel he's from England and
the number of goals here score is seven
so you can use SQL commands to query
update insert records and do a lot of
other tasks there are mainly four types
of SQL commands so first we have data
definition language or ddl
so ddl commands change structure of the
table like creating a table deleting a
table or altering a table all the
commands of ddl are Auto committed which
means it permanently save all the
changes in the database
we have create alter drop and turn
create as ddl commands
next we have data manipulation language
or DML for so DML commands are used to
modify your database it is responsible
for all forms of changes in the database
DML commands are not Auto committed
which means it can't permanently save
all the changes in the database
we have select update delete and insert
as DML commands now select command is
also referred to as dql or data query
language third we have data control
language or DCL
so DCL commands allow you to control
access to data within the database
these DCL commands are normally used to
create objects related to user access
and also control the distribution of
privileges among users
so we have Grant and revoke which are
the examples of data control language
finally we have something called as
transaction control language or TCL
so TCL commands allow the user to manage
database transactions commit and roll
back are example of TCL
now let's see the basic SQL command
structure
so first we have the select statement so
here you specify the various column
names that you want to fetch from the
table we write the table name using the
from statement
next we have the WHERE Clause to filter
out a table based on some conditions so
you can see here where condition 1
condition 2 and so on
then we have the group by Clause that
takes various column names so you can
write Group by column one column two and
so on
next we have the having Clause to filter
out tables based on groups
finally we have the order by Clause to
filter out the result in ascending or
descending order now talking about the
various data types in SQL
so we have exact numeric which has
integer small end bit and decimal then
we have approximate numeric which are
float and real
then we have some date and time data
type such as date time timestamp and
others
then we have string data type which
includes car this war car and text
finally we have binary data types
and binary data types have binary VAR
binary and image now let's see some of
the various operators that are present
in SQL
so first we have our basic arithmetic
operator so you have addition the
subtraction multiplication division and
modulus
then we have some logical operators like
all and any or between exists and so on
finally we have some comparison
operators such as equal to not equal to
this greater than less than greater than
equal to or less than equal to not less
than or not greater than now let me take
you to my MySQL workbench where we will
learn to write some of the import and
SQL commands use different statements
functions data types and operators that
we just learned
let me now go ahead and open my MySQL
workbench so in the search bar I'll
search for
MySQL workbench you can see I'm using
the 8.0 version
I'll click on it and
here it says welcome to mySQL workbench
and Below under connections you can see
I have already created a connection
which says local instance then you have
the root the localhost and the port
number let me click on it
you can see the service the username is
root and I'll enter my password
and hit ok now this will open the SQL
editor so this is how the MySQL
workbench looks like here we learn some
of the basic SQL commands so first let
me show you the databases that are
already present
so the command is so databases
you can hit tab to autocomplete I'll use
a semicolon
I'll select this and here you on the top
you can see the execute button so if I
run this
below you can see the output it says
showed seven rows are returned which
means
currently there are seven databases
you can see the names
all right now
let's say I want to see the tables that
are present inside this database called
world
so I'll use the command use
World which is the database name
now let me run it so currently I'm using
the world database so to display the
tables that are present in the world
database I can use the show command and
write so tables
give a semicolon and I'll hit Ctrl enter
this time to run it all right so you can
see the tables that are present inside
this
World Dairy tables in total City Country
and Country language
now if you are to see the rows that are
present in one of the tables you can use
the select command
so I'll write select star which
basically means I want to display all
the columns so star here means to
display all the columns
then I'll write my
from
the table name that is City
so this command is going to disclose
that are present inside the city table
so if I hit Ctrl enter
all right you can see the message here
it says
1000 rows were written which means there
were total thousand records present
inside the city table
so here you can see there's an ID column
A neem column this country code district
and population
all right
similarly you can check the structure of
the tea body describe command so I'll
write describe
and then
I'll give the
table name that is City now let's just
run it
there you go the field shows the column
name so we have ID name country code
district population
type here shows the data type of each of
the columns so district is character 20
ID is an integer population is also
integer
null says yes or no which means if no
then there are no null values if it's
yes which means there are null values in
your table
key here represents whether you have any
primary key or foreign key
and these are some extra information now
let's learn how to create a table in
MySQL so I'll use the create table
command for this and before that
let me create a database
and I'll name it as SQL intro so the
command is create
database
and I'll give my database name that is
SQL underscore in true
let me keep a semicolon and hit Ctrl
enter
so you can see I have created a new
database now
if I run this command that is show
databases
you can see this
newly created database that is SQL intro
if I scroll down
there you go you can see the name here
SQL intro
okay now within this database we'll
create a table called employee details
now this will have the details of some
employees
so let me first show you how to create a
table
that will be present inside the SQL
intro database
so use the command create
table
and then
I'll give my table name that is going to
be employee underscore details
next
the syntax is
to give the column names so my first
column would be the name column which is
basically the employee name
followed by
the data type for this column since name
is a text column so I'll use War car
and I'll give a value of 25. so it can
hold only 25 characters
okay next I also want the age of the
employee now age is always an integer so
I'll give int
okay
then
we can have the gender of the employee
so gender can be represented as F or m f
for female and M for male so I'm using
the car data type or character data type
and I'll give the value as 1
then let's have the
date of join or doj and this is going to
be of data type date
all right
next
we'll have the city name that is the
city to which the employee belongs to
so again this is going to be war card
15.
finally we'll have a salary column
and salary will keep it as float since
salary can be in
decimal numbers as well now I'll give a
semicolon all right so let me just
quickly run through it so first
I wrote my create command then the table
which is also a keyword
followed by
the table name which is employee details
here and then we keep the column names
such as name each this gender
need to join City and salary
for each of the columns we also give the
data type
all right so let me just run it
okay so here you can see we have
successfully created our first table
now you can use the describe command to
see the structure of the table
I'll describe EMP underscore details
if I run this
there you go
so under field you can see the column
names then you have the data types
null represents if the table can accept
null values or not and these are
basically empty
and we haven't set any default
constraint
all right
moving ahead
now let's learn to add data to our table
using the insert command so on a notepad
I have already
written my insert statement let me just
copy it and then I'll explain it one by
one all right
so if you see this
so we have used an insert into statement
or a command followed by
the table name that is EMP details then
this is the syntax using values I have
passed in all the records so first we
have Jimmy which is the name of the
employee then we have 35 it basically
represents the age then m means the
gender of the sex
then we have the date of join next we
have the city to which the employee
belongs to and finally we have the
salary of the employee so this
particular information represents one
record or a tuple
similarly the next employee we have is
Shane you can see the age and other
information then we have Mary this
Dwayne Sarah and Ami all right so let me
go ahead and run this
so this will help you insert
the values in the table that you have
created
you can see we have successfully
inserted six records
now
to display the records let me
use the select statement so I'm using
select star from EMP underscore details
if I run this
you can see my table here and the values
it has so we have the name column the
each column the state of join City
salary
and these are the values that you can
see here
moving ahead now let's say you want to
see the Unique city names present in the
table
so in this case you can use the distinct
keyword along with the column name in
the select statement so let me show you
how you can print the distinct
city names that are present in our table
now if you notice this table clearly
we have Chicago sheertle Boston Austin
this New York and this Seattle repeated
again so I only want to print the unique
values
so for that I can write my select
statement as select
distinct
then I'll give my column name which is
City from
my table name that is EMP details
if I run this
you can see
my query has returned five rows and
these are the values so we have Chicago
Seattle which was repeated twice is just
been shown once then we have Boston
Austin and New York if getting your
learning started is half the battle what
if you could do that for free visit
skillup by simply learn click on the
link in the description to know more
now let's see how you can use inbuilt
aggregate functions in SQL
so suppose you want to count the number
of employees in the table so in that
case you can use the count function in
the select statement so let me show you
how to do that
so I'll write
select
I'll use my function name that is count
now since I want to know the
total number of employees I am going to
use their name
inside the brackets from
employee underscore details
now if I run this
this will return the total number of
employees that are present in the table
so we have six employees in total
now if you see here in the result it
says count name now this column is
actually not readable at all so what SQL
provides is something called as an alias
name so you can give an alias to the
resultant output so here I can write
select count of name
and use an alias as as
I can give an alias as
count underscore name
and run this statement again
there you go you can see here in the
resultant
output we have the column name as count
name which was our Alias name now
suppose we want to get the total sum of
salaries you can use another aggregate
function called sum
so I'll write my
select statement and this time instead
of count I am going to write sum and
since I want to find the sum of salaries
so inside the bracket I'll give my
salary column from
my table name that is employee details
if I run this
this will result in the total sum of
salaries
so basically it adds up all the salaries
that were present in the salary column
now let's say you want to find the
average salary so instead of sum you can
write the average function which is AVG
so this will give you the average salary
from the column salary so you can see it
here this is average salary now if you
want you can give an alias name to this
as well now you can select specific
columns from the table by using the
column names in the select statement
so initially we were selecting all the
columns for example like you saw here
the star represents that we want to see
all the columns from the employee
details table
now suppose you want to
see only specific columns you can
mention those column names in the select
statement
so let's say I want to select
just the name age
and the city column
from my table that is employee details
so this will result
in displaying only the name agent City
column from the table if I run it there
you go it has given only three columns
to me
now SQL has a where Clause to filter
rows based on a particular condition
so if you want to filter your table
based on specific conditions you can use
where Clause now where Clause comes
after you give your table name
so suppose you want to find the
employees with age greater than 30 in
this case you can use a where Clause so
let me show you how to do it
I'll write select
star from
my table name that is
employee details
and after this I'll use my where Clause
so I'll write where
each
greater than
30.
if I run this
it will give me the output where the age
is only greater than 30 so it excluded
everything that is less than 30. so we
have four employees whose age is greater
than 30 here
now suppose you want to find only female
employees from the table you can also
use a where Clause here
so I'll write select let's say I want
only the name the gender which is sex
here
comma City
from my table that is employee details
where
I'll give my column name that is 6 is
equal to
since I want only the female employees
I'll give
f
and run this statement
okay you can see here our employee table
has three female employees now suppose
you want to find the details of the
employees who belong to Chicago or
Austin in this case you can use the or
operator
now the or operator in SQL displays a
record if any of the condition separated
by or is true
so let me show you what I mean
so since I want the employees who are
from Chicago and Austin
I can use an or operator so I'll write
select star from
EMP details which is my table name
then I'll give my where Clause where
City
equal to
I'll give my
city name as Chicago
and then I'm going to use the or
operator or
City equal to
I'll write Austin
I'll give a semicolon and let me run it
there you go so in the output you can
see all the employees
who belong to the city Chicago and
Austin
now that is another way to write the
same SQL query
so you can use an in operator to specify
multiple conditions
so let me just copy this
and instead of using the or operator
this time I am going to use the in
operator
so I'll delete this
after the where Clause I'm going to
write where City
and use the in operator
inside bracket
I'll give my city names as Chicago
and I want Austin so I'll give a comma
and write my next city name that is
Austin
so this query is exactly the same that
we wrote on top
let me run this
you will get the same output there you
go
so we have Jimmy and Dwayne who are from
Chicago and Austin respectively
now SQL provides the between operator
that selects values within a given range
the values can be numbers text or dates
so suppose you want to find the
employees whose date of join was between
1st of Jan 2000 and 31st of December
2010.
so let me show you how to do it
I'll write select star from
EMP details where
my date of join that is doj
between
I'll give my two date values that is
first of
Jan 2000
and
I'll give my
second value
the date value that is
31st of December 2010.
so every employee who has joined between
these two dates will be displayed in the
output
if I run it
we have two employees who had joined
between 2000 and 2010 so we have Jimmy
and Mary here who had joined in 2005 and
2009 respectively
all right
now in where Clause you can use the and
operator to specify multiple conditions
now the and operator displays a record
if all the conditions separated by and
are true
so let me show you an example I'll write
select
star from
employee details table
where
I want the age to be greater than 30.
and I want
sex to be male
all right so here you can see I have
specified two conditions so both the
conditions are true only then it will
result in an output
if I run it you can see there are two
employees who are male
under each is greater than 30. now let's
talk about the group by statement in SQL
so the group by statement groups rows
that have the same values into summary
rows like for example you want to find
the average salary of customers in each
department
now the group by statement is often used
with aggregate functions such as count
sum and average to group the results set
into one or more columns
let's say we want to find the total
salary of employees based on the gender
so in this case you can use the group by
clause
so I'll write select
let's say sex comma
I want to find the total sum of salary
as I'll give an alias name let's see
total salary
from
my table name that is employee details
next I am going to group it by sex
okay
let me run it there you go so we have
two genders male and female and here you
can see the total salary so what this
SQL statement did was first it grouped
all the employees based on the gender
and then it found the total salary
now SQL provides the order by keyword to
sort the result set in ascending or
descending order
now the order by keyword sorts the
records in ascending order by default to
sort the records in descending order you
can use the desc keyword
so let's say I want to sort my employee
details table
in terms of salary so I'll write select
star from
EMP underscore details
and I'll use my order by clause on the
salary column
so this will sort
all the records
in ascending order of their salary which
is by default you can see
the salary column is sorted in ascending
order
now suppose you want to sort the salary
column
and display it in descending order you
can use this keyword that is desc
let me run it
you can see the output now this time the
salary is sorted in descending order and
you have the other values as well now
let me show you some basic operations
that you can do using the select
statement
so suppose I write select
and do an addition operation let's say
10
plus 20
and I'll give an alias name as addition
if I run this
it will give me the sum of
10 and 20 that is 30.
similarly
you can use
the subtraction operator
and you can change the Alias name as
let's say subtract
let's run it
you get minus 10. now there are some
basic inbuilt functions there are a lot
of inbuilt functions in SQL but here
I'll show you a few
suppose you want to find the length of a
text or a string you can use the length
function
so I'll write select and then use the
length function I'll hit tab to auto
complete let's say I want to find the
length of country India
and I'll give an alias as total length
if I run it
you see here it returns 5 because there
are five letters
in India
there is another function called repeat
so let me show you how repeat works
so I'll write select
repeat
let's say I want to repeat the symbol
that is at the rate
I'll put it in single quotes because it
is a text character and I want to repeat
this character for 10 times
close the bracket
and let's run it
you can see here in the output it has
printed at the rate 10 times you can
count it
all right
now let's say you want to convert a text
or a string to uppercase or lowercase
you can do that as well
so I'll write select and use the
function called upper
let's say I want to convert
my string that is India to upper case
I'm not giving in any Alias name
if I run this
see my input was capital I and
everything else was in small letter
in the output you can see it is
converted
my input to all caps
similarly you can change this
let's say you want to print something in
lower case you can use the lower
function
let's say
this time everything is in
upper case if I run it
it converts India to lower case
now let's explore a few date and time
functions let's say you want to find the
current date there is a function called
Cur which stands for current
and this is the function I'm talking
about which is current date
if I run this you will get the current
date
that is 28th of Jan 2021
and let's say you want to extract the
day from a date value
so you can use the day function let's
say I'll use day and I want to find the
day from my current date
if I run this
you get 28 which is today's d
now similarly you can also
display the current date and time so for
that you can use a function that is
called now
so this will return the current date and
time
you can see this is the date value and
then we have the current time
all right
and this brings us to the end of our
demo session so let me just scroll
through whatever we have learned so
first I showed you how you can see the
databases present in MySQL then we used
one of the databases and checked the
tables in it then we created another
database called SQL intro for our demo
purpose we use the database and then
we created this table called employee
details with
column names like name integer the sex
date of joints City and salary
I showed you the structure of the
database let me run this again so you
get an idea you can see this was the
structure of our table
then we went ahead and inserted a few
records so we inserted records for six
employees so you have
the employee name the age the gender the
date of join the city to which the
employee belongs to and the salary of
the employee
then we saw how you can use the select
statement and display all the columns
present in the table
we learned how you can
display the Unique city names
we learned how to use
different aggregate functions like count
average and sum
then we learned how you could display
specific columns from the table
we learned how to use where clause
then we used an R operator we learned
about in operator
the between operator
then we used an and operator to select
multiple
conditions
finally
we learned about group by order by and
some basic SQL operations we are going
to learn two important SQL statements or
Clauses that are widely used that is
grouped by and having
first we'll understand the basics of
group by and having and then jump into
MySQL workbench to implement these
statements so let's begin first
what is Group by in SQL so the group by
statement or Clause groups records into
summary rows and returns one record for
each group it groups the rows with the
same group by item expressions and
computes aggregate functions for the
resulting group
a group by Clause is a part of Select
expression
in each group no two rows have the same
value for the grouping column or columns
now below
you can see the syntax of group by so
first we have the select statement and
Then followed by the column names that
we want to select
from we have the table name
followed by the where condition and next
we have the group by clause and here we
include the column names finally we have
the order by and the column names
now here is an example of the group by
Clause so we want to find the average
salary of employees for each department
so here you can see we have the
employees table it has the employee ID
the employee name the age of the
employee we have the gender the date on
which the employer joined the company
then we have the department to which
each of these employees belong to we
have the city to which the employees
belong to and then we have the salary in
dollars so actually we'll be using this
employee stable on MySQL workbench as
well
so if you were to find the average
salary of employees in each department
so this is how your SQL query with Group
by Clause would look like so we have
selected department and then we are
using an aggregate function that is AVG
which is average and we have chosen the
salary column and here we have given an
alias name which is average underscore
salary which appears in the output you
can see here from employees and we have
grouped it by department so here in the
output you can see we have the
department names and the average salary
of the employees in each department
now let me take you to my MySQL
workbench where we will Implement group
buy and solve specific problems if
getting your learning started is half
the battle what if you could do that for
free visit scale up by simply learn
click on the link in the description to
know more
okay so I am on my MySQL workbench so
let me make my connection first
I'll enter the password
so this will open my SQL editor
so first of all
let me
check the databases that I have
so I'll use my query that is show
databases
let's run it
okay you can see we have a list of
databases here I am going to use my SQL
underscore intro database so I'll write
use
SQL underscore intro
so this will take us inside this
database I'll run it all right
now you can check the tables that are
present in SQL underscore into database
if I write shoe tables you can see the
list of tables that are already present
in this database to do our demo and
understand Group by as well as having
let me first create an employee table
so I'll write create table
employees
next
I'll give my column name as employee
underscore ID which is
the ID for each employee
I'll give my data type as integer and
I'll assign
employee ID as my primary key
next I'll give employee underscore name
and my data type would be bar car
I'll
give the size as 25
my third column would be the age column
age would obviously be an integer
then I have
my gender column
I'll use character data type and assign
a value of 1 or size of 1.
next we have the date of join
and the data type will be date
we have the
Department column as well this is going
to be of
and 20 will be the size
next
we have the city column
which is actually the city to which the
employee belongs to
and finally we have the
salary column which will have the salary
for all the employees okay
now let me select and run this
you can see here we have successfully
created our table
now to check
if our table was created or not you can
use the describe command
I'll write describe employees
you can see the structure of the table
so far
all right now it's time for us to insert
a few records into this employee stable
so I'll write insert
into
employees
and I'll
copy paste the records which have
already
written on a notepad so let me show you
so this is my EMP notepad and you can
see I have already
put the information for all the
employees so let me just copy this
and
we'll paste it here
all right let me go to the top and
verify if
all the records are fine
all right
so let's run our insert query
okay so you can see here we have
inserted 20 rows of information
and
now let's check
the table information or the records
that are present in our employees table
I'll write select star from employees if
I run it you can see here
I have my employee ID the employee name
age gender
we have the city salary and in total we
have
instead 20 records
now
let me run a few SQL commands to check
how the structure of our table is
let's say I want to see the distant
cities that are present in our table
so I'll write select distinct City from
employees
if I run this you see here there are
total eight different cities present in
our employees table so we have Chicago
the Seattle Boston we have New York
Miami and Detroit as well
now let's see
you want to know the
total number of departments that are
present so you can use distinct
Department
if I run this
all right you can see we have seven rows
returned and here are the department
names so we have sales marketing product
Tech it finance and HR
all right
now let me show you
another SQL command
now this is to use an aggregate function
so I want to find the average age of all
the employees from the table
so I can write select
AVG which is the aggregate function for
average
inside that I have passed my
each column from employees if I run this
so the average age of all the employees
in our table is 33.35
now
say you want to find the average age of
employees in each department
so for this you need to use the group by
clause
I'll give a comment here I want to find
the
average age
in each department
so
I'll write select
Department
comma
I'll write average of age
from
employees
Group by
Department
now if I run this
you can see here
we have our
seven departments
on the left and on the right you can see
the average age of employees in each of
these departments now you can see here
in the output it says AVG of each which
is not readable so I can give an alias
name as
average age
all right
I can bring this down
and if you want you can round the values
also
so you can round the decimal places so
I'll use a round function before the
average function
and the round function takes two
parameters one is the variable and
the decimal place you want to round it
to
so if I run this there you go you can
see here we have the average each of all
the employees in each of these
departments
all right
now suppose you want to find the total
salary of all the employees for each
department so
you can write select
Department
comma
now I want the total salary so I'll use
the sum function
and I'll
pass my column as salary
from
employees
Group by
Department
let's run this query
you can see here
in the output we have the different
departments and on the right you can see
the total salary of all the employees in
each of these departments now here also
you can give an alias name as
total underscore salary
let's run it again
and you can see the output here
all right now moving ahead
you can also use the order by Clause
along with the group by Clause let's see
you want to find the total number of
employees in each City and group it in
the order of employee ID so
to do this I can use my select query
I'll write select
count of let's say employee ID
and I want to know the city as well
from
employees
Group by
City
and next you can use the order by clause
I'll write order by
count of
employee ID
and I'll write desc which stands for
descending
if I run this query
you can see here on the left you have
the account of employees and on the
right you can see the
city names so in Chicago we had the
highest number of employees working that
was four then we had Seattle Houston
Boston Austin
and the remaining also had two employees
so in this case we have ordered our
result based on the count of employee ID
in descending order so we have the
highest number appearing at the top and
then followed by the lowest okay
now let's explore another example
suppose we want to find the number of
employees that joined the company each
year we can use the year function on the
date of joining column then we can count
the employee IDs and group the result by
each year
so let me show you how to do it
so I'll write select
I am going to extract here from
the date of join column I'll give an
alias as year
next
I'll
count the
employee ID
from my table name that is employees
and
I am going to group it by
year
date of join
I give a semicolon
all right
so let's run this
grid you see here in the result we have
the year that we have extracted from the
data join column and on the right you
can see the
total number of employees that joined
the company each year so we have in 2005
there was one employee similarly we have
in 2009 there were two employees
if I scroll down you have information of
other years as well
now if you want you can order this as
well based on year or count
okay
now you can also use the group by to
join two or more tables together
so to show you this operation let me
first create a sales table
so I'll write create
table
seals and the sales table will have
columns such as the product ID which is
going to be of integer type then we have
the
selling price of the product
now this will be a a float value
then we have the quantity sold for each
of the
products so I'll write
quantity
quantity will of integer type
next we have the state in which the item
was sold
and state I'll put it as worker and give
the sizes 20.
let's run this so that we'll create our
sales table all right
so we have successfully created our
sales table
next we need to insert a few values to
our sales table so I've already written
the records in a notepad let me show you
okay so here you can see I have my sales
text file let me just copy these
information
I'll just paste it on
the query editor
okay
now let me go ahead and
run this
insert command all right so you can see
here we have successfully inserted nine
rows of information so let me just run
it through what we have inserted
so the first
column is the product ID column then we
have the selling price at which this
product was sold then we have the
quantity that was sold and in which
state it was sold so we have California
Texas Alaska
then we have
another product ID which is one two
three
and these are the states in which the
products were sold
so let me just
confirm with the
select statement I'll write select star
from
sales
I run this you can see we have
successfully created our table
okay
now suppose you want to find the revenue
for both the product IDs one to one and
let's say one two three since we have
just two product IDs here
so for that you can use the select query
so I'll write select
product ID
next I want to calculate the revenue so
revenue is nothing but selling price
multiplied by the quantity so I'll use
the sum function to find the total
revenue
and inside the sum function I'll use my
selling price column
multiplied by
quantity column
I'll give this
an alias name as Revenue
from
my table name that is sales
finally I'll group it by
product ID
let's run it
there you go so here you can see we have
the two product IDs one two one and one
two three and here you can see the
revenue that was generated from these
two products
all right
now let's see
we have to find the total profit that
was made from both the products one to
one and one two three so for that I'll
create another table now this table will
have the cost price of
both the products so let me create the
table first
I'll write create table
let's say the table name is C product
which stands for the cost price of the
products
I'll
Give My First Column as product ID
this will be an integer
and I'll have my second column as cost
price
cost price will have floating type
values
let's run this so we have successfully
created our product cost table
now let me insert a few values into the
C product table so I'll write insert
into
C underscore
product
I'll give
my values
for one to one
let's see the
cost price was 200 and
70 dollars
for each
and next we have my product is one two
three and let's say the cost price for
product one two three was two hundred
and fifty dollars
foreign
let's insert these two values okay next
we'll join our sales table and the
product cost table so this will give us
the profit that was generated for each
of the products
so I'll write select
C Dot
product underscore ID
comma
I'll write sum
s dot cell underscore price now here C
and S are Alias names
so if I subtract my
cost price from the selling price that
will return the profit that was
generated
I'll multiply this with s dot
quantity
close the bracket
I'll give an alias name as
profit
from
seals as s
so here s stands for the sales table
I am going to use inner join
C underscore
product table
as
the Alias name should be C
where
s dot
product underscore ID is equal to C Dot
product underscore ID we are using
product underscore ID because
this column is the common column to both
the tables
and finally
I am going to
group it by
C dot product underscore ID
all right
so let me tell you what I have done here
so I am selecting the product ID
next I am calculating the profit by
subtracting the cost price from the
selling price and I have multiplied the
quantity column
I am using an inner join to connect my
sales and the
product cost table
and I am joining on the column that is
product ID and I have grouped it by C
dot product ID let's run this there you
go so here you can see for product ID
one to one we made a profit of
eleven hundred dollars
and for product ID 123 we made a profit
of eight hundred and forty dollars so
now that we have learned Group by in
detail let's learn about the having
clause in SQL the having clause in SQL
operates on grouped records and returns
rows where aggregate function results
matched with given conditions only
so now having and where Clause are kind
of similar but where Clause can't be
used with an aggregate function
so here you can see the syntax of having
Clause you have the select statement
followed by the column names from the
table name then we have the WHERE
conditions next we have the group by
finally we have having and at last we
have order by column names so you can
see here we have a question at hand we
want to find the cities where there are
more than two employees
so you can see the employee table that
we had used in our
Group by Clause as well so if you were
to find the cities where there are more
than two employees so this is how your
SQL query should look like so we have
selected
the employee ID and we are finding out
the count using the count function
next we have selected the city column
from employees we have grouped it by
City And then we have user having clause
so we have given our condition having
count of employee ID should be greater
than 2. so if you see the output we have
the different city names and these were
the cities where the
count of employees was greater than 2.
all right so let's go to our MySQL
workbench and Implement how
having books so suppose
you want to find those departments where
the average salary is greater than 75
000 you can use the having clause for
this so let me first run my table which
is employees
if I run this you can see we had
inserted 20 rows of information and
the last column we had was salary so the
question we have is we want to find
those departments where the average
salary is greater than 75 000 dollars
so let me show you how to do it
so I'll write
select
Department comma
I'll use the aggregate function that is
average
salary
I'll give an alias name as
AVG
underscore salary
from
employees
next we'll use the
Group by clause and I want to group it
by each department
and then I am going to write my having
Clause so in having Clause I'll use my
condition that is having
average of salary
greater than
seventy five thousand dollars
let's run it
and see the output there you go so here
you can see there were total three
departments in the company that is sales
finance and HR where the average salary
is greater than 75 000 dollars
okay
next let's say you want to find the
cities where the total salary is greater
than two hundred thousand dollars
so this will again be a simple
SQL query so I'll write select City
comma
I want to find the total salary so I'll
use the sum function
and I'll pass my column as salary as
I'll give a
Alias name as total from
employees
Group by
City
and then I am going to use my having
clause
I'll pass in my condition as having
sum of salary
greater than
two hundred thousand dollars
all right
so let's run this query
there you go
so
so the different cities are Chicago
Seattle and Houston where the total
salary was greater than two hundred
thousand dollars now suppose you want to
find the Departments that have more than
two employees so let's see how to do it
I'll write select
Department comma
this time since I want to find the
number of employees I am going to use
the count function I'll write count Star
as
employee underscore count or EMP
underscore count which is my Alias name
from
employees
next I'll group it by
Department
having
I'll give my condition
count star greater than 2.
let's run this
okay
so you have departments such as sales
product Tech and ID where
there are more than two employees
okay now you can also use a where Clause
along with the having clause in an SQL
statement
so suppose I want to find the cities
that have more than two employees apart
from Houston
so
I can write my query as select
City comma
count star
as
EMP count
from
employees
where
I'll give my condition shitty
not equal to
Houston I'll put it in double quotes
since I don't want to see
the information regarding Houston
I'll group it by
City
having
count
of employees greater than 2.
so if I run this query
you see we have information for Chicago
and Seattle only
and we have excluded the information for
Houston
now you may also use aggregate functions
in the having Clause that does not
appear in the select clause
so if I want to find the total number of
employees for each department that have
an average salary greater than 75 000 I
can write it something like this
so select
Department
comma
count
star
as
EMP count
from
employees
Group by
Department
and in the having clause
I am going to provide the column name
that is not present in the select
expression so I'll write having
average salary
greater than 75 000.
this is another way to use the having
Clause let's run this
all right you can see we have department
sales finance and HR and you can see the
employee count where the average salary
was greater than 75 000.
okay
so let me run you from the beginning
what we did in our demo so first we
created a table called employee then we
inserted 20 records to this table
next we explored a few SQL commands like
distinct then we used average and
finally we started with our
Group by clause
followed by
looking at how group I can be used along
with another table and we joined two
tables that was sales and product cost
table to find out the profit
then you learned how to use the having
clause
so we explored several different
questions and learned how to use
having an SQL we will learn about joints
in SQL joints are really important when
you have to deal with data that is
present on multiple tables
I'll help you understand the basics of
joins and make you learn the different
types of joints with Hands-On
demonstrations on MySQL workbench
so let's get started with what are joins
in SQL
SQL join statement or command is often
used to fetch data present in multiple
tables SQL joints are used to combine
rows of data from two or more tables
based on a common field or column
between them now consider this example
where we have two tables an order stable
and a customer stable
now the order table has information
about the order ID which is unique here
we have the order date that is when the
order was placed then we have the
shipped date this has information about
the date on which the order was shipped
then we have the product name which
basically is the names of different
products we have the status of delivery
whether the product was delivered or not
or whether it was canceled then we have
the quantity which means the number of
products that were ordered and finally
we have the price of each product
similarly
we have another table called customers
and this customer table has information
about the order ID
which is the foreign key here
then we have the customer ID which is
the primary key for this table we also
have the phone number customer name and
address of the customers
now suppose you want to find the phone
numbers of customers who have ordered a
laptop
now to solve this problem
we need to join both the tables the
reason being the phone numbers are
present in the customers table as you
can see here
and laptop which is the product name is
present in the order stable which you
can see it here so using a join
statement you can find the phone numbers
of customers who have ordered a laptop
now let's see another problem where you
need to find the customer name so have
ordered a product in the last 30 days in
this case we want the customer name
present in the customers table and the
last 30 days order information which you
can get from the order date column that
is present in the order table
okay
now let's discuss the different types of
joints one by one so first we have an
inner join
so the SQL inner join statement returns
all the rows from multiple tables as
long as the conditions are met
from the diagram above you can see that
there are two tables A and B A is the
left table and B is the right table
the orange portion represents the output
of an inner join
which means an inner join Returns the
common records from both the tables
now you can see the syntax here so we
have the select command and then
we give the list of columns from table a
which you can see here is the left table
followed by the inner join keyword and
then the name of the table
that is B on a common key column from
both the tables A and B now let me take
you to the MySQL workbench and show you
how inner join Works in reality
so here I'll type MySQL
you can see I have got my SQL workbench
8.0 version installed I'll click on it
will take some time to open
okay
I'll click on this local instance
and here I'll give my password
okay
so this is how an SQL editor on MySQL
workbench looks like
so first of all
let me go ahead and create a new
database
so I'll write create
database this is going to be my command
followed by the name of the database
that is going to be SQL underscore joins
if I give a semicolon and hit Ctrl enter
this will create a new database you can
see here one row affected
now
you can check whether the database was
created or not using show databases
command
if I run it
here you can see I have SQL joins
database
created
now I'll use this database so I'll write
use SQL underscore joins
okay now to understand inner join
consider that there is a college
and in every College you have different
teams for different sports such as
Cricket football basketball and others
so let's create two tables cricket and
football
so I'll write
create
table
and my table name is going to be cricket
next
I am going to create two columns
in this table the First Column is going
to be cricket
ID
then
I'm going to give the data type as int
and use the auto increment operator I am
using Auto increment because my Cricut
ID is going to be my primary key
then I'm going to give
the name of the students who are part of
the cricket team
and for this I'll use war card data type
and give the length as 30
I'll give another comma and
I'll assign my Cricut ID as primary key
within brackets I'll give
Cricket underscore ID
Cricket ID is nothing but a unique
identifier for each of the players like
you have roll numbers in college
okay
let me just run it
all right so we have successfully
created our Cricut table
similarly let me just copy this and I'll
paste it here
I'll create
another table called football
this will have the information of all
the students who are part of the
football team
and instead of cricket I am going to
give this as football ID
all right
and
the name column will have the names of
the students
I'll change my primary key to football
ID
all right let me run this okay so now we
have also created our
football table
the next step is to insert a few player
names into both the tables
so I'll write my insert into command
first let's load some data to our
cricket table
so I'll write cricket and
I'll give my
name column
followed by
values and here I'll give some names
such as
Stuart
we have another comma
the next player I'll choose is let's say
Michael
similarly I'll add a few more let's say
we have
Johnson
the fourth player I'll take is let's say
hidden
and finally we have
let's say Fleming
okay
now I'll give a semicolon and run this
okay so let me just check if all the
values were inserted properly for this
I'll use
select star from table that is Cricket
if I run it you can see I have created a
table and have successfully inserted
five rows of information
now similarly
let's insert
a few student names
for our football table so I'll change
this to
football
and obviously there would be students
who will be part of both cricket and
football team so
I'll keep a few
repeated names let's say Stuart
Johnson and let's say hidden r part of
both cricket and football team
then we have let's say
Langer and
let's say we have another player in the
football team that is astral
I'll just run it okay you can see there
are no errors so we have successfully
inserted values to our football team as
well
let me just recheck it
I'll write select star from
football
all right so we have five players in the
football team as well okay
now the question is suppose you want to
find the students that are part of both
the cricket and football team in this
case you can use an inner join
so let me show you how to do it
so I'll write select star from
cricket
as I'm using an alias name as C which
stands for Cricket
then I am going to write inner join
my next table is going to be football
as F which is an alias name for the
football table
then I am going to use the on
command or operator
and then
I'll give the common
key that is name here so C dot name is
equal to F dot name
So based on this name column from both
the table my inner Zone operation will
be performed so let's just run it
there you go so Stuart Johnson and
Hayden are the only three students who
are part of both the teams
all right
now you can also individually select
each of the columns from both the tables
so let's say I write select C Dot
Cricket underscore ID
comma
C dot name
comma F Dot
football underscore ID
comma F dot name
from
I'll write
Cricket as C
inner join
football as f
on
C dot name
is equal to F dot name
now if I run this you see we get the
same output here as well
all right
now let's explore another example to
learn more about inner joins so we have
a database called classic models let me
first
use classic models
I'll run this
okay
now let me just show the different
tables that are part of classic tables
all right
so here you can see there are tables
like customers there's employees there's
office details orders payments products
and product lines as well
all right so
let me use
my select statement to show
what are the columns present in the
products table
okay so this product table has
information about different
product names you have the product code
now this product code is unique here
we also have the product vendor
a little description about the product
then we have the quantity in stock
buying price and MSRP
let's see what we have in
product lines
if I run it you see here we have the
product line which is the
primary key for this table then we have
the textual description for each of the
products this is basically some sort of
an advertisement
all right now suppose
you want to find the product code the
product name and the text description
for each of the products you can join
the products and product lines table
so let me show you how to do it write my
select statement and
choose my columns as product code then
we have
product name
and let's say I want the text
description so I'll write this
column name
okay
then I'll use from
my first table that is
products
inner join
product lines
I can use using
the common key column that is product
line
close the bracket I'll give a semicolon
and if I run it there you go so you can
see the different product codes then we
have the different product names and the
textual description for each of the
products so this we did by joining the
products table and the product lines
table
all right
now suppose you want to find the revenue
generated from each product order and
the status of the product to do this
task we need to join three tables that
is orders order details and products so
first let me show you
what are the columns we have in these
three tables we have obviously seen for
the products table now let me show you
for orders and Order details table
so I'll write select star from orders
pyron it you can see it has information
about the order number
the date on which the order was placed
we also have the shipment date we also
have the status column which has
information regarding whether the order
was shipped or canceled
then we have some comments column we
also have the customer number who
ordered this particular product
similarly let's check what we have under
order details
so I'll write select star from
order details if I run it
you can see it has the order number the
product code quantity of each product we
have the price of each product then we
have the order line number okay
so using the product
orders and Order details let's perform
an inner join
so I'll write select
O DOT
order number
comma
O DOT
status
comma
I need the product name which I'll take
from the products table so I'll write P
Dot
product name now here o p are all Alias
name for the tables orders products and
I'll use OD for order details
comma
since we want to find the revenue
we actually need to
find the product of quantity ordered
into
price of each product so I'll use a sum
function and inside the sum function
I'll give
quantity
ordered
multiplied by
the price of each item
I'll use an alias as Revenue
then I'll use my from clause
from orders as o
inner join
order
details as I'll use an alias name as OD
on
I'll write
O DOT
order number
is equal to
OD Dot
order number
I'll use another inner join
and this time we'll join the products
table so I will write inner join
products as
be on
P Dot
product code
is equal to
OD Dot
product code
and finally I'll use the group by clause
and group it by order number
all right
let me run this
okay there's some mistake here we need
to debug this
it says you have an error in your SQL
syntax check the manual all right okay I
think the name of the tables is actually
orders and not order
all right now let's run it
okay there's still some error
it says
classic models dot product doesn't exist
so again the product name is
I mean the table name is products are
not product so let's run it again all
right there you go so we have the order
number the status
the product name and the revenue
this we got it using
inner join from three different tables
if getting your learning started is half
the battle what if you could do that for
free visit scale up by simply learn
click on the link in the description to
know more
now talking about left joins
the SQL left join statement returns all
the rows from the left table and the
matching rows from the right table so if
you see this diagram you can see we have
all the rows from the left table that is
a and only the matching rows from the
right table that is B so you can see
this
overlapped region
and the syntax
for SQL left join is something like this
so you have the select statement and
then you give the list of columns from
table a which is your left table then
you use the left join keyword
followed by the next table that is table
B on the common key column so you write
a DOT key is equal to B dot key
okay now in our classic models database
we have two tables customers and orders
so if you want to find the customer name
and their order ID you can use these two
tables so first let me show you the
columns that are present in
customers and orders
I think orders we have already seen let
me first show you
what's there in the customers table okay
so you can see we have the customer
number the name of the customer then we
have the contact last name the contact
first name we have the phone number then
there's an address column there are two
address columns actually
we have the city name the state
and we have other
information as well and similarly we
have our order stable
so I'll write select start form
orders so I'll write select star from
orders
if I run this you can see these are the
information available in the
orders table
okay so let's perform a left join
where we want to find the customer name
and their order IDs so I'll write select
C Dot
customer name
or let's say first we'll choose the
customer number comma
then I want the customer name so I'll
write C Dot customer name
then we have the
order number column which is present in
the orders table and let's say I also
want to see the status
then I'll give my left table that is
customers
as C
left join
orders as o
on
C Dot
customer number equal to
O DOT customer number
let's run it
okay again there is some problem all
right so the table name is customers
let's run it
so there's another mistake here this is
customer number so B is missing cool let
me run it all right so here you can see
we have the information regarding the
customer number
then
the respective customer names we have
the order number and the
status of the shipment so if I scroll
down you'll notice one thing there are a
few rows you can see which have null
values this means for customer number
125 and for this particular customer
name there were no orders
and similarly if I scroll down you will
find a few more null values
you can see here there are two null
values here for
customer number 168 and 169 there were
no orders available
all right now to check those customers
who haven't placed any orders
you can use the null operator so what
I'll do is here I'll just continue with
this I'll use a where clause and write
where
order
number is null
now let me run this
okay so here you can see there are
24 customers from the table
that don't have any orders in their
names
okay now talking about right joints so
SQL right join statement returns all the
rows from the right table and only
matching rows from the left table so
here you can see we have our left table
as a and the right table as B so the
right join will return all the rows from
the right table and only the matching
rows from the left table
now talking about the syntax so here you
can see we have the select statement
followed by the select statement you
will have the list of columns that you
want to choose
from table a right join table B on the
common key column from both the tables
all right
now to show how right join works I'll be
using two tables that is customers and
employees so let's see
the rows of data that are present in the
customer table first so I'll write
select star from
customers
let's run it so here you have the
customer number the customer name
then we have the phone number the
address of the customers
you also have the country to which the
customer belongs to the postal code
and the credit limit as well similarly
let's see for
the employees table
here I'll change customers to employees
let's run it okay so we have the
employee number the last name the first
name we have the extension the email ID
the job title and also
reports to here means the manager
okay
So based on these two tables we'll find
the customer name
the phone number of the customer and the
email address of the employee and join
both the tables that is customers and
employees
so let me show you
the command so I'll write select
C Dot
customer name comma
then we have C Dot phone I'll give a
space here
next I want
the employee number from the employee
table so I'll write e Dot
employee number
comma
e dot email from customers as C
right join
employees
as e
on
e Dot my common key column is employee
number here so I'll write e Dot employee
number is equal to
C Dot
we have
sales
Representatives
employee number
and I'm also going to order it by
the employee number column
okay
so you can see I have my customer name
selected from the customers table the
phone number of the customer then we
have the employee number and the email
address so let me run it
okay there's some problem
all right so the table name is customers
actually
let's run it once again
there you go so you can see here
we have
all the values selected from our right
table which is the employees table you
can see rights on employees which means
your employee stable is to the right
and then we have the customer name and
phone numbers of
the customers from the customer table
which is actually your left table so you
have a few
employee numbers such as one zero zero
two this one zero five six which don't
have any customer name or phone numbers
okay so there's another popular join
which is very widely used in SQL known
as cell joins so self joins are used to
join a table to itself
so in our database we have a table
called employees let me show you the
table first
all right
so here you can see we have the employee
number the last name the first name of
the employee we have the email ID
and here if you see we have a column
called reports two now this you can
think of as the manager column so the
way to read is for example
for employee number 1056
the manager is one zero zero two so if
you check for one zero zero two we have
Dane Murphy
then if I scroll down
let's see for employee number
one one zero two
yeah for employee number 1102 the
manager is one zero five six so here you
can see who is at one zero five six you
have
Mary Patterson
similarly if I scroll down let's see
for employee number 1188
we have
the manager as one one
four three now if I check the table at
one one four three we have Anthony bow
so
so the employee Julie farrelli reports
to Anthony bow
all right
now suppose you want to know who is the
reporting manager for each employee so
for that you can use a self join so let
me show you how to join this employees
table
I'll write select
and then I am going to use a function
called concat
within brackets I'll start with my Alias
name that is m dot
then I'll write last name
I am going to concat last name
followed by a comma
then I'll have my first name
I'll close this bracket
and then I am going to give my
Alias name let's say manager
comma next I am going to concat
the same
last name and first name and this time I
am going to use a separate Alias let's
say e which stands for employee so I'll
write e dot last name comma and within
single quotes
I'll
give my comma and then I'll write e DOT
first name
I close this bracket I'll give an alias
as let's say
employee
from
I'll write
employees
as e
inner join
employees
as m
on
m dot
I'll use my common key column as
employee number so I'll write
m dot
employee number is equal to
e dot here I am going to use the reports
to column
and then
I'll
order it by
let's say manager
okay
now let's run this
there you go so you have your two
columns as manager and employee so for
employee Louis bonder
the manager is zeradbonder
similarly if I scroll down
you have there are multiple employees
reporting to this particular manager
similarly we have our manager as
anthonybo and we have
different employees who are reporting to
this particular manager and so on
all right now moving ahead
now let's see what a full join is
so SQL full outer join statement returns
all the rows when there is a match in
either left or right table now you must
remember that MySQL workbench does not
support full outer join by default but
there's a way to do it so by default
this is how the syntax of full outer
join looks like
now this statement will work on other
SQL databases like Microsoft SQL server
but it won't work on MySQL workbench
I'll show you the right way of using
full auto join on MySQL workbench
so to sow full auto join I am going to
first use a left join and then we'll
also use a right join and finally we'll
use a union operator so the union
operator is used to combine the result
set of two or more select statements
so first of all let me write
C Dot
customer name so for this example I'm
using the customer table and the order
table
comma O DOT
order number
so I just want to know the customer name
and the order number related to the
customer
from
I have
customers
as
C
left join
I'll write
orders as o on
C Dot
customer number
is equal to
O DOT
customer number
let me just copy this
and after this I am going to use my
union operator
so Union operator is used to merge
results from
two or more
tables
so basically this performs a vertical
join
and next I am going to use my right join
operation so here instead of left join
I'll write right
restore looks fine let me just run it
there you go so we have successfully run
our full outer join operation you can
see we have the different customer names
and the order that each customer had
placed
all right
at the Caltech data analytics bootcamp
you will learn everything you need to
know to succeed as a data analyst from
mastering Excel to creating data driven
presentations manipulating data with SQL
analyzing data with python and
visualizing data with W today we're
going to jump into some common questions
you might see on on bras and pandas data
frames and the python along with some
Excel Tableau and SQL
let's start with our first question what
is the difference between Data Mining
and data profiling
it's real important to note that data
mining is a process of finding relevant
information which has not been found
before it is a way in which raw data is
turned into valuable information you can
think of this as anything from the cells
stats and from their SQL Server all the
way to web scraping and Census Bureau
information where the heck do you mine
it from where do you get all this data
and information
then we look at data profiling is
usually done to assess a data set for
its uniqueness consistency and logic it
cannot identify incorrect or inaccurate
data values so if somebody has a
statistical analysis on one side and
they're doing their you might in the
wrong data to then program your data
setup so you got to be aware that when
you're talking about data mining you
need to look at the Integrity of what
you're bringing in where it's coming
from data profiling is looking at it and
saying hey how is this going to work
what's the logic what's the consistency
is it related to what I'm working with
find the term data wrangling and data
analytics data wrangling is a process of
cleaning structuring and enriching the
raw data into a desired usable format
for better decision making
and you can see a nice chart here with
our Discover it we structure the data
how we want it we clean it up get rid of
all those null values we enrich it so we
might take and reformat some of the
settings instead of having five
different terms for height of somebody
you know in American English or whatever
clean some of that up and we might do a
calculation and bring some of them
together
and validate I was just talking about
that in the last one need to validate
your data make sure you have a solid
data source and then of course it goes
into the analysis very important to
notice here in data wrangling 80 of data
analytics is usually in this whole part
of wrangling the data getting it to fit
correctly and don't confuse that with
data cooking which is actually when
you're going into neural networks
cooking the data so it's all between
zero and one values
what are common problems that data
analysts encounter during analysis
handling duplicate and missing values
collecting the meaningful write data the
right time making data secure and
dealing with compliance issues handling
data purging and storage problems again
we're talking about data wrangling here
eighty percent of most jobs are in
wrangling that data and getting it in
the right format and making sure it's
good data to use
number four what are the various steps
involved in any analytics project
understand the problem we may spend 80
percent doing wrangling but you better
be ready to understand the problem
because if you can't you're going to
spend all your time in the wrong
direction this is probably uh the most
important part of the process everything
after it falls in and then you can come
back to it
two data collection data cleaning number
three four data exploration analysis and
five interpret the results
number five is a close second for being
the most important if you can't
interpret what you bring to the table to
your clients you're in trouble
so when this question comes up you
probably want to focus on those two
noting that the rest of it does eighty
percent of the work is in two three and
four well one and five are the most
important parts
which technical tools have you used for
analysis and presentation purposes
being a data analyst you are expected to
have knowledge of the below tools for
analysis and presentation purposes
there's a wide variety out there SQL
Server MySQL you have your Excel your
SPSS which is the IBM platform tab blue
python you have all these different
Tools in here now certainly a lot of
jobs are going to be narrowed in on just
a few of these tools like you're not
going to have a Microsoft SQL Server
MySQL server but you better understand
how to do basic SQL polls and also
understanding Excel and how the
different formats from column and how to
get those set up
number six what are the best practices
for data cleaning this is really
important to remember to go through this
in detail these always come up because
80 percent of most data analysis is in
cleaning the data make a data cleaning
plan by understanding where the common
errors take place and keep
Communications open identify and remove
duplicates before working with the data
this will lead to an effective data
analysis process focus on the accuracy
of the data maintain the value types of
data provide mandatory constraints and
set Cross Field validation
standardize the data at the point of
entry so that it is less chaotic and you
will be able to ensure that all the
information is standardized leading to
fewer errors on Entry
number seven how can you handle missing
values in a data set list wise deletion
in listwise deletion method entire
record is excluded from analysis if any
single value is missing sometimes we're
talking about records remember this
could be a single line in a database so
if you have your SQL comes back and you
have 15 different columns every one of
those has a missing value you might just
drop it just to make it easy because you
already have enough data to do the
processing average imputation use the
average value of the responses from the
other participants to fill in the
missing value this is really useful and
they'll ask you why these are useful I
guarantee it if you have a whole group
of data that's collected and it doesn't
have that information in it at that
point you might average it in there
regression substitution you can use
multiple regression analysis to estimate
a missing value that kind of goes with
the average imputation input regression
model means you're just going to get
you're going to actually generate
generate a prediction as to what you
think that value should be for those
people based on the ones you do have
multiple imputation so we talked about
multiple inputs it creates plausible
values based on the correlations for the
missing data and then average the
simulated data sets by incorporating
random errors in your predictions
what do you understand by the term
normal distribution and the second you
hear the word normal distribution should
be you think in a bell curve like we see
here normal distribution is a type of
continuous probability distribution that
is symmetric about the mean and in the
graph normal distribution will appear as
a bell curve the mean median and mode
are equal that's a quick way to know if
you have normal distribution is you can
compute mean median and mode all of them
are located at the center of the
distribution 68 of the data lies within
one standard deviation of the mean 95
percent of the data Falls within two
standard deviations of the mean
99.7 percent of the data lies within
three standard deviations of the mean
what is time series analysis time series
analysis is a statistical method that
deals with ordered sequence values of a
variable of equally spaced time
intervals time series data on a covid-19
cases and you can see we're looking at
by day so our spaces of days and each
day goes by if we take a graph it you
can see the time series graph always
looks really nice if you have like two
different in this case we have what the
United States going over there I'd have
to look at the other setup in there but
they picked a couple different countries
and it is it's time sensitive you know
the next result is based on what the
last one was Cove is an excellent
example of this anytime you do any word
analytics where you're figuring out what
someone's saying what they said before
makes a huge difference is what they're
going to say next another form of Time
series analysis
10. how is joining different from
blending in tableau so now we're going
to jump into the Tableau package data
joining data joining can only be done
when the data comes from the same Source
combining two tables from the same
database or two or more worksheets from
the same Excel file all the combined
tables or sheets contains common set of
dimensions and measures
data blending data blending is used when
the data is from two or more different
sources combining the Oracle table with
the SQL server or two sheets from Excel
or combining Excel sheet and Oracle
table in data blending each data source
contains its own set of dimensions and
measures
how is overfitting different from
underfitting
always a good one overfitting probably
the biggest danger in data analytics
today is overfitting model trains from
the data too well using the training set
the performance drops significantly over
the test set happens when the model
learns the noise and random fluctuations
in the training data set in detail
and again the performance drops way
below what the test set has
the model neither trains the data well
nor can generalize to new data performs
poorly both on train and the test set
happens when there is less data to build
and an accurate model and also when we
try to build a linear model with a
non-linear data
in Microsoft Excel a numeric value can
be treated as a text value if it
proceeds with an apostrophe definitely
not an exclamation if you're used to
programming in Python you'll look for
that hash code and not an Amber sign
and we can see here if you enter the
value 10 into a fill but you put the
apostrophe in front of it it will read
that as a text not as a number
what is the difference between count
count a count blank and count if in
Excel
we can see here when we run in just
count D1 through D 23 we get 19 and
you'll notice that there is 19 numbers
coming down here
so it doesn't count the cost of each
which is a top bracket it doesn't count
the blank spaces either with the
straight count
when you do a count a you'll get the
answer is 20. so now when you do count a
it counts all of them even the title
cost of each
when you do count blank we'll get three
why there's three blank fields
and finally the count if if we do count
F of e 1 to e23 is greater than 10
there's 11 values in there basic
counting of whatever is in your column
pretty solid on the table there
explain how vlookup Works in Excel
vlookup is used when you need to find
things in a table or a range by row
the syntax has four different parts to
it we have our lookup value that's a
value you want to look up we have our
table array
the range where the lookup value is
located
column index number the column number
and range that contains the return value
and the range lookup specify true if you
want an approximate match or false if
you want an exact match of the return
value
so here we see vlookup F3 A2 to C8 2
comma zero for prints now they don't
show the F3 F3 is the actual cell that
prints is in that's what we're looking
at is F3 so there's your prints he pulls
in from F3 A2 to C8 is the the data
we're looking into and then number two
is a column in that data so in this case
we're looking for uh age and we count
name as one age is two keep in mind this
is Excel versus a lot of your Python and
programming languages where you start at
zero in Excel we always look at the
cells as one two three so two represents
the age
zero is false for having an exact match
up versus one we don't actually need to
worry about that too much in this 0 or 1
would work with this example and you can
see with the Angela lookup again her
name would be in the F column number
four that's what the f 4 stands for as
we where they pulled Angela from and
then you have A1 to C8 and then we're
looking at number three so number three
is height name being one H2 and then
height three and you'll see here pulls
inner height 5.8
so we're going to run jump over to SQL
how do you subset or filter data in SQL
to subset or filter data in SQL we use
where and having clause and you can see
we have a nice table on the left where
we have the title the director the year
the duration we want to filter the table
for movies that were directed by Brad
Bird why just because we want to know
who what Brad Bird did so we're going to
do select star you should know that the
star refers to all in this case we're
what are we going to return we're going
to return all title directory year and
duration that's what you mean by all
from movies movies being our table where
director equals Brad Bird and you can
see he comes back and he did the
incredible and Ratatouille
to subsetter filter data SQL we can also
use the where and having Clause so we're
going to take a closer look at the
different ways we can filter here filter
the table for directors whose movies
have an average duration greater than
115 minutes
so there's a lot of really cool things
into this SQL query and these SQL
queries can get pretty crazy so letter
some duration as total duration average
duration as average duration from movies
Group by director having average
duration greater than 115.
uh so again what are we going to return
we're going to return whatever we put in
our select which in this case is
director we're going to have total
duration and that's going to be the sum
of the duration we're going to have the
average duration average underscore
duration which is going to be the
average duration on there and then we of
course go ahead and group by director
and we want to make sure we group them
by anyone that has an having an average
duration greater than 115. these SQL
queries are so important I don't know
how many times you the SQL comes up and
there's so many different other
languages not just MySQL not Microsoft
SQL but in addition to that where the
SQL language comes in especially with
Hadoop in other areas so you really
should know your basic SQL doesn't hurt
to get that little cheat sheet and
glance over it and double check some of
the different features in SQL
what is the difference between where and
having clause in SQL where where Clause
works on row data in where Clause the
filter occurs before any groupings are
made aggregate conscience cannot be used
so the syntax is select your columns
from table where what the condition is
having Clause works on aggregated data
having is used to filter values from a
group aggregate functions can be used in
the syntaxes select column names from
table where the condition is grouped by
having a condition ordered by column
names
what is the correct Syntax for reshape
function in numpy so we're going to jump
to the numpy array program
and what you come up with is you have in
this case be numpy.reshape a lot of
times you do an import numpy as NP
reshape and then your array and the new
shape
and you can see here as we as the actual
example comes in the reshape is a and
we're going to reshape it in two comma
five setups and you can see the printout
in there that prints in two rows with
five values in each one
what are the different ways to create a
data frame in pandas
well we can do it by initializing a list
so you can Port your pandas as PD very
common data equals Tom 30 jerry20 Angela
35 we'll go ahead and create the data
frame and we'll say
pd.dataframe is the data columns equals
name and age so you can designate your
columns you can also it is a index in
there you should always remember that
the index in this case maybe you want
the index instead of one two to be the
date they signed up or who knows you
know whatever and you can see right
there it just generates a nice pandas
data frame with Tom Jerry and Angela
another way you can initialize a data
frame is from dictionary you can see
here we have a dictionary where the date
equals name Tom Jerry Angela Mary ages
20 21 1918 and if we do a DF PD dot data
frame on the data you'll get a nice the
same kind of setup you get your name age
Tom Jerry Angela and Mary
write the python code to create an
employee's data frame from the
emp.csv file and display the head and
summary of it to create a data frame in
Python you need to import the pandas
library and use the read CSV function to
load the CSV file
and here you can see we have import
pandas is PD employees or the data frame
employees equals pd.read CSV and then
you have your path to that CSV file
there's a number of settings in the read
CSV where you can tell it how many rows
are the top index you can set the
columns in there
you can have skip rows there's all kinds
of things you can also go in there and
double check with your read CSV but the
most basic one is just to read a basic
CSV
how will you select the department and
age columns from an employee's data
frame
so we have import pandas is PD you can
see we have created our data we will go
ahead and create our employees PD data
frame on the left
and then on the right to select
department and age from the data frame
we just do employees you put the
brackets around it now if you're just
doing one column you could do just
department but if you're doing multiple
columns you've got to have those in a
second set of brackets it's got to be a
reference with a list within the
reference
what is the criteria to say whether a
developed data model is good or not a
good model should be intuitive
insightful and self-explanatory follow
the old saying kiss keep it simple
the model developed should be able to
easily consumed by the clients for
actionable and profitable results
so if they can't read it what good is it
a good model should easily adapt to
changes according to business
requirements we live in quite a dynamic
world nowadays so it's pretty
self-evident and if the data gets
updated the model should be able to
scale accordingly to the new data so you
have a nice data pipeline going where
when something when you get new data
coming in you don't have to go and
rewrite the whole code
what is the significance of exploratory
data analysis
exploratory data analysis is an
important step in any data analysis
process exploratory data analysis Eda
helps to understand the data better it
helps you obtain confidence in your data
to a point where you're ready to engage
a machine learning algorithm it allows
you to refine your selection of feature
variables that will be used later for
model building
you can discover hidden Trends and
insights from the data
how do you treat outliers in a data set
an outlier is a data point that is
distant from other similar points they
may be due to variability in the
measurement or may indicate experimental
errors
one you can drop the outlier records
pretty straightforward you can cap your
outliers data so it doesn't go past a
certain value you can assign it a new
value you can also try a new
transformation to see if those outliers
come in if you transform it slightly
differently
explain descriptive predictive and
prescriptive analytics descriptive
provides insights into the past to
answer what has happened uses data
aggregation and data mining techniques
examples an ice cream company can
analyze how much ice cream was sold
which flavors were sold and whether more
or less ice cream was sold than before
predictive understands the future to the
answer what could happen uses
statistical models and forecasting
techniques
the example predicts a sale of ice
creams during the summer spring and
rainy days so this is always interesting
because you have your descriptive which
comes in and your businesses are always
looking to know what happened hey did we
have good sales last uh quarter what are
we expecting next quarter in sales and
we have a huge jump when we do uh
prescriptive suggest various courses of
action to answer what should you do uses
optimization and simulation algorithms
to advise possible outcomes example
lower prices to increase sell of ice
creams produce more or less quantities
of certain flavor of ice cream and we
can certainly uh today's world with the
covid virus we had that on our earlier
graph you could see that as a
descriptive what's happened how many
people have been infected how many
people have died in an area predictive
where do we predict that to go
um do we see it going to get worse is it
going to get better what do we predict
that we're going to need in hospital
beds and prescriptive what can we change
in our setup to have a better outcome
maybe if we did more social distancing
if we tracked the virus
how do these different things directly
affect the end and can we create a
better ending by changing some
underlying criteria
what are the different types of sampling
techniques used by data analysis
sampling is a statistical method to
select a subset of data from an entire
data set population to estimate the of
the whole population one we can do a
simple random sampling so we can just
pick out 500 random people in the United
States to sample them
we call it a population in regular data
we also call that a population just
because that's where it came from was
mainly from doing census
systematic sampling
cluster sampling
stratified sampling
and judgment or purposive sampling
then we have our systematic sampling
that's where you're doing like using one
five ten fifteen twenty use a very
systematic approach for pulling samples
from the setup cluster sampling that's
where we look at it we say hey some of
these things just naturally group
together if you were talking about
population which is the really a nice
way of looking at this cluster sampling
would be maybe by a zip code we're going
to do everybody's zip code and just
naturally cluster it that way
stratified sampling would be more
looking for shared things a group has
like income so if you're studying
something on poverty you might look at
their naturally group People based on
income to begin with and then study
those individuals in the income to find
out what kind of traits they have
and then judgmental that is where the
researcher very carefully selects each
member of their own group
so it's very much based on their
personal knowledge
jumping on the 26 what are the different
types of hypothesis testing
hypothesis testing is a procedure used
by estheticians and scientists to accept
or reject statistical hypothesis we
start with a hypothesis testing we have
null hypothesis and alternative
hypothesis
on the null hypothesis it states that
there is no relation between the
predictor and the outcome variables in
the population it is denoted by H naught
example there is no association between
patients BMI and diabetes
alternative hypothesis it states there
is some relation between the predictor
and outcome variables in the population
it is denoted by H1 example there could
be an association between patients BMI
and diabetes
and that's the body mass index if you
didn't catch the BMI and you're not in
medical
describe univariate bivariate and
multivariate Analysis
a unit variate analysis it is the
simplest form of data analysis where the
data being analyzed contains only one
variable an example is studying the
heights of players in the NBA because
it's so simple it can be described using
Central Tendencies dispersion quartiles
bar charts histograms pie charts
frequency distribution tables
the bivariate analysis it involves
analysis of two variables to find causes
relationships and correlations between
the variables example analyzing sale of
ice creams based on the temperature
outside
bivariate analysis can be explained
using correlation coefficients linear
regression logistic regression Scatter
Plots and box plots
and multivariate Analysis it involves
analysis of three or more variables to
understand the relationship of each
variable with the other variables
example analyzing Revenue based on
expenditure so if we have our TV ads we
have our newspaper ads our social media
ads and our Revenue we can now compare
all those together
the multiverted analysis can be
performed using multiple regression
factor analysis classification and
regression trees cluster analysis
principle component analysis clustering
bar chart dual axis chart
what function would you use to get the
current date and time in Excel
in Excel you can use that today and now
function to get the current date and
time and you can see down here with the
two examples are just equals today or
equals now
using the sum ifs function in Excel find
the total quantity sold by cells
Representatives whose names start with a
and the cost of each item they have sold
is greater than 10.
and you can see here on the left we have
our actual table
and then we want to go ahead and sumifs
so we want the E2 through E20
B2 through B20 greater than 10. and this
basically is just saying hey we're going
to take everything in the E column
and we're going to sum it up but only
those objects where the D column is
greater than 10 that's what that means
there
is the below query correct if not how
will you Rectify it
select customer ID year order date as
order Year from order where order year
is greater than or equal to 2016.
and hopefully you caught it right there
uh it's in the devils in the details we
can't not use the Alias name while
filtering data using the where Clause so
the correct format is all the same
except for where it says where the year
order date is greater than or equal to
16 versus using the order year which we
assign under the select setup
how are union intersect and accept used
in SQL the union operator is used to
combine the results of two or more
select statements
and you can see here we have select star
from region one and we're going to make
a union with select star from region two
and it basically takes both these SQL
tables and combines them to form a full
new table on there so that's your union
as we bring everything together
we look at the intersect operator
Returns the common records that are the
result of the two or more select
statements
so you can see here we select star from
region one intersect select star from
region two
and we come up with only those records
that are shared that have the same data
in them
and hopefully you jumped ahead to the
accept the accept operator returns The
Uncommon records that are the result of
two or more select statements so these
are the two records or the records that
are not shared between the two databases
using the product price table write an
SQL query to find the record with the
fourth highest market price
so here we have a little bit of a brain
teaser
they're always fun
and the first thing we want to do is
we're going to go ahead and I'm going to
if you look at the script on the left we
really want the fourth one down so we're
going to select the top four from
product price but we're going to order
it by Marketplace descending SP order by
market price ascending
so we do is we take the top four of the
market price ascending and that's going
to give us the four greatest values
and then we're going to reverse that
order and do descending and we're going
to take the top one of that which is
going to give us the lowest value which
will be the fourth greatest one in the
list
from the product price table find the
total and average market price for each
currency where the average market price
is greater than 100 and currency is in
the INR or the AUD
so INR or AUD India Rupal or Australia
dollar
you can see over here the SQL query if
you had trouble putting this together
you might actually do some of it in
reverse
and you can see right here where the
average market price is greater than 50.
remember we use having not where at the
end because it's part of the group so
Group by currency because we want those
two currencies and we want the currency
India the INR or the AUD
and as you keep going backwards we're
actually going to be selecting the
currency the sum of the market price as
total price and the average Marketplace
as average price so there's our select
it's going to come from the product
price which is just our table over there
and then we have where our currency is
in uh and like I said you can put it
together however you want but hopefully
you got to the end there
so this question will test your
knowledge in Tableau exploring the
different features of Tableau and
creating a suitable graph to solve a
business problem
and of course tab blue is very visual in
its use so it's very hard to test it
without actually just getting your hands
on and if you can't visualize some of
this and how to do it then you should go
back and refresh yourself
using the sample Superstore data set
Create A View to analyze the cells
profits and quantities sold across
different subcategories of items present
under each category so the first step is
to go ahead and load the sample
Superstore data set
so make sure you know how to load the
sample the superstore data set that's
underneath either the connect button in
the upper left or the Tableau icon up
there and be able to pull in the data
set and then once you've done that you
just drag the category and subcategory
on rows and salaries onto columns it
will result in a horizontal bar chart
so in this one we're just going to drag
profit onto color and quantity onto
label sort the sales axes in descending
order of sum and cells within each
subcategory
and if you're at home doing this you'll
see the chairs in their Furniture
category have the highest sales and
profit while tables had the lowest
profit for office supplies subcategory
binders made the highest profit even
though storage had the highest sales
under technology category copiers made
the highest profit though it was the
least amount of sales
let's work to create a dual axis chart
in Tableau to present cells and profits
across different years using sample
Superstore data set
load the orders sheet from the sample
Superstore data set
drag the ordered data field from the
dimensions onto columns and converted
into continuous months
drag cells onto rows and profits to the
right corner of the view until you see a
light green rectangle one of those
things if you haven't done this Hands-On
you don't know what you're doing you're
right into a buying so you can be just
kind of dropping it and wondering what
happened synchronize the right axes by
right-clicking on the profit axes
and then let's finalize it by going
under the marks card change some cells
to bar and some profit to line and
adjust the size
and then we have a nice display that we
can either print out or save and send
off to the shareholders
let's go and do one more Tableau design
a view in Tableau to show Statewide
cells and profits using the sample
Superstore data set
and here you go ahead and drag the
country field onto the view section and
expand it to see the states drag the
states field onto size and profit onto
color
increase the size of the Bubbles at a
border and a Halo color States like
Washington California and New York have
the highest sales and profits while
Texas Pennsylvania and Ohio have a good
amount of sales but the least amount of
profits
we'll go ahead and Skip back to python
numpy Suppose there is an array number
equals NP or numpy if you're using numpy
depending on how you set it up dot array
and we just have one to nine broken up
into three groups extract the value 8
using 2D indexing so you can see on the
left we have our import numpy is in p
number equals our NP array if we print
the number we have one two three four
five six seven eight nine since the
value 8 is present in the second row and
First Column we use the same index
position and pass it to the array and
you just have number two comma 1 and you
get eight and remember we're in Python
so you start at zero not one like you do
in Excel
always kiss me if I'm working between
Excel and python where I just kind of
flip and usually it's the Excel that
messes up because I do a lot more
programming
Suppose there is an array that has value
0 1 all the way up to nine how will you
display the following values from the
array one three five seven nine
uh so first of all we go ahead and
create the array NP dot a range of 10
which goes from 0 to 9 because there's
ten numbers in it but we don't include
the 10. we print it out the first thing
you want to do is what's going on here
with one three five seven nine well if
we divide by two there's going to be a
remainder equal to one and then from
python we remember if you use the
percentage sign you get the remainder on
there so the remainder is one and then
you have the your numpy array and then
we just want to do a logical statement
of all values that have a remainder of
one and that generates our nice one
three five seven nine
there are two arrays a and
bacteria is a and b horizontally boy
these horizontal vertical questions will
get you every time
and in numpy we go ahead and we've
created uh two different arrays over
here A and B uh the first one is your
concatenate NP dot concatenate
A and B on axes equal one
that is the same as H stack and in the
back end they're still identical they
run the same that's all each stack is a
concatenate and axes equals one
how can you add a column to a panda's
data frame
suppose there's an imp data frame that
has information about few employees
let's add address column to that data
frame and you can see on the left we
have our basic data frame you should
know your data frames very well
basically looks like an Excel
spreadsheet as you come over here it's
really simple you just do DF of address
equals the address once you've assigned
values to the address
using the below given data create a
pivot table to find the total cells made
by each cells represented for each item
display the cells as a percentage of the
grand total
so we're back in Tableau select the
entire table range click on insert Tab
and choose pivot table
select the table range and the worksheet
where you want to place the pivot table
it'll return a pivot table where you can
analyze your data
drag the cell total on the values and
sales rep and item onto row labels it'll
give the sum of the cells made by each
representative for each item they have
sold
and finally right click on sum of cell
total and expand show values as to
select percentage of grand total
real important just understand what a
pivot table is we're just pivoting it
from rows and columns and switching this
direction on there
and finally we have our final pivot
table and you can see the values rules
and sum of total sale
so we're going to go ahead and take a
product table this is off of an SQL so
we're going to do some SQL here
and we're going to use the product and
sales order detail table find the
products that have total units sold
greater than 1.5 million and here's our
sales order detail table so we have a
product table and a sales order detail
table two separate tables in the
database
and we're going to do is put together
the SQL query we want to select PP name
sum sod unit price as cells and then we
have our p p dot product ID from
production product as PP inner join
sales dot sales order detail as sod on
PP product ID equals sod.product ID
Group by pp.name comma pp.product ID
having a sum of sod.uniprice greater
than the 150 million there that's a
mouthful and again these SQL queries
they start looking really crazy until
you just break them apart and do them
step by step
and what we're looking for is the inner
join and how did you do the group by
this really wanted to know how do you do
this inner join this comes up so much in
SQL how do you pull in the ID from one
chart and the information from another
chart and the sum totals on that chart
how do you write a stored procedure in
SQL let's create a storage procedure to
find the sum the squares are the first n
natural numbers so here we have our
formula n times n plus 1 times 2N plus 1
over 6. and you can see from the command
prompt uh or the setup you have which
depending on what your login is the
command is create procedure Square sum1
declare our variable at n of integer as
begin then we're going to declare the
sum of integer set sum equals n times n
plus 1 plus 2 times n plus 1 over 6.
of course we can go ahead and print
those out print First cast
member sign in or our variable as a
variable character 20 natural numbers
print the sum of the square is cast the
at sum as a variable character 40 and
then we do the output display the sum of
the square for first four natural
numbers we have execute Square sum 1 and
then we're going to put in 4 and you can
see here where it brings up the first
four natural numbers sum of square is
30.
write a store procedure to find the
total even number between two user given
numbers
couple things to note here first we go
and create our procedure you have your
two different variables the N1 N2 and we
go ahead and begin we're going to
declare our variable count as an integer
we're going to set count equal to zero
and then we have while n is less than N2
we're going to begin and if N1 remainder
2 equals 0 so we're going to divide it
by two even number begin we're going to
set the count equal to count plus one
we're going to print even number plus
cast n as a variable character 10 for
printing count is plus cast variable
count as variable character 10 end else
print odd number plus cast variable
number one as variable character ten and
then we go ahead and set the increment
our variable one up one so it'll go from
in one all the way to N2 and I'll print
the total number of even numbers
and you can see here we went ahead and
executed it we're going to count the
even numbers between 30 and 45 and you
can see it goes all the way down to
eight
what is the difference between tree maps
and heat maps in tableau
now if you've worked in Python other
programmings you should automatically
know what a heat map is but a tree map
are used to display data in nested
rectangles use Dimensions to define the
structure of the tree map and measure to
define the size or color of individual
rectangles
tree maps are relatively simple data
visualization that can provide Insight
in a visually attractive format
and again you can see the squares over
here this is our tree map over here with
the each block also has this information
inside of its different blocks
a heat map helps to visualize measures
against Dimensions with the help of
colors and size to compare one or more
dimensions and up to two measures the
layout is similar to a text table with
variations in values encoded as colors
in heat map you can quickly see a wide
array of information
and you can see they use the colors to
denote one thing and the size of the
little square to denote something else
a lot of times you can even graph this
into a three-dimensional graph with
other data so it pops out but again a
heat map is the color and the size
using the sample Superstore data set
display the top five and bottom five
customers based on their profit so you
start by dragging the customer name
field onto rows and profit on columns
right click on the customer name column
to create a set
give a name to the set and select top
tab to choose top 5 customers by some
profit similarly create a set for the
bottom five customers by some profit
select both the sets right click to
create a combined set give a name to the
set and choose all members in both sets
and then you can drag top and bottom
customer sets onto the filters and
profit field onto color to get the
desired results
as we get down to the end of our list
we're going to try to keep you on your
toes we're going to skip back to numpy
how to print four random integers
between 1 and 15 using numpy to Generate
random numbers using numpy we use a
random random integer function and you
see here we did the import numpy as in p
random Arrangement equals np.random dot
random integer 1 through 15 of 4.
from the below data frame I'm going to
jump again on you now we're into pandas
how will you find the unique values for
each column and subset the data for age
less than 35 and height greater than 6.
to find the unique values and the number
of unique elements use the unique and
the in unique function
you can see here we just did DF Heights
we're selecting just the height column
and we want to look for The Unique that
returns an array or in unique if we do
that on the height or the age we'll
return just the number of unique values
and then we can do a subset the data for
ages less than 35 and height greater
than 6. so if we look over here we have
a new DF remember this is going to be
taking slices of our original data frame
it doesn't actually change the data
frame so our new DF equals the data
frame or DF the data frame where age is
less than 35
and the height is greater than 6.
and in case you're not using uh tab blue
which has a lot of its own uh different
mapping programs in there make sure you
understand how to use the basics of
matplot Library plot a sine graph using
numpy and matplot library in Python and
the way we did this is we went ahead and
generated an X we know our y equals NP
dot sine of x if you print out X you'll
see a whole value here our matplot
library Pi plot as PLT if you are
working in Jupiter notebook make sure
you understand the matplot library
inline that little percentage sign
matplot Library Online that prints it on
the page in the jupyter notebook the
newer version of jupyter notebook or
Jupiter Labs automatically does that for
you but I usually put it in there just
in case I end up on an older version
if you print y you can see here we have
our different y values and our different
X values
you simply put in plt.plot x y and do a
plot show
and before we go let's get one more in
we're going to do a pandas using the
below pandas data frame find the company
with the highest average cells derive
the summary statistics for the cells
column and transpose these statistics
that's a mouthful and just like any of
these computer problems break it apart
so first of all we're looking for the
highest average cells so group the
company column and use the mean function
to find the average cells you see here
by company equals df.group by company
once we've done that using the describe
function we can now go ahead and look at
the summary of statistics on here use
the describe function to find the
summary so by company those are groups
we're just going to describe them and
you could actually bundle those together
if you wanted and just do a Mullen one
line so here we go by company dot
describe you can see we have a nice
breakout always good to remember whether
you're using any of the packages whether
it's tab blue or pandas in python or
even r or some other package being able
to quick look and describe your data is
very important and then we go ahead and
just do a basic apply a transpose
function over the describe method to
transpose the statistics
all we've done here is flip the index
with the column names but if you're
following the numbers a lot of times
it's easier to follow across one line or
maybe you want to average out the count
or it's all kinds of different reasons
to do that
at the Caltech data analytics bootcamp
you will learn everything you need to
know to succeed as a data analyst from
mastering Excel to creating data driven
presentations manipulating data with SQL
analyzing data with python and
visualizing data with W so that's it for
this video on data analyst bootcamp we
hope you found it informative and
helpful in your journey towards a career
in data analysis if you are interested
in pursuing a career in this field we
highly recommend considering a boot camp
as a strong point with the right
training and skill set you can make a
real impact in the business world and
enjoy a lucrative and rewarding career
thanks for watching and don't forget to
like And subscribe for more content like
this
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here