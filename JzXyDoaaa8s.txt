1
00:00:00,240 --> 00:00:02,280
so hello there and welcome to another

2
00:00:02,280 --> 00:00:04,859
tutorial my name is Tammy Bakshi and

3
00:00:04,859 --> 00:00:06,660
today we're gonna be going over how you

4
00:00:06,660 --> 00:00:10,200
can use class activation maps in order

5
00:00:10,200 --> 00:00:12,630
to figure out where your convolutional

6
00:00:12,630 --> 00:00:15,089
neural networks are looking in Karass

7
00:00:15,089 --> 00:00:17,130
let's go ahead and take a look at how

8
00:00:17,130 --> 00:00:19,859
this works now I'm sure you're aware of

9
00:00:19,859 --> 00:00:21,990
the attention mechanism using the

10
00:00:21,990 --> 00:00:23,730
attention mechanism you can find out

11
00:00:23,730 --> 00:00:25,769
where it is that your neural network is

12
00:00:25,769 --> 00:00:28,859
focusing is well attention now this

13
00:00:28,859 --> 00:00:30,720
works great for recurrent neural

14
00:00:30,720 --> 00:00:33,420
networks and multi-layer perceptrons or

15
00:00:33,420 --> 00:00:34,200
also called

16
00:00:34,200 --> 00:00:36,230
dense neural networks in cross terms

17
00:00:36,230 --> 00:00:39,090
however they don't work very well for

18
00:00:39,090 --> 00:00:41,610
convolutional networks now what am I

19
00:00:41,610 --> 00:00:43,649
talking about let's go ahead and take a

20
00:00:43,649 --> 00:00:46,500
look at an example of attention for a

21
00:00:46,500 --> 00:00:48,600
dense Network now with a dense Network

22
00:00:48,600 --> 00:00:50,969
you are essentially trying to take a

23
00:00:50,969 --> 00:00:53,550
feature vector and you're trying to

24
00:00:53,550 --> 00:00:55,739
classify those features into a few

25
00:00:55,739 --> 00:00:56,879
different categories or maybe do a

26
00:00:56,879 --> 00:00:59,010
regression but let's just take a look at

27
00:00:59,010 --> 00:01:01,579
the example of classification for now

28
00:01:01,579 --> 00:01:05,519
let's just say that we've got a neural

29
00:01:05,519 --> 00:01:07,380
network where you've got the features

30
00:01:07,380 --> 00:01:11,760
alright so you have features all right

31
00:01:11,760 --> 00:01:14,610
and these features need to be fed into

32
00:01:14,610 --> 00:01:18,990
your dense layer now on the way to the

33
00:01:18,990 --> 00:01:21,540
dense layer you can go ahead and do some

34
00:01:21,540 --> 00:01:23,250
really interesting stuff like for

35
00:01:23,250 --> 00:01:25,290
example instead of sending this to

36
00:01:25,290 --> 00:01:28,500
another dense right instead of sending

37
00:01:28,500 --> 00:01:31,290
that to say a soft max for

38
00:01:31,290 --> 00:01:34,560
classification instead what you do is

39
00:01:34,560 --> 00:01:37,290
you do a little bit of a skip here you

40
00:01:37,290 --> 00:01:39,960
erase these two layers for now but we'll

41
00:01:39,960 --> 00:01:42,570
get back to them in just a moment then

42
00:01:42,570 --> 00:01:45,420
you go ahead and right after this dense

43
00:01:45,420 --> 00:01:50,939
layer you add a soft Max okay now what

44
00:01:50,939 --> 00:01:54,030
you do is you keep this dense layer the

45
00:01:54,030 --> 00:01:57,149
same number as the same number of units

46
00:01:57,149 --> 00:01:59,369
as there are features that you're

47
00:01:59,369 --> 00:02:01,890
feeding in so for example if you're

48
00:02:01,890 --> 00:02:03,810
working with the iris data set and

49
00:02:03,810 --> 00:02:06,899
you've got you know a three length

50
00:02:06,899 --> 00:02:09,450
feature vector coming in and let's just

51
00:02:09,450 --> 00:02:11,190
say you want to reduce that's where you

52
00:02:11,190 --> 00:02:13,050
want to go and put attention then you

53
00:02:13,050 --> 00:02:13,819
don't read

54
00:02:13,819 --> 00:02:16,159
all right in that case you pretty dense

55
00:02:16,159 --> 00:02:18,530
that has an output shape of three as

56
00:02:18,530 --> 00:02:21,409
well so you're not producing you're just

57
00:02:21,409 --> 00:02:23,540
going from three features to three

58
00:02:23,540 --> 00:02:26,239
features gaming nets now you're gonna go

59
00:02:26,239 --> 00:02:27,950
ahead and put in a soft max so you're

60
00:02:27,950 --> 00:02:29,659
probably thinking well how would I train

61
00:02:29,659 --> 00:02:31,790
this neural network what kind of output

62
00:02:31,790 --> 00:02:34,819
are we gonna expect here well this isn't

63
00:02:34,819 --> 00:02:38,000
where neural network ends even though

64
00:02:38,000 --> 00:02:40,819
there's a sock max later now what's the

65
00:02:40,819 --> 00:02:43,489
output going to be with a soft max think

66
00:02:43,489 --> 00:02:45,590
about that let's just say we've got

67
00:02:45,590 --> 00:02:47,930
three values this is not necessarily

68
00:02:47,930 --> 00:02:49,340
what you feed into a neural network I'm

69
00:02:49,340 --> 00:02:50,900
just thinking of random numbers say

70
00:02:50,900 --> 00:02:55,189
three six and one okay these are three

71
00:02:55,189 --> 00:02:57,049
different numbers room they put commas

72
00:02:57,049 --> 00:02:59,930
between them now these three different

73
00:02:59,930 --> 00:03:03,319
numbers let's just say we ran these room

74
00:03:03,319 --> 00:03:05,090
you know through this dance and through

75
00:03:05,090 --> 00:03:07,280
the softmax what would the end result be

76
00:03:07,280 --> 00:03:09,709
well it would be a bunch of numbers from

77
00:03:09,709 --> 00:03:13,700
0 to 1 the sum of which would be a 1 so

78
00:03:13,700 --> 00:03:17,030
it's a probability matrix now let's go

79
00:03:17,030 --> 00:03:18,109
ahead and take a look at what those

80
00:03:18,109 --> 00:03:24,069
values might be maybe it's 0.15 0.25 and

81
00:03:24,069 --> 00:03:28,220
0.4 or 0.6 actually and that would add

82
00:03:28,220 --> 00:03:29,079
up to 1

83
00:03:29,079 --> 00:03:32,180
now what's so interesting about the

84
00:03:32,180 --> 00:03:34,549
attention technique is you can go ahead

85
00:03:34,549 --> 00:03:38,530
and take these softmax values and

86
00:03:38,530 --> 00:03:41,810
multiply them by the actual values

87
00:03:41,810 --> 00:03:44,900
themselves and once you do that you've

88
00:03:44,900 --> 00:03:47,389
got weighted features that you can go

89
00:03:47,389 --> 00:03:49,849
ahead and feed into another view dense

90
00:03:49,849 --> 00:03:52,280
layers so basically what you're gonna do

91
00:03:52,280 --> 00:03:53,810
now is you're gonna have a

92
00:03:53,810 --> 00:03:57,139
multiplication layer okay so let's just

93
00:03:57,139 --> 00:04:01,120
say we branch out here and we have a

94
00:04:01,120 --> 00:04:04,340
multiply layer all right they're going

95
00:04:04,340 --> 00:04:06,439
to multiply the output of that softmax

96
00:04:06,439 --> 00:04:09,109
activation by the features that you

97
00:04:09,109 --> 00:04:11,659
originally fed it in then you're going

98
00:04:11,659 --> 00:04:13,699
to have weighted features that you can

99
00:04:13,699 --> 00:04:15,739
go ahead and feed right back into

100
00:04:15,739 --> 00:04:16,930
another

101
00:04:16,930 --> 00:04:19,449
dense Network all right and then you can

102
00:04:19,449 --> 00:04:21,100
go ahead and do your final

103
00:04:21,100 --> 00:04:25,120
classification so for example if you

104
00:04:25,120 --> 00:04:29,160
want to have another softmax layer I

105
00:04:29,160 --> 00:04:32,110
have another softmax layer and this

106
00:04:32,110 --> 00:04:34,060
softmax leaders given to the final

107
00:04:34,060 --> 00:04:35,830
classification so you're going to have

108
00:04:35,830 --> 00:04:39,940
you know one of one of say two classes

109
00:04:39,940 --> 00:04:41,560
here let's not take a look at the iris

110
00:04:41,560 --> 00:04:42,699
dataset let's just say we're classifying

111
00:04:42,699 --> 00:04:45,880
into two classes instead of three then

112
00:04:45,880 --> 00:04:47,169
they're gonna have your final

113
00:04:47,169 --> 00:04:52,449
classification down here now this is

114
00:04:52,449 --> 00:04:54,520
well and good prevents neural networks

115
00:04:54,520 --> 00:04:56,560
and there's a similar technique for the

116
00:04:56,560 --> 00:04:58,840
current neural networks as well however

117
00:04:58,840 --> 00:05:01,300
convolutional neural networks well they

118
00:05:01,300 --> 00:05:03,430
don't play very well with this technique

119
00:05:03,430 --> 00:05:05,620
in my experience you know whenever you

120
00:05:05,620 --> 00:05:07,270
would try this kind of technique it

121
00:05:07,270 --> 00:05:09,280
simply wasn't scalable enough or it

122
00:05:09,280 --> 00:05:11,470
wouldn't work well enough to be worth

123
00:05:11,470 --> 00:05:15,160
the extra effort however there is still

124
00:05:15,160 --> 00:05:18,280
a way that you can visualize where your

125
00:05:18,280 --> 00:05:20,250
neural network is paying attention

126
00:05:20,250 --> 00:05:22,930
without forcing it to pay attention like

127
00:05:22,930 --> 00:05:25,300
for example over here what you would do

128
00:05:25,300 --> 00:05:27,610
is you would force the neural network to

129
00:05:27,610 --> 00:05:30,340
pay attention and order the training the

130
00:05:30,340 --> 00:05:34,020
attention layers here and here however

131
00:05:34,020 --> 00:05:36,250
what happens this time of the

132
00:05:36,250 --> 00:05:38,229
convolutional neural networks and class

133
00:05:38,229 --> 00:05:41,020
activation mapping is you're not forcing

134
00:05:41,020 --> 00:05:42,580
the neural network to pay attention

135
00:05:42,580 --> 00:05:45,520
you're just figuring out where it is of

136
00:05:45,520 --> 00:05:48,190
the neural network pay attention so now

137
00:05:48,190 --> 00:05:50,349
how does this work let's take a look now

138
00:05:50,349 --> 00:05:52,810
first of all we've got to understand how

139
00:05:52,810 --> 00:05:55,120
do convolutional neural networks work in

140
00:05:55,120 --> 00:05:58,300
the first place now convolutional neural

141
00:05:58,300 --> 00:06:00,970
networks are interesting the reason I

142
00:06:00,970 --> 00:06:03,849
say that is let's just say we feed in

143
00:06:03,849 --> 00:06:06,909
one image now this image could be say

144
00:06:06,909 --> 00:06:11,560
you know 224 by 224 in size that's a

145
00:06:11,560 --> 00:06:14,650
pretty standard size for a neural

146
00:06:14,650 --> 00:06:17,050
network now the problem with the snow

147
00:06:17,050 --> 00:06:19,449
all Network it or the novice or the

148
00:06:19,449 --> 00:06:21,580
problem but an interesting thing is that

149
00:06:21,580 --> 00:06:23,050
you're not just gonna be

150
00:06:23,050 --> 00:06:25,300
in this one image into the neural

151
00:06:25,300 --> 00:06:27,159
network that's not how it works

152
00:06:27,159 --> 00:06:29,229
well you're gonna be doing is you're

153
00:06:29,229 --> 00:06:33,069
gonna be feeding three images into the

154
00:06:33,069 --> 00:06:35,289
neural network three images that

155
00:06:35,289 --> 00:06:39,180
represent this one image so for example

156
00:06:39,180 --> 00:06:42,400
let's just say that you've got a regular

157
00:06:42,400 --> 00:06:43,060
image

158
00:06:43,060 --> 00:06:47,830
it's an RGB image right it's a red green

159
00:06:47,830 --> 00:06:51,520
and blue image you've got three

160
00:06:51,520 --> 00:06:53,229
different channels there so you're

161
00:06:53,229 --> 00:06:55,750
actually going to be taking three

162
00:06:55,750 --> 00:06:58,629
different images right your red your

163
00:06:58,629 --> 00:07:00,909
green your blue and you're gonna be

164
00:07:00,909 --> 00:07:04,259
feeding those as channels into your

165
00:07:04,259 --> 00:07:06,669
convolutional layer let's just say you

166
00:07:06,669 --> 00:07:08,680
know in kernofs that would be a comm 2d

167
00:07:08,680 --> 00:07:10,539
layer and that's what you're going to be

168
00:07:10,539 --> 00:07:16,240
doing now let's just ignore this input

169
00:07:16,240 --> 00:07:18,280
part for now let's get back to the

170
00:07:18,280 --> 00:07:20,650
convolutional neural network this time

171
00:07:20,650 --> 00:07:22,810
we're not gonna be taking a look at the

172
00:07:22,810 --> 00:07:24,250
beginning of the neural network we're

173
00:07:24,250 --> 00:07:25,539
going to be taking a look at the very

174
00:07:25,539 --> 00:07:29,349
end of the neural network so not towards

175
00:07:29,349 --> 00:07:31,240
the end of course let's just say we draw

176
00:07:31,240 --> 00:07:32,710
a whole neural network right you've got

177
00:07:32,710 --> 00:07:37,389
you've got convolutional layers you go

178
00:07:37,389 --> 00:07:41,069
down and you have more convolutional

179
00:07:41,069 --> 00:07:44,409
laters okay we'll see it more and more

180
00:07:44,409 --> 00:07:46,360
and more and usually your filters will

181
00:07:46,360 --> 00:07:47,949
get more and more of a time the number

182
00:07:47,949 --> 00:07:52,389
of channels will increase right this is

183
00:07:52,389 --> 00:07:54,159
not what an actual neural network

184
00:07:54,159 --> 00:07:55,330
architecture would look like it's

185
00:07:55,330 --> 00:07:56,800
probably going to be very well optimized

186
00:07:56,800 --> 00:07:59,110
but let's just say that you've got two

187
00:07:59,110 --> 00:08:01,449
to two layers with there two builders

188
00:08:01,449 --> 00:08:05,080
then 164 and then one with 128 filters

189
00:08:05,080 --> 00:08:08,620
now that means there are 128 channels in

190
00:08:08,620 --> 00:08:11,759
the final in the final convolutional

191
00:08:11,759 --> 00:08:16,690
Network you've got 128 images that your

192
00:08:16,690 --> 00:08:18,849
neural network is working with I say

193
00:08:18,849 --> 00:08:20,770
images in a loose sense because it's

194
00:08:20,770 --> 00:08:22,779
actually just 120 different channels

195
00:08:22,779 --> 00:08:24,759
that your neural network is working with

196
00:08:24,759 --> 00:08:26,139
you could of course reduce or increase

197
00:08:26,139 --> 00:08:28,300
that based on your neural network but

198
00:08:28,300 --> 00:08:32,169
that's not a problem for now now over

199
00:08:32,169 --> 00:08:34,120
time unless you're doing things like

200
00:08:34,120 --> 00:08:36,220
zero padding your neural

201
00:08:36,220 --> 00:08:38,830
or reduce the dimensions of the image

202
00:08:38,830 --> 00:08:40,330
that's just how these convolutional

203
00:08:40,330 --> 00:08:42,460
torques work of course you could put

204
00:08:42,460 --> 00:08:44,080
zeros on the outside of the image and

205
00:08:44,080 --> 00:08:45,880
that's how you would usually keep the

206
00:08:45,880 --> 00:08:47,710
image size the same for example in

207
00:08:47,710 --> 00:08:49,630
ResNet for skip connections you kind of

208
00:08:49,630 --> 00:08:53,590
need to do that however for this were

209
00:08:53,590 --> 00:08:56,230
let's just assume that the size is going

210
00:08:56,230 --> 00:08:58,990
down over time so if we start off with

211
00:08:58,990 --> 00:09:04,150
say of 224 5 224 here this is going to

212
00:09:04,150 --> 00:09:07,900
go down to 222 by 222 then it's going to

213
00:09:07,900 --> 00:09:11,530
go down to 220 by 220 and then it's

214
00:09:11,530 --> 00:09:15,130
going to go down to 218 by two hundred

215
00:09:15,130 --> 00:09:19,440
and eighteen assuming that it is a 3x3

216
00:09:19,440 --> 00:09:23,470
filter size all the way down now the

217
00:09:23,470 --> 00:09:25,960
interesting part here again is that

218
00:09:25,960 --> 00:09:28,540
towards the end let's just say you know

219
00:09:28,540 --> 00:09:30,880
for the sake of simplicity for now that

220
00:09:30,880 --> 00:09:33,550
we're working with a smaller set of

221
00:09:33,550 --> 00:09:37,660
images let's see we're working with 28

222
00:09:37,660 --> 00:09:41,980
by 28 all right and the 28 by 28 becomes

223
00:09:41,980 --> 00:09:46,870
26 by 26 which becomes 24 by 24 which

224
00:09:46,870 --> 00:09:51,970
becomes 22 by 22 all right now what

225
00:09:51,970 --> 00:09:58,030
happens at the end is we've got 22 by 22

226
00:09:58,030 --> 00:10:02,350
by 128 set of data all right this is

227
00:10:02,350 --> 00:10:04,240
basically an umpire layer with this

228
00:10:04,240 --> 00:10:09,030
shape okay now what if there were way to

229
00:10:09,030 --> 00:10:12,010
visualize the weights that the neural

230
00:10:12,010 --> 00:10:15,130
network gives for each of these

231
00:10:15,130 --> 00:10:19,530
individual filters at the end okay

232
00:10:19,530 --> 00:10:24,310
towards the final classification okay

233
00:10:24,310 --> 00:10:26,589
that's what we're doing here with class

234
00:10:26,589 --> 00:10:29,140
activation mapping there's a special

235
00:10:29,140 --> 00:10:31,420
kind of pooling you've probably heard of

236
00:10:31,420 --> 00:10:34,000
maximum cooling right you actually take

237
00:10:34,000 --> 00:10:37,120
say let's just say you've got a six by

238
00:10:37,120 --> 00:10:39,210
six set of data all right so you have

239
00:10:39,210 --> 00:10:42,910
six different or let's just say you've

240
00:10:42,910 --> 00:10:46,570
got a four by four agree all right now

241
00:10:46,570 --> 00:10:47,889
let's just say

242
00:10:47,889 --> 00:10:50,879
that this 4x4 grid or this four by three

243
00:10:50,879 --> 00:10:58,509
grid here so we might 4x4 grid let's

244
00:10:58,509 --> 00:11:00,759
just say that you want a max pool with a

245
00:11:00,759 --> 00:11:03,639
window size of two by two so what you're

246
00:11:03,639 --> 00:11:05,439
doing is you're taking the values from

247
00:11:05,439 --> 00:11:07,480
this window you're taking the values

248
00:11:07,480 --> 00:11:09,579
from this window you're taking the

249
00:11:09,579 --> 00:11:11,410
values from this window and you're

250
00:11:11,410 --> 00:11:14,790
taking the values from this window and

251
00:11:14,790 --> 00:11:17,350
you're getting the average of those

252
00:11:17,350 --> 00:11:19,839
groups and then you're putting them into

253
00:11:19,839 --> 00:11:23,019
a new image this new image is simply

254
00:11:23,019 --> 00:11:29,019
saying let's see here two by two in size

255
00:11:29,019 --> 00:11:33,309
and this is the result all right you

256
00:11:33,309 --> 00:11:35,109
know how to do this this is simple

257
00:11:35,109 --> 00:11:37,029
convolutional neural network stuff and

258
00:11:37,029 --> 00:11:39,040
it's stuff that you've been doing for a

259
00:11:39,040 --> 00:11:41,230
long time if you work with convolutional

260
00:11:41,230 --> 00:11:43,929
neural networks the global average

261
00:11:43,929 --> 00:11:46,359
pooling is another really interesting

262
00:11:46,359 --> 00:11:48,970
kind of ruling and what it lets you do

263
00:11:48,970 --> 00:11:53,100
is take this whole 4x4 grid alright and

264
00:11:53,100 --> 00:11:56,439
bring that down to not four different

265
00:11:56,439 --> 00:12:00,389
data points but instead just one

266
00:12:00,389 --> 00:12:03,189
individual data point so what you're

267
00:12:03,189 --> 00:12:04,839
gonna be doing is not getting the

268
00:12:04,839 --> 00:12:06,669
maximum value from any individual group

269
00:12:06,669 --> 00:12:08,319
you're gonna be getting the whole

270
00:12:08,319 --> 00:12:11,019
average value of this whole filter and

271
00:12:11,019 --> 00:12:12,489
you're going to be getting that down

272
00:12:12,489 --> 00:12:16,360
into one individual value that consists

273
00:12:16,360 --> 00:12:19,209
of the average of all the values that

274
00:12:19,209 --> 00:12:22,899
you just collected now what you can do

275
00:12:22,899 --> 00:12:24,850
with the possibilities that this opens

276
00:12:24,850 --> 00:12:27,339
up is that well let's just say the enemy

277
00:12:27,339 --> 00:12:31,360
here right by 22 by 22 by 128 gauge with

278
00:12:31,360 --> 00:12:34,029
global average pooling using no weights

279
00:12:34,029 --> 00:12:36,369
just a slight averaging method for each

280
00:12:36,369 --> 00:12:40,660
of those 128 channels you get down to a

281
00:12:40,660 --> 00:12:47,739
1 by 1 by 128 set of data now with this

282
00:12:47,739 --> 00:12:51,699
one by one by 128 set of data you can go

283
00:12:51,699 --> 00:12:53,999
ahead and assume that of course each

284
00:12:53,999 --> 00:12:56,410
individual data point right let's just

285
00:12:56,410 --> 00:13:00,279
say there are 128 there are 3

286
00:13:00,279 --> 00:13:04,420
here so let's just say there are 125 of

287
00:13:04,420 --> 00:13:08,019
these you know data points going forward

288
00:13:08,019 --> 00:13:09,879
and let's just say you have two

289
00:13:09,879 --> 00:13:12,040
classifications right your two classes

290
00:13:12,040 --> 00:13:14,470
that you're trying to classify into over

291
00:13:14,470 --> 00:13:16,899
here and over here these are your dense

292
00:13:16,899 --> 00:13:20,769
units you can connect directly from this

293
00:13:20,769 --> 00:13:22,839
global average pooling and flattening

294
00:13:22,839 --> 00:13:29,499
layer to those not to those units now

295
00:13:29,499 --> 00:13:32,050
I'm sure you're starting to see what I'm

296
00:13:32,050 --> 00:13:35,410
pointing towards if the final activation

297
00:13:35,410 --> 00:13:38,550
here is softmax or really anything else

298
00:13:38,550 --> 00:13:41,559
what you've just done is you have

299
00:13:41,559 --> 00:13:45,249
provided a weight okay you've provided a

300
00:13:45,249 --> 00:13:50,800
weight value for each individual filter

301
00:13:50,800 --> 00:13:52,930
because remember each of these

302
00:13:52,930 --> 00:13:55,329
individual data points came from one

303
00:13:55,329 --> 00:13:57,819
bigger filter in the actual

304
00:13:57,819 --> 00:14:00,519
convolutional neural network so all of

305
00:14:00,519 --> 00:14:01,839
these came from different filters that

306
00:14:01,839 --> 00:14:04,689
found different shapes or edges in the

307
00:14:04,689 --> 00:14:08,399
image and these represent those filters

308
00:14:08,399 --> 00:14:11,889
so if you could somehow exploit the way

309
00:14:11,889 --> 00:14:13,899
that you're actually capturing those

310
00:14:13,899 --> 00:14:16,329
weights or individual filters to the

311
00:14:16,329 --> 00:14:17,949
actual units that have different

312
00:14:17,949 --> 00:14:20,069
meanings to you or your classification

313
00:14:20,069 --> 00:14:22,839
you can go ahead and assume that the

314
00:14:22,839 --> 00:14:25,000
filters these represent contain

315
00:14:25,000 --> 00:14:27,550
meaningful information asks of a

316
00:14:27,550 --> 00:14:31,059
relationship between a certain class so

317
00:14:31,059 --> 00:14:33,670
basically what I'm trying to say is that

318
00:14:33,670 --> 00:14:36,699
you were safe all right this is a dog

319
00:14:36,699 --> 00:14:39,579
unit and this is a cat unit in terms of

320
00:14:39,579 --> 00:14:42,370
dog versus cap classification if you go

321
00:14:42,370 --> 00:14:43,870
ahead and take a look at the weights

322
00:14:43,870 --> 00:14:46,329
from the dog unit took all of the

323
00:14:46,329 --> 00:14:48,459
different filters that you've fed into

324
00:14:48,459 --> 00:14:51,279
that one individual unit whichever ones

325
00:14:51,279 --> 00:14:54,959
are higher have a good representation of

326
00:14:54,959 --> 00:14:57,639
where the doggies in the image or what

327
00:14:57,639 --> 00:14:59,499
the dog looks like the ones that are

328
00:14:59,499 --> 00:15:01,689
lower are capturing information that is

329
00:15:01,689 --> 00:15:04,679
related to anything but the dog and that

330
00:15:04,679 --> 00:15:07,209
is what I'm going to be showing you how

331
00:15:07,209 --> 00:15:09,480
to use in cross

332
00:15:09,480 --> 00:15:11,519
sounds really complex but it's not

333
00:15:11,519 --> 00:15:14,670
nearly as complex as you may imagine now

334
00:15:14,670 --> 00:15:16,649
what I may be doing is showing you how

335
00:15:16,649 --> 00:15:18,930
exactly you can get a good class

336
00:15:18,930 --> 00:15:21,209
activation mapping designing neural

337
00:15:21,209 --> 00:15:22,290
network architectures that are

338
00:15:22,290 --> 00:15:24,240
specifically built toward great with

339
00:15:24,240 --> 00:15:26,250
clouds activation mapping so you can do

340
00:15:26,250 --> 00:15:28,920
object detection directly within your

341
00:15:28,920 --> 00:15:31,079
object recognition or object

342
00:15:31,079 --> 00:15:33,089
classification workflows this could be

343
00:15:33,089 --> 00:15:36,360
expanded to a lot of other places as

344
00:15:36,360 --> 00:15:37,920
well but we're gonna be covering

345
00:15:37,920 --> 00:15:39,839
convolutional neural networks for image

346
00:15:39,839 --> 00:15:42,329
classification in specific today all

347
00:15:42,329 --> 00:15:44,519
right so now without any further ado

348
00:15:44,519 --> 00:15:46,769
let's go ahead and get into the code

349
00:15:46,769 --> 00:15:47,970
part where I'm going to show you how you

350
00:15:47,970 --> 00:15:50,610
can implement this system and take a

351
00:15:50,610 --> 00:15:52,440
look at where your neural network is

352
00:15:52,440 --> 00:15:54,870
actually looking all right so welcome

353
00:15:54,870 --> 00:15:57,149
back and now let's take a look at the

354
00:15:57,149 --> 00:15:59,130
actual code that goes behind this now

355
00:15:59,130 --> 00:16:00,750
first of all let's take a look at the

356
00:16:00,750 --> 00:16:02,970
neural network architecture so let's go

357
00:16:02,970 --> 00:16:05,370
ahead and open up first of all before we

358
00:16:05,370 --> 00:16:06,540
get you into the neural network

359
00:16:06,540 --> 00:16:09,389
architecture the data loading sprint now

360
00:16:09,389 --> 00:16:12,019
of course in order to do this CatDog

361
00:16:12,019 --> 00:16:14,910
classification we're going to need data

362
00:16:14,910 --> 00:16:16,560
this data has been downloaded from

363
00:16:16,560 --> 00:16:19,170
cackled and you can do so as well now

364
00:16:19,170 --> 00:16:20,970
this is a bunch of really simple code

365
00:16:20,970 --> 00:16:22,500
that I've put together that essentially

366
00:16:22,500 --> 00:16:25,410
uses multi processing as well as numpy

367
00:16:25,410 --> 00:16:28,680
glob and pillow in order to load a bunch

368
00:16:28,680 --> 00:16:31,010
of images and then save them to the disk

369
00:16:31,010 --> 00:16:36,060
as dogs are n py and cats n py it loads

370
00:16:36,060 --> 00:16:40,319
them in at 224 by 224 resolution and it

371
00:16:40,319 --> 00:16:42,360
does the necessary pre-processing like

372
00:16:42,360 --> 00:16:45,029
for example dividing the RGB values by

373
00:16:45,029 --> 00:16:48,209
255 to make sure they're good ranges for

374
00:16:48,209 --> 00:16:51,149
the neural networks to capture now if I

375
00:16:51,149 --> 00:16:52,829
were to go ahead from here as you can

376
00:16:52,829 --> 00:16:54,660
see basically it's just using a multi

377
00:16:54,660 --> 00:16:57,480
processing pool in order to import

378
00:16:57,480 --> 00:17:00,839
images very very quickly of course multi

379
00:17:00,839 --> 00:17:02,550
processing multi-threading these kinds

380
00:17:02,550 --> 00:17:04,650
of things in Python don't usually work

381
00:17:04,650 --> 00:17:06,750
very well together but in this case it

382
00:17:06,750 --> 00:17:08,059
gave enough of a performance improvement

383
00:17:08,059 --> 00:17:11,790
to be worth in the extra hassle now if I

384
00:17:11,790 --> 00:17:13,589
go ahead and exit out of this here we

385
00:17:13,589 --> 00:17:16,110
can get to the real main part which is

386
00:17:16,110 --> 00:17:19,020
the actual neural network training that

387
00:17:19,020 --> 00:17:21,230
I do now the neural network training

388
00:17:21,230 --> 00:17:22,750
since I had

389
00:17:22,750 --> 00:17:25,810
eight different GPUs on the dgx one I

390
00:17:25,810 --> 00:17:28,060
can actually go ahead and build the

391
00:17:28,060 --> 00:17:30,670
model on my CPU and that go ahead and

392
00:17:30,670 --> 00:17:33,610
train it on the GPU or specifically

393
00:17:33,610 --> 00:17:36,430
eight GPUs and because of that I take my

394
00:17:36,430 --> 00:17:39,190
batch size which is 64 multiply it by 8

395
00:17:39,190 --> 00:17:41,470
so that of course I mean that 6 + 4

396
00:17:41,470 --> 00:17:43,840
times 8 that's 512 being done in a

397
00:17:43,840 --> 00:17:47,500
single batch of course 5 12/8 still 64

398
00:17:47,500 --> 00:17:50,080
so 64 image is being done per batch per

399
00:17:50,080 --> 00:17:54,880
GPU basically now I'm doing 150 epochs

400
00:17:54,880 --> 00:17:58,180
but I won't necessarily wait for 150 of

401
00:17:58,180 --> 00:17:59,950
them to be done I actually have this

402
00:17:59,950 --> 00:18:01,750
callback that'll actually force the

403
00:18:01,750 --> 00:18:04,240
model is safe at every single epoch and

404
00:18:04,240 --> 00:18:06,880
also log down what epoch it was and the

405
00:18:06,880 --> 00:18:10,210
valid agent accuracy and so basically I

406
00:18:10,210 --> 00:18:12,250
can go ahead and take whichever epoch

407
00:18:12,250 --> 00:18:15,250
had the highest validation accuracy into

408
00:18:15,250 --> 00:18:18,670
my into my experiment now of course in

409
00:18:18,670 --> 00:18:20,890
terms of test size I'm keeping 20% of

410
00:18:20,890 --> 00:18:23,470
the data for testing and only 80% of the

411
00:18:23,470 --> 00:18:25,450
data for training and I'm glad to say

412
00:18:25,450 --> 00:18:28,330
that I was able to get tore down 91%

413
00:18:28,330 --> 00:18:31,030
accuracy using this neural network that

414
00:18:31,030 --> 00:18:35,170
is barely optimized for this kind of for

415
00:18:35,170 --> 00:18:37,090
this kind of classification of course I

416
00:18:37,090 --> 00:18:38,950
can get much higher if I did something

417
00:18:38,950 --> 00:18:40,750
like transfer learning or using larger

418
00:18:40,750 --> 00:18:44,080
datasets however for now I'm using just

419
00:18:44,080 --> 00:18:47,230
a plain old new neural network using

420
00:18:47,230 --> 00:18:49,960
this global Albert puling 2d layer at

421
00:18:49,960 --> 00:18:52,270
the end and then directly a dense

422
00:18:52,270 --> 00:18:56,530
connection now though let's go ahead and

423
00:18:56,530 --> 00:18:58,870
get into how we can actually use this

424
00:18:58,870 --> 00:19:01,210
with can now the beauty of this

425
00:19:01,210 --> 00:19:04,360
technique lies in its simplicity so if I

426
00:19:04,360 --> 00:19:07,000
were to go ahead and quit out of this I

427
00:19:07,000 --> 00:19:09,100
go ahead and take the train model save

428
00:19:09,100 --> 00:19:11,590
it to my disk and this is where it's

429
00:19:11,590 --> 00:19:13,780
located in the cam folder or class

430
00:19:13,780 --> 00:19:16,600
activation so we've got a few main files

431
00:19:16,600 --> 00:19:19,270
here we've got the cam file of course

432
00:19:19,270 --> 00:19:21,280
we've got a katfoe we've got a captive

433
00:19:21,280 --> 00:19:25,390
picture and we've got a dog picture over

434
00:19:25,390 --> 00:19:28,510
here now we've also got this model this

435
00:19:28,510 --> 00:19:32,350
is actually what I downloaded from from

436
00:19:32,350 --> 00:19:34,929
me from my limbic server but there's

437
00:19:34,929 --> 00:19:35,980
just one

438
00:19:35,980 --> 00:19:39,610
little problem with these models let me

439
00:19:39,610 --> 00:19:41,140
show you

440
00:19:41,140 --> 00:19:44,200
tensa flow cross models in for load

441
00:19:44,200 --> 00:19:46,059
model so I'm gonna go ahead and load the

442
00:19:46,059 --> 00:19:48,010
model that I actually downloaded from

443
00:19:48,010 --> 00:19:52,210
from my from my name big server here now

444
00:19:52,210 --> 00:19:54,700
what you'll notice is that when I go

445
00:19:54,700 --> 00:19:56,590
ahead and load the model and also just

446
00:19:56,590 --> 00:19:57,910
one more thing you should notice though

447
00:19:57,910 --> 00:20:00,640
is that this model check point which is

448
00:20:00,640 --> 00:20:03,730
what I download it from I told it to not

449
00:20:03,730 --> 00:20:05,770
just save the weights but also to save

450
00:20:05,770 --> 00:20:07,270
the neural network architecture in the

451
00:20:07,270 --> 00:20:09,760
same file so that's why I can just do

452
00:20:09,760 --> 00:20:11,710
load model without defining the model

453
00:20:11,710 --> 00:20:13,330
and then loading the weights now if I

454
00:20:13,330 --> 00:20:15,760
were to print out a sari you see it's

455
00:20:15,760 --> 00:20:19,210
not saved in the same format that I

456
00:20:19,210 --> 00:20:21,880
saved that I wanted it in right so we've

457
00:20:21,880 --> 00:20:23,679
got all these input layers you know

458
00:20:23,679 --> 00:20:25,179
these lambda layers we've got the

459
00:20:25,179 --> 00:20:26,500
sequential and we've got this weird

460
00:20:26,500 --> 00:20:29,350
concatenate stuff however if you take a

461
00:20:29,350 --> 00:20:31,809
look at the layers right in the model

462
00:20:31,809 --> 00:20:33,850
you take a look at this array the

463
00:20:33,850 --> 00:20:37,150
second-last is this sequential layer

464
00:20:37,150 --> 00:20:39,179
which is actually technically a model

465
00:20:39,179 --> 00:20:41,320
now if I were to take a look at that

466
00:20:41,320 --> 00:20:44,799
that is indeed sequential and because

467
00:20:44,799 --> 00:20:47,169
that model is a layer I can go ahead and

468
00:20:47,169 --> 00:20:49,419
treat that as an individual model which

469
00:20:49,419 --> 00:20:52,809
gives me this whole this real sort of

470
00:20:52,809 --> 00:20:55,690
model which is what I wanted and so if I

471
00:20:55,690 --> 00:20:58,390
were to go ahead and actually save this

472
00:20:58,390 --> 00:21:01,299
as a model it would go ahead and work

473
00:21:01,299 --> 00:21:05,140
because this is what I'm truly after not

474
00:21:05,140 --> 00:21:08,020
this weird format of a neural network

475
00:21:08,020 --> 00:21:10,000
because this has my global average

476
00:21:10,000 --> 00:21:13,000
pooling and my convolution layers so I'm

477
00:21:13,000 --> 00:21:14,679
going to go ahead and exit out of this

478
00:21:14,679 --> 00:21:16,660
and let's go ahead and take a look at

479
00:21:16,660 --> 00:21:19,390
the actual code now the class activation

480
00:21:19,390 --> 00:21:21,640
map code as I mentioned is very simple

481
00:21:21,640 --> 00:21:24,400
it's taking the actual weights from the

482
00:21:24,400 --> 00:21:26,200
final node that made the highest

483
00:21:26,200 --> 00:21:27,730
prediction of the highest confidence

484
00:21:27,730 --> 00:21:29,799
value and it's trying to figure out

485
00:21:29,799 --> 00:21:32,919
which filters had the highest weights

486
00:21:32,919 --> 00:21:34,960
whichever ones had the highest will have

487
00:21:34,960 --> 00:21:38,080
more of their map wanted to final class

488
00:21:38,080 --> 00:21:40,929
activation map which everyone's had less

489
00:21:40,929 --> 00:21:44,140
weights or less less important weight

490
00:21:44,140 --> 00:21:47,290
will actually reduce their pixels or

491
00:21:47,290 --> 00:21:48,570
their pixels importance

492
00:21:48,570 --> 00:21:50,790
from the final class activation there

493
00:21:50,790 --> 00:21:52,050
will be a link to where you can download

494
00:21:52,050 --> 00:21:54,480
this library or this function in the

495
00:21:54,480 --> 00:21:57,180
description below as well I'm going to

496
00:21:57,180 --> 00:21:59,370
go ahead and quit out of this but the

497
00:21:59,370 --> 00:22:01,680
really interesting part is inside of the

498
00:22:01,680 --> 00:22:04,260
make prediction file because of the make

499
00:22:04,260 --> 00:22:06,060
prediction file I go ahead and actually

500
00:22:06,060 --> 00:22:10,200
use that class activation code now in

501
00:22:10,200 --> 00:22:12,780
order to create a class activation mode

502
00:22:12,780 --> 00:22:15,360
essentially what I do is I run a

503
00:22:15,360 --> 00:22:17,640
prediction through the model all right

504
00:22:17,640 --> 00:22:20,520
and then from there I create a new total

505
00:22:20,520 --> 00:22:24,000
class activation I want all the pixels

506
00:22:24,000 --> 00:22:26,370
that are supporting that evidence for

507
00:22:26,370 --> 00:22:28,530
the for the for the main prediction that

508
00:22:28,530 --> 00:22:30,600
the neural network make and then I

509
00:22:30,600 --> 00:22:33,090
remove the pixels that are supporting

510
00:22:33,090 --> 00:22:35,430
the evidence for the lower prediction so

511
00:22:35,430 --> 00:22:37,830
for example if you were to put in a dog

512
00:22:37,830 --> 00:22:39,630
and the neural that were predicted that

513
00:22:39,630 --> 00:22:42,210
this is a dog it will go ahead and add

514
00:22:42,210 --> 00:22:45,180
the dog heat map pictures of pixels and

515
00:22:45,180 --> 00:22:50,250
then minus the cat heat pixels from that

516
00:22:50,250 --> 00:22:52,530
from that final class activation map and

517
00:22:52,530 --> 00:22:55,200
then from there it's gonna go ahead and

518
00:22:55,200 --> 00:22:57,500
divide it by the maximum of the absolute

519
00:22:57,500 --> 00:23:00,990
value total class activation then it'll

520
00:23:00,990 --> 00:23:03,510
click the values at 0 to 1 multiplying

521
00:23:03,510 --> 00:23:06,270
on my 255 convert them to au and convert

522
00:23:06,270 --> 00:23:08,940
them to a real image and then save it at

523
00:23:08,940 --> 00:23:11,070
a certain location and of course return

524
00:23:11,070 --> 00:23:14,070
it as well now over here I've just got

525
00:23:14,070 --> 00:23:15,600
some simple code to actually go ahead

526
00:23:15,600 --> 00:23:18,300
and load my image put it into a numpy

527
00:23:18,300 --> 00:23:20,820
array and then over here I call that

528
00:23:20,820 --> 00:23:22,830
function with the image and the original

529
00:23:22,830 --> 00:23:25,800
one as well so if I were to go ahead and

530
00:23:25,800 --> 00:23:29,160
exit outside of this file and run Python

531
00:23:29,160 --> 00:23:31,740
make pred I can go ahead and feed it a

532
00:23:31,740 --> 00:23:34,200
model like for example the cat dog age

533
00:23:34,200 --> 00:23:36,270
five month let's go ahead and speed it

534
00:23:36,270 --> 00:23:41,840
you know your cat is a cat jpg alright

535
00:23:41,840 --> 00:23:44,160
now over here it's gonna go ahead and

536
00:23:44,160 --> 00:23:46,170
run it through the model and as you can

537
00:23:46,170 --> 00:23:50,340
see with 99% confidence in fact over 99%

538
00:23:50,340 --> 00:23:53,070
it predicts that this is a cat because

539
00:23:53,070 --> 00:23:56,010
the first Club is indeed a capped now

540
00:23:56,010 --> 00:23:59,610
what you do is open this directory in

541
00:23:59,610 --> 00:24:00,880
finder

542
00:24:00,880 --> 00:24:03,220
and let's take a look at the cat and the

543
00:24:03,220 --> 00:24:07,840
cats heat map now this is really

544
00:24:07,840 --> 00:24:10,780
interesting take a look at those white

545
00:24:10,780 --> 00:24:12,940
spots in the image and take a look at

546
00:24:12,940 --> 00:24:16,390
the cat now what's happening is wherever

547
00:24:16,390 --> 00:24:18,760
the white spots are located that's where

548
00:24:18,760 --> 00:24:20,320
the convolutional neural network was

549
00:24:20,320 --> 00:24:22,870
like yes this is part of a cat and one

550
00:24:22,870 --> 00:24:25,780
of the black spots are are either a not

551
00:24:25,780 --> 00:24:29,080
cat activations or B wherever there was

552
00:24:29,080 --> 00:24:32,110
a dog activation so this is what the

553
00:24:32,110 --> 00:24:33,400
neural network is seeing when it sees a

554
00:24:33,400 --> 00:24:35,560
cat of course there's a lot of room for

555
00:24:35,560 --> 00:24:37,540
improvement we could have a more

556
00:24:37,540 --> 00:24:40,030
high-resolution class activation map we

557
00:24:40,030 --> 00:24:41,710
could have a more accurate neural

558
00:24:41,710 --> 00:24:43,630
network we do transfer learning from

559
00:24:43,630 --> 00:24:45,850
something like from the visual geometry

560
00:24:45,850 --> 00:24:48,460
group the vgg net we can do all these

561
00:24:48,460 --> 00:24:50,740
little you know sets of fine tuning in

562
00:24:50,740 --> 00:24:52,810
order to get a better class activation

563
00:24:52,810 --> 00:24:56,050
map but let's go ahead and take a look

564
00:24:56,050 --> 00:24:58,870
at the dog now so if I were to feed in

565
00:24:58,870 --> 00:25:05,410
dog dog jpg as you can see it runs it

566
00:25:05,410 --> 00:25:07,540
through the model and it lets me know

567
00:25:07,540 --> 00:25:09,790
that with 77 percent confidence this is

568
00:25:09,790 --> 00:25:13,690
indeed a dog I go ahead and open up both

569
00:25:13,690 --> 00:25:15,910
those images show them side-by-side and

570
00:25:15,910 --> 00:25:18,850
as you can see well I'm gonna go ahead

571
00:25:18,850 --> 00:25:20,350
and slip these up into two different

572
00:25:20,350 --> 00:25:25,900
windows over here so you can take a look

573
00:25:25,900 --> 00:25:29,080
at them and switch them over so we're

574
00:25:29,080 --> 00:25:32,200
consistent now this part of the image

575
00:25:32,200 --> 00:25:34,990
over here which is of course near the

576
00:25:34,990 --> 00:25:37,870
dog's head in ears is what really

577
00:25:37,870 --> 00:25:39,640
activated the neural network to say yes

578
00:25:39,640 --> 00:25:43,270
this is a dog and so did this lower part

579
00:25:43,270 --> 00:25:46,780
over here towards the bottom and so did

580
00:25:46,780 --> 00:25:49,750
the dog's tail so as you can see the

581
00:25:49,750 --> 00:25:51,670
neural network is definitely basing a

582
00:25:51,670 --> 00:25:53,920
lot of its predictions off of the

583
00:25:53,920 --> 00:25:56,050
animals tail you saw it the cat you see

584
00:25:56,050 --> 00:25:57,940
with the dog it's trying to find some

585
00:25:57,940 --> 00:25:59,650
sort of patterns in the tail that say

586
00:25:59,650 --> 00:26:01,150
yeah this is a dog Taylor yeah this is a

587
00:26:01,150 --> 00:26:03,220
cat tail and it's also taking a look at

588
00:26:03,220 --> 00:26:04,930
those feet for examples take a look

589
00:26:04,930 --> 00:26:05,500
those arms

590
00:26:05,500 --> 00:26:06,520
it's taking a look at all these

591
00:26:06,520 --> 00:26:08,860
different things in order to make its

592
00:26:08,860 --> 00:26:10,960
final prediction of course this can be

593
00:26:10,960 --> 00:26:12,670
used for debugging your neural networks

594
00:26:12,670 --> 00:26:14,169
this can be used to

595
00:26:14,169 --> 00:26:15,730
you're out whether or not you're finding

596
00:26:15,730 --> 00:26:17,529
the right features within your neural

597
00:26:17,529 --> 00:26:20,559
networks as well and that was a quick

598
00:26:20,559 --> 00:26:23,619
demo of class activation Maps through

599
00:26:23,619 --> 00:26:26,289
Python and Karass there's many there's

600
00:26:26,289 --> 00:26:27,850
much more to come when it comes to class

601
00:26:27,850 --> 00:26:30,129
activation apps and object localization

602
00:26:30,129 --> 00:26:33,159
using neural networks only trained for

603
00:26:33,159 --> 00:26:36,009
object recognition but that was a good

604
00:26:36,009 --> 00:26:37,960
tutorial on how you can use class

605
00:26:37,960 --> 00:26:40,450
activation ops I do hope you enjoy thank

606
00:26:40,450 --> 00:26:41,950
you very much everyone for joining in

607
00:26:41,950 --> 00:26:43,989
today if you do like this tutorial make

608
00:26:43,989 --> 00:26:45,399
sure to leave a like down below and

609
00:26:45,399 --> 00:26:46,840
share it with your family and friends if

610
00:26:46,840 --> 00:26:48,220
you think they could benefit from it as

611
00:26:48,220 --> 00:26:50,169
well apart from that if you do have any

612
00:26:50,169 --> 00:26:51,970
questions suggestions or feedback feel

613
00:26:51,970 --> 00:26:53,470
free to leave that down in the comments

614
00:26:53,470 --> 00:26:56,139
section below or if you do really like

615
00:26:56,139 --> 00:26:57,700
my content you want to see more of it

616
00:26:57,700 --> 00:27:00,519
feel free to subscribe to my channel as

617
00:27:00,519 --> 00:27:02,080
it really does help out a lot and turn

618
00:27:02,080 --> 00:27:03,669
on notifications if you'd like to be

619
00:27:03,669 --> 00:27:05,499
notified whenever I release new content

620
00:27:05,499 --> 00:27:07,600
so again thank you for joining today

621
00:27:07,600 --> 00:27:09,119
that's what I had for this tutorial

622
00:27:09,119 --> 00:27:12,119
goodbye
