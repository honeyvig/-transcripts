hello everyone and welcome to this
fantastic machine learning complete full
course by simply learn before we begin
with the video if you enjoy watching
these kind of videos and find them
interesting then subscribe to our
Channel as we bring the best videos for
you daily also hit the Bell icon to
never miss any updates from Simply learn
so let's go through the agenda for this
video we'll brief you with a machine
learning introduction after that we'll
see the types of machine learning moving
forward we'll cover the machine learning
road map skills required for machine
learning engineers and the top
applications and projects for machine
learning we'll also walk you through
some fantastic Concepts like the
difference between AI machine learning
and deep learning proceeding with the
course we teach you different machine
learning algorithms like linear
regression logistic regression and many
more with Hands-On demo explanations in
Python we'll also walk you through some
unique projects like fake news detection
object detection and many others these
projects will serve you as the finest
portfolios for your future interviews
speaking of interviews we have covered
you along with the most frequently Asked
machine learning interview questions to
help you crack the most challenging
interviews but before we begin if you
are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns postgraduate program in
artificial intelligence and machine
learning from Perdue University in
collaboration with the IBM should be
your right choice for admission to this
postgraduate program in machine learning
candidate should have a bachelor's
degree with an average of 50% or higher
marks along with the understanding of
basic programming Concepts and
Mathematics and candidates with two plus
years of work experience are preferred
to enroll in this program so what are
you waiting for check out the link
mentioned in the description box below
for more details and with that in mind
let's hand over this session to a
training experts here we have our um it
looks a little bit like Frankenstein our
Frankenstein looking robot today let me
tell you what is machine learning
machine learning works on the
development of computer programs that
can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in two days this was
back in July 29th
2016 so that's pretty impressive this
amount of time to continue to grow in
his development and it's smart enough to
leave spaces in the Bri work for wiring
and plumbing and can even cut and shape
bricks to size Amazon Echo relies on
machine learning and with more data it
becomes more accurate play your favorite
music order pizza from dominoes voice
control your home request rides from
Uber have you ever wondered the
difference between AI machine learning
and deep learning artificial
intelligence a technique which enables
machines to mimic human behavior this is
really important because this is how we
are a able to gauge how well our
computations or what we're working on
works is the fact that we're mimicking
human behavior we're using this to
replace human work and make it more
efficient and make it more streamlined
and more accurate and so the center of
artificial intelligence is a big picture
of all this put together IBM deep blue
chess electronic game characters those
are just a couple examples of artificial
intelligence machine learning a
technique which uses statistical methods
enabling machines to learn from their
past data so this means if you have your
input from last time and you have your
answer you use that to help prove the
next guess it makes for the correct
answer IBM Watson Google search
algorithm email spam filters these are
all part of machine learning and then
deep learning which is a subset of
machine learning composing algorithms
that allow a model to train itself and
perform tasks Alpha go natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we'd
have a very hard time tracking and it's
able to come up with an answer from that
now let's see how machine learning works
first we start with training the data
once we've trained the data the train we
go into the machine learning algorithm
which then puts the data into a
processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training data and we find out it didn't
do a good job predicting it and it gets
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns postgraduate program in
artificial intelligence and machine
learning from Purdue University in
collaboration with the IBM should be
your right choice find me to this
postgraduate program in machine learning
candidate should have a bachelor's
degree with an average of 50% or higher
marks along with the understanding of
basic programming Concepts and
Mathematics and candidates with 2 plus
years of work experience are preferred
to enroll in this program so what are
you waiting for check out the link
mentioned in the description box below
for more details now let's look into the
types of machine learning machine
learning is primarily of three types
first one is supervised machine learning
as the name suggests you have to
supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled finally there's reinforcement
learning wherein the system learns on
its own let's talk about all these types
in detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train this is how the
training is done we provide a data set
that contains pictures of a kind of a
fruit say an apple then we provide
another data set which lets the model
know that these pictures were that of a
fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can actually tell you what fruit
it is and it will will remember this and
apply this knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used into
filtering spam mails from your email
accounts as well yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to fix F out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works
imagine a newborn baby you put a burning
candle in front of the baby the baby
does not know that if it touches the
flame its fingers might get burned so it
does that anyway and gets hurt the next
time you put that candle in front of the
baby it will remember what happened the
last time and would not repeat what it
did that's exactly how reinforcement
learning works we provide the machine
with a data set wherein we ask it to
identify a particular kind of a fruit in
this case an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind on the next time
when we ask a same question it gives us
the right answer it is able to tell us
that it's actually an apple that is a
reinforced response so that's how
reinforcement learning works it learns
from its mistakes and experiences this
model is used in games like Prince of
Persia or Assassin's Creed or FIFA where
in the level of difficulty increases as
you get better with the games just to
make it more clear for you let's look at
a comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this is actually an
apple that is called label data so the
system learns from the label data and
makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning on the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised learning
artificial intelligence and machine
learning have permuted every aspect of
Our Lives if you are a programmer
chances are you have begun utilizing
github's co-pilot an AI tool that
transforms natural language PRS into
coding suggestions streamlining the
programming process
as a writer you may have encountered
open AIS gp3 or similar Auto regressive
language models that leverage deep
learning to generate text resembling
human language many of us dedicated a
few hours to experimenting with d to the
AIML power text to image generator
capable of producing intricate visuals
based on the most unusual request not
only is d 2 exponentially more powerful
but it also has the potential to
revolutionize the field of digital art
consider its impact on a digital artist
illustrator or graphic designer career
imagine creating a highly realistic
image within seconds using an app these
technologies have significant real world
applications with far reaching
implications let me tell you some
fascinating facts about machine learning
according to recent studies machine
learning related Joe postings have
increased by a staggering
344 in the past 5 years companies across
the globe are actively seeking
professionals who can harness the power
of data and build intelligent systems
the average salary is
$190,000 in us and 26 lakhs perom in
India accelerate your career in AI ml
with our comprehensive post-graduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and heck pH and ask me
anything sessions by IBM with three
Capstone and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Purdue
faculty and IBM experts ensure Top Notch
education simply learns job assist helps
you get noticed by Leading companies
live sessions on AI trends like chat GPD
generative Ai and explainable AI this
program CS statistics python supervised
and unsupervised learning NLP neural
networks computer vision G caras tensor
flow and many more skills enroll now and
unlock exciting a ml opportunities the
link is mentioned in the description box
below with that having said now here is
a comprehensive step-by-step guide to
learning machine learning number one is
skills required number two job rolls in
ml number third companies hiring ml
engineers and the number four is future
scope of ml now start starting with the
number one that is fundamentals of
machine learning familiarize yourself
with the fundamentals of machine
learning in the initial stages of
learning to drive we are familiarized
with the different elements varieties
and regulations pertaining to operating
a car it is essential to delve into the
fundamentals of machine learning to
understand what lies ahead and the
required knowledge now moving to number
two that is acquiring Proficiency in
python or RP programming language to
excel in the field of artificial
intelligence and machine learning it is
crucial to develop a strong command of
python or our programming language both
Python and R are widely used in the data
science Community due to their
versatility and extensive libraries for
scientific Computing so now moving on to
the third step that is gain knowledge of
essential python libraries for machine
learning once you have acquired
Proficiency in python as part of for
machine Learning Journey the subsequent
step involves familiarizing yourself
with essential python libraries crucial
for working with data and implementing
machine learning Solutions the key
python libraries that you should learn
from machine learning are numpy pandas
matplot lib and pyit learn now moving to
step four that is learn and Implement
various machine learning algorithms
after gaining Proficiency in python as
part of our machine Learning Journey the
subsequent Milestone involves learning
various machine learning algorithms and
their implementation using python listed
below are some of the key machine
learning algorithms that are essential
to learning number one linear regression
number two logistic regression number
three passive regression number four na
Bas number five support Vector machines
now moving to step five Master the
concepts and implementation of neural
networks once you have acquired
knowledge of python and machine learning
algorithms the next significant step in
the machine learning learning road map
is to learn neural network architecture
and their implementations using python
outlined below are several crucial
neural network architectures that are
essential to learning number one
artificial neural networks number two
CNN number three
RNN number four long shortterm memory
now moving to step six that is engage
Hands-On projects to apply your
knowledge and reinforce your
understanding after requiring knowledge
of python machine learning algorithms
and neural network architectures the
next crucial step is the machine
learning road map and is to gain
practical experience by working on
projects that allow you to apply what
you have learned the first project is
Iris flower classification number two
California house price prediction number
three stock price prediction number four
customer segmentation now we'll move to
job roles in machine learning machine
learning as highlighted earlier has
gained IM immense popularity for its
ability to elevate human efforts and
enhance machine performance through
autonomous learning this popularity has
resulted in lucrative and sought after
career options within the field
including roles like machine learning
engineer so the duty of a machine
learning engineer encompasses the
creation construction and deployment of
machine learning models collaborating
closely with data scientist and software
Engineers they participate in the
development and execution of machine
machine Learning Systems according to
glasto ml Engineers can earn up to
$150,000 in us and 11 laks perom in
India now moving to data scientist the
role of a data scientist involves
Gathering scrutinizing and interpreting
extensive data sets leveraging machine
learning algorithms they uncover
patterns and glean insights from the
data utilizing this knowledge to inform
decisions and address challenges
according to Glass data scien earn
$145,000 in us and 13 lakhs perom in
India now moving to NLP Engineers the
specific duties may vary based on the
role and sector but as outlined by
freelancer map an NLP engineer typically
engage in tasks such as designing
natural language processing systems and
addressing speech patterns and AI speech
recognition according to glast Door NLP
engineer can earn
$120,000 in us and 10 lakh perom in
India now moving to computer vision
engineer Engineers specializing in
computer vision operate within the realm
of computer vision employing machine
learning to empower computers to
comprehend and interpret visual data
from the surroundings the
responsibilities include tasks such as
image and video analysis and object
detection according to glasto CV
Engineers earn
$156,000 in us and 8 l TA perom in India
now moving to business intelligence
developer the main role of a bi
developer is to develop deploy and
maintain bi tools and interfaces they
are also responsible for simplifying
highly technical language and complex
information into layman terms for
everyone else in the company to
understand according to Glass Door
business intelligence developers earn
$15,000 in us and 7 lakhs perom in India
now we'll see the top company is hiring
for machine learning engineer number one
is Amazon then we have asenta Google
Apple Intel so these are the top hiring
companies that hire machine learning
Engineers now we'll talk about future of
machine learning machine learning has a
bright future but faces several
difficulties ml is predicted to grow
increasingly pervasive as technology
develops revolutionizing sectors
including Healthcare Banking and
transportation the work Market will
change due to AI driven automation
necess setting new position and skills
Welcome to our video on skills required
for an ml engineer machine learning has
been the Talk of the Town lately every
organization has realized the potential
of machine learning in improving their
business objectives and attaining the
Enterprise goals this expanding demand
has led to a lot of people applying for
machine learning jobs and upskilling
themsel in the field of machine learning
you can take up this growing opportun in
the field of machine learning and
utilize it to land yourself a very
challenging fulfilling and high ping job
in this video we will be breaking down
in complete detail each and every skill
that you would need in order to crack
the machine learning engineer job
interview well ml is not just a passing
Trend it's a sismic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery so
by embracing a career in ml you become
part of dynamic field that thrives on
solving complex problem pushing
boundaries and making a profound impact
on society so the demand for ML
professional is skyrocketing across
industries from Healthcare and finance
to entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skill does it takes to
become an ml engineer how can you embark
on this thrilling Journey we have the
answers to all your questions also
accelerate your career in AI ml with our
comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything sessions by IBM with
three Capstone projects and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience master classes by
keltech faculty and IBM experts ensure
top-notch education simply learns job
assist help you get noticed by Leading
companies this program covers Python
supervis and unsupervised learning NLP
neural networks computer Visions g k
tlow and many more other skills so
enroll now and unlock exciting Ai and
MLM opportunities the link is in the
description box below so without any
further delay let's get started to
become a machine learning engineer you
need a combination of technical skills
non-technical skills and some bonus
skills so here are some essential skills
required to pursue a career as an ml
engineer so first we will talk about
some technical skills to become a ml
engineer so first one in the list is
programming languages strong programming
skills are essential you should be
proficient in at least one programming
languages such as python or R python is
widely used in the ml Community due to
its Rich libraries that is numai pandas
tens oflow and pytorch that supports ml
task and the second on the list is
machine learning algorithms and
techniques you should have a solid
understanding of various ml algorithm
such as linear regression logistic
regression decision trees random Forest
neural network and deep learning
familiarize yourself with the principles
Behind These algorithms their pros and
cons and when to use them so third one
on the list is data pre-processing ml
models require clean and well prepared
data you should know how to handle
missing data deal with normalize and
standardized data and the final is
perform feature engineering
understanding data pre-processing
technique is crucial for Effective ml
model training and the fourth one is
data manipulation and Analysis data is
the foundation of ml model you should be
skilled in the data manipulation and
Analysis using libraries like numai and
pandas this includes cleaning and
transforming data exploratory data
analysis which is Eda and understanding
the statical properties of the data and
the fifth one is machine learning
libraries and Frameworks familiarity
with popular ml libraries and framework
is essential some some some commonly
used one include numai pandas tensorflow
and pyot these Library provide
pre-implemented ml algorithms neural
network architectures and tools for
model training and evaluation now that
we have seen the Technical Machine
learning engineering skills let us have
a look at the non-technical machine
learning skills so the first one is
industry knowledge machine learning
projects that effectively tackle genuine
challenges are likely to achieve great
success regardless of the industry you
are involved in it is crucial to have a
comprehensive understanding of its
operation and identify ways to optimize
business outcomes and the second one on
the list is effective communication
effective communication plays a crucial
role in facilitating these interaction
companies seeking skilled ml engineer
value candidates who can effectively
convey technical discoveries to
non-technical terms like marketing or
sales demonstrating Clarity and fluency
in their explanation moving forward
let's see some bonus skills to become ml
engineer the first one on the list is
reinforcement learning learning in 2023
reinforcement learning emerged as a
catalyst for numerous captivating
advancement in deep learning and
artificial intelligence to pursue a
career in robotics self-driving cars or
any other AI related feed it is crucial
to comprehend this concept and the
second on the list is computer vision
computer vision and machine learning are
fundamental branches of computer science
that can independently fuel highly
Advanced system relying on CV and ml
algorithm however their combination has
a potential to unlock greater even
possibilities and achievements so
remember that the field of ml is
constantly evolving so continuous
learning and staying updated with the
latest development and the research
papers machine learning has improved our
lives in a number of wonderful ways
today let's talk about some of these I'm
Rahul from Simply learn and these are
the top 10 applications of machine
learning first let's talk about virtual
personal assistance Google Assistant
Alexa Cortana and Siri now we've all
used one of these at least at some point
in our lives now these help improve our
lives in a great number of ways for
example you could tell them to call
someone you could tell them to play some
music you could tell them to even
schedule an appointment so how do these
things actually work first they record
whatever you're saying send it over to a
server which is usually in a cloud
decode it with the help of machine
learning and neural networks and then
provide you with an output so if you
ever noticed that these systems don't
work very well without the internet
that's because the server couldn't be
contacted next let's talk about traffic
predictions now say I wanted to travel
from Buckingham Palace to L's cricket
ground the first thing I would probably
do is to get on Google Maps so search
it and let's put it
here so here we have the path you should
take to get to large cricet ground now
here the map is a combination of red
yellow and blue now the blue regions
signify a clear road that is you won't
encounter traffic there the yellow
indicate that they're slightly congested
and red means they're heavily congested
so let's look at the map a different
version of the same map and here as I
told you before red means heavily
congested yellow means slow moving and
blue means clear so how exactly is
Google able to tell you that the traffic
is clear slow moving or heavily
congested so this is the help of machine
learning and with the help of two
important measures first is the average
time that's taken on specific days at
specific times on that route the second
one is the real-time location data of
vehicles from Google Maps and with the
help of sensors some of the other
popular map services are Bing Maps
maps.me and here we go next up we have
social media personalization so say I
want to buy a drone and I'm on Amazon
and I want to buy a DJI mavic Pro the
thing is it's close to one La so I don't
want to buy it right now but the next
time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which have already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity fake
accounts where these accounts only last
for how long the transaction takes place
and stop existing in after that and man
in the middle attacks where they steal
your money while the transaction is
taking place the feed forward neural
network helps determine whether a
transaction is genuine or fraudulent so
what happens with feed forward in unit
networks are that the outputs are
converted into hash values and these
values become the inputs for the next
round so for every real transaction that
takes place there's a specific pattern a
fraudulent transaction would stand out
because of the significant changes that
it would cause with the hash values
Stock Market trading machine learning is
used extensively when it comes to stock
market market trading now you have stock
market indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and ischemic stroke lesions they
can also be used in fetally Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other finding a suitable
job in the field of machine learning is
becoming increasingly difficult the
ideal way to display your machine
learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
reccommendation system fake news
detection and many more are the best way
to improve your early programming skills
you may have the knowledge but putting
it to use what is keep you competitive
here are 10 machine learning projects
that can increase your portfolio and
enable you to acquire a job as a machine
learning engineer at number 10 we have
loan approval prediction system in this
machine learning project we will analyze
and make prediction about the loan
approval process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting fake news this ml project for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with es skillon the model is then fitted
using a passive aggressive classifier
that has been initialized finally the
accuracy score and the confusion Matrix
indicate how well our model performs the
link for the project is in the
description box below at number eight we
have personality prediction system the
idea is based on determining an
individual personality using machine
learning techniques a person personality
influences both his personal and
professional life nowadays many company
are shortlisting applicant based on
their personality which increases job
efficiency because the person is working
on what he is good at rather than what
is compelled to do in our study we
attempted to combine personality
prediction system using machine learning
techniques such as SVD na Bas and
logistic regression to predict a
personal personality and talent
prediction using phrase frequency method
this model or method allows users to
recognize their personality and
Technical abilities easily to learn
about mode this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a Progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than 1 million worldwide
each other in this machine learning
project we will develop an svm model
using python modules psyched learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design an SV model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtedly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compar able with python 2 and Python 3
before API various pre-train models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below add number five we have speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text ml speech
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the phenon and the phonetics
on the speech for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3 to
learn more about this project check the
link in the description box below at
number four we have sentiment analysis
sentiment analysis also known as opinion
mining is a straightforward process of
determining the author's feeling about a
text what was the user intention when he
or she wrote something to determine what
could be personal information we employ
a variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement used
deep learning is an any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use open CV and pH
recognition libraries to create a phe
detection system open CV provides a
realtime computer vision tool library
and Hardware we can create amazing
realtime projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dat set
generated by more than 6,000 users to
learn how to create movie
recommendations system for yourself or
for your loved one check the project in
the description box below human versus
artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every Nick CH Nook we've gone
to the Moon uh we've gone into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you're
a human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
taxs that requir human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer whereas it
didn't identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give them
a bad Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by there's a
few hours and suddenly you wonder how
many cars buses trucks and so on passed
by in the six hours now chances are
you're not going to sit by the road for
6 hours and count buses cars and trucks
unless you're working for the city and
you're trying to do City Planning and
you want to know hey do we need to add a
new truck route maybe we need a Bicycle
Link we have a lot of bicyclists here
that kind of thing so maybe City
Planning would be great for this machine
learning well the way machine Learning
Works is we have labeled data with
features okay so you have a truck or a
car a motorcycle a bus or a bicycle and
each one of those are labeled it comes
in and based on those labels and
comparing those Fe features it gives you
an answer it's a bicycle it's a truck
it's a motorcycle let's look a little
bit more in depth on this and the model
here it actually the features we're
looking at would be like the tires
someone sits there and figures out what
a tire looks like takes a lot of work if
you try to try to figure the difference
between a car tire a bicycle tire a
motorcycle tire uh so in the M machine
learning field this could take a long
time if you're going to do each
individual aspect of a car and try to
get result on there and that's what they
did do that was a a very this is still
used on smaller amounts of data where
you figure out what those features are
and then you label them deep learning so
with deep learning one of our Solutions
is to take a very large unlabel data set
and we put that into a training model
using artificial neural networks and
then that goes into the neural network
itself when we create a neural network
and you'll see um the arrows are
actually kind of backward but uh which
actually is a nice point because when we
train the neural network we put the
bicycle in and then it comes back and
says if it said truck it comes back and
says well you need to change that to
bicycle and then it changes all those
weights going backward they call it back
propagation and let it know it's a
bicycle and that's how it learns once
you've trained the neural network you
then put the new data in and they call
this testing the model so you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28x 28 pixels
that's a lot of information there um
could you imagine trying to guess that
this is a bicycle image by looking at
each one of those pixels and trying to
figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in is all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and out comes and says okay this
is a
bicycle a lot of times they call this
the magic Black Box why because as we
watch it go across here all these
weights and all the math behind this and
it's not it's little complicated on the
math side you really don't need to know
that when you're programming or doing
working with the Deep learning but it's
like magic you you don't know you really
can't figure out what's going to come
out by looking what's in each one of
those dots and each one of those lines
are firing and what's going in between
them so we like to call it the magic box
uh so that's where deep learning comes
in and in the end it comes up and you
have this whole neural notwork it comes
up and it says okay we fire all these
different pixels and we connects all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning machine learning
we're going to take a step into machine
learning here and you'll see how these
fit together in a minute the system is
able to make predictions or take
decisions based on past data that's very
important for machine learning is that
we're looking at stuff and based on
what's been there before we're creating
a decision on there we're creating
something out of there we're coloring a
beach ball we're telling you what the
weather is in Chicago what's nice about
machine learning is a very powerful
processing capability it's quick and
accurate outcomes so you get results
right away once you program the system
the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people
hundreds of years to go through some of
this data and do the same thing that the
machine learning can do in a very short
period of time and it's inexpensive
compared to hiring hundreds of people so
it becomes a very affordable way to move
into the future is to apply the machine
learning to whatever businesses you're
working on and deep Learning Systems
think and learn like humans using
artificial neural networks again it's
like a magic box performance improves
with more data so the more data the Deep
learning gets the more it gives you
better results it's scalability so you
can scale it up you can scale it down
you can increase what you're looking at
currently you know we're limited by the
amount of computer processing power as
to how big that can get but that
envelope continually gets pushed every
day on what it can do problem solved in
an end to-end method so instead of
having to break it apart and you have
the first piece coming in and you
identify tires and the second piece is
identifying uh labeling handlebars and
then you bring that together that if it
has handlebars and tires it's a bicycle
and if it has something that looks like
a large square is probably a truck the
neural networks does this all in one
network you don't really know what's
going on in all those weights and all
those little bubbles uh but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own best
features are selected by the system and
it this is important they kind of put
it's on a bullet on the side here it's a
subset of machine learning this is
important when we talk about deep
learning it is a form of machine
learning there's lots of other forms of
machine learning data analysis but this
is the newest and biggest thing that
they apply to a lot of different
packages and they use all the other
machine learning tools available to work
with it and it's very fast to test um
you put in your information you then
have your group of uh test and then you
held some aside you see how does it do
it's very very quick to test it and see
what's going on with your deep learning
and your neural network are they really
all that different H AI versus machine
learning versus deep learning concepts
of AI so we have concepts of AI you'll
see natural language processing uh
machine learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books that goes through some deep
learning the Digital Data is analyzed
text mining comes through the particular
book you're looking for maybe it's a
genre books is identified and in this
case uh we have a robot that goes goes
and gives a book to the patron I have
yet to be at a library that has a robot
bring me a book but that will be cool
when it happens uh so we look at some of
the pieces here this information goes
into uh there as far as this example the
translation of the handwritten printed
data to digital form that's pretty hard
to do that's pretty hard to go in there
and translate hundreds and hundreds of
books and understand what they're trying
to say if you've never read them so in
this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
U what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as the big
picture how does this compare to The
Human Experience in either uh doing the
same thing as a human we do or it it
does it better than us and machine
learning which has a lot of
tools uh is something that learns from
data past experiences it's programmed
it's uh comes in there and it says hey
we already had these five things happen
the sixth one should be about the same
and then uh then there's a lot of tools
in machine learning but deep learning
then is a very specific tool in machine
learning it's the artificial neural
network which handles large amounts of
data and is able to take huge pools of
experiences pictures and ideas and bring
them together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the uh backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the processing
machine learning uh spam detection very
common machine learning doesn't really
have the human interaction part so this
is the part where it goes and says okay
that's a Spam that's not a Spam and it
puts it in your spam
folder search engine result refining uh
another example of machine learning
whereas it looks at your different
results and it Go and it uh is able to
categorize them as far as this had the
most hits this is the least viewed this
has five stars um you know however they
want to waight it uh all exam good
examples of machine learning and then
the Deep learning uh deep learning
another example is as you have like a
exit sign this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so it's able to
look at the exit in the middle and it
goes okay we want to know what that is
in French and it's able to push that out
in French French and learn how to do
that and then we have chatbots um I
remember when Microsoft first had their
little paper clip um boy that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being were you just doing a search on
certain words it's now able to start
linking those words together and form a
sentence in that chat box types of AI
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to reiter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence
systems look into the past information
is added over a period of time and
information is shortlived when we're
talking about this and you look at like
a neural network that's been programmed
to identify cars it doesn't remember all
those pictures it has no memory as far
as the hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as the
final output for that neural network we
looked at so when they talk about
limited memory this is what they're
talking about they're talking about I've
created this based on all these things
but I'm not going to remember any one
specifically theory of Mind systems
being able to understand human emotions
and how they affect decision-making to
adjust their behaviors according to
their human understanding this is
important because this is our page mark
this is how we know whether it is an
artificial intelligence or not is it
interacting with humans in a way that we
can understand uh without that
interaction is just an OP object uh so
we talk about theory of mind we really
understand how it interfaces that whole
if you're in web development user
experience would be the term I would put
in there so a theory of mind would be
user experience how is the whole UI
connected together and one of the final
things is as we get into artificial
intelligence is systems being aware of
themselves understanding their internal
States and predicting other people's
feelings and act appropriately so as
artificial intelligence continues to
progress uh we see one are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people uh if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time uh to help your
life and help you grow that'd be the the
self-awareness is being able to know all
those different things types of machine
learning and like I said on the last
slide this is very important this is
very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first one is supervised learning systems
are able to predict future outcome based
on past data requires both an input and
an output to be given to the model for
it to be trained so in this case we're
looking at anything where you have 100
images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them CU they
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees all we told it was hey separate
this picture by things that don't match
and things that do match and come
together and finally there's
reinforcement learning systems are given
no training it learns on the basis of
the reward punishment it received for
performing its Last Action it helps
increase the efficiency of a tool
function or a program reinforced
learning a reinforcement learning is
kind of you give it a yes or no yes you
gave me the right response no you didn't
and then it looks at that and says oh
okay so based on this data coming in uh
what I gave you was a wrong response so
next time I'll give you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
we're grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe you have like the case
of one of the things 500 points of data
and 30 different fields it starts
getting really confusing there in
artificial intelligence or machine
learning and the Deep learning aspect
really shines when you get to that
larger data that's really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop
with no problem and do the calculations
there where with the machine learning
usually needs a higher-end system to
work it takes a lot more processing
power to build those neural networks and
to train them it goes through a lot of
data when we're talking about the
general machine learning tools most
features need to be identified in
advanced and manually coded so there's a
lot of human work on here the machine
learns the features from the data it is
provided so again it's like a magic box
you don't have to know what a tire is it
figures it out for you the problem is
divided into parts and solved
individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts and the
problem is solved in an end to end
manner so you only have one neural
network or two neural networks that is
bringing the data in and putting it out
that's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem you might have
solving one piece of the puzzle with
regular machine learning and most
machine learning tools out there they
they take longer to test and understand
how they work and with the Deep learning
it's pretty quick once you build that
neural network you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in health care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant if you
are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns postgraduate program in
artificial intelligence and machine
learning from perdu University in
collaboration with the IBM should be
your right choice for admission to this
postgraduate program in machine learning
candidate should have a bachelor's
degree with average of 50% or higher
marks along with the understanding of
basic programming Concepts and
Mathematics and candidates with 2 plus
years of work experience are preferred
to enroll in this program so what are
you waiting for check out the link
mentioned in the description box below
for more details what are the different
types of machine learning algorithms
machine learning algorithms are broadly
classified into three types the
supervised learning unsupervised
learning and reinforcement learning
supervised learning learning in turn
consists of techniques like regression
and classification and unsupervised
learning we use techniques like
Association and clustering and
reinforcement learning is a recently
developed technique and it is very
popular in gaming some of you must have
heard about alphago so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning we do not have
fast labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a a value which
can be measured and that's where we use
regression like for example weather
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud fraud or not a fraud or if it is
email spam whether it is Spam or not
spam so that is a classification example
so if you know or this is known as
labeled information if you have the
labeled information then you use
supervised learning in case of
unsupervised learning we have input data
but we don't have the labels or what the
output is supposed to be so that is when
we use unsupervised learning techniques
like cling and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum that's an example of
reinforcement learning when we look at
our different machine learning
algorithms we can divide them into three
areas supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our a gorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression regression
and we have polom linear regression now
on these three simple linear regression
is the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory under understanding
linear regression linear regression is
the statistical model used to predict
the relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
a highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 - y1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
you're going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our IND dependent
variable X and our dependent variable Y
and when X was 1 we got Y = 2 when X was
uh two y was four and so on and so on if
we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so 1 + 2 + 3 + 4 + 5
over 5 = 3 and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get 3 comma 4
which draws a nice line down the middle
a good estimate here we're going to dig
deeper into the math behind the
regression line now remember before I
said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x * Y and then it
takes all of X and adds them up all of Y
adds them up all of x s adds them up and
so on and so on and you can see we have
the sum of equal to 15 the sum is equal
to 20 all the way up to x * Y where the
sum equals 66 this all comes from our
formula for calculating a straight line
where y equals the slope time X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is in just a
minute and where that information comes
from it's called the square means error
but we'll go into that in detail in a
few minutes all you need to do is look
at the formula and see how we've gone
about Computing it line by line instead
of trying to have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x * Y which we
computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X2 easily computed as 86 for
the sum minus I'll take all that and
subtract the sum of X2 and we end up as
we come across with our formula you can
plug in all those numbers which is very
easy to do on the computer you don't
have to do the math on a piece of paper
or calculator and you'll get a slope of6
and you'll get your C coefficient if you
continue to follow through that formula
you'll see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where M = 6 and C = 2.2 we're going to
take these values and we're going to go
ahead head and plot them we're going to
predict them so y = 6 * where x = 1 +
2.2 = 2.8 so on and so on and here the
Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e square if
we put the into a nice chart where you
can see X and you can see Y what we
actual values were and you can see why
predicted you can easily see where we
take Yus y predicted and we get an
answer what is the difference between
those two and if we square that y- y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2. 4 we check this
error for each line and conclude the
best fit line having the least e Square
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square distance between the data
points and the regression line now we
only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize the
distance between the line and the data
points like sum of squared errors some
of absolute errors root mean square
error Etc what you want to take away
from this is whatever formula is being
used you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two Val values
when you're figuring out a solution so
let's move on to the next topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y equals m + x + C and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just X we have X1 X2 X3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have M1 M2 M3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y = M1 *
X1 + M2 * X2 so on all the way to m to
the n x to the n and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in Python if you
remember before we were looking at a
company and just based on its R&D trying
to figure out its profit we're going to
start looking at the expenditure of the
company we're going to go back to that
we're going to predict this profit but
instead of predicting it just on the R&D
we're going to look at other factors
like Administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company's going to be
to to start our coding we're going to
begin by importing some basic libraries
and we're going to be looking through
the data before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it's such a
visual to look at it's so easy to use um
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries were
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is used usually denoted as NP and you
have to almost have that for your SK
learn toolbox you always import that
right off the beginning pandas although
you don't have to have it for your
sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
we usually like to use the pandas when
we can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplot library. pip plot as PLT and
then caborn as SNS caborn works with the
matplot library so you have to always
import matplot library and then Seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seaborn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign matap plot
Library inline
that is only because I'm doing an inline
IDE my interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting
because we're just setting up variables
in fact it's not going to do anything
that we can see but it is importing
these different libraries and setup the
next step is load the data set and
extract independent independent
variables now here in the slide you'll
see companies equals pd. read CS V and
it has a long line there with the file
at the end 1000 companies. CSV you're
going to have to change this to fit
whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals pd.
reader CSV and I've changed this path to
match my computer colon simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the ication and because
this is companies as a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can easily extract the data from
our Panda data set with this notation
and then why we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Xcel spread sheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of
$2,261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to grab
graph it so we can get a better
understanding of our data and what it
means so at this point we're going to
use a single line of code to get a lot
of information so we can see where we're
going with this let's go ahead and paste
that into our uh notebook and see what
we got going and so we have the
visualization and again we're using SNS
which is pandas as you can see we
imported the map plot library. pyplot as
PLT which then the Seaborn uses and we
imported the caborn as SNS and then that
final line of code helps us show this in
our um inline coding without this it
wouldn't display and you can display it
to a file and other means and that's the
matap plot library in line with the
Amber sign at the beginning so here we
come down to the single line of code
caborn is great because it actually
recognizes the panda data frame so I can
just take the companies. core for
coordinates and I can put that right
into the caborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one uh we
have the columns and the rows we have R
and D spending we have Administration we
have marketing spending and profit and
if you cross index any two of these
since we're interested in profit if you
cross index profit with profit it's
going to show up if you look at the
scale on the right way up in the dark
why because those are the same data they
have an exact correspondence so R&D
spending is going to be the same as R&D
spending and the same thing with
Administration cost so right down the
middle you get this dark row or dark um
diagonal row that shows that this is the
highest corresponding data that's
exactly the same and as it becomes
lighter there's less connections between
the data so we can see with profit
obviously profit is the same as profit
and next it has a very high correlation
with R&D spending which we looked at
earlier and it has a slightly less
connection to marketing spending and
even less to how much money we put put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn preprocessing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before
four and see what I'm talking about here
if you remember when we did the
companies. head and we printed the top
five rows of data we have our columns
going across we have column zero which
is R&D spending column one which is
Administration column two which is
marketing spending and column three is
State and you'll see under State we have
New York California Florida now to do a
linear regression model it doesn't know
how to process New York it knows how to
process a number so the first thing
we're going to do is we're going to
change that New York California and
Florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in Brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
to take the label coder and we're going
to fit and transform the X also the
third row so we're going to take that
third row we're going to set it equal to
a transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x = x
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different uh models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals 2
that basically means 20% So 20% of the
rows are going to be tested we're going
to put them off to the side so since
we're using a, lines of data that means
that 200 of those lines we're going to
hold off to the side to test for later
and then the random State equals zero
we're going to randomize which ones it
picks to hold off to the side we'll go
ahead and run this it's not overly
exciting it's setting up our variables
but the next step is the next step we
actually create our linear regression
model now that we got to the linear
regression model we get that next piece
of the puzzle let's go ahead and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good luck and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
or we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor do fet in this case we do
X train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the jupter notebook
you can do it this way print y predict
and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do the
regressor intercept and let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equal M1 * X1 + M2 * X2 and
so on and so on plus C the coefficient
so these variables fit right into this
formula y = slope 1 * column 1 variable
plus slope 2 * column 2 variable all the
way to the m to the n and x to the N
plus C the coefficient or in this case
you have- 8 89 to the power of 2 etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from sklearn docs we're going to
import R2 score that's the R squar value
we're looking at the error so in the R2
score we take our y test versus our y
predict y test is the actual values
we're testing that was the one that was
given given to us that we know are true
the Y predict of those 200 values is
what we think it was true and when we go
ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
R squ value of. 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IVM should be your
right choice for more details use the
link in the description box below with
that in mind all right what is logistic
regression as I mentioned earlier
logistic regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if it is going to
break down in the near future so this is
like a classification problem find out
whether your car will break down or not
so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y AIS X is the number of years since the
last service was performed and why is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y AIS so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to one
as you all aware probability can be
between 0er and one and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year 2 years or 3 years till
after the service the chances of your
car breaking down are very limited right
so for example chances of your car
breaking down or the probability of your
car breaking down within 2 years of your
last service are .1 probability
similarly 3 years is maybe3 and so on
but as the number of years increases
let's say if it was six or seven years
there is almost a certainty that your
car is going to break down that is what
this graph shows so this is an example
of a application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class at the
out come rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is is considered
to be zero and any value above 0.5 is
considered to be 1 so an output of let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of. 29 is
considered as zero which means that the
car will not break down so that's the
way logistic regression works now let's
do a quick comparison between logistic
regression and linear regression because
they both have have the term regression
in them so it can cause confusion so
let's try to remove that confusion so
what is linear regression linear
regression is a process is once again an
algorithm for supervised learning
however here you're going to find a
continuous value you're going to
determine a continuous value it could be
the price of a real estate property it
could be your hike how much hike you're
going to get or it could be a stock
price these are all continuous values
these are not discrete compared to a yes
or a no kind of a response that we are
looking for in logistic regression so
this is one example of a linear
regression let's say the HR team of a
company tries to find out what should be
the salary hike of an employee so they
collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you trying to find continuous
values is where we use linear regression
so let's say just to extend on this
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are these are the
ratings and then some people got the
promotion and this is the ratings for
which people did not get promotion that
is a no and this is the rating for which
people got promotion we just plotted the
data about whether a person has got an
employees has got promotion or not yes
no right so there is nothing in between
and what is the employees rating okay
and ratings can be continuous that is
not an issue but the output is discrete
in this case whether employee got
promotion yes no okay so if we try to
plot that and we try to find a straight
line this is how it would look and as
you can see it doesn't look very right
because looks like there will be lot of
Errors the root mean square error if you
remember for linear regression would be
very very high and also the the values
cannot go beyond zero or Beyond one so
the graph should probably look somewhat
like this clipped at 0er and one but
still the straight line doesn't look
right therefore instead of using a
linear equation we need to come up with
something different and therefore the
logistic regression model looks somewhat
like this so we calculate the
probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotion s and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odd so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0o
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to zero and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line Looks So
y equal to Beta 0 + beta1 x where beta 0
is the Y intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 + beta 1 x which is the
equation of the straight line and now
from here if you want to find out the
value of PX we will see we can take the
exponential on both sides and then if we
solve that equation we will get the
equation of PX like this PX is equal to
1 by 1 + e^ of minus beta 0 + beta 1 x
and recall this is nothing but the
equation of the line which is equal to y
y equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and um linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s-shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and uh show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8x8 image and the the image has
a number 1 2 3 4 and you need to train
your model to predict what this number
is so how do we do this so the first
thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks box so
first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using mat plot lip some of the images or
a sample of these images and um then we
split the data into training and test as
I mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about 0.94
which is pretty good or 94% which is
pretty good all right so what is a
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like
like in this case that's why it has
almost 93 94% because the diagonals
should have the maximum numbers and the
others other than diagonals the cells
other than the diagonals should have
very few numbers so here that's what is
happening so there is a two here there
are there's a one here but most of them
are along the diagonal this what does
this mean this means that the number
that has been fed is zero and the number
that has been detect detected is also
zero so the predicted value and the
actual value are the same so along the
diagonals that is true which means that
let's let's take this diagonal right if
if the maximum number is here that means
that like here in this case it is 34
which means that 34 of the images that
have been fed or rather actually there
are two misclassifications in there so
36 images have been fed which have
number four and out of which 34 have
been predicted correctly as number four
and one has been predicted as number
eight and another one has been predicted
as number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model Okay so that's uh
gist of how to use this Matrix uh how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are use so predicted label
and the actual label that's all it is
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the the
predicted value and the actual value is
is exactly the same whereas in this case
right it has there are I think 37 + 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of of them
have been predicted as number seven and
two of them have been predicted as
number eight and so on and so forth okay
all right so with that let's go into
Jupiter notebook and see how the code
looks so this is the code in in Jupiter
notebook for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and map plot and we will take a
look at uh some of the sample images
that we have loaded so this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn uh for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into training and test so
so that the training data set is used to
train the system so we pass this
probably multiple times uh and then we
test it with the test data set and the
split is usually in the form of there
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as 20 23
that means 23% of the entire data is
used for testing and the remaining 77%
is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is the labeled data for
each of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train Yore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is
1383 by 64 and ycore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observation in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psychic learn
so we we don't have to implement the
logistic regression process itself we
just call these the function and uh let
me go ahead and execute that so that uh
we have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line where is where
we are passing our data the training
data set right this is our the the
predictors and uh this is our Target we
are passing this data set to train our
model all right so once we do this in
this case the data is not large but by
and large uh the training is what takes
usually a lot of time so we spend in
machine learning activities in machine
learning projects we spend a lot of time
for the training part of it okay so here
the data set is relatively small so it
was pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will will test it out in probably
faces so let me first try out how well
this is working for uh one image okay I
will just try it out with one image my
the first entry in my test data set and
see whether it is uh correctly
predicting or not so and in order to
test it so for training purpose we use
the fit method there is a method called
fit which is for training the model and
once the training is done if you want to
test for uh a particular value new input
you use the predict method okay so let's
run the predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed
performed so we use the score method to
find what is the percentage of accuracy
and we see here that it has performed up
to 94% Accurate okay so that's uh on
this part now what we can also do is we
can um also see this accuracy using what
is known as confusion Matrix so let us
go ahead and try that as well uh so that
we can also visualize how well uh this
model has uh done so let me execute this
piece of code which will basically
import some of the libraries that are
required and um we we basically create a
confusion Matrix an instance of
confusion matrix by running confusion
Matrix and passing these uh values so we
have so this confusion underscore Matrix
method takes two parameters one is the
Yore test and the other is the
prediction so what is the Yore test
these are the labeled values which which
we already know for the test data set
and predictions are what the system has
predicted for the test data set okay so
this is known to us and this is what the
system has uh the model has generated so
we kind of create the confusion Matrix
and we will print it and this is how the
confusion Matrix looks as the name
suggests it is a matrix and um the key
point out here is that the accuracy of
the model is determined by how many
numbers are there in the diagonal the
more the numbers in the diagonal the
better the accuracy is okay and first of
all the total sum of all the numbers in
this whole Matrix is equal to the number
of observations in the test data set
that is the first thing right so if you
add up all these numbers that will be
equal to the number of observations in
the test data set and then out of that
the maximum number of them should be in
the the diagonal that means the accuracy
is pretty good if the the numbers in the
diagonal are less and in all other
places there are a lot of numbers uh
which means the accuracy is very low the
diagonal indicates a correct prediction
that this means that the actual value is
same as the predicted value here again
actual value is same as the predicted
value and so on right so the moment you
see a number here that means the actual
value is something and the predicted
value is something else right similarly
here the actual value is something
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44 Plus 43 and so on
and divide that by the total number of
test observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slight ly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
or all the values are here most of the
values so which means reasonably this
seems to be reasonably accurate and yeah
basically the accuracy score is 94% this
is calculated as I mentioned by adding
all these numbers divided by the total
test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
a few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this is also
been predicted correctly nine and actual
value is nine and this is a image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learn professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the Link in the description
box below with that in mind there's all
kinds of regression models that come out
of this so we put them side to side we
have our linear regression which is a
predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we talking about linear regression and
we're talking about regression it means
the numberers coming out linear usually
means we're looking for that line
versus a different model and it's used
in business domain forecasting stocks uh
it's used as a basis of almost of most
um uh predictions with numbers so if
you're looking at a lot of numbers
you're probably looking at uh a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know highend neural networks and all
these other different machine learning
and AI models because they're numbers
they're just a straight set of numbers
you have a high value a low value volume
uh that kind of thing so when you're
looking at something that straight
numbers um and are connected in that way
usually you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of of um like
a window of things so you could take a
window of stock history and you could CL
generate classifications based on that
and separate the data that way if it's
going to be that this particular pattern
occurs it's going to be upward trending
or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care or do I even want to look
at it so let's do a demo so you can get
a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and nonpoisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda Navigator in this one
I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or LOD it you'll see
L git T um is one of the one of the
names that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models
and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we pre-load all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet you have rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's a
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um um
clear all that there we go uh these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh insurance
costs uh how close to the insurance cost
does it measure that we expect it to be
you know if you're an insurance company
you don't want to promise to pay
everybody's medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
a eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this has
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
this might be something that the
insurance company gets from the doctor
it says hey we're going to this is what
we need to know to give you a quote for
what we're going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 two 3 four five six
seven so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn um what to expect them to charge
on there uh so is this going to be you
know is this person going to cost me uh
$1
16,849
66 how do we guess that so that we can
guess what the minimal charges for their
insurance and then there's one other
thing thing you really need to notice on
this data um and I mentioned it before
but I'm going to mention it again
because pre-processing data is so much
of the work in data science um sex well
how do you how do you deal with female
versus male um are you a smoker yes or
no what does that mean region how do you
look at Region it's not a number how do
you draw a line between Southwest and
Northwest um you know they're they're
objects it's either your Southwest or or
Northwest it's not like I'm southwest I
guess you could do longitude and
latitude but the data doesn't come in
like that it comes in as true false or
whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. six. drop
duplicates and then for data set sex
we're going to do the label transform
the data s sex and so we're looking
right here at um male or female and so
it usually just converts it to a 01
because there's only two choices on
here same thing with the smoker it's 0
one so we're going to transfer the trans
change the smoker uh 01 one on this and
then finally we did region down here
region does it a little bit different
we'll take a look at that and um it it's
I think in this case it's probably going
to do it because we did it on this label
transform um with this particular setup
it gives each region a number like 0 1
two three so let's go ahe and take a
look and see what that looks like go and
run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organize them but it
turns the smoker into zero or one yes or
no uh and then region it did this as uh
01 23 so there three
regions now a lot of times in in when
you're working with data science science
and you're dealing with uh regions or
even word
analysis um instead of doing one column
and labeling it 0 one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set drop the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the data and
then finally the big ones uh this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from and this
comes from the Sid kit uh learn and this
is the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and would you
one of the things when I looking at this
is I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so when we go in this and we
go ahead and run this you'll see it
loads the linear regression model and
just has a nice output that says hey I
loaded the linear regression model and
then the second part is we did the fit
and so this model is now trained our
linear model is now trained on the
training
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so when
you're looking at the back end of the
data you remember we had that formula uh
BX X1 plus BX X2 plus the plus the uh
intercept uh and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know you know what uh smoker look
how smoker impacts the cost um it's just
massive uh so this is a flag that hey
the value of the smoker really affects
this model and then you can see here
where the body mass index uh so somebody
who is overweight is probably less
healthy and more likely to have cost
money and then of course age is a factor
um and then you can see down here we
have uh sex is than a factor also and it
just it changes as you go in there
negative number it probably has its own
meaning on there again it gets really
complicated when you dig into the um
workings in how the linear model works
on that and so um we can also look at
the intercept this is just kind of fun
um so it starts at this negative number
and then adds all these numbers to it
that's all that means that's our
intercept on there and that fits the
data we have on that and so you can see
right here we can go back and oops give
me just a second there we go we can go
ahead and predict the unknown data and
we can print that out and if you're
going to create a model to predict
something uh we'll go ahead and predict
it here's our y prediction value linear
model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction
and so when we pull this up uh you can
see here that we have uh the actual cost
and what we predicted the cost is going
to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we' built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here
uh now with mushrooms we want to yes or
no we want to know whether we can eat
them or not and so here's our mushroom
file we're going to go and run this take
a look at the data and again you can ask
for a copy of this file uh send a note
over to Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is um poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color um these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns and the
data and we're going to transform it
into into a um label encoder and so when
we run this you can see how this gets
shifted from uh xbxx K to 012 3 4 5 or
whatever it is class is 01 one being
poisonous zero looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase increase the number of features
as opposed to laboring 0 1 2 3 4 5 um in
this particular example it's not going
to make that big of a difference how we
encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our uh train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because then that that's
what it does is it goes through the math
and it goes hey can we minimize the
error and it keeps finding a lower and
lower error and it still is changing
that number so that means it hasn't
conversed yet it hasn't find the lowest
amount of error it can and the default
is 100 uh there's a lot of settings in
here so when we go in here to let me
pull that up from the sklearn uh so we
pull that up from the sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and I'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we actually predicting
the data so we we've done our first two
lines of code as we create the model we
fit the model to our training data and
then we go ahead and predict for our
test data now in the previous model we
didn't dive into the test score um I
think I just showed you a graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here uh
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh start eat
picking mushrooms off the side of the
trail so so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as uh the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their insurance
and so these really are the fundamentals
of data science when you pull them
together uh when I say data science I'm
talking about your machine learning
code and hopefully you got a little bit
out of here again you can contact our
simply learn team and get a copy of
these files or get more information on
this if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
SE such no more simply learns
professional certification program in Ai
and machine learning from P University
in collaboration with IVM should be a
right choice for more details use the
link in the description box below with
that in
mind a confusion Matrix represents a
table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh simple chart predicted and actual the
confusion Matrix
helps us identify the correct
predictions of a model for different
individual classes as well as the errors
so you'll see here that the values
predicted by our classifier are along
the rows this is what we're going to
guess it is our our model is guessing
what this is based on its training so
we've already trained the model to um
guess whether it's spam or not spam or
whatever it is you're working on and
then the actual values of our data set
are along the
columns so this is the actual value it's
supposed to be
people who can speak English will be
classified as positives so because they
have a remember 01 do you speak English
yes no and you could extend this that
they might have do you speak uh French
do you speak whatever languages and so
you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
English will be classified as negatives
so they'll be a zero so you know zero
ones the number of times our actual
positive values are equal to predicted
positive values gives us true positive
TP the number of times our actual
negative values are equal to predictive
negative values gives us true negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false
positive
FP and you'll see when you're working
with these a lot you know memorizing
that it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the number times our model wrongly
predicts positive values as negatives
gives us a false negative FP now I'm
going to do a quick step out
here let's say you're working in the
medical and we're talking about
cancer uh do you really want a bunch of
false negatives you want zero under
false negative uh so when we look at
this confusion Matrix if you have 5%
false positives and 5 % false negatives
it'd be much better to even have 20%
false positives because they go in and
test it and zero false negatives the
same might be true if you're working on
uh uh say uh a car driving is this a
safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff so again when you're working on
the project or whatever it is you're
working on this chart suddenly has huge
value uh we were talking about spam
email how many important emails say from
your banking overdraft charge coming in
that you want to be uh a true a false
negative you don't want it to go in the
spam folder likewise you want to get as
much as the spam out of there but you
don't want to miss anything really
important confusion Matrix metrics are
performance measures which help us find
the accuracy of our classifier there are
four main metrics accuracy prision
recall and F1 score the F1 score is the
one I usually hear the most and accuracy
is usually what you put on your chart uh
when you're sending in front of the
shareholders how accurate is it people
understand
accuracy um F1 score is a little bit
more on the math side and so you got to
be a little careful when you're quoting
F1 scores in the when you're sitting
there with all the shareholders because
a lot of them will just glaze over so
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy the accuracy is used to
find the portion of the correctly
classified values it tells us how often
our classifier is right it is the sum of
all True Values divided by the total
values and this makes sense uh again
it's one of those
things I don't want to F you know
depends on what you're looking for are
you looking for uh not to miss any spam
mails are you looking to drive down the
road and not run anybody over Precision
is used to calculate the model's ability
to classify positive values correctly it
answers the question when the model
predicts a positive value how often is
it right it is the true positive divided
by the total number of predicted
positive values again this one uh
depends on what project you're working
on whether this is what you're going to
be focusing on uh so recall it is used
to calculate the model's ability to
predict positive values how often does
the model actually predict the correct
positive values it is the true positives
divided by the total number of actual
positive values and then your F1 score
it is the harmonic mean of recall and
precision it is useful when you need to
take both precision and recall into
account consider the following two
confusion Matrix derived from two
different classifier to figure out which
one performs better we can find the
confusion Matrix for both of them and
you can see we're back to uh does it
classify whether they can speak English
or or non-speaker they speak something
they don't know the English language and
so we put these two uh uh confusion
matrixes out here we can go ahead and do
the math behind that we can look up the
accuracy that's a tpn plus TN over the
TF plus TN plus FP plus FN and so we get
an accuracy of
8125 and we have a Precision if you do
the Precision which is your TP truth
positive over TP plus
FP uh we get
891 and if we do the recall we'll end up
with the 0 825 that's your TP over TP
plus FN and then of course your F1 score
which is 2 * Precision Time recall over
Precision plus
recall and we get the point
857 and if we do that um with another
model let's say we had two different
models and we're trying to see which one
we want to use uh for whatever reason uh
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our F1
score and uh as we're looking at this we
might uh look at the accuracy because
that's really what we're interested in
is uh how many people are we able to
classify as being able to speak English
I really don't want to know if I you
know I I I I really don't want to know
if I if they're non-speakers um I'd
rather Miss 10 people speaking English
instead of 15 and so you can see from
these charts we probably go with the
first model because it does a better job
guessing who speaks English and has a
higher accuracy because in this case
that is what we're looking
for so uh with that we'll go ahead and
pull up a demo so you can see what this
looks like in the python uh setup and in
the actual coding for this we'll go with
into Anaconda Navigator if you're not
familiar with Anaconda uh it's a really
good tool to use as far as doing display
and demos and for quick development um
as a data scientist I just love the
package now if you're going to do
something heavier lifting uh there's
some limitations with anaconda and with
the setup but in general you can do just
about anything in here with your Python
and for this we'll go with Jupiter
notebook uh Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with py charm if you
work in py charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up um my simply
learned files I work on and create a new
file called confusion Matrix demo and
the first thing we want to note is the
data we're working with uh here I've
opened it up in a word pad or noad or
whatever uh you can see it's got a row
of uh headers and comma separated and
then all the data going down below and
then I saved this in the same file so I
don't have to remember what path I'm
working on uh of course if you have your
data separated and you're working with a
lot of data you probably want to put it
in a different folder or file depending
on what you're doing and the first thing
we want to do is go ahead and import our
tools uh we're going to use the pandas
that's our data frame if you haven't had
a chance to work with the data frame
please review Panda's data frame going
to Simply learn you can pull up the
panda data frame um tutorial on
there and then we're going to use uh the
syip framework which is all denoted as
sklearn and I can just pull this in you
can see here's the um
scikit-learn dorg with the stable
version that you can import into your
Python and from here we're going to use
the train test split for splitting our
data we're going to do some
pre-processing we're going to do use the
listic regression model that's our
actual uh machine learning model we're
using and then what this C this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report so let
me go ahead and run that and bring all
that information
in and just like we opened the file we
need to go ahead and load our data in
here uh so we're going to go ahead and
do our pandas read CSV and then just
because we're in jupyter Notebook we can
just put data to read the data in here a
lot of times we'll actually let me just
do this I prefer to do the just the head
of the data or the top
part and you can see we have age sex um
I'm not sure what CP stands for test BPS
cholesterol uh so a lot of different
measurements uh if you were in this
domain you'd want to know what all these
different measurements mean I don't want
to focus on that too much because when
we're talking about data science a lot
of times you have no idea what the data
means if you've ever looked up the
breast cancer measurement it's just a
bunch of measurements and numbers uh
unless you're a doctor you're going to
have no idea what those measurements
mean but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target uh so here's our Target value
here and it's either one or
zero so we have a classifier if you're
dealing with one zero true false what do
you have you have a
classifier and then our X is going to be
uh everything except for the Target uh
so we're going to go ahead and drop the
target axis equals one remember that's
columns versus uh the index or rows axis
equals zero would would give you an
error but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have um Y data
X data uh and you can see from the X
data we have the X head and we can go
ahead and just do
print the Yad
data and run
that so this is all loading the data
that we've done so far uh if there's a
confusion in there go back and rewind
the tape and review it and then we need
to go ahead and split our data into our
XT train X test y train y test
and then keep in mind you always want to
split the data before we do the scaler
and the reason is is that uh you want
the scaler on the training data uh to be
set on the training data data or fit to
it but not on the test data think of
this as being out in the field uh you're
not it could actually alter your results
uh so it's always important to do make
sure whatever you do to the training
data or whatever um fit you're doing is
always done done on the training not on
the test and then we want to go ahead
and scale the data now we are working
with um linear regression model I I'll
mention this here in a minute when we
get to the actual model uh so some
sometimes you don't need to scale when
you're working with linear regression
models it's not going to change your
result as much as say a neural network
where it has a huge
impa uh but we're going to go ahead and
take here's our XT train X test y train
y test we create our scaler we go ahead
and scale uh the scale is going to fit
the X
train and then we're going to go ahead
and take our X train and transform it
and then we also need to take our X test
and transform it based on the scale on
here so that our X is now between that
nice minus one to one and so this is all
uh our pre- dat setup and hopefully uh
all of that looks fairly familiar to you
if you've done a number of our other
classes in your up to the setup on
here and then we want to go ahead and do
is create our model and we're going to
use the logistic regression model and
from the logistic regression model uh
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here and so let's go
ahead and run that and so now we are we
actually have like our X test and our
prediction so if you remember
from our Matrix we're looking for the
actual versus the prediction and how
those
compare and if I take this back up here
you're going to notice that we imported
the accuracy score the confusion Matrix
and the classification report uh and
there's of course our logistic
regression the model we're using for
this and I did mention I was going to
talk a little bit about scaler and the
regression
model the scaler on a lot of your
regression models uh your basic Mass
standard regression models and I'd have
to look it up for the logistic
regression model when you're using a
standard regression model you don't need
to scale the data uh it's already just
built in by the way the model
Works in most cases uh but if you're in
a neural network and there's a lot of
other different setups then you really
want to take this and uh fit that on
there and so we can go in and do the
accuracy uh and this is if you remember
correctly we were looking at the
accuracy with the english- speaking uh
so this is saying our accuracy as to
whether this person is I believe this is
the heart data
set um it's going to be accurate about
85% of the time as far as whether it's
going to predict the person's going to
have um a heart condition or the one as
it comes up with the 01 on there which
would mean at this point that you have
an 85% uh being correct on telling
someone they're extremely high risk for
a heart attack kind of thing
and so we want to go ahead and uh create
our confusion Matrix and let me just do
that of course the software does
everything for us so we'll go ahead and
run this and you can see right here um
here's our
25 uh prediction uh correct predictions
right
here and if you remember from our slide
I'll just bring this over so it's a nice
visual we have our true positive false
positive
uh so we had 25 which were true that it
said hey this person's going to be high
risk at um heart and we had four that
were still high risk that it said were
false um so out of these 25 people or
out of these 29 people and that that
makes sense because you have 085 uh out
of 29 people it was correct on 25 of
them and so uh here's our accuracy score
we were just looking at that our
accuracy is your true positive and your
true negative over all of them so how
true is it there was our accuracy um
coming up here 085 and then we have our
nice Matrix generated from
that uh and you can see right here is a
similar Matrix we had going for from the
slide and this starts to this should
start asking questions at this point um
so if you're in a board meeting or
you're working with this you really want
to start looking at this data here and
saying well is this good enough is uh
this number of people and hopefully
you'd have a much larger data set it
might is my confusion Matrix showing for
the true positive and false positive is
that acceptable for what we're doing uh
and of course if you're going to put
together whatever data you're putting
out you might want to separate the uh
true negative false positive false
negative true positive and you can
simply do that uh by doing the confusion
Matrix uh and then of course the Ravel
part that you um set that up so you can
just split that right up into a nice
tupal and the final thing we want to
show you here in the coding on this part
is the confusion Matrix
metrics and so we can come in here and
just use the Matrix equals
classification report the Y test and the
predict and then we're going to take
that classification report and go ahead
and print that out and you can see here
it does a nice job uh giving you your
accuracy uh your micro average your
weighted average um you have your
Precision your recall call your F1 score
and your support all in one window so
you can start looking at this data and
saying oh okay our precisions at
83 uh 087 for getting a a positive and
83 for the negative site for a zero and
we start talking about whether this is a
valid information or not to use and when
we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not
um you might have something where uh we
went back to languages maybe you also
want to know whether they speak English
or Hindi uh or French and you can see
right here that we can now take our
confusion Matrix and just expand it as
big as we need to depending on how many
different classifiers we're working on
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind what is a
decision tree let's go through a very
simple example before we dig in deep
decision tree is a tree-shaped diagram
used to determine a course of action
each branch of the tree represents a
possible decision or currence or
reaction let's start with a simple
question how to identify a random
vegetable from a shopping bag so we have
group of vegetables in here and we can
start off by asking a simple question is
it red and if it's not then it's going
to be the purple fruit to the left
probably an eggplant if it's true it's
going to be one of the red fruits is a
diameter greater than two if false it's
going to be a what looks to be a red
chili and if it's true it's going to be
a bell pepper from the capsicum family
so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error before we dig deeper into
the mechanics of the decision tree let's
take a look at the advantages of using a
decision tree and we'll also take a
glimpse at the disadvantages the first
thing you'll notice is that it's simple
to understand interpret and visualize it
really shines here because you can see
exactly what's going on in a decision
tree little effort is required for data
preparation so you don't have to do
special scaling there's a lot of things
you don't have to worry about when using
a decision tree it can handle both
numerical and categorical data as we
discovered earlier and nonlinear
parameters don't affect its performance
so even if the data doesn't fit an easy
curved graph you can still use it to
create an effective decision or
prediction if we're going to look at the
advantages of a decision tree we also
need to understand the disadvantages of
a decision tree the first disadvantage
is overfitting over fitting occurs when
the algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variant
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second
definition which is Information Gain
Information Gain it is the measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
baseed on the height true or false
equals 10 and on the other side height
is less than 10 true or false and as you
see as we split it the entropy continues
to be less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math at how
they compute INE finally we went under
the different parts of our tree and they
call the leaf node Leaf node carries the
classification or the decision so it's
the final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with the gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a uh e funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math don't really have to memorize this
math just be aware that it's there and
this is what's going on in the
background and so we have three giraffes
two tigers one monkey two elephants a
total of eight animals gathered and if
we plug that into the formula we get an
entropy that equals 3 over 8 so we have
three drafts a total of eight times the
log usually they use base two on the log
so log base 2 of 3 over8 plus in this
case let's say it's the elephants 2 over
8 2 elephants over total of 8 * log base
2 2 over8 plus 1 monkey over total of 8
log base 2 1 over8 and plus 2 over 8 of
the Tigers log base 2 over 8 and if we
plug that into our computer our
calculator I obviously can't do logs in
my head we get an entropy equal to
0.571 the program will actually
calculate the entropy of the data set
similarly after every split to calculate
the gain now we're not going to go
through each set one at a time to see
what those numbers are we just want you
to be aware that this is a Formula or
the mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
theor yellow you're correct let's say
the condition that gives us the maximum
gain is yellow so we will split the data
based on the color yellow if it's true
that group of animals goes to the left
if it's false it goes to the right the
entropy after the splitting has de
decreased considerably however we still
need some splitting at both the branches
to attain an entropy value equal to zero
so we decide to split both the nodes
using height as a condition since every
Branch now contains single label type we
can say that entropy in this this case
has reached the least value and here you
see we have the giraffes the Tigers the
monkey and the elephants all separated
into their own groups this tree can now
predict all the classes of animals
present in the data set with 100%
accuracy that was easy use case loan
repayment prediction let's get into my
favorite part and open up some Python
and see what the programming code in the
scripting looks like in here we're going
to want to do a prediction and we start
with this individual here who's
requesting to find out how good his
customers are going to be be whether
they're going to repay their loan or not
for this bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and run it in so let's flip on on
over and here I'm using the Anaconda
Jupiter notebook now you can use any
python IDE you like to run it in but I
find the jupyter notebooks really nice
for doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with mathematics
especially with with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numberers it's called number python it
has your mathematics in there in this
case we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an an error why is
this error here well it's looking at
this it says I need to read a file and
when this was written the person who
wrote it this is their path where they
stored the file so let's go ahead and
fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this this very lengthy setup
on here where I stor the data 2. CSV
file don't worry too much about the full
path because on your computer will be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn so let's just go
ahead and take a look at this and use
this as a uh opportunity to understand
what's going on here if you read the
warning it says the cross validation is
depreciated so it's a warning on it's
being removed and it's going to be moved
in favor of the model selection so if we
go up here we have sklearn Doc
crossvalidation and if you research this
and go to sklearn site you'll find out
that you can actually just swap it right
in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to fix
before we go any further I went ahead
and opened up the data from this file
remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 4
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet so you can see what
that looks like and here I've opened it
up in the open Office Cal which is
pretty much the same as um Excel and
zoomed in and you can see we've got our
columns and our rows of data little
easier to read in here we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lin function from Python and when
I run this you'll see that it's a th
long and that's what we expected there's
a th lines of data in there if you
subtract the column head and this is one
of the nice things when we did the uh
balance data from the panda read CSV
you'll see that the header is row zero
so it automatically removes a
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the uh pandas module for head
another beautiful thing in the data set
that we can utilize so let's put that on
our sheet here and we have print data
set and balance data. head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as the head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 one 2 3 4 instead
of 1 2 3 4 5 that's a standard scripting
and programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember we always start with
zero is a yes or no and that's whether
the loan went default or not so we want
to start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to3
so it's roughly 30% will be used in the
test and then we use a random state so
it's completely random which rows it
takes out of there and then finally we
get to actually build our decision tree
and they've called it here clf entropy
that's the actual decision tree or
decision tree classifier and in here
they've added a couple variables which
we'll explore in just a minute and then
finally we need to fit the data to that
so we take our clf entropy that we
created and we fit the XT train and
since we know the answ is for X train or
the Y train we go ah and put those in
and let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf entropy equal
decision tree classifier it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that it's
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves is five
so it's going to have at least five
leaves at the end so I'll have at least
three splits I'll have no more than
three layers and at least five end
leaves with the final result at the
bottom now that we've created our
decision tree classifier not only
created it but trained it let's go ahead
and apply it and see what that looks
like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here and before we run it let's just
take a quick look at what's it's doing
here we have a variable y predict that
we're going to do and we're going to use
our variable clf entropy that we
created and then you'll see do predict
and that's very common in the sklearn
modules that there different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
would be the new loan you're putting in
here to guess whether the person's going
to be uh pay them back or not in this
case so we need to test out the data and
just see how good our sample is how good
of our tree does at predicting the loan
payments and finally since Anaconda
jupyter notebook is works as a command
line for python we can simply put the Y
predict e in to print it I could just as
easily have put the
print and put brackets around y predict
in to print it out we'll go ahead and do
that doesn't matter which way you do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th000 so you
should have about 300 answers in here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from sklearn docs import accuracy
score and you could probably run a
script make your own script to do this
very easily how accurate is it how many
out of 300 do we get right and so we put
in our y test that's the one we run the
predict on and then we put in our y
predict in that's the answers we got and
we're just going to multiply that by 100
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage and let's run that
and see what it looks like
and if you see here we got an accuracy
of 93.6
6667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy set up on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
University ities and in collaboration
with leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
P University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind let us dig
deep into the theory of exactly how it
works and let's look at what is random
for us random forest or random decision
Forest is a method that operates by
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and this uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree to where we have
that picture of the fruit goes in and
this one decides that it's a lemon and
the decision 3 tree gets another image
and it decides it's an apple and then
this all comes together in what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision tree and looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes hm
orange or red well if it's true then it
comes out as the orange and if it's
false that leaves apples so in this
example it sorts out the fruit in the
bowl or the images of the fruit a
decision tree these are very important
terms to know because these are very
Central to understanding the decision
tree and when working with them the
first is entropy everything on the
decision tree and how it makes those
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set uh then
they also have Information Gain the leaf
node the decision node and the root node
we'll cover these other four terms as we
go down the tree but let's start with
entropy so starting with entropy we have
here um a high amount of Randomness what
that means is that whatever is coming
out of this decision if it was going to
guess based on this data it wouldn't be
able to tell you whether it's a lemon or
an apple it would just say it's a fruit
uh so the first thing we want to do is
we want to split this apart and we take
the initial data set we're going to
create a data set one and a data set two
we just split it in two and if you look
at these new data sets after splitting
them the entropy of each of those sets
is much less so for the first one
whatever comes in there it's going to
sort that data and it's going to say
okay this data goes this direction it's
probably an apple and if it goes into
the other direction it's probably a
lemon so that brings us up to
Information Gain it is the measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that information gain is increased
increas greatly in this example and so
you can find that the information grain
simply equals uh decision E1 minus E2 as
we're going down our list of uh
definitions we'll look at the leaf node
and the leaf node carries the
classification or the decision so we
look down here to the leaf node we
finally get to our set one or our set
two when it comes down there and it says
okay this object's gone into set one if
it's gone into set one
it's going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right I'll either be an apple
or lemons those Leaf nodes or those
final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more branches and you
can can see here where we have the uh
five apples and one lemon and in the
other case the five lemons and one apple
they have to make a choice of which tree
It Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information so far we've looked at a
very general image um with the fruit
being split let's look and see exactly
what that means to split the data and
how do we make those decisions on there
uh let's go in there and find out how
does a decision tree work so let's try
to understand this and let's use a
simple example and we'll stay with the
fruit we have a bowl of fruit and so
let's create a problem statement and the
problem is we want to classify the
different types of fruits in the B bow
based on different features the data set
in the bowl is looking quite messy and
the entropy is high in this case so if
this bow was our decision maker it would
know what choice to make it has so many
choices which one do you pick Apple
grapes or lemons and so we look in here
we're going to start with a dra a
training set so this is our data that
we're training our data with and we have
a number of options here we have the
color and under the color we have red
yellow purple uh we have a diameter uh
331 331 and we have a label Apple lemon
Grapes apple lemon grapes and how do we
split the data we have to frame the
conditions to split the data in such a
way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 2 3 four threes
that's a pretty high selection so let's
say the condition gives us the maximum
gain of three so we have the most pieces
fall into that range so our first split
from our decision node is we split the
data based on the diameter is it greater
than or equal to three if it's not
that's false it goes into the grape bowl
and if it's true it goes into a bowl
fold of lemon and apples the interp
after splitting has decreased
considerably so now we can make two
decisions if you look at they're very
much less chaos going on there this node
has already attained an entropy value of
zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
in our choices of color and diameter too
and if the color
is yellow it's going to go to the right
bowl and if it's false it's going to go
to the left Bowl so the entropy in this
case is now zero so now we have three
bowls with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with 100%
accuracy and we can predict the Apple
also with 100% accuracy along with our
grapes up there so we've looked at kind
of a basic tree in our forest but what
we really want to know is how does a
random Forest work as a whole so to
begin our um random Forest classifier
let's say we already have built three
trees and we're going to start with the
first tree that looks like this just
like we did in the example this tree
looks at the diameter if it's greater
than or equal to three it's true
otherwise it's false so one side goes to
the smaller diameter one side goes to
larger diameter and if the color is
orange it's going to go to the right
true we're using oranges now instead of
lemons and if if it's red it's going to
go to the left false and we build a
second tree very similar but split
differently instead of the first one
being split by a diameter uh this one
when they created it if you look at that
first Bowl it has a lot of red objects
so it says is the color red because
that's going to bring our entropy down
the fastest and so of course if it's
true it goes to the left if it's false
it goes to the right and then it looks
at the shape false or true and so on and
so on and tree three is the diameter
equal to one and it came up with this
because there's a lot of cherries in
this bowl so that would be the biggest
split on there is is the diameter equal
to one that's going to drop the entropy
the quickest and as you can see it
splits it into true if it goes false and
they've added another category does it
grow in the summer and if it's false it
goes off to the left if it's true it
goes off to the right let's go ahead and
bring these three trees so you can see
them all in one image so this would be
three completely different trees
categorizing a fruit and let's take a
fruit now let's try this and this fruit
if you look at it we've blackened it out
you can't see the color on it so it's
missing data remember one of the things
we talked about earlier is that a random
Forest works really good if you're
missing data if you're missing pieces so
this fruit has an image but maybe a
person had a black and white camera when
they took the picture and we're going to
take a look at this and it's going to
have um they put the color in there so
ignore the color down there but the
diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is the diameter greater than equal to
three is a color orange well it doesn't
really know on this one but it if you
look at the value it say true and it go
to the right tree 2 classifies it as
cherries is a color equal red is the
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal one well that's false does it grow
in the summer true so it goes down and
looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says H
it's an orange and you can guess if you
have two oranges and one says it's a
cherry uh when you add that all together
the majority of the vote says orange so
the answer is it's classified as an
orange even though we didn't know the
color and we're missing data on it I
don't know about you but I'm I'm getting
tired of fruit so let's switch and I did
promise you we'd start looking at a case
example and get into some python coding
today we're going to use the case the
iris flower analysis o this is the
exciting part as we roll up our sleeves
and actually look at some python coding
before we start the python coding we
need to go ahead and create a problem
statement wonder what species of Iris do
these flowers belong to let's try to
predict the species of the flowers using
machine learning in Python let's see how
it can be done so here we begin to go
ahead and Implement our python code and
you'll find that the first half of our
implementation is all about organizing
and exploring the data coming in let's
go ahead and take this first step which
is loading the different modules into
Python and let's go ahead and put that
in our favorite editor whatever your
favorite editor is in this case I'm
going to be using the Anaconda Jupiter
notebook which is one of my favorites
certainly there's notepad++ and E clips
and dozens of others or just even using
the python terminal window any of those
will work just fine to go ahead and
explore this python coding so here we go
let's go ahead and flip over to our
Jupiter notebook and I've already opened
up a new page for Python 3 code and I'm
just going to paste this right in there
and let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the SK learn.
sets import load Iris now this isn't the
actual data this is just the module that
allows us to bring in the data the load
Iris and the iris is so popular it's
been around since 1936 when Ronald fiser
published a paper on it and they're
measuring the different parts of the
flower and based on those measurements
predicting what kind of flower it is and
then if we're going to do a random
Forest classifier we need to go ahead
and import a random forest classifier
from the sklearn module so SK learn.
Ensemble import random force classifier
and then we want to bring in two more
modules um and these are probably the
most commonly used modules in Python and
data science with any of the um other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD PD is the common term used
for pandas and pandas is basically
creates a data format for us where when
you create a panda data frame it looks
like an Xcel spreadsheet and you'll see
that in a minute when we start digging
deeper into the code panda is just
wonderful cuz it plays nice with all the
other modules in there and then we have
numpy which is our numbers Python and
the numbers python allows us to do
different mathematical sets on here
we'll see right off the bat we're going
to take our NP and we're going to go
ahead and Seed the randomness with it
with zero so np. random. seed is seeding
that is zero this code doesn't actually
show anything we're going to go ahead
and run it because I need to make sure I
have all those loaded and then let's
take a look at the next module on here
the next six slides including this one
are all about exploring the data
remember I told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our Jupiter notebook and
here we go we've gone ahead and uh run
the Imports and I'm going to paste the
code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells it how to
get the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so that's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what's going on
here the first thing you want to notice
is that our PF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. dat down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
all of Iris and see what that looks like
so when I print alliv irus I get this
long list of information and you can
scroll through here and see all the
different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of SE length seel
width pedal length petal width these are
are the different names they have for
the four different columns and if you
scroll down far enough you'll also see
data down here oh goodness it came up
right towards the top and uh data is
equal to the different data we're
looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go ahead and set
that back to the head and this is one of
the neat features of pandas and P Panda
data frames is when you do dfad or the
panda datf frame. head it'll print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
headers set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first 5 it's
going to be 0 1 2 3 4 not 1 2 3 4 5 so
now we've got our IRS data imported into
a data frame let's take a look at the
next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have a DF
species and this is interesting cuz
right here you'll see where I have DF
species in brackets which is uh the key
code for creating another column and
here we have iris. Target now these are
both in the pandas setup on here so in
pandas we can do either one I could have
just as easily done Iris and then in
Brackets Target depending on what I'm
working on both are um acceptable let's
go ahead and run this code and see how
this changes and what we've done is
we've added the target from the iris
data set as another column on the end
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what a Sosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you that where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
De now and here we go we're going to put
that down here and we're going to run
that and let's talk a little bit about
what we're doing now we're exploring
data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is trained so
we're going to use part of it for
training and this equals NP remember
that stands for numpy random. uniform so
we're generating a random number between
0 and one and we're going to do it for
each of the rows that's where the length
DF comes from so each row gets a
generated number and if it's less than
75 it's true and if it's greater than 75
it's false this means we're going to
take 75% of the data roughly because
there's a Randomness involved and we're
going to use that to train it and then
the other 25% we're going to hold off to
the side and use that toest test it
later on so let's flip back on over and
see what the next step is so now that
we've labeled our database for which is
training and which is testing let's go
ahead and sort that into two different
variables train and test and let's take
this code and let's bring it into our
project and here we go let's paste it on
down here and before I run this let's
just take a quick look at what's going
on here is we have up above we created
remember there's our def. head which
prints our first five rows and we've
added a column is train at the end and
so we're going to take that we're going
to create two variables we're going to
create two new data frames one's called
train one's called test 75% in train 25%
in test and then to sort that out we're
going to do that by doing DF our main
original data frame with the iris data
in it and if DF is train equals true
it's going to go in the train and if DF
is train equals false it goes in the
test test and so when I run this we're
going to print out the number in each
one let's see what that looks like and
you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you can see that there's 150 lines
and that's roughly 75% in one and 25%
for us to test our model on afterward so
let's jump back to our code and see
where this goes in the next two steps we
want to do one more thing with our data
and that's make it readable to humans um
I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sepa length sea width
pedal length and petal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first
four one thing I want you to notice here
is that when you're in a command line
whether it's jupyter notebook or you're
running command line in the uh terminal
window if you just put the name of it
it'll print it out this is the same as
doing
print
features and the Shand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing but for this we want to go ahead
and we'll just leave it as features cuz
it doesn't really matter
and this is one of the fun thing about
Jupiter notebooks is I'm just building
the code as we go and then we need to go
ahead and create the labels for the
other part so let's take a look and see
what that for our final step in prepping
our data before we actually start
running the training and the testing is
we're going to go ahead and convert the
species on here into something the
computer understands so let's put this
code into our script and see where that
takes
us all right here we go we set y equal
to pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what train species is and why
we're looking at the group zero on there
and let's go up here and here is our
species remember this on that we created
this whole column here for species and
then it has satsa satsa Sosa Sosa and if
you scroll down enough you'd also see
virginica and Vera color we need to
convert that into something the computer
understands zeros and ones so the
trained species of zero because this is
in the format of a of an array of arrays
so you have to have the zero on the end
and then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our full script that we're running
and let's paste this in here and let's
take a look and see what this is we have
we're creating a variable CF and we're
going to set this equal to the random
forest classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals 2 if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts zero is fine
for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more clf
do fit so we're fitting we're training
it we are actually creating our random
Forest classifier right here this is the
code that does everything and we're
going to take our training set remember
we kept our test off to the side and
we're going to take our training set
with the features and then we're going
to go ahead and put that in and here's
our Target the Y so the Y is 0 1 and two
that we just created and the features is
the actual data going in that we put
into the training set let's go ahead and
run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what it's
printed out is it shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals 2 you can see the random
State equals zero those are the two that
we sent in there you would really have
to dig deep to find out all these
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing words there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process
and leaf nodes that's the end nodes
remember we had the fruit and we're
talking about the leaf nodes like I said
there's a lot in this we're looking at a
lot of stuff here so you might have uh
in this case there's probably only think
three leaf nodes maybe four you might
have thousands of leaf nodes at which
point you do need to put a cap on on
that and say okay it can only go so far
and then we're going to use all of our
resources on processing this and that
really is what most of these are about
is limiting the process and making sure
we don't uh overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false warm start
is if you're programming it one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the
sklearn and if you're digging in deep
and running a major project on here for
today though all we need to do is fit or
train our features and our Target y so
now we have our training model what's
next if we're going to create a
model we now need to test it remember we
set aside the test featur test group 25%
of the data so let's go ahead and take
this code and let's put it into our uh
script and see what that looks like okay
okay here we go and we're going to run
this and it's going to come out with a
bunch of zeros ones and twos which
represents the three type of flowers the
satsa the virginica and the Versa color
and what we're putting into our predict
is the test features and I always kind
of like to know what it is I am looking
at so real quick we're going to do
test features and remember features is
an
array of SEO length SEO width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this case we're going to go ahead and
put those into our predict so we're
going to put each one of these lines of
data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 01
21222 and and uh again this is the FL
type satos of virginica and Versa color
so now that we've taken our test
features let's explore that let's see
exactly what that data means to us so
the first thing we can do with our
predicts is we can actually generate a
different prediction model when I say
different we're going to view it
differently it's not that the data
itself is different so let's take this
next piece of code and put it into our
script so we're pasting it in here and
you'll see that we're doing a predict
and we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we run it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means so when
we do the predict underscore proba for
probability it generates three numbers
so we had three leaf nodes at the end
and if you remember from all the theory
we did this is the predictors the first
one is predicting a one for satsa it
predicts a zero for virginica and it
predicts a zero for versacolor and so on
and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and that's what we're going to do in
these next two steps we're going to
start by setting up our predictions and
mapping them to the name so let's see
what that looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to it a little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we've been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're going
ahead and run it again see lf. predict
test features remember that Returns the
zeros the ones and the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this it distributes and it comes out as
an array and the array is sosa Sosa
stosa stosa Sosa we're only looking at
the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts uh mapping it to all
the different flower types Versa color
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and I went ahead and did uh
predictions through 25 let's just do
five and so we have ptosis ptosis ptosis
sosis that's what we're predicting from
our test model and then we come down
here and we look at test species I
remember I could have just done test.
species. head and you'll see it says
Sosa Sosa Sosa Sosa and they match so
the first one is what our forest is
doing and the second one is what the
actual data is now is we need to combine
these so that we can understand what
that means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our Jupiter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the Jupiter notebook I can do a control
minus you can see the whole line
there there we go resize it and let's
take a look and see what going on here
we're going to create in pandas remember
PD stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
Sosa vers color vinica and the actual
species Sosa Versa color virica and so
the way to read this chart and let's go
ahead and take a look on how to read
this chart here when you read this chart
you have satsa where they meet you have
verol where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ID 32 and if we multiply by 100
we can say that it is 93% accurate so we
have a 93% accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this uh line of
code from up above I don't know if you
remember it but predicts equals the
iris. target underscore names so we're
going to map it to the names and we're
going to run the prediction and we ran
it on test features but you know we're
not just testing it we want to actually
deploy it so at this point I would go
ahead and change this and this is an
array of arrays this is really important
when you're running these to know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here and and then uh
I actually want to see what the answer
is so let's go ahead and type in PRS and
print that out and when I run this
you'll see that I've now predicted two
flowers that maybe I measured in my
front yard as Versa color and Versa
color not surprising since I put the
same data in for each one this would be
the actual uh end product going out to
be used on data that you don't know the
answer
for so that's going to conclude our
scripting part of this yes if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind by now we
all know machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog little
kid looking over there and watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purs doesn't love to play
around dogs they have dle claws bigger
length of ears barks loves to run around
you usually don't see a cat running
around people although I do have a cat
that does that where dogs do and we can
look at these we can say uh we can
evaluate the sharpness of the claws how
sharp are their claws and we can
evaluate the length of the ears and we
can usually sort out cats from dogs
based on even those two characteristics
now tell me if it is a cat or a dog odd
question usually little kids know cats
and dogs by now unless they live a place
where there's not many cats or dogs so
if we look at the sharpness of the claws
the length of the ears and we can see
that the cat has a smaller ears and
sharper claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and it
goes in the cat group because KNN is
based on feature similarity we can do
classification using KNN classifier so
we have our input value the picture of
the black cat it goes into our trained
model and it predicts that this is a cat
coming out so what is KNN what is the
KNN algorithm K nearest neighbors is
what that stands for it's one of the
simplest supervised machine learning
algorithms mostly used for
classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine
another favorite mind KNN stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and KN andn is a perimeter
that refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equal 5
we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals five I mean
that's was the value we put in there so
we're going to talk about it how do we
choose the factor K KN andn algorithm is
based on feature similarity choosing the
right value of K is a process called
parameter tuning and is important for
better accuracy so at k equal 3 we can
classify we have a question mark in the
middle as either a as a square or not is
it a square or is it in this case a
triangle and so if we set k equals to 3
we're going to look at the three nearest
neighbors we're going to say this is a
square and if we put k equal to 7 we
classify as a triangle depending on what
the other data is around and you can see
as the K changes depending on where that
point is that drastically changes your
answer and uh we jump here we go how do
we choose the factor of K you'll find
that in all machine learning choosing
these factors that's the face you get
it's like oh my gosh did I choose the
right K did I set it right my values in
whatever machine learning tool you're
looking at so that you don't have a huge
bias in One Direction or the other and
in terms of knnn the number of K if you
choose it too low the bias is based on
it's just too noisy it's it's right next
to a couple things and it's going to
pick those things and you might get a
skewed answer and if your K is too big
then it's going to take forever to
process so you're going to run into
processing issues and resource issues so
what we do the most common use and
there's other options for choosing K is
to use the square root of n so N is a
total number of values you have you take
the square root of it in most cases you
also if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where I you might
get into gig of data if it's really
clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using KNN so if
we have new data coming in that says 57
kilg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the square root of x -
a^ 2 + y - b^ 2 and you can remember
that from the two edges of a triangle
we're Computing the third Edge since we
know the X side and the yide let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance D1 is a square otk
of 170 - 167 2 + 57 - 51 2ar which is
about 6.7 and distance two is about 13
and distance three is about 13.4
similarly we will calculate the ukian
distance of unknown data point from all
the points in the data set and because
we're dealing with small amount of data
that's not that hard to do and it's
actually pretty quick for a computer and
it's not a really complicated MTH you
can just see how close is the data based
on the ukian distance hence we have
calculated the ukian distance of unknown
data point from all the points as
showing where X1 and y1 equal 57 and 170
whose class we have to classify so now
we're looking at that we're saying well
here's the ukian distance who's going to
be their closest neighbors now let's
calculate the nearest neighbor at k
equals 3 and we can see the three
closest neighbors puts them at normal
and that's pretty self-evident when you
look at this graph it's pretty easy to
say okay what you know we're just voting
normal normal normal three votes for
normal this is going to be a normal
weight so majority of neighbors are
pointing towards normal hence as per K&N
algorithm the class of 57 170 should be
normal so recap of knnn positive integer
K is specifi IED along with a new sample
we select the K entries in our database
which are closest to the new sample we
find the most common classification of
these entries this is the classification
we give to the new sample so as you can
see it's pretty straightforward we're
just looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes or not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
one 2 3 4 5 6 7 8 um eight columns with
a particular tribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes because
that's why they're taking it and that
could cause issue on some of the machine
learning packages but for a very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
didn't take very much to open it up um I
can scroll down to the bottom of the
data there's
768 it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and uh since
we're working on the KNN predict whether
a person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
sell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run as python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy as NP
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so we've brought in our pandas
our numpy our two general python tools
and then you can see over here we have
our train test split by now youed should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number pregnancies isn't going to get
very large where the amount of insulin
they take and get up to 256 so 256
versus 6 that will skew results so we
want to go ahead and change that so
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it let me just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so we have our two general
python modules we're importing and then
we have our six modules specific from
the sklearn setup and then we do need to
go ahead and run this so these are
actually imported there we go and then
move on to the next step and so in this
set we're going to go ahead and load the
database we're going to use pandas
remember pandas is PD and we'll take a
look at the data in Python we looked at
it in a simple spreadsheet but usually I
like to also pull it up so that we can
see what we're doing so here's our data
set equals PD . read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my IPython
script is if you put in a different
folder you'd need the full length on
there we can also do a quick length of
uh the data set that is a simple python
command Len for length we might even
let's go ahead and print that we'll go
print and if you do it on its own line
link. data set in the jupyter notebook
it'll automatically print it but when
you're in most of your different setups
you want to do the print PR in front of
there and then we want to take a look at
the actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame Panda's data frame doing
it this way doesn't really matter the
other way works just fine and you can
see when we hit the Run button we have
the 700 68 lines which we knew and we
have our pregnancies it's automatically
given a label on the left remember the
head only shows the first five lines so
we have zero through four and just a
quick look at the data you can see it
matches what we looked at before we have
pregnancy glucose blood pressure all the
way to Ag and then the outcome on the
end and we're going to do a couple
things in this next step we're going to
create a list of columns where we can't
have zero there's no such thing as zero
skin thing thickness or zero blood PR
pressure zero glucose uh any of those
you'd be dead so not a really good
Factor if they don't if they have a zero
in there because they didn't have the
data and we'll take a look at that
because we're going to start replacing
that information with a couple of
different things and let's see what that
looks like so first we create a nice
list as you can see we have the values
talked about glucose blood pressure skin
thickness uh and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the na but we're
going to go ahead and do it as a data
set column equals data set column.
replace this is this is still pandas you
can do a direct there's also one that
that you look for your n a lot of
different options in here but the N nump
Nan is what that stands for is non
doesn't exist so the first thing we're
doing here is we're replacing the zero
with a numpy none there's no data there
that's what that says that's what this
is saying right here so put the zero in
and we're going to replace zeros with no
data so if it's a zero that means the
person's well hopefully not dead
hopefully it just didn't get the data
the next thing we want to do is we're
going to create the mean which is the in
integer from the data set from the
column do mean where we skip Nas we can
do that that is a panda's command there
the skip na so we're going to figure out
the mean of that data set and then we're
going to take that data set column and
we're going to replace all the
npnn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did not get the data and
the data is missing one of the tricks is
you replace it with the average what is
the most common data for that this way
you can still use the rest of those
values to do your computation and it
kind of just brings that particular
value of those missing values out of the
equation let's go ahead and take this
and we'll go ahead and run it doesn't
actually do anything so we're still
preparing our data if you want to see
what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that that's
go into our data set with a printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the panda database code there we go
my drawing tool we've added in this
right here off the data set and what
this says is that the first one in
pandas this is from the PD pandas it's
going to say within the data set we want
to look at the iocation and it is all
rows that's what that says so we're
going to keep all the rows but we're
only looking at zero column 0 to 8
remember column 9 here it is right up
here we printed in in here as outcome
well that's not part of the training
data that's part of the answer yes it's
column 9 but it's listed as eight number
eight so 0 to8 is nine columns so uh
eight is the value and when you see it
in here zero this is actually 0 to 7 it
doesn't include the last one and then we
go down here to Y which is our answer
and we want just the last one just
column 8 and you can do it this way with
this particular notation and then if you
remember we imported the train test
split that's part of the sklearn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one when you SE it I'd have
to look that up and then the test size
test size is 0.2 that simply means we're
going to take 20% of the data and put it
aside so that we can test it later
that's all that is and again we're going
to run it not very exciting so far we
haven't had any print out other than to
look at the data but that is a lot of
this is prepping this data once you prep
it the actual lines of code are quick
and easy and we're almost there with the
actual writing of our KNN we need to go
ahead and do a scale the data if you
remember correctly we're fitting the
data in a standard scaler which means
instead of the data being from you know
5 to 303 in one column and the next
column is 1 to six we're going to set
that all so that all the data is between
minus1 and 1 one that's what that
standard scaler does keeps it
standardized and we only want to fit the
scaler with the training set but we want
to make sure the testing set is the X
test going in is also transformed so
it's processing it the same so here we
go with our standard scaler we're going
to call it scor X for the scaler and
we're going to import the standard
scaler into this variable and then our X
train equals score x. fit transform so
we're creating the scale on the XT train
variable and then our X test we're also
going to transform it so we've trained
and transformed the XT train and then
the X test isn't part of that training
it isn't part of that of training the
Transformer it just gets transformed
that's all it does and again we're going
to go and run this if you look at this
we've now gone through these steps all
three of them we've taken care of
replacing our zeros for key columns that
shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here and we split
the data so now we have our test data
and our training data and then we've
taken and we scaled the data so all of
our data going in now no we don't tra we
don't train the Y part the Y train and Y
test that never has to be trained it's
only the data going in that's what we
want to train in there then Define the
model using K neighbors classifier and
fit the train data in the model so we do
that data prep and you can see down here
we're only going to have a couple lines
of code where we're actually building
our model and training it that's one of
the cool things about Python and how far
we've come it's such an exciting time to
be in machine learning because there's
so many automated tools let's see before
we do this let's do a quick length of
and let's do y we want let's just do
length of Y and we get 768 and if we
import math we do math. square root root
let's do y train there we go it's
actually supposed to be X train before
we do this let's go ahead and do import
math and do math square root length of Y
test and when I run that we get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love jupyter notebook cuz
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier n neighbors equal
11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the ukian metric there are other means
of measuring the distance you could do
like square square means value there's
all kinds of measure this but the ukan
is the most common one and it works
quite well it's important to evaluate
the model let's use the confusion Matrix
to do that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score and finally
accuracy score which is probably the
most commonly used quoted number when
you go into a meeting or something like
that so let's go ahead and paste that in
there and we'll set the cm equal to
confusion Matrix y test y predict so
those are the two values we're going to
put in there and let me go ahead and run
that and print it out and the way you
interpret this is you have the Y
predicted which would be your title up
here we could do uh let's just do p r d
predicted across the top and actual
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and prediction
and the actual agreed on 94 and 32 this
number here the 13 and the 15 those are
what was wrong so you could have like
three different if you're looking at
this across three different variables
instead of just two you'd end up with
the third row down here and the column
going down the middle so in the first
case we have the the and I believe the
zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
at high risk and the 32 that had
diabetes it had correct but our
prediction said another 15 out of that
15 it classified as incorrect so you can
see where that classification comes in
and how that works on the confusion
Matrix then we're going to go ahead and
print the F1 score let me just run that
and you see we get a 69 in our F1 score
the F1 takes into account both sides of
the balance of false positives where if
we go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're a data
scientist and you're talking to other
data scientists they're going to ask you
what the F1 score the F score is if
you're talking to the general public or
the U decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics and running an SK
learn and running the knnn the K nearest
neighbor on it if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
postgraduate program in artificial
intelligence and machine learning from
Purdue University in collaboration with
the IBM should be your right choice
choice for admission to this
post-graduate program in machine
learning candidate should have a
bachelor's degree with an average of 50%
or higher marks along with the
understanding of basic programming
Concepts and Mathematics and candidates
with two plus years of work experience
are preferred to enroll in this program
so what are you waiting for check out
the link mentioned in the description
box below for more details so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurac score we got the
0818 was pretty close to what we got and
we can pretty much round that off and
just say we have an accuracy of 80%
tells us that is a pretty fair fit in
the model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
suchar no more simply learns
professional certification program in Ai
and machine learning from P University
in collaboration with IBM should be a
right choice for more details use the
link in the description box below with
that in mind and find out why support
Vector machine so in this example last
week my son and I visited a fruit shop
dad is that an apple or a strawberry so
the question comes up what fruit did
they just pick up from the Fruit Stand
after a couple of seconds you can figure
out that it was a strawberry so let's
take this model a step further and let's
uh why not build a model which can
predict an unknown data and in this
we're going to be looking at some sweet
strawberries or crispy apples we wanted
to be able to label those two and decide
what the fruit is and we do that by
having data already put in so we already
have a bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a super supervised learning method that
looks at data and sorts it into one of
two categories and in this case we're
sorting the strawberry into the
strawberry side at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's apply this to
our fruit scenario we have our support
Vector machine we've taken it and we've
taken labeled sample of data
strawberries and apples and we draw a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the fruit example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set of female
where we have their height height and
weight 174 65 17488 and so on and we'll
need a sample data set of the male they
have a height 17990 180 to 80 and so on
let's go ahead and put this on a graph
so you have a nice visual so you can see
here we have two groups based on the
height versus the weight and on the left
side we're going to have the women on
the right side we're going to have the
men now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we drawn two lines through the data in
the middle that separates the men from
the women but to predict the gender of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space that separates the two
that is why this line best splits the
data we don't want to just do this by
eyeballing it and before we go further
we need to add some technical terms to
this we can also say that the distance
between the points and the line should
be as far as possible in technical terms
we can say the distance between the
support vector and the hyper plane
should be as far as possible and this is
where the support vectors are the
extreme points in the data set and if
you look at this data set they have
circled two points which seem to be
right on the outskirts of the women and
one on the outskirts of the men and
hyperplane has a maximum distance to the
support vectors of any class now you'll
see the line down the middle and we call
this the hyperplane because when you're
dealing with multiple Dimensions it's
really not just a line but a plane of
intersections and you can see here where
the support vectors have been drawn in
dash lines the math behind this is very
simple we take D+ the shortest distance
to the closest positive point which
would be on the Min side and D minus
minus is the shortest distance to the
closest negative point which is on the
women's side the sum of D+ and D minus
is called the distance margin or the
distance between the two support vectors
that are shown in the dash lines and
then by finding the largest distance
margin we can get the optimal hyper
plane once we've created an optimal
hyper plane we can easily see which side
the new data fits in and based on the
hyperplane we can say the new data point
belongs to the male gender hopefully
that's clear how that works on a visual
level as a data scientist you should
also be asking what happens if the
hyperplane is not optimal if we select a
hyperplane having low margin then
there's a high chance of
misclassification this particular svm
model the one we discussed so far is
also called a referred to as the
lsvm so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it looked like this where
we have two sets of data but one of the
them occurs in the middle of another set
you can see here where we have the blue
and the yellow and then blue again on
the other side of our data line in this
data set we can't use a hyper plane so
when you see data like this it's
necessary to move away from a ond view
of the data to a two-dimensional view of
the data and for the transformation we
use what's called a kernel function the
kernel function will take the 1D input
and transfer it to a two-dimensional
output as you can see in this picture
here the 1D when transfer to a
two-dimensional makes it very easy to
draw a line between the two data sets
what if we make it even more complicated
how do we perform an svm for this type
of data set here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyper plane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a hyper
plane right through it and easily split
the data before we start looking at a
programming example and dive into the
script let's look at the advantage of
the support Vector machine we'll start
with high-dimensional input space or
sometimes referred to as the curse of
dimensionality we looked at earlier on
Dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high-dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words in documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the
regularization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data data whether
it's going to be overfitted to very
specific instance or it's going to be
biased to a high or low value with the
svm it naturally avoids the overfitting
and bias problems that we see in many
other algorithms these three advantages
of the support Vector machine make it a
very powerful tool to add to your
repertoire of machine learning tools now
we did promise you a used case study
we're actually going to dive into some
Python Programming and so we're going to
go into a problem statement and start
off with the zoo so in the zoo example
we have um family members going to the
zoo and we have the young child going
dad is that a group of crocodiles or
alligators well that's hard to
differentiate and zoos are a great place
to start looking at science and
understanding how things work especially
as a young child and so we can see the
parents sitting here thinking well what
is the difference between a crocodile
and an alligator well one crocodiles are
larger in size alligators are smaller in
size snout width the crocodiles have a
narrow snout and alligators have a wider
snout and of course in the modern day
and age the father sitting there is
thinking how can I turn this into a
lesson for my son and he goes let a
support Vector machine segregate the two
groups I don't know if my dad ever told
me that but that would be funny now in
this example we're not going to use
actual measurements and data we're just
using that for imagery and that's very
common in a lot of machine learning
algorithms and setting them up but let's
roll up our sleeves and we'll talk about
that more in just a moment as we break
into our python script so here we arrive
in our actual coding and I'm going to
move this into a python editor and just
a moment but let's talk a little bit
about what we're going to cover first
we're going to cover in the code the
setup how to actually create our svm and
you're going to find that there's only
two lines of code that actually create
it and the rest of it is done so quick
and fast that it's all here in the first
page and we'll show you what that looks
like as far as our data because we're
going to create some data I talked about
creating data just a minute ago and so
we'll get into the creating data here
and you'll see this nice correction of
our two blobs and we'll go through that
in just a second and then the second
part is we're going to take this and
we're going to bump it up a notch we're
going to show you what it looks like
behind the scenes but let's start with
actually creating our setup I like to
use the Anaconda Jupiter notebook
because it's very easy to use but you
can use any of your favorite python
editors or setups and go in there but
let's go ahead and switch over there and
see what that looks like so here we are
in the Anaconda python notebook or
anaconda jupyter notebook with python
we're using Python 3 I believe this is
3.5 but it should be work in any of your
3x versions and uh you'd have to look at
the sklearn and make sure if you're
using a 2X version or an earlier version
let's go and put our code in there and
one of the things I like about the
Jupiter notebook is I go up to view and
I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and matplot library. pyplot as PLT
now these are very standardized Imports
when you're doing work the first one is
the numbers python we need that because
part of the platform we're using uses
that for the numpy array and I'll talk
about that in a minute so you can
understand why we want to use a numpy
array versus a standard python array and
normally it's pretty standard setup to
use NP for numpy the m plot library is
how we're going to view our data so this
has uh you do need the NP for the SK
learn module but the map plot library is
purely for our use for visualization and
so you really don't need that for the
svm but we're going to put it there so
you have a nice visual aid and we can
show you what it looks like that's
really important at the end when you
finish everything so you have a nice
display for everybody to look at and
then finally we're going to I'm going to
jump one ahead to line number four
that's the SK learn. dat sets. samples
generator import make blobs and I told
you that we were going to make up data
and this is a tool that's in the SK
learn to make up data I personally don't
want to go to the zoo get in trouble for
jumping over the fence and probably get
eaten by the crocodiles or alligators as
I work on measuring their snouts and
width and length instead we're just
going to make up some data and that's
what that make blobs is It's a Wonderful
tool if you're ready to test your your
uh setup and you're not sure about what
data you're going to put in there you
can create this blob and it makes it
really easy to use and finally we have
our actual VM the sklearn import svm on
line three so that covers all our
Imports we're going to create remember I
used the make blobs to create data and
we're going to create a capital x and a
lowercase y equals make blobs in samples
equals 40 so we're going to make 40
lines of data it's going to have two
centers with a random State equals 20 so
each each each group is going to have 20
different pieces of data in it and the
way that looks is that we'll have under
X um an XY plane so I have two numbers
under X X and Y will be 0 1 that's the
two different centers so we have yes or
no in this case alligator or crocodile
that's what that represents and then I
told you that the actual SK learner the
svm is in two lines of code and we see
it right here with clf equals svm SVC
kernel equals linear and I set Cal to
one although in this example since we
are not uh regularizing the data because
we want to be very clear and easy to see
I went ahead you can set it to a
thousand a lot of times you're not doing
that but for this thing linear because
it's a very simple linear example we
only have the two dimensions and it'll
be a nice linear hyperplane will'll be a
nice linear line instead of a full plane
so we're not dealing with a huge amount
of data and then all we have to do is do
cf. fit X comma Y and that's it clf has
been created and then we're going to go
ahead and display it and I'm going to
talk about this display here in just a
second but let me go ahead and run this
code and this is what we've done is
we've created two blobs you'll see the
the blue on the side and then kind of an
orangish uh on the other side that's our
two sets of data they represent one
represents crocodiles and one represents
alligators and then we have our
measurements in this case we have like
the uh width and length of the snout and
I did say I was going to come up here
and talk just a little bit about our
plot and you'll see PLT that's what we
imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma 0o that's
from numpy if you did that in a regular
array you'll get an error in a python
array you have to have that in a numpy
array it turns out that our make blobs
returns a numpy array and this notation
is great because what it means is the
first part is the colon means we're
going to do all the rows that's all the
data in our blob we created under
capital x and then the second Point has
a comma zero we're only going to take
the first value and then if you notice
we do the same thing but we're going to
take the second value remember we always
start with zero and then one so we have
column zero and column one and you can
look at this as our XY plots the first
one is the X plot and the second one is
the Y plot so the first one is on the
bottom 0 2 4 6 8 and 10 and then the
second one X of the one is the 4 5 6 7 8
9 10 going up the left hand side s equal
30 is just the size of the dot so we can
see them in said real tiny dots and then
cmap equals pt. cm. paired and you'll
also see the C equal y That's the color
we're using two colors 01 and that's why
we get the nice blue and the two
different colors for the alligator and
the crocodile now you can see here that
we did this the actual fit was done in
two lines of code a lot of times
there'll be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice cuz you can actually
see what's going on and then if we
wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case I'm giving it a uh width and
length 34 and a width and length 56 and
note that I put the data as a set of
brackets and then I have the brackets
inside and the reason I do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see cf. predict new data so the clf and
the predict part is going to give us an
answer and let's see what that looks
like and you'll see 01 so predicted the
first one the 34 is going to be on the
one side and the 56 is going to be on
the other side so one came out as a
alligator and one came out as a
crocodile now that's pretty short
explanation for this setup but really we
want to dig in and see what going on
behind the scenes and let's see what
that looks like
so the next step is to dig in deep and
find out what's going on behind the
scenes and also put that in a nice
pretty graph we're going to spend more
work on this and we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines look like and how
to set those up and finally we're going
to plot all that on here and show it and
you'll get a nice graph with the what we
saw earlier when we were going through
the theory behind this where it shows
the support vectors and the hyper plane
and those are done where you can see the
support vectors as the dash lines and
the solid line which is the hyper plane
let's get that into our Jupiter notebook
before I scroll down to a new line I
want you to notice line 13 it has Plot
show and we're going to talk about that
here in just a second but let's scroll
down to a new line down here and I'm
going to paste that code in and you'll
see that the plot show has moved down
below let's scroll up a little bit and
if you look at the top here of our new
Section 1 2 3 and four is the same code
we had before and let's go back up here
and take a look at that we're going to
fit the values on our svm and then we're
going to plot scatter it and then we're
going to do a plot show if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply Lear professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind what is
naive Bay let's start with a basic
introduction to the Baye theorem named
after Thomas Baye from the 1700s who
first coined this in the western
literature naive Bay's classifier works
on the principle of conditional
probability as given by the base theorem
before we move ahead let us go through
some of the simple Concepts in the
probability that we will be using let us
consider the following example of
tossing two coins here we have two
quarters and if we look at all the
different possibilities of what they can
come up as we get that they could come
up as head heads they come up as head
tail tail head and tell tail when doing
the math on probability we usually
denote probability as a p a capital P so
the probability of getting two heads
equals 1/4 you can see in our data set
we have two heads and this occurs once
out of the four possibilities and then
the probability of at least one tail
occurs 3/4 of the time you'll see on
three of the coin tosses we have tails
in them and out of four that's 3/4s and
then the probability of the second coin
being head given the first coin is tail
is 1/2 and the probability of getting
two heads given the first coin is a head
is 1/2 we'll demonstrate that in just a
minute and show you how that math works
now when we're doing it with two coins
it's easy to see but when you have
something more complex you can see where
these Pro these formulas really come in
and work so the base theorem gives us
the conditional probability of an event
a given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event a we
want to know what that is so we're going
to label that a has our focus and then
given another event B has occurred in
the Baye theorem as you can see on the
left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B time
probability of b equals the probability
of B given a * probability of a you can
easily move that around around and
multiply it and divide it out let us
apply base theorem to our example here
we have our two quarters and we'll
notice that the first two probabilities
of getting two heads and at least one
tail we compute directly off the data so
you can easily see that we have one
example HH out of four 1/4 and we have
three with tells in them giving us 3/4
or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins cuz you can easily understand the
math the probability of throwing a tail
doesn't matter what comes before it and
the same with the head so still going to
be 50% or 1 half but when that come when
that probability gets more complicated
let's see you have a D6 dice or some
other instance then this formula really
comes in handy but let's stick to the
simple example for now in this sample
space let a be the event that the second
coin is head and B be the event that the
first coin is Tails again we reversed it
cuz we want to know what the second
event is going to be so we're going to
be focusing on a and we write that out
as a probability of a given B and we
know this from our formula that that
equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being Tails given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being TS given the second
coin is heads times the probability of
the second coin being heads over the
probability of the first coin being
tails you can see it's a simple formula
to calculate we have 1/2 * 1/2 over 1/2
or 1 12 = .5 or 1/4 so the base theorem
basically calculates the conditional
probability of the occurrence of an
event based on prior knowledge of
conditions that might be related to the
event we will explore this in detail
when we take up an example of online
shopping further in this tutorial
understanding naive Bay and machine
learning like with any of our other
machine learning tools it's important to
understand where the naive Baye fits in
the hierarchy so under the machine
learning we have supervised learning and
there is other things like unsupervised
learning there's also reward system This
falls under the supervised learning and
then under the supervised learning
there's classification there's also
regression but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is leave
Bay used let's look at some of the Ed
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
ey this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
elements and news classification you
look at the Google news and it says well
is this political or is this world news
or a lot of that's all done with the
naive Baye understanding naive Bay
classifier now we already went through a
basic understanding with the coins and
the two heads and two tails and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Baye classifier is
based on the bay theorem which gives a
conditional probability of event a given
event B and that's where the probability
of a given b equals the probability of B
given a times probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easily come up with
a new formula for the probability of B
to me staring at these algebraic
functions kind of gives me a slight
headache
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in uh table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's uh on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead and
take a look at the data set we have a
small sample data set of 30 rows we're
showing you the first 15 of those rows
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like and we put that into
tables also note in today's world we're
not usually looking at three different
variables in 30 rows nowadays cuz we're
able to collect data so much we're
usually looking at 27 30 variable Ables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those days
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend holiday and did they buy yes
or no as we dig in deeper to this table
for our base theorem let the event buy
ba a now remember we looked at the coins
I said we really want to know what the
outcome is did the person buy or not and
that's usually event a is what you're
looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so is 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right you can see that N9
out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and eight out of 24 purchases on
a holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dates two out of six didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out 11 out
of 30 so out of the 30 people who came
into the store throughout the weekend
weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or02 so there's a
20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B if a in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without
purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making a
purchase at all doesn't matter what day
of the week is roughly 02 or 20% and the
probability of a weekday a no purchase
is roughly two out of six so two out of
six of our no purchases were made on the
weekday and then finally we take our P
of a b if you looked we've kept the
symbols up there we got P of probability
of B probability of a probability of B
if a we should remember that the
probability of a if B is equal to the
first one times the probability of no
per buys over the probability of the
weekday so we could calculate it both
off the uh table we created we can also
calculate this by the formula and we get
the 367 which equals or uh 33 * 2 over
367 which equals. 179 or roughly uh 17
to 18% and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these fact
factors day equals holiday discount
equals yes free delivery equals yes
let's dig deeper into the math and
actually see what this looks like and
we're going to start with looking for
the probability of them not purchasing
on the following combinations of days we
are actually looking for the probability
of a equal no buy no purchase and our
probability of B we're going to set
equal to is it a holiday did they get a
discount yes and was it a free delivery
yes before we go further let's look at
the original equation the probability of
a if B equals the probability of B given
the condition a and the probability
times probability of a over the
probability of B occurring now this is
basic algebra so we can multiply this
information together so when you see the
probability of a given B in this case
the condition is b c and d or the three
different variables we're looking at and
when you see the probability of B that
would be the conditions we're actually
going to multiply those three three
separate conditions out probability of
you'll see that in just a second in the
formula times the full probability of a
over the full probability of B so here
we are back to this and we're going to
have let a equal no purchase and we're
looking for the probability of B on the
condition a where a sets for three
different things remember that equals
the probability of a given the condition
B and in this case we just multiply
those three different variables together
so we have the probability of the
discount times the probability of free
delivery times the probability is the
day equal a holiday those are our three
variables of the probability of a if B
and then that is going to be multiplied
by the probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a if B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the prop probability of a no
buy is across all the data so that's
where we get the 6 out of 30 we divide
that out by the probability of each
category over the total number so we get
the 20 out of 30 had a discount 23 out
of 30 had a yes for free delivery and 11
out of 30 we're on a holiday we plug all
those numbers in we get
178 so in our probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a prob
ability equation and we'll get to that
normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase did we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall probab
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 * 21 over 24 * 8
over 24 time the P of a 24 over 30
divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 time 11 over 30
and that gives us our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals
986 we have a probability of no purchase
equals
178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals =
98686 plus. 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is
84.7% and the likelihood of no purchase
is
15.29% given these three different
variables so it's if it's on a holiday
if it's a with a discount and has free
delivery then there's an 84.7 1% chance
that the customer is going to come come
in and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand why you want to why you want
to use the naive Bay what are the
advantages of naive bays and then once
we understand those advantages we're
just look at that briefly then we're
going to dive in and do some pip python
coding advantages of naive Baye
classifier so let's take a look at the
six advantages of the naive Bay
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top or the bottom got to
really pay close attention to that when
you put it into to python it's really
nice CU you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see you just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand the this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts uh referrals spam filters is
because there's no time delay as it has
to go through and figure out a neural
network or one of the other mini setups
where you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive Bays and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
and another one that overlaps and
because the two over overlap they can
then predict the unknowns for the group
that they haven't done the second study
on or vice versa so it's very powerful
in that it is not sensitive to the
irrelevant features and in fact you can
use it to help predict features that
aren't even in there so now we're down
to my favorite part we're going to roll
up our sleeves and do some actual
programming we're going to do the use
case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bays so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
the actual data we're going to use from
it's one of the standard sets but
certainly this can be used on any of our
news headlines and classification so
let's see how it can be done using the
naive Bay classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph cuz it's always it's good to have
a graph to show what's going on and once
we've traded it and we've shown you a
graph of what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever uh interface IDE
you want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
uh and we're we're using Python 3 in
this case 3.5 so this would work in any
of your 3x if you have it set up
correctly it should also work in a lot
of the 2x you just have to make sure all
the the versions of the modules match
your python version and in here you'll
notice the first line is your percentage
mat plot library in line now three of
these lines of code are all about
plotting the graph this one let's The
Notebook not since this is the inline
setup that we want the graphs to show up
on this page
without it in a notebook like this which
is an Explorer interface it won't show
up now a lot of Ides don't require that
a lot of them like on if I'm working on
one of my other setups it just has a pop
up and the graph pops up on there so you
have a that setup also but for this we
want the matap plot library in line and
then we're going to import numpy as NP
that's number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any the upper math functions
in Python and it's very common to see
that as NP nump as NP the next two lines
are all about our graphing remember I
said three of these were about graphing
well we need our map plot library.
pyplot as PLT and you'll see that PLT is
a very common setup as is the SNS and
just like the NP and we're going to
import caborn as SNS and we're going to
do the SNS do set now caborn sits on top
of P plot and it just makes a really
nice heat map really good for heat maps
and if you're not familiar with heat
maps that just means we give it a color
scale the term comes from the brighter
red it is the hotter it is in some form
of data and you can set it to whatever
you want and we'll see that later on so
those you'll see that those three lines
of code here are just importing the
graph function so we can graph it and as
a data scientist you always want to
graph your data and have some kind of
visual it's really hard just to shove
numbers in front of people and they look
at it and it doesn't mean anything and
then from the SK learn. dat sets we're
going to import the fetch 20 news groups
very common one for analyzing tokenizing
words and setting them up and exploring
how the words work and how do you
categorize different things when you're
dealing with documents and then we set
our data equal to fetch 20 news groups
so our data variable will have the data
in it and we're going to go ahead and
just print the target names data. Target
names and let's see what that looks like
and you'll see here we have alt atheism
comp Graphics comp osms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually get in here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our jupyter notebook and
let's see what this code does first
we're going to set our categories now if
you noticed up here I could have just as
easily set this equal to data. Target
names because it's the same thing but we
want to kind of spell it out for you so
you can see the different categories it
kind of makes it more visual so you can
see what your data is looking like in
the background once we've created the
categories we're going to open up a
train set so this training set of data
is going to go into fetch 20 News Group
groups and it's a subset in there called
train and categories equals categories
so we're pulling out those categories
that match and then if you have a train
set you should also have the testing set
we have test equals fetch 20 news groups
subset equals test and categories equals
categories let's go down one size so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates train.
dat and we're just going to look look at
data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train. dat number five under train it
prints out one of the Articles this is
article number five you can go through
and read it on there and we can also go
in here and change this to test which
should look identical because it's
splitting the date up into different
groups train and test and we'll see test
number five is a a different article but
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
length of train. dat and if we run that
you'll see that the training data has
11,314 articles so we're not going to go
through all those articles that's a lot
of articles but um we can look at one of
them just so you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article uh Etc
and you can scroll all the way down and
see all the different parts to there now
we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you wait this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this this is the
actual naive Bas and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our Jupiter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we had
the print modu module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction. text we're going to
import
tfidf vectorizer I told you we're going
to throw a module at you we can't go too
much into the math behind this or how it
works you can look it up the notation
for the math is usually tf.idf and
that's just a way of weighing the words
and it weighs the words based on how
many times they're used in a document
how many times or how many documents
they're used in and it's a well-used
formula it's been around for a while
it's a little confusing to put this in
here uh but let's let it know that it
just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you're put a weight on it if you
remember I was talking about that up
here earlier if these are all emails
they probably all have the word from in
them from probably has a very low weight
it has very little value in telling you
what this document's about same with
words like in an article and articles in
cost of un maybe cost might or where
words like criminal weapons destruction
these might have a heavier weight
because they describe a little bit more
what the article's doing well how do you
figure out all those weights in the
different articles that's what this
module does that's what the tfidf
vectorizer is going to do for us and
then we're going to import our SK learn.
naive Bay and that's our multinomial NB
multinomial naive Bay pretty easy to
understand that where that comes from
and then finally we have the sky learn
pipeline import make pipeline now the
make pipeline is just a cool piece of
code because we're going to take the
information we get from the tfidf
vectorizer and we're going to pump that
into the multinomial INB so a pipeline
is just a way of organizing how things
flow it's used commonly you probably
already guess what it is if you've done
any businesses they talk about the sales
pipeline if you're on a work work crew
or project manager you have your
pipeline of information that's going
through where your projects and what has
to be done in what order that's all this
pipeline is we're going to take the tfid
vectorizer and then we're going to push
that into the multinomial NB now we've
designated that as the variable model we
have our pipeline model and we're going
to take that model and this is just so
elegant this is done in just a couple
lines of code model.fit and we're going
to fit the data and first the the train
data and then the train Target now the
train data has the different articles in
it you can see the one we were just
looking at and the train. target is what
category they already categorize that
that particular article as and what's
Happening Here is the train data is
going into the TF ID vectorizer so when
you have one of these articles it goes
in there it weights all the words in
there so there's thousands of words with
different weights on them I remember
once running a model on this and I
literally had 2.4 million tokens go into
this so when you're dealing like large
document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial NB and once we go into
our naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial INB
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um Talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
use the term. predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual truth was either
agreed with it or is something different
and it's going to add up those numbers
so we can take a look and just see how
well it worked and we're going to set a
variable Matt equal to confusion Matrix
we have our test Target our test data
that was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in and we create
our SN heat map the s SNS is our caborn
which sits on top of the P plot so when
we create a SNS do heatmap we take our
confusion Matrix and it's going to be uh
matt. T and do we have other variables
that go into the SNS do heat map we're
not going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the 00001 format d and c bar equals
false have to do with the uh format if
you take those out you'll see that some
things disappear and then the X tick
labels and the Y tick labels those are
our Target names and you can see right
here that's the alt atheism comp
Graphics comp osms windows.
miscellaneous and then finally we have
our PLT dox label remember the SNS or
the Seaborn sits on top of our map plot
Library our PLT and so we want to just
tell it xlabel equals a true is is true
the labels are true and then the Y label
is prediction label so when we say a
true this is what it actually is and the
prediction is what we predicted and
let's look at this graph cuz that's
probably a little confusing the way I
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides because they have a
black background they put in there that
helps it shine a little bit better so
you can see the graph a little bit
easier so in reading this graph what we
want to look at is how the color scheme
has come out and you'll see a line right
down the middle diagonally from upper
left to bottom right what that is is if
you look at the labels we have our
predicted label on the left and our true
label on the right those are the numbers
where the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk politics
miscellaneous social religion religion
Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics so you could
understand why it might mislabel them
but overall it did a pretty good job if
we're going to create these models we
want to go ahead and be able to use them
so let's see what that looks like to do
this let's go ahead and create a
definition a function to run and we're
going to call this function let me just
expand that just a notch here there we
go I like mining big letters predict
categories we want to predict the
category we're going to send it s a
string and then we're sending it train
equals train we have our training model
and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model.
predicts so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and uh tokenize it and put it through
the TF IDF convert that into numbers and
weights for all the different documents
and words and then it'll put that
through our naive Bay and from it we'll
go ahead and get our prediction we're
going to predict what value it is and so
we're going to return train. Target
names predict of zero and remember that
the train. target names that's just
categories I could have just as easily
put uh categories in there that predict
of zero so we're taking the prediction
which is a number and we're converting
it to an actual category we're
converting it from um I don't know what
the actual numbers are but let's say 0
equals alt atheism so we're going to
convert that zero to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out is returning the
variable train. Target names it'll
automatically print that for you in your
own ID you might have to put in print
let's see where else we can take this
and maybe you're a space science buff so
how about sending
load to
International Space
Station and if we run that we get
science space or maybe you're a uh
automobile buff and let's do um oh they
were going to tell me Audi is better
than BMW but I'm going to do BMW is
better than an Audi so maybe you're a
car buff and we run that and you'll see
it says recreational I'm assuming that's
what recc stands for Autos so I just did
a pretty good job labeling that one how
about uh if we have something like a
caption running through there president
of India and if we run that it comes up
and says talk politics
miscellaneous so when we take our
definition or our function and we run
all these things through Kudos we made
it we were able to correctly classify
texts into different groups based on
which category they belong to using the
naive base classifier now we did throw
in the pipeline the TF IDF vectorizer we
threw in the graphs those are all things
that you don't necessarily have to know
to understand the the naive Bay setup or
classifier but they're important to know
one of the main uses for the naive Bay
is with the tfidf tokenizer a vectorizer
where tokenizes the word and adds labels
and we use the pipeline because you need
to push all that data through and it
makes it really easy and fast you don't
have to know those to understand naive
Bays but they certainly help for
understanding the industry in data
science and we can see our categorizer
our naive Bas classifier we were able to
predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our train
model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IVM should be your
right choice for more details use the
link in the description box below with
that in
mind so what is K me's clustering K
means clustering is an unsupervised
learning algorithm in this case you
don't have labeled data unlike in
supervised learning so you have a set of
data and you want to group them and as
the name suggests you want to put them
into clusters which means means objects
that are similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically is a
number so we need to tell the system how
many clusters we need to perform so if K
is equal to two there will be two
clusters if K is equal to three three
clusters and so on and so forth that's
what the k stands for and of course
there is a way of finding out what is
the best or Optimum value of K for given
data we will look at that so that is K
means clustering so let's take an
example K means clustering is used in
many many scenarios but let's take an
example of Cricket the game of cricket
let's say you received data of a lot of
players from maybe all over the country
or all over the world and this data has
information about the runs scored by the
people or by the player and the wickets
taken by by the player and based on this
information we need to Cluster this data
into two clusters batsmen and Bowlers so
this is an interesting example let's see
how we can perform this so we have the
data which consists of primarily two
characteristics which is the runs and
the wickets so the bowlers basically
take wickets and the batsman score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsmen who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Bowlers
so how does this work let's say this is
how the data is so there are information
there is information on the y- AIS about
the Run scored and on the x-axis about
the wickets taken by the players so if
we do a quick plot this is how it would
look and um when we do the clustering we
need to have the Clusters like shown in
the third diagram out here so we need to
have a cluster which consists of people
who have scored High runs which is
basically the batsman and then we need a
cluster with people who have taken a lot
of wickets which is typically the
bowlers there may be a certain amount of
overlap but we will not talk about it
right now so with kin's clustering we
will have here that means K is equal to
two and we will have two clust which is
batsman and Bowlers so how does this
work the way it works is the first step
in cayman's clustering is the allocation
of two centroids randomly so two points
are assigned as socalled centroids so in
this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the centroid okay so these points
though in our presentation here we have
shown them one point closer to these
data points and another closer to these
data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned cides
so for example we take this point and
find the distance from this centroid and
the distance from this centroid this
point is taken and the distance is found
from this centroid and this Center and
so on and so forth so for every point
the distance is measured from both the
centroids and then whichever distance is
less that point is assigned to that
centroid so for example in this case
visually it is very obvious that all
these data points are assigned to this
centroid and all these data points are
assigned to this centroid and that's
what is represented here in blue color
and in this yellow color the next step
is to actually determine the central
point or the actual centroid for these
two clusters so we have this one initial
cluster this one initial cluster but as
you can see these points are not really
the centroid centroid means it should be
the central position of this data set
Central position of this data set so
that is what needs to be determined as
the next step so the central point or
the actual centroid is determined and
the original randomly allocated centroid
is repositioned to the actual centroid
of this new clusters and this process is
actually repeated now what might happen
is some of these points may get
reallocated in our example that is not
happening probably but it may so happen
that the distance is found between each
of these data points once again with
these centroids and if there is if it is
required some points may be reallocated
we will see that in a later example but
for now we will keep it simple so this
process is continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
Clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which which means
that the algorithm has converged and we
have the final cluster with the data
points and the cids so this is what
you're going to learn from this session
we will talk about the types of
clustering what is Cain's clustering
application of Cain's clustering Cain's
clustering is done using distance
measure so we will talk about the common
distance measures and then we will talk
about how kin's clustering works and go
into the det details of Cain's
clustering algorithm and then we will
end with a demo and a use case for
Cain's clustering so let's begin first
of all what are the types of clustering
there are primarily two categories of
clustering hierarchical clustering and
then partitional clustering and each of
these categories are further subdivided
into agglomerative and divisive
clustering and K means and fuzzy c means
clustering let's take a quick look at
what each of these types of clustering
are are in hierarchical clustering the
Clusters have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottomup
approach we begin with each element as a
separate cluster and merge them into
successively larger clusters so for
example we have a b CDE e f we start by
combining BNC form one cluster DN form
one more then we combine d and f one
more bigger cluster and then add BC to
that and then finally a to it compared
to that divisive clustering or divisive
clustering is a top- down approach we
begin with the whole set and proceed to
divide it into successively smaller
cluster so we have ABCDE e f we first
take that as a single cluster and then
break it down into a b c d e and f then
we have partitional clustering split
into two subtypes k K means clustering
and fuzzy c means in K means clustering
the objects are divided into the number
of clusters mentioned by the number K
that's where the K comes from so if we
say k is equal to two the objects are
divided into two clusters C1 and C2 and
the way it is done is the features or
characteristics are compared and all
objects having similar characteristics
are clubbed together so that's how K
means clustering is done we will see it
in more detail as we move forward and
fuzzy c means is very similar to K means
in the sense that it clubs objects that
have similar characteristics together
but while in kin's clustering two
objects cannot belong to or any object a
single object cannot belong to two
different clusters in c means objects
can belong to more than one cluster so
that is the primary difference between K
means and fuzzy c means so what are some
of the applications of kin's clustering
kin's clustering is used in a variety of
examples or variety of business cases in
real life starting from academic
performance diagnostic systems search
engines and wireless sensor networks and
many more so let us take a little deeper
look at each of these examples academic
performance So based on the scores of
the students students are categorized
into a b c and so on clustering forms a
backbone of search engines when a search
is performed the search results need to
be grouped together the search engines
very often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially K means clustering uses
distance measure so let's take a look at
what is distance measure so while these
are the different types of clustering in
this video we will focus on K means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is ukian distance there is
Manhattan distance then we have squared
ukian distance measure and cosine
distance measure these are some of the
distance measures supported by K means
clustering let's take a look at each of
these what is ukian distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square < TK of Y2
- y1 s + X2 - X1 whole s so this is an
extension of that formula so that is the
ukian distance between between two
points what is the squared ukian
distance measure it's nothing but the
square of the ukian distance as the name
suggests so instead of taking the square
root we leave the square as it is and
then we have Manhattan distance measure
in case of Manhattan distance it is the
sum of the distances across the x- axis
and the Y AIS and note that we are
taking the absolute value so so that the
negative values don't come into play so
that is the Manhattan distance measure
then we have cosine distance measure in
this case we take the angle between the
two vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
K means now let's go and check how
exactly K me's clustering works okay so
this is how k means clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or four or five to
start with and then as we progress we
keep changing until we get the best
clusters or there is a technique called
elbow technique whereby we can determine
the value of K what should be the best
value of K how many clusters should be
formed so once we have the value of K we
specify that and then the system will
assign that many centroids so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that I have been formed it
calculates the mean position there by
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are reallocated to the
new centroids and the mean position or
the new centroid is calculated once
again if the centroid moves then the
iteration continues which means the
convergence has not happened the
clustering has not converged so as long
as there is a movement of the centroid
this iteration keeps happening but once
the centroid stops moving which means
that the cluster has converged or the
clustering process has converged that
will be the end result so now we have
the final position of the centroid and
the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
form there is a parameter called within
sum of squares and the lower this value
is the better the cluster is that means
all these points are very close to each
other so we use this within sum of
squares as a measure to find the optimum
number of clusters that can be formed
for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within SS or WSS that is taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the Y AIS the within sum of squares
or WSS and on the xaxis we have the
number number of clusters so as you can
imagine if you have K is equal to 1
which means all the data points are in a
single cluster the with SS value will be
very high because they are probably
scattered all over the moment you split
it into two there will be a drastic fall
in the within SS value and that's what
is represented here but then as the
value of K increases the decrease the
rate of decrease will not be so high it
will continue to decrease but probably
the rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either two or three
or at the most four but beyond that
increasing the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from from C2 so these points will
be assigned which are close to C1 will
be assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids theyve
been randomly assigned points and and
only thing that has been done was the
data points which are closest to them
have been assigned to them but now in
this step the actual centroid will be
calculated which may be for each of
these data set somewhere in the middle
so that's like the mean point that will
be calculated and the centroid will
actually be positioned or repositioned
there same with C2 so the new centroid
for this group is C2 in this new
position and C1 is in this new position
once again
the distance of each of the data points
is calculated from these cids now
remember it's not necessary that the
distance Still Remains the or each of
these data points still remain in the
same group by recalculating the distance
it may be possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is a repeative
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these data
points as a part of each cluster so I
hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 n number of points
as our inputs and we want to split this
into K clusters or we want to create K
clusters so the first step is to
randomly pick K points and call them C
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular particular random
centroid repeat that process for X2
calculate its distance from each of the
centroid C1 C2 C3 up to CK and find
which is the lowest distance and assign
X2 to that particular Cent same with X3
and so on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and uh so there there are K
groups all these inputs have been split
into K groups however remember we picked
the centroids randomly so they are not
real centroids so now what we have to do
we have to calculate the actual
centroids for each of these groups which
is like the main position which means
that the position of the randomly
selected centroids will now change and
they will be the main positions of this
newly formed k groups and once that is
done we once again repeat this process
of calculating the distance right so
this is what we are doing as a part of
step four we repeat step two and three
so we again calculate the distance of X1
from the centroid C1 C2 C3 and then see
which is the lowest value and assign X1
to that calculate the distance of X2
from C1 C2 C3 over whatever up to CK and
find whichever is the lowest distance
and assign X2 to that croid and so on in
this process there may be some
reassignment X1 was probably assigned to
Cluster C2 and after doing this
calculation maybe now X1 is assigned to
C1 so that kind of reallocation may
happen so we repeat the steps two and
three till the position of the centroids
don't change or stop changing and that's
when we have convergence so let's take a
detailed look at at each of these steps
so we randomly pick K cluster centers we
call them centroids because they are not
initially they are not really the
centroids so we let us name them C1 C2
up to CK and then step two we assign
each data point to the closest Center so
what we do we calculate the distance of
each x value from each C value so the
distance between X1 C1 distance between
X1 C2 X1 C3 and then we find which is
the lowest value right that's the
minimum value we find and assign X1 to
that particular centroid then we go next
to X2 find the distance of X2 from C1 X2
from C2 X2 from C3 and so on up to CK
and then assign it to the point or to
the centroid which has the lowest value
and so on so that is Step number two in
Step number three We Now find the actual
centroid for each group so what has
happen as a part of Step number two we
now have all the points all the data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will will
have K groups but remember the initial
values of the C1 C2 were not really the
centroids of this groups right we assign
them randomly so now in step three we
actually calculate the centroid of each
group which means the original point
which we thought was the centroid will
shift to the new position which is the
actual centroid for each of these groups
okay and we again calcul calate the
distance so we go back to step two which
is what we calculate again the distance
of each of these points from the newly
positioned centroids and if required we
reassign these points to the new
centroids so as I said earlier there may
be a reallocation so we now have a new
set or a new group we still have K
groups but the number of items and
actual assignment may be different from
what was in step two here okay so that
might change then we perform step three
once again to find the new centroid of
this new group so we have again a new
set of clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of uh K means
clustering we will actually see some
live Demos in uh python notebook using
python notebook but before that let's
find out what's the problem that we are
trying to solve the problem statement is
let's say Walmart wants to open a chain
of stores across the State of Florida
and uh it wants to find the optimal
store locations now the issue here is if
they open too many stores close to each
other obviously the they will not make
profit but if they if the stores are too
far apart then and they will not have
enough sales so how do they optimize
this now for an organization like
Walmart which is an e-commerce giant
they already have the addresses of their
customers in their database so they can
actually use this information or this
data and use K means clustering to find
the optimal location now before we go
into the python notebook and show you
the Live code I wanted to take you
through very quickly a summary of the
code in the slides and then we will go
into the python notebook so in this
block we are basically importing all the
required libraries like numpy ma plot
lib and so on and we are loading the
data that is available in the form of
let's say the addresses for Simplicity
sake we will just take them as some data
points then the next thing we do is
quickly do a scatter plot to see how
they are related to each other with
respect to each other so in the scatter
plot we see that there are a few
distinct groups already being formed so
you can actually get an idea about how
the cluster would look and how many
clusters what is the optimal number of
clusters and then starts the actual K
means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct
clusters and um which is we can say that
this is how the population is probably
distributed across Florida State and uh
the centroids are like the location
where the store should be the optimum
location where the store should be so
that's the way we determine the best
locations for the store and that's how
we can help Walmart find the best
locations for the stores in Florida so
now let's take this into python notebook
let's see how this looks when we are
learning running the code live all right
right so this is the code for K means
clustering in Jupiter notebook we have a
few examples here which we will
demonstrate how K means clustering is
used and even there is a small
implementation of K means clustering as
well okay so let's get started okay so
this block is basically importing the
various libraries that are required like
M plot lip and numpy and so on and so
forth which would be used as a part of
the code then we we are going and
creating blobs which are similar to
clusters now this is a very neat feature
which is available in psyit learn make
blobs is a nice feature which creates
clusters of data sets so that's a
wonderful functionality that is readily
available for us to create some test
data kind of thing okay so that's
exactly what we are doing here we are
using make blobs and we can specify how
many clusters we want so centers we are
mentioning here so it will go ahead and
so we just mentioned four so it will go
ahead and create some test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard K means functionality that
is readily available so we really don't
have to implement K means itself the K
means functionality or the the function
is readily available you just need to
feed the data and we'll create the
Clusters so this is the code for that we
import K means and then we create an
instance of K means and we specify the
value of K this ncore clusters is the
value of K remember K means in K means K
is basically the number of clusters that
you want to create and it is a integer
value so this is where we are specifying
that so we have K is equal to 4 and so
that instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training uh kind
of thing but that's the call okay so we
are calling fit and what we are doing
here we are just passing the data so X
has these values the data that has been
created right so that is what we are
passing here and uh this will go ahead
and create the Clusters and uh then we
are
using after doing uh fit We Run The
predict which basically assigns for each
of these observations which cluster it
belongs to all right so it will name the
Clusters Maybe this is cluster one this
is two three and so on or will actually
start from zero cluster 0 1 2 and three
maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in Yore
K means when we call predict that is
what it does and we can take a quick
look at these uh Yore K means or the
cluster numbers that have been assigned
for each of observation so this is the
cluster number assigned for observation
one maybe this is for observation two
observation three and so on so we have
how many about I think 300 samples right
so all the 300 samples there are 300
values here each of them the cluster
number is given and the cluster number
goes from 0 to three so there are four
clusters so the numbers go from 0 1 2 3
so that's what is seen here okay now so
this was a quick example of gen
generating some dummy data and then
clustering that okay and this can be
applied if you have proper data you can
just load it up into X for example here
and then run the C so this is the
central part of the cin clustering
program example so you basically create
an instance and you mention how many
clusters you want by specifying this
parameter andore clusters and that is
also the value of K and then pass the
data to get the values now the next
section of
this code is the implementation of a k
means now this is kind of a rough
implementation of the K means algorithm
so we will just walk you through I will
walk you through the code uh at each
step what it is doing and then we will
see a couple of more examples of how K
means clustering uh can be used in maybe
some real life examples real life use
cases all right so in this case here
what we're doing is basically
implementing K means clustering and
there is a function for a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what K means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to that centr so
that is basically available as a
standard function and we will be using
that here so as explained in the slides
the first step that is done in case of
C's clustering is to randomly assign
some cids so as a first step we randomly
allocate a couple of centroids which we
call here we are calling as
centers and then we put this in a loop
and we take it through an iterative
process for each each of the data points
we first find out using this function
pairwise distance argument for each of
the points we find out which one which
Center or which uh randomly selected
centroid is the closest and accordingly
we assign that data or the data point to
that particular centroid or cluster and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
the center position right so we
calculate the new centroid and then we
check if the new centroid is the
coordinates or the position is the same
as the previous centroid the positions
we will compare and if it is the same
that means the process has converged so
remember we do this process till the
centroids or the centroid doesn't move
anymore right so the centroid gets
relocated each time this reallocation is
done so the moment it doesn't change
anymore the position of the centroid
doesn't change anymore we know that
convergence has occurred so till then so
you see here this is like an infinite
Loop while true is an infinite Loop it
only breaks when the centers are the
same the new center and the old Center
positions are the same and once that is
uh done we return the centers and the
labels now of course as explained this
is not a very sophisticated and advanced
implement
very basic implementation because one of
the flaws in this is that sometimes what
happens is the centroid the position
will keep moving but in the change will
be very minor so in that case also that
is actually convergence right so for
example the change is
01 we can consider that as convergence
otherwise what will happen is this will
either take forever or it will be never
ending so that's a small flaw here so
that is something additional checks may
have to be added here but again as
mentioned this is not the most
sophisticated implementation this is
like a kind of a rough implementation of
the K means clustering okay so if we
execute this code this is what we get as
the output so this is the definition of
this particular function and then we
call that find underscore clusters and
we pass our data X and the number of
clusters which is four and if we run
that and plot it this is the output that
we get so this is of course each cluster
is represented by a different color so
we have a cluster in green color yellow
color and so on and so forth and these
big points here these are the centroids
this is the final position of the
centroids and as you can see visually
also this appears like a kind of a
center of all these points here right
similarly this is like the center of all
these points here and so on so this is
the example or this is an example of a
implementation of K means clustering
and uh next we will move on to see a
couple of examples of how K means
clustering is used in maybe some real
life scenarios or use cases in the next
example or demo we're going to see how
we can use kin's clustering to perform
color compression we will take a couple
of images so there will be two examples
and uh we will try to use Cam's
clustering to compress the color this is
a common situation in image processing
when you have an image with millions of
uh colors but then you cannot render it
on some devices which may not have
enough memory uh so that is the scenario
where where something like this can be
used so before again we go into the
python notebook let's take a look at
quickly the the code as usual we import
the libraries and then we import the
image
and uh then we will flatten it so the
reshaping is basically we have the image
information is stored in the form of
pixels and if the image is like for
example 427 by 640 and it has three
colors so that's the overall dimension
of the of the initial image we just
reshape it and um then feed this to our
algorithm and this will then create
clusters of only 16 clusters so this
this colors there are millions of colors
and now we need to bring it down to 16
colors so we use K is equal to 16 and U
this is how when we visualize this is
how it looks there are these are all
about 16 million possible colors the
input color space has 16 million
possible colors and we just some
compress it to 16 colors so this is how
it would look look when we compress it
to 16 colors and this is how the
original image looks and after
compression to 16 colors this is how the
new image looks as you can see there is
not a lot of information that has been
lost though the image quality is
definitely reduced a little bit so this
is an example which we are going to now
see in Python notebook let's go into the
python notebook and and once again as
always we will import some libraries and
load this image called flower. jpg okay
so let we load that and this is how it
looks this is the original image which
has I think 16 million colors and uh
this is the shape of this image which is
basically what is the shape is nothing
but the overall size right so this is
427 pixel by 640 pixel and then there
are three layers which is this three
basically is for RGB which is red green
blue so color image will have that right
so that is the shape of this now what we
need to do is data let's take a look at
how data is looking so let me just
create a new cell and show you what is
in data basically we have captured this
information so data is what let me just
show you
here all right so let's take a look at
at China what are the values in China
and uh if we see here this is how the
data is stored this is nothing but the
pixel values okay so this is like a
matrix and each one has about for for
this 427 by 640 P pixels all right so
this is how it looks now the issue here
is these values are large the numbers
are large so we need to normalize them
to between zero and one right so that's
why we will basically create one more
variable which is data which will
contain the values between 0 and one and
the way to do that is divide by 255 so
we divide China by 255 and we get the
new values in data so let's just run
this piece of code and this is the shape
so we now have also yeah what we have
done is we changed using reshape we
converted into the threedimensional into
a two-dimensional data set and let us
also take a look at how
let me just
insert probably a cell here and take a
look at how data is looking all right so
this is how data is looking and now you
see this is the values are between zero
and one right so if you earlier noticed
in case of china the values were large
numbers now everything is between zero
and one this is one of the things we
need to do all right so after that the
next thing that we need to do is to
visualize this and uh we can take random
set of maybe 10,000 points and plot it
and check and see how this looks so let
us just plot this and so this is how the
original the color the pixel
distribution is these are two plots one
is red against Green and another is red
against Blue and this is the original
distribution of the color so then what
we will do is we will use K mean's
clustering to create just 16 clusters
for the various colors and then apply
that to the image now what will happen
is since the data is large because there
are millions of colors using regular K
means maybe a little time consuming so
there is another version of kin which is
called mini batch kin so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code code and also visualize this so
that we can see that there are the this
is how the 16 colors uh would look so
this is red against Green and this is
red against Blue there is uh quite a bit
of similarity between this original
color schema and the new one right so it
doesn't look very very completely
different or anything like that now we
apply this the newly created colors to
the image and uh we can take a look how
this is uh looking now we can comp both
the images so this is our original image
and this is our new image so as you can
see there is not a lot of information
that has been lost uh it pretty much
looks like the original image yes we can
see that for example here there is a
little bit uh it appears a little dlish
compared to this one right because uh we
kind of took off some of the finer
details of the color but overall the
high level information has been
maintained at the same time the main
advantage is that now this can be this
is an image which can be rendered on a
device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the sum Palace in China and we repeat
the same process this is a high defin
color image with millions of colors and
also uh
three-dimensional uh now we will reduce
that to 16 colors using K means
clustering and um we do the same process
like before we reshape it and then we
cluster the colors to 16 and then we
render the image once again and we will
see that the color the quality of the
image slightly deteriorates as you can
see here this has much finer details in
this which are probably missing here but
then that's the compromise because there
are some devices which may not be able
to handle this kind of high density
images so let's run this code in Python
notebook all right so let's apply the
same technique for another picture which
is uh even more intricate and has
probably much complicated color schema
so this is the image now once again uh
we can take a look at the shape which is
427 by 640 by 3 and this is the new data
would look somewhat like this compared
to the flower image so we have some new
values here and we will also bring this
as you can see the numbers are much big
so we will much bigger so we will now
have to uh scale them down to values
between 0o and one and that is done by
dividing by 255 so let's go ahead and uh
do that and reshape it okay so we get a
two Di dimensional Matrix and we will
then as a next step we will go ahead and
visualize this how it looks the the 16
colors and this is basically how it
would look 16 million colors and now we
can create the Clusters out of this the
16 cin clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
go ahead and uh apply this and visualize
how it is looking for with the with the
new just the 16 color so once again as
you can see this looks much richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done uh using
kin's clustering and we have also seen
in the previous examples of how to
implement K means the code to roughly
how to implement K means clustering and
we use some sample data using blob to
just execute the C in's clustering if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind so let's go
a little deeper into hierarchial
clustering let's consider we have a set
of cars and we have a group similar we
want to group similar ones together so
below we have you'll see four different
cars down there and we get two clusters
of car types sedan and SUV so if you're
just looking at it you can probably
think oh yeah we'll put the sedans
together and the SUVs together and then
at last we can group everything into one
cluster so we just have just cars so you
can see as we have this we make a nice
little tree this is very common when you
see anybody talks about hierarchial
clustering this is usually what you see
and what comes out of it we terminate
when we are left with only one cluster
so we have as you can see we bring them
all together we have one cluster we
can't bring it together anymore because
they're all together hierarchial
clustering is separating data into
different groups based on some measure
of similarity so we have to find a way
to measure what makes them alike what
makes them different a glomera of
clustering is known as bottom up
approach remember I said think of that
as bringing things together you see I
think the Latin term agow is together
because you have your glomerate rocks
where all the different pieces of rocks
are in there so we want to bring
everything together together that's a
bottom up and then divisive is we're
going to go from the top down so we take
one huge cluster and we start dividing
it up into two clusters into three four
five and so on digging even deeper into
how hierarchial clustering Works let's
consider we have a few points on a plane
so this plane is 2D so we have an XY
coordinates kind of makes it easy we're
going to start with measure the distance
so we want to figure a way to compute
the distance between each point each
data point is a cluster of its own
remember if we're going from the bottom
up a glaive then we have each point
being its own cluster we try to find the
least distance between two data points
to form a cluster and then once we find
those with the least distance between
them we start grouping them together so
we start forming clusters of multiple
points this is represented in a tree
likee structure called dendogram so this
is another key word dendogram and you
can see it is it's just the branch we've
looked at before and we do the second
second group the same so it gets its own
uh dendogram and the third gets its own
dendogram and then we might group two
groups together so now those two groups
are all under one dagram because they're
closer together than the P1 and P2 and
then we terminate when we are left with
one cluster so we finally bring it all
together you can see on the right how
we've come up all the way up to the top
whoops and we have the gray hierarchial
box coming in there and connecting them
so we have just one cluster and that's a
good place to terminate because there is
no way we can bring them together any
further so how do we measure the
distance between the data points I mean
this is really where it starts getting
interesting up until now you can kind of
eyeball and say hey these look together
but when you have thousands of data
points how are we going to measure those
distances and there is a lot of ways to
get the distance measure so let's go ahe
and take a look at that distance
measures will determine the similarity
between two elements and it will
influence the shape of the Clusters and
we have ukian distance measure we have
squared ukian distance measure which is
almost the same thing but with less
computations and we have the Manhattan
distance measure which will give you
slightly different results and we have
the cosine distance measure which again
is very similar to the ukian playing
with triangles and sometimes it can
compute faster depending on what kind of
data you're looking at so let's start
with eitan distance measure the most
common is we want to know the distance
between the two points so if we have
Point p and point Q the idian distance
is the ordinary straight line it is the
distance between the two points in ukian
space and you should recognize D equals
in this case we're going to sum all the
points so if there was more than one
point we could figure out the distance
to the not more than one points this is
the sum of more than two Dimensions so
we can have the distance between each of
the different dimensions squared and
that will give us and then take the
square root of that and that gives us
the actual distance between them and
this should look familiar from ukian
geometry maybe you haven't played too
much with multiple Dimensions so the
summation symbol might not look familiar
to you but it's pretty straightforward
as you add the distance between each of
the two different points squared so if
your y difference was 2 - 1 squared
would be two and then you take the
difference between the X again squared
and if there was a z coordinates it
would be uh you know Z1 minus Z2 squar
and then take the square root of that
and sum it all or sum it all together
and then take the square root of it so
to make it compute faster since the
difference in distances whether one is
farther apart or closer together than
the other we can do What's called the
squared ukian distance measurement this
is identical to the ukian measurement
but we don't take the square root at the
end there's no reason to certainly gives
us the exact distance but as far as
doing calculations as to which one's
bigger or smaller than the other one it
won't make a difference so we'll just go
with the so we just get rid of that div
that final square root computes faster
and it gives us the pretty much the
idian squared distance on there now the
Manhattan distance measurement is a
simple sum of horizontal and vertical
components or the distance between two
points measured along axes at right
angles now this is different because
you're not looking at the direct line
between them and in certain cases the
individual distances measured will give
you a better result now generally that's
not true most times you go with ukian
squared method because that's very fast
and easy to see but the Manhattan
distance is you measure just the yalue
and you take the absolute value of it
and you measure just the X difference
you take the absolute value of it and
just the Z and if you had more you know
different dimensions in there a b c d EF
however many dimensions you would just
take the absolute value of the
difference of those dimensions and then
we have the cosine distance similarity
measures the angle between the two
vectors and as you can see as the two
vectors get further and further apart
the cosine distance gets larger so it's
another way to measure the distance very
similar to the ukian so you're still
looking at the same kind of measurement
so it should have a similar result as
the first two but keep in mind the
Manhattan will have a very different
result and you can end up with a bias
with the Manhattan if your data is very
skewed if one set of values is very
large and another set of values is very
small but that's a little bit beyond the
scope of this it's just important to
know that about the Manhattan distance
so let's dig into the gomera of
clustering and gomric clustering begins
with each element as a separate cluster
and then we merge them into a larger
cluster how do we represent a cluster of
more than one point so we're going to
kind of mix the distance together with
the actual agglomerative and see what
that looks like and we're actually going
to have three key questions that are
going to be answered here so how do we
represent a cluster of more than one
point so we want to look at the math
what it looks like mathematically and
geometrically how do we determine The
Nearness of clusters when to stop
combining clusters always important to
to have your computer script or your
whatever you're working on have a
termination point so it's not going on
eternally we've all done that if you do
any kind of computer programming or or
writing of script let's assume that we
have six data points in a ukian space so
again we're dealing with X Y and Z in
this case just X and Y so how do we
represent a cluster of more than one
point let's take a look at that and
first we're going to make use of
centroids very common terminology in a
lot of machine learning languages when
grouping things together so we're going
to make use of Centro which is the
average of its points and you can see
here we're going to take the one two and
the 2 one and we're going to group them
together because they're close and if we
were looking at all the points we'd look
for those that are closest and start
with those and we're going to take those
two we're going to compute a point in
the middle and we'll give that point5
1.5 1.5 and that's going to be the
centroid of those two points and next we
start measuring like another group of
points we got 41 5 0 when they're pretty
close together so we'll go ahead and set
up a centroid of those two points in
this case it would be the
4.5 and 05 would be the measurements on
those two points and once we have the
centroid of the two groups we find out
that the next closest point to a
centroid is over on the left and so
we're going to take this and say oh 0 0
is closest to the 1.5 1.5 centroid so
let's go ahead and group that together
and we compute a new centroid based on
those three points so now we have a
centroid of 1.1 or 1 comma 1 and then we
also do this again with the last point
the 53 and it computes into the first
group and you can see our dagram on the
right is growing so we have each of
these points are become connected and we
start grouping them together and finally
we get a centroid of that group too and
then finally the last thing we want to
do is combine the two groups by their
centroids and you can see here we end up
with one large group and it'll have its
own centroid although usually they don't
compute the last centroid we just put
them all together so when do we stop
combining clusters well hopefully it's
pretty obvious to you in this case when
they all got to be one but there are
actually many approaches to it so first
pick a number of clusters K up front and
this is done in the fact that we don't
want to look at 200 in clusters we only
want to look at the top five clusters or
something like that so we decide the
number of clusters required in the
beginning and we terminate when we reach
the value K so if you looked back on on
our clustering let me just go back a
couple screens you'll see how we
clustered these all together and we
might want just the two clusters and so
we look at just the top two or maybe we
only want three clusters and so we would
compute which one of these has a wider
spread to it or something like that
there's other computations to know how
to connect them and we'll look at that
in just a minute but to note that when
we pick the K value we want to limit the
information that's coming in so that can
be very important especially if you're
feeding it into another algorithm that
requires three three values or you set
it to four values and you need to know
that value coming in so we might take
the clustering and say okay only uh
three clusters that's all we want for K
so the possible challenges this only
makes sense when we know the data well
so when you're clustering with K
clusters you might already know that
domain and know that that makes sense
but if you're exploring brand new data
you might have no idea how many clusters
you really need to explore that data
with let's consider the value of K to be
two so in this case in our previous
example we stop near left with two
clusters and you can see here that this
is where they came together the best
while still keeping separate the data
the second approach is stop when the
next merge would create a cluster with
low cohesion so we keep clustering till
the next merge of clusters creates a bad
cluster low cohesion setup on there that
means the point is so close to being
between two clusters it doesn't make
sense to bring them together but how is
cohesion defined oh let's dig a little
deeper into cohesion the diameter of a
cluster so we're looking at the actual
diameter of our cluster and the diameter
is the maximum distance between any pair
of points in the cluster we terminate
when the diameter of a new cluster
exceeds the threshold so as that
diameter gets bigger and bigger we don't
want the two circles or clusters to
overlap and we have radius of a cluster
radius is the maximum distance of a
point from centroid we terminate when
the radius of a new cluster exceeds the
threshold again we're not we don't want
things to overlap so when it crosses
that threshold and is overlapping with
other data we stop so let's look at
divisive clustering remember we went
from the bottom up now we want to go
from the top down divisive clustering
approach begins with a whole set and
proceeds to divide it into smaller
clusters so we start with a single
cluster composed of all the data points
we split it into different clusters this
can be done using monothetic divisive
methods what is monothetic divisive
method and we'll go backwards and let's
consider the example we took in the
agglomerative clustering to understand
this so we consider a space with six
points in it just like we did before
same points we had before and we name
each point in the cluster so we have in
this case we just gave it a letter value
a b CDE e f since we follow top down
approach in divisive clustering obtain
all possible splits into two columns so
we want to know where you could split it
here and we could do like a Ab split and
a cdef split we could do BCE ADF and you
can see this starts generating a huge
amount of data
ABCDEF and so for each split we can
compute cluster sum of squares and we
can see here the actually have the
formula out for us B J12 equals N1 of
absolute value of x minus absolute value
of x squared so again we're Computing
all the different distances in their
squared back to your kind of ukian
distances on that and so we can actually
compute B AJ between clusters 1 and two
and we have the mean of the cluster and
the grand mean depending on number of
members in the cluster and we select the
cluster with the largest sum of squares
let's assume that the sum of squared
distance is largest for the third split
we had up above and that's where we
split the ABC out and if we split the
ABC out we're left with the DF on the
other side we again find the sum of
squared distances and split it into
clusters so we go from ABC we might find
that the a splits into b c and d into e
F and again you start to see that
hierarchial dendogram coming down as we
start splitting everything apart and
finally we might have a splits in b and
c and then each one gets their own DF
and it continues to divide until we get
little nodes at the end and every data
has its own point or until we get to K
if we have set a k value so we've kind
of learned a little bit about the
background and some of the math in
hierarchial clustering let's go ahead
and dive into a demo and our demo today
is going to be for the problem statement
we're going to look at us oil so a US
oil organization needs to know its Sals
in various States in us and cluster of
the states based on the sales so what
are the steps involved into setting this
problem up so the steps we're going to
look at and this is really useful for
just about any processing of data
although I believe we're going to be
doing this in R today we're going to
import the data set so we'll explore our
data a little bit there create a scatter
plot it's always good to have a visual
if you can once you have a visual you
can know if you're really far off in the
model you choose to Cluster the data in
or how many splits you need and then
we're going to normalize the data so
we're going to fix the data up so it
processes correctly we'll talk more
detail about normalization when we get
there and then calculate the ukian
distance and finally we'll create our
dendogram so it looks nice and pretty
and we have something we can show to our
shareholders so that they have something
to go on and know why they gave us all
that money and salary for the year so we
go ahead and open up R and we're
actually using R Studio which is the
really has some nice features in it it
automatically sets up the three Windows
where we have our script file on the
upper left and then we can execute that
script and it'll come down and put it
into the console bottom left and execute
it and then we have our plots off to the
right and I've got it zoomed in
hopefully not too large a font but large
enough that you can see it and let's
just go ahead and take a look at some of
the script going in
here it's clustering analysis and we're
going to work we'll call it my data and
we're going to assign it in R this is a
symbol for assigning and we're going to
go read
CSV read CSV file and we'll put that in
Brackets and let's before we go any
further let's just look at the data
outside of R it's always nice to do if
you
can and the file is going to be called
utilities. CSV this would also be the
time to get the full path so you have
the right path to your file and remember
that you can always Post in the comments
down
below and when you post down there just
let us know you want to connect up with
simply learn so that they can get you
this file so you can get the same file
we're working on and you can repeat it
and see how this works this is
utilities. CSV it's a comma separated
variable file so it's pretty
straightforward and you can see here
they have the city fixed charge and a
number of different features to the data
and so we have our Ro cost load demand
cells nuclear fuel cost on here and then
going down the other side we have US
cities Arizona Boston they have Central
us so I guess they're grouping a number
of areas together the Commonwealth area
you can see down here Nevada New England
Northern us Oklahoma the Pacific region
and so on so we have a nice little chart
of different data they brought in
and so I'm going to take that complete
path that ends in the utilities.
CSV and we're going to import that file
let me just enlarge this all the way
oops I had an extra set of brackets here
somehow maybe I missed a set of
brackets this can happen if you're not
careful you can get brackets on one side
and not the other or in this case I got
double brackets on each side there we go
and then the magic hot keys in this case
are your controll enter which will let
me go ahead and run the script and so
I've now loaded the data and as you can
see I went ahead and shrunk the plot
since we're going to be looking at the
window down below and we can simply
convert the data to a string now all of
us do this automatically the first time
we say hey just put it all out as a
string and then we get this huge mess of
stuff that doesn't make a whole lot of
sense so and you can see here they have
uh you can probably kind of pull it
together as looking at it but let's go
ahead and just do the head I we'll do
the head of my
data there we go and control enter on
that and the head shows the first five
rows you'll see this in a lot of
different scripts in R it's you type in
head and then in Brackets you put your
data and it comes through and list the
first five rows as you can see below and
it shows Arizona Boston Central and it
has the same columns we just looked at
so we have the fixed charge the RO the
cost the load the D demand I'm not an
expert in oil so I'm not even sure what
D demand is cells I'm guessing nuclear
how much of it's supplied by nuclear and
the actual fuel
cost and then the different states that
it's in or different areas and one of
the wonders of R is all these cool easy
to ous tools that are so quick so we'll
do
Pairs and pairs creates a nice graph so
let me go ahead and run
this uh whoops the reason it gave me an
error is because I forgot to resize it
so let me bring my plot way out so we
can see it and let's run that again and
you'll see here that we have a nice
graph of the different data and how it
plots together how the different points
kind of come
together this is neat because if you
look at this I would look at this and
say hey this is a great candidate for
some kind of clustering and the reason
is is when I look at any two pairs so
let's go down to say cells and fuel cost
towards the bottom right and when you
look at them where they cross over you
sort of see things how they group
together but it's a little confusing you
can't really pinpoint how they group
together you could probably look at
these two and say yeah there's pretty
good commonalities there and if you look
at any of the other pairs you'll start
seeing some patterns there also and so
we really want to know what are the
patterns on all of them put together not
just any two of them but the whole setup
let me let me go ahead and Shrink my um
this down for just a second just a notch
here and let's create a uh scatter plot
oops and this is simply just use the
term plot in Brackets and then which
values do we want to plot and if we
remember when we looked at the data
earlier let me just go back this
way in this case let's go ahead and
compare two just two values to see how
they
look and we'll do fuel cost and cells
and it's in my data so we got to let her
know which two columns we're looking at
next to each other and it will open up
our plot thing and then go ahead and
execute that and we can see on those
closeup of what we were just looking at
in the
pairs and if I was eyeballing this I
would say oh look there's a kind of a
cluster up here of five items and this
one it's hard to tell which cluster it'd
be with but maybe it's six It Go in the
top one and you have a middle cluster
and a bottom clust cluster maybe two
different clusters so you can sort of
group them together fuel cost and the
cells and see how they connect with each
other and again that's only two
different values we're looking at so in
the long run we want to look at all of
them and then the people in the
back they sent me the gript so we can go
ahead and add labels so with my data
text fuel cost cells the labels equals
City position four these are numbers you
can kind of play with till they look
nice and oops again I forgot to resize
my plot it doesn't like having it too
small we'll run
that I miss type something in
here oh I did a lowercase D and they did
a capital D there we go so now we can go
in here and do this with my data and you
can see a little hard to see on my
screen with all the the different things
in there it plots the actual cities so
we can now see where all the cities are
in connection with in this case fuel
cost and cells so you have a nice label
to go with the
graph and then we can also go ahead and
plot in this case let's do oh the r o r
oops and we'll do
that also with the cells and do my data
remember to leave it lowercase this time
so when we plot those two we'll come
over here and it's going to I'm
surprised it didn't give me an error
and then we'll also add in the U wi
statement so we put some nice labels in
there and it's going to be the same as
before except instead of doing the fuel
cost cells we want the r cells and we'll
execute
that oops and of course it gives me an
error because I shrunk it down so let's
redo those two
again there we go and we can see now we
have the RO with cells and we'd probably
get a slightly different clustering here
if I was looking at these cities they're
probably different than what we had
before but you could probably look at
this and say h these kind of go together
and those kind of go together but again
we're going to be looking at all of them
instead of just one or
two and so at this point we want to dive
into the next step we're going to start
looking at a little bit of coding or
scripting
here this is very important because
we're going to be looking at
normalization we put that in there Norm
alization and if you've done any of the
other machine learning skills and setup
this should start to look very normal in
your pre-processing of data whether
you're in r or python or any of these
scripts we really want to make sure you
normalize your data so it's not biased
remember we're dealing with distances
and if I have let's say the RO is even
look at this graph here on the right you
can see where my RO varies between 8 and
14
that's a very small variable and our
cells varies between 4,000 and
16,000 so you can imagine the distance
between 4,000 and 8,000 which is a
distance of 4,000 versus 8 to 10 versus
2 cells is going to dominate so if we do
any kind of special work on this it's
going to look at cells and it's going to
Cluster them just by the cells alone and
then Ro might have a little tiny effect
of two versus 4,000 we want to level the
playing
field turns out there's actually a
number of ways in script to
normalize so I'm just going to put in
the code that they put together in the
back for me and let's talk about it a
little bit so we have Z we're going to
sign it to my
data and it's go ahead
and we're going to do a little reshaping
across all
rows or I mean across all columns so
each of the rows is going to have a
little reshaping there there and then
we're going to get M which stands for
means and we're going to apply it to my
data so again we want to go ahead and
create a um the most common variable in
there and then s is going to be SD
stands for standard
deviation so instead of just doing a lot
of times what they do with normalization
of data is we just reshape the data
everything between zero and one so that
if the lower end is eight that now
become zero and the upper end is 14 that
now becomes
one that doesn't help if it is not a
linear set of data so with this we're
going to look for the means and the
standard deviation for reshaping the
data and that way the most common values
now become the kind of like the center
point and then the standard deviation is
how big the spread so we want the
standard deviation to be equal amongst
all of them and then finally we go ahead
and take Z and with the Z we're going to
reassign it and we're going to scale the
original my data which we
re-shaped based on M and based on the
standard
deviation and the two in here that just
means we're looking at everything in
kind of a XY kind of
plot and we can quickly run these contrl
enter contrl enter contrl enter contrl
enter so now we have Z which is a scaled
version of my
data and now we can go ahead and
calculate the ukian
distance oops
CCU there we
go and in um R this is so easy once
you've got to here we've done all that
pre dat
processing we'll call it
distance and we'll assign this
to dist so Di
s is the computation for getting the
ukian distance and we can just put Z in
there because we've already reformatted
and scaled Z to fit what we want let me
go ahead and just hit enter on that and
I'm going to widen my left hand side
again I'm always curious what does this
data look like so let's just type in
distance which will print the variable
down below oops you have to hit control
enter and this print had a huge amount
of
information as you can see just kind of
streams down
there and let's go ahead and enlarge
this and I don't know about you but when
I look at something like this it doesn't
mean a whole lot to me other than I see
2 3 4 5 six and then you kind of have
the top part 6 17 18 so imagine this is
like a huge chart is what we're looking
at
and we can go ahead and use print oh
distance
[Applause]
digits equal three and let's run
that oops I keep forgetting that it has
to go through the graph on the right and
we see a different slightly different
output in here let me just open this up
so we can see what we're looking at and
by cutting down the distance you can
start to see the patterns here of it's
looking at the different
distances uh so if I go to the top we
have the distance between one and two 1
and three 1 and four 1 and 5 one and six
and then two and three and so on
obviously distance between itself is
zero and it doesn't repeat the data so
we don't care to see two versus one
again because we already know the
distance between one and
two and so we have a nice view of all
the distances in the chart and that's
what we're looking at right here and
it's a little easier to read that's why
we did the print statement up here to do
digits equals three make it a little bit
smaller we could even just do digits
oh let's just do two see what that looks
like we might lose some data on this one
if it's uh if something's way off but we
have a nice setup and we can see the
different distances and that's what we
were computed here between each of the
points and then the whole reason we're
doing this is to get ourselves a nice
dagram going a nice clustering dagram
we'll do a couple of these looking at
different
things we'll take a variable HC DOL and
we're going to assign it H
cluster and then
distance that easy we've already
computed the distances so the H
clustering does all the work for us and
let me hit enter on there so now we have
our HCL which is assigned the H
clustering computation based on
distances and a part and then I'm going
to expand my graph because we would like
to go ahead and see what this looks like
and we can simply plot
that and hit the control enter so it
runs and look at that we have a really
nice clustering dendogram except when I
look at it the first thing I notice is
it really shows like numbers down below
now if you were a shareholder and some
data scientist came up to you and said
look at this this is what it means you'd
be looking at that going what the heck
does 394 19118 mean so let's give it
some words
there so let's do the same plot with our
HCL
HC there we
go and let's add in labels and this is
just one of the commands and plots so we
have labels equals my
data and then under my data we want to
know the city and we'll have it hang
minus one that's just the instructions
to make it pretty so we'll run
that oops I accidentally ran just hang
my one let me try that again there we go
okay so now you can see what it's done
and the and the hang my one turns it
sideways that way we can see Central
which is Central us and Kentucky and we
start to actually get some information
off our clustering setup and the
information you start looking at is that
we put all the information together you
probably want to look at Central America
and Kentucky together Oklahoma and Texas
has a lot of commonality as does Arizona
and Southern us and you can even group
all five of those Florida Oklahoma Texas
Arizona and Southern us these regions
for some reason share a lot of
similarities and so we want to start
asking what those similarities are but
this gives us a place to look it says
hey these things really go together you
should be grouping these together as far
as your CES and what's what you're
doing and then one of the things you
might want to do is there's also we can
do the dagram
average and this changes how we do the
clustering so it looks very similar like
we had before there's our HCL we're
going to assign it we're going to do an
H cluster we're still doing it on
distance oops distance and this time
we're going to set the method to average
so we can change the methodology in
which it computes the values and before
if you remember correctly we did median
median a little bit different than means
we did the most common one and then we
want the average of the median and let's
go ahead and run
that and then we can
plot and here's our
HCL oops there we go here's our HCL and
I can run that plot and you can see this
changed a little bit so our way it
computes and groups things looks a
little different than it did before and
let's go and put the cities back in
there and do the hang copy let me just
real quickly copy that down
here because we want the same
labels and again you can see Nevada
Idaho pujet I remember we were looking
at um Southern us and Arizona Texas and
Oklahoma Florida so the grouping really
hasn't changed too much so we still have
a very similar grouping it's almost kind
of flipped it as far as the uh distance
based on average has but this is
something you could actually take to the
shareholders and say hey look these
things are connected and at which point
you want to explore a little deeper as
to why they're connected because they're
going to ask you okay how are they
connected and why do we
care but that's a little bit beyond the
actual scope of what we're working on
today but we are going to cover
membership what's called clustering
membership on
there and let's create a member we'll
just call it Member One oops there
member. one and we're going to assign to
this
we're going to do cut
tree and cut
tree it limits it so what that means is
I take my HC
dot l in
here oops there we go L and so I'm
taking the cluster I just built and we
want to take that cluster and we want to
limit it to just a depth of three so we
go ahead and do that and run that one
oops let me go run there we go so now
I've created my member
one and then whoops let me just move
this out of the way we're going to do an
Aggregate and we're going to use Z
remember Z from above and we're going to
turn Member One into a list and then
we're going to aggregate that together
based on the mean let me go ahead and
enter run
that I did member L it's actually Member
One
there we
go and if we take a look at this we now
have our group one fixed charge and then
all your
different columns listed there and most
of them should come up kind of looking
between zero and one but you'll see a
lot of variation because we're varying
it based on the means so it's a means
the standard deviation not just forcing
it in between zero and one which is a
much better way usually to normalize
your data than just doing the 01
setup and finally we can actually uh
look at the actual
values and the same chart we just did
oops I made a mistake on there with my
data there we go
okay and again we now have our actual
data instead of looking at just the if
you looked up here it's all between zero
and one
and when we look down here we now have
some actual connections and how far
distance this different data is uh again
because more of a domain issue in
understanding the oil company and what
these different values means and you can
look at these as being the distances
between different items so a little bit
different View and you have to really
dig deep into this data but we really
want you to take away from this is the
dendogram and the charts that we did
earlier
and that is the cluster output and our
nice dagram so this would be stage one
in data analysis of the cells again
you'd have to have a lot more domain
experience to find out what all the
individual numbers we looked at mean and
what the distance is and what's
difference between Central America and
Kentucky and why they're similar and why
it groups all of Central Kentucky
Florida Oklahoma Texas Arizona and
Southern us together into one larger
group so be Way Beyond the scope of this
but you can see how we start to explore
data and we start to see things in here
where things are grouped together in
ways we might not have seen before and
this is a good start for understanding
and giving advice for sales and
marketing maybe Logistics City
development there's all kinds of things
that kind of come together in the
hierarchial clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know as this
case if you're the lowest sales no one
wants to be in the region of the lowest
sales if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IVM should be your
right choice for more details use the
link in the description box below with
that in mind regression algorithms is
used to find out the connection between
dependent and independent variables
dependent variables are nothing but a
variable that we are trying to predict
or forecast and independent variables
are the factor that influence the
analysis it is usually used to make
projections medical researchers
frequently utilize linear regression to
study the connection between patient
blood pressure and dosage researcher may
give patient different quantities of a
specific medication and track the how
their blood pressure changes a
regression model might use doses as a
predictor variable and blood pressure as
a response variable there are some
popular regression algorithms that come
under supervised machine learning first
one is linear regression regression
trees nonlinear regression basial linear
regression after discussing what is
supervised learning and its types let's
move forward to see what actually
regression analysis is
a St technique known as regression
analysis is used to simulate the
relationship between dependent and
independent variables using one or more
independent variables regression
analysis more precisely enables us to
comprehend and when other independent
variables are held constant changes in
the dependent variables value that
correspond to an independent variables
are changing prediction include values
that are continuous or real such as
temperature nature age pay and cost
there are some following concept or
terminologies that must be understood in
order to completely grasp regression
analysis the first one is dependent
variable in a regression analysis the
primary variable we wish to predict or
comprehend is the dependent variable
here dependent variables are also known
as Target variable the next one is
independent variable the term
independent variables also known as
predictor refers to the elements that
impact the dependent variables or are
used to forecast their values and the
third one is outliers an observation
that is extremely high or extremely low
in relation to the other observed values
is referred as an outlier an outlier
should be avoided as it might affect the
outcomes the fourth one is
multicolinearity the situation is set to
as occurring when independent variables
have a higher correlation with one
another than other variables it
shouldn't be included in the data set
because it causes issues when
determining which variable has the
greatest impact and the fifth one is
overfitting and underfitting overfitting
is the problem that occurs when our
system problems well well with the
training data set but poorly with the
test data set underfitting is the term
used when an algorithm does not perform
well even with the training data set
after understanding what regression
analysis is and its terminologies let's
move forward and see why we use
regression analysis including those
involving the weather sales marketing
Trends and other factors in this
situation we need regression analysis
that can make forecast more precisely so
let's understand the concept of
regression analysis using an example
business frequently use linear
regression to understand the
relationship between advertising
spending and income for instance they
might run a simple linear model with
advertising as a response variable and
revenue as a predictor variable the
regression model would look like this
the V equal to Beta 0 plus B1 add
spending where there are no advertising
the coefficient Z would indicate the
whole expected Revenue when advertising
expenditures are increased by one unit
coefficient will represent the typical
change in the total revenue that is $1
if beta 1 is negative then higher
advertising expenditure will result in
lower Revenue if if beta 1 is nearly
zero advertising expenditures have
little impact on revenue and if beta 1
is positive it would imply that more
advertising expenditures are linked to
higher
income the amount of beta1 will
determine whether a corporation decide
to or decrease its advertising
budget regression means creating a graph
that connects variable the best fit the
given data point the direction analysis
model can then make prediction about
data using the plot regression analysis
is defined as showing a line or a curve
that goes through all all the data
points on the target predicted graph in
such a way that vertical distance
between the data points and the
regression is the smallest whether a
model has captured a strong link between
determined by the distance between data
points and the line there are some more
use case of regression analysis like we
can use temperature and other variable
to predict or forecast rain regression
analysis can be used for identification
of market trends for predicting traffic
accidents caused by reckless driving we
can use regression analysis for the same
and many more so by understanding why we
need regression analysis moving forward
let's see some popular regression
algorithms in
detail the first one is linear
regression linear regression is one of
the most famous and straightforward
machine learning algorithm utilized for
predictive analysis linear regression
show the linear connection between
dependent and independent factors the
line of the equation is y = to mx + b
here y stand for the response variable
or a dependent variable whether X is for
the predictor variable or an independent
variable here m is the estimated slope
and B is for the estimated intercept and
the next one is regression trees a
regression trees is worked through a
cycle known as binary recursive which is
iterative interaction that divides the
information into segments or branches
and afterward keep splitting the data
into smaller group as the technique
climbs each branch this tree is used for
the dependent variable which continuous
values for example a regression tree is
name food which is divided into segments
V and nonv further it keep splitting
into smaller groups and the third one is
nonlinear regression nonlinear
regression is a type of regression
examination wherein information is to
fit a model and afterward communicated
with the numerical function simple
linear regression relates two Factor X
and Y with a straight line Y = MX plus b
while nonlinear regression relates the
two factor in a nonlinear relationship
nonlinear regression can be predict
population growth over time or the
relationship between GTP and a country's
time and the fourth one is basan linear
regression the algorithm is a way to
deal with a linear regression in which
statical examination is attempted inside
the setting of basian inference linear
regression and basian regression can
generate the same prediction and with
the help of basent processing we can
retrieve the complete variety of
inferential solution instead of a point
estimate after seeing some regression
analysis algorithm let's move forward
and see advantages and disadvantages of
different regression analysis
models and the first model is linear
regression model the advantages are work
well irrespective of data set size and
the second one is gives information
about the relevance of features and the
disadvantages are the assumptions of
linear regression and the next one is
polinomial regression the advantages of
works on any size of the DAT set and the
second one is works very well on
nonlinear problems and disadvantages are
we need to choose the right polinomial
degree for the variance stateof the next
one is support Vector regression
algorithms and the advantages are easily
adaptable works very well on nonlinear
problems and disadvantages are difficult
to understand and not well known and the
next one is decision tree regression the
advantages of decision trees are no need
to apply scaling and the second one is
works very well on both linear and
nonlinear problems and disadvantages are
poor results on a small dat set and
overfitting can easily occur and the
last one is random Forest regression the
advantages are powerful and second one
is accurate and disadvantages are no
interpretability and the second one is
overfitting can easily occur after
seeing advantages and disadvantages of
different types of regression analysis
model let's move forward and see some
applications of regression
analysis the first one is forecasting
regression analysis is frequently used
in business to predict potential
opportunities and dangers for example
demand analysis predicts how many items
a buyer is likely to purchase demand
however is not only the dependent
variables when it comes to business much
more than just direct income can be
predicted using regression analysis and
and the second one is capm capital asset
pricing model the linear regression
model is a key component of the capital
asset pricing model capm which
determines the relationship between an
asset expected return and the associated
Market risk premium financial analyst
typically use it for forecast corporate
return and operational performance when
doing financial analysis and the third
one is comparing with competition it can
be used to evaluate how well a business
is doing financially in comparison to a
certain rival it can also be used to
figure out how the stock prices of two
companies are related to one another
this can be extended to find the
correlation between two competing
companies it might help the company
identifying the factors affecting the
sales in contrast to the comparable
company these methods can help small
business succeed quickly within a short
period of time and the last one is
identifying problem regression is a
beneficial for identifying incorrect
judgment as well as for providing
factual support for the management for
instance a retail store manager might
believe that extending the hours of
operation will greatly increase sales if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learn professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be right choice for more details
use the link in the description box
below with that in
mind so let's go ahead and talk about
overfitting uh when we talk about
overfitting it's a scenario where the
machine learning model tries to learn
from the details along with the noise
and the data tries to fit each data
point on the curve uh you can see that
um if you plug in your coordinates
you're just going to get the whatever
it's fited every point on the data
stream there's no average there's no two
points that might have the you me a y
might have two different answers cuz uh
if the wind blows a certain way um and
your efficiency of your car maybe you
have a headwind so your car might alter
how efficient it is as it goes and so
there's going to be this variance on
here and this says no you can't have any
variance with you know the this is it's
going to be exactly this there can't be
any you can't be the same speed or the
same car and have a slightly different
efficiency
so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the
prediction uh so you'll get like a
really high error on
here and so uh reasons for overfitting
uh data used for training is not cleaned
and contains noise garbage values in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of
something wrong with the data coming in
it needs to be addressed whether it's a
source of the data maybe they use in
medical different measuring tools uh so
you now have to adjust for data that
came in from hospital a versus Hospital
b or even off of machine a and machine B
that's testing something and those those
numbers are coming in
wrong the model has a high variance uh
again wind is a good example I was
talking about that with the car you may
have 100 tests but because the wind's
blowing it's all over the
place uh size of training data used is
not enough so a small amount of data is
going to also cause this problem you
only have a few points and you try to
plot everything the model is too complex
uh this comes up a lot we put too many
pieces together and how they interact
can't even be tracked um and and so you
have to go back back break it up and
find out actually what correlates and
what
doesn't
so what is underfitting a scenario where
machine learning models can neither
learn the relationship between the data
points nor predict or classify a new
data point and you can see here we have
our efficiency of our car and our line
drawn and it's just going to be way off
for both the training and the predicting
data as the model doesn't fully learn
the patterns it accepts every new data
point during the prediction so instead
of looking for a general pattern uh we
just kind of accept
everything data used for training is not
cleaned and contains noise garbage and
values again underfitting and
overfitting same issue you got to clean
your
data the model has a high bias uh we've
seen this in all kinds of things
from uh the mo the most common is the
driving cars to facial identification or
whatever ever it is the model itself
when they build it might have a bias
towards one thing and this would be an
underfitted model would have that bias
because it's averaged it out so if you
have um five people from India and 10
people from um Africa and 20 people from
the US you've created a bias uh because
it's looking at the 20 people and you
only have a small amount of data to work
with size of training data used is not
enough
uh that goes with the size I was just
talking about so we have a model with a
high bias we have size of training data
used it's not enough the model is too
simple again this a one straight line
through all the data when it needs has a
slight shift to it for other
reasons so what is a good fit uh a
linear curve that best fits the data is
neither overfitting or underfitting
models but is just right and of course
we have the nice examples here where we
have overfitting lines going up and down
every Point's starting to be include
included
underfitting um the line really is off
from where the data is and then a good
fit is got to get rid of that minimize
that um error coming through so this is
all exciting but what does this look
like so we really need to jump in and
put a code together and see what this
looks like when we're programming for
this uh demo we'll bring up our trusty
anaconda and go into to Jupiter notebook
for
python move myself out of the way here
uh and so we're going to start off this
is going to be a demo on overfitting and
underfitting using
Python and let's start with our uh
Imports now if you've been through
enough of these tutorials we don't want
to spend a huge amount of time on what
we're bringing in and what we're doing
uh so you should be up on doing this
with python and how to bring in your
different modules uh we're going to
bring in the sklearn or the uh s kit
processing SK learn. neural network
import
MLP regressor so there's our regressor
model right there uh that's going to be
our linear regression model and we have
our metrics mean absolute error if you
remember we had our U that's how we
figure out how well it fits is how far
off that error is based on the um mean
square error value msse and then of
course numpy because we just like to
work with numpy it's a great data array
uh we always import it as MP that's the
most common way of doing it and then we
have SK learn model selection import
validation curve so we're going to look
at a validation curve to see how good
our models are and then we have the uh
data set we'll use the um very famous
Iris data set and that's embedded in the
S kit so the SK key learn data sets have
a load Iris in there and then we have
the matplot library because if you're
doing any kind of demo or showing this
off to your shareholders we want to have
something nice to display it
on and then we have sklearn model
selection we're going to import import K
fold and we'll talk about that when we
get to it uh and then we're going to go
ahead and do um just for our numpy we're
going to do like a random seed for
random numbers and then for our plot
style we'll use the GG plot that's just
some backend setup um you could even
probably leave the plot style out uh
depending on what version of um you're
using of um depending on what version
you're using a map plot
library and then we'll go ahead and run
this uh it's not going to do anything
that we can visibly see CU it's just
loading those modules and then we also
want to load our Iris uh data in here
and the iris data has an iris data and
Iris Target uh we're going to load that
as X and
Y and just so you can have an idea what
we're talking about we're going to go
ahead and print
um X and just the first bit of X we'll
just do the top of X and we'll also
print y so you can see what the top of Y
looks like uh print
y uh and since we're in numpy we're
going to go ahead and do our own thing
if this was uh of course some pandas we
could just do the head of it and see
what it looks like and you can see here
we've loaded this up and in X we have
these different measurements that they
take of the flower the iris flowers from
the this particular data set and what
kind of flower it is it's going to be a
zero a one or a two is actually what the
target comes out of even though that
doesn't show in
here and so we're going to come in here
and we're going to use the K folds cross
validation with 20
folds um and a good catch that this was
a model selection we're we're we're
going through and we're selecting
different parts of the data in here here
we use K folds cross validation with 20
folds k equal 20 to evaluate the
generalization efficiency of the model
within each fold we will then estimate
the training and test error using the
training and test sets
respectfully so here we have our KF
equals KF uh kfold here's our splits on
the top then we need to go ahead and
have our list training error we're going
to create an array for that we're going
to list our testing error
and for train index and test index in
kf. spit X XT Trin y uh X train and X
test we're going to go ahead and split
up our our data our X values and the
same thing with the Y values so now we
have an X train and an X test a y train
and a y
test and then here's our model our m m
uh MLP
regressor that's your linear regression
model in there and we have used a
multi-layer Perron MLP so this is a
neural network uh multi-layer Perron
that's what the MLP is for regressor
means that it is dealing with numbers
we're not categorizing
things
um and then let's go ahead uh kind of
went off the screen here we'll just go
ahead and bring that down it's a class
of feed forward artificial neural
networks uh they kind of loosely call
ITN don't get caught up in the Ann nnn
mnn there's NN is neural network and
then everybody puts their own flavor on
it depending on what they're doing uh so
if you see the NN you know you're
dealing with the neural
network so we go ahead and fit our data
here's our model.fit we have X train and
Y train and the YT Trin data we're going
to predict equals uh model. predict XT
Trin so here's our prediction of what
it's going to be so we've trained it and
we've predicted it we've trained our our
the train data and then we have our y TR
train and then we have our y test and
the Y test equals a model predict X
test now notice what we did here is
we're going to use our model to predict
what we think y should be but this is
the training Set uh so we've trained it
with this data now we want to see how
good our model fits our training data
and then we want to see how well it fits
our testing data so we take our fold
training error mean absolute error y
train y train data predict and we're
going to do our full testing error the
mean absolute error of Y test and Y test
data
predict and we do this and here's our
mean absolute error there's our um uh a
little bit different connotation but
that's that's taking the Square value um
and finding the in this case just using
the absolute value so instead of the
Square value we get rid of the minus and
pluses by using an absolute value and we
find the average of that and that works
the same way as doing a squared value
and then we take our list training error
and we're going to go and just depend it
for each uh each one of these runs we go
through so every time we fold the
data think of it like this uh we want to
go ahead and take a piece of data that's
going to be one piece of the data and
we're going to look
at each section and we want to go
through each section to see how well it
does and splits it up this way we have a
nice picture uh when we're looking at it
from a distance I do this a lot when I
do X and when I split my X train and my
y train I'll take 2/3 of the data and
then 1/3 of the data and then I'll
switch it and I'll do three different
models so I can really see how well it
tests out and how that averages
out this is the same thing but with the
uh with the K fold we're doing and we're
doing it across 20
sections we'll go ahead and run
this and when we run this it's not too
exciting because we're just loading up
the data and appending it into our
list
and so we want to take with this uh is
we're going to go ahead and plot it and
this is where we can really see what's
going on this is where where it gets
exciting uh so we take it and we're
going to create uh a couple subplots uh
that way we have a nice setup down here
we're splitting it up into a couple
different uh
graphs and let's go ahead and run this
and then we'll walk through it a little
bit uh so our subplot comes in
um there's our sub plot and then our PLT
plot we're going to do in there range
one uh we're going to ahead and do the
splits plus one in p list training error
uh Ravel it um this is of course just a
code to how we properly set it up on
there so that it sees it correctly and
then we have our X label which is our
number fold uh plot the Y Lael training
error plot the title training error
across folds plot the tight layout plot
the subplot so we're going to move on to
this is one two one one two2 there's our
one just one and
two it has to do with how it how it
layers it on there for doing multiple
plots because you can do all kinds of
cool things with our plot our pip plot
Library uh and again we're going to go
ahead and do the same thing for the
error and we end up with our training
eror cross folds and our testing eror
cross
folds and so you can see these different
folds how they kind of Spike and how
they look and so we're talking about
overfitting or underfitting we're
comparing these two graphs
and if one of them is more off than the
other one uh if you're looking at these
two graphs you got to say hey is this
one uh um overfit or underfit and this
is always a good question to ask I mean
what do we got going here is that
overfit or is that underfit and I would
say based on these two graphs and the um
training data uh the training data is
more sporadic than the testing
data so I would look at this and say hey
uh this might need to be fit a littleit
bit better um maybe we don't have enough
data with the iris we probably don't
something else is going on here so it's
a little underfit maybe a different
model would fit better um I would not
use a neural network model for this I
would actually use just a basic linear
reg uh linear model on
this lot of different choices uh but
this gives you an idea what we're
looking at is how chaotic are these two
is it getting better or is it getting
worse if at some point the training data
gets so much better than the testing
data
you know you've overfitted and that's
where you start running into the
overfitting this to me looks like it's
underfit so that concludes underfitting
and overfitting if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind
machine learning has improved our lives
in a number of wonderful ways today
let's talk about some of these I'm Rahul
from Simply learn and these are the top
10 applications of machine learning
first let's talk about virtual personal
assistants Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever noticed
that these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to logs cricket ground the first thing I
would probably do is to get on Google
Maps so search
it
and let's put it
here so here we have the path you should
take to get to Lodge cricket ground now
here the map is a combination of red
yellow and blue now the blue regions
signify a clear road that is you won't
encounter traffic there yellow indicate
that they're slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as I told you
before red means heavily congested
yellow means slow moving and blue means
clear so how exactly is Google able to
tell you that the traffic is clear slow
moving or heavily congested so this is
with the help of machine learning and
with the help of two important measures
first is the average time that's taken
on specific days at specific times on
that route the second one is the
real-time location data of vehicles from
Google Map maps and with the help of
sensors some of the other popular map
services are Bing Maps maps.me and here
we go next up we have social media
personalization so say I want to buy a
drone and I'm on Amazon and I want to
buy a DJI mavic Pro the thing is it's
close to one lap so I don't want to buy
it right now but the next time I'm on
Facebook I'll see an advertisement for
the product next time I'm on YouTube
I'll see an advertisement even on
Instagram I'll see an advertisement so
here with the help of machine learning
Google has understood that I'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which have already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam F filters that Gmail uses
is content filters header filters
General Blacklist filters and so on next
we have online fraud detection now there
are several ways that online fraud can
take place for example there's identity
theft where they steal your identity
fake accounts where these accounts only
last for how long the transaction takes
place and stop existing after that and
man in the middle attacks where they
steal your money while the transaction
is taking place the feed forward neural
network helps determine whether a
transaction is genuine or fraudulent so
what happens with feed forward in un
Networks that the outputs are converted
into hash values and these values become
the inputs for the next round so for
every real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice gemic stroke lesions they
can also be used in fetal emerging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment trug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other so first we will
import some major libraries of python so
so here I will write
import pandas as
pdimport numai as
NP then
import
cbor as
SNS okay then
import skar
dot model
selection
Port train
underscore testore
split before that I will
import M plot
Li do p plot
as
PLT okay
then I will write here from
eSalon dot
matrix
import
accuracy
for then
from
skon do m
MX
import
classification
report and import R then import
string
okay then press enter so it is
saying okay here I have to write from
everything seems
good loading let's
see okay till then numai is a python
Library used for working with arrays it
also has function for working with
domain of linear algebra and
matrices it is an open source project
and you can use it
freely number stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and time
series then cbon an open source python
Library based on M plot lib is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbone functions
with ease
then matplot lip for Python and its
numerical extension numpy met plot lip
is a crossplatform for the data
visualization and graphical charting
package as a result it presents a strong
open source suitable for
metlab the apis for met plot Li allow
programmers to incorporate graphs into
gii applications then this train test is
split we may build our training data and
the test data with the aid of escalan
train test split function this is so
because the original data set often
serves as both the training data and the
test data starting with the single data
set we divide it into two data sets to
obtain the information needed to create
a model like hor and test accuracy score
the accuracy score is used to gge the
model's Effectiveness by calculating the
ratio of total true positive to Total
true negative across all the model
prediction this re regular expression
the function in the the model allow you
to determine whether a given text fits a
given regular expression or not which is
known as
re okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating data the Str
Str class is a built-in string class in
Python because python strings are
immutable they cannot be modified after
they have been formed okay so now let's
import the data set we will be going to
import two data set one for the fake
news and one for the True News or you
can say not fake news okay so I will
write here PF
underscore P equals
to PD
do read underscore
CSV or what can I say DF
okay _
fake
okay then fake do CSV you can download
this data set from the description box
below then data dot true equals to pd.
read
CSV
sorry
CSC then fake news sorry true true.
CSV okay then press
enter so these are the two data set you
can download these data set from the
description box below so let's see the
board data set okay then I will write
here
dataor
fake do
head so this is the fake data okay
then data underscore
true
Dot and this is the two
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here data let's go
fake
Plus equal to
0 then data underscore
true
plus = to
1
okay
then I will write here data underscore
fake dot shape and data underscore true
dot
shape okay then press
enter so the shape method return the
shape of an array the shape is a tle of
integers these number represent the
length of the corresponding array
dimension in other words a tle
containing the quantities of entries on
each axis is an array shape Dimension so
what's the meaning of
shape in the fake word in this data set
we have 2 3 4 8 1 rows and five col
columns and in this data set true we
have 2 1 41 7 rows and five column okay
so these are the rows column rows column
for the particular data
set so now let's
move and let's remove the last 10 rows
for the manual testing okay then I will
write here data underscore
fake let go
manual testing
to dataor
fake do
tail for the last 10 rows I have to
write here
10 okay so for
I in
range 2 3
4 8
1 sorry
zero comma 2 3
470 comma
minus1
okay
then DF underscore not DF
data underscore
fake dot drop
one instead of one I can write here
I
comma X is equals
to0 in
place equals to
true then data
not
here data
underscore same I will write for I will
copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true there I can write
true
okay then I have to change a
[Music]
number
2
1
416 21 40
6 -
one
same
so press
enter x equal
to0 since X maybe you mean d0 or of
this okay we will put here double
course I'm putting
this f. drop I is z in place
okay also write equals to equals
to yeah
so okay axis is not
defined
now it's working
so let me
see now
daore P do
shape
okay and data dot
true data underscore
true do
as you can
see 10 rows are deleted from each data
set yeah so I will write here data
underscore fake underscore
manual
testing class
= to
0o and data
uncore true
underscore manual underscore
testing equals
to
1
okay just ignore this
warning then let's
see data
underscore
bore
manual
testing.
head as you can see we have this and
then data dot sorry underscore
truecore manual
testing dot at
10
okay this is this is the uh true data
set so here I will merge data let us go
merge
to PD Dot
concat concat is used for the
concatination
dataor
fake data
underscore comma
XIs = to
Z then data underscore
merge do head
the top 10
rows
yeah as you can see the data is merged
here okay first it will come for the
fake news and then with the for the True
News then let's merge true and fake data
frames
okay we did this
and let's merge the column then data do
merge dot columns or let's see the
columns it has not defined but data
underscore
March these are the columns name title
Tex subject date class
okay
now let's remove those columns which are
not required for the further process so
here I will write data
underscore or equals to data underscore
merge
prop title we don't
need
then subject we don't need
then so
one so let's check some null
values it's giving
error of
this
that's good then
data dot
isnull
sum
Center so no null values okay then let's
do the random shuffling of the data
frames okay for that that we have to
write here data equals to data do
sample
one
then
data okay
data do
head okay now you can see here the
random shuffling is
done and one for the
true data set and zero for the fake news
one
okay then let me write here data
dot
reset underscore
index
Place equals to
True data dot
drop
comma X is = to
1 then comma in
place equals to
True
okay then let me see columns now data do
columns so here we have two columns only
rest we have deleted
okay let me see data dot
add yeah everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write
[Music]
here
but okay you can use any
name
text and text equals
to text.
lower okay and
text to r dot for the
substring
remove these
things uh from
the datas okay so for that I'm writing
here
comma
okay then text equals to R do
substring
comma comma
text okay then and I have to write text
equals
to do
substring
www
dot
s+
comma comma
text okay then text equals
to I do
substring
then
comma okay then text equals to R do
substring
then percentage
as again percentage for r. SK
function right here
string do
punctuation
comma then comma then
text
right then text equals to r dot
substring and
N
comma text equals to r dot
subring WR
here then again
D then
again
okay then
comma then again
texture okay then at the end I have to
write here return text so everything
like uh this this type of special
character will be removed from the data
set okay let's run this let's
see yeah so here I will addite DF sorry
not DF
data
data
then
text was to
data
okay dot
apply to the function name wordp word
opt
okay press enter yeah so now let's uh
Define the dependent and independent
variables okay x equals to
data
text and Y =
to
data class
okay then splitting training and testing
data okay sorry so here I will write
xcore
train comma xcore
test uh then Yore
train comma Yore test equs to train
underscore testore
plit then X comma
y comma
test let's go size equals to
0.25 okay let
Center so now let's convert X to vectors
for that I have to write
here that is
X
so here I will write from
Escalon dot
feature
extraction dot text
import T
vectorizer
okay then
vectorization
alss to T
FID
vectorizer okay
then 3
underscore
train equals
to
factorization
R1
factorization do
fit then
transform xcore
train okay then X Vore test equals
to
factorization
condition Dot
transform xcore
test okay then press
enter
uh so now let's see our first model
logistic
regression so here I will write
from eSalon
dot linear underscore
model OKAY import
logistic then allot equ
to
logistic
regression have to write here l
dot
fit
then
XV
dot not DOT X do train
comma X Vore
test okay let's
Center xv.
TR okay here I have to write y
train
okay and press
enter will work so here I will write
prediction under score linear
regression equal
to l r.
predict Vore
test okay let's see the accuracy
score for that I have to write LR Dot
score then XV uncore
test comma
Yore
test okay let's see the accuracy so here
as you can see accuracy is quite good
98% now let's
print the
classification
put Yore test
comma prediction of linear regression
okay so this is you can see Precision
score then F1 score then support value
accuracy okay so now we will uh do this
same for the decision free gradient
boosting classifier random Forest
classifier okay okay then we will do
model testing then we will predict the
school okay so now for the decision tree
classification so for that I have to
import from
skon dot
tree
import
decision
free
classifier okay
then at the short form I will write here
copy it from
here
then okay and I have to write the same
as this so I will copy it from
here
and
yeah
let's change linear
regression to season 3
classific
okay then I will write here
same go
DT to DT do
predict
X
Vore
test still loading it's it will take
time
okay till then let me write here for the
accuracy D do
score
vcore test
comma
y
okay let's wait
okay
run the accuracy so as you can see
accuracy is good than this linear
regression okay logistic
regression yeah so let
me show you the
let me
predict
print so this is the accuracy score this
is the all the
report
yeah now let's move for the uh gradient
boosting
classifier okay for that I have write
from
Escalon dot
emble
Port
radiant
boosting
classifier
classifier I will write here GB
equals to let me copy it from
here
okay I will give here random let's go
State equals to
zero wait wait wait wait so I will write
here GB dot
fit Vore train comma Yore train okay
then press
enter here I will write predict
underscore
GBS to GB dot pit sorry
predi
three DOT
test do go
test till then it's loading so I will
write here uh for the score then I will
add GB do
score then Vore test
comma Yore test okay so let's wait it is
running this
part till then let me write for the
printing
this okay it's taking
time taking time still taking
time what if I will run
this it's not coming because of
this yeah it's done now so you can see
the
accuracies uh not good
than decision Tre but yeah it is also
good
99.4 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from Escalon
dot
symbol
import
random
Forest
classifier
okay and here I will write
RF
to I will copy it from
here
then random
State equals
to Z
then RF dot
fit Vore
train comma y score
train okay then press
enter and
predict
underscore
RC R
fals
to RF do
predict 3core test
okay till then I will write here it's
still loading
it will take time so till then I will
write for the score score accuracy
score X Vore test comma Yore
test okay then I will write here till
then
print
classification
port and why _
test
comma will take time little
bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you but after WR writing the code
so so I have made two functions one for
the output label and one for the manual
testing okay so it will predict the all
the from the all models from
the repeat so it will
predict the it the news is fake or not
from all the models okay so for that let
me write here need
news to
string
put
okay then I will write here manual
underscore
testing
so here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the internet okay from
the Google the news which is not fake
okay I'm adding which is not fake
because I already know I searched on
Google so I'm entering this so just run
it let's see what is
showing okay string inut
object is not callable okay let me check
this
first yeah I have to give here s strr
only yeah let's check okay I have to add
here again the
script yeah manual testing is not
defined let me see manual
testing
okay I have to edit
something it is just GB and it is just
RF GPC is not defined okay okay so what
I have to do I have to remove
this this
okay everything seems
sorted
now as I said to you I just copied this
news from the internet I already know
the news is not fake so it is showing
not a fake news okay so now what I will
do I I will
copy one fake news from the
internet and let's see it is detecting
it or not
okay so let me run
this and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand till
here so I hope you guys must have
understand how to detect a fake news
using machine learning you can you can
copy any news from the internet and you
can check it is fake or not okay or if
your model is predicting right or not if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration ation
with leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
perue University in collaboration with
IBM should be your right choice for more
details use the link in the description
box below with that in mind open CV open
source computer vision library is an
open source computer vision and machine
learning software Library it is written
in C++ but has a binding for various
programming languages such as python
Java MLB open CV was designed with a
goal of providing a common
infrastructure for computer vision
applications and to accelerate the use
of machine learning perception in
commercial product open CV is widely
used in a variety of Industries
including robotics automative and
Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide support to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide support to its users now let's
see what is object detection object
detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name so here I will write object
detection
demo okay
so so first we will import some major
Library like open CV so for that we will
write import CV2 and the next one is
import m plot
lib P
plot as PLT so why we are writing PLT
because we can't write again and again
matplot li. pyplot okay it's a long one
so we can write a short form
PLT so yeah so let's run this so what is
open CV opencv is an open source
software library for computer vision and
machine learning the opencv full form is
open source computer vision Library it
was created to provide a shared
infrastructure for application for
computer vision and to speed up the use
of machine learning perception in
consumer products open CV as a BSD
licens software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and open CV is one of them
second one is mat BL lib Matt blood Li
is a easy to use and an amazing
visualized library in Python it is built
on numpy array and designed to work with
with broader CPI stack and consist of
several plots like line bar scatter
histogram and many others okay so moving
forward we will import our file okay so
here I will write
config file equals to this is our file
name
SSD
uncore Mobil
net
V3
large
Coco
202 14.
DB okay so you can find this file on the
description box
below Frozen model equals
to I explain you every single thing
inference
graph. PB okay so let me run it first
mobile net as a name applied the mobile
net model is designed to use in mobile
application and it's tflow first mobile
computer vision model mobile net use
depthwise separable convolutions it
significantly deduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the NS this result in the light
weight of the deep neural network so
mobilet is a class of CNN that was open
source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the
state-of-the-art of neural network its
versatility and the multi-purpose scene
variations of best to train a computer
vision model and Benchmark its
performance okay so what is coko the
common object in context is one of the
most popular large scale label images
data set available for public use it
represent a handful of object we
encounter on a daily basis and contains
image Inn notations in 80 categories I
will show you the categories I have with
over 1.5 million object instances okay
so modern day AI driven solution are
still not capable of producing Absolut
accuracy and result which comes down to
the fact that Coco data set is a major
Benchmark for CV to train test and
polish refine models for faster scaling
of The annotation Pipeline on the top of
that the Coco data set is a supplement
to transfer learning where the data use
for one model serves as a starting point
for the another so what is frozen
Insurance graph like freezing is the
process to identify and save all the
required graphs like weights and many
others in a single file that you can
usually use typical trans ofro model
contains four files and this contains a
complete graph okay so forward let's
create one model here I will write model
to CV2 do
DNN
model
Chen and then config
file here I'm giving the parameters two
parameters like frozen model and config
file score here yeah run it first okay
there is error
return okay so error is CV2 DNN
detection model return Is
Res then exception set the question
comes what is the meaning of deduction
model or DNN deduction model this class
represent the high level API for object
detection networks detection models
allows to set parameters for
pre-processing input image detection
model creates net from file and with
train weights and config sets it
processing input runs forward pass and
return the result
deductions okay moving forward let's set
the class
labels okay
class
labels F
name to labels do
dxt I will put this file on the
description box below you can download
from there
open file
name as
labels
this
stct so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into these class labels okay so
here if I will
print class
labels so these are the 80 categories in
the Coco data
set okay this person bicycle car
motorbike airplane bus train these all
are the
categories I will put this file label.
txt in the description box below you can
download from there okay
fine so let's print the length of the
Coco data set or you can see class
labels this 80 as you can see I have
already told
you the length will be 80
so here let's set this some model input
size scaling mean and all so I will
write here model dot set
inut
size T20 comma
320el do
set
input
scale 1.0
slash
127.5 okay I will explain you don't
worry model do
set
set
input
[Music]
mean
127.5 comma
127.5 comma
127.5 okay and then model do
set
inut will
be what is set input
size
okay so set input size is a size of new
frame a shape of the new blob l than
zero okay so this is the size of the new
frame the second one is set input scale
so set input scale is a scale factor of
the value for the frame or you can say
the parameter will be the multiplier of
the frame values or you can say
multiplyer for the frame values okay so
input mean so it set the mean value for
the frame the frame in which the photo
will come the video will come or my
webcam will come so it set the mean
value for the frame or the for
parameters mean is scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swap RB flag which
indicates the swap first and the last
channels so moving forward we will Port
one
image
I am
read do
jpg
do IM am
show so this is the size of 320x 320
okay so first thing is you can download
this the random picture from the Google
I took from Google itself so now what we
will do we will set the class
index the confidence
value
value the B box B box is the boundary
box which I will create for the
particular person cycle motorbike and
the car
okay to
model
the confidence
threshold threshold is used
for if my model will confirm it the
particular image which is the texting is
correct it will print the name
okay so let me
print PR
class class index is coming 1 2 3
4 okay so one means
person two means Bicycle three means car
and four means motorbike this is the
class index index for particular label I
will do I will print the
boxes
font
scale = to three
and the font equals to CB2
dot font
her
plane
for
class index and the confidence and the
boxes
and Dot
PN
that box the boundary
box
okay then I will write here CB2 do
rectangle make the
rectangle set the
image and box
is 5 comma 0 comma 0 this is the color
of the box and this will be the
thickness okay then I will write CV2 dop
put
text image then
class
labels I will write class index minus
one because always index start with zero
that's why and the box
is
z comma
boxes one
okay
F
comma
scale
font
scale color
to this will be the text color 0 comma
255 comma
0 and the
thickness three let me run
it h no error okay now PLT dot I am
show then CV2 do
CVT
color
then CV
T2 dot
color color then
BG to
brg that is why we wrote swap RB equals
to true because every time we will
convert BGR to brg
sorry GB RGB so we don't have to write
again and again it will convert all the
files into RGB okay run
it okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will car is coming okay
so it's detecting the
right for the image now we will do this
for the video and for the
webcam we are done with this image one
and then now I will write
here okay so this is we do for the
video for the video I will write here
cap equals to capture you can write any
name so CV2
dot
video
capture so you can take any random video
I took this
pixels
George can comment
down share the
link
have dot is
open so here I will write
cap equals
to
CV2 sorry
CV2 dot
video
capture
zero and if if
not cap do is
open
then then
ra input
output
error
can't open the video
can't open the
video here everything will be the same
font
scale equal to
three okay font equals to CV2
dot
font
her
okay so here I will write while
true comma
frame equals to cap dot sheet this is
for the reading of the
file the same I will write class
index comma
confidence
my boundary
boxum model do
detect name and the
confidence
threshold to
0.55 okay everything is the same we did
before so here I will
print
class
index okay so here I will write
if
and of the class
index does not equals to
zero then what to perform is here I have
to write
for class
index comma confidence
comma
boxes
inip
CLX do
flatten flatten is a layers
okay
confides
flaten e box
and
if
class index is greater than equals to
80
then what to do then I will copy from
here okay the same thing I have to write
here
so here I will write CV2 dot I'm
show this will be the return in the
frame object
detection by simply
learn
and
frame so if CV2 dot vit
key
2
and
zero
FFX =
to o
d q okay then
here I will write
break will be break when it get into
into two the weight key will be two okay
I will tell you what is the weight
key so here I will write cap dot
release and CV2
do
destroy all
Windows
okay so now let me
run let's see error there will be error
okay see Python Programming
modules let me run it
again wa
Keys
okay video is
here the video is here as you can see
see bicycle the person the person the
bus car traffic light the person person
so our object detection for the video is
coming right okay person okay person
traffic light
bus this is how you can do for the video
okay so now let's we will do for the uh
webcam
live so this is for the video
so if we want to do for the webcam we
okay so we need to just
change one one thing only we have to
change instead of giving the file
we have to write one here okay the rest
will be the
same got it so I have to just shut down
my webcam so let me shut down the webcam
and get back to
you as you can see
this is a 320x 320 box
so so this is coming right okay so I if
I will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
per University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind so first we
will open command prom to write a
command to open jupyter notebook so here
we will write
Jupiter
notebook then press enter so it will
take some time
yeah so this is the landing page of
jupyter notebook and here you have to
new then Python
3 so this is how the jupter notebook UI
look likees so at first we will import
some major libraries of python which
help in creating a mass detection system
so here I will write
import CV2 comma OS and press enter then
I will give the path then data path
equals to
C then
slash
users then SL
SLP 0
9375 then
slash
desktop then slash then
phase
mask
detection then slash data
set okay
so here it will be slash my bad sorry
and
slash okay it's look
fine so here I will write
categories equals to OS
do
list directory list
di then I will assign data
uncore path
then I will create some
labels equals
to uh here I will
write I will write here
I
for I in
range then here I will give
length of
categories
yeah then press enter Then here I will
write
label then take directory equals
to directory
then zip then
categories
comma
labels so here I will write
print then label
directory okay then I will
print
categories then I will print
labels
okay then press enter
yeah so here CV means capturing video
the CV2 function in open CV can read
video video capture by using pass zero
as a function parameter we can access
our webcam we may pass rtsp URL in the
function parameter to capture CCTV
footage which is quite helpful for video
analysis then OS
this this OS module the OS module in
Python has function for adding and
deleting folders retrieving their
contents changing the directory locating
the current directory and more before
you can communicate with the underlying
operation system you must import the OS
module and this OS list directory to
retrieve a list of all files and folders
in the given directory use Python os.
list directory method the list of files
or directory in the current working
directory will be returned if no
directory is
specified then label the the TK inter
visit class called a label is used to
show text or an image the user views the
label they cannot be interact with it so
moving forward I will write code so just
stay with me after that I will explain
you line wise
okay so here I will write
that image underscore
size equals to
100 then I will create two classes
data then
Target okay
arrays then I will write here for
category in
categories
folder
underscore
path equals to OS do
paath then dot
join then
data underscore
path comma
category okay then
IMG image names equals to os.
list directory then folder
path just stay with me I will explain
you line
wise okay so here I will write
for IMG
name in
IMG
names I Amore
path equals to os.
paath
dot
join then I will write right here
folder I
repeat I repeat so here we write folder
underscore
path then IMG underscore
name so IMG equals to
CV2 do IM am
read then IMG uncore
part so here I will write
try that gray equals to
CV2 dot
CVT then Capital C
color then IMG comma
CV2
Dot
then
color then underscore
BGR to
RGB
okay so I will go for the gray
one CVT color
BGR to RGB
yeah so I will write here resize
okay not in capital letter
resize equals to
CV2 dot
resize then
gray comma
IMG
size comma I am just
s why this two IMG size because like
length and and uh like width and breadth
would be
shame like 100 so press
enter so I will write data
dot
append then
resized then
Target do
append then
label then
directory of
category
okay
so I will write here
accept
exception s
e so here I will print
then
exception comma
e okay then pressenter let's see there
should be no
error so it is loading let's
wait okay no
error so image size should be like 100
by 100 so that is why I wrote here 100
the image size 100 by 100 and I made two
arrays like for one for data then one
for
targets
so this for this gray equal to CV2 do
CVT color image so converting the image
into gray scale okay then this line
resize equals to CV2 do resize gray IMG
so resizing the gray scale into like 100
by 100 since we need a fixed common size
for all the images in the data set
okay then this target do append label di
so appending the image like and the
label categories into the list list what
is list data set so like for the last
print exception e so if any exception
raised the exception will be printed
here and the pass to the next image
okay so moving forward let's import n
and save this data and the target okay
so I will write here
import
numpy as
NP
right then I will write here
data equals to NP do
array then again
data by 255
0 okay then again data equals to NP
dot
reshape then
data
comma
data dot
shape than
zero comma mg size
comma IMG
size comma
1 okay then press enter Then for
Target let me put do it just like this
then Target equals to NP do
array then
Target
okay okay
so here I will write
from kasas
dot
utils
import
NPS okay so here I will write new
underscore
Target equals to npor utils
dot
2 underscore
categories categorical I guess
categorical then
Target
okay then press
enter so here I will save np.
save this
data comma
data okay and np.
save this
target comma new
Target
okay so press
enter
yeah so numai numai is a python Library
used for working with ar it also has a
function for working with in the domain
of linear La algebra and matrices it is
an open source project and you can use
it freely numai stand for numerical
Python and this scas a python interface
for artificial neutral network is
provided by the open source software
package known as kasas the tensorflow
library interface is provided by the
kasas a number of backends were
supported by kasas up until version 2.3
including tens oflow Microsoft
cognitive so this is the part of the
data processing this all this part of
the data processing so let me write here
data pre-processing
preprocessing so the data pre-processing
part is done now we will create the
another file of this python for the
training CNN CNN means convocation
neural network okay test
data
comma
Trainor
Target comma
testore
Target equals to
train underscore
split then we will split on data and
Target comma test size should
be
0.1 okay so now I will give press enter
okay train split train split is not
defined
okay it is
train test
split
yeah so now I will write
checkpoint
equals to
model
checkpoint then
then
model I repeat so here I will write
model
comma
EPO than Z
3D do
model
okay so here I write comma
monitor equals to Value
loss
underscore
loss
comma
bubos equals to Z comma
save
best
only equals
to True comma
mode equals to mode should be
Auto
okay then I will press
enter then here I will write
history equals to model
dot
fit to
train underscore
data
comma okay train underscore
Target
comma apocs equals to 20
comma call
backs call equals
to
checkpoint comma
validation validation underscore
split equals to 0.2 the best ratio
okay so let me press enter okay there is
one error model. fit c n
x okay there should be a spelling
mistake
e
PS = to
20
do model it is
fine then
monitor go to Value
loss then veros =
to0 save best only equals to true then
mode
okay
so here I have to
write maybe it will so it will take some
time to go till 20 okay so it will
download one by one so we will wait for
a
while
so the model checkpoints are completed
so here I will write
print then
model dot
evaluate okay here I will write test
underscore
data comma
test underscore
Target
then press
enter okay so I hope you guys understand
till here if you have any question or
any query regarding any code just put as
in comments our team will shortly
provide you the correct solution okay so
moving forward we will do we will create
another file for detecting mask okay so
I will go here and then new
file python
so I will write here
detecting
okay yeah so I will import some
libraries here kasas do
models import
load
models we will
import
CV2 and we will import
v
numai as
NP so press enter okay num by as
NP okay so
presenter so what I will do I will write
here model equals to load underscore
model
then OKAY kasas model from kasas cannot
import
name okay
so it is model only yeah so here I will
add model equals to load
model so I will write here
model
07 do
model
okay okay
my so here I will write face underscore
classifier
equals to
CB2 dot
cascate cascate
classifier so this is one file for
frontal phas default so you can find
this file on the description box below
so I will write
here our casket
underscore
frontal
pH underscore
default do
XML okay so
Source equals
to
CV2 dot
video
capture zero it will open up our camera
so
labels underscore
directory equals
to
zero
mask and one
for no
mask
okay so
color directory equals to
like no mask for red and mask for green
okay so WR
zero then 0 comma
2 55 comma
0 then again comma for one there should
be 0 comma 0 comma
251
okay so I hope you guys understand till
here so if you have any question or any
query regarding any code so just put as
in comments our team will shortly
provide you the correct
solution
so the code is
written so let me run this for this
first I have to like so first I have to
close my this person on screen
camera so the code is running
fine now it is showing
noas
now it is showing
Mar so I hope you guys understand till
here if you have any queries any
question regarding any code just put as
a comment our team will shortly provide
with the correct solution if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply lar professional certification
program in Ai and machine machine
learning from Pur University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind we'll talk about interview
questions for machine learning now this
video will probably help you when you're
attending interviews for machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked uh questions and to help
you in answering these questions we
tried our best to give you the best
possible answers but of course what is
more important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of
getting those concept Concepts cleared
up as well but what is more important is
that you need to supplement these
responses with your own practical
experience okay so with that let's get
started so one of the first questions
that you may face is what are the
different types of machine learning now
what is the best way to respond to this
there are three types of machine
learning if you read any material you
will always be told there are three
types of machine learning but what is
important is you would probably be
better of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is the third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like what what
are the ways of Performing uh supervisor
learning and unsupervised learning or
what are some of the techniques so
supervised learning we we perform or we
do uh regression and classification and
unsupervised learning uh we do
clustering okay and clustering can be of
different types similarly regression can
be of different types but you don't have
to probably elaborate so much if they
are asking uh for uh just the different
types you can just mention these and
just at a very high level you can but if
they want you to elaborate give examples
then of course I think there is a
different question for that we will see
that later then the third so we have
supervised then we have unsupervised and
then reinforcement you need to provide a
little bit of information around that as
well because it is sometimes a little
difficult to come up with a good
definition for reinforcement learning so
you may have to little bit elaborate on
how reinforcement learning works right
so reinforcement learning works in in
such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and uh every time it is moving
in the direction of the target so the
agent basically has to take some action
and every time it takes an action which
is moving uh the agent towards the
Target right towards a goal a Target is
nothing but a goal okay then it is
rewarded and every time it is going in a
direction where it is away from the goal
then it is punished so that is the way
you can a little bit explain and uh this
is used primarily or very very impactful
for teaching the system to learn games
and so on examples of this are basically
used in alphago you can throw that as an
example where alphao used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good good
enough okay then there could be a
question on overfitting uh so the
question could be what is overfitting
and how can you avoid it so what is
overfitting so let's first try to
understand the concept because sometimes
overfitting may be a little difficult to
understand overfitting is a situation
where the model has kind of memorized
the data so this is an equivalent of
memorizing the data so we can draw an
analogy so that it becomes easy to
explain this now let's say you're
teaching a child about some recognizing
some fruits or something like that okay
and you're teaching this child about
recognizing let's say three fruits
apples oranges and pineapples okay so
this is a a small child and for the
first time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapple app okay and you take this
basket to this child and there may be
let's say hundreds of these fruits so
you take this basket to this child and
keep showing each of this fruit and then
first time obviously the child will not
know what it is so you show an apple and
you say hey this is Apple then you show
maybe an orange and say this is orange
and so on and so and then again you keep
repeating that right so till that basket
is over this is basically how training
work in machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then the process
what has happened the child has pretty
much memorized these so even before you
finish that basket right by the time you
are halfway through the child has
learned about recognizing the Apple
orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100% accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple right so that means
it has kind of memorized this data now
let's say you bring another basket of
fruits and it will have a mix of maybe
apples which were already there in the
previous set but it will also have in
addition to Apple it will probably have
a banana or maybe another fruit like a
jack fruit right so this is an
equivalent of your test data set which
the child has not seen before some parts
of it it probably has seen like the
apples it has seen but this banana and
jack fruit it has not seen so then what
will happen in the first round which is
an equivalent of your training data set
towards the end it has 100% it was
telling you what the fruits are right
Apple was accurately recognized orange
were was accurately recognized and
pineapples were accurately recognized
right so that is like 100% accuracy but
now when you get another a fresh set
which were not a part of the original
one what will happen all the apples
maybe it will be able to recognize
correctly but all the others like the
jack fruit or the banana will not be
recognized by the child right so this is
an analogy this is an equivalent of
overfitting so what has happened during
the training process it is able to
recognize or reach 100% accuracy maybe
very high accuracy okay and we call that
as very low loss right so that is the
technical term so the loss is pretty
much zero and accuracy is pretty much
100% whereas when you use testing there
will be a huge error which means the
loss will be pretty high and therefore
the accuracy will be also low okay this
is known as overfitting this is
basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple ways of of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like Dropout in case of neural
networks and a few other examples but I
think if you uh give example or if you
give regularization as the technique
probably that should be sufficient so so
there will be some questions where the
interview Vier will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have th records with labeling
information now one way of doing it is
you use all the Thousand records for
training and then maybe right which
means that you have exposed all this
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay okay so
obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may pass this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times till the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that here that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation that like a realistic
situation because when the model is
deployed that is what will happen it
will receive some new data not the data
that it has already seen right so this
is a realistic test so you put some new
data so this data which you have set
aside is for the model it is new and if
it is able to accurately predict the
values that means your training has
worked okay the model got drain properly
but let's say while you're testing this
with this test data you're getting lot
of errors that means you need to
probably either change your model or
retrain with more data and things like
that now coming back to the question of
how do you split this what should be the
ratio there is no fixed uh number again
this is like individual preferences some
people split it into 50/50 50% test and
50% training Some people prefer to have
a larger amount for training and a
smaller amount for test so they can go
by either 60/40 or 7030 or some people
even go with some odd numbers like
6535 or uh
63.33% there is no fixed rule that it
has to be something the ratio has to be
this you can go by your individual
preferences all right then you may have
questions around uh data handling data
manipulation or what do you call data
management or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were the this
many rows where the data was missing
that would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or correct
data now let's take a few examples now
let's say you have data where some
values in some of the colums are missing
and you have pretty much half of your
data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like 10% okay now between
these two cases how do we so let's
assume that this is not a mission
critical situation and in order to fix
this 10% of the data the effort that is
required is much higher and obviously
effort means also time and money right
so it is not so Mission critical and it
is okay to let's say get rid of these
records so obviously one of the easiest
ways of handling the data part or
missing data is remove those records or
remove those observations from your
analysis so that is the easiest way to
do but then the downside is as I said in
as in the first case if let's say 50% of
your data is like that because some
column the other is missing so it is not
like every in every place in every Row
the same column is missing but you have
in maybe 10% of the records column one
is missing and another 10% column two is
missing another 10% column 3 is missing
and so on and so forth so it adds up to
maybe half of your data set so you
cannot completely remove half of your
data set then the whole purpose is lost
okay so then how do you handle then you
need to come up with ways of filling up
this data with some meaningful value
right that is one way of handling so
when we say meaningful value what is
that meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely we off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
a TOS right there may be situations
where if the missing values instead of
putting mean maybe a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this question okay so then the
next question can be how can you choose
a classifier based on a training set
data size so again this is one of those
questions uh where you probably do not
have like a one size fitall answer first
of all you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that yes this is the
classifier that will work here no that's
not the right way so you need to still
uh you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion Matrix so the question
can be explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by taking an example Le
and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and uh
it is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many no so you have that
information and you have the predicted
values how many yes and how many no
right so the total actual values the
total yes is 12 + 1 13 and they are
shown here and the actual value NOS are
9 + 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and uh out
of this what is the meaning convey so
there are two or three things that needs
to be explained outright the first thing
is for a model to be accurate the values
across the diagonal should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 + 3 15 + 10 25 so
that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let me uh take
a pen so your your diagonal values is
equal to if I say d is equal to 12 + 9
so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find diagonal values that is
equal to your percentage okay all right
so the next question can be a related
question about false positive and false
negative so what is false positive and
what is false negative now once again
the best way to explain this is using a
piece of paper and Pen otherwise it will
be pretty difficult to to explain this
so we use the same example of the
confusion Matrix and uh we can explain
that so A confusion Matrix looks
somewhat like this and um when we just
take yeah it looks somewhat like this
and we continue with the previous
example where this is the actual value
this is the predicted value and uh in
the actual value we have 12 + 1 13 yeses
and 3 + 9 12 Nos and the predicted
values there are 12 + 3 15 yeses and uh
1 + 9 10 NOS okay now this particular
case which is the false positive what is
a false positive first of all the second
word which is positive okay is referring
to the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right nine is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
du diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set to check accuracy
whether it is working fine or not so you
test the model before you actually put
it into production right so once you
test the model you're satisfied it's
working fine then you go to the next
level which is putting it for production
and then in production obviously new
data will come and uh the inference
happens so the model is readily
available and only thing that happens is
new data comes and the model predicts
the values whether it is regression or
classification now so this can be an
iterative process so it is not a
straightforward process where you do the
training do the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will detate so that
is again a recursive process so once in
a while you need to keep checking
whether the model is working fine or not
and if required you need to tweak it and
modify it and so on and so forth so net
net this is a continuous process of um
tweaking the model and testing it and
making making sure it is up to date then
you might have question around deep
learning so because deep learning is now
associated with AI artificial
intelligence and so on so can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you are defining deep learning
so first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
uh scope and deep learning is one one
part of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning uh the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our aing label data and
uh this data has several let's say if it
is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have sensus data and we
have all these so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to to our model right so we
as a data scientist we have to decide
whether we should take all of them all
the features or we should throw away
some of them because again if we take
all of them number one of course your
accuracy will probably get affected but
also there is a computational part so if
you have so many features and then you
have so much data it becomes very tricky
so in case of machine learning we
manually take care of identifying the
features that do not contribute to the
learning process and thereby we
eliminate those features and so on right
so this is known as feature engineering
and in machine learning we do that
manually whereas in deep learning where
we use neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on Fast data so here we are
talking primarily of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance manually coded so
basically the feature engineering part
is done manually and uh the problem is
divided into parts and solved
individually and then combine so that is
about the machine learning part in deep
learning deep learning basically enables
machines to take decisions with the help
of artificial neural network so here in
deep learning we use neural length so
that is the key differentiator between
machine learning and deep learning and
usually deep learning involves a large
amount of data and therefore the
training also requires usually the
training process requires high-end
machines uh because it needs a lot of
computing power and the Machine learning
features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of De planning therefore it is said that
the problem is handled end to end so
this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the the
meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
well for supervised machine learning the
very common one is email spam detection
so you want to train your application or
your system to detect between spam and
non-spam so this is a very common
business application of supervised
machine learning so how does this work
the way it works is that you obviously
have historical data of your emails and
they are categorized as spam and not
spam so that is what is the labeled
information and then you feed this
information or the all these emails as
an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning
here the way it works is that existing
images it could be x-ray images it be
MRI or any of these images are available
and they are tacked saying that okay
this x-ray image is defective or the
person has an illness or it could be
cancer which your illness right so it is
stacked as defective or clear or good
image and defective image something like
that so we come up with the binary or it
could be multiclass as well saying that
this is defective to 10% this is 25%
person and so on but let's keep it
simple you can give an example of just a
binary classification that would be good
enough so you can say that in healthcare
Diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised and so there can be a
question around semisupervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggests is Falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images
we can just let's say take X-ray images
now there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
effective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semisupervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no a quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types clust
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying laptops or also buying the mouse
so that means there is an association
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and
unsupervised and in case of supervised
and again here probably the keyword that
the person may be wanting to hear is
labeled data now very often people say
yeah we have historical data and if we
run it it is supervised and if we don't
have historical data yes but you may
have historical data but if it is not
labeled then you cannot use it for
supervised learning so it is it's very
key to understand that we put in that
keyword labeled okay so when we have
labeled data for training our model then
we can use supervised learning and if we
do not have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so the there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as a
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person a video that
fire can burn the F burn his finger or
fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is K&N and another is kin K
stands for K nearest neighbors and K
means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them KNN is completely
different K means clustering is
completely different KNN is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a
unsupervised okay when you have K andn
when you want to Implement K andn which
is basically K nearest neighbors the
value of K is a number so you can say k
is equal to 3 you want to implement KN
andn with K is equal to 3 so which means
that it performs the classification in
such a way that how does it perform the
classification so it will take three
nearest objects and that's why it's
called nearest neighbor so basically
based on the distance it will try to
find out its nearest objects that are
let's say three of the nearest objects
and then it will check whether the CL
class they belong to which class right
so if all three belong to one particular
class obviously this new object is also
classified as that particular class but
it is possible that they may be from two
or three different classes okay so let's
say they are from two classes and then
if they are from two classes now usually
you take a odd number you assign a odd
number to so if there are three of them
and two of them belong to one class and
then one belongs to another class so
this new object is assigned to the class
to which which the two of them belong
now the value of K is sometimes tricky
whether should you use three should you
use five should you use seven that can
be tricky because the ultimate
classification can also vary so it's
possible that if you're taking K as
three the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to class one and one belong
to class two whereas when you take five
of them it is possible that only two of
them belong to class one and three of
them belong to class two so which means
that this object will belong to class
two right so you see that so this the
class allocation can vary depending on
the value of K now K means on the other
hand is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to to some of their
features okay and but the similarity of
course is the the letter K and in case
of K means also we specify its value and
it could be three or five or seven there
is no technical limit as such but it can
be any number of clusters that uh you
can create okay so based on the value
that you provide the system will create
that many clusters of similar objects so
there is a similarity to that extent
that K is a number in both the cases but
actually these two are completely
different processes we have what is
known as KN based classifier and people
often get confused thinking that naive
base is the name of the person who found
this uh classifier or who developed this
classifier which is not 100% true base
is the name of the person b a y s is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classif ifier na based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of one feature
of a class is not related to the
presence of any other feature of maybe
other classes right so which is not very
strong or not a very what do you say
accurate assumption because these
features can be related and so on but
even if we go with this assumption this
whole algorithm works very well even
with this assumption and uh that is the
good side of it but the term comes from
there so that is the explanation that
you can give then there can be question
around reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score high value High number
or it could be that your uh number of
lives should be as high as possible
don't lose lives so these could be some
of them more advanced examples could be
for driving automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the Ts for example example it is
rewarded and every time it takes a step
which goes against that goal right
contrary or in the reverse Direction it
is penalized okay so it is like a carrot
and stick system now how do you use this
to create a game of chess so to create a
system to play a game of chess now the
way this works is and this could
probably go back to this alphao example
where alphao defeated a human Champion
so the way it works is in reinforcement
learning the system is allowed for
example if in this case we're talking
about Chess so we allow the system to
first of all watch playing a game of
chess so it could be with a human being
or it could be the system itself there
are computer games of Chess right so
either this new learning system has to
watch that game or watch a human being
play the game because this is
reinforcement uh learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent works on that
so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scrp
okay and as it moves forward it it it's
right at the beginning the system really
knows nothing about the game of chess
okay so initially it is a clean slate it
just starts by observing how you are
playing so it will make some random
moves and keep losing badly but then
what happens is over a period of time so
you need to now allow the system or you
need to play with the system not just 1
2 3 4 or five times but hundreds of
times thousands of times maybe even
hundreds of thousands of times and
that's exactly how alpha go has done it
played millions of games between itself
and the system right so for the game of
chess also you need to do something like
that you need to allow the system to
play chess and uh then learn on its own
over a period of repetition so I think
you can probably explain it to this much
to this extent and it should be uh
sufficient now this is another question
which is again somewhat similar but here
the size is not coming into picture so
the question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have spvm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously ly something like uh you know
Amazon a website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age may be your
gender and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user and with that we have
come to the end of this machine learning
complete full course I hope you found it
valuable and entertaining please ask any
questions about the topics covered in
this video in the comment box below and
a team of expert will assist you in
addressing your problems as soon as
possible so thank you so much for being
here today we'll see you next time time
until then keep learning and stay tuned
to Simply learn staying ahead in your
career requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here