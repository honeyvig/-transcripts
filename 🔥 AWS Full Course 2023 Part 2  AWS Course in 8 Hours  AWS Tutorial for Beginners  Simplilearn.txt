welcome to AWS full course part 2 by
simply learn in previous video we
covered various concepts of AWS like AWS
Foundation Services AWS architecture AWS
Lambda in this video we will be learning
about AWS elastic Beanstalk AWS redshift
AWS versus Azure AWS elb AWS EBS and
more so before we begin consider
subscribing to our Channel and hit the
Bell icon to never miss any updates from
Simply learn if you are looking to
become a cloud expert then you are at
the right place the postgraduate program
in cloud computing designed in
collaboration with Caltech ctme helps
you to become an expert in Azure AWS and
gcp this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
so hurry up and find the course Link in
the description box for more details our
Learners have experienced youth success
in their career listen to their
experience find the simple and course
Link in the description box so without
any further Ado let's begin so today's
session is on AWS elastic Beanstalk so
what's in it for you today we'll be
discussing about what is AWS why we
require AWS elastic lean stock what is
AWS elastic mean stock the advantages
disadvantages the components of mean
stock along with that the architecture
and the companies that are primarily
using the AWS bean stock so let's get
started and first understand what is AWS
AWS stands for Amazon web services it's
a cloud provider and that offers a
variety of services such as compute
power database storage content delivery
and many other resources so we know that
AWS is the largest cloud provider in the
market and so many services are
available in the AWS where you can apply
the business Logics and create the
solutions using the cloud platforms now
why AWS elastic mean stock now what
happened earlier and that whenever the
developer used to create the software or
the modules related to the softwares it
has to be joined together to create a
big application now one developer
creates a module that has to be shared
with another developer and if the
developers are geographically separated
then it has to be shared over a medium
probably an internet so that is going to
take some time it would be a difficult
process and in return it makes the
application or a software development a
NVA process the building of the software
development linear process so there were
challenges which the developers were
facing earlier and to overcome that we
have the mean stock as a service
available in the AWS so why AWS elastic
bean stock is required AWS elastic beam
stock has made the life of the
developers quite easy in terms of that
they can share the applications across
different devices at a shorter time
duration now let's understand what is
AWS elastic bean stock AWS elastic bean
stock is a service which is used to
deploy and scale web applications by
developers not only web application any
application that is being developed by
the developers this is a simple
representation of the AWS plastic mean
stock NOW along with that the AWS
elastic beam stock supports lower
programming language the runtime
environments that are java.net PHP
node.js python Ruby go and Docker and in
case if you're looking for any other
programming language or a runtime
environment then you can make a request
with AWS to arrange that for you now
what are the advantages associated with
the elastic bean stock first advantages
that it's a highly scalable service now
when we talk about a scalability it
means that whenever we require the
resources in demand we can scale up the
resources or we can scale down the
resources so that is kind of a
flexibility we get in terms of changing
the type of resources whenever we need
it and in that case the elastic
Beanstalk is a highly scalable service
now that is something which is very
difficult to achieve in case of an
on-prem environments because you have to
plan for the infrastructure and in case
if you're short of the resources within
that infrastructure then you have to
procure it again the second Advantage
associated with the Beanstalk is that
it's a fast and simple to begin now when
we say it's fast and simple that means
that you just have to focus on the
development of an application building
an application and then you can just
deploy the application directly using
the Beanstalk what the Beanstalk is
going to do that every networking aspect
is being taken care by the bean stock it
deploys your application in the back end
on the servers and then you can directly
access your application using the URL or
through the IP address the third
Advantage is that it offers the quick
deployment that is what we discussed in
the fast and simple to begin as well so
why it offers a quick deployment you
don't have to bother about the
networking Concepts you just have to
focus on the application development and
then you can just upload your
application deploy that and then you are
good to go the other Advantage is that
it supports multi-tenant architecture
when we talk about tenants or
multi-tenants that means we can have a
virtual environments for separate
organizations or the divisions within
the organizations that will be virtually
isolated so likewise you can have a
virtually isolated environments created
on the Beanstalk and they can be
separated used as a separate entities or
a separate divisions within the
organization and we know that it's a
flexible service since it's a scalable
then which is a flexible also now coming
to the simplifies operations as an
advantage now once the application is
deployed using the Beanstalk then it
becomes very easy to maintain and
support that application using the
Beanstalk Services itself and the last
advantage that we can have from the
Beanstalk is that it's a cost efficient
service the cost efficient as we know
that many of the AWS services are cost
effective the cost optimization can be
better managed using the AWS mean stock
as compared to if you are developing or
if you are deploying any kind of an
application or a solution on the on-prem
servers now there are some components
that are associated with the AWS bean
stock and it has to be created in the
form of a sequence manner so AWS elastic
means law consists of few important
components which are required while
developing an application now what are
these components these are four
components one is application the second
is application version the third is
environment and the fourth one is the
environment tier and we have to progress
while deploying our applications or the
softwares using the same sequence now
let's understand what are the different
components of the Beanstalk are the
application it refers to a unique label
which is used as a Deployable code for a
web application so generally you deploy
your web application or you create your
application and that is something which
is basically used as a unique label then
the second component is application
versions so it resembles a folder which
stores a collection of components such
as environments versions and environment
configurations so all these components
are being stored using the application
version the third most important
component is the environment in the
environment only the current versions of
the applications runs now remember that
elastic means stock supports multiple
versions as well and using the
environment you can only run the current
version of the application file if you
wanted to have another version of an
application to be running then you have
to create another environment for that
then comes the environment here and in
the environment here it is basically it
designates the type of application that
the environment runs on now generally
there are two types of environment here
one is the web and the other one is the
worker node and that's something which
we'll be discussing later as well now
let's understand how does elastic bean
stock in AWS works so first we have to
create an application Edition and this
is a task that would be done by the
developers and for that you can actually
select any runtime environment or a
programming language like Java Docker
Ruby gopal or python as well and once
you select that environment you can
develop your application using that
runtime environments now after that once
the application is created then you have
to upload the version of an application
on the AWS and after that once the
version is uploaded and then you have to
launch your environment so just have to
click on the buttons that's it nothing
more you have to do once the environment
is launched then you can actually view
that environment using a web URL or
using the IP address now what happens in
that case is when you launch an
environment in the back end the elastic
Beanstalk runs automatically runs an ec2
instance and using a metadata the mean
stock deploys our application within
that ec2 instance that is something
which you can look into the ec2
dashboard as well so you don't have to
take care of the security groups you
don't have to take care of the IP
addressing and even you don't have to
login into the instance and deploy your
application it would be done
automatically by the Beanstalk it's just
that you just have to monitor the
environment and the statistics will be
available there itself in the bin stock
dashboard otherwise you can view those
statistics in the cloudwatch logs as
well now in case if you wanted to update
any kind of a version then you just
upload a new version and then just
deploy that and then monitor your
environment so these are the essentials
to create a local applications for any
platform whether it's a node.js python
Etc these are the things that you have
to actually take care and this is the
sequence you have to follow while
creating an environment so you can say
that it's a four steps creation of or
deployment of your application that's it
now after users upload their versions
the configuration is automatically
deployed with a load balancer yes and
with the load balancer that means you
can access the applications using the
load balancer DNS so and apart from load
balancer if you wanted to put any other
feature that includes the auto scaling
for example if you wanted to create your
ec2 instances where the application will
be deployed within the virtual private
cloud or in a particular subnet within
the VPC all those features that are
available and you can select them using
the mean stock itself you don't have to
move out to the VPC you don't have to
actually go to the ec2 dashboard and
select all those separately everything
would be available within the Beanstalk
dashboard so that's what it says in the
presentation that after creating an
application the deploy service can be
specifically accessed using the URL so
once the environment is created there
will be a URL defined now you can put a
URL name also that is something which
you wanted to put for your application
you can Define that you can check for
the availability of that URL and then
you have to use that URL to access your
application or the browser now once it
is done then in the monitor environment
it says the environment is monitored
provided capacity provisioning load
balancing Auto scaling and hand monitor
features all those features are
available there itself in the mean stock
now let's understand the architecture of
AWS elastic bean stock now there are two
types of environments that you have to
select you can select one is the web
server environment and the other one is
the worker environment So based on the
client requirement Beanstalk gives you
two different types of environment that
you have to select generally the web
server environment is the front-end
facing that means the client should be
as accessing this environment directly
using a URL so mostly a web applications
are deployed using that environment the
worker environment is the backend
applications or on the micro apps which
are basically required to support the
running of the web applications now it
depends on the client requirement what
kind of an environment you wanted to
select now in the web server environment
it only handles the HTTP request from
the clients so that's why we use the web
server environment mostly for the web
applications or any application which
works on the HTTP https requests so it's
not only the HTTP you can use the https
as well the worker environment it
processes background task and minimizes
the consumption of resources so again it
is just like a kind of a micro service
or an application services that are
running in the back end to support the
web server environment now coming to the
understanding of the AWS mean stock so
this is how the architecture of the AWS
bean stock is designed and you can refer
to that image also now in the web server
environment let's say if we select a web
server environment and it says that if
the application receives client request
the Amazon Route 53 sends his request to
the elastic load balancer now obviously
we discussed here that the web server
environment is primarily an environment
which receives the HTTP requests it's a
kind of a client-facing environment now
if the application receives a client
request Amazon from the Amazon Route 53
this Route 53 is a service which is
primarily used for DNS mapping it's a
global Service and it may route you can
route the traffic from the Route 53
matching your domains towards the load
balancer and from the load balancer you
can point the traffic to the web server
environment obviously the web server
environment is nothing is just the ec2
instances that would be running in the
back end now here in the diagram you can
see that there are two web server
environments and they are created in the
auto scaling group that means there is
some kind of scaling options that are
defined as well and these instances are
created in an availability zone or they
can be created in a different
availability Zone also for the
redundancy as well and these web
application servers are further
connected to your databases which
primarily will be in a different
security groups probably it can be an
RDS database also so all these
functionalities all these features are
basically available on the elastic mean
stock dashboard itself now what happens
in that case is if the application
receives client requests Amazon Route 53
send these requests to the load balancer
later the load balancer shares those
requests among the ec2 instances how
does that happen it happens using a
predefined algorithm the equal
distribution of a load is distributed to
both the ec2 instances or n number of
ac2 instances running in the
availability zone now in the
availability zones every ec2 instance
would have its own Security Group they
can have a common Security Group also
they can have their own Security Group
as well now after the security group the
load balancer is then connected to the
Amazon ec2 instance which are part of
the auto scaling group so that's
something which we have discussed
already now this Auto scaling group is
would be defined from the Beanstalk
itself and there will be some scaling
options that will be created it could be
a possibility that it might be the
minimum number of instances that would
be running as of now and based on the
threshold defined it may increase the
number of ec2 instance and the load
balancer will keep on Distributing the
load to as many instances that will be
created inside the availability Source
obviously there will be an internal
health check that the load balancer will
be first doing before Distributing the
real-time traffic to this instances
created by the mean stock now what does
Auto scaling group does it automatically
starts the additional ec2 instance to
accommodate increasing load on your
application that's something which we
know that and also it monitors and scale
sales instances based on the workload as
well so depends on what kind of a
scaling threshold you have defined in
the auto scaling groups and when the
load of an application decreases the ec2
instance will also be decreased so
whenever we talk about the auto scaling
generally it comes in our mind is that
we scale up the resources that means we
it increases the ec2 instances in the
auto scaling you might have the scale
down option also scale down policy also
created in which if the load minimizes
it can terminate the additional ec2
instances as well so that is something
which will be automatically managed all
these features can be achievable using
the elastic mean stock and with this
feature accommodated it gives you the
better cost optimization in terms of
managing your resources now it says that
elastic bean stock has a default
Security Group and the security group
acts as a Firefall for the instances now
here in this diagram it says about the
Security Group auto scaling also you
might create it in a default VPC also
you might create it in your custom EPC
also where you can have the additional
level of security is also created you
can have the nacls knuckles also defined
here before the security groups so that
would give you the additional filtering
option or the firewall option now it
says that with these groups with these
security groups it allows establishing
security groups to the database server
as well so every database would also
have its own Security Group and the
connection can be created between the
web servers environment that is created
by the Beanstalk to the database
security groups as well now let's
discuss about the worker environment now
understanding the worker environment
what happens is that the client the web
server environment is the client facing
the client sends a request for an access
to the web server and in this diagram
the web server further sends it to the
sqs which is a simple queue service and
the queue service sends it to the worker
environment and then whatever the worker
environment is created for doing some
kind of a processing or some kind of an
application that is running in the back
end that environment initiates and then
send back the results to this sqs and my
servers so let's understand the
architecture of a AWS plastic bean stock
with the worker environment so when a
worker environment here is launched AWS
elastic bean stock install the server on
every ec2 instance so that is in the
case of a web server environment also
and later the server passes the request
to the simple queue service now this
service is an asynchronous service
instead of a simple queue service you
can have other services also it is not
necessary that you need to have the sqs
also this is an example that we are
discussing about and the sqs shares
those message via a post request to the
HTTP path over the worker environment
and there are many case studies also
with respect to this kind of an
environment that is being created that
is being done on many customers and you
can search for this kind of a case
studies available on the internet now
the worker environment executes the task
given by the sqs with the HTTP response
after the operation is completed now
here what happens is a quick recap the
client and request for an access of an
application to a web server using an
HTTP request the web server passes that
request to the queue service the queue
service shares the message with a worker
probably a worker might be the manual
worker and generally it's an automated
worker so it would be shared via the
worker environment only and the worker
sends back the response with the HTTP
response back to the queue that response
can be viewed directly from the queue
service by the client using the web
server so this is one of the example
likewise as I said that there can be
many other examples also where you can
have the worker environments defined now
what are the companies that are using
the elastic bean stock these are few of
the companies that are primarily using
on a Zillow jelly button games then you
have League of Women Voters eBay these
are some of the few listed companies and
obviously you search on the AWS site and
you find many more organizations that
are using the last mean stock primarily
for deploying their applications now the
next thing is to go with the practicals
that how actually we use the elastic
Beanstalk so let's look into the demo
using the AWS elastic Beanstalk now
first you have to login into the AWS
console and I'm sure that you might be
having the accounts created or you can
use the IM credentials as well and then
you have to select the region also now I
am in the North Virginia region likewise
you can select any of the regions that
are listed here now click on the
services and you have to search for the
elastic beam stock you can find the
elastic Beanstalk under the compute
section so here itself you'll find the
elastic beam stock as a service now open
this service and there it will give you
an option to create an environment you
have to specifically select an
environment properly a worker
environment or a web service environment
so let's wait for the service to open so
we have the dashboard now available with
us this is how the elastic bean stock
looks and this is the symbol
representation offer means now what you
have to do is we have to click on get
started and that will load and you have
to create a web app so instead of
creating a web app what we'll do we'll
create a new application so just click
on create a new application put an
application name let's say we put
something like x y z you can put any
description to your application let's
say it's a demo app and click on create
now it says you have to select an
environment now the environment the
application name XYZ is created you just
have to select an environment so click
on create one now and it is going to ask
you that what kind of an environment
here you wanted to select so as we
discussed that there are two types of
environments one is the web server and
the other one is the worker and one
let's look into it what is defined by
the AWS AWS says that it has two types
of environment tiers to support
different types of web applications web
servers are standard applications that
listen for and then process HTTP request
typically over port number 80 workers
are special visualized application that
have a background processing task that
listens for message on an Amazon sqs
queue workers application posts those
messages to your application by using
the HTTP response so that's what we saw
in the case of the mean stock slides
also now the usability of a worker
environment can be anything now we'll do
a demo for creating a web server
environment so just click on select and
you we have the environment name created
now we can Define our own domain it ends
with the region dot elasticbeanstalk.com
let's say I look for a domain which is
XYZ only that's the environment name now
I'll check for the availability whether
that domain name is available with us or
not and it says we don't have that
domain name so probably I'll try to make
it with some other name and let's look
for the availability XYZ ABC and it says
yes it is available now once I deploy my
application I would be able to access
the application using this complete DNS
so you can put a description it's a demo
app that we are creating and then you
have to define a platform as well now
these are the platforms that are
supported by the AWS let's say I wanted
to run a node.js environment so I'll
just click on the node.js platform the
application codes is something which is
basically developed by the developers
and you can upload the App application
right now or you can do that later as
well once the environment is ready now
either you can select to create an
environment if you wanted to go with all
the default settings otherwise if you
wanted to customize it more you can
click on configure more options so let's
click on configure more options and here
you would be able to define various
different features like the type of an
instance for example what kind of an ec2
instance or a server that should be
running so that the Beanstalk can deploy
your applications over it if you wanted
to modify just click on on a modify
button and here you can modify your
instances with respect to the storage as
well now apart from that if you wanted
to do some modification in the case of
monitoring in the case of databases in
the case of security or in the case of a
capacity let's look into the capacity so
here you can actually do the
modification so in the capacity you can
select the instance type also by default
it is t2.micro but in case if your
application requires a larger type of an
instance then you can actually go for
the instance type as well similarly you
can Define your Emi IDs also because
obviously for the application to run you
would require the operating system also
so you can select that particular Ami ID
for your operating system as well let's
cancel that likewise you have many other
features that you can actually Define
here from the dashboard and you don't
have to go to the ec2 dashboard to do
the modifications now let's go and
create an environment let's assume that
we are going with the default
configuration so this is going to create
our environment the environment is being
created and you can get the environment
and the logs defined in the dashboard
itself so you'll see that the Beanstalk
environment is being initiated the
environment is being started and in case
if there would be any errors or if it is
deployed correctly you will get all the
logs here itself now the environments
are basically color coded so there are
different color codings that are defined
if you get the environment in a green
color that means everything is good to
go so here you can see that it has
created an elastic IP it has checked the
health of the environment now it has
created the security groups and that
would be an auto security groups created
by the mean stock and the environment
creation has been started you can see
that elastic bean stock as Amazon S3
storage bucket for your environment data
as well this is the URL through which
you will be accessing the environment
but right now we cannot do that since
the environment is being created let's
click on the application name and here
you can see that it is in a gray color
that means right now the build is being
done it is being created once it will be
successfully created it should change to
the green color and then we will be able
to access our environment using the URL
now if I move to the ec2 instances and
see in the ec2 dashboard if I see
whether the instance is being created by
the Beanstalk or not so let's see and
let's see what are the differences in
terms of creating an instance manually
and getting it created from the
Beanstalk so click on the ec2 let's go
to the old ec2 experience that's what we
are familiar with and let's see what's
there in the dashboard so here you can
see one running instance let's open that
and the XYZ environment which was
created from the Beanstalk is being
initiated the instance is being
initiated and that is something which is
being done by the mean stock itself we
have not gone to the dashboard and
created it manually now in the security
groups if you see that here the AWS mean
stock security groups are defined it has
the elastic IPS also defined so
everything is being created by the
Beanstalk itself right now let's go back
to the Beanstalk and let's look into the
status of our environment whether the
color coding has been changed from Gray
to green or not and here you can see the
environment is successfully created and
we have that environment colored in
green we'll access the environment and
it says it's a web server environment
its platform is node.js running on
64-bit Amazon Linux Ami and it's a
sampler app sample application health
status is okay now the other thing is
that if you do not want to use the web
console the Management console to access
the main stock then the Beanstalk offers
you the elastic Beanstalk CLI as well so
you can install the command line
interface and then you have the command
references CLI command references that
you can actually play with and get your
applications deployed using the
Beanstalk itself so this is one of the
sample CLI commands that you can
actually look into now let's look into
the environment let's click on the
environment and we'll be represented
with the URL it says health is okay
these are the logs that you have to
follow in case if there are any issues
the platform is node.js that is what we
selected now the next thing is you just
have to upload and deploy your
applications so just click on upload and
deploy select the version label or the
name select file and wherever your
application is hosted at just select
that upload it and deploy your
application you'll see that the like
your environment is created similarly
your application will be deployed
automatically on the instance and from
this URL you will be able to view the
output it is as simple as just like you
have to follow these four steps now
let's see whether the node.js
environment is running on our instance
before deploying an application so we'll
just click on this URL since the
Beanstalk has already opened up the
security groups or HTTP Port 80 for all
we can actually view that output
directly from the URL so we have the
node.js running that's visible here and
after that you just have to upload and
deploy your application and then from
that URL you can get the output now this
URL you can map it with the root 53
service so using the root 53 DNS
Services the domain names can be pointed
to the elastic bean stock URL and from
there it can be pointed to the
applications that are running on the ec2
instance whether you wanted to point it
to the URL directly using the Beanstalk
you can do that otherwise as we saw in
the slides you can use the root 53
pointer to the load balancer and then
point it to the instances directly also
once it is created by the mean stock so
that was the demo guys with respect to
the bean stock and how we can actually
run the environments apart from that the
operational task like system operations
you can manage all these things from the
environment dashboard itself so you have
the config Integrations you have the
logs you can actually check the health
status of your environment you can do
the monitoring and you can actually get
the alarms and the events here so let's
say if I wanted to if I wanted to see
the logs I can request for the logs here
itself and I'll be represented with the
full log report and I can now download
that log file and I can view the logs so
it's in the so we have this bundle locks
in the zip file right so if you want to
see some kind of logs with respect to
elastic bean stock activity it's in the
form of a notepad and here you can see
what all configurations the Beanstalk
has done on your environment on your
instance similarly you can go for the
health monitoring alarms events and all
those things video here we're going to
talk about Amazon ECS a service that's
used to manage Docker containers so
without any further Ado let's get
started in this session we would like to
talk about some Basics about AWS and
then we're going to immediately Dive In
to why Amazon ECS and what is Amazon ECS
in general and then it uses a service
called Docker so we're going to
understand what Docker is and there are
competitive services available for ECS I
mean you could ECS is not the on and
only service to manage Docker containers
but why ECS advantage of ECS we will
talk about that and the architecture of
ECS so how it functions what are the
components present in it and what are
the functions that it does I mean each
and every component what are all the
functions that it does all those things
will be discussed in the architecture of
Amazon ECS and how it works how it all
connects together that's something we
will discuss and what are the companies
that are using ECS what were the
challenge and how ECS helped to fix the
challenge that's something we will
discuss and finally we have a wonderful
lab that talks about how to deploy
Docker containers on an Amazon ECS so
let's talk about what is AWS Amazon web
service in short call as AWS is an web
service in the cloud that provides a
variety of services such as compute
power database storage content delivery
and a lot of other resources so you can
scale your business and grow not focus
more on your ID needs and the rest of
the ID demands rather you can focus on
your business and let Amazon scale your
it or let Amazon take care of your it so
what is that you can do with AWS with
AWS we can create deploy any application
in the cloud so it's not just deploying
you can also create your application in
the cloud it has all the tools and
services required the tools and services
that you would have installed in your
laptop or you would have installed in
your on-premises a desktop machine for
your development environment you know
the same thing can be installed and used
from the cloud so you can use cloud for
creating and not only that you can use
the same Cloud for deploying and making
your application available for your end
user the end user could be internal
internal users the end user could be the
could be in the internet the end user
could be kind of spread all around the
world it doesn't matter so it can be
used to create and deploy your
applications in the cloud and like you
might have guessed now it provides
service over the Internet that's how
your users worldwide would be able to
use the service that you create and
deploy right so it provides service over
the Internet so that's for the End
customer and how will you access those
Services that's again through the
internet it's like the extension of your
data center in the internet so it
provides all the services in the
internet it provides compute service
through the internet so in other words
you access them through the internet it
provides database service through the
internet over the internet in other
words you can securely access your
database through the internet and lot
more and the best part is this is a pay
as you go or pay only for what you use
there is no long term or you know
beforehand commitment here most of the
services does not have any commitment so
there is no long term and beforehand
commitment you only pay exactly for what
you use there's no overage there's no
overpaying right there's no buying in
advance right you only pay for what you
use let's talk about what ECS is so
before ECS before containers right ECS
is a service that manages Docker
containers right it's not a product or
it's not a feature all by itself it's a
service that's dependent on Docker
container so before Docker containers
all all the applications were running on
VM or on an host or on a physical
machine right and that's memory bound
that's latency bound the server might
have issues on and on right so let's say
this is Alice and she's trying to access
her application which is running
somewhere in her on premises and the
application isn't working what could be
the reason some of the reasons could be
Memory full the server is currently down
at the moment we don't have another
physical server to launch the
application a lot of other reasons so a
lot of reasons why the application
wouldn't be working in on-premises some
of them are Memory full issue and server
down issue very less High availability
or in fact single point of failure and
no high availability if I if I need to
tell it correctly with ECS the services
can kind of breathe free right the
services can run seamlessly now how how
is that possible those thing we will
discuss in the upcoming sessions so
because of containers and easiest
managing containers the applications can
run in a high available mode they can
run in a high available mode meaning if
something goes wrong right there's
another container that gets spun up and
your application runs in that particular
container very less chances of your
application going down that's what I
mean this is not possible with a
physical Host this is very less possible
with an a VM or at least it's going to
take some time for another VM to get
spun up so why ECS or what is ECS Amazon
ECS maintains the availability of the
application and allows every user to
scale containers when necessary so it
not only meets the availability of the
application meaning one container
running your application or one
container hosting your application
should be running all that time so to
meet that high availability availability
is making sure your service is running
24.7 so Container makes sure that your
services run 24 bar 7 not only that not
only that suddenly there is an increase
in demand how do you meet that Demand
right let's say you have like thousand
users suddenly the next week there are
like 2000 users all right so how do you
meet that demand Container makes it very
easy for you to meet that demand in case
of VM or in case of physical host you
literally will have to go buy another
physical host or you know add more RAM
add more memory add more CPU power to it
all right or kind of Club two three
hosts together clustering you would be
doing a lot of other things to meet that
high availability and also to meet that
demand but in case of ECS it
automatically scales the number of
containers it automatically scales the
number of containers needed and it meets
your demand for that particular are so
what is Amazon ECS the full form of ECS
is elastic container service right so
it's basically a container Management
Service which can quickly launch and
exit and manage Docker containers on a
cluster so it's the function of ECS it
helps us to quickly launch and quickly
exit and manage Docker container so it's
kind of a Management Service for the
docker containers you will be running in
Amazon or running in the AWS environment
so in addition to that it helps to
schedule the placement of container
across your cluster so it's like this
you have two physical hosts you know
joined together as a cluster and ECS
helps us to place your containers now
where should your container be placed
should it be placed in host one should
be placed in host 2. so that logic is
defined in ECS which can Define it you
can also let ECS take control and Define
that logic most cases you will be
defining it so schedule the placement of
containers across your cluster let's say
two containers want to interact heavily
you really don't want to place them in
two different hosts all right you would
want to place them in one single host so
they can interact with each other so
that logic is defined by us and these
container services you can launch
containers using AWS Management console
and also you can launch containers using
SDK kids available from Amazon you can
launch through a Java program you can
launch container using an.net program
you can launch container using an
node.js program as in when the situation
demands so there are multiple ways you
can launch containers through Management
console and also programmatically and
ECS also helps to migrate application to
the cloud without changing the code so
anytime you think of migration the first
thing that comes to your mind is that
how will that environment be based on
that I'll have to alter my code what's
what's the IP what is the storage that's
been used what what are the different
parameters I'll have to include the
environment parameters of the new
environment with containers now that
worry is already taken away because we
can create an pretty exact environment
the one that you had on premises the
same environment gets created in the
cloud so no worries about changing the
application parameter no worries about
changing the code in the application
right you can be like if it ran in my
laptop a container that I was running in
my laptop it's definitely going to run
in the cloud as well because I'm going
to use the same container in the laptop
and also in the cloud in fact you're
going to ship it you're gonna move the
container from your laptop to Amazon ECS
and make it run there so it's like the
same the very same image the very same
container that was running in your
laptop will be running in the cloud or
production environment so what is Docker
we know that it ECS helps to quickly
launch exit and manage darker containers
what is Docker let's let's answer that
question what is a Docker now Docker is
a tool that helps to automate the
development of an application as a
lightweight container so that the
application can work efficiently in
different environments this is pretty
much what we discussed right before the
slide I can build an application in my
laptop or in on-premises in a container
environment Docker container environment
and anytime I want to migrate right I
don't have to kind of rewrite the code
and then re-run the code in that new
environment I can simply create an image
a Docker image and move that image to
that production or the new Cloud
environment and simply launch it there
right so no compiling again no
relaunching the application simply pack
all your code in a Docker container
image and ship it to the new environment
and launch the container there that's
all so Docker container is a light
weight package of software that contains
all the dependencies so because you know
when packing you'll be packing all the
dependencies you'll be packing the code
you'll be packing the framework you'll
be packing the libraries that are
required to run the application so in
the new environment you can be pretty
sure you can be guaranteed that it's
going to run because it's the very same
code it's the very same framework it's
the very same libraries that you have
shipped right there's nothing new in
that new environment it's the very same
thing that's going to run in that
container so you can be rest assured
that they are going to run in that new
environment and these Docker containers
are highly scalable and they are very
efficient some only you wanted like 20
more Docker containers to run the
application think of adding 20 more
hosts 20 more VMS right how much time
would it take and compared to that time
the amount of time that Docker
containers would require to kind of
scale to that amount like 20 more
containers it's very less or it's
minimal or negligible so it's a highly
scalable and it's a very efficient
service you can subtly scale number of
Docker containers to meet any additional
demand very short boot up time because
it takes uh it's not going to load the
whole operating system and these Docker
containers you know they use the Linux
kernel and features of the kernel like c
groups and namespaces to kind of
segregate the processor so they can run
independently any environment and it
takes very less time to boot up and the
data that are stored in the containers
are kind of reusable so you can have
have an external data volume and I can
map it to the container and whatever the
space that's occupied by the container
and the data that the container puts in
that volume they are kind of reusable
you can simply remap it to another
application you can kind of remap it to
the next successive container you can
kind of remap it to the next version of
the container next version of the
application you'll be launching and you
don't have to go through building the
data again from the scratch whatever
data the container was using previously
or the previous container was using that
data is available for the next container
as well so the volumes that the
containers users are very reusable
volumes and like I said it's isolated
application so it kind of isolates by
its nature it kind of by the way it's
designed by the way it is created it
isolates one container from another
container meaning anytime you run
applications on different containers you
can be rest assured that they are very
much isolated though they are running on
the same host though they are running on
the same laptop let's say though they
are running on the same physical machine
let's say running 10 containers 10
different applications you can be sure
that they are well disconnected or well
isolated applications now let's talk
about the advantages of ECS the
advantage of ECS is improved security
it's security is inbuilt in ECS with ECS
we have something called as a container
registry you know that's where all your
images are stored and those images are
accessed only through https not only
that those images are actually encrypted
and access to those images are allowed
and denied through identity and access
management policies IAM and in other
words let's say two container running on
the same instance the one container can
have access to S3 and the others or the
rest of the others are denied access to
S3 so that kind of granular security can
be achieved through containers when we
mix and match the other security
products available in Amazon like IAM
encryption accessing it using https
these containers are very cost efficient
like I've already said these are
lightweight processors right we can
schedule multiple containers on the same
node and this actually allows us to
achieve high density on an ec2 instance
imagine an ec2 instance that that's very
less utilized that's not possible with a
container because you can actually dance
or crowd an ec2 instance with more
container in it so to best use those
resources in ec2 straightforward you can
just launch one application but with
when we use containers you can launch
like 10 different applications on the
same ec2 server that means 10 different
applications can actually feed on on
those resources available and can
benefit the application and ECS not only
deploys the container it also maintains
the state of the containers and it makes
sure that the minimum a set of
containers are always running based on
the requirement that's another cost
efficient way of using it right and
anytime an application fails and that
has a direct impact on the revenue of
the company and easiest make sure that
you're not losing any Revenue because
your application has failed and ECS is
in pretty extensible Services it's like
this in many organization there are
majority of unplanned work because of
environment variation a lot of
firefighting happens when we kind of
deploy the code from one or kind of move
the code or redeploy the code in a new
environment a lot of firefighting
happens there right this Docker
containers are pretty extensible like we
discussed already environment is not a
concern for containers because and it's
going to kind of shut itself inside a
Docker container and anywhere the docker
container can run the application will
run exactly the way it performed in the
past so environment is not a concern for
the docker containers in addition to
that ECS is easily scalable we have
discussed this already and it improves
it has improved compatibility we have
discussed this already let's talk about
the architecture of ECS like you know
now the architecture of ECS is the ECS
cluster itself that's group of servers
running the ECS service and it
integrates with Docker right so we have
a Docker registry Docker registry is a
repository where we store all the docker
images or the container images so it's
like three components ECS is of three
components one is the ECS cluster itself
right when I say easy as itself I'm
referring to easiest cluster cluster of
servers that will run the containers and
then the repository where the images
will be stored right the repository
where the images will be stored and the
image itself so container image is the
template of instructions which is used
to create a container right so it's like
what's the OS what is the version of
node that should be running and any
additional software do we need so those
question gets answered here so it's the
template template of instructions which
is used to create the containers and
then the registry is the service where
the docker images are stored and shared
so many people can store there and many
people can access or if there's another
group that wants to access they can
access the image from there or one
person can store the image and rest of
the team can access and the rest of the
team can store imagine this one person
can pick the image from there and kind
of ship it to the customer or ship it to
the production environment all that's
possible in this container registry and
Amazon's version of the container
registry is ECR and there's a third
party Docker itself has a container
registry that's Docker Hub ECS itself
which is the the group of servers that
runs those containers so these two the
container image and the container
registry they kind of handle Docker in
an image format just an image format and
an ECS is where the container gets live
and then it becomes an compute resource
and starts to handle request now starts
to serve the page and starts to do the
batch job you know whatever your plan is
with that container so the cluster of
servers ECS integrates well with the
familiar services like VPC VPC is known
for securing VPC is known for isolating
the whole environment from rest of the
customers or isolating the whole
environment or the whole infrastructure
from the rest of the clients in your
account or from the rest of the
applications in your account on and on
so VPC is a service that provides or
gives you the network isolation ECS
integrates well with VPC and this VPC
enables us to launch AWS resources such
as Amazon ec2 instance in a virtual
private Network that we specified this
is basically what we just discussed now
let's take a closer look at the ECS how
does ECS work let's find answer for this
question how does ECS work ECS has got a
couple of components within itself so
these easier servers can run across
availability Zone as you can see there
are two availability zones here they can
actually run across availability zones
and ECS has got two modes of fargate
more and the ec2 mode right here we've
seen fargate more and then here we see
seeing nothing that means it's an ec2
mode and then it has got different
network interfaces attached to it
because they need to be running in an
isolated fashion right so anytime you
want Network isolation you need separate
IP and if you want separate IP you need
separate network interface card and
that's what you have elastic network
interface card separate elastic network
interface card for all those tasks and
services and this runs within an VPC
let's talk about the far gate service
tasks are launched using the far gate
service so we will discuss about task
what is forget now fargate is a compute
engine in ECS that allows users to
launch containers without having to
monitor the cluster ECS is a service
that manages the containers for you
right otherwise managing containers will
be an full-time job so easiest manages
it for you and if you and you get to
manage ECS that's the basic service but
if you want Amazon to manage ECS and the
containers for you we can go for fargate
so fargate is a compute engine in ECS
that allows users to launch containers
without having to monitor the ECS
cluster and the tasks the tasks that we
discussed the tasks has two components
you see task right here so they have two
components we have ECS container
instance and then the container agent so
like you might have guessed right now
easiest container instance is actual and
ec2 instance right capable of running
containers not all ec2 instances can run
containers so these are like specific
ec2 instances that can run containers
they are ECS container instances and
then we have container agent which is
the agent that actually binds those
clusters together and it does a lot of
other housekeeping work right kind of
connects class Masters make sure that
the version needed is present so it's
all part of that agent or it's all job
of that agent container instances
container instances is part of Amazon
ec2 instance which run Amazon ECS
container agent pretty straightforward
definition and then a container agent is
responsible for communication between
ECS and the instance and it also
provides the status of the running
containers kind of monitors the
container monitors the state of the
container make sure that the content is
up and running and if there's anything
wrong it kind of reports it to the
appropriate service to fix the container
on and on it's a container agent when we
don't manage container agent it it runs
by itself and you really don't have to
do anything to make the container agent
better it's already better you really
won't be configuring anything in the
agent and then elastic network interface
card is in Virtual interface Network
work that can be connected to an
instance in VPC so in other words
elastic network interface is how the
container interacts with another
container and that's how the container
interacts with the ec2 host and that's
how the container interacts with the
internet external world and a cluster a
cluster is a set of ECS container
instances it's not something that's very
difficult to understand it's simply a
group of ec2 instances that runs that
ECS agent and this cluster it cluster
handles the process of scheduling
monitoring and scaling the request we
know that ECS can scale the containers
can scale how does it scale that's all
Monitor and managed by this ECS cluster
let's talk about the companies that are
using Amazon ECS there are a variety of
companies that use ACS clusters to name
a few okta users easiest cluster and
OCTA is a product that use identity
information to Grant people access to
applications on multiple devices at any
given point of time they make sure that
they have a very strong security
protection so OCTA uses Amazon ECS to
run their OCTA application and serve
their customers and abima abhima is an
TV channel and they chose to use
microservices and a Docker containers
they already had microservices and
Docker containers and when they thought
about a service that they can use in AWS
ECS was the only service that they can
immediately adapt to and because in
abima TV the engineers have already been
using Docker and Docker containers it
was kind of easy for them to adapt
themselves to ECS and start using it
along with the benefits that ECS
provides free previously they had to do
a lot of work but now ECS does it for
them all right similarly remind and
Ubisoft GoPro or some of the famous
companies that use Amazon ECS and get
benefited from its scalability get
benefited from its cost gets benefited
from its Amazon managed Services get
benefited from the portability that ECS
and the migration option that ECS
provides let's talk about how to deploy
a Docker container on Amazon ECS the way
to deploy Docker container on ECS is
first we need to have an AWS account and
then set up and run our first ECS
cluster so in our lab we're going to use
the launch wizard to run an ECS cluster
and run containers in them and then task
definition task definition tells the
size of the container the number of the
container and when we talk about size it
tells how much of CPU do you need how
much of memory do you need and talking
about numbers you know it requires how
many numbers of container you're going
to launch you know is it five is it 10
or is it just one running all the time
now those kind of information goes in
the task definition file and then we can
do some Advanced configuration on ECS
like a load balancers and you know what
port number you want to allow when you
don't want to allow you know who gets
access who shouldn't get access and
what's the IP that you want to allow and
deny requests from on and on and this is
where we would also mention the name of
the container so the differentiate one
container from the other and the name of
the servers you know is it an a backup
job is it a web application is it an a
data container is it going to take care
of your data data backend and the
desired number of tasks that you want to
be running all the time those details go
in when we try to configure the ECS
service right and then you configure
cluster so you put in all the security
in the configure your cluster step or
configure cluster stage and finally we
will have an instance and bunch of
containers running in that instance all
right let's do a demo so here I have
logged in to my Amazon portal and let me
switch to the appropriate region I'm
going to pick North Virginia North
Virginia look for ECS and it tells ECS
is a service that helps to run and
manage Docker containers well and good
click on it I'm a not Virginia I just
want to make sure that I'm in the right
region and go to clusters and here we
can create cluster this is our forget
and this is our ec2 type launching for
Linux and windows environment but I'm
going to launch through this walkthrough
portal right this gives lot of
information here so the different steps
involved here is creating a container
definition which is what we're going to
do right now and then a task definition
and then service and finally the cluster
it's a four-step process so in container
definition we Define the image the base
image we are going to use now here I'm
going to launch an uh httpd or a simple
HTTP web page right so a simple httpd
2.4 image is fair enough for me and it's
not an heavy application so 0.5 gigabit
of memory is enough and again it's not a
heavy application so 0.25 virtual CPU is
enough in our case right you can edit it
based on the requirement you can always
edit it and because I'm using hdpd the
port mapping is already Port 80 that's
how the container is going to receive
the request and there's no health check
as of now when we want to design
critical and complex applicated
environments we can include health check
right and this is the CPU that we have
chose we can edit it and I'm going to
use some bash commands to create an HTML
page right this page says that you know
Amazon ECS sample app right and then it
says Amazon ECS sample app your
application is running on a container in
Amazon ECS so that's the page the HTML
page that I'm going to create
index.html so I'm going to create and
put it in an appropriate location so
those pages can be served from the
container right if you replace this with
any of your own content then it's going
to be your own content ECS comes with
some basic logs and these are the places
where they get stored that's not the
focus as of now all right so I was just
saying that you can edit it and
customize it to your needs we're not
going to do any customization now we're
just getting familiar with ECS now and
the task definition
name of the task definition is first run
task definition and then we are running
it in a VPC and then this is an fargate
mode meaning the servers are completely
handled by Amazon and the task memory is
0.5 gigabit and the task CPU is 0.25
virtual CPU name of the service is it a
batch job is it an you know a front end
is it an back end or is it a simple copy
job what's the service name of the
service goes here again this you can
edit it and here's a security group as
of now I'm allowing for 80 to the whole
world if I want to restrict to a certain
IP I can do that the default option for
load balancing is no load balancer but I
can also choose to have a load balancer
and use port 80 to map that Port 80 to
The Container Port 80 right I can do
that the default is no load balancer all
right let's do one thing let's use load
balancer let's use low balancer and Port
80 that receives information on Port 80
HTTP what's going to be the cluster name
we're in the last step what is the
cluster name cluster name can be simply
learn ECS demo next we're done and we
can create so it's launching a cluster
as you can see and it's picking the task
definition file that we've created and
it's using that to launch and service
and then these are the log groups that
we discussed and it's creating your VPC
remember ECS clubs well with the VPC
it's creating a VPC and it's creating
two subnets here for high availability
it's creating that Security Group Port
80 allowed to the whole world and then
it's putting it behind and load balancer
right generally would take like five to
ten minutes so which is need to be
patient and let it complete its creation
and once this is complete we can simply
access these servers using the load
balancer URL and and when this is
running let me actually take you to the
other products or the other services
that are integrated with the ECS it's
getting created our service is getting
created as of now ECR repository this is
where all our images are stored now as
of now I'm not pulling my image from ECR
I'm pulling it directly from the
internet Docker Docker Hub but all
custom images all custom images they are
stored in this repository so you can
create a repository call it app one
create a repository so here's my
repository so any image that I create
locally or any Docker image that I
create locally I can actually push them
push those images using these commands
right here and they get stored here and
I can make my ECS connect with ECR and
pull images from here so they would be
my custom images and as of now because
I'm using a default image it's directly
pulling it from the internet let's go to
easy two and look for a load balancer
because we wanted to access the
application from behind a load balancer
right so here is a load balancer created
for us and anytime I put the URL so
cluster is now created you know you see
there's one service running all right
let's click on that cluster here is the
name of our application and here is the
tasks the different containers that we
are running and if you click on it we
have an IP right IP of that container
and it says it's running it was created
at such and such time and started at
such and such time and this is the task
definition file that it this container
uses meaning the template the details
the all the version details they all
come from here and it belongs to the
cluster called simply learn ECS demo all
right and you can also get some logs
container logs from here so let's go
back and the there are no ECS instances
here because remember this is forget
you're not managing any ECS instance all
right so that's why you're not seeing
any easiest instance here so let's go
back to tasks and go back to the same
page where we found the IP pick that IP
put it in the browser and you have this
sample HTML page running from an
container so let me go back to load
balancer ec2 and then under ec2 I'll be
able to find a load balancer find that
load balancer pick that DNS name put it
in the browser and now it's accessible
to the load balancer URL right now this
URL can be mapped to other services like
DNS this URL can be emboded hi there I'm
Sam from Simply long and I'm very glad
to walk you through this lesson about
route 53. so in this section we are
going to talk about basics of AWS and
then we're going to immediately dive
into why why we need Amazon Route 53 and
then we're going to expand and talk
about the details of Amazon Route 53 the
benefits it provides over its
competitors and the different types of
routing policy it has and some of Amazon
route 53's key features and we're going
to talk about how to access Route 53 I
mean the different ways the different
methods you can access Route 53 and
finally we're going to end with then a
wonderful demo in route 53. so let's
talk about what is AWS Amazon web
services or AWS in short is a cloud
provider that offers a variety of
services such as a variety of ID
services or infrastructure services such
as compute power database a Content
delivery and other resources that helps
us to scale and grow our business and
AWS is hard AWS is picking up AWS is
being adapted by a lot of customers
that's because AWS is easy to use even
for a beginner and talking about safety
the the AWS infrastructure is designed
to keep the data safe irrespective of
the size of the data with small data be
it very minimal data the all the data
that you have in terabytes and in
petabytes Amazon can keep it safe in
their environment and the wonderful
thing and the most important reason why
a lot of customers move into the cloud
is that the pay as you go pricing there
is no long-term commitment and it's very
cost effective what this means is that
you're not paying for a resource that
you're not using in on-premises you do
pay for resources you're not using a lot
meaning you go and buy a server you do
the estimate for the next five years and
only after like 3 or four years you'll
be hitting the peak capacity but still
you would be buying that capacity before
four years right and then you will
gradually be you know utilizing it from
you know 40 percent date 60 percentage
70 80 and then 100. so what you have
done is that even though you're not
using the full capacity you still have
bought it and are and are paying for it
from day one but in the cloud it's not
like that you only pay for the resources
that you use anytime you want more you
scale up the results and you you pay for
the scaled up resource and anytime you
want less you scale down the results and
you pay less for that scaled down
resource let's talk about why Amazon
Route 53 let's take this scenario where
Rachel is trying to open her web browser
and the URL that she hit isn't working a
lot of reasons behind why the URL isn't
working it could be the server
utilization that went High it could be
it could be the memory usage that went
High a lot of reasons and she started to
think is there an efficient way to scale
resources according to the user
requirements or is there an efficient
way to kind of mask all those failures
and kind of divert the traffic to the
appropriate active you know active
resource or active service that's
running our application you always want
to hide the failures right in I.T kind
of mask the failure and direct the
customer to another healthy service
that's running right none of your
customers would want to see a server not
available or you know none of the
customers your customers would want to
see your service not working not
impressive to them and this is Tom Tom
is an I.T guy and he comes up with an
idea and he's answering Rachel yes we
can scale resources efficiently using
Amazon Route 53 in a sense he's saying
that yes we can mask the failure and we
can keep the services up and running
meaning we can provide more High
availability to our customers with the
use of Route 53 and then he goes on and
explains Amazon Route 53 is a DNS
service that gives developers an
efficient way to connect users to
internet applications without any
downtime now downtime is the key Amazon
Route 53 helps us to avoid any downtime
that customers will experience you still
will have downtime in your server and
your application but your customers will
not be made aware of it and then Rachel
is kind of interested and she's like
yeah that sounds interesting I want to
learn more about it and Tom goes on and
explains the important concepts of
Amazon Rock 53 that's everything that
I'm going to explain it to you as well
alright so what is Amazon Route 53
Amazon raw 53 is a highly scalable DNS
or domain name system web service this
service this Amazon raw 53 it functions
three main things or it has three main
functions so the first thing is if a
website needs a name Route 53 registers
the name for the website domain let's
say you want to buy google.com you want
to buy the domain name let's say you
want to buy that domain name you buy
that through route 53. secondly a Route
53 is the service that actually connects
your server which is running your
application or which is holding which is
serving your web page so that's the
service that actually Route 53 is the
service that connects the user to your
server when they hit google.com in the
browser or whatever domain name that you
have purchased so you bought a domain
name and the user types in your
domainname.com and then rock53 is a
service that helps the user to connect
their browser to the application that's
running in an easy to instance or any
other server that you are using to serve
that content now not only that Route 53
checks Health after resource by sending
automated requests over the internet to
a resource so that's how it identifies
if there is any resource that has failed
when I say resource I'm referring to any
infrastructure failure any application
Level failure so it kind of keeps
checking so it understands it first
before the customer notices it and then
it does the magic kind of shifts the
connection from a one server to the
other server we call it routing we will
talk about that as we progress so the
benefits of using Route 53 it's highly
scalable meaning suddenly let's say the
number of requests the number of people
trying to access your website through
that domain name that you have bought
let's say it has increased Route 53 is
highly scalable right it can handle even
millions and millions of requests
because it's highly scalable and it's
managed by Amazon uh the same thing it's
reliable it's a highly scalable it can
handle large queries without the users
Without You interacting without the user
who bought it interact with it you don't
have to scale up you know when you're
expecting more requests it automatically
scales and it is very reliable in a
sense that it's very consistent it has
the ability to Route the users to the
appropriate application through the
logic that it has it's very easy to use
when we do the lab you're going to see
that it's very easy easy to use you buy
the domain name and then you simply map
it to the application you simply map it
to the server by putting in the IP or if
you you can simply map it to another
load balancer by putting in the load
balancer URL you can simply map it to
another S3 Buckhead by simply putting
the S3 bucket name or the S3 bucket URL
it's pretty straightforward easy to set
up and it's very cost effective in a way
that we only pay for the service that we
have used so no wastage of money here so
the billing is set up in such a way that
you are paying only for the amount of
requests that you have received right
the amount of traffic the amount of
requests that you have received and
couple of other things the the number of
hosted zones that you have created right
and a couple of other things it's very
cost effective in such a way that you
only pay for the service that you are
using and it's secure in a way that
access to Route 53 is integrated with
identity and access management IAM so
you only have authorized users gain
access to Route 53. the trainee who just
joined s today won't get access and the
contractor or the consultant the third
party consultant you have given access
or who is using your environment you can
block access to that a particular person
because he's not the admin or he's not a
privileged user in your account so only
privileged users and admin gain access
to Route 53 through IAM now let's talk
about the routing policies so when you
create a record in in Route 53 record is
nothing but an entry so when you do that
you choose a routing policy all right
routing policy is nothing but it
determines how Route 53 responds to your
queries how the DNS queries are being
responded right that's that's a record
or that's a routing policy so the first
one is a simple routing policy so we use
Simple routing policy for a single
resource in other words a simple routing
allows to configure DNS with no special
Route 53 routing it's kind of one to one
you use an single resource that performs
a given function to your domain for
example if you want to Simply map an URL
to a web server that's pretty
straightforward simple routing so it
routes traffic to a single resource
example web server to a website and with
simple routing multiple records with the
same name cannot be created but multiple
values can be created created in the
same record the second type of routing
policy is failover routing so we would
be using failover routing when we want
to configure active passive failover if
something failed right you want to fail
over to the next resource which was
previously the backup resource now the
active resource or which was previously
the backup server now it's an active
server so you would be failing over to
that particular resource or that
particular IP if you want to do that we
use failover routing so failure routing
routes traffic to a resource when the
resource is healthy or to a different
resource when the previous resource is
unhealthy in other words anytime a
resource goes unhealthy I mean it does
all that's needed to shift the traffic
from the primary resource to the
secondary resource in other words from
the unhealthy resource to the healthy
resource and this records can Route
traffic to anything from an Amazon S3
bucket or or you can also configure a
complex tree of Records now when we
configure the records it will be more
clear to you so as of now just
understand that Route 53 can route or
this routing policy the failover routing
policy can Route traffic to Amazon S3
bucket or to a website that has complex
tree of Records geolocation routing
policy now geolocation routing just like
the name says it takes that routing
decision based on the geographic
location of the user in other words you
know when you want to Route traffic
based on the location of the user so
that's your primary criteria for sending
that request to the appropriate server
we will be using jio location routing so
it localizes the content and presence a
part or the entire website in the
language of the user for example a user
from us you would want to direct them to
an English website and a user from
German if you want to send them to the
German website and a user from France
you know you want to send those requests
or you want to show content specific to
a customer who lives in France a French
website so this is if it's that's your
condition this is the routing policy we
would be using and the geographic
locations are specified by either
continent or by country or by state in
the United States so only in the United
States you can actually split it to
state level and for the rest of the
countries you can do it on a country
level on a high level you can also do it
on a continent level the next type of
routing policy would be your proximity
routing geoproximity routing policy when
we want to Route traffic based on the
location of our resource and optimally
shift traffic from resources in one
location to resource in another location
we would be using geoproximity Raw
outing so geoproximity routing routes
traffic to the resources based on the
geographic location of the user and the
resources they want to access and it
also has an option to Route more traffic
or less to a given resource by
specifying a value known as a biased
kind of weight but we also have weighted
routing that's different so we've chosen
different name bias you can send more
traffic to a particular resource by
having a bias on that particular routing
condition and a bias expands or shrinks
the size of the geographic region from
which traffic is routed to a resource
and then we have latency based routing
just like the name says we use latency
based routing if we have resources in
multiple AWS regions and if you want to
Route traffic to the region that
provides the best latency at any given
point of time so let's say if one one
single website needs to be installed and
hosted on multiple AWS regions then
latency routing policy is what is being
used it improves the performance of the
users by serving their request from the
AWS region that provides the lowest
latency so at any given point if
performance is your criteria and at any
given point of time irrespective of what
happens in Amazon infrastructure
irrespective of what happens in the
internet if you want to route your users
to the best performing website best
performing region then we would be using
latency based routing and for using
latency based routing we should create
latency records for the resources in
multiple AWS regions and then the other
type of a routing policy is multi-value
routing policy where we can make Route
53 to respond to DNS queries with up to
eight healthy records selected at random
so you're not kind of loading one
particular server we can Define eight
records and on a random basis Route 53
will respond to queries from these eight
records so it's not one server that gets
all the requests but eight servers gets
the request in a random fashion so it's
multi-value robotic policy and what we
get by this is that we are Distributing
the traffic to many servers instead of
just one server so multi-value routing
configures Route 53 to return multiple
values in response to a single or
multiple DNS queries it also checks the
health of order sources and Returns the
multiple values only for the healthy
resources let's say out of the eight
servers we have Define one server is not
doing healthy it will not respond to the
query with the details of the unhealthy
server right so now it's going to treat
it as only seven servers in the list
because one server is unhealthy and it
has the ability to return multiple
Health checkable IP addresses to improve
availability and load balancing the
other type of routing policy is weighted
routing policy and in here we use to
Route traffic or this is used to Route
traffic to multiple resources in a
proportion that we specify so this is an
weighted routing and weighted routing
routes multiple resources to a single
domain name or a subdomain and control
the traffic that's routed to each
resources so this is very useful when
you are doing load balancing and testing
new versions of the software so when you
have a new version of the software you
really don't want to send 100 of the
traffic to it so you want to get
customers feedback about the new
software that you've launched new
version or new applications that you've
launched so you would kind of send only
20 of the traffic to that application
get custom more feedback and if all is
good then we would move the rest of the
traffic to that new application so any
software launches application launches
will be using weighted routing now let's
talk about the key benefits or key
features of Route 53. some of the key
features of Route 53 are traffic flow it
routes end users to the endpoint that
should provide the best user experience
that's what we discussed in the routing
policies right it uses a routing policy
a latency based routing policy and Geo
based routing policy and then failover
routing policy so it kind of improves
the user experience and the key feature
the other key feature of Route 53 is we
can buy domain names using a Roth 53
using Route 53 console we can buy it
from here and use it in route 53.
previously it was not the case but now
we can buy it directly from Amazon
through Route 53 and we can assign it to
any resources that we want so anybody
browsing that URL the connection will be
directed to the server in AWS that runs
our website a health checks it monitors
health and performance of the
application so it comes with an health
check attached to it health check are
useful to make sure that the unhealthy
resources are retired right the
unhealthy resources are taken away or
your customers are not kind of hitting
the unhealthy resources and they see an
service down page or something like that
we can have weighted round robin load
balancing that's helpful and spreading
traffic between several services or
servers we are round robin algorithm so
no one server is fully hit or no one
server kind of fully absorbs all the
traffic you know you can shift you can
split and shift the traffic to different
servers based on the way that you would
be configuring and also weighted routing
also helps with a soft launch soft
launch of your new application or the
new version of your website there are
different ways we can access Amazon
Route 53 so you can access Amazon Route
53 through AWS console you can also
access Amazon Route 53 using AWS sdks
and we can access it using we can
configure it using the apis and we can
also do it through the command line
interface that's Linux type Linux flavor
AWS command line interface we can also
do that using Windows command line
Windows Powershell flavored command line
interface as well now let's look at some
of the companies that are using raw 53.
so some of the famous companies that use
Route 53 are medium medium is an online
publishing platform and it's more like a
social journalism it's kind of having
hybrid collection of professionals
people and Publications or exclusive
blogs or Publishers on medium it's kind
of an blog website and that uses Route
53 for the DNS service our Reddit is an
social news aggregation or web content
rating and discussion website that uses
route 53s so these are some websites
that that are accessed throughout the
world and they are using Roth 53 and
it's highly scalable suddenly there is a
new news right their website will be
accessed a lot and they need to keep
their service up and running all the
time more availability otherwise
customers will end up in a broken page
and the number of customers who will be
using the website will come down so it's
very critical these sites these
companies are very critical you know
they're being highly available the page
their site being highly available and
the internet is very critical and
crucial for them and they rely and use
Route 53 to meet that particular demand
and Airbnb is another company instacart
khoza is another company stripe is
another company that uses Route 53 to as
their DNS provider for the DNS service
they use Route 53 so their customers get
best performance they use Route 53 so
their website is highly available they
use Route 53 to kind of shift to the
traffic between the resources so their
resources are properly used with all the
weighted routing the resources are
properly used now let's quickly look at
a demo I'm in my AWS console and I'm in
Route 53 so let me click on Route 53 so
in this lab we're actually going to
simulate buying a domain name and then
we're going to create an S3 static
website and we're going to map that
website to this DNS name right so the
procedure is the same for mapping load
balancer the procedure is the same for
mapping cloudfront the procedure is the
same for mapping ec2 instances as well
we're picking S3 for Simplicity right
but our focus is actually on Route 53.
so let's go in here and I will see if we
can but we will buy a domain name here
so let's first check the availability of
a domain name called simply learn hyphen
demo hyphen Route 53 let's check its
availability it is available for 12 so
let me add it to cart and then come back
here and then once you continue it last
for personal information once you give
the personal information you finally
check out and then it gets added to your
shopping list once you pay for it Amazon
takes like 24 to 40 dollars to make that
a DNS name available so the next stage
would be contact details and then the
third stage would be verify and purchase
so once we've bought the domain name it
will become available in our DNS portal
and and I do have a domain name which I
bought some time back and it's now
available for me to use so I can go to
hosted Zone and simply start creating I
can go to hosted Zone and then here it's
going to list all the domain names for
me all right click on the domain name
and then click on the record set and
here I can actually map elastic load
balancer S3 website VPC endpoint API
Gateway and cloudfront elastic Beanstalk
domain names right all that gets mapped
through this portal quite simple like
four or five step button clicks and then
it'll be done so I have an domain name
bot and then I'm going to go to S3 and
I'll show you what I've done in S3 so
I've created a bucket name call as DNS
name let me clear the content in them so
I've created a bucket and then
permissions I've turned off Public
Access blocking and then I've created a
bucket policy so this bucket is now
publicly accessible and then I went on
the properties and created the static
website hosting right and I pointed that
this is the file that's my index file
that I'm going to put or name of the
file that's going to be my index file
that I'm going to put in this S3 bucket
so put the index file.html saved it and
we're going to create a file now we're
going to create an index file so this is
a sample code it says amazon.53 getting
started routing internet traffic to S3
bucket for your website and then couple
of other information so save it as an
index.html file in my desktop so let me
upload that from my desktop into this
bucket so that's index.html and it's in
capital I so let me go to properties and
go to static website hosting and make
sure that I spell it properly right it's
case sensitive and then save it so now
this means that my website should be
running through this URL and it does
it's running to the static website you
URL we're halfway through so now let me
go back to Route 53 go back to Route 53
go back to hosted zones go into the
domain name and then create a record set
and it's going to be an alias record and
I I see my S3 static website endpoint
there all right so click on it and
create it has now created an record
that's pointing my domain name to the S3
endpoint that I have created and my
static website is running from it so let
me test it right so let me go to the
browser put the domain name in there and
sure enough the domain name when we
browser queried for the domain name
around 53 returned a response saying
this domain name is actually mapped to
the S3 bucket historic website hosting
enable S3 bucket and this is the URL for
that static website hosting and then my
browser was able to connect to that S3
bucket and download the details and show
it in my browser all right so it's that
simple hi guys this is Akil and today we
are going to discuss about Amazon
redshift which is one of the data
warehouse service on the AWS but before
starting up with the Amazon redshift I
would request you guys to subscribe our
Channel you can find the link just below
this video at the right side so let's
begin with Amazon redshift and let's see
what we have for today's session so
what's in it for you today we'll see
what is AWS why we require Amazon
redshift what do we mean by Amazon
redshift the advantages of Amazon
redshift the architecture of Amazon
redshift some of the additional Concepts
associated with the redshift and the
companies that are using the Amazon
redshift and finally we'll cover up one
demo which will show you the Practical
example that how you can actually use
the redshift service now what is AWS as
we know that AWS stands for Amazon web
service it's one of the largest cloud
providers in the market and it's
basically a secure cloud service
platform provided from the Amazon also
on the AWS you can create and deploy the
applications using the AWS service along
with that you can access the services
provided by the AWS over the public
network that is over the Internet they
are accessible plus you pay only
whatever the service you use for now
let's understand why we require Amazon
redshift so earlier before Amazon
redshift what used to happen that the
people used to or the developers use to
fetch the data from the data warehouse
so data warehouse is basically a
terminology which is basically
represents the collection of the data so
a repository where the data is stored is
generally called as a data warehouse now
fetching data from the data warehouse
was a complicated task because might be
a possibility that the developer is
located at a different geography and the
data data warehouse is at a different
location and probably there is not that
much network connectivity or some
networking challenge changes internet
connectivity challenges security
challenges might be and a lot of
Maintenance was required to manage the
data warehouses so what are the cons of
the traditional data warehouse Services
it was time consuming to download or get
the data from the data warehouse
maintenance cost was high and there was
the possibility of loss of information
in between the downloading of the data
and the data rigidity was an issue now
how these problems could overcome and
this was basically solved with the
introduction of Amazon redshift over the
cloud platform now we say that Amazon
redshift has solved traditional data
warehouse problems that the developers
were facing but how what is Amazon
redshift actually is so what is Amazon
redshift it is one of the services over
the AWS Amazon web services which is
called as a data warehouse service so
Amazon redshift is a cloud-based service
or a data warehouse service that is
primarily used for collecting and
storing the large chunk of data so it
also helps you to get or extract the
data analyze the data using some of the
bi tools so business intelligence tools
you can use and get the data from the
redshift and process that and hence it
simplifies the process of handling the
large-scale data sets so this is the
symbol for the Amazon redshift over the
AWS now let's discuss about one of the
use case so DNA is basically a
telecommunication company and they were
facing an issue with managing their
website and also the Amazon S3 data
which led down to slow process of their
applications now how could they overcome
this problem let's see that so they
overcome this issue by using the Amazon
redshift and all the company noticed
that there was a 52 increase in the
application performance now did you know
that Amazon redshift is basically cost
less to operate than any other cloud
data warehouse service available on the
cloud computing platforms and also the
performance of an Amazon redshift is the
fastest data warehouse we can say that
that is available as of now so in both
cases one is that it saves the cost as
compared to the traditional data
warehouses and also the performance of
this redshift service or a data
warehouse service the fastest available
on the cloud platforms and more than 15
000 customers primarily presently they
are using the Amazon redshift service
now let's understand some of the
advantages of Amazon redshift first of
all as we saw that it is one of the
fastest available data warehouse service
so it has the high performance second is
it is a low cost service so you can have
a large scale of data warehouse or a
databases combined in a data warehouse
at a very low cost so whatever you use
you pay for that only scalability now in
case if you wanted to increase the nodes
of the databases in your redshift you
can actually increase that based on your
requirement and that is on the fly so
you don't have to wait for the
procurement of any kind of a hardware or
the infrastructure it is whenever you
require you can scale up or scale down
the resources so the scalability is
again one of the advantage of the Amazon
lecture availability since it's
available across multiple availability
zones so it makes this service as a
highly available service security so
whenever you create whenever you access
redshift you create a clusters in the
redshift and the Clusters are created in
the you can define a specific virtual
private Cloud for your cluster and you
can create your own security groups and
attach it to your cluster so you can
design the security parameters based on
your requirement and you can get your
data warehouse or the data items in a
secured Place flexibility and you can
remove the Clusters you can create under
clusters if you are deleting a cluster
you can take a snapshot of it and you
can move those snapshots to different
regions so that much flexibility is
available on the AWS for the service and
the other Advantage is that it is
basically a very simple way to do a
database migration so if you're planning
that he wanted to migrate your databases
from the traditional data center over
the cloud on the redshift it is
basically a very simple to do a database
migration you can have some of the
inbuilt tools available on the AWS
access you can connect them with your
traditional Data Center and get that
data migrated directly to the redshift
now let's understand the architecture of
the Amazon redshift so architecture of
an Amazon redshift is basically it
combines of a cluster and that we call
it as a data warehouse cluster in this
picture you can see that this is a data
warehouse cluster and this is a
representation of a Amazon redshift so
it has some of the compute nodes which
does the data processing and a leader
node which gives the instructions to
these compute nodes and also the leader
node basically manages the client
applications that require the data from
the redshift so let's understand about
the components of the redshift the
client application of Amazon redshift
basically interact with the leader node
using jdbc or the odbc now what is jdbc
it's a Java database connectivity and
the odbc stands for open database
connectivity the Amazon redshift service
using a jdbc connector can monitor the
connections from the other client
applications so the leader node can
actually have a check on the client
applications using the jdbc connections
whereas the odbc allows a leader node to
have a direct interaction or to have a
live interaction with the Amazon
redshift so odbc allows a user to
interact with live data of Amazon
redshift so it has a direct connectivity
direct access of the applications as
well as the leader node can get the
information from the compute nodes now
what are these compute nodes these are
basically kind of a databases which does
the processing so Amazon redshift has a
set of computing resources which we call
it as a nodes and the nodes when they
are combined together they are called it
as a clusters now a cluster a set of
computing resources which are called as
nodes and this gathers into a group
which we call it as a data warehouse
cluster so you can have a compute node
starting from one to n number of nodes
and that's why we call that the redshift
is a scalable service because we can
scale up the compute nodes whenever we
require now the data warehouse cluster
or the each cluster has one or more
databases in the form of a nodes now
what is the leader node this node
basically manages the interaction
between the client application and the
compute node so it acts as a bridge
between the client application and the
compute nodes also it analyzes and
develops designs in order to carry out
any kind of a database operations so
leader node basically sends out the
instructions to the compute nodes
basically perform or execute that
instruction and give that output to the
leader node so that is what we are going
to see in the next slide that the leader
node runs the program and assign the
code to individual compute nodes and the
compute nodes execute the program and
share the result result back to the
leader node for the final aggregation
and then it is delivered to the client
application for analytics or whatever
the client application is created for so
compute nodes are basically categorized
into slices and each node slice is
allotted with specific memory space or
you can say a storage space where the
data is processed these node slices
Works in parallel in order to finish
their work and hence when we talk about
a redshift as a fast a faster processing
capability as compared to other data
warehouses or traditional data
warehouses this is because that these
node slices work in a parallel operation
that makes it more faster now the
additional concept associated with
Amazon redshift is there are two
additional Concepts associated with the
Redshirt one is called as the column
storage and the other one is called as
the compression let's see what is the
column storage as the name suggests
column storage is basically kind of a
data storage in the form of a column so
that whenever we run a query it becomes
easier to pull out the data from The
Columns so column storage is an
essential factor in optimizing query
performance and resulting in quicker
output so one of the examples are
mentioned here so below example show how
database tables store record into disk
block by row so here you can see that if
we wanted to pull out some kind of an
information based on the city address
age we can basically create a filter and
from there we can put out the details
that we require and that is going to
fetch out the details based on the
column storage so that makes data more
structured more streamlined and it
becomes very easier to run a query and
get that output now the compression is
basically to save the column storage we
can use a compression as an attribute so
compression is a column level operation
which decreases the storage requirement
and hence it improves the query
performance and this is one of the
Syntax for the column compressor now the
companies that are using Amazon Redshirt
one is Lya the other one is Equinox the
third one is the Pfizer which is one of
the famous Pharmaceuticals company
McDonald's one of the burger chains
across the globe and Phillips it's an
electronic company so these are one of
the biggest companies that are basically
relying and they are putting their data
on the redshift data warehouse service
now in another video we'll see the demo
for using the Amazon redshift let's look
into the Amazon redshift demo so these
are the steps that we need to follow for
creating the Amazon redshift cluster and
in this demo what we'll be doing is that
we'll be creating an IM role for the
redshift so that the redshift can call
the services and specifically we'll be
using the S3 service so the role that we
will be creating will be giving the
permission to redshift to have an access
of an S3 in the read-only format so in
the step one what we require we'll check
the prerequisites and what you need to
have is the AWS credentials if you don't
have that you need to create your own
credentials and you can use your credit
and the debit card and then in the Step
2 we'll proceed with the IM role for the
Amazon redshift once the rule is created
rated we'll launch a sample Amazon
redshift cluster mentioned in the step 3
and then we'll assign a VPC security
groups to our cluster now you can create
it in the default VPC also you can
create a default security groups also
otherwise you can customize the security
groups based on your requirement now to
connect to the sample cluster you need
to run the queries and you can connect
to your cluster and run queries on the
AWS Management console query editor
which you will find it in the redshift
only or if you use the query editor you
don't have to download and set up a SQL
client application separately and in the
step 6 what you can do is you can copy
the data from the S3 and upload that in
the redshift because the redshift would
have an access a read-only access for
the S3 as that will be created in the IM
role so let's see how we can actually
use the redshift on the AWS so I am
already logged in into my account I am
in North Virginia region I'll search for
redshift service and here I find Amazon
redshift so just click on it
let's wait for the redshift to come now
this is a redshift dashboard and from
here itself you have to run the cluster
so to launch a cluster you just have to
click on this launch cluster and once
the cluster is created and if you wanted
to run queries you can open query editor
or you can basically create queries and
access the data from the redshift so
that's what it was mentioned in the
steps also that you don't require a
separate SQL client application to get
the queries run on the data warehouse
now before creating a cluster we need to
create the role so what we'll do is
we'll click on the services and we'll
move to IM role section so IM rule I can
find here under the security identity
and compliance so just click on the
identity access management and then
click on create roles so let's wait for
IM page to open so here in the IM
dashboard you just have to click on the
rules I already have the role created so
what I'll do is I'll delete this role
and I'll create it separately so just
click on a create role and under the AWS
Services you have to select for the
redshift because now the the redshift
will be calling the other services and
that's why we are creating the role now
which other services that the redshift
will be having an access of S3 why
because we'll be putting up the data on
the S3 and that is something which needs
to be uploaded on the redshift so we'll
just search for the redshift service and
we can find it here so just click on it
and then click on redshift customizable
in the use case now click on next
permissions and here in the permissions
give the access to this role assign the
permissions to this role in the form of
an S3 read-only access so you can search
here for the S3 Also let's wait for the
policies to come in here it is let's
type S3 and here we can find Amazon S3
read-only access so just click on it and
assign the permissions to this role tags
you can leave them blank click on next
review put a name to your role let's put
my redshift role and click on a create
role now you can see that your role has
been created now the next step is that
we move to Redshirt service and we'll
create one cluster so click on the
services click on Amazon redshift you
can find that in the History Section
since we browsed it just now and from
here we are going to create a sample
cluster now to launch a cluster you just
have to click on launch this cluster
whatever the uncompressed data size you
want in the form of a gigabyte terabyte
or petabyte you can select that and
let's say if you select in the form of
GB how much GB memory you want you can
Define it here itself this also gives
you the information about the costing On
Demand is basically pay as you use so
they are going to charge you 0.5 dollars
per hour for using the two node slices
so let's click on launch this cluster
and this will be a dc2 DOT large kind of
an instance that will be given to you it
would be in the form of a solid state
drive ssds which is one of the fastest
way of storing the information and the
nodes two are mentioned by default that
means there will be two node slices and
that will be created in a cluster you
can in increase them also let's say if I
put three note slices so it is going to
give us 3 into 0.16 DB per node storage
now here you have to define the master
username password for your redshift
cluster and you have to follow the
password instructions so I would put a
password to this cluster and if it
accepts that means it does not give you
any kind of a warning otherwise it is
going to tell you about you have to use
the ASCII characters and all and here
you have to assign this cluster the role
that we created recently so in the
available IM rules you just have to
click on my redshift role and then you
have to launch the cluster if you wanted
to change something in with respect to
the default settings let's say if you
wanted to change the VPC from default
VPC to your custom VPC and you wanted to
change the default security groups to
your own security groups so you can
switch to advanced settings and do that
modification now let's launch the
cluster and here you can see the
redshift cluster is being created now if
you wanted to run queries on this thread
shift cluster so you don't require a
separate SQL client you just have to
follow the simple steps to run a query
data and the query editor you will find
it on the dashboard so let's click on
the cluster and here you would see that
the redshift cluster would be created
with the three nodes in the US East 1B
availability zone so we have created the
redshift cluster in the Ohio region and
now what we'll do is we'll see how we
can create the tables inside the
redshift and we'll see how we can use
the copy command so that we can directly
move the data uploaded on the S3 bucket
to the redshift database tables and then
we'll query the results of a table as
well so how we can do that first of all
after creating the redshift cluster we
have to install SQL workbench slash J
this is not a MySQL which is managed by
Oracle and you can find this on the
Google you can download it from there
and then you have to connect this client
with direct shift database how you can
do click on file click on connect window
and after connecting a window you have
to paste the URL which is a jdbc driver
this driver link you can find it onto
the AWS console so if you open up a
redshift cluster there you would find
the jdbc driver link let's wait for it
so this is our cluster created let's
open it and here you can find this jdbc
URL and also make sure that in the
security groups of a redshift you have
the port 5439 open for the traffic
incoming traffic you also need to have
the Amazon redshift driver and this is
the link where you can download the
driver and specify the path once you are
done with that you provide the username
and the password that you created while
creating the redshift cluster click on
OK so this connects with the database
and now the database connection is
almost completed now what we will be
doing in the SQL workbench will be first
creating the sales table and then in the
sales table we'll be adding up the
entries copied from the S3 bucket and
then move it to the redshift database
and after that we'll query the results
in the sales table now whatever the
values you are creating in the table the
same values needs to be in the data file
and I have taken up this sample data
file from this link which is
docs.aws.amazon.com redshift sample
database creation and here you can find
a download file ticketdb.zip file this
folder has basically multiple data files
sample data files which you can actually
use it to practice uploading the data on
the redshift cluster so I have extracted
one of the files from this folder and
then I have uploaded that file in the S3
bucket now we'll move into the S3 bucket
let's look for the file that has been
uploaded on the S3 bucket so this is the
bucket sample and sales underscore tab
dot text is the file that I have
uploaded this has the entries data
entries that will be uploaded using a
copy command onto the redshift cluster
now after executing after putting up the
command for creating up the table then
we'll use a copy command and copy
command we have to define the table name
the table name is sales and we have to
define the path from where the data
would be copied over to the sales table
in the redshift now path is the S3
bucket and this is the redshift bucket
sample and it has to look for the data
inside the sales underscore tab dot text
file also we have to define the role Arn
that was created previously and once it
is done then the third step is to query
the results inside the sales table to
check whether our data has been uploaded
correctly on the table or not now what
we'll do is we'll execute all these
three syntax it gives us the error
because we have to connect it again to
the database let's wait for it execute
it it again gives us the error let's
look into the name of the bucket it's
redshift bucket sample so we have two
T's mentioned here right let's connect
with the database again and now execute
it so table sales created and we got the
error the specified bucket does not
exist uh redshift bucket sample let's
view the bucket name redshift bucket
sample let's copy that put it here
connect to the window connect back to
the database right and now execute it so
table sales created uh the data in the
table has been copied from the S3 bucket
to sales underscore tab dot text to the
redshift and then the query of the
results now the results from the
this section we're going to take a look
at Amazon dynamodb
Amazon dynamodb is a fast flexible nosql
database service for all applications
that need consistent single digit
millisecond latency at any scale
so basically it's a fully managed nosql
database that's suitable for document
and key Value Store models it's perfect
for mobile web gaming ad Tech Etc
let's take a look at some of the
benefits of dynamodb firstly it's always
stored on SSD storage so you get great
disk throughput
it's also spread across three
geographically distinct data centers so
you get high availability built in it's
fast and flexible
and it's suited for read heavy
applications and it's worth remembering
that because you might see questions
saying I have a read intensive
application with a database what should
I use and dynamodb would be the answer
dynamodb offers two different
reconsistancy types
the first is eventually consistent reads
which is the default and then they're
strongly consistent reads
so what does this mean well when you
write something to your dynamodb the
changes have to be replicated to the
three geographically distinct data
centers on which dynamodb is being
stored and obviously this isn't instant
I mean it takes a couple of seconds or
less but it takes some time so you can
choose your read consistency depending
on whether you want users to be
guaranteed to have the correct data or
whether on occasions when they run a
query they might not get the latest
up-to-date version
eventually consistence reads which is
the default means that consistency is
usually reached within one second
I'm repeating a read after a short time
should return the correct data
and it gives the best read performance
but there is the danger of something
called Dirty reads which is where you
might request a read and you might not
get the latest up-to-date data
if it's essential that your application
gets the latest in correct data then you
can enable strongly consistent reads and
this guarantees that all rights have
committed prior to the read taking place
so it could result in a delay to your
results being returned
dynamodb is charged using something
called provisioned throughput capacity
when you create a dynamodb table you
define the capacity that you want to
reserve for reads and writes
for right farupa there's a charge per
hour for every 10 units of right
capacity
and this equates to 36 000 writes per
hour
the read throughput there's a charge per
hour for every 50 units of recapacity
and this equates to 180 000 strongly
consistent reads or 360
000 eventually consistent reads
what this shows is that dynamodb can be
expensive for rights but cheap for reads
so if your application has lots of reads
little writes and wants to be scalable
then you should be looking at dynamodb
rather than RDS
there's also a storage cost per gigabyte
for the amount of data you use with
dynamodb
and here's a very brief real-life case
study Duolingo the online language
learning site uses dynamodb to store 31
billion items for their online learning
site and they deliver lessons in 80
different languages
and their application hits 24 000 read
units per second and 3 300 write units
per second so you can see this is a very
read heavy application and is perfect
for dynamodb
illustration we're going to take a very
quick look at dynamodb
so we go to the database section and
we'll find dynamodb which is the manage
nosql database provided by AWS
so we'll click on that we're taken to
the dashboard I have no dynamodb tables
so we need to create a new table
and we need to give it a name so let's
call this
simply learn
underscore users that's going to be the
name of our table we have to give it a
key so a primary key so I'm going to put
in last name
because that's going to be the primary
key for this particular table and that's
going to be a string
now down here we have table settings so
we can use the default settings and it
says no secondary indexes his provision
capacity is set to five reads and five
rights and has basic alarms with 80
upper thresholds so if an alarm if one
of your read or write capacities is
greater than 80 of what you've set then
you'll get an alert
so if we untick this you can see we're
not going to take a look at secondary
indexes but the thing that's important
is here we can change to read and write
capacity units so if we set it both to
one we'll get an estimated cost of just
59 cents a month there conversely if we
push this way up
push it up to a thousand then you see
it's 580 dollars a month so it's fully
scalable and you just will pay for what
you need so let's just set this to one
and one because this is just a
demonstration
and we'll click on create table
okay so the table has been created okay
so our table has been created so here we
can see the details of it and its
creation date
so what we now need to do is we can add
items to our table so we click on the
items tab
okay and we need to create an item so
you see this is now this should look
familiar to you because we're basically
doing Json code which is the same as for
the policies in the I am lesson so we
get to stick in a last name so let's put
let's put me in so I'll stick Weaver and
then we can add
more values to append let's append a
string and we'll put a
first name
and we'll put my name in there
and we could put more information if we
wanted but we're just going to leave it
as that so we click on Save
and there we are so now I'm in there so
I'll click on create item
and let's add a new user in there so
call the last name Smith
let's add another string so we'll have a
new field called first name
and we'll call this person Wendy and
we'll add another row let's put a string
in there and we'll call this uh position
so this shows what you can do with nosql
you can basically just keep
changing things and each row can be
different so without enter the position
of manager and we'll click save so now
we can see that it's different so you
can do whatever you want basically you
can add as much or as little information
to each row as you wanted
now you can also do a search so if we
wanted to add a filter we could say
search for last name
we have a string that equals
and there we are so we have Weaver Mark
and just to clear that we just remove
that and click on start search again so
that gives you a very basic overview of
what you can do with nosql there's
unstructured data and you can add as
much or as little as you want some other
things to look at the metrics tab here
you can see your read and write capacity
what what's actually happening so you
can see whether you're paying too much
or if you need to be paying too much
more
as the alarms alarms that we talked
about so they were set up for us some
basic alarms if the threshold goes over
0.08 reads or writes for a 12 minute
period will get notified
and here's the capacity so this is the
thing that you'll see in the exam
is that dynamodb you can just change
your capacity and as and when you want
on the fly so
our table is up and running but we could
just say we want five read capacity
units and five right capacity units
click save and we're done it's up and
running straight away
and if you knew how much you wanted and
you had a table that you were going to
be using for a look for a long period
like a couple of years you can click on
reserve capacity on the left hand side
and you can purchase Reserve capacity
and you'll get some significant
discounts by doing that so if you know
you have a table that's going to have
some heavy reads and writes going
forward you would purchase Reserve
capacity
okay so that's a very basic overview of
dynamodb you just need to remember that
it's instantly scalable
and it's a nosql database hi guys I'm
Raul from Simply learn and today I'd
like to welcome you all to the greatest
debate of the century today I am joined
by two giants of the cloud computing
industry they'll be going head to head
with each other to decide who amongst
them is better it's going to be one hell
of fight now let's meet our candidates
on my left we have AWS who's voiced by a
picture hi guys and on my right we have
Microsoft Azure whose voiced by Anjali
hey there so today we'll be deciding
who's better on the basis of their
origin and the features they provide
their performance on the present day and
comparing them on the basis of pricing
market share and options free tier and
instance configuration now let's listen
to their opening statements let's start
with AWS launched in 2006 AWS is one of
the most commonly used cloud computing
platforms across the world companies
like Adobe Netflix Airbnb HTC Pinterest
and Spotify have put their faith in AWS
for their proper functioning it also
dominates the cloud computing domain
with almost 40 percent of the entire
market share so far nobody's even gotten
close to beating that number AWS also
provides a wide range of services that
covers a great number of domains domains
like compute networking storage
migration and so much more now let's see
what Azure has to say about that Azure
was launched in 2010 and is trusted by
almost 80 percent of all Fortune 500
companies the best of the best companies
in the world choose to work only with
Azure Azure also provides its services
to more regions than any other cloud
service provider in the world Azure
covers 42 regions already and 12 more
are being planned to be made Azure also
provides more than 100 Services spanning
a variety of domains now that the
opening statements are done let's have a
look at the current market status of
each of our competitors this is the
performance route here we have the stats
for the market share of AWS Azure and
other cloud service providers this is
for the early 2018 period Amazon web
services takes up a whopping 40 percent
of the market share closely followed by
sgr at 30 and other cloud services
adding 30 this 40 indicates most
organizations clear interest in using
AWS VR number one because of our years
of experience and Trust we've created
among our users sure you're the market
leader but we are not very far behind
let me remind you more than 80 percent
of the Fortune 500 companies trust Azure
with their cloud computing needs so it's
only a match of time before Azure takes
the lead the rest of the 30 percent that
is in AWS or Azure accounts to the other
cloud service providers like Google
Cloud platform Rackspace IBM software
and so on now for our next round the
comparison round first we'll be
comparing pricing we'll be looking at
the cost of a very basic instance which
is a virtual machine of two virtual CPUs
and 8 GBS of RAM for AWS this will cost
you approximately
0.0928 US dollars per hour and for the
same instance in Azure it will cost you
approximately 0.096 US dollars per hour
next up let's compare market share and
options as I mentioned before AWS is the
Undisputed market leader when it comes
to the cloud computing domain taking up
40 of the market share by 2020 AWS is
also expected to produce twice its
current Revenue which comes close to 44
billion dollars not to mention AWS is
constantly expanding its already strong
roaster of more than 100 services to
fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gartner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a matter
of time before Azure can increase from
its 30 market share and surpass AWS this
becomes more likely considering how all
companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirements now
let's compare AWS and azure's free
offerings AWS provides a significant
number of services for free helping
users get get hands-on experience with
the platform products and services the
free tier Services fall under two
categories services that will remain
free forever and the others that are
valid only for one year the always free
category offers more than 20 services
for example Amazon SNS sqs cloudwatch
Etc and the valid for your category
offers approximately 20 services for
example Amazon S3 ec2 elastic cache Etc
both types of services have limits on
the usage for example storage number of
requests compute time Etc but users are
only charged for using services that
fall under the valid for a year category
after a year of their usage a show
provides a free tier as well it also
provides services that belong to the
categories of free for a year and always
free there are about 25 plus always free
services provided by Azure these include
app service functions container service
active directory and lots more and as of
the valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of 200
US dollars to access all their services
for 30 days now this is a unique feature
that Azure provides where its users can
use their credits to utilize any service
of a choice for the entire month now
let's compare instance configuration the
largest instance that AWS offers is that
of a whopping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of RAM and 16 virtual CPUs and now
for the final round each of our
contestants will be shown facts and they
have to give explanations for these
facts we call it the rapid fire round
first we have features in which AWS is
good and Azure is better AWS does not
cut down on the features it offers its
users however it requires slightly more
Management on the user's part Azure goes
slightly deeper with the services that
fall under certain categories like
platform as a service and infrastructure
as a service
next we have hybrid Cloud where AWS is
good and Azure is better OK although AWS
did not emphasize on hybrid Cloud
earlier they are focusing more on
technology now Azure has always
emphasized on hybrid cloud and has
features supporting it since the days of
its inception for developers AWS is
better and Azure is good of course it's
better because AWS supports integration
with third-party applications well Azure
provides access to data centers that
provide a scalable architecture for
pricing both AWS and Azure are at the
same level it's good for AWS because it
provides a competitive and constantly
decreasing pricing model and in the case
of azure it provides offers that are
constantly experimented upon to provide
its users with the best experience and
that's it our contestants have finished
giving their statements now let's see
who won surprisingly nobody each cloud
computing platform has its own pros and
cons choosing the right one is based
entirely on your organization
hi guys today we've got something very
special in store for you we're going to
talk about the best cloud computing
platform available Amazon web services
uh Rahul I think you said something
wrong here the best cloud computing
platform is obviously Google Cloud
platform no it isn't AWS is more than
100 services that span a variety of
domains all right that Google Cloud
platform has cheaper instances what do
you have to say about that well I guess
there's only one place we can actually
discuss this a boxing ring so guys I'm
apeksha and I will be Google Cloud
platform and I'm Rahul I'll be AWS so
welcome to fight night this is AWS
versus gcp the winner will be chosen on
the basis of their origin and the
features they provide their performance
in the present day and comparing them on
the basis of pricing market share and
options the things they give you for
free and instance configuration now
first let's talk about AWS AWS was
launched in 2004 and is a cloud service
platform that helps businesses grow and
scale by providing them services the
number of different domains these
domains include compute database storage
migration networking and so on a very
important aspect about AWS is its years
of experience now AWS has been on the
market a lot longer than any other cloud
service platform which means they know
how businesses work and how they can
contribute to the business growing also
AWS has over 5.1 billion dollars of
Revenue in the last quarter this is a
clear indication of how much faith and
trust people have in AWS they occupy
more than 40 of the market which is a
significant chunk of the cloud computing
Market they have at least 100 services
that are available at the moment which
means just about every issue that you
have can be solved within AWS service
now that was great but now can we talk
about gcp I hope you know that GCB was
launched very recently in 2011 and it is
already helping businesses significantly
with a suite of intelligent secure and
flexible Cloud Computing Services it
lets you build deploy and scale
applications websites services on the
same infrastructure as Google the
intuitive user experience that gcp
provides with dashboards Wizards is way
better in all the aspects gcp has just
stepped in the market and it is already
offering a modest number of services and
the number is rapidly increasing and the
cost for a CPU instance or Regional
storage that JCP provides is a whole lot
cheaper and you also get a
multi-regional cloud storage now what do
you have to say on that I'm so glad you
asked let's look at present day in fact
let's look at the cloud market share of
the fourth quarter of 2017. this will
tell you once and for all that AWS is
the leader when it comes to cloud
computing Amazon web services
contributes 47 of the market share
others like Rackspace or Verizon cloud
contribute 36 Microsoft Azure
contributes 10 the Google Cloud platform
contributes four percent and IBM
software contributes three percent 47
percent of the market share is
contributed by AWS you need me to
convince you any more wait wait wait all
that is fine but we only started a few
years back and have already grown so
much in such a less span of time having
to heard the latest news our revenue is
already a billion dollars per quarter
wait for a few more years and the world
shall see and AWS makes 5.3 billion
dollars per quarter it's going to take a
good long time before you can even get
close to us yes yes we'll see now let's
compare a few things for starters let's
compare price for AWS a compute instance
of two CPUs and 8GB ram costs
approximately 68 US Dollars now compute
instance is a virtual machine in which
you can specify what operating system
Ram or storage you want to have for
cloud storage it costs 2.3 cents per GB
per month with AWS you really want to do
that because gcp wins this hands down
let's take the same compute instance of
two CPUs with 8GB Ram it will cost
approximately 50 dollars per month with
gcp and as per my calculations that's a
25 annual cost reduction when compared
to AWS talking about cloud storage costs
it is only 2 cents per GB per month with
gcp what else do you want me to say
let's talk about market share and
options now AWS is the current market
leader when it comes to cloud computing
now as you remember we contribute at
least 47 of the entire market share AWS
also has at least 100 services available
at the moment which is a clear
indication of how well AWS understands
businesses and helps them grow yeah
that's true but you should also know
that gcp is steadily growing we have
over 60 services that are up and running
as you can see here and a lot more to
come it's only a matter of time when we
will have as many services as you do
many companies have already started
adopting gcp as their cloud service
provider now let's talk about things you
get for free with AWS you get access to
almost all the services for an entire
year with usage limits now these limits
include an hourly or by the minute basis
for example with Amazon ec2 you get 750
hours per month you also have limits on
the number of requests to services for
example with AWS Lambda you have 1
million requests per month now after
these limits are crossed you get charged
standard rates with gcp you get access
to all Cloud platform products like
Firebase the Google Maps API and so on
you also get 300 in credit to spend over
a 12 month period on all the cloud
platform products and interestingly
after the free trial ends you won't be
charged unless you manually upgrade to a
paid account now there is also the
always free version for which you will
need an upgraded billing account here
you get to use a small instance for free
and 5 GB of cloud storage any usage
above this always free usage limits will
be automatically billed at standard
rates now let's talk about how you can
configure instances with AWS the largest
instances offered is of 128 CPUs and 4
degrees of ram now other than the
on-demand method like I mentioned before
you can also use Spartan sensors now
these are for situations where your
application is more fault tolerant and
can handle an interruption now you pay
for the spot price which is effective at
a particular are so these part prices do
fluctuate but are adjusted over a period
of time the largest instance offered
with Google cloud is 160 CPUs and 3.75
TBS Ram like spot instances of AWS
Google Cloud offers short-lived compute
instances suitable for back jobs and for
tolerant workloads they are called as
preemptable instances so these instances
are available at 80 percent off on
on-demand price hence they reduce your
compute engine costs significantly and
unlike AWS these come at a fixed price
Google Cloud platform is a lot more
flexible when it comes to instance
configuration you simply choose your CPU
and RAM combination of course you can
even create your own instance types this
way before we wrap it up let's compare
on some other things as well Telemetry
it's a process of automatically
collecting periodic measurements from
remote devices for example GPS gcp is
obviously better because they have
Superior Telemetry tools which help in
analyzing services and providing more
opportunities for improvement when it
comes to application support AWS is
obviously better since they have years
of experience under their belt AWS
provides the best support that can be
given to the customers containers are
better with gcp a container is a virtual
process running in user space as
kubernetes was originally developed by
Google TCP has full native support for
the tool other cloud services are just
fine-tuning a way to provide kubernetes
as a service also the containers help
with abstracting applications from their
environment they originally run it the
applications can be deployed easily
regardless of their environment when it
comes to geographies AWS is better since
they have a head start of a few years
AWS in this span of time has been able
to cover a larger market share and
geographical locations now it's time for
the big decision so who's going to be
yeah who is it going to be TCP or AWS I
think I'm going to go for choosing the
right cloud computing platform is a
decision that's made on the basis of the
user or the organized requirement on
that note I believe it's time for us to
wrap this video up we hope you guys
enjoyed this video and learn something
new that AWS is better right no that
choosing a platform entirely depends on
you and your organization's requirement
if you're interested you could also go
through the AWS versus visual video
thank you for watching
AWS Azure and gcp are three of the
world's largest cloud service providers
but how are they different from each
other let's find out hey guys I'm Rahul
and I'll be representing Amazon web
services I'm chinmayi and I'll be
representing Microsoft Azure and I am
Shruti and I'll be representing Google
Cloud platform so welcome to this video
on AWS vs Azure versus gcp talking about
market share Amazon web services leads
with around 32 percent of the worldwide
public cloud share Azure owns up to 16
of the worldwide market share and gcp
owns around nine percent of the world's
market share let's talk about each of
these service providers in detail AWS
provides services that enable users to
create and deploy applications over the
cloud these services are accessible via
the Internet AWS being the oldest of the
lot was launched in the year 2006. Azure
launched in 2010 is a Computing platform
that offers a wide range of services to
build manage and deploy applications on
the network using tools and Frameworks
launch in the year 2008 gcp offers
application development and integration
services for its end users in addition
to Cloud management it also offers
services for Big Data machine learning
and iot now let's talk about
availability zones these are isolated
locations within data center regions
from which public cloud services
originate and operate talking about AWS
they have 69 availability zones within
22 geographical regions this includes
regions in the United States South
America Europe and Asia Pacific they are
also predicted to have 12 more editions
in the future Azure available in 140
countries has over 54 regions worldwide
grouped into six geographies these
geographical locations have more than
100 data centers gcp is available in 200
plus countries across the world as of
today gcp is present in 61 zones and 20
regions with Osaka and Zurich being the
newly added regions now let's talk about
pricing these Services follow the pay as
you go approach you pay only For The
Individual Services you need for as long
as you use them without requiring
long-term contracts or complex licensing
now on screen you can see the pricing
for each of these cloud service
providers with respect to various
instances like general purpose compute
optimized memory optimized and GPU now
let's talk about the compute services
offered first off we have virtual
servers for AWS we have ec2 it is a web
service which eliminates the need to
invest in Hardware so that you can
develop and deploy applications in a
faster manner it provides virtual
machines in which you can run your
applications azure's virtual machines is
one of the several types of computing
resources that Azure offers Azure gives
the user the flexibility to deploy and
manage a virtual environment inside a
virtual Network gcp's VM service enables
users to build deploy and manage virtual
machines to run workloads on the cloud
now let's talk about the pricing of each
of these Services aw cc2 is free to try
it is packaged as part of aws's free
tier that lasts for 12 months and
provides 750 hours per month of both
Linux and Windows Virtual machines Azure
virtual machine service is a part of the
free tier that offers this service for
about 750 hours per month for a year the
user gets access to Windows and Linux
virtual machines gcp's VM service is a
part of a free tier that includes micro
instance per month for up to 12 months
now let's talk about platform as a
service or past services for AWS elastic
Beanstalk is an easy to use service for
deploying and scaling web applications
and services developed with java.net
node.js python and much more it is used
for maintaining capacity provisioning
load balancing Auto scaling and
application Health monitoring the past
backbone utilizes virtualization
techniques where the virtual machine is
independent of the actual Hardware that
hosts it hence the user can write
application code without worrying about
the underlying Hardware Google app
engine is a cloud computing platform as
a service which is used by Developers
for hosting and building apps in Google
data centers the app engine requires the
apps to be written in Java or Python and
store data in Google big table and use
the Google query language for this next
let's talk about virtual private server
Services AWS provides light sale it
provides everything you need to build an
application or website along with the
cost effective monthly plan and minimum
number of configurations in simple words
VM image is a more comprehensive image
for Microsoft Azure virtual machines it
helps the user create many identical
virtual machines in a matter of minutes
unfortunately gcp does not offer any
similar service next up we have
serverless Computing Services AWS has
Lambda it is a serverless compute
service that lets you run your code
without facilitating and managing
servers you only pay for the compute
time you use it is used to execute
back-end code and scales automatically
when required Azure functions is a
serverless compute service that lets you
run even triggered code without having
to explicitly provision or manage
infrastructure this allows the users to
build applications using serverless
simple functions with the programming
language of their choice gcp Cloud
functions make it easy for developers to
run and scale code in the cloud and
build image driven serverless
applications it is highly available and
fault tolerant now let's talk about
storage services offered by each of
these service providers First off we
have object storage AWS provides S3 it
is an object storage that provides
industry standard scalability data
availability and performance it is
extremely durable and can be used for
storing as well as recovering
information or data from anywhere over
the Internet blob storage is an Azure
feature that lets developers store
unstructured data in Microsoft's Cloud
platform along with storage it also
offers scalability it stores the data in
the form of tiers depending on how often
data is being accessed Google Cloud
Storage is an online storage web service
for storing and accessing data on Google
Cloud platform infrastructure unlike the
Google Drive Google Cloud Storage is
more suitable for Enterprises it also
stores objects that are organized in to
buckets Amazon provides EBS or elastic
Block store it provides high performance
block storage and is used along with ec2
instances for workloads that are
transaction or throughput intensive
Azure managed disk is a virtual hard
disk you can think of it like a physical
disk in an on-premises server but
virtualized these managed disks allow
the users to create up to 10 000 VM
disks in a single subscription
persistent storage is a data storage
device that retains data after power to
the device is shut off Google persistent
disk is durable and high performance
block storage for gcp persistent disk
provides storage which can be attached
to instances running in either Google
compute engine or kubernetes engine next
up we have Disaster Recovery Services
AWS provides a cloud-based recovery
service that ensures that your it
infrastructure and data are recovered
while minimizing the amount of downtime
that could be experienced site recovery
helps ensure business continuity by
keeping business apps and workloads
running during outages it allows
recovery by orchestrating and automating
the replication process of azure virtual
machines between regions unfortunately
gcp has no disaster recovery service
next let's talk about database Services
first off for AWS we have RDS or
relational database service it is web
service that's cost effective and
automates administration tasks basically
it simplifies the setup operation and
scaling of a relational database
Microsoft Azure SQL database is a
software as a service platform that
includes built-in intelligence that
learns app patterns and adapts to
maximize performance reliability and
data protection it also eases the
migration of SQL Server databases
without changing the user's applications
Cloud SQL is a fully managed database
service which is easy to set up maintain
and administer relational postgresql
MySQL and SQL Server databases in the
cloud hosted on gcp cloud SQL provides a
database infrastructure for applications
running anywhere next we have nosql
database Services AWS provides dynamodb
which is a managed durable database that
provides security backup and restore and
in-memory caching for applications it is
well known for its low latency and
scalable performance Azure Cosmos DB is
Microsoft's globally distributed
multi-model database service it natively
supports nosql it natively supports
nosql created for low latency and
scalable applications gcp cloud data
store is a nosql database service
offered by Google on the gcp it handles
replication and scales automatically to
your applications load with cloud data
stores interface data can easily be
accessed by any deployment Target now
let's talk about the key Cloud tools for
each of these service providers for AWS
in networking and content delivery we
have AWS Route 53 and AWS cloudfront for
management we have AWS cloudwatch and
AWS cloud formation for development we
have AWS code star and AWS code build
for secure T we have IAM and Key
Management Service for Microsoft Azure
networking and content delivery we have
content delivery Network and express
route for management tools we have Azure
advisor and network Watcher for
development tools for management we have
Azure advisor and network Watcher for
development we have Visual Studio IDE
and Azure blob studio for security we
have Azure security Center and Azure
active directory for gcp we have the
following tools for networking and
content delivery we have Cloud CDN and
Cloud DNS for management we have
stackdriver and gcp monitoring for
development we have Cloud build and
Cloud SDK and finally for security we
have Google cloud IM and Google and
Cloud security scanner now let's talk
about the companies using these Cloud
providers for AWS we have Netflix
Unilever Kellogg's NASA Nokia and Adobe
Pixar Samsung eBay Fujitsu EMC and BMW
among others use Microsoft Azure so as
seen on your screens the companies that
use gcp are Spotify HSBC Snapchat
Twitter PayPal and 20th Century Fox
let's talk about the advantages of each
of these Services Amazon provides
Enterprise friendly Services you can
leverage Amazon's 15 years of experience
delivering large-scale Global
infrastructure and it still continues to
hone and innovate its infrastructure
management skills and capabilities
secondly it provides instant access to
resources AWS is designed to allow
application providers isvs and vendors
to quickly and securely host your
applications whether an existing
application or a new SAS based
application Speed and Agility AWS
provides you access to its services
within minutes all you need to select is
what you require and you can proceed you
can access each of these applications
anytime you need them and finally it's
secure and reliable Amazon enables you
to innovate and scale your application
in a secure environment it secures and
hardens your infrastructure more
importantly it provides security at a
cheaper cost than on-premise
environments now talking about some of
the advantages of azure Microsoft Azure
offers better development operation it
also provides strong security profile
Azure has a strong focus on security
following the Standard Security model of
detect assess diagnose stabilize and
close Azure also provides a
cost-effective solution the cloud
environment allows businesses to launch
both customer applications and internal
apps in the cloud which saves an I.T
infrastructure costs hence its Opex
friendly let's now look at the
advantages of gcp Google bills in minute
level increments so you only pay for the
compute time you use they also provide
discounted prices for long-running
workloads for example you use the VM for
a month and get a discount gcp also
provides live migration of virtual
machines live migration is the process
of moving a running VM from one physical
server to another without disrupting its
availability to the users this is a very
important differentiator for Google
Cloud compared to other Cloud providers
gcp provides automatic scalability this
allows a site's container scale to as
many CPUs as needed Google Cloud Storage
is designed for 99.9 durability it
creates server backup and stores them in
an user configured location let's talk
about the disadvantages of each of these
services for AWS there's a limitation of
the ec2 service AWS provides limitations
on resources that vary from region to
region there may be a limit to the
number of instances that can be created
however you can request for these limits
to be increased secondly they have a
technical support fee AWS charges you
for immediate support and you can opt
for any of these packages developer
which costs 29 per month business which
costs more than 100 an Enterprise that
costs more than fifteen thousand dollars
it has certain network connectivity
issues it also has General issues when
you move to the cloud like downtime
limited control backup protection and so
on however most of these are temporary
issues and can be handled over time
talking about some of the disadvantages
of Microsoft Azure code base is
different when working offline and it
requires modification when working on
the cloud pass ecosystem is not as
efficient as iaas Azure Management
console is frustrating to work work with
it is slow to respond and update and
requires far too many clicks to achieve
simple tasks Azure backup is intended
for backing up and restoring data
located on your on-premises servers to
the cloud that's a great feature but
it's not really useful for doing bare
metal industry stores of servers in a
remote data center let's now look into
the disadvantages of gcp so when it
comes to Cloud providers the support fee
is very minimal but in the case of gcp
it is quite costly it is around 150
dollars per month for the most basic
service similar to AWS S3 gcp has a
complex pricing schema also it is not
very budget friendly when it comes to
downloading data from Google Cloud
Storage
hi there now that you've learned about
the products and services offered by AWS
let us talk about designing Cloud
architecture
designing it infrastructure for the
cloud is completely different than for
traditional on-premise data centers
before the Advent of cloud computing
planning your infrastructure involved
making a series of estimates and
educated guesses to determine the
capacity requirements of your products
and applications
this often resulted in serious over or
underestimations
with AWS you get to rethink your
approach to architecting an I.T
environment so you can take full
advantage of cloud computing
this lesson covers how cloud has brought
in an entirely new way to build it
environments and how one can achieve it
using the AWS recommended best practices
in this lesson you'll learn why building
cloud services requires a different
mindset to that of traditional
on-premise infrastructure
you'll be able to define the key points
in the AWS well architected framework
and understand the four pillars that
make up this framework
you'll learn about the key principles
involved in planning and designing Cloud
infrastructure
you'll have an understanding of the
tools available to enable AWS monitoring
and logging
and you'll also have knowledge on the
tools and functionality available to run
hybrid Cloud infrastructure
this is the how to design cloud services
section where we'll take an overview of
the AWS Cloud design principles
in the topic how to design cloud
services we will take a look at the AWS
well architected framework
and we'll take a deeper look at the four
pillars which make up the framework and
these are security
reliability performance efficiency and
cost optimization
we'll then take a look at where you can
find more information about designing
cloud services for Amazon
and we will look at the AWS quick start
reference deployments
and where you can find out real life
examples about how real businesses are
using Amazon web services
so let's start by looking at the AWS
well architected framework
now this was introduced because building
Cloud infrastructure Services is vastly
different to what you're used to with
on-premise infrastructure
so AWS created this framework to help
you understand the pros and cons of
decisions you make while Building
Systems on AWS
the AWS framework documents a set of
foundation questions that allow you to
understand if a specific architecture
aligns well with bitcloud best practices
and there's five principles it helps you
with the first of which is stop guessing
your capacity needs
test systems of production scale
lower the risk of architecture change
automate to make architecture
experimentation easier
and allow for evolutionary architectures
and we'll take a look at each of these
in Greater detail
so firstly let's take a look at stop
guessing your capacity needs now imagine
you're designing a brand new application
for your business
all the infrastructure decisions you
will make are made using estimates which
are basically guesses
if you overestimate you might end up
buying too much hardware and have a
fleet of expensive idle resources
conversely you might underestimate and
have to deal with the performance
implications of limited capacity
with cloud computing these problems go
away as AWS helps you eliminate the
guesswork in your infrastructure
capacity needs you can use as much or as
little capacity as you need and scale up
and down automatically depending on your
demand
the cloud allows you to test systems of
production scale traditionally in a
non-cloud environment is usually very
difficult to fully Test new productsal
services in development as it's cost
prohibitive to have like the like
environments or you just don't have the
resources available
personally I've lost count how many
projects I've worked on when things have
gone wrong as soon as the product's Gone
live due to insufficient testing
with the cloud you can duplicate
environments on demand and when you've
completed your testing just shut them
down and you only have to pay for the
time the test environment was up and
running so you can test fully before you
go live in production
AWS allows you to lower the risk of
architecture change as you can automate
the creation of test environments
so you can emulate your production
configurations and Carry Out testing
against comparable environments
you can also remove testing bottlenecks
where various teams are lining up to use
your test environments as with AWS you
can cheaply spin up as many test
environments as you need and ensure that
all the production changes have been
sufficiently tested
you can automate to make architecture
experimentation easier
if you need to make a change to
production but not sure what the impact
will be well you can automate the
creation and replication of your systems
at low cost and low effort to test these
changes you can then audit the impact
and then if necessary you can revert the
changes and try again
with AWS you can allow for evolutionary
architectures
in a traditional environment
architecture decisions are often
implemented as a static one-time event
that you have to live with during the
lifetime of a system but with a cloud
the capability exists to automate and
test on demand to lower the risk of
design changes so you can Implement new
Innovations and features easily
what this means is you aren't stuck with
last year's technology and you can
Implement new technology as soon as it
comes out
as mentioned earlier the well
architected framework is based on four
pillars security reliability performance
efficiency and cost optimization
so in the coming slides we're going to
take a further look at these four
pillars starting with security
so Amazon defines security as the
ability to protect information systems
and assets while delivering businesses
value through risk assessments and
mitigation strategies
what this means is you can apply
security at all layers
so rather than just running minimal
security like firewalls at the edge of
your infrastructure
you can use firewalls and other security
controls on all of your resources for
example security groups on ec2 instances
allow you to Define who and what can
access that specific resource rather
than just defining it at a global
infrastructure level
you can enable traceability so you can
log and order all changes to your
environment
you can automate responses to security
events so you can monitor and
automatically trigger alerts depending
on your needs
you can focus on sharing your system the
AWS shared responsibility model allows
you to focus on securing your
application data and operating systems
whilst AWS secures your infrastructure
and services
and you can automate security best
practices AWS allows you to create and
save a custom Baseline image of a
virtual server and then you can use that
image to automatically launch new
servers
this way you can ensure that all new
resources adhere to your security
standards
so I just briefly mentions the AWS
shared responsibility model now we take
a look at this in more depth in a later
lesson
but what it means is that the AWS shared
responsibility model allows AWS
customers to focus on using services to
accomplish their security and compliance
goals because AWS physically secures the
infrastructure that supports your cloud
services
so if you look at this diagram you can
see it split into two
security measures that the cloud service
provider implements and operates is the
security of the cloud and that's a
responsibility of AWS
then there's a security measures that
the customer inputs and operates and
this is related to the security of
customer content and applications that
make use of the AWS services and this is
Security in the cloud and this is the
responsibility of you
so let's take a look at security in the
cloud and this is composed of four areas
data protection privilege management
infrastructure protection and detective
controls
let's start with data protection
before architecting any system practices
that ensure security should be in place
with data protection these are
categorizing data based on levels of
sensitivity
granting least privilege while still
allowing users to perform their work and
encryption to protect your sensitive
data
if you're storing customer credit card
data obviously this needs to be
categorized as sensitive and it needs to
be encrypted but log files from a daily
maintenance task
wouldn't need this level of security
now there's a number of tools AWS offers
to assist you with this
AWS makes it easy to encrypt data and
manage keys and key rotation using IAM
and Key Management Service
you can perform detailed logging using
cloudtrail
and AWS offers resilient storage systems
like Amazon S3 which has 11 nines of
durability
now other lines of durability means that
if you store 10 000 objects with Amazon
S3 on average you can expect to incur a
loss of a single object once every 10
million years so in other words you're
not going to lose many files and finally
AWS offers versioning and lifecycle
management again with S3 so you can
predict again accidental deletes or file
over rights
now a central part of any information
security program is privilege management
so that you can ensure that only
authorized and authenticated users are
able to access your resources
and that they can only access them in a
way that is acceptable for example if a
user needs access to a resource don't
just give them admin or read write
permissions by default because they
might actually only need read-only
access
Now to control your privilege management
you need to have a few things in place
the first of which is an access control
list and this is a document that needs
to be maintained which lists access
permissions attached to an object so if
you have some particular documents you
need to Define what access is allowed to
it
then you have role-based access controls
and this defines the permissions for a
particular end user's role or function
so an administrator would obviously need
administrator access to various
resources but you know a date a regular
end user might only need read-only
access so you need to Define which roles
have which permissions
then there's password management
and this includes complexity
requirements and change intervals so you
define how often people need to change
their passwords and how complex they
need to be
now Amazon helps you with all this with
the identity and access management or I
am service and this is the primary
service you use to control access to AWS
services and resources
we cover I am in huge detail in a later
lesson but the briefly it allows you to
apply granular policies which assign
permissions to users groups roles or
resources and you can also control the
password strength using this as well as
Federation which are existing directory
services such as Microsoft active
directory
we also need to protect your
infrastructure and to do this it's
recommended to have multiple layers of
defense and also multi-factor
authentication to meet best practices
with industry or regulatory obligations
now Amazon allows you to do this
by implementing stateful or stateless
packet inspection EVS in AWS native
Technologies or by using partner
products and services available through
the AWS Marketplace
Amazon virtual Cloud VPC enforces
boundary protection and monitoring
points of Ingress and egress and then
there's cloudtrail and cloudwatch which
provides comprehensive logging
monitoring and alerting
you should always have detective
controls in place so you can detect or
identify security breaches
this needs to happen so you can provide
both a quality support process and meet
compliance obligation
and by a quality support process I mean
one in which when you have a problem
you're committed to finding out the root
cause and actually resolving the issue
rather than just saying oh we think it
was that let's hope it doesn't happen
again
I mean I know I've worked for some
places like that and it's certainly not
a quality support process
AWS helps you with your detective
controls by providing these Services AWS
cloudtrail is a service that logs API
calls so you can find out the identity
of callers the time of calls Source IP
addresses and things like that so you
can find out who is doing what and when
Amazon cloudwatch is a monitoring
service for AWS resources so you can log
ec2 CPU usage or network activity and
you can use cloudwatch with many
different services like RDS EBS and more
you can also set up alarms so you can be
notified when certain things happen
AWS config is an inventory and
configuration history service that
provides information about the
configurations and changes in
infrastructure over time
with Amazon S3 you can see to access
auditing so you can find out who's been
accessing your S3 data and who when they
did it and when
and Amazon Glacier you can use the Vault
lock feature to preserve Mission
critical data with compliance controls
that are designed to be auditable for
long-term retention
the next pillar is reliability and
Amazon defines this as the ability of a
system to recover from infrastructure or
service failures
dynamically acquire Computing resources
to meet demand and mitigate disruptions
such as misconfigurations or transient
network issues
with reliability in the cloud there's a
number of things to look at firstly you
can test recovery procedures
Cloud infrastructure means you can test
how your system fails and you can
validate your recovery strategy
places I've worked at in the past
Disaster Recovery was a big event it was
only done annually it took months of
preparation it involved working a whole
weekend and then some and was also
incredibly disruptive to the business
imagine being able to duplicate your
production environment and test failure
and failover whenever you want
you can automatically recover from
failure with the cloud monitoring a
system for a key performance indicator
or kpi you can automatically trigger
automated recovery processes when a
threshold is breached
you can scale horizontally to increase
aggregate system availability
using multiple small resources instead
of one large resource will reduce your
single points of failure
and also you can stop guessing capacity
gone are the days of using lack of
memory or not enough CPU is a failure
for a resource with the cloud you can
scale as and when you need to so you
can't blame badly estimated capacity as
the reason for reliability issues
now reliability in the cloud is composed
of three areas these are foundations
change management and failure management
to achieve reliability a system must
have a well-planned foundation and
monitoring in place plus systems for
handling changes in demand or
requirements is also required in an
Ideal World your system would be
designed to detect failure and
automatically heal itself
before architecting any system you
should have foundations in place so that
your reliability is not impacted you
should have sufficient Network bandwidth
to your data center you should have
sufficient compute capacity
your staff should be trained in the
areas they need to be and you should
have support contracts with your vendors
with AWS Network bandwidth and compute
capacity is already taken care of
AWS also offers a range of support
contracts to help you with issues and a
range of training courses for your staff
change management is a critical part
being aware of how a change affects the
system allows you to plan proactive and
monitoring allows you to quickly
identify trends that could lead to
capacity issues or SLA breaches
with AWS you can monitor the behavior of
a system and automate the response to
kpis for example you can add additional
servers as a system gains more users so
you can control who has permissions to
make system changes and order the
history of these changes
unfortunately it's a given that failures
will occur so you need to have some
failure Management in place
it's good practice to understand why
things failed so you can prevent it from
happening again no one wants to work for
a company that when things go wrong they
just get it working again with no
understanding of why
a key to managing failure is frequent
and automated testing of systems to
failure and through recovery ideally you
do this on a regular schedule and also
after you've made significant system
changes
now Amazon allows you to do this with a
few products firstly AWS cloud formation
this allows you to launch temporary
copies of whole systems at very low cost
so you can use automated testing to
verify your recovery processes
you can use cloudwatch to set up
automation to react to monitoring data
that indicates a failure
and you can store all your data on
Amazon S3 buckets for future use
the next pillar is performance
efficiency and Amazon defines this as
the ability to use Computing resources
efficiently to meet system requirements
and to maintain that efficiency as
demand changes and Technologies evolve
so what does this mean well performance
efficiency in the cloud means you can
democratize Advanced Technologies
AWS provides products such as nosql or
Media transcoding or even machine
learning as a service so rather than
your it team having to learn how to host
and run these new technologies AWS does
it for you
you can go Global in minutes AWS allows
you to easily deploy your systems in
multiple regions around the world with
just a few clicks of your mouse so this
leads to lower latency for your
customers
you can use serverless architectures in
the cloud which remove the need for you
to run and maintain servers to carry out
traditional compute activities
for example storage Services can act as
static websites which remove the need
for web servers and you can also use
serverless Technologies such as Lambda
or elastic Beanstalk
and you can also experiment more often
with virtual and automotable resources
you can quickly carry a comparative
testing using different types of
instances storage or configuration
so performance efficiency in the cloud
is composed of four areas compute
storage database and something called
space time trade-off and let's take a
look at each of these
with the cloud finding the optimal
server configuration for a product will
vary based on application design usage
patterns and configuration settings
traditional it infrastructure requires
making estimates in advance which can
lead to incorrect server configurations
and lower performance efficiency
AWS is virtualized so you can quickly
change the ec2 server configuration to
increase its performance efficiency
it may be optimal to run serverless
Computing for example AWS Lambda allows
you to execute code without running an
instance an elastic Beanstalk allows you
to run web applications in a similar
manner
from an operational standpoint you
should have monitoring in place to
notify you of any degradation in
performance
the optimal storage solution for a
particular system will vary based on
whether you need block file or object
storage the type of throughput required
frequency of access and availability and
durability constraints
well architected systems use multiple
Storage Solutions and enable different
features to improve performance
in AWS storage is virtualized and is
available in a number of different types
this makes it easier to match your
storage methods more closely with your
needs and also offers storage options
that are not easily achievable with
on-premise infrastructure for example
Amazon S3 is designed for 11 9's
durability and provides a variety of
storage classes for your different data
categories like Glacier for archive data
EBS and EFS offer numerous options for
your ec2 instances based on the level of
performance you require from your
storage
the optimal database solution for a
particular system can vary based on
requirements for consistency
availability partition tolerance and
latency
many systems use different database
solutions for various subsystems and
enable different features to improve
performance selecting the wrong database
solution and features for a system can
lead to much lower performance
efficiency now for database usage you
need to be able to monitor them test
them and also you need to have some
database platform knowledge but Amazon
makes this much easier for you as it
provides a suite of Amazon relational
database Services which are fully
managed relational databases
for example Amazon dynamodb is a fully
managed nosql database that provides
single digit millisecond latency at any
scale Amazon redshift is a managed
petabyte scale data warehouse that
allows you to change the number or type
of nodes as your performance or capacity
needs changed you can also make use of
Aurora or Amis where you install your
own databases from templates
so what is the space-time trade-off well
you can think of space as memory or
storage and you can think of time as the
processing time or the compute time to
complete a task
so when you're architecting Solutions
there's always a series of trade-offs
where it might be better to have more
memory or storage to reduce the
processing time or maybe you're happy to
sacrifice the processing time in order
to reduce your memory or storage
requirements AWS gives you the option to
maximize one or the other
for example you could launch your
systems globally so you're increasing
space and memory so that they're closer
to the end users in order to reduce
latency all the time
but whatever resource you use you need
to have monitoring in place to notify
you of any degradation in performance
and the final pillar of the well
architected framework is cost
optimization this is defined as the
ability to avoid or eliminate unneeded
cost or sub-optimal resources
AWS cloud has a number of ways you can
provide cost optimization
firstly you can transparently attribute
expenditure as the cloud makes it easy
to identify the costs of a system and
attribute I.T costs of individual
business owners so those owners will
have an incentive to optimize their
resources and reduce costs
you can use managed services to remove
the operational burden of maintaining
servers for tasks such as sending email
or managing databases
you can trade Capital expense for
operating expense instead of investing
heavily in data centers and servers
before you know how you're going to use
them with AWS you pay only for the
Computing resources you consume when you
consume them
also AWS allows you to achieve higher
economies of scale because they can buy
way more servers and Hardware than you
could ever imagine so it's much cheaper
to use their Hardware than buy your own
you should also stop spending money on
data center operations AWS is the heavy
lifting of racking stacking and powering
servers so you can focus on your
customers and business projects rather
than on your it infrastructure
cost optimization in the cloud is
composed of four areas
match supply and demand cost effective
resources expenditure awareness and
optimizing over time
matching Supply to demand will deliver
the lowest costs for a system
but you'll need to be able to provide
sufficient extra capacity to cope with
demand and failures
in AWS you can automatically provision
resources to match demand Auto scaling
time-based and event-driven and cue
based approaches allow you to add and
remove resources as needed
monitoring tools and regular
benchmarking can help you achieve much
greater utilization of your resources
using the appropriate instances and
resources for your system is key to cost
savings for example a reporting process
might take five hours to run on a
smaller server but a larger server that
is twice as expensive might better do it
in just one hour
both jobs give you the same outcome but
the smaller server will incur more costs
over time
but to be able to find out these things
you need to have monitoring of
expenditure in place and also regular
benchmarking
AWS helps you with this AWS trusted
advisor goes through your resources and
tells you where you can make savings
you can use on-demand instances so you
only pay for compute capacity by the
hour with no minimum commitments
required or you could use reserved
instances which allow you to reserve
capacity and offer Savings of up to 75
percent off on-demand pricing
and with spot instances you can bid on
unused ec2 capacity at significant
discounts you can also take advantage of
managed AWS services such as RDS or
dynamodb which can also lower your costs
AWS and it's virtually unlimited
on-demand capacity requires a new way of
thinking about expenditures
so you need to have a different way of
budgeting and you need to understand
your business unit's usage
to do this you can use cost allocation
tags to categorize and track your AWS
costs for AWS resources such as ec2 or
Amazon S3
an AWS generates a cost allocation
Report with your usage and costs
aggregated by your tags
so you can set up tags for each of your
business categories and units
this increased visibility of costs makes
it easier to identify resources or
projects that are no longer generating
value and should be decommissioned
billing alerts can be set up to notify
you of predicted overspend and the AWS
simple monthly calculator allows you to
calculate data transfer costs and you
can set up SNS alerts so you can be
notified when certain resources or cost
breaches occur
optimizing over time is an important
strategy to reduce your costs associated
with your Cloud environment AWS is
always releasing new Production Services
as such you need to reassess your
existing setup to see if it's the most
cost effective for example rather than
running a database on an ec2 instance it
might be cheaper to run an Amazon RDS
database
or Lambda might be more cost effective
than ec2
but where did you find out more
information about these new products and
services from Amazon
well the first place to look for any new
architecture guide is the AWS
architecture Center
this is a resource that provides you
with the guidance and application
architecture best practices to build
highly scalable and reliable
applications in the cloud
here you'll find the AWS reference
architectures
the AWS reference architecture data
sheets provide architectural guidance to
help you build an application on the AWS
Cloud you'll find data sheets that
include a description of how each
service is used plus visual
representations of the application
architecture for example web application
hosting or large-scale processing and
huge data sets or even building elastic
web front ends for an e-commerce website
AWS white papers provide a comprehensive
list of technical AWS white papers that
cover all Amazon related topics such as
architecture security and economics
there are new white papers being
released all the time and are written by
some of the most knowledgeable AWS
people around
AWS quick start reference deployments
use AWS cloud formation templates to
rapidly deploy a fully functional
environment for a variety of enterprise
software applications
supplementary deployment guides describe
the architecture and implementation in
detail
in this course we'll deploy things
manually step by step so you can see how
it works but with cloud formation
templates you can do several hours work
with just the click of a button
examples of cloud formation templates
are SharePoint Rd Gateway or even SQL
server on Windows Server failover
clusters
and finally case studies AWS maintains a
large list of case studies and success
stories from their clients you can check
to see how some of the largest and most
successful companies on the planet use
AWS for their business
in the planning and designing section
we'll take a look at the principles
involved in planning and designing Cloud
infrastructure
you'll learn how the AWS well
architected framework is used in
planning and designing
we'll take a look at scaling and all the
different versions
we'll look at the importance of loose
coupling
how you can build redundancy into your
Cloud infrastructure how you can save
money by cost optimization
we'll look at Automation and how it can
make management of an I.T environment
easier and also we'll take a look at
security and how you can secure your
Cloud resources
so let's start with scalability
in the past you would have to estimate
the peak load of your systems and
purchase your Hardware accordingly so
perhaps you were setting up a new
website and you are estimating that
you're going to get a million people per
month hitting your website so you buy
the hardware accordingly but then you
find out that only 50 000 people are
hitting it or 10 million people are
hitting it so your Hardware is
inappropriate for the work
cloud computing provides virtually
unlimited on-demand capacity so you can
scale whenever you need to to do this
your designs need to be able to
seamlessly take advantage of these
resources and there are two ways to
scale vertically and horizontally
vertical scaling means increasing the
specifications of an individual resource
for example increasing the memory or the
number of CPUs on the server
with AWS this is easily achieved with a
restart of your virtual server so that
it resizes to a larger or smaller
instance type the advantage of this
approach is that it's very easy and
requires little for in the short term
however this type of scaling can
eventually hit a limit and sometimes be
cost inefficient especially if these
scale up to some very very large
instances
horizontal scaling means increasing the
number of resources rather than the
specifications of each individual
resource
for example adding additional web
servers to help spread the load of
traffic hitting your application
this is a great way to build
applications that Leverage The
elasticity of cloud computing however
not all architectures can distribute
their workload to multiple resources
make this application is one that needs
no knowledge of previous interaction and
stores no session information
an example of this would be a web server
that provides the same web page to any
end user estate this application can
scale horizontally as any request from
any end user can be serviced by any of
the available compute resources
you can add more resources as required
and then remove them when the capacity
is no longer available
stateless applications do not need to be
aware of their peers all that is
required is a way to distribute the end
user sessions to them
and there are two ways to distribute
load for multiple nodes the first of
these is the push model
a load balancer such as the AWS elastic
load balancer is a popular way to
distribute a workload across multiple
resources
an alternative but less recommended
approach is to implement a DNS round
robin using Amazon Route 53 where DNS
responses return an IP address from a
list of valid hosts in a round robin
fashion this is an easy to set up option
but it can cause issues if DNS records
are cached
the alternative to the push model is the
pool model
tasks that need to be performed can be
stored as messages in a queue and
multiple compute resources can pull and
process the messages in a distributed
fashion
examples of this are big data processing
scenarios where many servers work to
analyze data and return results or media
file conversion processes where multiple
servers convert the files as they arrive
Amazon sqls or Amazon Kinesis are
services that can provide pool model
load balancing
most applications need to maintain some
kind of state information for example an
automated multi-step process will also
need to track previous activity to
decide what its next action should be
you can make components in your
architecture stateless by not storing
anything on the local file system and
instead storing user or session-based
information in a database like dynamodb
or MySQL or on shared storage like
Amazon S3 or EFS
additionally Amazon swf can be used to
centrally store execution history and
make your workload stateless
so the opposite stateless applications
are stateful applications as it's not
possible to turn some layers of your
architecture into stateless components
for example databases which are stateful
by definition or applications that were
designed to run on a single server
providing multiple users a consistent
view of the database or application is
much simpler when your components are
not distributed
Distributing the load is possible low
through session Affinity this means that
all transactions of a session are bound
to a specific compute resource however
this means that existing sessions would
not be able to utilize newly introduced
compute nodes but also if a node is shut
down or becomes unavailable users bound
to that session will be disconnected
for situations when you need to process
large amounts of data is always
beneficial to see if a distributed
processing approach can be used
imagine a task that requires a huge
amount of data processing if it was to
run on a single compute resource it
would max out the resources and take a
long time to complete
you can divide the task into smaller
fragments of work then each of the tasks
can be executed across a larger set of
compute resources
examples of this are the AWS elastic map
reduce service which allows you to run
Hadoop workloads across multiple ec2
instances or Amazon Kinesis allows you
to run multiple shards of data on ec2 or
Lambda resources for real-time
processing of streaming data
the concept of disposable resources is
completely new with cloud computing
as cloud computing completely changes
the mindset of an I.T infrastructure
environment
traditional environments involve
purchasing Hardware up front and
required manual configuration of the
software Network
Etc
whereas with cloud computing all
infrastructure is temporary or
disposable
you can launch new instances when you
need them and get rid of them when you
are done
automate compute resource initiation is
a way to speed up the creation of new
environments AWS has plenty of features
to make new environment creation and
automated and repeatable process and
there's three options available which
we'll look at in the coming slides
bootstrapping golden images and a hybrid
approach
bootstrapping means you can take a newly
launched AWS resource with its default
configuration and then execute automated
scripts to install software or copy data
to bring that resource to a required
state
so you could launch an ec2 instance and
copy data on so that it becomes a web
server
scripts can be parameterized so that
different environments production or
development can be initiated easily with
AWS bootstrapping can be achieved with
your own scripts Chef or puppet Ops work
lifecycle events or cloud formation
golden images mean you take snapshots of
your ec2 instances or RDS instances or
even EBS volumes and they can be used to
launch new instances
these ec2 instances can also be
customized and saved as Amis Amazon
machine images and then you can launch
as many instances as you want from this
template
if you have an on-premise virtualized
environment you can also use AWS VM
import export to create your own AWS
Amis
the hybrid approach makes use of both
bootstrapping and golden images
you can launch new instances from a
golden image but then bootstrap to
configure other actions
basically with AWS you are only
restricted by your own imagination or
coding skills
AWS assets are programmable so you can
apply all these techniques and
principles that we've just talked about
to entire environments and not just
individual resources so you can
completely rethink the way you work your
it infrastructure
automation is a big part of planning and
designing Cloud infrastructure AWS
allows you to reduce the level of manual
interaction in your environment using
automation you can react to a variety of
events without having to do anything
let's take a look at some examples AWS
elastic Beanstalk developers can upload
their code and the Beanstalk service
automatically handles Resource
provisioning Auto scaling load balancing
and monitoring ec2 Auto Recovery in some
situations you can use auto recovery to
automatically recover an ec2 instance if
it becomes impaired the recovered
instance is identical to the original
instance name IP address metadata
Etc
Auto scaling you can automatically add
resources to cope with demand spikes and
decrease again during quiet times
cloudwatch alarms and events cloudwatch
can send SNS notifications to alert when
a particular event occurs for example
CPU usage is too high the notifications
can be used to perform follow-up actions
like run a Lambda function
Ops works like cycle events you can
continually update your instances
configuration to adapt to environment
changes for example if a new database
instance is added to your environment
then an event can automatically trigger
a chef reppi that points existing
applications to the new server
loose coupling is an important concept
Mission should be designed so that
they're broken into smaller Loosely
coupled components
the desired outcome is that a failure in
one component should not cause other
components to fail
this is achieved for a variety of
measures firstly well-defined interfaces
by ensuring that all components only
interact with each other through
specific technology agnostic interfaces
for example restful apis will result in
being able to modify resources without
affecting other components Amazon API
Gateway is a fully managed service for
apis which you can do this with
loose coupling needs services to be able
to interact with each other without
having any prior knowledge of their
existence for example imagine two
servers communicating on hard-coded IP
addresses then the addition or
modification of resources will result in
manual interaction to update the
configuration however Loosely coupled
infrastructure is an essential
ingredient in making the most of cloud
computing elasticity the easiest way to
achieve this is to use an elastic load
balancer service to provide a stable
endpoint for all your services this way
they don't have to worry about each
other the elastic load balancer does
that for them
rather than use synchronous integration
where server a completes an action and
passes it to server B which then passes
it to server C asynchronous integration
involves the use of an intermediate
storage area like an sqsq
this approach means that when a server a
completes its action it sends a
notification to sqs this way the compute
resources are decoupled and not directly
linked to each other this means you can
easily add or remove resources without
having to update the configuration of
the existing compute resources
designing applications to fail
gracefully allows other resources to
continue a service without causing a
complete outage
examples of this would be if your
primary website fails the Route 53 DNS
failover feature points your users to a
backup site hosted on a static website
on Amazon S3
the idea is to continue to provide a
service even if it isn't the full
experience
traditional it infrastructure involves
developing managing and operating
applications with a wide variety of
underlying technology components
AWS isn't just about ec2 instances there
are Suite of products to do everything
mentioned and a lot more besides to help
you lower your it costs and allow you to
move faster as an organization
for example manage services AWS provides
many services that are fully managed
databases machine learning analytics
search email
Etc this lessens the burden on your it
infrastructure Department
RDS databases mean you don't have to
worry about backups for tolerance for
your simple databases
S3 allows you to store as much data as
you need without having to concern
yourself with capacity management
serverless architectures you cannot
reduce operational complexity of your
applications through the use of
serverless architectures tools such as
the Internet of Things Lambda S3
Beanstalk all allow you to run your
applications without paying for any
server infrastructure
in a traditional it environment the type
of database in use is typically
restricted by constraints on licensing
support capabilities Hardware
availability and things like that
AWS provides you with multiple managed
database server options that remove
these constraints meaning that you can
choose exactly the right database for
your needs rather than making use of
what is available to you AWS offers
fully managed easy scalable services for
relational databases nosql data
warehouses and search functions that
provide fully managed easily scalable
services
that can create synchronously replicated
standby instances in different
availability zones
and they'll also automatically fail over
without the need for manual intervention
we'll cover databases in a whole lesson
later on in this course
now we'll take a look at redundancy
introducing redundancy will ensure your
systems are highly available so they can
withstand the failure of individual or
multiple components for example hard
disks servers or network links
introducing redundancy will remove
single points of failure from your
systems this is achieved for having
multiple resources for the same tasks in
either standby or active mode
standby redundancy is often used for
stateful components like databases the
standby resource becomes the primary
resource by failing over to it the
standby resource can be brought online
only when required to save cost or it
can already be running to speed up the
failover process and minimize the outage
active redundancy is where multiple
redundant compute resources share
requests and are able to absorb the loss
of one or more of the instances failing
examples of this would be multiple web
servers sitting behind a load balancer
automatic failure detection allows you
to react to an outage automatically
without the need for manual intervention
services like elastic load balancer and
Route 53 let you configure health checks
so you can automatically Route traffic
to healthy resources
for example setting up a health check to
test for a HTTP 200 response which means
okay would allow AWS to know if a node
is healthy or not services such as Ops
Works elastic load balancer and ec2 auto
recovery will let you replace unhealthy
Resources with brand new healthy ones
durable data storage is an important
part of planning your Cloud resources
replicating your data to other sites or
resources will protect its availability
and integrity
there's three ways you can replicate
your data synchronously asynchronously
or Quorum based
synchronous replication ensures that
your data has been durably stored on
both the primary and replication
locations any write operation will only
be acknowledged as complete when this
has taken place
this should only be used on low latency
network connections were absolutely
necessary to try to ensure maximum
performance
asynchronous replication decouples the
primary node from the replica so that a
right operation doesn't have to wait for
any acknowledgment this can introduce a
replication lag but does not impact
performance and from personal experience
I've seen the difference between
synchronous and asynchronous replication
changing performance as much as a
hundred percent
Quorum based replication is a mix of
both synchronous and asynchronous
replication replication to multiple
nodes is controlled by defining the
minimum number of nodes that must
participate in a successful write
operation so if you have a cluster of
four replicas you can say I need
confirmation from at least two of them
before the acknowledgment is sent back
to the end user
a traditional data center failover
scenario involves failing over all your
resources to a secondary distant Data
Center
because of the long distances between
the two data centers synchronous
replication is often impractical slow
and usually involves data loss and isn't
tested very often however it does
protect against low probability
scenarios such as a natural disaster so
if your main data center is in New York
and your secondary was in say Washington
if New York had a massive outage you
could fail over to Washington but it's
going to be slow and practical and take
some time
a more likely scenario is a shorter
interruption in your data center which
isn't predicted to be too long in this
situation your choice is to sit it out
and wait or initiate the complex
failover procedure
AWS data centers are configured to
provide multiple availability zones in a
region with low latency network
connectivity this means you can
replicate your data across data centers
in a synchronous Manner and as a result
failover becomes much simpler also this
concept is baked into many AWS services
such as RDS and S3
active redundancy is great for Disaster
Recovery situations or balancing traffic
but what if all the requests to your
resources were harmful
for example if a particular request
caused a bug and it resulted in multiple
instances crashing
a practice called Shuffle sharding means
only sending some of the requests to
some of your resources this way of one
Shard of resources is infected or down
the other Shard of resources will be up
and running
and the example shown here if request 2
is bad then at least one set of
resources is still protected
AWS economies of scale offer
organizations huge opportunities to make
cost savings by making further use of
AWS capabilities there's even more scope
to create cost optimized cloud
architectures
right sizing is one such approach in
stark contrast to a traditional I.T
infrastructure cloud computing means you
can select the most cost-effective
resource and configuration to fit your
requirements
ec2 RDS redshift and elasticsearch give
you a wide variety of instant types to
choose from through benchmarking you can
determine the optimal configuration for
your workload in some cases it might be
optimal to select many of the cheapest
instant type available in others it
might be more beneficial to select fewer
instances but of a larger instance type
AWS storage offers the same level of
right sizing opportunities Amazon S3
offers many different storage classes to
suit your needs EBS also comes in a
variety of different volume types all of
which will be covered in later lessons
AWS offers many elasticity options to
help you save money you can Auto scale
your ec2 workloads horizontally or
vertically you can turn off production
workloads when you don't need them and
where possible you can use Lambda
compute workloads because you never have
to pay for idle resources
ec2 on-demand instance pricing means you
only pay for what you use with no
long-term commitments however there are
two more ways to pay for ec2 instances
one of these is reserve capacity by
committing to a defined period of
between 12 to 36 months you can receive
significantly discounted hourly rates
compared to on-demand pricing
AWS trusted advisor or AWS ec2 usage
reports identify the resources that
would benefit from Reserve capacity
there is technically no difference
between on-demand and reserved instances
the only difference is the way you pay
for it this concept also exists for
redshift RDS dynamodb and cloudfront
ec2 spot instances are ideal for
workloads that have flexible start and
end times as you are allowed to bid on
spare ec2 Computing capacity and spot
instances are often available at
significant discounts compared to
on-demand pricing
your ec2 spot instance is launched when
your bid exceeds the current spot market
price and it will continue to run until
evu terminate it or the stock market
price exceeds your bid when the latter
happens your instance is terminated and
you will not be charged for the partial
hour that it was running
so there's three strategies to use for
spot instances there's a one we've just
discussed which is bidding you could bid
much higher than the spot market price
for as long as the instant runs this way
even if the market price spikes
occasionally you'll still make a saving
overall
the mix strategy AWS recommends
designing applications that use a
mixture of reserved on-demand and spot
instances this way you can combine
predictive capacity with opportunistic
access to cheaper additional compute
resources
and then finally there's spot blocks AWS
allows you to bid for fixed duration
spot instances these have a different
hourly pricing but allow you to define a
duration requirement if your bid is
accepted your instance will run until
you terminate it or the duration ends
now let's take a look at caching caching
data means storing previously calculated
data for future use so you don't have to
recalculate it again
there's two approaches application
caching you can design applications to
store and retrieve information from Fast
managed in-memory caches
this way an application can look for
results in the cache first and if the
data isn't there it can then calculate
it or retrieve the data and store it in
the cache for subsequent requests
this approach approves user response
times and reduces load on your resources
an example of this is Amazon elastic
cache which is a service that provides
an in-memory cache in the cloud suitable
for use with web services
Edge caching uses Amazon cloudfront so
you can cache static content such as
images or video and dynamic content such
as live video around the world using
Edge locations this way users can be
served to content that is closest to
them and resulting in low latency
response times
the principle applies to both download
and upload with Edge caching
and finally security it's important to
have security built into your plans and
designs for AWS infrastructure
AWS allows you to take most of the
security tools and techniques that you
already know and improve them by
formalizing security control design into
your AWS platform this simplifies the
system for administrators and Auditors
alike now we're not going to cover too
much here because this is all covered in
the I am lesson but there's things like
defense in depth which allows you to add
multiple layers of protection to your
resources reduced privileged access to
only give people the access they require
security is a code lets you capture
security requirements in one script that
you can use to deploy new environments
and real-time auditing AWS has multiple
services to ensure you can test in order
to your environment in real time
in this section we're going to take a
look at monitoring and logging and an
overview of the tools available to
enable AWS monitoring and logging
in this section you're going to learn
about Amazon Cloud watch
Amazon cloudtrail
and Amazon config and how they can be
used to Monitor and log your Cloud
infrastructure
so Amazon cloudwatch monitors your
Amazon web services resources and
applications in real time in a
particular region it collects and tracks
metrics
it uses alarms to send notifications
and you can automatically make changes
to monitored resources based on defined
rules
Amazon cloudwatch events deliver a
stream of system events which alerts
about changes to AWS resources
alerts can be sent to services such as
AWS Lambda Amazon SNS Amazon sqs and
Amazon Kinesis streams
for example you can monitor the CP usage
and disk reads and rights of your ec2
compute Cloud instances then you can use
this data to determine whether you
should launch additional instances to
handle the increased load you can also
use this data to stop underused
instances and save money
you can also use cloudwatch events to
schedule events such as a snapshot
creation or an instance reboot
in addition to the monitoring with the
built-in metrics that come with AWS you
can monitor your own custom metrics to
enable system-wide visibility into
resource utilization application
performance operational health and even
cost
you can use Amazon cloudwatch logs to
monitor store and access your
application or system logs from Amazon
ec2 instances
AWS cloudtrail or other sources you can
then retrieve the associated log data
from cloudwatch logs
so cloudwatch has a number of limits for
metrics events and logs now I'm not
going to go into all of them because
there's quite a lot but some of the key
ones are you get 10 Cloud watch metrics
per customer per month for free
as well as 10 alarms per customer per
month for free 1 million API requests
1000 Amazon SNS email notifications
there's also no limit on the number of
custom metrics you can create you can
create up to 5000 alarms per AWS account
and your metric data is kept for two
weeks
AWS cloudtrail is a web service that
records AWS API calls for your account
the recorded information includes the
identity of the API caller the time of
the API call
The Source IP address of the API caller
and other request parameters
cloudtrail also stores and delivers log
files to you and it provides a history
of AWS API calls for your account
cloudtrail provides increased visibility
so you can answer questions such as what
actions did a given user take over a
given time period
or for a given resource which user has
taken actions on it over a given period
of time
what is the source IP address of a given
activity and which activities fail due
to inadequate permissions
so cloudtrail will help you when
Auditors come asking questions or if
you're trying to solve a problem
it also offers durable and inexpensive
log file storage cloudtrail uses Amazon
S3 for log file storage so log files are
stored durably and inexpensively you can
also use Amazon S3 lifecycle
configuration rules to further reduce
storage costs for example you can Define
rules to automatically delete old log
files or archive them to Amazon Glacier
for additional savings
cloudtrail is a fully managed service so
no installation required you turn on
cloudtrail for your account in the AWS
Management console and it will start
creating Cloud trial log files in an
Amazon S3 bucket that you specify
you get notifications for log file
delivery cloudtrail uses the Amazon
simple notification service so that you
can be notified when a new log file is
delivered or when a specific event has
occurred
also companies such as Splunk Sumo logic
or logli offer Integrated Solutions to
analyze your cloudtrail log files
cloudtrail can also be configured to
aggregate log files across multiple
accounts and regions so that log files
are delivered to a single S3 bucket
bowtrow automatically encrypts all log
files delivered to the specified S3
bucket by using the Amazon S3
server-side encryption SSE you can also
further secure your log files using the
AWS Key Management Service
you can also validate the Integrity of
your cloudtrail log files so if you're
concerned that an admin user is trying
to cover up his tracks by deleting
entries from the log files you can see
whether the log files are unchanged
modified or have been deleted since
cloudtrail delivered them
AWS config is a fully managed service
that provides you with an AWS resource
inventory configuration history and
configuration change notifications
you can automatically check the
configuration of AWS resources
by checking AWS config
using config roles in itadministrator
can quickly determine when and how a
resource went out of compliance for
example you can ensure that your EBS
volumes are encrypted
your ec2 instances are properly tagged
or elastic IP addresses are attached to
instances and this is all enabled with a
few Mouse Clicks in the AWS console
AWS config enables configuration
visibility you can view all the
configuration attributes of your AWS
resources in real time and SNS will
notify you of any updated configuration
or specific changes from the previous
state
you can continuously assess the overall
compliance of your AWS resource
configurations based on your
organization's policies and guidelines
AWS config rules give you a visual
dashboard with lists charts and graphs
to help you quickly spot non-compliant
resources and take appropriate action
AWS config is also compatible with
third-party applications such as Splunk
in this section we're going to take a
look at hybrid it architectures and have
an overview of the tools and
functionality available to run hybrid
cloud architectures
AWS offers a wise and deep set of
functionality that allows you to
integrate your existing network security
data lifecycle management and Resource
Management capabilities with the cloud
AWS allows you to extend your existing
on-premise network configuration onto
your AWS virtual private clouds so that
your AWS resources will operate as if
they're part of your existing Network
you can do this using Amazon virtual
private Cloud VPC which is a
logistically isolated Network in the
Amazon Cloud that gives you complete
control over your virtual networking
environment you can choose your own IP
range subnets route tables Network
Etc then there's Direct Connect
rather than use internet-based
connections between your on-site
resources and the Amazon Cloud Direct
Connect lets you establish a dedicated
network connection between the two to
provide lower costs and a higher level
of service
you can use AWS to reliably and cost
effectively backup and secure your data
AWS allows you to replicate your data
across geographical regions manage the
life cycle of the data or even
synchronously replicate your data to a
local AWS Data Center
AWS storage Gateway is a VM that you
install in your data center and
configure to be associated with your AWS
account
then you can either use Gateway cached
which lets you use Amazon S3 storage as
your primary data storage while
retaining frequently accessed data
locally in your storage Gateway or
Gateway stored
Gateway stored volumes let you store
your primary data locally and
asynchronously back that up to AWS
Amazon simple storage service S3 offers
a practically unlimited storage that's
available for backing up And archiving
your critical data and Amazon Glacier is
extremely low-cost storage for
infrequently accessed data for which
recovery intervals of several hours are
suitable
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to secure
the control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
and there's three flavors of active
directory as Microsoft ad which is a
fully blown managed Microsoft active
directory service that supports up to 50
000 users the simple ad which is a
standalone manage directory that's
available in two sizes small and large
small list for up to 500 users and large
supports up to 5000 users
all this ad connector and this is a
directory Gateway that allows you to
proxy directory requests to your
on-premise Microsoft active directory
and this also comes in two sizes small
and large a small ad connector is
designed for small organizations of up
to 500 users and a large ad connector is
designed for larger organizations for up
to 5000 users
you can also use integrated resource and
deployment management Tools in your
hybrid environment
AWS provides monitoring and management
tools with robust apis so you can easily
integrate your AWS resources of on-site
tools and most major software vendors
already support AWS like Microsoft
VMware and BMC
tools such as AWS Ops Works allow you to
deploy and operate applications in the
AWS cloud or in your own Data Center
you can use templates or build your own
tasks to specify an application's
architecture
AWS code deploy automates code
deployments to instances in your
existing data center or in the AWS cloud
AWS code deploy simplifies the code
deployment process to your instances
this is the practice assignment for
Designing hybrid storage where you'll
configure a basic plan to resolve an
on-premise storage problem
you've been hired by a medium-sized Law
Firm who have an aging storage solution
that they would like to replace but they
do not want to purchase any new hardware
they store several terabytes of data
which is comprised of documents and
images a lot of the data is Legacy data
which is rarely accessed and can be
archived however the most recent files
need to be available instantly
you have been tasked with providing a
very basic high-level plan of a hybrid
storage solution using the client's
existing Data Center and AWS
detail the products and services you
would use and sketch out a basic plan of
the infrastructure
so let's review the key takeaways from
this lesson
in this topic we will learn about Amazon
glacier
Amazon Glacier is an economical storage
solution to store data that would remain
forever but rarely accessed
it is an ideal choice for data backup
And archiving provides data security of
the highest level and offers flexibility
in both storing and retrieving data
AWS bills you for only the used data or
storage and current lease price for
storing data in Amazon Glacier is zero
zero seven cents per gigabyte per month
maintaining historical data can be a
grief as it is added to the
administrative liability of managing the
scaling storage
Amazon Glacier eases this hardship by
providing features such as capacity
planning Hardware provisioning detecting
and repairing Hardware failure data
replication and Hardware migrations
when you preserve data in Amazon Glacier
it is stored as archives
this enables a user to store a single
file or a combination of several files
archives are arranged in vaults which
can be accessed using the AWS IAM
service
Amazon Glacier stores data in transit
via SSL and uses the 256-bit advanced
encryption system
AWS enables organizations to store data
in a location that is convenient for
their businesses and organizations tend
to utilize Amazon glacier to support the
following use cases
archiving off-site Enterprise
information
backing up media assets
storing research and scientific data
preserving Digital Data
and replacing magnetic tapes
the Amazon Glacier archives offers an
average annual resilience of
99.99999
999 percent
the archiving service maintains
resilience by continuously utilizing
several facilities and devices within
each facility to store the data
the task continues till the service
returns success on the uploading
archives
any individual object archived into
Amazon Glacier such as a document video
or any other type of file is referred to
as an archive
each archive has a unique ID assigned to
it by AWS
the archives are stored in vaults a
vault is addressed by a unique name
assigned to it by its creator a given
AWS account May create up to 1000 volts
in Amazon glacier
you can refer to an archive in Amazon
Glacier via a URL that points to the
glacier service and consists of the
following three components
the account ID of your AWS master
account
the name of the Vault
the ID of the individual archive
data stored in Amazon Glacier by Amazon
S3 cannot be retrieved using the Amazon
Glacier API this is the data that Amazon
S3 manages on your behalf and it does
not show up as a vault or archive
Amazon S3 archived files can be restored
using the S3 Management console
interface or using the Amazon S3 API
files added directly to Amazon Glacier
using the Amazon Glacier API can be
retrieved using the glacier API
retrieval requests can be any one of the
following
direct retrieval of a single archive
object by archive ID
filter by archive creation date
ranged archive retrieval retrieve only a
specific range of bytes from a specific
archive
you can Poll for job completion using
the describe job API function Amazon
Glacier completion notifications can
also be sent using Amazon's simple
notification service or SNS
once an Amazon Glacier job has finished
executing the user can request a
download of their thought data
in this demonstration you'll learn how
to configure Amazon S3 buckets for
archiving in Amazon Glacier which is a
lost Cost Storage service
first you need to go to S3 under the
storage and content delivery section
select your required bucket for this
demonstration we'll select the test 11
October bucket
click the life cycle option
click add rule
select the whole bucket option
click the configuration rule button
select the archive to the glacier
storage class option
State the number of days after the
completion of which you want the data in
your S3 bucket to move to the glacier
for this demonstration we'll provide 30
days
as you can see in the example provided
the objects in S3 bucket will move to
Glacier after the completion of 30 days
acknowledge the message that you are
aware that setting up this particular
rule will likely increase your storage
costs
click review
provide a rule name
you can review your rule on this page
click create and activate rule
as you can see the rule has already been
added
you have learned how to configure Amazon
S3 buckets for archiving in Amazon
glacier
Amazon cloudwatch is a monitoring
service for AWS Cloud resources and the
applications you run on AWS
cloudwatch enables monitoring for ec2
and other Amazon cloud services so you
can get alerts when things go wrong
you can use Amazon cloudwatch to collect
and track metrics so you can get
system-wide visibility into resource
utilization application performance and
overall operational health and you can
use these insights to react and keep
your applications running smoothly
cloudwatch offers two types of
monitoring there's basic monitoring
which is included free of charge and
polls every five minutes and gives you
10 metrics 5 gigabytes of data ingestion
and 5 gigabytes of data storage
then there's detailed monitoring and
this costs more as a price per instance
per month but it pulls every minute so
if you want more detailed monitoring
then you can pay for it
AWS cloudwatch allows you to record
metrics for services such as EBS ec2
elastic load balancer and Amazon S3
and using these metrics you can add them
to dashboards typically visual or
text-based notifications of what's going
on and this is a diagram of a dashboard
in Amazon cloudwatch
metrics are at the hypervisor level so
you can get things like CPU disk network
but you cannot see memory usage
metrics appear as you add more resources
to your AWS account
you can create events based on your
cloudwatch monitoring for example
triggering Lambda functions
so perhaps if an EBS volume fills up you
could trigger an event so that data is
removed and archived from the volume or
a new volume is created so there's many
things you can do
you can install cloudwatch agents on ec2
instances and this will send monitoring
data about the instance to cloudwatch
so you can monitor things like HTTP
response codes in Apache or you can
count exceptions in application logs
you can set alarms to warn based on
resource usage for example if CPU
utilization is too high then it could
send a notification
it can also Auto scale so if your CPU is
maxed out you can get another instance
launched to take care of some of the
load
or you can send card watch monitoring
alarms to ec2 actions to say recover an
instance or reboot an instance if
something happens
you can also use alarms to shut down
instances it isn't just used for
starting them up so if you have idle
instances you can get cloudwatch to shut
them down for you
in this demonstration we're going to
take a look at AWS cloudwatch and how we
can use it to shut down idle instances
so before I started this demonstration I
launched an Amazon Linux instance and
let it run for 10 minutes just so we'd
have some data so now I'm going to go to
management tools and click on cloudwatch
and this brings us to the cloudwatch
dashboard
so what we're going to do is we're going
to create a new dashboard
and we'll click on create dashboard so
that we can have a look at the
monitoring statistics for our new
instance so let's just give it a name
and we'll call it
just simply learn just for a change
okay so it says create dashboard
and now we get the option do we want to
put text based or metric graphs widgets
to our dashboard and we're going to
select metric graphs so we'll click on
configure
and then we're presented with all the
cloudwatch metrics that I have available
in my AWS account so obviously I've just
launched an ec2 instance so I'm going to
click on ec2 metrics
and my new server is called simply learn
cloudwatch demo so I want to add
CPU
utilization so let's click on that so
let me create this widget
and then that now appears on our
dashboard so we can add more so we click
on add widget we'll do another metric
graph
we'll go another per instance metric
and let's choose
Network in and network out and we'll
create the widget
now as you can see there's not a lot
happening here because I just launched
this instance and I haven't really done
anything on it but this is how you would
create a dashboard and it's quite useful
and very easy to see so you can create a
dashboard for your instance types
so what we're also going to do is set up
an alarm so let's go to alarms
so now we want to create an alarm so we
click on create alarm
and we get to choose the metric we want
to base the alarm on now obviously we
have an ec2 instance so we're going to
choose that option and we're going to do
it based on CPU utilization of our
simply learn cloudwatch demo instance so
we'll click on that and we'll click next
so now we get to give our alarm a name
so I'm just going to call it simply
learn underscore alarm just for the
purposes of this demo we can give it a
description and now we get to set the
alarm threshold so we're saying whenever
CPU utilization is and we're going to
say less than or equal to 50 percent
for one consecutive period and remember
a period is five minutes in basic
monitoring we want this alarm to fire
so why would you do this well imagine if
you had a really high powered server
that you're using every night to do some
high the Intensive compute computations
so you might know that it runs for a
couple of hours and it's charging you
quite a lot of money per hour to run but
when the CPU utilization drops below say
50 or 10 you know the job is complete so
then you can get an alarm to Fire and an
action to happen
so you could set a notification so we
could together to send an email when
this alarm fires we're not going to do
that in this demonstration but we can do
an ec2 action
so whenever this state is in alarm I
want it to stop this instance
so this instance is going to be stopped
whenever the CPU utilization is less
than 50 percent for five minutes
and it's just telling us that there so
now if I click on create alarm
and here is our alarm so it's just uh
so it's telling us we're in alarm
already because this instance has been
up and running it has monitoring data
for the last five minutes and it's
saying the alarm has fired so let me
click on it and we'll see why so State
change to alarm reason threshold crossed
one data point zero zero three four was
less than equal to the threshold of 50
percent
and it says when it's in alarm it's
going to stop the instance
so if we go to the ec2 dashboard
and here we can see that the simply
learn cloudwatch demo instance has been
stopped and it's in a status of alarm so
it's telling us that our alarm fired and
it took it straight down obviously
that's a pretty dramatic example you
wouldn't expect anything like that to
happen in a real world but it's a good
way of seeing how alarms work
in this section you'll learn what
elastic load balancing is
elastic load balancing automatically
distributes incoming application traffic
across multiple Amazon ec2 instances
it enables you to achieve increased
levels of fault tolerance for your
applications by seamlessly providing the
required amount of load balancing
capacity needed to distribute the
application traffic
you can distribute incoming traffic
across your ec2 instances in a single
availability zone or multiple
availability zones
elastic load balancing automatically
scales its request handling capacity in
response to incoming traffic
elastic load balancing can also detect
the health of your Amazon ec2 instances
when it detects an unhealthy instance
it'll spread the load across the
remaining instances and no longer Route
traffic to the unhealthy instance
when you're using VPC you can create and
manage security groups associated with
elastic load balancing to provide
additional networking and Security
Options
you can also create a load balancer
without a public IP address so it serves
as an internal IE non-internet-facing
load balancer
elastic load balancing also supports the
ability to stick user sessions to
specific ec2 instances using cookies and
if you remember from lesson two when we
talked about session Affinity this is a
version of that
in this demonstration we're going to
create a load balancer for our simply
learn web servers
so to do that we'll go to the compute
ec2 section
and I want to show you that before we
can configure load balancing there's a
couple of things we need to do so if we
go to load balances on the left hand
side
and then click on create load balancer
so we want to create a load balancer
inside simply learn VPC now I want to
show you this warning that comes up says
please select at least two subnets in
different availability zones to provide
High availability for your load balancer
and you could think well that's great we
have two subnets we have a public subnet
and a private subnet and they're both in
different availability zones so if we
add them both
we get another warning says this is an
internet-facing elastic load balancer
but there is no internet gateway
attached to the sudden that you've just
selected so obviously our private
Gateway private subnet only has a Nat
gateway to provide its internet access
so it's not a public internet-facing
subnet so what that means is we need to
create a new public subnet and a
different availability zone for our load
balancer
so we already have this one public in Us
East 1B we're going to create a public
Subnet in U.S east 1C as well so to do
that let's go back
to the networking VPC section
and this is going to be a quick review
of the VPC lesson so we'll go to subnets
we'll sort by VPC so here are our two
existing subnets
so we'll create a new subnet and we're
going to call it and we just paste what
I copied we're going to call it 10.0.3.0
Us East 1C simply learn Pub
and we're going to put it in our simply
learned VPC
and we want to put it in the US East 1C
availability zone so it's different to
our existing public subnet
and we're going to put it in
that cidr block
so let's click create
so there we are now we have
three subnets in our simply learn BBC
two of them are public now if you
remember when you create a new subnet it
will get attached to your default root
table by default so that's no good
because our default route table doesn't
have internet access so we need to go to
root tables
we click on our main default root table
and have a look at subnet associations
we can see that our new subnet is in
here so we need to move it
into our custom root table
so it will be the same as our existing
public subnet so if we click on edit
we just tick the box next to our new
public subnet
and click save
so now our new public subnet is in
this root table so it has internet
access so now we're good to go so let's
go back to the ec2 section and we want
to launch a new web server into this new
public subnet
so we'll go to but we'll launch it from
our Ami because we've already configured
our web servers as an Ami so we'll click
launch
we'll choose TT Nano or configure our
instance details
so we're going to place it in the simply
land VPC
we're going to place it in our new
public Subnet in the US East 1C
availability Zone
we're going to enable the public IP
we're going to give it the administrator
I am roll
and we're going to protect against
accidental termination
the storage is fine
we're going to tag this one as
simply learn web server 3.
we'll configure the security group so
we'll put it in an existing Group which
will be
the
Web Service Group
and we'll launch it
using our existing simply learn key pair
which I have possession of
okay so that's launching
so we can
see our simply learn web servers
so there we are they've got three of
them we've got one two which are already
up and running and number three which is
coming online
so now we're ready to create our load
balancing so let's go to load balances
again
and create load balancer
you want to put it in rvpc and we'll
give it a name so this is the kind of
internal name we can tag it differently
later on
because you can't put any underscores or
hyphens in this section so we'll call it
simply learn lb simply learn VPC if we
wanted to create an internal load
balancer so it was balancing traffic
that was just internal to our AWS VPC we
could tick this box here's the listener
configuration so what we can do here is
we're saying that any HTTP tracker that
comes in on Port 80
to the load balance that is being pushed
to Port HTTP 80 on our instances so what
we could do is you could change this you
could have a different port but the load
balancer and it Maps it to your web
service you could protect your web
servers but we're just going to leave it
as it is now we want to add the subnet
so we're going to add
both of our public subnets and that
warning's now gone so now we're good we
have two subnets and two availability
zones
next we assign security groups so we
could just use our existing web server
Security Group because that has HTTP
Port 80 open it also has
httpps open and SSH so it's probably
good practice to create a new Security
Group just for our load balancer so
we'll call it simply then
load balancer
SG
and we're simply going to have
HTTP Port 80 open from anywhere and
nothing else
we should probably just give it a quick
description so we know what it is
Security Group for load balancing
okay so let's click next configure
security settings and it's just telling
us that our load balance security isn't
using a secure listener because we're
not using https or SSL which is fine for
this demonstration next we configure the
health check
so this is to check that the load
balancer is going to perform to see if
our instances are healthy so it's using
HTTP Port 80 and it's going to look for
existence of a file so we're going to
tell that to look for
our healthcheck.html file and if that
doesn't exist then the instance isn't
healthy
now we can configure some more details
so the response timeout is five seconds
we'll leave that the interval between
checks the default value is 30. so let's
bring that down because we want to want
things to happen quicker so that's 10
seconds unhealthy threshold is set to
two so if it has two checks or it
doesn't find the file it's going to
determine that the instance is unhealthy
and the health of threshold is 10. we're
also going to bring that down we'll
bring that down to three so after three
consecutive healthy checks it will say
the instance is healthy
okay so now we can add our ec2 instances
so we'll add our three web servers
we've enabled cross Zone load balancing
so it can push traffic across both
availability zones and the connection
draining setting will just leave as
default that's when it detects an
unhealthy instance that's how long
before it will start redirecting traffic
elsewhere
so next we'll click on ADD tags so we
can give
this a more meaningful name
simply learn
underscore load balancer and review and
create okay so we can review everything
we did
well if you should be okay so let's
click create
and our load balancer is now being
created
so let's have a look so here it is
if we click on instances you can see the
instances and it's saying they're all
out of service at the moment so that's
because it's just configuring itself
and it will take a while to come up so
I'm going to pause the demonstration and
we'll come back when everything's up and
running
okay so the load balancer has configured
and you can see that two of our three
web servers are in service but our third
web server is out of service
so what do you think this is
well I'll tell you why it's because our
new web server simply learn web server 3
has just come up and we haven't done
anything to it other than just launch
the Ami so let's go and check out what's
going on on the instance
so I've opened sessions to our three web
servers here and this is the new one and
it's because the Apache service
isn't started by default so we need to
start the Apache service
okay so we get that weird error but the
it tells me it's okay so if we now go
back to the um the load balancer
and we wait a few seconds we should see
simply then web server number three come
online
so now we're online so it was as simple
as just starting the service
so that shows how load balancing works
now if something was out of service we
could remove it from the load balancer
or we could go investigate the problem
or we could use Auto scaling to manage
it automatically on our behalf and
that's what we're going to look at in
the next demonstration welcome to the
overview of Amazon EBS section
Amazon elastic Block store or EBS for
sure provides persistent Block Level
storage volumes for use with Amazon ec2
instances in the AWS cloud
each Amazon EBS volume is automatically
replicated within its availability Zone
to protect you from component failure to
offer High availability and durability
so basically EBS is the disk volumes
that you attach to your ec2 instance
you can create an EBS volume in the
Management console when you create a new
EBS volume you need to define the region
and the availability zone for it
you can also create new volumes through
Amazon API or command line tools
the screen displays an example of
creating a volume with the AWS CLI
create volume command
note that the size of the volume
availability Zone and the volume type is
defined in the command
the region is assumed from the default
settings of AWS CLI
you can also copy a volume to a
different region using snapshots
one important point to be noted is that
a first access penalty is attached to
both Amazon EBS volumes and instant
store volumes
this penalty can be a 5 to 50 percent
reduction in performance on the first
read or write
it is recommended that you warn the
volume in order to eliminate this
penalty from affecting production
workloads though this is not mandatory
and in many cases may not be noticeable
Amazon EBS volumes can be stripped RAID
0 for performance or configured
redundant storage raid 1 by using raid
arrays you can create raid arrays by
using the standard operating system
tools
you already know you can backup the data
on your EBS volumes to Amazon S3 by
taking point in time snapshots
initial snapshot of the root or boot
volume must be consistent so it is
highly recommended to stop the instance
before making the first snapshot or
freeze the file system
snapshot command runs and returns
asynchronously and remains in a pending
State until it is finished
you can watch the status of the snapshot
in the AWS Management console or through
the API using AWS CLI tools
you can scale an Amazon EBS volume using
snapshots if you are performing the
process on the root volume be sure to
stop the instance before detaching the
original volume
detach the original volume
if it is the root volume then you will
have to perform this action from the
Management console or from another
Amazon ec2 instance that will continue
running
create a snapshot of the detached volume
use the AWS ec2 create volume command to
create a new volume
you can specify in the command that the
volume will be created from the snapshot
you previously took of the smaller or
larger volume
attach a new volume to the stopped ec2
instance and if you expand the root
volume restart the instance
keep in mind the following points to
help you better manage snapshots
use automation to manage a large number
of snapshots across a large Fleet
use CLI tools to automate creating and
managing snapshots through Linux shell
and windows batch
create backup commands using CLI and
schedule them through cron for Linux or
task scheduler for Windows
use services such as AWS data pipeline
or configuration Management Systems like
Chef to set up scheduling tasks remotely
on this screen CLI commands for three
common snapshot management operations
are displayed
taking the snapshot moving the snapshot
across regions and finding the snapshot
ID and restore it to a new volume
the AWS CLI commands are
self-explanatory note that it is always
useful to pass a description for the
newly created snapshot
although snapshots are stored in Amazon
S3 they are not directly accessible
using the Amazon S3 utilities
you need to use the Amazon EBS tools to
restore and manage snapshots
[Music]
demonstration we're going to take a
quick look at EBS snapshots
so we'll start by going to compute ec2
and we'll go to the volume section
now here are the two volumes the root
volumes that are attached to our simply
learn web servers so I've tagged them as
simply learn web server one root and web
server 2 root so we can see what they
are
Now to create a snapshot is very easy
we highlight the volume you want to take
a snap of
click on actions
and click on create snapshot
then we get to give it a name so
simply learn web
web server one
root snap give it a description
snap a whole
simply one web server root
and it's saying encrypted no and that's
because the root volume isn't encrypted
if this was an encrypted volume then
this would also be an encrypted snapshot
so we click on create
so snapshot creation started
so if we go to snapshots on the left
hand side
we can see here
that our snapshot
has already been created
so that was a quick one now if you had a
much larger volume
then it would take longer because
remember it takes the snap blend it
copies the snapshot to Amazon S3 so that
can sometimes take a while
so this is 100 available so what can we
do with this well we can keep this for
safe keeping and we could take a
snapshot every night or every week of
the web server so we can restore to a
point in time
or if we wanted to we could create a
volume from this particular snapshot
so we could say yeah create a new snap
we could change
the different volume type so here we are
as we mentioned here are the two
different HDD options which are new as
of April 2016.
and we can change the availability zone
so once you have the snap you can do
pretty much anything with it
in your region
you can also create an Ami from your EBS
snapshot so you could create an image of
it and you could use that to create a
new instance if you wanted to
so that's a very brief overview of
Amazon EBS snapshots
hi guys uh welcome and today's topic is
on AWS Auto scaling so this is Akil I
would be taking up a tutorial on the
auto scaling but before that I would
request you guys to subscribe our
channel the link you can find is below
this video at the right side
let's begin with our tutorial and let's
look into what we have in today's
session
so I would be covering up why we require
AWS Auto scaling what is AWS Auto
scaling
what are the benefits of using the
scaling service
how this Auto scaling works the
different scaling plans we have
what is the difference between the
snapshot and the Ami
what is the load balancer and how many
types of load balancers we have and
along with that I would be covering up a
real life demo on the AWS
let's begin with why we require AWS
order scaling
now before AWS Cloud scaling there was a
question in the mind of Enterprises that
they were spending a lot of money on the
purchase of the infrastructure if they
have to set up some kind of a solution
so they have to purchase an
infrastructure and a one-time cost was
required so that was a burden for them
in terms of procuring a server Hardware
software and then having a team of
experts to manage all those
infrastructure
so they used to think that no longer
they require these resources if there
was a cost efficient solution for their
project that was the project manager
used to think
now after the AWS Cloud scaling that was
introduced automatically the auto
scaling maintains the application
Performance Based on the user
requirements at the lowest possible
price
so what does the auto scaling does is
that whenever there is a scalability
required it manages it automatically
and hence the cost optimization became
possible
now what is AWS Auto scaling let's look
into deep so AWS Auto scaling is a
service that helps users to monitor
their applications and the servers and
automatically adjust the capacity of
their infrastructure to maintain the
steadiness so they can increase the
capacity they can even decrease the
capacity also for the cost optimization
and also predictable performance at the
lowest possible
now what are the benefits of Auto
scaling it gave the better fault
tolerance applications you can get the
servers created and you can have a clone
copy of the servers so that you don't
have to deploy the applications again
and again better cost management because
the scalability is decided by the AWS
automatically based on some threshold
parameters
it was a reliable service and whenever
the scaling is created or initiated you
can get the notifications onto your mail
IDs or to your cell phones
uh scalability as I mentioned is always
there in the auto scaling it can scale
up it can scale down as well
and it has the flexibility the
flexibility in terms of whenever you
want to schedule it if you want to stop
it if you want to keep the size of the
servers at a fixed number you can always
make the changes on the Fly
and the better availability
now
with the use of the auto scaling we come
around with the terminology called
snapshot and Ami let's look into the
difference between the snapshots and the
Emi
uh snapshots versus Ami so in a company
there was one of the employee that was
facing an issue with launching the
virtual machines
so he asked his colleague a question is
it possible to launch multiple virtual
machines with a minimum amount of time
because it takes a lot of time in terms
of creating the virtual machines
the other colleague said that yes it is
possible to launch multiple ec2 instance
and that can be done at a lesser time
and with the same configuration
and this can be done either you use a
snapshot or the Ami on the AWS
then the colleague said that what are
the differences between the snapshot and
Airline
uh let's look into the difference now
the snapshots basically kind of a backup
of a single ABS volume
which is just like a virtual hard drive
that is attached to the ec2 instance
whereas the Ami it is basically used as
a backup of an ec2 instance only
the snapshots opts for this vendor
instance contain multiple static EVS
when you opt for the snapshot whenever
the instance contains multiple Statics
EVS volumes
Ami this is widely used to replace the
field ec2 instance
in the snapshots here you pay only for
the storage of the modified data
whereas with the Ami you pay only for
the storage that you use
uh the snapshots are non-bootable images
on ABS volume
whereas Ami are bootable images on the
ec2 instance
however creating an Emi image will also
create the EVS snapshots
now how does AWS Auto scaling work let's
look into it
so for the AWS order scaling to work you
have to configure single unified scaling
policy for application resource
and this scaling policy with that you
can explore the applications also and
then select the service you want to
scale also for the optimization select
do you want to optimize the cost or do
you want to optimize the performance
and then keep track of scaling by
monitoring or getting the notifications
now what are the different scaling plans
we have so in the auto screening a
scaling plan basically helps a user to
configure a set of instructions for
scaling based on the particular software
requirement
the scaling strategy basically guides
the service of AWS Auto screening on how
to optimize resources in a particular
application so it's basically a kind of
the parameters that you set it up so
that how the resource optimization can
be achieved in the auto scaling
with the scaling strategies users can
create their own strategy based on their
required metrics and thresholds and this
can be changed on the fly as well
what are the two types of scaling
policies we have so there are basically
Dynamic scaling and the predictive
scaling
uh now what is dynamic scaling it
basically guides the service of AWS Auto
scaling on how to optimize the resources
and it is helpful in optimizing
resources for availability and
particular price
now with scaling strategies users can
create their plan based on the required
metrics and thresholds so a metric can
be like let's say a networking Network
out or it can be a CPU utilization
memory utilization likewise
now in the predictive scaling its
objective is to predict future workload
based on daily and weekly Trends and
regular forecast future network traffic
so it is kind of a forecast that happens
based on the previous past experiences
it uses a machine learning technique for
analyzing that Network graphic and this
scaling is like how weather forecast
works right
it provides schedule scaling actions to
ensure the resource capacity is
available for application requirement
now with the auto scaling you would need
the load balancers also because if there
are multiple instances that are created
then you would need a load balancer to
distribute the load to rows instances so
let's understand what do we mean by a
load balancer
a load balancer basically acts as a
reverse proxy and it is responsible for
Distributing the network or the
application traffic across multiple
servers
with the help of a load balancer you can
achieve a reliability you can achieve a
fault tolerance of an application that
is basically it increases the fault
tolerance and the reliability
so for example when there is a high
Network traffic that is coming to your
application and if that much traffic
comes to your application to the
instances your instances May crash
so how you can avoid that situation
so you need to manage the network
traffic that is coming to your instances
and that can be done with the load
balancer so thanks to the AWS load
balancers which helps in distributing
Network traffic across backend servers
in a way that it increases performance
of an application
here in the image you can see the
from a different resources Landing onto
the ec2 instance and the load balancer
is actually Distributing that traffic to
all the three instances hence managing
the network traffic quite properly
now what are the types of load balancers
we have there are three types of load
balancers on the AWS one is the classic
load balancer second is the application
load balancer and the third one is the
network load balancer
let's look into what we have in the
classical manager
so the classic load balancer is the most
basic form of load balancing and we call
it as a primitive load balancer also and
if it is widely used for the ec2
instances it is based on the IP address
and the TCP port and it routes Network
traffic between end users as well as in
between the backend servers
and it does not support host based
routing and it results in low efficiency
of resources
let's look into what we have in the
application work balancer
this is one of the advanced forms of
load balancing it performs a task on the
application Level in the OSI model
it is used when there are HTTP and https
traffic routing is required
and also it supports the host based and
Pathways routing and performs well with
the micro services or the backend
applications
the network load balancer performs the
task at layer 4 of the connection level
in The OSI model
the prime role of the network load
balancer is to Route the TCP traffic
and it can manage a massive amount of
traffic and is also suitable to manage
the low latencies
let's look into the demo and see how
practically we can
create their order scale
hi guys let's look into the demo for how
we can create an auto Skilling on the
AWS console
so right now I'm logged into the AWS
console and I am in the Mumbai region uh
what you need to do is you have to go to
the compute section and under that click
on the easy to service
let's wait for the ec2 servers to come
now just scroll down
and under the load balancing there is an
option called Auto scaling so there
first you have to create a launch
configuration and then after that you
have to create the auto scaling groups
so click on launch configuration
and then you have to click on create
launch configurations
so click on create launch configuration
now this launch configuration is
basically this set of parameters that
you define while launching an auto
scaling so that this uniformity is
maintained with all the instances
so that includes let's say if you select
a Windows Os or a Linux OS that
particular type of an operating system
will be implemented in all the instances
that will be part of an auto scale
so there are certain set of parameters
that we have to specify during the
launch configuration so that we can have
a uniformity in terms of launching the
servers so here uh I would select an
Amazon Linux Ami
and then I would select the type of
server which will be t2.micro
click on configure details put the name
to the launch configuration let's say we
put it as a demo
and the rest of the things we'll keep it
default
click on add storage since it's a Linux
Ami we can go with the 8GB storage that
should be fine click on configure
Security Group
let's create a new Security Group which
has the SSH Port open and that is open
for
anywhere which is basically source ipv4
and IPv6 IPS any IP would be able to
access that
click on review uh just review your
launch configuration if you want to make
changes you can do that otherwise click
on create a launch configuration
uh you would need the key pair and this
key pair will be a unique key pair which
will be used with all the instances that
are part of the auto scaling group so we
can select an existing key pair if you
have that otherwise you can create a new
keeper so I have an existing key pair
I'll go with that acknowledge it and
click on create launch condition
now we have successfully launched the
configuration of an auto scaling the
next thing is to create an auto scaling
group so click on create an auto scaling
group using this launch configuration
put a group name let's say we put
something like test
and the group size to start with it says
one instance so that means at least a
single instance will always be running
and it will be initiated and running 24
cross 7 till the auto scaling is
available you can increase the size of
the minimum base instances Also let's
say you can change it to 2 also so you
would get at least two servers running
all the time
so we'll go with the one instance the
network would be the VPC default and in
the VPC that particular region we can
select the availability zones so let's
say if I select availability Zone 1A and
then availability is on 1B
so how the instances will be launched so
one instance will be launched in money
the other one in the 1B the third one in
the 1A both one and the 1B likewise it
will be equally spreaded among the
availability zones
next part is to configure the scaling
policies so click on it
if you want to keep this group at its
initial size let's say if you want to go
with only a single instance or two
instances and you don't want the scaling
to progress you can put it keep this
group at its initial size so this is
basically a way to Halt the scaling
but we'll use the scaling policies to
adjust the capacity of this group so
click on it and we would scale between
let's say minimum one instance that we
have and we'll scale it between one two
for instances
and
what condition on what basis these
instances will be scaled up or scaled
down would be defined in the scale group
size
so the scaling policies you can
Implement based on a scale group size or
using the simple scaling policies using
this
so in the scale group size you have a
certain metrics you can use average CPU
utilization you can define a metric
related to average networking average
Network out or the load balancer request
counts per Target and if you create a
simple scaling policies using steps then
you need to create the alarms and there
you can get some more metrics that you
can add up as a parameter for the auto
scaling
let's go over the scaling group size
let's go with the metric type as average
CPU utilization and the target value
here you have to specify what would be
the threshold that when the instance CP
utilization is crossed then a new
instance should be initiated
so you can put a reasonable threshold
for that let's say we put something like
85 percent and whenever the instance CP
utilization is crossed 85 threshold you
will see that there will be a new
instance created
let's go to the next configure
notifications
and here you can add notifications so
let's say if there is a new instance
that is initiated and you want to
basically be notified so you can get
notifications over your email IDs or you
can get it on the cell phones so for
that for adding the notification you
would need the SNS service that is
called as a simple notification service
and you have to create a topic there you
have to subscribe for the topic using
your email ID and then you should get
the notifications
uh click on configure tags the tags are
not mandatory you can basically
put the tag let's say if you want to
identify the instance for the purpose it
was created otherwise you can leave it
blank also click on review
and review your scaling policies
notification tags as well as the scaling
group details
click on create auto scaling group and
here you go your scaling has been
launched click on close
and you should get at least a single
instance initiated automatically by the
way so let's wait for the details to
appear so here you can see
on launch configuration name demo
Auto scaling group name test
minimum instance we want one the maximum
instances we want four
we have selected two availability zones
it is South 1B
and uh
the instance one has been initiated and
if you want to verify where exactly this
instance has been initiated just click
on the instances here and here you will
see that our single instance has been
initiated that is in service and that
has been initiated in AP South one way
now once the threshold of this instance
crosses 85 percent that is what we have
defined in the scaling policies
then you should see that another
instance will be Shader so likewise this
is basically I have created steps to
initiate a scaling policy that tends to
increase the number of servers whenever
the threshold crosses likewise here
itself you can add another policy to
scale down
the resources in case if the CPU
utilization goes to a normal value so
what exactly is an AWS certification it
represents a degree of expertise in AWS
it is obtained after passing one or more
exams provided by Amazon and each
different role like this is Ops
administrator developer or Solutions
architect has a different exam
associated with it this in turn helps
employees demonstrate and validate
technical Cloud knowledge which means
that the people who are certified
actually know what they're talking about
I have seen a common misconception on
people where they believe that just
because they're certified they're
entitled a job that's not true now
without proper practice or hands-on
experience the certification is wasted
on you so remember just because you're
certified does not mean you'll get a job
it only makes the job application
process slightly easier relative to
someone who's not been certified now why
is AWS given so much importance some of
the important reasons are an AWS
certified individual gains credibility
for their skills in AWS they also gain
access to free practice exams which help
them prepare for the next AWS
certification there's an increase in
monetary compensations which only means
that they can get paid more and you gain
recognition for your Knowledge and
Skills in AWS right now we're in the AWS
certification website whose link will be
in the description and now we're going
to talk about the types of AWS
certification as you can see here there
are three levels of AWS certification
there's the foundational level associate
level and professional level
certification now the foundational level
certification only requires you to have
a basic understanding of how the AWS
Cloud works for the AWS certified
practitioner is optional for the
architect path developer path and
operations path it is mandatory for the
specialty certifications like the
advanced networking big data and
security certifications now the
associate level certifications are
mid-level certifications for a technical
role now a professional certification is
the highest level of certification that
you can have for a technical role now
you have the solutions architect for the
architect path and the devops engineer
certification for both the developer and
operations path so how do you decide
which of these certifications is
suitable for you so you've seen here
that AWS provides various certifications
for a number of job roles exercise
administrator solution architect
developer so you need to make the right
choice taking into consideration the
areas of your interest and the
experience level that you have now we're
going to talk about each of these
certifications in detail so first let's
talk about the AWS certified Cloud
practitioner now we all understand that
AWS is a widely recognized product in
the market so this certification helps
you validate how well you know the AWS
Cloud so this is just the basic
understanding now it is optional for the
developer path on the operations path I
would suggest it's a good idea to start
here because it forms a solid Bedrock on
all the other things that you're going
to learn soon now more importantly it
does not require any technical knowledge
of other roles such as development
architecture Administration and so on so
it's a great place to start for
newcomers now you have the architect
role certifications now this is for you
if you are interested in becoming a
Solutions architect or a solution design
engineer or someone who just works with
designing applications or systems on the
AWS platform now first we have the AWS
certified Solutions architect associate
level certification now this
certification is for you if you want to
show off how well you can architect and
deploy applications on the AWS platform
now it is recommended that you have at
least a year of experience working with
distributed systems on the AWS platform
at the same time it's also required that
you understand the AWS services and be
able to recommend a service based on
requirements you need to be able to use
architectural best practices and you
need to estimate the AWS cost and how
you can reduce them next up you have the
AWS certified Solutions architect
professional level certification now you
will not get the certification unless
you're done with the AWS certified
Solutions architect associate level
certification this is the show of your
technical skills and experience in
designing distributed applications on
the AWS platform now this does require
you to have two years of experience
working with Cloud architecture on AWS
at the same time it also requires you to
be able to evaluate requirements and
then make architectural recommendations
you also need to provide guidance on the
best practices on architectural design
across a number of different platforms
the developer level certifications are
for you if you are interested in
becoming a software developer now the aw
a certified developer associate
certification is to test how well you
know how to develop and maintain
applications on the AWS platform it does
require you to have a year or more of
hands-on experience to design and
maintain AWS based applications like any
software developer role it is necessary
that you know in depth at least one high
level programming language it's also
necessary that you understand the core
of AWS Services uses and basic
architectural best practices you need to
be able to design develop and deploy
cloud-based Solutions on AWS platform
and you need to understand how
applications can be created you need to
have experience in developing and
maintaining applications for a number of
AWS services like Amazon SNS dynamodb
sqs and so on now for the AWS certified
devops engineer professional level
certification note here that this
certification is exactly the same as the
one you have under the operation sold so
both of them are the same thing so here
it tests your ability to create operate
and manage distributed applications on
the AWS platform now it is necessary or
it is mandatory to have the AWS
certified developer associate
certification for the AWS certified
sysops administrator certification with
two or more years of hands-on experience
in doing the same in AWS environments it
requires you to be able to develop code
in at least one high level language you
need to be able to automate and test
applications via scripting and
programming and to understand agile or
other development processes the
operation certifications are for you if
you want to become a cisops
administrator systems administrator or
someone in devops role who wants to
deploy applications networks and systems
in an automatable and beatable way the
AWS certified stops administrator
associate certification tests your
knowledge in deployment management and
operations on the AWS platform now you
need to have one or more years of
hands-on experience in AWS based
applications you need to be able to
identify and gather requirements then
Define a solution to be operated on AWS
you need to be able to provide guidance
for the best practices through the life
cycle of a project as well now the
specialty certifications are for you if
you're well versed in AWS and want to
showcase your expertise in other Tech
local areas the aw certified Big Data
certification showcases your ability to
design and Implement AWS Services which
can help derive value from a large
amount of complex data you are however
required to have completed the
foundational or associate level
certification before you can attempt
this you need a minimum of five years of
hands-on experience in the data
analytics field as well next we have the
aw certified Advanced networking
certification this validates your
ability to design and Implement AWS
Solutions as well as other hybrid ID
Network architectures at scale this also
requires you to have completed the
foundational or associate level
certification you need to have a minimum
of five years of hands-on experience
architecting and implementing Network
Solutions and lastly we have the AWS
certified security certification it
helps showcase your ability to secure
the AWS platform you're required to have
an associate or Cloud practitioner level
of certification a minimum of five years
of I.T security experience and two years
of hands-on experience securing AWS
workloads now say I wanted to schedule
an examination so for example I want to
do the solutions architect certification
so first I would go there now here I can
click on register now and the process
continues or I can click on learn more
by doing this again I can show you the
examination here I can also get access
to other data like the number of
questions available the cost of an
examination the portions I need to study
and so on now let's talk about Solutions
architect certification with a little
more detail now this certification exam
cost 150 US Dollars and the practice
exam cost 20 US Dollars now here I can
schedule the examination or download the
exam guide I've already downloaded the
exam guide and here it is now this exam
guide tells you about what you need to
learn and what is expected from you here
they want you to define a solution based
on requirements and provide guidance in
its implementation it is also
recommended that you know about how the
AWS Services work one years of hands-on
experience with distributed systems on
AWS to identify and Define technical
requirements and so on the rest is
available in the exam guide and most
importantly they tell you the main
content domains and their weightages now
we have five domains first domain is to
design resilient architectures which
holds 34 percent of waiting page at the
pain too you have to Define performant
architectures three is to specify secure
applications and architectures cost
optimized architectures and five to
define operationally excellent
architectures now like you've seen here
you've selected one certification and
learned it in detail you can do the same
for any of these other certifications
you can press learn more and download
their exam guide and learn everything
that you need to know and welcome to the
session on AWS certification in this
session I'll be telling you about
several things like why AWS is so
important next we'll talk about the job
roles in AWS the career path to become
an AWS Solutions architect the
certifications that are available in AWS
and companies that are hiring in 2020
also guys if you have any questions
regarding the topic or AWS let us know
in the comment section below our team of
experts will help you as soon as
possible
now before we get on to this let's talk
about why or what exactly is AWS
AWS or Amazon web services is a cloud
computing platform that offers more than
175 different Services across several
domains
now among these several cloud service
providers in the market right now like
Microsoft is your VMware Rackspace and
so on AWS is more commonly used than any
of its competitors
but what makes AWS so special why must
you get certified by them
so to answer our earlier question
let's have a look at 2019's third
quarters Cloud market share which is
provided by Canada's
now AWS dominated the cloud service
market share with 32 percent of the
entire share
and it had a growth of over 2.3 billion
dollars year over year now you can see
in the graph here that Azure and gcp
only account for 17 and seven percent
respectively the others like Rackspace
and IBM Cloud are contributing 44
percent
now if you do need more convincing as to
choosing AWS let me give you more
reasons why
firstly it provides pay as you go
pricing which means you only pay for how
much you use
now you use what you need and then the
costs are scaled automatically and
accordingly now for example say you use
ec2 which provides virtual machines you
only use for how much virtual machines
you've created and how long you use them
AWS is also very secure now this
provides end-to-end privacy
and security and Storage
AWS also has experience now AWS has over
four years of leg up over other cloud
service providers like Microsoft azure
this means over the years they have
skills and infrastructure management
that can give you more
next AWS is also very flexible
this allows users to select their
operating system language database and
other services with great ease
AWS is also very easy to use you can
host your applications very quickly
and securely whether it's existing or a
completely new application
AWS is also extremely scalable now if
you're a small scale company you don't
need a lot of resources
AWS understands that and scales down if
you're a larger organization obviously
you're going to need a lot more
resources
AWS can help with that as well
now let's talk about the job roles
within AWS
now here are some of them these are
these aren't all of them but we have AWS
Cloud architect sysops administrator
Cloud developer system integrator Cloud
devops engineer
Solutions architect Network specialist
and Big Data specialist
now getting each of these roles becomes
much easier when you're certified by AWS
now let's talk about the steps you need
to take so that you can become an AWS
Solutions architect
now we're selecting this one because
this is one of the most common and
talked about Job roles in AWS
now most of the job job roles I
mentioned earlier have similar steps but
but with a few steps added or removed
somewhere else
now first off you need an understanding
of operating systems
you'll need to have a basic
understanding of operating systems like
Windows or Linux now although this isn't
exactly mandatory this will help you
understand cloud services better
since most of them are based on Linux
next up you need to have experience with
programming languages
languages like SQL go Python c-sharp.net
and so on will help you develop
cloud-based applications based on
requirements from your organization now
although this isn't mandatory either
some services do require coding so that
you can use them properly in the next
step we have networking you need to
understand networking fundamentals like
virtual networks local area networks
wide area networks networking models and
so on now this is really important
because this will help you understand
how the backend infrastructure of the
cloud works
after that the most self-explanatory
step learn AWS now you need to focus on
learning the different services that are
offered by AWS now AWS provides a
year-long free tier where you get access
to several important services like ec2
and S3
so you can use it to get practical
experience with these services
now you can decide among several
different career paths that are offered
by AWS
paths like Cloud practitioner Cloud
architect Cloud developer operations
devops engineer and certain some others
and finally you need to be certified by
AWS
you need to choose from different levels
of certification that are provided by
AWS
these can help you showcase your
Proficiency in working with different
AWS Solutions
now let's talk about certifications
available in AWS now as you can see on
screen there are three levels four
actually firstly you have foundational
level certification that includes the
cloud practitioner certification
now for this you need to have six months
of basic Cloud knowledge and Industry
knowledge as well
so basically the cloud practitioner
certification access the starting point
for just about every other job role you
want to take be it architect operational
or developer
then in associate you need to have a
year of experience working with AWS
Cloud solving Solutions and implementing
Solutions you have options for Solutions
architect sysops administrator developer
and so on and finally you have
professional level certification now
after two years of experience with the
AWS Cloud you can take up the
certification you have options of
solutions architect professional and
devops engineer professional
now there are certain specialty
certifications that are provided by AWS
these are based on Advanced networking
big data security machine learning and
Alexa skill Builder
now let's have a look at aws's website
let's go to Google
such a AWS certification
and here you can see that
like like I told you here are the
certifications
let's go to the bottom here
and you can see that here all the
certifications that I mentioned are
divided into different levels like I
told you Cloud practitioner architect
developer operations and devops engineer
let's have a look at Cloud practitioner
so here you can see the steps you need
to take to become a cloud practitioner
now AWS suggests that you take up their
course and then follow it up by taking
the certification test
now similarly let's have a look at how
to become an AWS Cloud architect
you can select the link here
so like we saw with Cloud practitioner
they give you a set of courses you need
to take followed up by a
certification test
basically you need to first get the
associate level certification and then
move on to the professional level
certification after you've gotten some
experience and knowledge similarly you
can look at other job roles like
developer
very similar and operations likewise
everything else you can see here
now let's go back and talk about the
companies that are hiring in 2020 so
this is basically every company that's
hiring in 2020. so you have examples
like IMDb Lamborghini Adobe bitdefender
HTC and so much more this is just the
tip of the iceberg hi guys this is Raul
from Simply learn and today I'm going to
tell you how you can become an AWS
Solutions architect so who is an aw
Solutions architect now the main role of
an AWS Solutions architect is to help
you deploy your applications on the AWS
platform now this is nothing but a cloud
computing platform so it's not to say
that you can't deploy your applications
on the cloud computing platform yourself
it's just that when it comes to
organizations the applications that you
need to deploy become a whole lot more
complex that's where an aw Solutions
architect can help ever since cloud
computing became a thing companies
around the world have started migrating
their physical infrastructure onto the
cloud that's what an AWS Solutions
architect does they help you migrate
your physical infrastructure onto the
AWS Cloud companies around the world
work on a budget an naw Solutions
architect will help design a cloud info
structure based on the organization's
budget before that can be done however
an AWS Solutions architect has to create
a design with an intricate and detailed
blueprint of the cloud infrastructure
that they plan to set up now aw
Solutions Architects also have to focus
mainly on non-functional requirements
like usability reliability scalability
and performance of the cloud
infrastructure they're also responsible
when it comes to minimizing risks that
an organization can face when it comes
to cloud computing platforms now they
could face risks like security leaks
calculation mistakes and application
down times and aw Solutions architect
has to ensure that these don't happen
before we can talk about how you can
become an AWS Solutions architect I have
some exciting news for you guys we've
launched our own YouTube Community
you'll get to see a lot of quizzes polls
offers and much more to make your
learning experience a whole lot more fun
you can find all of this on your
subscription feed or you can also click
on the top right corner right now to get
started and let's get back to how can
you become an aw solution architect now
to become an aw certified Solutions
architect you need to clear the aw
certified Solutions architect associate
level examination now here's some
details about it the exam score ranges
from 100 to 1000 marks and the minimum
passing score is 720. however there's a
catch the passing marks are actually set
using statistical analysis so they can
be changed based on how difficult the
examination actually is the exam fee is
150 dollars and you can also take a
practice examination which costs 20
dollars now regardless of the
examination you take be IT solutions
architect sysops administrator or
developer any associate level
examination costs 150 dollars for the
professional level examination it's 300
now if you want to learn more about the
AWS certifications I suggest you click
on the top right corner and watch our
video aw certifications in 10 minutes to
learn more now the exam duration is of
130 minutes and you have two types of
questions multiple choice and multiple
answer now the multiple choice questions
have four options out of which one of
them is right and you have multiple
answer where you have five options out
of which two of them are correct you can
take the examination in English Japanese
Korean and simplified Chinese now let's
talk about how you can schedule an
examination first let's go to Google and
search for AWS certifications
click on the first link
so on this page we can go to the bottom
and find the different certifications
AWS provides
click on architect
and select the AWS certified Solutions
architect associate certification click
on register now
and here you need to click on the AWS
certification account
you can sign in with your Amazon account
or create a new one
I already have an Amazon account that
I'll be using for signing in here in
case you don't you can click on the
create your Amazon account button here
and create an account for yourself
now after that is done you can schedule
new examination
and you can scroll down to the bottom
here aw certified Solutions architect
associate level certification
click on schedule exam
press continue
in here you need to select your language
which I'm assuming is English
I'm from India so I don't need to change
anything here but you can change your
country or time zone and other details
based on your requirement and select
your preferred month I want to do it in
October
now select search for Exam Center
select the one that's closest to you
I'm going to select this one
and here you can select the day that you
want to take your test and the time
available I want to do it on 15
select the time that I want
and press continue
now you can go through all the details
here change them if you want to
otherwise you can press continue
after this is done
close
and here we have the final step which is
the fees
you can enter your details here
and pay now to finish the process now
let's look at an outline of the exam
content now what you're going to see in
the exam are five major domains and here
you have each of the domains with their
respective weightages first you have a
34 designing resilient architectures at
24 you have both defining performant
architectures and specifying secure
applications and architectures a 10
person you have designing cost optimized
architectures and at six percent you
have defining operationally excellent
architectures now let's look at each of
these domains in detail now the first
domain or design resilient architectures
can be divided into four parts firstly
you need to know how you can choose
reliable or resilient storage using
services like AWS S3 AWS Glacier and AWS
EBS then you have how you can design
decoupling mechanisms using AWS Services
now this is possible with the help of
AWS SNS now these aren't the only
services that enable this these are just
some of the services then how you can
design a multi-tier architecture
solution now this is important because
you need to know how you can create a
solution that involves several other
services then you need to know how you
can design highly available and fall
tolerant architectures now for the
second domain defining performance
architectures first you have how to
choose performance storages and
databases services that are used are AWS
RDS AWS threadshift and AWS dynamodb the
second step is how you can apply caching
to improve performance a service that
can be used for this is AWS elastic
cache third how you can design Solutions
with elasticity and scalability you have
AWS Lambda AWS cloudwatch and AWS data
pipeline now for the third domain which
is specifying secure applications and
architectures you need to know how you
can secure applications using services
like AWS inspector AWS cloudtrail and
AWS iaf you need to know how to secure
data using Cloud HSM and AWS Macy and
and how you can Define the networking
infrastructure using cloudfront VPC and
elastic load balancer for the fourth
domain you have designing cost optimized
architectures firstly you need to know
how you can design cost optimized
compute Solutions you can use AWS ec2
elastic Beanstalk Lambda and AWS light
sail then you need to know how you can
design cost optimized Storage Solutions
using AWS S3 Glacier EBS and elastic
file system and the final domain to
define operationally excellent
architectures you need to set up design
features and solutions that enable
operational excellence now some of the
features are that you perform operations
as code you annotate documentation you
make frequent and small reversible
changes and anticipate and Tackle
failures now let's have a look at the
job opportunities when it comes to AWS
Solutions architects now if you have a
look at this graph you can see that AWS
has always provided a lot more job
postings as compared to the other two
giants in the cloud computing domain
which are Microsoft Azure and Google
Cloud platform now if you do a little
bit of research you'll find out that
there's a significant lacking of
experience Personnel when it comes to
AWS so I would suggest this is the best
time for you to get certified in AWS so
how can simply learn help you with your
AWS certifications now we're on the
simpleton on home page
and here we have the AWS Solutions
architect course
on this course which is the AWS
Solutions architect certification
training course we have 36 hours of
instructor-led training 20 hours of
self-paced learning 16 live demos three
simulation exams three Hands-On practice
projects and so much more here our main
emphasis is on making sure that you have
enough hands-on experience with the
services that you crack the examination
at your first attempt
so here we also go through some of the
more important services that you need to
learn like IAM VPC ec2 S3 and so much
more we'll also talk about databases
application Services security practices
disaster recovery and so on we even have
practice examinations to help you
prepare
we do by simply learn today I am going
to help you understand how to pass the
AWS Cloud practitioner exam so let's get
started before we get started let us
look at today's agenda first we will go
through what is AWS Cloud practitioner
and its eligibility criteria then we
will go through the exam objectives and
its exam content further we will learn
how to prepare for the exam and look at
a few sample questions and finally we
will learn a few tricks related to the
exam now what is AWS Cloud practitioner
AWS has a variety of certifications
under foundational level comes AWS Cloud
practitioner going up we can achieve
professional and advanced level
certificates AWS gives a variety of
specialty certificates which is
something very unique it is a
foundational level certificate provided
by AWS it requires 6 months of AWS cloud
and Industry knowledge becoming an AWS
certified Cloud practitioner is
recommended it is an optional step
towards achieving an associate heat
level or specialty certification with
the cloud practitioner certificate you
can get a job as a cloud engineer or AWS
practitioner in many companies now let
us talk about the eligibility criteria
every exam has its own eligibility in
order to take the certification the
candidate must follow two important
criteria first he should have at least
six months of experience working with
AWS Cloud Concepts in various rules like
technical sales and finance next it is
mandatory for him to have knowledge of
the AWS Cloud platform and it's working
also he should know about the various ID
Services now let's talk about the exam
overview the exam duration is 90 minutes
the candidate should get 65 of the
answers right in order to pass the exam
the question types are multiple choice
and multiple response questions the cost
of this exam is hundred dollars every
certificate is valid for a few months or
years AWS Cloud practitioner is valid
for two years after two years this
certificate needs to be renewed as in
the exam needs to be taken again let's
get into the exam objectives now the AWS
Cloud practitioner validate the
candidate strength in various domains
first he should be able to explain the
cloud knowledge and its platform he
should know how to manage the account he
should know about the billing and
pricing details the candidate should
have knowledge of key services available
on AWS Cloud platform the candidates
should be able to explain the various
Cloud values the candidate should be
able to explain security Concepts
security model and compliance Concepts
the candidate should recognize the main
sources of technical assistance or
documentation the candidate must be able
to explain the key features of operating
and deploying in AWS cloud and finally
he should be able to explain the cloud
architecture and its basic principles in
order to excel in this exam we should
know the topics that occur in the exam
so let's take a look at the exam content
the exam has four major domains namely
Cloud Concepts security and compliance
technology and billing and pricing the
pie chart indicate is the weightage of
each domain in the exam the cloud
Concepts contain 28 percent of the total
exam content security and compliance
contains 24 of the exam content
technology has the maximum weightage of
36 percent of the total exam content and
billing and pricing has the lowest
weight rate of 12 of the entire exam
content now let's dive deep into each of
these Technologies first comes Cloud
Concepts in this you need to have some
basic knowledge of cloud computing
Concepts questions asked from this field
are broad now so to perform well you
must have high level cloud Concepts
which include elasticity scalability
High availability and fault tolerance
next we have security and compliance
security is marked as an important topic
of whether you are working with
infrastructure or not this certification
exam includes a variety of questions
related to security and culpable
management you may find some important
topics regarding this domain in your
exam as follows shade security model
Cloud logs dtos protection and I am that
is managing users password policies and
MFA the next domain is technology
technology is the most important domain
of the AWS Cloud practitioner exam if
you want to adapt in this section then
you should have the knowledge of core
AWS Services some of the AWS sources
that you must know are ec2 elb RDS S3
SNS and AWS Lambda and finally we have
billing and pricing the AWS certified
Cloud practitioner exam constraints on
the business applications of AWS billing
and pricing becomes important topics
that we should know you must have
knowledge of general account information
AWS support how certain services are
built how to calculate the cost of
services and using what tools Etc there
may be some questions which overlap with
other exam domains example AWS cost
calculator service may fall into billing
and pricing domain as well as technology
domain now that we know the domains
let's go ahead and see how to prepare
for the exam the first thing is we have
to start off with the AWS in classes the
following three AWS training courses
will help you pass the exam that is AWS
Cloud practitioner essential AWS
technical Essentials and AWS Business
Essentials moving forward the next step
is to read the AWS certified clock
practitioner certification exam guide
this guide would give an idea about the
essential areas that needs to be
concentrated on it provides an overview
of all the exam objectives with
preparation instructions the next step
is to get familiar with the subject
areas before taking any exam knowing the
subject areas is very important the exam
content has been explained before so
review the subject areas carefully plan
and prepare how to attempt the test
accordingly moving forward we have to go
through the AWS white papers these white
papers contain useful information and
cover many important topics some of the
popular AWS white papers are overview of
AWS how AWS pricing Works maximizing
value with AWS and many more cell study
is enough to pass the certification exam
the online training will help you under
understand the subject areas a lot of
material can be found on the AWS website
also read the facts related to the AWS
cloud services applications and moreover
architecture the next important step is
to examine sample questions and take
free tests to Ace the exam a practice
test is always necessary to see where
you stand and what subjects you need to
concentrate on AWS has many practice
tests and you can take them before the
exam the final step is to schedule the
exam and get certified so once you're
prepared well enough then enroll for the
exam choose any testing center near you
at AWS training website and register
yourself going forward let's look at few
of the sample questions that AWS website
has so all these questions are present
under the AWS website so you can go
through them now the first question is
why is AWS more economical than
traditional data centers for
applications with varying Computing
workloads so you can see the answer
choices on the screen the correct answer
is Amazon ec2 instances can be launched
on demand when needed the next question
is which AWS service would simplify
migration of a database to AWS the
choices are AWS storage Gateway AWS
database migration service Amazon
elastic compute Cloud Amazon appstream
2.0 so the correct answer is AWS
database migration service the next
question is which AWS offering enables
customers to find buy and immediately
start using software Solutions in the
AWS environment so you can see the
answer choices on the screen the correct
answer is AWS Marketplace which AWS
networking service enables a company to
create a virtual Network within AWS the
choices are AWS config AWS root 53 AWS
direct connect Amazon virtual private
Cloud the correct answer is Amazon
virtual private Cloud which of the
following is aws's responsibility under
the AWS shared responsibility model so
the options are configuring third-party
applications maintaining physical
Hardware securing application access and
data managing custom Amazon machine
images so the correct answer is
maintaining physical Hardware the sixth
question is which component of AWS
Global infrastructure does Amazon
cloudfront used to ensure low latency
delivery the choices are AWS regions AWS
Edge locations AWS availability Zone
Amazon virtual private Cloud the correct
answer is AWS Edge locations so the
seventh question is how would a system
administrator add an additional layer of
login security to users AWS Management
console the answer choices are use AWS
Cloud directory audit AWS identity and
access management rules enable
multi-factor authentication enable AWS
cloudtrail so the correct answer is to
enable multi-factor authentication the
next question is which service can
identify the user that made the API call
when an Amazon elastic compute Cloud
instance is terminated so the options
are Amazon cloudwatch AWS cloudtrail AWS
x-ray AWS identity and access management
the correct answer is AWS cloudtrail the
next question is which service would you
use to send alerts based on Amazon Cloud
watch alarms the options are Amazon
simple notification service AWS
cloudtrail AWS trusted advisor Amazon
Route 53 the correct answer is Amazon
simple notification service and finally
the last question is where can a
customer find information about
prohibited actions on AWS infrastructure
so the correct answer for this is AWS
acceptable use policy so you can see we
get sample questions like these you can
go to the website and see all the other
preparation and exam guides you need
finally the last Topic in this video is
the tips while taking the exam since the
time is limited wasting time of
difficult questions is unavoidable the
next tip is to keep a track of time and
learn how to manage time to answer all
the questions third one is analyze the
questions first and then answer because
sometimes they may be traps and finally
as per the guidelines some questions may
have multiple answers so read the
questions properly
there are over 18 million jobs in the
field of cloud computing globally
simply learns AWS certified solution
architect associate level course will
help you understand the key Concepts
required to prepare for the
certification exam
the practice assignments and the three
live projects this course comprises of
24 hours of instructor-led training 15
hours of e-learning over 50
demonstrations to understand
architecting through the AWS console and
you will also be provided with three
question sets totaling 180 questions the
worldwide cloud computing Market grew 28
to 110 billion dollars in revenues in
2015.
in the last quarter of 2015 AWS Revenue
was up 69 year on year AWS Solutions
architect associate level is the perfect
certification to start your journey with
AWS you'll enhance your existing
knowledge of AWS and will deepen your
understanding of every product and
service that AWS offers you will learn
how to design plan and scale in the AWS
Cloud using Amazon recommended best
practices with the certification you'll
be an asset to any organization helping
it to leverage the benefits of best
practices around Advanced cloud-based
Solutions and migrate existing workloads
to the cloud
whether you're looking to Future proof
for your career or just prove you'll
know how about the leading cloud
provider in the global market becoming
an AWS Solutions architect associate
will put you well on your way to
achieving your career goals you can
reach out to our teaching assistant
support staff anytime during the course
and For assistance with projects
candidates can submit their queries that
help and support on our website or they
can get connected with our staff using
simply talk and the live chat options
this concludes the course introduction
and we hope you're all geared up to
begin the AWS Solutions architect
associate level course
happy learning
hello and welcome to the introductory
video of the AWS sysops administrator
associate level course offered by simply
learn
in this video you will get to know some
of the interesting facts about AWS get a
brief overview about the course and know
how the certification can help your
career Amazon web services provides a
highly reliable scalable low-cost
infrastructure platform in the cloud
that powers hundreds of thousands of
businesses in 190 countries around the
world the worldwide cloud computing
Market grew 28 to 110 billion dollars in
revenues in 2015.
in the first quarter of 2016 Amazon web
services or AWS Revenue was up by 58
percent
the certification holder takes home an
average salary of 125
871 up from 114 935 last year
according to Forbes there are over 18
million jobs in the field of cloud
computing globally
AWS sysops administrator associate is
the perfect certification to prove your
technical expertise in deploying
managing and operating scalable highly
available and fault tolerant systems on
AWS platform
you'll enhance your existing knowledge
of AWS so you can select the appropriate
products and tools to leverage AWS in
the most efficient and cost-effective
way
you'll learn how to implement and
control the flow of data to and from AWS
and also be able to estimate AWS usage
costs and identify operational cost
control mechanisms
with the certification you will be an
asset to any organization helping it to
leverage the benefits of best practices
around Advanced cloud-based Solutions
and migrate on-premises applications to
AWS
simply learns AWS sysops administrator
associate level course will help you
understand the key Concepts required to
prepare for the certification exam
before diving into the course details
let us get a glimpse of all the
interesting ways you get to learn about
AWS
delve into the various AWS services
work with AWS Management console
learn the CLI commands
validate your knowledge
the practice assignments and the three
life projects are included especially
for you to gain hands-on experience with
AWS Management console and the CLI to
become a domain expert in building
deploying and managing systems in AWS
this course comprises 24 hours of
instructor-led training 15 hours of
e-learning and three hours of
demonstrations to understand technical
details associated with data management
networking security and more you will
also be provided with three question
sets totaling 180 questions to validate
the depth of your knowledge
learning path
this certification also serves as a
prerequisite for the AWS devops engineer
professional exam whether you are
looking to Future proof your career or
enhance your know-how about the leading
cloud provider in the global market
becoming an AWS sysops administrator
associate will put you well on your way
to achieving your career goals
you should ideally be an I.T
professional with experience in both
traditional I.T infrastructure and the
AWS platform
you should have experience in everyday
operations of a system administrator
which include deployment of the it
systems backups OS management and have
some experience with the AWS platform
can reach out to the teaching assistance
support staff anytime during the course
and For assistance with projects
you can submit your queries at help and
support on our website or get connected
with our staff using simply talk and
live chat options
this concludes the course introduction
and we hope you are all geared up to
begin the AWS sysops administrator
associate level course
let's get started with the first lesson
monitoring and metrics happy learning
hi there I'm Samuel and I'm here to walk
you through some of the AWS interview
questions which we find are important
and our hope is that you would use this
material in your interview preparation
and be able to crack that cloud
interview and step into your dream Cloud
job by the way I'm and Cloud technical
architect trainer and an interview
panelist for cloud Network and devops so
as you progress in watching are you
gonna see that these questions are
practically scenario based questions
that tests the depth of the knowledge of
a person in a particular AWS product or
in a particular AWS architecture so why
wait let's move on all right so in an
interview you would find yourself with a
question that might ask you define and
explain the three basic types of cloud
services and the AWS products that are
built based on them see here it's a very
straightforward question just explain
three basic types of cloud service and
when we talk about basic type of cloud
service it's compute obviously that's a
very basic service storage obviously
because you need to store your data
somewhere and networking that actually
connects a couple of other services to
your application these basic will not
include monitoring these basic will not
include analytics because they are
considered as optional they are
considered as Advanced Services you
could choose a non-cloud service or a
product for monitoring of and for
analytics so they're not considered as
basic so when we talk about Basics they
are compute storage and networking and
the second part of the question says
explain some of the AWS products that
are built based on them of course
compute ec2 is a major one that's that's
the major share of the compute resource
and then we have platform as a service
which is elastic bean stock and then
function as a service which is Lambda
Auto scaling and light cell are also
part of compute services so the compute
domain it really helps us to run any
application and the compute service
helps us in managing the scaling and
deployment of an application again
Lambda is a compute service so the
compute service also helps in running
event initiated stateless applications
the next one was storage a lot of
emphasis is on storage these days
because if there's one thing that grows
in a network on a daily basis that
storage every new day we have new data
to store process manage so storage is
again a basic and an important cloud
service and the products that are built
based on the storage services are S3
object storage Glacier for archiving EBS
elastic block storage as a drive
attachment for the ec2 instances and the
EFS files share for the ec2 instances so
the storage domain helps in the
following aspects it holds all the
information that the application uses so
it's the application data and we can
also archive old data using storage
which would be Glacier and any object
files and any requirement for Block
storage can be met through elastic Block
store and S3 which is again an object
storage talking about networks it's just
not important to answer the question
with the name of the services and the
name of the product it'll also be good
if you could go in depth and explain how
they can be used right so that actually
proves you to be a person knowledgeable
enough in that particular service or
product so talking about networking
domain VPC networking can't imagine
networking without VPC in in the cloud
environment especially in AWS Cloud
environment and then we have Route 53
for domain resolution or for DNS and
then we have cloudfront which is an edge
caching service that helps customers get
our customers to read their application
with low latency so networking domain
helps with some of the following use
cases it controls and manages the
connectivity of the AWS services within
our account and we can also pick an IP
address range if you're a network
engineer or if you are somebody who
works in networks or are planning to
work a network you will soon realize the
importance of choosing your own IP
address for easy remembering so having
an option to have your own IP address in
the cloud owned Range of IP address in
the cloud it really helps really really
helps in Cloud networking the other
question that get asked would be the
difference between the availability Zone
and the region actually the question
generally gets asked so to test how well
you can actually differentiate and also
correlate the availability Zone and the
region relationship right so a region is
a separate geographic area like the
us2s-1 I mean which represents uh North
California or the AP South which
represents Mumbai so regions are a
separate geographic area on the contrary
availability Zone resides inside the
region you shouldn't stop there you
should go further and explain about
availability zones and availability
zones are isolated from each other and
some of the services will replicate
themselves within the availability zone
so availability Zone does replication
within them but regions they don't
generally do replication between them
the other question you could be asked is
what is auto scaling what have we
achieved by Auto scaling so in short
Auto scaling it helps us to
automatically provide and launch new
instances Whenever there is an demand it
not only helps us meeting the increasing
demand it also helps in reducing the
resource usage when there is low demand
so Auto scaling also allows us to
decrease the resources or resource
capacity as per the need of that
particular R now this helps business in
not worrying about putting more effort
in managing or continuously monitoring
the server to see if they have the
needed resource or not because Auto
scaling is gonna handle it for us so
business does not need to worry about it
and auto scaling is one big reason why
people would want to go and pick a cloud
service especially an AWS service the
ability to increase and Shrink based on
the need of that art that's how powerful
is auto scaling the other question you
could get asked is what's your targeting
in Cloud front now we know that
cloudfront is caching and it caches
content globally in the Amazon caching
service Global wide the whole point is
to provide users worldwide access to the
data from a very nearest server possible
that's the whole point in using or going
for cloudfront then what do you mean by
Geo targeting Geo targeting is showing
customer and specific content based on
language we can customize the content
based on what's popular in that place we
can actually customize the content the
URL is the same but we could actually
change the content a little bit not the
whole content otherwise it would be
dynamic but we can change the content a
little bit specific a file or a picture
or a particular Link in a website and
show customized content to users who
will be in different parts of the globe
so how does it happen cloudfront will
detect the country where the viewers are
located and it will forward the country
code to the origin server and once the
origin server gets the specialized or
specific country code it will change the
content and it will send to the caching
server and it get cached there forever
and the user gets to view a Content
which is personalized for them for the
country they are in the other question
you could get asked is the steps
involved in using cloud formation or
creating a cloud formation or a backing
up an environment within cloud formation
template we all know that if there is a
template we can simply run it and it
Provisions the environment but there is
a lot more going into it so the first
step in moving towards infrastructure as
a code is to create the cloud formation
template which as of now supports Json
and yaml file format so first create the
cloud formation template and then save
the code in an S3 bucket S3 bucket
serves as the repository for our code
and then from the cloud formation call
the file in the S3 bucket and create a
stack and now cloud formation uses the
file so reads the file understands
services that are being called
understands the order understands how
they are connected with each other cloud
formation is actually an intelligent
service it understands the relation
based on the code it would understand
the relationship between the different
services and it would set an order for
itself and then would provision the
services one after the other let's say a
service has a dependency and the
dependent service the other service
which this service let's say serves A
and B service B is dependent on service
a let's say cloud formation is an
intelligent service it would provision
the resource a first and then would
provision resource B so what happens if
we inverse the order if we inverse the
order resource B first gets provision
and because it does not have dependency
chances that the cloud formation's
default behavior is that if something is
not provisioned properly if something is
not healthy it would roll back chances
that the environment provisioning will
roll back so to avoid that cloud
formation first Provisions all the
services that has or that's dependent on
that's depended by another service so it
Provisions those service first and then
Provisions the services that has
dependencies and if you are being hired
for a devops or you know if the
interviewer wanted to test your skill on
systems aside this definitely would be a
question in his list how do you upgrade
or downgrade a system with near zero
downtime now everybody's moving towards
zero downtime or near zero downtime all
of them want their application to be
highly available so the question would
be how do you actually upgrade or
downgrade a system with near zero
downtime now we all know that I I can
upgrade an ec2 instance to a better ec2
instance by changing the instance type
stopping and starting but stopping and
starting is gonna cause a downtime right
so that's you should be answering or you
shouldn't be thinking in those terms
because that's the wrong answer
specifically the interviewer wants to
know how do you upgrade a system with
zero downtime so upgrading system with
zero downtime it includes launching
another system parallely with the bigger
ec2 instance type over the bigger
capacity and install all that's needed
if you are going to use an Ami of the
old machine well and good you don't have
to go through installing all the updates
and installing all the application from
the Ami once you have launched it in a
bigger instance locally test the
application to see if it is working
don't put it on production yeah test the
application to see if it is working and
if the application works we can actually
swap if your server is behind and behind
the Route 53 let's say all that you
could do is go to Route 53 update the
information with the new IP address new
IP address of the new server and that's
going to send traffic to the new server
now so the cutover is handled or if
you're using static IP you can actually
remove the static IP from the old
machine and assign it to the new machine
that's one way of doing it or if you are
using elastic Nik card you can actually
remove the new card from the old machine
and attach the new cards to the new
machine so that way we would get near
zero downtime if you're hired for an
architect level you should be worrying
about cost as well along with the
technology and this question would test
how well you manage cost so what are the
tools and techniques we can use in AWS
to identify and correct identify and
know that we are paying the correct
amount for the resources that we are
using our how do you get a visibility of
your AWS resources running one way is to
check the billing there's a place where
you can check the top services that were
utilized it could be free and it could
be paid Service as well top services
that can be utilized it's actually in
the dashboard of the cost Management
console so that table here shows the top
five most used services so looking at it
you can get it all right so I'm using a
lot of storage I'm using a lot of ec2
why is storage high you can go up and
try to justify that and you will find if
you are storing things that shouldn't be
storing then clean it up why is compute
capacity so high why is data transfer so
high so if you start thinking in those
levels you'll be able to dig in and
clean up unnecessary and be able to save
your bill and there are cost Explorer
services available which will help you
to view your usage pattern or view your
spin ending for the past 13 months or so
and it will also forecast for the next
three months now how much will you be
using if your pattern is like this so
that will actually help and will give
you a visibility on how much you have
spent how much you will be spending if
the trend continues budgets are another
excellent a way to control cost you can
actually set up budget all right this is
how much I am willing to spend for this
application for this team or for this
month for this particular resource so
you can actually put a budget Mark and
anytime it exceeds anytime it's nearing
you would get an alarm saying that well
we're about to reach the allocated
budget amount stuff like that that way
you can go back and know and you know
that how much the bill is going to be
for that month or you can take steps to
control bill amount for that particular
month so AWS budget is another very good
tool that you could use cost allocation
tags helps in identify find which team
or which resource has spent more in that
particular month instead of looking at
the bill as one list with no
specifications into it and looking at it
as an expenditure list you can actually
break it down and tag the expenditure to
the teams with cost allocation tags the
dev team has spent so much the
production team has spent so much the
training team has spent more than the
dev and the production team why is that
now you'll be able to you know think in
those levels only if you have cost
allocation tags now cost allocation tags
are nothing but the tags that you would
put when you create a resource so for
Production Services you would put as a
production you would create a production
tag and you would associate that
resources to it and at a later point
when you actually pull up your bill
that's going to show a detailed a list
of this is the owner this is the group
and this is how much they have used in
the last month and you can move forward
with your investigation and encourage or
stop users using more services with the
cost allocation tax the other famous
question is are there any other tools or
is there any other way of accessing AWS
resource other than the console console
is GUI right so in other words other
than GUI how would you use the AWS
resource and how familiar are you with
those tools and Technologies the other
tools that are available that we can
leverage and access the AWS resource are
of course footy you can configure Puri
to access the AWS resources like log
into an ec2 instance an ec2 instance
does not always have to be logged in
through the console you could use putty
to log into an ec2 instance and like the
jump box like the proxy machine and like
the Gateway machine and from there you
can actually access the rest of the
resources so this is an alternative to
the console and of course we have the
AWS CLI in any of the Linux machines or
Windows machines we can install so
that's 0.23 and 4. we can install AWS
CLI for Linux Windows also for Mac so we
can install them and from there from
your local machine we can access run AWS
commands and access provision monitor
the AWS resources the other ones are we
can access the AWS resource
programmatically using AWS SDK and
Eclipse so these are a bunch of options
we have to use the AWS resource other
than the console if you are interviewed
in a company or buy a company that
focuses more on security and want to use
AWS native services for their security
then you will come across this question
what services can be used to create a
centralized logging solution the basic
Services we could use are cloudwatch
logs store them in S3 and then use
elasticsearch to visualize stem and use
Kinesis to move the data from S3 to
elasticsearch right so log management it
actually helps organizations to track
the relationship between operational and
security changes and the events that got
triggered based on those logs instead of
logging into an instance or instead of
logging into the environment and
checking the resources physically I can
come to a fair conclusion by just
looking at the logs every time there's a
change the system will scream and it
gets tracked in the cloud watch and then
Cloud watch pushes it to S3 Kinesis
pushes the data from S3 to elasticsearch
and I can do a time-based filter and I
would get an a fair understanding of
what was going on in the environment for
the past one hour or whatever the time
window that I wanted to look at so it
helps in getting a good understanding of
the infrastructure as a whole all the
logs are getting saved in one place so
all the infrastructure logs are getting
saved in one place so it's easy for me
to look at it in an infrastructure
perspective so we know the services that
can be used and here are some of the
services and how they actually connect
to each other it could be logs that
belongs to a one account it could be
logs that belongs to multiple accounts
it doesn't matter you know those three
services are going to work fairly good
and they're gonna inject or they're
gonna like suck logs from the other
accounts put it in one place and help us
to monitor so as you see you have Cloud
watch here that actually tracks the
metrics you can also use cloudtrail if
you want to log API calls as well push
them in an S3 bucket so there are
different types of blog flow logs are
getting captured in an instance
application logs are getting captured
from the same VPC from a different VPC
from the same account from a different
account and all of them are analyzed
using elasticsearch using the kibana
client so step one is to deploy the ECS
cluster step two is to restrict access
to the ECS cluster because it's valid
data you don't want anybody to put their
hands and access their data so resting
access to the ECS dashboard and we could
use Lambda also to push the data from
cloud watch to The elasticsearch Domain
and then kibana is actually the
graphical tool that helps us to
visualize the logs instead of looking at
log as just statements or a bunch of
characters a bunch of files kibana helps
us to analyze the logs in a graphical or
a chart or a bar diagram format again in
an interview the interview is more
concerned about testing your knowledge
on AWS security products especially on
the logging monitoring even management
or Incident Management then you could
have a question like this what are the
native AWS security logging capabilities
now most of the services have their own
logging in them like have their own
logging like S3 S3 has its own login and
cloudfront has its own logging DS has
its own logging VPC has its own logging
in additional there are account level
logins like a cloud trail and AWS config
services so there are variety of logging
options available in the AWS like Cloud
soil config cloudfront redshift logging
RDS logging VPC flow logs S3 object
logging S3 access logging stuff like
that so we're going to look at two
servers in specific cloudtrail now this
cloud trail the very first product in
that picture we just saw the cloudtrail
provides an very high level history of
the API calls for all the account and
with that we can actually perform a very
good security analysis a security
analysis of our account and these logs
are actually delivered to you can
configure it they can be delivered to S3
for long time archivals and based on a
particular event it can also send an
email notification to us saying hey just
got this error thought I'll let you know
stuff like that the other one is config
service now config service helps us to
understand the configuration changes
that happened in our environment and we
can also set up notifications based on
the configuration changes so it records
the cumulative changes that are made in
a short period of time so if you want to
go through the lifetime of a particular
resource what are the things that
happened what are the things it went
through they can be looked at using AWS
config all right the other question you
could get asked is if you know your role
includes taking care of cloud security
as well then the other question you
could get asked is the native services
that Amazon provides automaticate DDOS
which is denial of service now not all
companies would go with Amazon native
services but there are some companies
which want to stick with Amazon native
Services just to save them from the
headache of managing the other softwares
or bringing in another tool a
third-party tool into managing DDOS they
simply want to stick with Amazon
proprietary Amazon native services and a
lot of companies are using Amazon
service to prevent DDOS denial of
service now denial of service is if you
already know what denial of service is
well and good if you do not know then
let's know it now denial of service is a
user trying to or maliciously making
attempt to access a website or an
application the user would actually
create multiple sessions and he would
occupy all the sessions and he would not
let legimate users access the service so
he's in turn denying the service for the
user a quick picture review of what
denial of services now look at it these
users instead of making one connection
they are making multiple connections and
there are cheap software programs
available that would actually trigger
connections from different computers in
the internet with different Mac
addresses so everything kind of looks
legitimate for the server and it would
accept those connections and it would
keep the sessions open the actual users
won't be able to use them so that's
denying the service for the actual users
denial of service all right and
distributed denial of service is
generating attacks from multiple places
you know from a distributed environment
so that's distributed denial of service
so the tools the native tools that helps
us to provide event the denial of
service attacks in AWS is cloud shield
and web access firewall AWS web now they
are the major ones they are designed to
mitigate a denial of service if your
website is often bothered by denial of
service then we should be using AWS
Shield or AWS Waf and there are a couple
of other tools that all showed us when I
say that also does denial of service is
not their primary job but you could use
them for denial of service route 53's
purpose is to provide DNS cloudfront is
to provide caching elastic load balancer
elb's work is to provide load balancing
VPC is to create and secure a virtual
private environment but they also
support mitigating denial of service but
not to the extent you would get in AWS
shield and AWS web so AWS shield and Waf
are the primary ones but the rest can
also be used to mitigate distributed
denial of service the other tricky
question is this actually will test your
familiarity with the region and the
services available in the region so when
you're trying to provision a service in
a particular region you're not seeing
the service in that region how do we go
about fixing it or how do we go about
using the service in the cloud it's a
tricky question and if you have not gone
through such situation you can totally
blow it away you really need to have a
good understanding on regions the
services available in those regions and
what a particular service is not
available how to go about doing it the
answer is not all services are available
in all regions anytime Amazon announces
a new service they don't immediately
publish them on all regions they start
small and as in when the traffic
increases as and when it becomes more
likeable to the customers they actually
move the service to different regions so
as you see in this picture within
America North Virginia has is more
services compared to Ohio or Compared to
North California So within not America
itself North Virginia is the preferred
one so similarly there are preferred
regions within Europe Middle East and
Africa and prefer regions within Asia
Pacific so anytime we don't see a
service in a particular region chances
that the service is not available in
that region yet we got to check the
documentation and find the nearest
region that offers that service and
start using the service from that region
now you might think well if I'm looking
for a service in Asia let's say in
Mumbai and if it is not available why
not simply switch to North Virginia and
start using it you could but you know
that's going to add more latency to your
application so that's why we need to
check for application which is check for
region which is very near to the place
where you want to serve your customers
and find nearest region instead of
always going back to North Virginian
deploying an application in North
Virginia again let's a place there's a
link in aws.com that you can go and look
for services available in different
region and that's exactly what you're
seeing here and if your service is not
available in a particular region switch
to the other region that provides your
service the nearest other region that
provides that service and start using
service from there with the coming up of
cloud a lot of companies have turned
down their monitoring team instead they
want to go with the monitorings that
cloud provides you know nobody wants to
or at least many people don't want to go
through the hassle of at least new
startups and new companies that are
thinking of having a monitoring
environment that they don't want to go
with traditional not monitoring instead
they would like to leverage AWS
monitorings available because it
monitors a lot of stuff not just the
availability but it monitors a lot of
stuff like failures errors it also
triggers emails stuff like that so how
do you actually set up a monitor to
website how to set up a Monitor to
Monitor the website metrics in real time
in AWS the simple way anytime you have a
question about monitoring cloudwatch
should strike your mind because
cloudwatch is meant for monitoring it is
meant for collecting metrics is is meant
for providing graphical representation
of what's going on in a particular
Network at a particular point of time so
cloudwatch cloudwatch helps us to
monitor applications and using
cloudwatch we can monitor the state
changes not only the state changes the
auto scaling life cycle events anytime
there are more services added there is a
reduction in the number of servers
because of less usage and very
informative messages can be received
through cloudwatch any cloudwatch can
now support scheduled events if you want
to schedule anything cloudwatch has an
event that would schedule an action all
right schedule a Trigger Time based not
incident based you know anything
happening and then you get an action
happening that's incident based on the
other hand you can simply schedule few
things on time based so that's possible
with Cloud watch So This Cloud watch
integrates very well with a lot of other
services like notifications for
notifying the user or for notifying the
administrator about it and it can
integrate well with Lambda so to trigger
an action anytime you're designing an
auto healing environment This Cloud
watch can actually monitor and send an
email if we are integrating it with SNS
simple notification service or this
Cloud watch can monitor and based on
what's Happening it can trigger an event
in Lambda and that would in turn run a
function till the environment comes back
to normal so cloudwatch integrates well
with a lot of other AWS Services all
right so cloudwatch has three statuses
green when everything is going good
hello when the service is degraded and
red when the service is not available
green is good so we don't have to do
anything about it but anytime there's an
Lo the picture that we're looking at
it's actually calling an Lambda function
to debug the application and to fix it
and anytime there's a red alert it
immediately notifies the owner of the
application about well this service is
down and here is the report that I have
here is the metrics that I've collected
about the service stuff like that if the
job role requires you to manage the
servers as well there are certain job
roles which are on the system side there
are certain job roles which is
development plus system side now you're
responsible for the application and the
server as well so if that's the case you
might be tested with some basic
questions like the different types of
virtualization and AWS and what are the
difference between them all right the
three major types of virtualization are
hvm which is Hardware virtual machine
the other one is PV para virtualization
and the third one is PV on hvm para
virtualization on Hardware virtual
module all right the difference between
them are actually describing them is
actually the difference between them hvm
it's actually a fully virtualized
Hardware you know the whole Hardware is
virtualized and all virtual machines act
separate from each other and these VMS
are booted by executing Master boot
record in the root block and when we
talk about para virtualization paragraph
is actually the special boot loader
which boots the PV Amis and when we talk
about PV on hvm it's it's actually the
marriage between hvm and PV and this
para virtualization on hvm in other
words PV on hvm it actually helps
operating system take advantage in
storage and the network input output
available through the host another good
question is name some of the services
that are not region specific now you've
been thought that all services are
within a region and some services are
within an availability zone for example
ec2 is within an availability Zone EBS
is within an availability Zone S3 is
region specific dynamodb is region
specific stuff like that VPC is both
availability and region specific meaning
you know subnets are availability Zone
specific and vpc's region specific stuff
like that so you might have thought you
might have learned in that combination
but that could be some tricky questions
that tests you how well you have
understood the region non-region and
availability non-availability Services I
should say there are services that are
not region specific that would be IAM so
we can't have IAM for every availability
Zone and for every region which means
you know users will have to use one
username and password for one region and
anytime they switch to another region
they will have to use another username
and password that that's more work then
that's not a good design as well
authentication has to be Global so IM is
a global Service and which means it's
not region specific on the other hand
Route 53 is again a regional Pacific so
we can't have a Route 53 for every
region Route 53 is not a region specific
service it's a global Service and it's
one application users access from
everywhere or from every part of the
world so we can't have one URL or one
DNS name for each region if your
application is a global application and
then web application firewall works well
with cloudfront then cloudfront is a
region based service so the web
application firewall it's not region
specific service it's a global Service
and cloudfront is again a global Service
though you can you know cache content on
a continent and country basis it's still
considered a global Service right it's
not bound to any region so when you
activate cloudfront you're activating it
away from region or availability zone so
when you're activating a web application
firewall because it's not a region
specific service you're activating it
away from availability Zone and regions
so a quick recap I am users groups roles
and accounts they are Global Services
they can be used globally around 53
services are offered at Edge locations
and they are Global as well web
application firewall a service that
protects our web application from common
web exploits they are Global Services
well cloudfront cloudfront is global
content delivery Network CDN and they
are offered at Edge locations which are
a global Service in other words
non-region specific service or Beyond
region service all right this is another
good question as well in the project
that you are being interviewed if they
really want to secure their environment
using Nat or if they are already
securing their environment using Nat by
any of these two methods like Nat
Gateway or Nat instances you can expect
this question what are the difference
between a Nat Gateway and Nat instances
now they both saw the same thing right
so they're not two different Services
trying to achieve two different things
they both serve the same thing but still
they do have differences in them all
right on a high level they both achieve
providing nothing for the service behind
it but the difference comes when we talk
about the availability of it not Gateway
is a managed service by Amazon whereas
not instance is managed by us now I'm
talking about the third Point
maintenance here not Gateway is managed
by Amazon that instance is managed by us
and availability of nat Gateway is very
high and availability of nat instance is
less compared to the NAT Gateway because
it's managed by us you know it's on an
ec2 instance which could actually fail
and if it fails we'll have to relaunch
it but if it is not Gateway if something
happens to that service Amazon would
take care of reprovisioning it and
talking talking about bandwidth it can
burst up to 75 gigabits now traffic
through the night Gateway can burst up
to 75 gigabits but for that instance it
actually depends on the server that we
launch and if we are launching a T2
micro it barely gets any bandwidth so
there's a difference there and the
performance because it's highly
available because of the bigger pipe 75
gigabits now the performance of the NAT
Gateway is very high but the performance
of the NAD instance is going to be
average again it depends on the size of
the NAT instance that we pick and
billing a billing for Nat Gateway is the
number of gateways that we provision and
the duration for which we use the NAT
Gateway but billing for Nat instance is
number of instance and the type of
instance that we use of course number of
instance duration and the type of
instance that we use Security in that
Gateway cannot be assigned meaning it
already comes with full backed security
but but in that instance security is a
bit customizable I can go and change the
security because it's a server managed
by me or managed by us I can always
change the security well allow this
allow don't allow this stuff like that
size and load of the NAT Gateway is
uniform but the size and the load of the
NAT instance changes as per around
Gateway is a fixed product but in that
instance can be small instance can be a
big instance so the size and the load
through it varies
right the other question you could get
asked is what are the difference between
stopping and terminating an ec2 instance
now you will be able to answer only if
you have worked on environments where
you have your instance stopped and where
you have your instance terminated if you
have only used lab and are attending the
interview chances are that you might you
always lost when answering this question
it might look like both are the same
well stopping and terminating both are
the same but there is a difference in it
so when you stop an instance it actually
performs a normal shutdown on the
instance and it simply moves the
instance to the stop state but when you
actually terminate the instance the
instance is moved to this stop State the
EBS volumes that are attached to it are
deleted and removed and will never be
able to recover them again so that's a
big difference between stopping and
terminating an instance if you're
thinking of using the instance again
along with the data in it you should
only be thinking of stopping the
instance but you should be terminating
the instance only if you want to get rid
of that instance forever
if you are being interviewed for an
architect level position or a junior
architect level position or even an
Cloud consultant level position or even
in an engineering position this is a
very common question that get asked what
are the different types of ec2 instances
based on their cost or based on how we
pay them right they're all compute
capacity for example the different types
are on-demand instances Port instances
and reserved instances it kind of looks
the same they all provide the compute
capacity they all provide the same type
of Hardwares for us but if you are
looking at Cost saving or optimizing
cost in our environment we're going to
be very careful about which one are we
picking now we might think that well
I'll go with on-demand instance because
I pay on a per hour basis which is cheap
you know I can use them anytime I want
and anytime I don't want I can simply
get rid of it by terminating it you're
right but if the requirement is to use
the service for one year the requirement
is to use the service for three years
then you'll be wasting a lot of money
buying on-demand instances you'll be
wasting a lot of money paying on an
hourly basis instead we should be going
for reserved instance where we can
reserve the capacity for the complete
one year or complete three years and
save huge amount in buying reserved
instances alright so on-demand is cheap
to start with if you are only planning
to use it for a short while but if
you're planning to run it for a long
while then we should be going for
reserved instance that is what is cost
efficient so spot instance is cheaper
than on-demand instance and there are
different use cases for spot instance as
well so let's look at one after the
other the on-demand instance the
on-demand instance is purchased at a
fixed rate per hour this is very short
term and irregular workloads and for
testing for development on-demand
instance is a very good use case we
should be using on-demand for production
spot instance spot instance allows users
to purchase ec2 at a reduced price and
anytime we have more instances we can
always go and sell it in spot instances
I'm referring to anytime we have more
reserved instances we can always sell
them in spot instance catalog and the
way we buy Spot instance is we actually
put a budget this is how much I'm
willing to pay all right would you be
able to give service within this cost so
anytime the price comes down and meets
the cost that we have put in will be
assigned an instance and anytime the
price shoots up the instance will be
taken away from us but in case of
on-demand instances we have bought that
instance for that particular R and it
stays with us but with spot instances it
varies based on the price if you meet
the price you get the instance if you
don't meet the price it goes away to
somebody else and the spot instance
availability is actually based on supply
and demand in the market there's no
guarantee that you will get support
instance at all line all right so that's
that's a caveat there you should be
familiar with that's a caveat there you
should be aware when you are proposing
somebody that we can go for spot
instance and save money it's not always
going to be available if you want your
spot instance to be available to you
then we need to carefully watch the
history of the price of the spot
instance now how much was it last month
and how much was it how much is it this
month so how can I code or how much can
I code stuff like that so you got to
look at those history before you propose
somebody that well we're gonna save
money using spot instance on the other
hand reserved instance provide cost
savings for the company or we can opt
for reserved instances for you know one
year or three years there are actually
three types of reserved instances light
medium and heavy reserved instances they
are based on the amount that we would be
paying and cost benefit also depends
with reserved instance the cost benefit
also depends based on are we doing all
upfront or no upfront or partial payment
then split the rest as monthly payments
so there are many purchase options
available but overall if you're looking
at using an application for the next one
year and three years you should not be
going for on-demand instance you should
be going for reserved instance and
that's what gives you the cost benefit
and in an AWS interview sometimes you
might be asked you know how you interact
with the AWS environment are you using
CLI are you using console and depending
on your answer whether console or a CLI
the panelist put a score okay this
person is CLI specific this person is
console specific or this person has used
AWS environment through the SDK stuff
like that so this question tests whether
you are a CLI person or a console person
and the question goes like this how do
you set up SSH agent forwarding so that
you do not have to copy the key every
time you log in if you have used Puri
anytime if you want to log into an ec2
instance you will have to put the IP and
the port number along with that you will
have to map or we will have to map the
key in the Puri and this has to be done
every time that's what we would have
done in our lab environments right but
in production environment using the same
key or mapping the same key again and
again every time it's actually an hassle
it's concerned as a blocker so you might
want to Cache it you might want to
permanently add it in your putty session
so you can immediately log in and start
using it so here in the place where you
would actually map the private key
there's a quick button that actually
fixes or that actually binds your SSH to
your pretty instance so we can enable
SSH agent forwarding and that will
actually bind our key to the SSH and
next time when we try to log in we don't
have to always go through mapping the
key and trying to log in all right this
question what are Solaris and ax
operating systems are they available
with AWS that question generally gets
asked to test how familiar are you with
the Amis available how familiar are you
with ec2 how familiar are you with the
ec2 Hardwares available that basically
tests that now the first question or the
first thought that comes to your mind is
well everything is available with AWS
I've seen Windows I've seen in Ubuntu
I've seen Red Hat I've seen Amazon Amis
and if I don't see my operating system
there I can always go to Marketplace and
try them if I don't final Marketplace I
can always go to community and try them
so a lot of Amis available there are a
lot of operating systems available I
will be able to find Solaris and ax but
that's not the case Solaris and ax are
not available with AWS that's because
Solaris uses a different and Solaris
does not support the architecture does
not support public Cloud currently the
same goes for ax as well they run on
power CPU and not on Intel and as of now
Amazon does not provide Power machines
this should not be confused with HPC
which is a high performance Computing
should not be confused with that now
these are different Hardwares different
CPU itself that the cloud providers did
do not provide yet another question you
could get asked in organizations that
would want to automate the their
infrastructure using Amazon native
Services would be how do you actually
recover an ec2 instance or Auto recover
an ec2 instance when it fails oh well we
know that ec2 instances are considered
as immutable meaning irreparable we
don't spend time fixing bugs in an OS
stuff like that you know once an ec2
instance crashes like it goes on a OS
Panic or there are various reasons why
it would fail so we don't have to really
worry about fixing it we can always
relaunch that instance and that would
fix it but what if it happens at two
o'clock in the night what if it happens
that during a weekend when nobody is in
office now looking or monitoring those
instances so you would want to automate
that not only on a weekend or during
midnights but it's general practice good
to automate it so you could face this
question how do you actually automate an
ec2 instance once it fails and the
answer to that question is using Cloud
watch we can recover the instance so as
you see there is an alarm threshold a
set in Cloud watch and once the
threshold is met meaning if there is an
error if there is a failure if the ec2
instance is not responding for a certain
while we can set an alarm and once the
alarm is met let's say the CPU
utilization stayed high for 5 minutes
all right it's not taking any new
connections or the instance is not
pinging for five minutes or in this case
it's two minutes it's not pinging so
it's not going to respond connection so
in those cases you would want to
automatically recover that ec2 instance
by rebooting the instance all right now
look at this the take this action
section under the action so there we
have a bunch of options like recover
this instance meaning reboot the
instance so that's how we would recover
the other two options are beyond the
scope of the question but still you can
go ahead and apply just like I'm gonna
do it so the other option to stop the
instance that's very useful when you
want to stop instances that are having
low utilizations nobody's using the
system as of now you don't want them to
be running and wasting the cloud
expenditure so you can actually set an
alarm that stops the ec2 instance that's
having low utilization so somebody was
working in an instance and they left it
without or they forgot to shut down that
instance and it gets I mean they will
only use it again the next day money so
in between there could be like 12 hours
that the system is running idle nobody's
using it and you're paying for it so you
can identify such instances and actually
stop them when the CPU utilization is
low meaning nobody is using it the other
one is to terminate let's say you want
to give system to somebody temporarily
and you don't want them to hand the
system back to you all right this is
actually an idea in other words this is
actually the scenario so you hand over a
system to somebody and when they're done
they're done we can actually terminate
the system so you could instruct the
other person to terminate the system
when they're done and they could forget
and the instance could be running
forever or you can monitor the system
after the specified time is over and you
can terminate the system or best part
you can automate the system termination
so you assign a system to somebody and
then turn on this cloudwatch action to
terminate the instance when the CPU is
low for like two hours meaning they've
already left or CPU is low for 30
minutes meaning they've already left
stuff like that so that's possible and
if you're getting hired for an system
side architect or even on the sysop site
you could face this question what are
the common and different types of Ami
designs there are a lot of Ami designs
the question is the common ones and the
difference between them so the common
ones are the full back Amis and the
other one is just enough OS Ami je OS
Ami and the other one is hybrid type
Amis so let's look at the difference
between them the full backed Ami just
like the name says it's fully baked it's
ready to use Ami and this is the
simplest Ami to deploy can be a bit
expensive it can be a bit a cumbersome
because you'll have to do a lot of work
beforehand you could use the Amis a lot
of planning a lot of thought process
will go into it and the Ami is ready to
use right you hand over the Ami to
somebody and it's ready to use or if you
want to reuse the Ami it's already ready
for you to use so that's full baked Ami
the other one is just enough operating
system Ami just like the name says it
has I mean as you can also see in the
diagram or in the picture it covers a
part of the OS all bootstraps are
already packed properly and the security
monitoring logging and the other stuff
are configured at the time of deployment
or at the time you would be using it so
not much thought process will go in here
the only focus is on choosing the
operating system and what goes the
operating system specific agents or
bootstraps that goes into the operating
system that's all we worry about the
advantage of this is it's flexible
meaning you can choose to install
additional softwares at the time of
deploying but that's going to require an
additional expertise on the person who
will be using the Ami so that's another
overhead there but the advantage is that
it's kind of flexible I can change the
configurations during the time of
deployment the other one is the hybrid
Ami now the hybrid Ami actually falls in
between the fully baked Ami and just
enough operating system options so these
Amis have some features of the big type
and some features of the just enough OS
type so as you see the security
monitoring logging are packed in that
Ami and the runtime environments or
insta installed during the time of a
deployment so this is where the strict
company policies would go into the Ami
company policies like you got to lock
this you got to monitor this these are
the ports that generally gets open in
all the systems stuff like that so they
strictly go into the Ami and sits in an
AMF format and during deployment you
have the flexibility of choosing the
different runtime and the application
that sits in an ec2 instance another
very famous question you would face in
an interview is how can you recover
login to an ec2 instance to which you
lost the key well we know that if the
key is lost we can't recover it there
are some organizations that integrate
their ec2 instances with an ad that's
different all right so you can go and
reset the password in the ad and you
will be able to log into the new
password but here the specific tricky
question is you are using a key to log
in and how do you recover if you have
lost the key generally companies would
have made a backup of the key so we can
pick from the backup but here the
specific question is we have lost the
key literally no backups on the key at
all so how can we log in and we know
that we can't log into the instance
without the key present with us so the
steps to recover is that make the
instance use another key and use that
key to log in once the key is lost it's
lost forever we won't be able to recover
it you can't raise a ticket with Amazon
not possible they're not going to help
it's beyond the scope so make the
instance use another key it's only the
key that's the problem you still have
valid data in it you got to recover the
data it's just a key that's having the
problem so we can actually focus on the
key part alone and change the key and
that will allow us to log in so how do
we do it step by step procedure so first
verify the ec2 config service is running
in that instance if you want you can
actually beforehand install the ec2
config in that service or you can
actually make the easy to config run
through the console just couple of
button clicks and that will make the
easy to configure one in that easy to do
instance and then it detach the root
volume for that instance of course it's
going to require a stop and start to
detach the root volume from the instance
attach the root volume to another
instance as a temporary volume or it
could be a temporary instance that
you've launched only to fix this issue
and then login to that instance and to
that particular volume and modify the
configuration file configuration file
modify it to use the new key and then
move the root volume back to its
original position and restart the
instance and now the instance is going
to have the new key and you also have
the new key with which you can log in so
that's how we go ahead and fix it now
let's move on to some product specific
or S3 product specific questions a
general perception is S3 and EBS can be
used interchangeably and the interviewer
would want to test your knowledge on S3
and EVS well EBS uses S3 that's true but
they can't be interchangeably used so
you might face this question what are
some key differences between AWS S3 and
EBS well the differences are S3 is an
object store meaning you can't install
anything in it you can store drive files
but you can't actually install in it
it's not a file system but ABS is a file
system you can install Services I mean
install applications in it and that's
going to run stuff like that and talking
about performance S3 is much faster and
abs is super faster when accessing from
the instance because from the instance
if you need to access S3 you'll actually
have to go out through the internet and
access the S3 or S3 is an external
service very external service you'll
have to go through or you'll have to go
outside of your VPC to access S3 S3 does
not come under a VPC but EBS comes under
a VPC it's on the same VPC so you would
be able to use it kind of locally
compared to S3 EBS is very local so that
way it's going to be faster and
redundancy talking about redundancy of
S3 and abs S3 is a replicated the data
in S3 is replicated across the data
centers but EBS is replicated within the
data center meaning S3 is replicated
across availability zones EBS is within
an availability zone so that way
redundancy is a bit less in EVS in other
words duranancy is higher in S3 than EPs
and talking about security of S3 is3 can
be made private as well as public
meaning anybody can access S3 from
anywhere in the internet that's possible
with S3 but EBS can only only be
accessed when attached to an ec2
instance right just one instance can
access it whereas S3 is publicly
directly accessible the other question
related to S3 security is how do you
allow access to a user to a certain a
user to a certain bucket which means
this user is not having access to S3 at
all but this user needs to be given
access to a certain bucket how do we do
it the same case applies to servers as
well in few cases there could be an
instance where a person is new to the
team and you actually don't want them to
access the production service now he is
in the production group and by default
he or she is granted access to that
server but you specifically want to deny
access to that production server till
the time he or she is matured enough to
access or understand the process
understand the do's and don'ts before
they can put their hands on the
production server so how do we go about
doing it so first we would categorize
our instances well these are critical
instances these are normal instances and
we were actually put a tag on them
that's how we categorize right so you
put a tag on them put attacks saying
well they are highly critical they are
medium critical and they are not
critical at all still there in
production stuff like that and then you
would pick the users who wants to or who
should be or should not be given access
to a certain server and you would
actually allow the user to access or not
access servers based on a specific tag
in other words you can use actually tags
in in the previous step we put tags on
the critical server right so you would
Define that this user is not going to
use this tag all right this user is not
allowed to use the resources for this
stack so that's how you would make your
step forward so you would allow or deny
based on the tags that you have put so
in this case he or she will not be
allowed to servers which are TAG
critical servers so that's how you allow
deny access to them the same goes for
bucket as well whereas if an
organization is excessively using S3 for
their data storage because of the
benefit that it provides the cost and
the durability you might get asked this
question which is organizations would
replicate the data from one region to
another region for additional data
durability and for having data
redundancy not only for that they would
also do that for Dr purposes for
Disaster Recovery if the whole region is
down you still have the data available
somewhere else and you can pick and use
it some organizations would store data
in different regions for compliance
reasons to provide low latency access to
their users who are local to that region
stuff like that so when companies do
replication how do you make sure that
there is consistency in the replication
how do you make sure that a replication
is not failing and the data gets
transferred for sure and there are logs
for that replication this is something
that the companies would use where they
are excessively using S3 and they're
fully relying on the replication in
running their business and the way we
could do it is we can set up a
replication monitor it's actually set of
tools that we could use together to make
sure that the cloud replication a region
level replication is happening properly
so this is how it happens now on this
side on the left hand side we have the
region one and on the right hand side we
have Region 2 and region 1 is the source
bucket and region two is the destination
bucket all right so object is put in the
source bucket and it has to go directly
to the region to bucket or made a copy
in the region 2 bucket and the problem
is sometimes it fails and there is no
consistency between them so the way you
would do it is connect these Services
together and create an cross replication
or cross region replication monitor that
actually monitors that actually monitors
your environment so there are cloudwatch
that make sure that the data is removed
no data is failing again there's Cloud
watch on the other end make sure that
the data is moving and then we have the
logs generated through cloudtrail and
that's actually written in dynamodb and
if there is an error if something is
failing you get notified through an SMS
or you get notified through an email
using the SNS service so that's how we
could leverage these tools and set up
and cross region replication monitor
that actually monitors your data
replication some common issues that
company companies face in VPC is that we
all know that I can use Route 53 to
resolve an IP address externally from
the internet but by default the servers
won't connect to the other servers using
our custom DNS name that it does not do
that by default so it's actually a
problem there are some additional things
that as an administrator or as an
architect or as a person who uses it you
will have to do and that's what we're
going to discuss so the question could
be VPC is not resolving the server
through the DNS you can access it
through the IP but not through the DNS
name and what could be the issue and how
do you go about fixing it and you will
be able to answer this question only if
you have done it already it's a quick
and simple step by default all VPC does
not allow that's the default feature and
we will have to enable the DNS hostname
resolution before now this is for the
custom DNS not for the default DNS that
comes along this is for the custom DNS
so we will have to enable the DNS
hostname resolution so our will have to
enable DNS hostname resolution so they
actually resolve let's say I want to
connect to a server one dot
simplylearn.com by default it's not
allowed but if I enable this option then
I will be able to connect to server one
simplylearn.com if a company has vpcs in
different regions and they have a head
office in a central place and the rest
of them are Branch offices and they are
connecting to the head office for Access
or you know for saving data or for
accessing certain files or certain data
or storing data all right so they would
actually mimic The Hub and spoke
topology where you have the VPC which is
centrally in an accessible region a
centrally accessible region and then you
would have a local vpcs or Branch
offices in different other regions and
they get connected to the VPC in the
central location and the question is how
do you actually connect the multiple
sites to a VPC and make communication
happen between them by default it does
not do that we know that vpcs they need
to be paired between them in order to
access the resources let's look at this
picture right so I have like a customer
Network or Branch offices in different
parts and they get connected to a VPC
that's fine so what we have achieved is
those different offices the remote
offices they are connecting to the VPC
and they're talking but they can't
connect or they can't talk to each other
that's what we have built but the
requirement is the traffic needs to or
they should be able to talk to each
other but they should not have direct
connection between them which means that
they will have to come and hit the VPC
and then reach the other customer
Network which is in Los Angeles or which
is in New York right that's the
requirement so that's possible with some
architecting in the cloud so that's
using VPN Cloud Hub you look at this
dotted lines which actually allows
customers or which actually allows the
corporate networks to talk to each other
through the VPC Again by default it
doesn't happen cloudhub is an
architecture that we should be using to
make this happen and what's the
advantage of it as a central office or
as the headquarters office which is in
the VPC or headquarters data center
which is in the VPC you have control or
the VPC has control on who talks to who
and what traffic can talk to I mean what
traffic can be routed to the other head
office stuff like that that centralized
control is on the VPC the other question
you could get asked is name and explain
some security products and features
available in VPC well VPC itself is a
security service it provides security
service to the application but how do
you actually secure the VPC itself
that's the question and yes there are
products that can actually secure the
VPC or the VPC delivers those products
to secure the application access to the
vpcs restricted through a network access
control list all right so that's a
security product in VPC and a VPC has
security groups that protects the
instances from unwanted inbound and
outbound traffic and network access
control list protects the subnets from
unwanted inbound and outbound access and
there are flow logs we can capture in VP
scene that captures incoming and
outgoing traffic through a VPC which
will be used for later analysis as in
what's the traffic pattern what's the
behavior of the traffic pattern stuff
like that so there are some security
products and features available in VPC
now how do you monitor VPC VPC is a very
important concept very important Service
as well everything sits in a VPC most of
the service sits in a VPC except for
Lambda and S3 and dynamodb and couple of
other services most of them sit in a VPC
for security reasons so how do you
monitor your VPC how do you gain some
visibility on your VPC well we can gain
visibility on our VPC using VPC flow log
that's the basic Service as you see it
actually captures what's allowed what's
not allowed stuff like that which IPS
allowed which IP is not allowed stuff
like that so we can gather it and we can
use that for analysis and the other one
is cloud watch and Cloud watch logs the
data transfers that happen so this is
you know who gets allowed and who does
not get allowed I mean the flow logs is
who is allowed and who's not allowed
that kind of detail and Cloud watch
gives information about the data
transfer how much data is getting
transferred we can actually pick unusual
data transfers if there is a sudden hike
in the graph there's a sudden hike and
something happens at 12 on a regular
basis and you weren't expecting it
there's something suspicious it could be
valid backups it could be a malicious
activity as well so that's how you know
by looking at cloudwatch logs and
cloudwatch dashboard now let's talk
about multiple choice questions when
going for an interview you might
sometimes find yourself that the company
is conducting an online test based on
the score they can put you to a panelist
and then they would take it forward so
we thought we'll also include multiple
choice questions to help you better
handle such situation if you come across
all right when you find yourself in such
situation and the key to clear them is
to understand the question properly read
between the lines that's what they say
in know there can be like a big
paragraph with three lines or ten lines
you really got to understand what the
question is about and then try to find
answer for that question so that's a
come rule number one and then the second
rule is try to compare and contrast the
services mentioned or try to compare and
contrast the answers you can easily read
out one or two answers and then you will
be left with only two answers to decide
from you know so that also helps you
with time and that's how that also helps
you with some Precision in your answer
so number one read between the lines
number two compare and contrast the
services and you'll be able to easily
read out the wrong ones so let's try
answering this question suppose you are
a game designer and you want to develop
a game with a single digit millisecond
latency which of the following database
Services would you choose so we know
that the following are database services
are good enough all right and it talks
about about millisecond latency that's
the key point and the third thing is
it's a game it could be a mobile game
it's a game that you are trying to
design and you need the millisecond
latency and it has to be a database all
right so let's talk about the options
available RDS RDS is a database for sure
is it good for game design we'll come
back to that Neptune Neptune is a graph
a database service in Amazon so that's
kind of out of the equation and snowball
is actually a storage all right it's
it's a transport medium I would say so
that's again out of the equation so the
tie is between RDS and dynamodb if we
need to talk about RDS RDS is an a
platform as a service it provides cost
efficient resizable capacity but it's an
SQL database meaning the tables are kind
of strict you know it's good for Banking
and other type of applications but not
really good for anything that has to do
with the gaming so the only option left
is dynamodb again it's the right answer
dynamodb is actually an fast and
flexible nosql database service and it
provides a single digit millisecond
latency at any scale and it's a database
at the same time it's a key Value Store
model database so the right answer is
dynamodb all right let's look at the
next question if you need to perform
real-time monitoring of AWS services and
get actionable insights which service
would you use all right let's list the
services so it talks about real-time
monitoring a firewall manager what does
it provide now firewall manager is not
really a monitor just like the name says
it's a manager it manages multiple
firewalls and AWS guard duty is an
thread detection service it does
monitoring it does continuously monitor
our environment but it monitors for
threats all right only threats now let's
talk about Cloud watch a cloud watch is
a service that helps to track metrics
it's a service that is used to monitor
the environment and give us a
system-wide visibility and also it helps
us to store logs so at the moment it
kind of looks like that could be the
right answer we don't know that yet but
I mean we have one more option left
that's EBS so what's EBS EBS is a block
storage elastic Block store if we
abbreviate EBS it's elastic Block store
so all three of them are easily out of
the question the first one is to manage
second one is to find threats of course
it does monitoring so there's I mean if
there is one relation between Cloud
watch and guard Duty that's monitoring
so easily we can actually find ourselves
slipped towards picking our duty but
know that God duties only for gaining
security inside but not about gaining
AWS service inside so cloudwatch is a
service that helps us to get a
system-wide or an AWS wide or an account
wide and it has number of metrics we can
monitor and get a very good Insight of
how a service is performing be it CPU be
it Ram beat Network utilization beat the
connection failures Cloud watch is a
service that helps us perform a
real-time monitoring and get some
actionable insights on the services all
right let's talk about this 33rd
question as a web developer you are
developing an app especially for the
mobile platform all right there is a
mention that this is especially for the
mobile platform so a lot of services
gets filtered out mobile platform right
which of the following lets you add user
sign up sign in and access control to
your web and mobile app quickly and
easily alright so this is all about
signing in to your mobile app so if we
need to read between the lines that's
how we can read sign up or sign in into
a mobile platform all right so we have
like four options here Shield AWS Massey
AWS inspector Amazon Cognito so let's
try to weed out Services which are not
relevant to it so what's AWS Shield AWS
Shield is actually a service that
provides a DDOS mitigation or DDOS
protection denial of service protection
it's a security feature let's talk about
the second option AWS Maxi is again a
security service that uses machine
learning to automatically discover and
classify the data it again talks about
security and this security is all about
encrypting or saving the data does not
come close with signing up an mobile
platform all right let's talk about the
other one AWS inspector now AWS
inspector has something to do with apps
it definitely has something to do with
apps so kind of looks like that as
relevant as of now so it actually helps
with improving the security and
compliance of the apps that we deploy in
the cloud so kind of looks like it could
be because it has to do with apps the
last one Cognito now Cognito is a
service that actually lets the
administrator to have control access
over web and mobile apps and it's a
service that helps us to sign up and
assign in to an mobile and web app so
that very much looks like we found the
answer so cognitive service that helps
web app and mobile app for sign up and
signing in and also gives the
administrator to have control over who
has I mean access control over the web
and the mobile app pretty much we found
it so it's cognitive Cognito is a
service that helps us to set up sign up
sign in and have access control over the
users who would be using our mobile and
web app all right how about this
question uh you are an ml engineer or
learning engineer who is on the lookout
for a solution that will discover
sensitive information that your
Enterprise stores in AWS and then uses
NLP to classify that data and provide
business related insights which among
the following Services would you choose
so we have a bunch of services that's
going to help us achieve or one of it is
going to help us achieve the about
requirement so it's a service that deals
with machine learning your machine
learning engineer who's looking for a
service that will help you to discover
information at your Enterprise store so
we're talking about storage discover
information in store and then classify
the data depending on severity the
sensitivity classify the data so which
service is that so firewall manager just
like the name says it's a manager and
the AWS IAM if we abbreviate it it's a
identity and access manage management so
it's identity and access management
nothing to do with identifying sensitive
data and managing it so the first two is
already out of the equation then the AWS
is Massey we already had a quick
definition description for AWS Massey
that it's actually a security service
that uses machine learning kind of looks
like it could be it it's a security
service that uses machine learning and
it discovers and classifies the
sensitive information not only that it
does not stop there it goes beyond and
protects the sensitive data AWS massive
it kind of looks like but we still have
one more option to look at which is
cloud HMS Cloud HMS is also a security
service kind of looks like that could be
the answer as well and it enables us to
generate encryption keys and save the
data so kind of 50 of it it's a security
service it encrypts helps us protect our
data but AWS Maxi is right on spot it's
a machine learning service it helps us
to classify the data and also to protect
the data so the answer for this question
would be AWS Massey so hope you kind of
get it how this is going so first we
apply the thumb rule identify the
question that's being asked read between
the lines and then try to find the
service that meets your requirement then
finding the service is by verse reading
out the wrong ones recollect everything
that you've learned about the service
and see how well that matches with those
hints that you have picked up and if
that doesn't match weed that out then
you'll end up with two just two to
decide from at some point and then it
becomes easy for you to decide click on
the question submit it and then move on
to the other question in your interview
all right so how about this one you are
a system administrator in your company
which is running most of its
infrastructure on AWS you are required
to track your users and keep a look on
how your users are being authenticated
all right so this is where the problem
statement starts right you need to keep
track of how your users are being
authenticated and you wish to create and
manage AWS users and use permissions to
allow and deny their access to the AWS
resources right you are to give them
permission number one and then I mean if
we put them in the right order first
giving them permissions and then
tracking their usage let's see which of
the service will help us achieve it IEM
is a service that helps us to looking at
the permissions we can actually predict
whether the user or the group will have
servers or not so that helps us to get a
track of who's able to use who's not
able to use certain servers and all that
stuff so it kind of looks like but we
have other three options left let's look
at AWS firewall manager just like the
name says it's actually a firewall
manager it helps us to manage multiple
firewalls simple as that and shield is a
service it's a service that's used to
protect denial of service or distributed
denial of service an API Gateway is a
service that makes it easy for
developers to create publish maintain
and monitor and secure API so I mean
it's completely on the API side very
Less on user and how you authenticate
your user we can get that by looking at
the name itself right if you abbreviate
it or if we if you try to find a
definition for the name API Gateway you
would get it it has to do with API but
if you upgrade AWS IAM its identity and
access management pretty much meets the
requirement but for the problem
statement about it's AWS identity and
access management that's the right
answer all right let's look at this one
if you want to allocate various private
and public IP address in order to make
them communicate with the internet and
other instances you will use this
service which of the following is this
service so it talks about using public
and private IP address so this service
uses IP address and then this service
helps us to allow love and deny
connections to the internet and to the
other instances so you get the question
is it let's pick the service that helps
us achieve it Route 53 Route 53 is
actually a DNS service right so it's not
a service that's used to allow or deny
no it does not do that VPC VPC uses
public and private IP address yes so
kind of looks like a VPC helps us to
allow I mean the security in VPC the
security group the network access
control list in a VPC the routing table
in a VPC that actually helps us to allow
or deny a connection to a particular IP
address or to a particular service
within the VPC or outside of the VPC so
as of now it kind of looks like it could
be but let's look at the other services
what if if we find a service that
closely matches to the above requirement
than the Amazon VPC Gateway API Gateway
we know that it's a managed service that
makes it easy for developers to create
publish maintain and monitor apis and
secure API so that has completely do
with API not with IP Cloud front we know
about cloudfront that it's a Content
delivery Network and it provides global
distribution of servers where our
content can be cached it could be video
or or bulk Media or anything else they
can be cached locally so users can
easily access them and download them
easily alright so that's Cloud front now
at this point after looking at all four
it looks like VPC is the right answer
and in fact VPC is the right answer VPC
has public IP address VPC can help us
with the private IP address VPC can be
used to allow deny connection based on
the security group Access Control list
and routing table it has so that's right
answer is VPC all right how about this
one this platform as a service or
platform as a DB service provides us
with a cost efficient and resizable
capacity while automating time consuming
administrative tasks so this question is
very clear it's a DB service we gotta
look for and it's a service that can
provide the automating some of the time
consuming tasks it has to be resizable
at the same time so let's talk about
Amazon rational database it's a database
kind of matches the requirement we can
resize it as and when needed
all right looks like it's a fit as of
now it actually automates some of the
time consuming work looks like it's a
fit as of now let's move on to elastic
cache and then try to see if that
matches the definition that we've
figured out about elastic cache it's
actually a caching service it's again an
in-memory data store which helps in
achieving high throughput and low
latency in memory data store so it's not
a full-blown database and it does not
come with any Amazon provisioned
Automation in it for automating any of
the administration tasks now it does not
come up with anything like that yeah we
can resize the capacity as and when
needed but automation it's not there yet
and moreover it's not a database so
that's out of the equation VPC is not a
resizable one you know once we have
designed VPC it's fixed it can be
resized so that's out of the equation
and uh Amazon Glacier Glacier is a
storage but not a database all right so
that's again of the equation so the tie
is kind of between Amazon rational
database service and Amazon elastic
cache because they both Aid the database
service but elastic cache is not a
full-blown database it actually helps
the database but it's not a full blown
database so it's Amazon relational
database so that's the one which is a
platform as a service it's the one which
can be resized it's the one which can be
used to automate the time consuming
administrative tasks all right let's
talk about this one which of the
following is a means for accessing human
researchers or Consultants to help solve
a problem on a contractual or a
temporary basis all right let's read the
question again which of the following is
a means for accessing human researchers
or consultant to help solve problems on
a contractual or a temporary basis it's
like assigning task or hiring AWS
exports for a temporary job so let's try
to find that kind of service in the four
services that are listed Amazon elastic
map reduce mapreduce is actually an
framework service that makes it easy and
cost effective to analyze large amount
of data but that has nothing to do with
accessing human researchers all right
let's talk about mechanical terms it's a
web service that provides a human
Workforce that's the definition for it
for example automation is good but not
everything can be automated for
something to qualify for automation it
has to be a repeated task a one-time
task can't be automated or the time and
money that you would be spending in
automation is not worth it instead you
could have done it manually so that does
not qualify for Automation and anything
that requires intelligence or anything
that's a special case all right
automation can do reperative tasks
automation can do precise work but it
has to be repeated tasks scenario you
know it should have been there already
only then that can be executed but if
it's a new scenario and it requires
appropriate addressing then it requires
human thoughts so we could hire
researchers and Consultants who can help
solve a problem using Amazon Mechanical
Turk the other two are already out of
the equation now Dev pay is actually a
payment system through Amazon and
multi-factor authentication as it says
it's an authentication system so the
right answer is Amazon Mechanical Turk
all right this sounds interesting let's
look at this one this service is used to
make it easy to deploy manage and scale
containerized applications using
kubernetes on AWS which of the following
is this AWS service so it's a service to
deploy manage and scale containerized
applications so it deals with containers
it also should have the ability to use
kubernetes which is a container
orchestration service all right the
first one an Amazon elastic container
service kind of looks like it's the one
the name itself has the word and the
relation we're looking for elastic
container service so this container
service is an highly scalable high
performance container orchestration
service let's look at the other one AWS
batch it's a service that enables ID
professionals to schedule and execute
batch processing I mean the name itself
says that that's meant for batch
processing elastic bean stock that's
another service that helps us to deploy
manage and scale but it helps us with
easy two instances not with
containerized instances so that's again
out of the equation would light scale be
a good tie for elastic container service
what's slide sale now light sale is a
service it's called as virtual private
server without a VPC it's called as a
virtual private server it comes with a
predefined in the compute storage
networking capacity it's actually a
server not a container right so at this
point that also becomes out of the
equation so it's Amazon elastic
container service that's the one that
helps us to easily deploy manage scale
container services and it helps us
orchestrate the containers using
kubernetes all right how about this one
all right this service lets us to run
code without provisioning or managing
servers so no servers run code select
the correct service from the below
option all right so no servers but we
should be able to run code Amazon ec2
Auto scaling easy to order scaling ec2
is elastic compute Cloud which is a
server and auto scaling is a service
that helps us to achieve scaling the
server so that's the definition for it
could be that's out of the equation AWS
Lambda now Lambda is a service It's
actually an event driven serverless
Computing platform and Lambda runs code
in response to the event that it
receives and it automatically manages
the compute resource that's required for
that code as long as we have uploaded a
code that's correct and setup events
correctly to map to that code it's going
to run seamlessly so that's about Lambda
it kind of looks like it could be the
answer because Lambda runs code we don't
have to manage servers it manages
service by itself but we can't conclude
as of now we have other two service to
talk about AWS batch all right batch is
service that enables ID professionals to
run batch job we know that and about the
inspector Amazon inspector it's actually
a service that helps us to increase and
identify any security issues and align
our application with compliance well
that's not the requirement of the
question the requirement and the
question was run code without
provisioning a server and without any
more space for confusion AWS Lambda is a
service or is the service that runs code
without provisioning managing Services
right the right one
all right let's get started so in an
environment where there's a lot of
automation infrastructure automation
you'll be posted with this question how
can you add an existing instance to a
new auto scaling group now this is when
you are taking an instance away from the
auto scaling group to troubleshoot to
fix a problem you know to look at logs
or if you have suspended the auto
scaling you know you might need to
re-add that instance to the auto scaling
group only then it's going to take part
in it right only then the auto scaling
is going to count it has part of it it's
not a straight procedure you know when
you remove them you know it doesn't get
automatically re-added I've had worked
with some clients when their developers
were managing their own environment they
had problems adding the instance back to
the auto scaling group you know
irrespective of what they tried the
instance was not getting added to the
auto scaling group and whatever they
fixed that they were provided or
whatever fix that they have have
provided we're not you know getting
encountered in the auto scaling group so
like I said it's not a straight you know
a click button procedure there are ways
we'll have to do it so how can you add
an existing instance uh to the auto
scaling group there are a few steps that
we need to follow so the first one would
be to under the ec2 instance console
right under the instance under actions
in specific you know there's an option
called attach to Auto scaling group
right if you have multiple Auto scaling
groups in your account or in the region
that you're working in then you're going
to be posted with the different Auto
scaling groups that you have in your
account let's say you have five Auto
scaling groups or five different
application you know you're going to be
posted with five different or scaling
groups and then you would select the
auto scaling the appropriate Auto
scaling group and attach the instance to
that particular Auto scaling group while
adding to the auto scaling group if you
want to change the instance type you
know that's possible as well sometimes
when you want to add the instance back
to the auto Skilling group there would
be requirement that you change the
instance type to a better one to a
better family to the better instance
type you could do that at that time and
after that you are or you have
completely added the instance back to
the auto scaling group so it's actually
an seven step up process adding an
instance back to the auto scaling group
in an environment where they're dealing
with migrating the instance or migrating
an application or migrating an instance
migrating and VM into the cloud you know
if the project that you're going to work
with deals with a lot of migrations you
could be posted this question what are
the factors you will consider while
migrating to Amazon web services the
first one is cost is it worth moving the
instance to the cloud given the
additional bills and whistles features
available in the cloud is this
application going to use all of them is
moving into to the cloud beneficial to
the application in the first place you
know beneficial to the users who will be
using the application in the first place
so that's a factor to think of so this
actually includes you know cost of the
infrastructure and the ability to match
the demand and Supply transparency is
this application in high demand you know
is it going to be a big loss if the
application becomes unavailable for some
time so there are few things that needs
to be considered before we move the
application to the cloud and then if the
application does the application needs
to be provisioned immediately is there
an urge is there an urge to provision
the application immediately that's
something that needs to be considered if
the application requires to go online if
the application needs to hit the market
immediately then we would need to move
it to the cloud because in on-premises
procuring or buying an infrastructure
buying the bandwidth buying the
switchboard you know buying an instance
you know buying their software as buying
the license related to it it's going to
take time at least like two weeks or so
before you can bring up an server and
launch an application in it right so if
the application cannot wait you know
waiting means uh you know Workforce
productivity loss is it so we would want
to immediately launch instances and put
application on top of it in those case
if your application is of that type if
there is an urge in making the
application go online as soon as
possible then that's a candidate for
moving to the cloud and if the
application or if the the software or if
the product that you're launching it
requires Hardware it requires an updated
Hardware all the time that's not going
to be possible in on premises we try to
deal with Legacy infrastructure all the
time in on-premises but in the cloud
they're constantly upgrading their hard
ways only then they can keep themselves
up going in the market so they
constantly the cloud providers are
constantly updating their hard ways and
if you want to be benefited of your
rough vacation wants to be benefited by
the constant upgrading of the Hardwares
making sure the hardware is as latest as
possible the software version the
licensing is as latest as possible then
that's a candidate to be moved to the
cloud and if the application does not
want to go through any risk if the
application is very sensitive to
failures if the application is very much
stacked to the revenue of the company
and you don't want to take a chance in
you know seeing the application fail and
you know seeing the revenue drop then
that's a candidate for moving to the
cloud and business agility you know
moving to the cloud at least half of the
responsibility is now taken care by the
provider in this case it's Amazon at
least half of the responsibility is
taken care by them like if the hardware
fails Amazon makes sure that they're
fixing the hardware immediately and
notifications you know if something
happens you know there are immediate
notifications available that we can set
it up and make yourself aware that
something has broken and we can
immediately jump in and fix it so you
see there are the responsibility is now
being shared between Amazon and us so if
you want to get that benefit for your
application for your organization for
the product that you're launching then
it needs to be moved to the cloud so you
can get that benefit from the cloud the
other question you could get asked is
what is RTO and RPO in AWS they are
essentially Disaster Recovery terms when
you're planning for Disaster Recovery
you cannot avoid planning disaster
recovery without talking about RTO and
RPO now what's the RTO what's the RPO in
your environment or how do you define
RTO how do you define RPO or some
general questions that get asked RTO is
recovery time objective RTO stands for
the maximum time the company is willing
to wait for the recovery to happen or
for the recovery to finish when an
disaster strikes so RTO is in the future
right how much time is it going to take
to fix and bring everything to normal so
that's RTO on the other hand RPO is
recovery Point objective which is the
maximum amount of data laws your company
is willing to accept as measured in time
RPO always refers to the backups the
number of backups the the frequency of
the backups right because when an outage
happens you can always go back to the
latest backup right and if the latest
backup was before 12 hours you have lost
the in between 12 hours of data data
storage right so RPO is the acceptable
amount if the company wants less RPO RPO
is 1R then you should be planning on
taking backups everyone up if RPO is 12
hours then you should be planning on
taking backups every 12 hours so that's
how RPO and RTO you know helps disaster
recovery the fourth question you could
get asked is if you would like to
transfer huge amount of data which is
the best option among snowball snowball
Edge and snowmobile again this is a
question that get asked if the company
is dealing with a lot of data transfer
into the cloud or if the company is
dealing with the migrating data into the
cloud I'm talking about a huge amount of
data data in petabytes snowball and all
of the snowball series deals with the
petabyte sized data migrations so there
are three options available as of now
AWS snowball is an data transport
solution for moving high volume of data
into and out of a specified AWS region
on the other hand AWS snowball Edge adds
additional Computing functions snowball
is simple storage and movement of data
and snowball Edge has a compute function
attached to it a snowmobile on the other
hand is an exabyte scale migration
service that allows us to transfer data
up to 100 petabytes that's like 100
000 terabytes so depending on the size
of data that we want to transfer from
our data center to the cloud we can hire
we can rent any of these three services
let's talk about some cloud formation
questions this is a classic question how
is AWS cloud formation different from
AWS elastic bean stock you know from the
surface they both look like the same you
know you don't go through the console
provisioning resources you don't you
know you don't go through CLI and
provision resources both of them
provision resources through click button
right but underneath they are actually
different Services they support they aid
different services so knowing that is
going to help you understand this
question a lot better let's talk about
the difference between them and this is
what you will be explaining to the
interviewer or the panelist so so the
cloud formation the cloud formation
service helps you describe and provision
all the infrastructure resources in the
cloud environment on the other hand
elastic bean stock provides an simple
environment to which we can deploy and
run application cloud formation gives us
an infrastructure and the elastic bean
stock gives us a small contained
environment in which we can run our
application and cloud formation supports
the infrastructure needs of many
different types of application like the
Enterprise application the Legacy
applications and any new modern
application that you want to have in the
cloud on the other hand the elastic bean
stock It's a combination of developer
tools they are tools that helps manage
the life cycle of a single application
so cloud formation in short is managing
the infrastructure as a whole and
elastic Beanstalk in short is managing
and running an application in the cloud
and if the company that you're getting
hired is using cloud formation to manage
their infrastructure using or if they're
using infrastructure or any of the
infrastructure as a code Services then
you would definitely face this question
what are the elements of an AWS cloud
formation template so it has a four or
five basic elements right and the
template is in the form of Json or in
yaml format right so it has parameters
it has outputs it has data it has
resources and then the format or the
format version or the file format
version for the cloud formation template
so parameter is nothing but it actually
lets you to specify the type of ec2
instance that you want the type of RDS
that you want all right so ec2 is an
umbrella RDS is an umbrella and
parameters within that easy do and
parameters within the rdas are the
specific details of the ec2 or the
specific details of the RDS service so
that's what parameters in a cloud
formation template and then the element
of the cloud formation template is
outputs for example if you want to
Output the name of an S3 bucket that was
created if you want to Output the name
of the ec2 instance if you want to
Output the name of some resources that
have been created instead of looking
into the template instead of you know
navigating through in the console and
finding the name of the resource we can
actually have them outputted in the
results section so we can simply go and
look at all the resources created
through the template in the output
section and that's what output values or
output does in the cloud formation
template and then we have resources
resources are nothing but what defines
what are the cloud components or Cloud
resources that will be created through
the is cloud formation template now ec2
is a resource RDS is a resource and S3
bucket is a resource elastic load
balancer is a resource and the NAT
Gateway is a resource VPC is a resource
so you see all these components are the
resources and the resource section in
the cloud formation defines what are the
AWS Cloud resources that will be created
through this cloud formation template
and then we have version a version
actually identifies the capabilities of
the template you know we just need to
make sure that it is of the latest
version type and the latest version is
0909 2010 that's the latest version
number you'll be able to find that on
the top of the cloud formation template
and that version number defines the
capabilities of the cloud formation
template so just need to make sure that
it's the latest all the time still
talking about cloud formation this is
another classic question what happens
when one of the results in a stack
cannot be created successfully well if
the resource in a stack cannot be
created the cloud formation
automatically rolls back and terminates
all the resources that was created using
the cloud formation template so whatever
resources that were created through the
cloud formation template from the
beginning let's say we have created like
10 resources and the 11th resource is
now failing cloud formation will roll
back and delete all the 10 resources
that were created previously and this is
very useful when the cloud formation
cannot you know go forward cloud
formation cannot create additional
resources because we have reached the
elastic IP limits elastic IP limit per
region is 5 right and if you have
already used five IPS and a cloud
formation is trying to buy three more
IPS you know we've hit the soft limit
till we fix that with Amazon cloud
formation will not be able to you know
launch additional you know resources and
additional IPS so it's going to cancel
and roll back everything that's true
with a missing ec2 Ami as well if an Ami
is included in the template and but the
Ami is not actually present then cloud
formation is going to search for the Ami
and because it's not present it's going
to roll back and delete all the
resources that it created so that's what
cloud formation does it simply rolls
back all the resources that it created I
mean if it sees a failure it would
simply roll back all the resources that
it created and this feature actually
simplifies the system administration and
layout Solutions built on top of AWS
cloud formation so at any point we know
that there are no orphan resources in
the in in our environment you know
because something did not work or
because there was an you know cloud
formation executed sum there are no
orphan resources in our account at any
point we can be sure that if cloud
formation is launching a resource and if
it's going to fail and it's going to
come back and delete all the resources
it's created so there are no orphan
resources in our account now let's talk
about some questions in elastic Block
store again if the environment deals
with a lot of automation you could be
thrown this question how can you
automate easy to backup using EBS it's
actually a six step process to automate
the ec2 backups we'll need to write a
script to automate the below steps using
AWS API and these are the steps that
should be found in the scripts first get
the list of instances and then and then
the script that we are writing should be
able to connect to AWS using the API and
list the Amazon ABS volumes that are
attached locally to the instance and
then it needs to list the snapshots of
each volume make sure the snapshots are
present and it needs to assign a
retention period for the snapshot
because over time the snapshots are
going to be invalid right once you have
some 10 latest snapshots any snapshot
that you have taken before that then
becomes invalid because you have
captured the latest and 10 snapshot
coverage is enough for you and then the
fifth point is to create a snapshot of
each volume create a new snapshot of
each volume and then delete the old
snapshot anytime a new snapshot gets
created the oldest snapshot of the list
needs to go away so we need to include
options we need to include scripts in
our script lines in our script that make
sure that it's deleting the older
snapshots which are older than the
retention period that we are mentioning
another question that you could see in
the interview be it written interview
beat online interview or beat and
telephonic or face to face interview is
what's the difference between EBS and
instance store let's talk about EBS
first EBS is kind of permanent storage
the data in it can be restored at a
later point when we save data in EBS the
data lives even after the lifetime of
the ec2 instance for example we can stop
the instance and the data is still going
to be present in EBS we can move the EBS
from one instance to another instance
and the data is simply going to be
present there so ABS is kind of
permanent storage when compared to
instance on the other hand instance
store is a temporary storage and that
storage is actually physically attached
to the host of the machine abs is an
external storage an instant store is
locally attached to the instance or
locally attached to the host of The
Machine we cannot detach an instant
store from one instance and attach it to
another but we can do that with EBS so
that's a big difference one is permanent
data and another one is ebs's permanent
instant store is a volatile data and
instant store with instant store we
won't be able to detach the storage and
attach it to another instance and
another feature of instant store is data
in an instant store is lost if the disk
fails or the instance is stopped or
terminated so instant store is only good
for storing cache data if you want to
store permanent data then we should
think of using EBS and not instant store
while talking about storage on the same
lines this is another classic question
how can you take backups of EFS like EBS
and if you can take backup how do you
take that backup the answer is yes we
can take EFS to EFS backup solution EFS
does not support snapshot like EBS EFS
does not support snapshot snapshot is
not an option for EFS elastic file
system right we can only take backup
from one EFS to another EFS and this
backup solution is to recover from
unintended changes or deletions of the
EFS and this can be automated right any
data that we store in EFS can be
automatically replicated to another EFS
and once this EFS goes down or gets
deleted or data gets deleted or you know
the whole EFS is for some reason
interrupted or deleted we can recover
the data from we can use the other EFS
and bring the application to consistency
and to achieve this it's not an One Step
configuration it's a cycle there are
series of steps that's involved before
we can achieve EFS to EFS backup the
first thing is to sign in to the AWS
Management console and under EFS or
click on EFS to EFS restore button from
the services list and from there we can
use the region selector in the console
navigation bar to select the actual
region in which we want to work on and
from there ensure that we have selected
the right template you know some of the
templates would be you know EFS to EFS
backup granular backups incremental
backups right so there are some template
the kind of backups that you want to
take do you want to take granular do you
want to take increment backups stuff
like that and then create a name to that
solution the kind of backup that we have
created and finally review all the
configurations that you have done and
click on Save and from that point
onwards the data is going to be copied
and from that point onwards any
additional data that you put is going to
copy it and replicate it now you have an
EFS to EFS backup this is another
classic question in companies which
deals with a data management there are
easy options to create snapshots but
deleting snapshots is not always and you
know click button or a single step
configuration so you might be facing a
question like how do you Auto delete old
snapshots and the procedure is like this
now as best practice we will take
snapshots of EBS volume to S3 all
snapshots get stored in S3 we know that
now and we can use AWS Ops automator to
automatically handle all snapshots the
Ops automator service it allows us to
create copy delete EBS snapshots so
there are cloud formation templates
available for AWS Ops automator and this
automator template will scan the
environment and it would take snapshots
it would you know copy the snapshot from
one region to another region if you want
I know if you're setting up a Dr
environment and not only that based on
the retention period that we create it's
going to delete the snapshots which are
older than the retention period so life
or managing snapshot is made a lot
easier because of this Ops automator
cloud formation template moving into
questions in elastic load balancer this
again could be an a question in the
interview what are the different types
of load balances in AWS and what's their
use case what's the difference between
them and as of now as we speak there are
three types of load balances which are
available in AWS the first one being
application load balancer just like the
name says the application load balancer
works on the application layer and deals
with the HTTP and https requests and it
it also supports path based routing for
example simplylearn.com
some web page simplylearn.com another
website so it's going to direct the path
based on the slash value that you give
in the URL so that's path based routing
so it supports that and not only that it
can support a port based a colon 8080
colon 8081 or colon 8090 you know based
on that Port also it can take a routing
decision and that's what application
load balancer does on the other hand we
have Network load balancer and the
network load balancer makes routing
decisions and the transport level it's
faster because it has very less thing to
work on it works on Lower OSI layer it
works on a lower layer so it has very
less information to work with than
compared with application layers so
comparatively it's a lot faster and it
handles millions of requests per second
and after the load balancer receives the
connection it selects a Target group for
the default rule using the flow hash
routing algorithm it does simple routing
right it does not do path based or Port
based routing it does simple routing and
because of it it's faster and then we
have classic load balancer which is kind
of expiring as we speak Amazon is
discouraging people using classic load
balancer but there are companies which
are still using classic load balancer
they are the ones over the first one to
step into Amazon when classic load
balancer was the first load balancer or
the only load balancer available at that
point so it supports HTTP https TCP SSL
protocol and it has a fixed relationship
between a load balance report and the
container Port so initially we only have
classic load balancer and then after
some point Amazon said instead of having
one load balancer address all type of
traffic we're gonna have a two load
balances called as the child from the
classic to load balancer and one is
going to specifically address the
application requirement and one is going
to specifically address the network
requirement and let's call it as
application load balancer and network
load balancer so that's how now we have
two different load balances talking
about load balance another classic
question could be what are the different
uses of the various load balancer in AWS
elastic a load balancing there are three
types of load balancer we just spoke
about it application load balancer is
used if we need a flexible application
management and TLS termination and
network load balancer if we require
Extreme Performance and the load
balancing should happen on based on
static IPS for the application and
classic load balancer is an old load
balancer which is for people who are
still running their Android government
from ec2 classic Network now this is an
older version of VPC or this is what was
present before VPC was created easy to
Classic network is what was present
before ec2 was created so there are the
three types and they are the use cases
of it let's talk about some of this
security related questions you would
face in the interview when talking about
security and firewall and AWS we cannot
avoid discussion talking about Waf web
application firewall and you would
definitely see yourself in this
situation where you've been asked how
can you use AWS Waf in monitoring your
AWS applications Waf or web application
firewall protects our web application
from common web exploits and Waf helps
us control which traffic Source your
application should be allowed or a Blog
which traffic from a certain Source now
which source or which traffic from a
certain Source should be allowed or
blocked your application with Waf we can
also create custom rules that blocks
common attack patterns you know if it is
a banking application it has a certain
type of attacks and if it is simple data
management data storage application it
has I mean content management
application it has a separate type of
attack So based on the application type
we can identify a pattern and create
rules that would actually block that
attack based on the rule that we create
and Waf can be used for three cases you
know the first one is allow all requests
and then block all requests and count
all requests for a new policy so it's
also an monitoring and Management
Service which actually counts all the
policies or counts all the requests that
matches a particular policy that we
create and some of the characteristics
we can mention in AWS web or the origin
IPS and the strings that appear in the
request we can allow block based on
Origin IP allow block based on strings
that appear in the request we can allow
of block or count based on the origin
country length of the request yeah we
can block and count the presence of
malicious scripts in an connection you
know we can count the request headers or
we can allow block a certain request
header and we can count the presence of
malicious SQL code in a connection that
we get and that want to reach our
application still talking about security
what are the different AWS IM categories
we can control using AWS IAM we can do
the following one is create and manage
IM users and once the user database gets
bigger and bigger we can create and
manage them in groups and in IM we can
use it to manage the security
credentials kind of setting the
complexity of the password you know
setting additional authentications you
know like MFA and you know rotating the
passwords you know resetting the
password there are few things we could
do with IAM and find finally we can
create policies that actually grants
access to AWS services and resources
another question you will see is what
are the policies that you can set for
your users password so some of the
policies that we can set for the user
password is at the minimum length or you
know the complexity of the password by
at least having one number or one
special characters in the password so
that's one and then the requirement of a
specific character types including you
know uppercase lowercase number and
non-alphabetic characters so it becomes
very hard for somebody else to guess
what the password would be and and try
to hack them so we can set the length of
the password we can set the complexity
in the password and then we can set an
automatic expiration of the password so
after a certain time the user is forced
to create a new password so the password
is not still old and easy to guess in
the in environment and we can also set
settings like the user should contact
the admin I mean when the password is
about to expire so you know you can get
a hold of how the user is setting their
password is it having good complexity in
it is it meeting company standards or
there are few things that we can control
and set for the users when the users are
setting or recreating the password
another question that could be posted in
an interview so to understand your
understanding of IEM is what's the
difference between an IM role and an IM
user let's talk about IM user let's
start small and then go big or let's
start simple and then talk about the
complex one the IM user has a permanent
long term credential and it's used to
interact directly with AWS services and
on the other hand IEM role is an IM
entity that defines a set of permissions
for making AWS service request so IM
user is an permanent credential and
enroll our temporary credentials and IM
user has full access to all AWS IEM
functionalities and with role trusted
entities such as IEM users application
or AWS Services assume are the role so
when an IM user is given an a permission
you know it sticks within the IM user
but with roles we can give permissions
to Applications we can give permissions
to users in the same account in a
different account the corporate ID we
can give permissions to ec2 S3 RDS VPC
and lot more role is wide and IM user is
is not so wide you know it's very
constrained only for that IM user let's
talk about manage policies in AWS manage
policies there are two types you know
customer managed and Amazon managed so
managed policies are IM resources their
Express permissions using the IAM policy
C language and we can create policies
edit them manage them manage them
separately from the IM user group and
roles which they are attached to so they
are something that we can do to managed
policies if it is customer managed and
we can now update policy in one place
and the permissions automatically extend
to all the attached entries so I can
have like three services four Services
point to a particular policy and if I
edit that particular policy it's going
to reflect on those three or four
services so anything that I allow is
going to be allowed for those four
Services anything that I denied is going
to be denied for the four Services
imagine what would be without the IM
managed policy will have to go and
specifically allow deny on those
different instances four or five times
depending on the number of instances
that we have so like I said there are
two types of managed policies one is
managed by us which is customer managed
policies and then the other is managed
by AWS which is aw WS managed policy
this question can you give an example of
an IM policy and a policy summary this
is actually to test how well-versed are
you with the AWS console the answer to
that question is look at the following
policy this policy is used to Grant
access to add update and delete objects
from a specific folder now in this case
name of the folder is example folder and
it's present in a bucket called example
bucket so this is an IAM policy on the
other hand the policy summary is a list
of access level resource and conditions
for each service defined in a policy so
IM policy is all about one particular
resource and the policy summary is all
about multiple resources with IM policy
it was only talking about S3 bucket and
one particular S3 bucket here it talks
about cloud formation template Cloud
watch logs ec2 elastic bean stock
Services summary summary of resources
and the permissions and policies
attached to them that's what policy
summary is all about another question
could be like this what's the use case
of IAM and how does IM help your
business two important or primary work
of IM is to help us manage IM users and
their access it provides a secure access
to multiple users to their appropriate
AWS resources so that's one it does and
the second thing it does is manage
access for Federated users Federated
users or non-iam users and through IAM
we can actually allow and provide a
secured access to resources in our AWS
account to our employees without the IM
user no they could be authenticated
using the active directory they could be
authenticated using the Facebook
credential Google credential Amazon
credential and a couple of other
credentials third party identity
management right so we could actually
trust them and we could give them access
to our account based on the trust
relationship that we have built with the
other identity systems right so two
things one is manage users and their
access for manage IAM user and their
access in our AWS environment and second
is manage access for Federated users who
are non-im users and more importantly IM
is a free service and with that will
only be charged for the use of the
resources not for the IM username and
password that we create all right let's
now talk about some of the questions in
Route 53 one classic question that could
be asked in an interview is what is the
difference between latency based routing
and Geo DNS or Geo based DNS routing now
the Geo based DNS routing takes routing
decisions on the basis of the geographic
location of the request and on the other
hand the latency based routing utilizes
latency measurements between networks
and data centers now latency based
routing is used where you want to give
your customers the lowest latency as
possible so that's when we would use
latency based routing and on the other
hand a Geo based routing is when we want
to direct customers to different
websites based on the country they are
browsing from you know you could have
you know two different or three
different websites for the same URL you
know take Amazon the shopping website
for example when we go to amazon.com
from in the US it directs us to the US
web page where the products are
different the currency is different
right and the flag and and couple of
other advertisements that shows up are
different and when we go to amazon.com
from India it gets directed to the
amazon.com
Indian site where again the currency the
product and the advertisements they're
all different right so depending on the
country they're trying to browse if you
want to direct customers to two or three
different websites we would use a
geo-based routing another use case of
geobase routing is if you have a
compliance that you should handle all
the DNS requests sorry if you should
handle all the requests you know from a
country within the country then you
would do geo-based routing now you
wouldn't direct the customer to a server
which is in another country all right
you would direct the customer to a
server which is very local to them
that's another use case of geo-based
routing and like I said for latency
based routing the whole goal or aim is
to achieve minimum end user latency if
you are hired for the architect role and
if that requires working a lot on the
DNS then you could be posted with this
question what is the difference between
domain and host Zone a domain is
actually a collection of data describing
a self-contained administrative and
Technical unit on the internet right so
for example you know simplylearn.com is
actually a domain on the other hand
hosted zone is actually an container
that holds information about how you
want to Route traffic on the internet to
a specific domain for example
lms.simplyleon.com is an hosted Zone
whereas simplylearn.com is an a domain
so in other words in hosted Zone you
would see the domain name plus and a
prefix to it LMS is a prefix here FTP is
a prefix mail.simplieron.com as a prefix
so that's how you would see a prefix in
hosted zones another question from
another classic question from Route 53
would be how does Amazon Route 53
provide High availability and low
latency the way Amazon Route 53 provides
High availability and low latency is by
by globally distributed DNS service
Amazon is a global Service and they have
DNA Services globally any customer doing
a query from different parts of the
world they get to reach an DNS server
which is very local to them and that's
how it provides low latency now this is
not true with all the DNS providers
there are DNS providers who are very
local to a country who are very local to
a continent so they don't they generally
don't provide low latency service right
it's always high latency it's low
latency for local users but anybody
browsing from a different country or a
different continent it's going to be
high latency for them but that's not
again true with Amazon Amazon is a
globally distributed DNS provider it has
DNS servers Global wide and like I said
it has optimal locations it has got
global Service or in other words it has
got servers around the globe different
parts in the globe and that's how they
are able to provide High availability
and because it's not running on just one
server but on many servers they provide
High availability and low latency if the
environment that you're going to work on
is going to take a lot of configuration
backups environmental backups then you
can expect questions in AWS config a
classic question would be how does AWS
config work along with AWS cloudtrail
AWS cloudtrail actually records user API
activity on the account and you know any
HTTP https access or any any sort of
access I know that's made to the cloud
environment that's recorded in the cloud
trail in other words any APA calls the
time is recorded the type of call is
recorded and what was the response given
was it a failure was it successful they
also get recorded in cloudtrail it's
actually a log it actually records the
activity in your Cloud environment on
the other hand config is an a point in
time configuration details of your
resources for example at a given point
what are all the resources that were
present in my environment what are all
the resources or what are the
configuration in those resources at a
given point they get captured in AWS
config right so with that information
you can always answer the question what
did my AWS resource look like at a given
point in time that question gets
answered when we use AWS config on the
other hand with cloudtrail you can
answer the question I mean by looking at
the cloud trail or with the help of
cloudtrail you can easily answer the
question who made an APA call to modify
this resource that's answered by
cloudtrail and with cloudtrail we can
detect if a security group was
incorrectly configured and who did that
configuration let's say there happened
to be in downtime and you want to
identify let's say there happened to be
a downtime and you want to identify who
made that change in the environment you
can simply look at Cloud Trail and find
out who made the change and if you want
to look at how the environment looks
like before the change you can always
look at AWS config can AWS configure or
AWS config aggregate data across
different AWS accounts yes it can now
this question is actually to test
whether you have used AWS config or not
I know some of the services are very
local is it some of these services are
availability Zone specific some of them
are Regional specific and some of them
are Global Services in Amazon and though
some of the services are region Services
you still can do some changes you know
add some configuration to it and collect
Regional data in it for example S3 is a
regional service but still you can
collect logs from all other regions into
an S3 bucket in one particular region
that's possible and cloud trail is and
Cloud watch is an regional service but
still you can with some changes to it
with some adding permissions to it you
can always monitor the cloud watch that
belongs to cloudwatch logs that belongs
to other regions you know they're not
Global by default but you can do some
changes and make it Global similarly AWS
config is a service that's a
region-based service but still you can
make it act globally you can aggregate
data across a different region and
different accounts in an AWS config and
deliver the updates from different
accounts to one S3 bucket and can access
it from there AWS config also works or
integrates seamlessly with SNS topic so
you know anytime there is a change
anytime a new data gets collected you
can always notify yourself or notify a
group of people about the new log or the
new config or new edit that happened in
the environment let's look at some of
the database questions you know database
should be running on reserved instances
all right so whether you know that fact
or not the interviewer wants to
understand how well you know that fact
by asking this question how are reserved
instance is different from on-demand DB
instances reserved instances and
on-demand instances are exactly the same
when it comes to their function but they
only differ based on how they are built
reserved instances are purchased for one
year or three year reservation and in
return we get a very low per hour
pricing because we're paying upfront
it's generally said that reserved
incidence is 75 percentage cheaper than
on-demand instance and Amazon gives you
that benefit because you know you're
committing for one year and sometimes
you're paying in advance for the whole
year on the other hand on-demand
instances are built on an hourly hourly
price talking about Auto scaling how
will you understand the different types
of Auto scaling the interviewer might
ask this question which type of scaling
would you recommend for RDS and why the
two types of scaling as you would know
now vertical and them horizontal and in
vertical scaling we can vertically scale
up the master database with a couple of
clicks all right so that's vertical
scaling vertical scaling is keeping the
same node and making it bigger and a
bigger if previously it was running on
T2 micro now we would like to run it on
M3 two times large instance previously
it had one virtual CPU one gigabit now
it's gonna have eight virtual CPU and 30
gigabit of ram so that's vertical
scaling on the other hand horizontal
scaling is adding more nodes to it
previously it was running on one VM now
it's going to run on 2 3 10 VMS right
that's horizontal scaling so database
can only be scaled vertically and there
are 18 different types of instances we
can resize our rds2 right so this is
true for RDS MySQL postgres SQL mariadb
Oracle Microsoft SQL servers there are
18 type of instances we can and
vertically scale up to on the other hand
horizontal scaling are good for replicas
so they are read-only replicas we're not
going to touch the master database we're
not going to touch the primary database
but I can do horizontal scaling only
with Amazon Aurora and I can add
additional read replicas I can add up to
15 Read replicas for Amazon Aurora and
up to five a read replicas for RDS MySQL
postgres SQL and mariadb RDS instances
and when we add replica we are
horizontally scaling adding more nodes
right I read only nodes so that's
horizontal scaling so how do you really
decide that between vertical and
horizontal scaling if you are looking
into increase the storage and the
processing capacity we'll have to do a
vertical scaling if you are looking at
increasing the performance or of the
read heavy database we need to be
looking for horizontal scaling or we
need to be implement commenting
horizontal scaling in our environment
still talking about database this is
another good question you can expect in
the interview what is a maintenance
window and Amazon RDS will your DB
instance be available during the
maintenance event all right so this is
really to test how well you have
understood the SLA how well you have
understood the Amazon rdas the failover
mechanism of Amazon rdas stuff like that
so RDS maintenance window it lets you
decide when a DB instance modification a
database engine upgrades or software
patching has to occur and you you
actually get to decide should it happen
at 12 in the night or should it happen
at afternoon should it happen early in
the morning should it happen in the
evening you actually get to decide an
automatic scheduling by Amazon is done
only for patches that are security and
durability related sometimes Amazon
takes down and does automatic scheduling
if you know if there is a need for for a
patch update that deals with security
and durability and by default the
maintenance window is is for 30 minutes
and the important point is the DB
instance will be available during that
event because you're going to have
primary and secondary right so when that
upgrade happens Amazon would shift the
connection to the secondary do the
upgrade and then switch back to the
primary another classic question would
be what are the consistency models in
dynamodb in dynamodb there is eventual
consistency read this eventual
consistency model it actually maximizes
your read throughput and the best part
with eventual consistency is all copies
of data reach consistency within a
second and sometimes when you write and
when you're you know trying to read
immediately chances that you you would
still be reading the old data that's
eventual consistency on the other hand
there is another consistency model
called the the strong consistency you
are strongly consistent read where there
is going to be a delay in writing the
data you know making sure the data is
written in all places but it guarantees
one thing that is once you have done a
write and then you're trying to do a
read it's going to make sure that it's
going to show you the updated data not
the old data now you can be guaranteed
of it that it is going to show the
updated data and not the old data that's
strongly consistent still talking about
database talking about nosql dynamodb or
nosql database which is dynamodb and
Amazon you could be asked this question
what kind of query functionality does
dynamodb support dynamodb supports get
and put operation dynamodb supports or
dynamodb provides a flexible querying by
letting you query on non-primary key
attributes using Global secondary index
and local secondary indexes a primary
key can be be either a single attribute
partition key or a composite partition
sort key in other words a dynamodb
indexes a composite partition sort key
as a partition key element and a sort
key element and by holding the partition
key you know when doing a search or when
doing a query by holding the partition
key element constant we can search
across the sort key element to retrieve
the other items in that table and at the
composite partition sort key should be a
combination of user ID partition and a
timestamp so that's what the composite
partition sort key is made of let's look
at some of the multiple choice questions
you know sometimes some companies would
have an written test or an McQ type
online test before they call you for at
the first level or before they call you
for the second level so these are some
classical questions that companies asked
or companies ask in their multiple
choice online questions let's look at
this question as a developer using a
this pay-per-use service you can send
store and receive messages between
software components which of the
following is being referred here let's
look at it right we have AWS step
functions Amazon mq Amazon simple queue
service Amazon simple notification
service let's read the question again as
a developer using this pay-per-view
service so the service that we are
looking for is a pay-per-view service
you can send store and retrieve messages
between two software components kind of
like a queue there so what would be the
right answer it would be Amazon simple Q
service now Amazon simple Q service is
the one that's used to decouple the
environment it breaks the tight coupling
and then it introduces decoupling in
that environment by providing a queue or
by inserting a queue between two
software components let's look at this
other question if you would like to host
a real-time audio and video conferencing
application on AWS right it's an audio
and video conferencing application on
AWS This Server us provides you with a
secure and easy to use application what
is this service let's look at the
options they are Amazon chime Amazon
workspace Amazon mq Amazon appstream all
right you might tend to look at Amazon
appstream because it's real time and
video conference but it's actually for a
different purpose it's actually Amazon
chime that lets you create chat and
create a chat board and then collaborate
with the security of the AWS services so
it lets you do the audio it lets you do
the video conference all supported by
AWS security features it's actually
Amazon China let's look at this question
as your company's AWS solution architect
you are in charge of Designing thousands
of individual jobs which are similar
which of the following service best
serves your requirement AWS ec2 Auto
scaling AWS snowball AWS fargate AWS
batch let's read the question again as
you your company's AWS solution
architect you are in charge of Designing
thousands of individual jobs which are
similar it looks like it's batch service
let's look at the other options as well
AWS snowball is actually an storage
Transport service ec2 auto scaling is uh
you know in introducing scalability and
elasticity in the environment and AWS
far gate is container services AWS batch
is the one is being referred here that
actually runs thousands of individual
jobs which are similar AWS batch it's
the right answer let's look at the other
one you are a machine learning engineer
and you're looking for a service that
helps you build and train machine
learning models in AWS which among the
following are we referring to so we have
Amazon sagemaker and AWS deep lens
Amazon comprehend AWS device form let's
read the question again you are a
machine learning engineer and you're
looking for a service that helps you
build and train machine learning models
in AWS which among the following are
referred here the answer is Sage maker
it provides every developer and data
scientist with the ability to build
train and deploy machine learning models
quickly that's what sagemaker does now
for you to be familiar with you know the
products I would recommend you to you
know simply go through the product
description you know there's one page
available on Amazon that explains all
the products a quick neat and simple and
that really helps you to be very
familiar with you know what the product
is all about and what it is capable of
you know is it the DB service is it a
machine learning service or is it a
monitoring service is it a developer
service stuff like that so get that
information get that details before you
attend an interview and that should
really help to answer or face such
questions with a great confidence so the
answer is Amazon Sage maker because
that's the one that provides developers
and a data scientist the ability to
build the train and deploy machine
learning models quickly as possible all
right let's look at this one let's say
that you are working for your company's
ID team and you are designated to adjust
the capacity of the AWS resource based
on the incoming application and network
traffic how do you do it so what's the
service that's actually helps us to
adjust the capacity of the AWS resource
based on the incoming application let's
look at it Amazon VPC Amazon IAM Amazon
inspector Amazon elastic load balancing
Amazon VPC is a networking service
Amazon IAM is an username password
authentication Amazon inspector is a
service that actually does security
audit in our environment and Amazon
elastic load balancer is a service that
helps in scalability that's in one way
you know indirectly that helps in
increasing the availability of the
application right and monitoring it
monitoring you know how much requests
are coming in through the elastic load
balance we can actually adjust the
environment that's running behind it so
the answer is going to be Amazon elastic
load balancer all right let's look at
this question this cross-platform video
game development engine that supports PC
Xbox PlayStation IOS and Android
platforms allows developers to build and
host their games on Amazon's service so
we have Amazon game lift Amazon green
grass Amazon Lumberyard Amazon Sumerian
let's read the question again this
cross-platform video game development
engine that supports PC Xbox PlayStation
IOS and Android platforms allows
developers to build and host their games
on Amazon servers the answer is Amazon
lumber yard this lumber yard is an free
triple a gaming engine Depot integrated
with AWS and twitch with full source
this lumber yard provides a growing set
of tools that helps you create an
highest game quality applications and
they connect to a lot of games and vast
compute and storage in the cloud so it's
that service they are referring to let's
look at this question you are the
project manager of your company's Cloud
architect team you are required to
visualize understand and manage your AWS
cost and usage over time which of the
following service will be the best fit
for this we have AWS budgets we have AWS
cost Explorer we have Amazon workmail we
have Amazon connect and the answer is
going to be cost Explorer now cost
Explorer is an option you need the
Amazon console that helps you to
visualize and understand and even manage
the AWS cost over time who's spending
more who's spending less and what is the
trend what is the projected cost for the
coming month all these can be visualized
in AWS cost Explorer let's look at this
question you are a chief Cloud architect
at your company and how can you
automatically Monitor and adjust
Computer Resources to ensure maximum
performance and efficiency of all
scalable resources so we have a cloud
formation as a service we have AWS
Aurora as a solution we have AWS Auto
scaling and Amazon API Gateway let's
read the question again here the chief
Cloud architect at your company how can
you automatically Monitor and adjust
Computer Resources how can you
automatically Monitor and adjust
Computer Resources to ensure maximum
performance and efficiency of all
scalable resources this is an easy
question to answer the answer is auto
scaling right that's a basic service and
solution architect course is it auto
scaling is the service that helps us to
easily adjust Monitor and ensure the
maximum performance and efficiency of
all scalable resources it does that by
automatically scaling the environment to
handle the inputs let's look at this
question as a database administrator you
will use a service that is used to set
up and manage databases such as MySQL my
DB and postgres SQL which service are we
referring to Amazon Aurora Amazon
elastic cache AWS RDS AWS database
migration service Amazon Aurora is
Amazon's flavor of the RDS service and
elastic cache is is the caching service
provided by Amazon they are not
full-fledged database and database
migration service just like the name
says it helps to migrate the database
from on-premises to the cloud and from
one a database flavor to another
database flavor Amazon RDS is the
service is the console is the service is
the umbrella service that helps us to
set up manage a databases like MySQL
maridb and postgres SQL it's Amazon RTS
let's look at this last question a part
of your marketing work requires you to
push messages to onto Google Facebook
Windows and Apple through apis or AWS
Management console you will use the
following service so the options are AWS
cloudtrail AWS config Amazon chime AWS
simple notification service all right it
says a part of your marketing work
requires you to push messages it's
dealing with pushing messages to Google
Facebook Windows and Apple through apis
or AWS Management console you will use
the following service it's simple
notification service simple notification
service is a message pushing service and
sqs is pulling similarly SMS is pushing
right here it talks about a pushing
system that pushes messages to Google
Facebook Windows and Apple through API
and it's going to be a simple
notification system or a simple
notification
hello everyone
in today's session we are going to talk
about AWS best practices so this is the
agenda for today we will talk about
billing best practices we will have a
look at Best Practices for ec2 which is
the Amazon elastic computer Cloud we
will walk through best practices for IAM
which is identity and access management
then we will walk through the best
practices for S3 which is simple storage
service and then we will have a general
look at the security best practices from
the Amazon account perspective as well
as from the general Amazon web services
perspective now let's start with billing
best practices so billing essentially is
the core of every company every company
wants to keep billing to the minimum and
cost to the Lower Side so one of the
best things about Amazon is that it
gives you a good way of controlling your
costs so from the billing specific side
from the billing console side there are
a lot of things you can do as a best
practice and also reduce cost
so Amazon gives you something called
billing budgets now this is interesting
so they give you an option to actually
create a budget kind of thing wherein
you specify you want x amount of dollars
for the next budget
now whenever you hit that budget let's
say it's two hundred dollars you get an
email notification
now these budgets can be service
specific they can be specific to some
service like ec2 so if you want to have
a billing alert when the ec2 instances
of the account hit almost two hundred
dollars you can create that
one of the other best practices from
Amazon billing side is to give limited
access to IAM account users to billing
so by default Amazon does not allow
billing access to IAM users but you can
specifically allow them if you have got
some users who look at the billing
perspective of your account
now if you have a bill which crosses 10
to 15 and it's not possible to pay using
a single credit card or debit card
Amazon gives you an option to pay using
multiple credit cards so as a best
practice if your bill has crossed a big
amount and you are not able to pay using
a single credit card if you are a good
Enterprise it's always good to have two
or three credit cards added to your
account so that even in the case when
your one card can't fulfill the bill you
can have another account which
automatically lets you pay the bill so
that way you don't risk your
infrastructure your infrastructure is
always safe
now one of the other best practices is
to have Regional taxation enabled in
your account
for every country there's a different
taxation regime and the tax methodology
and tax rates are different so instead
of getting a notice from your income tax
department or to avoid being
non-compliant to taxes it's the best
practice to have the billing number so
every country has the specific unique
billing number which you can put into
the Amazon account and then it
automatically charges that or files
taxes from Amazon's behalf on that
number
so at the end of the year when you're
doing tax filing you can easily show
that you have paid x amount of money
from Amazon so you don't have to repay
that or if in your country the tax was
collected at the end of the year in a
Consolidated way you are more compliant
now let's say that an Enterprise which
has got multiple accounts and Amazon web
services so let's say for project a you
have a different account for Project B
you have a different account so you can
consolidate the billing of those if you
don't want to have a headache of having
multiple account billing you don't want
to have to separate credit cards for
every account so what you can do is you
can consolidate the billing of each and
every account into a single account that
would essentially give you a single
Consolidated bill so you can also
consolidate your taxes there
in that case you can have a single
cooperate credit card which can have a
lot of limit there are credit cards
which give you no limit virtually so you
can use those kind of credit cards now
coming to ec2 best practices
one of the best practices from ec2 side
is Ami hardening
so Amazon gives you Amazon machine
images which are exact snapshots of
servers now these machine images are
used to spin up new machines so these
are like seat files for new servers
now you just can't have a stock OS image
and use that as a base for your entire
infrastructure you need to have a
hardened Ami now when I say A hardened
Ami what I exactly mean is to have an
Ami which has got all the security
practices implemented into it
now as far as the Linux OS is concerned
an Ami which is hardened would have the
SSH Port changed SSH password-based
logins disabled security enhanced Linux
set to enabled if it's Centos if it's
Ubuntu maybe you can enable the firewall
and only allow port 22. the next thing
is VPC Port lockdown so apart from the
instant level firewall Port lockdown you
should also lock down ports specifically
to IP addresses if not possible to IP
addresses then at least you should make
sure that in your VPC all the security
groups have got only the specific ports
which are required by that server or
application open to specific IP
addresses
if you have to open something up to the
entire world make sure that you've got
adequate checks PPP inside your
application so you don't have a malware
user trying to do DDOS service attack
although Amazon has got a required
hardware in place to check the DDOS
attack but yes there can be still valid
amounts of attacks that go into your
application if you don't plan the
application correctly
one of the other things you should take
care when you are running instances on
ec2 is that you have private subnets for
everything other than the load balancer
or the web application tier
now this is important because you don't
want anyone to be directly able to
access your rediff CCC your database or
your internal backend systems
because most of these systems for
example let's say rediff don't have
authentication built into them so if you
have the port exposed anyone can go in
and pull all the keys
now from an application division
perspective you should make sure that
the application is divided into small
micro services
so it used to be known as service
oriented architecture SOA now the term
has changed and people call it
microservice architecture
so the basic idea behind microservice
architecture is that big monolithic
applications are divided into small
chunks of application which are exposed
over as a service to the internet to the
end users
now this helps you to scale them
individually and one of the best
advantages you get out of such a micro
service architecture is that even if one
of the applications is down the others
can actually still function without
affecting this one now this is very
important because in today's e-commerce
World a single minute of downtime can
cost millions of dollars of loss so when
you have Micro service architecture you
can scale vertically or horizontally
each microservice without worrying about
downtime
you should take care while designing
microservice architecture so that the
microservice architecture is independent
at every level
one of the mistakes which happens is
that you have a microservice
architecture at the web server or the
application server layer but then you
have a common database so how is it
going to affect the microservice
architecture in any way
I understand that it's very difficult to
have independent database microservice
architecture but yes there are companies
who do sync UPS of databases just to
make sure they have totally independent
microservices
now in such a scenario even if the
backend database of a particular
microservice is down the other
functionality can work
take an example let's say there is login
functionality which talks to the backend
database and gets to data from there
let's say there is another functionality
which lets you buy stuff both of them
should have independent databases and
there should be a sync up every minute
or there should be push-based mechanism
where there's sync up only when they're
in one of the DBS
now if you have both these microservices
having independent databases your logins
will still work even if your payments
are down or your payments will still
work if your login functionality is
experiencing some degraded performance
you have stock images on Amazon that
Amazon provides you environment-based
key or Keys which are there for every
instance so every machine you launch on
Amazon has got a key you can't just use
the username and password to log in
now the advantage of key is that you
don't have to provide clear text
passwords but you have to make sure that
for every environment so let's say you
got a production environment QA test
environment make sure every environment
has got a different key the reason is if
you have a single key across all the
environments and let's say you give a
developer access to one of the
environments he can automatically gain
access to each and every environment so
that's not a good thing right you don't
want developers to access production
environments now if you have 100 ec2
instances and you have users to manage
let's say you should make 10 users are
there on every machine which are 10
members of your Dev team so it's very
difficult to create manually let's say
you also want to do password expiry and
a lot of intelligent stuff on these
users you can actually use something
called ldap this ldap is basically
centralized authentication system which
lets you authenticate users across
machines
so you can change the password for a
user on one ldap server and it's
reflected across all the machines
so let's have a look at IAM best
practices
so IAM is identity and access management
service of Amazon and it allows you to
have fine gain controls over the access
now as a best practice for your teams
you should have various roles to find
each of those roles should be specific
to what each member is doing
let's say you have a read-only Dev role
assigned to your developers you can also
have devops engineer role which you
assign to your devops engineering team
members
you can assign roles to the finance team
which has billing access
now this gives you the flexibility to
easily assign policies or attach
policies to various users without having
to manually do them this is useful in a
scenario wherein you have a custom role
wherein you have a reporting engineer
which needs access to billing to create
reports of CPU utilization
now you can give them a specific access
by assigning specific policies
you can have new join ease Refreshers
who have joined your team have a
read-only access so that way they don't
mess up the account and they also have
got full access to the account so they
can go and they can see what the
environment's like they can read but
they can't change anything so that way
you Safeguard your account by any
accidental Mistakes by those freshers or
new joinies
you can have service specific access so
let's say you have an ec2 instance which
requires access to S3 buckets so you
actually create a role in IAM and assign
that specific role to ec2 instances
you can also have special service
specific roles
now these service specific roles have
got access to specific service of Amazon
and you can use this let's say in case
you want to give some developer access
to some Dev environment of elastic
Beanstalk
so you can create a role which has the
access and the policies attached for
elastic Beanstalk read-only access to
that specific region so that way you can
control a specific service and specific
action
now let's have a look at S3 best
practices S3 is simple storage service
and it's one of Amazon's most important
Services it's one of the most used
Services as well
now in case of S3 you should make sure
that you have contacts specific names
now these names can be the type of
function that S3 bucket is performing so
let's say images and then hyphen Dev and
then hyphen region
and then hyphen top level domain so you
have a unique name
so Amazon has a policy wherein they have
a single namespace for all the S3
buckets of the world which in other
words means that if you have a name
which is already chosen by someone in
the world of an S3 bucket you will not
get that name
so obviously you will have to have long
names since all shorter names would have
been taken
in order to make that long name
meaningful make sure you break that into
a specific context so that when you look
at that bucket you already know this is
what this bucket is doing
you should make sure you have bucket
policies for buckets which are
accessible by other users
these policies can be generated in the
policy generator or you can create
fine-grained self-made policies
for buckets which are very much live and
large and have got tons of data you
should archive them to Glacier because
Glacier gives you very cheap storage
although there is a retrieval time
associated with that but yes you should
make sure to Archive the buckets
so if you've got some critical objects
in a bucket you should make sure that
you also save a version of your object
because you can go back to a previous
version of that same object
now in this case you don't actually lose
any version of that file you always have
the important version of that object
from security best practices never share
your root Account Details with anyone
because if someone has got root Account
Details of your account you can actually
do anything
make sure people who leave your
organization you remove their access and
you don't keep them as stale accounts
you should also enable two-factor
authentication either with Google
Authenticator or some Hardware device
access token which flashes a six or four
digit random number so that it can be
added as an added layer of security you
also should enable IAM login don't give
root access to even trusted users so in
IAM you have a name and an ID which
shows you that this guy did this at this
time and with that we have come to the
end of this video on AWS full course I
hope you found it useful and
entertaining please ask away any
questions about the topics covered in
this video in the comments box below our
experts will assist you in addressing
your problems thank you for watching
stay safe and keep learning
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here