welcome to this new full course by
simply learn Docker is a platform for
creating and running containers which
are isolated environments for
applications and their dependencies
kubernetes is an orchestration system
for managing and scaling containers it
automates task like deployment scaling
and load balancing together doer is used
to build and package applications into
containers and cuberes is used to manage
and scale those containers in production
environments making it easier to develop
and deploy modern applications average
salary of doco kubernetes Administrators
is
$168,000 per year on that note if you
are looking to get certified into devops
and become a kubernetes and Docker
expert have a look at wide range of
courses on devops by simply learn in
collab collaboration with topnotch
universities across the globe these
programs offers Blended learning that
combines live devop certification
classes with interactive Labs that will
give you hands-on experience you will
gain expertise in the tools like
terraform mayin Anil Jenkins Docker
junit git and more we prefer one year of
Prior experience to enroll in these
courses so why wait enroll the course
now from the link in the description box
so before we begin consider subscribing
to Simply learn and hit the Bell icon to
never miss any updates from us so
without any further delay let's begin
over to our
tutors we're going to break up this
presentation into four key areas we're
going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a Dev Ops
environment and then finally we're going
to break down the architecture and
working infrastructure for kubernetes so
you understand what's happening and why
the actions are happening the way that
they are so let's jump into our first
section of life before kubernetes so the
way that you have done work in the past
or you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is deploying code
out to Virtual machines so let's go
through and look at the two different
types of deployment that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire our
environment there's going to be problems
with resource allocation and you're
going to have issues where applications
may be running effectively and not not
effectively and not load balance um
effectively across the environment the
problem that you have with this kind of
infrastructure is that it gets very
expensive uh you can only install one
piece of software one service on one
piece of Hardware so your Hardware is
being massively underutilized this is
where virtual machine have become really
popular with a virtual machine you're
able to have better resource utilization
and scalability at much less cost and
this allows you to be able to run
multiple virtual machines on a single
piece of Hardware the problem is is that
VMS or for virtual machines are not
perfect either some of the challenges
you run with VMS is that the actual
hardware and software need needed to
manage the VM environment can be
expensive there are security risks with
virtual with VMS there are security
risks with VMS there have been data
breaches recorded about solutions that
run in virtualized environments you also
run into an issue of availability and
this is largely because you can only
have a finite number of virtual machines
running on a piece of hardware and this
results in limitations and restrictions
in the types of environment you want to
be running and then finally setting up
and managing a virtualized environment
is time consuming uh it can take a lot
of time and it can also get very
expensive so how about kubernetes well
kubernetes is a tool that allows you to
manage containerized deployment of
solutions and inherently kubernetes is a
tool that is really a Next Level
maturity of deployment so if you can
think of your maturity curve as
deploying code in directly to Hardware
in a Data Center and then deploying Your
solution to Virtual machines the next
evolution of that deployment is to use
containers and kubernetes so let's kind
of go through and look at the
differences between a virtual machine
and kubernetes and we've got a few here
that we want to highlight and you'll get
an understanding of what the differences
are between the two so first of all with
virtual machines there is inherently the
security risks and what you'll find as
we get dig through the architecture
later in the presentation is that
kubernetes is inherently secure um and
this is largely because of the Legacy
code uh the legacy of kubernetes and
where it came from we'll talk about that
in just a moment but kubernetes is
inherently secure uh virtual machines
are not easily portable now with that
said they they are technically portable
they're just not very easily portable
whereas with kubernetes it's working
with darker container Solutions it is
extremely portable that means that you
can actually spin up and spin down and
man B your infrastructure exactly the
way that you want it to be managed and
scale it on the demands of the customers
as they're coming into use the solution
from a timec consuming point of view
kubernetes is much less time consuming
than with a virtual machine a few other
areas that we want to kind of um
highlight differences virtual machines
use much less isolation when than
building out the encapsulated
environment than kubernetes does uh for
instance with a a virtual machine you
have to run hypervisor on top of the OS
and hardware and then inside of the
virtual machine you also have to have
the operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging a darker
container and or container like
Technologies it only has to have the OS
and the hardware and then inside of each
container it doesn't need to have that
additional OS layer it's able to inherit
what it needs to be able to run the
application this makes the whole
solution much more flexible and allows
you to run many more containers on a
piece of Hardware than versus running
virtual machines on a single piece of
Hardware so as we um highlighted here
VMS are not as portable as kubernetes
and kubernetes is portable directly
related to the use of containerization
and because kubernetes is built on top
of containers it is much less time
consuming because you can actually
script and automatically allocate
resources to nodes within your
kubernetes environment this allows the
infrastructure to run much more
effectively and much more efficiently so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of the software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came about when it did so
what is kubernetes let's let's dig into
the history of kubernetes and how it
came about so in essence kubernetes is
an open-source platform that allows you
to manage and deploy and maintain groups
of containers and a container is
something like doer and if you're
developing code you're probably already
using darker today consider kubernetes
as the tool that manages multiple Docker
environments together now we talk a lot
about Docker and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility and being able
to scale up your Solutions and they were
designed for the postdigital world that
we live and exist in today so a little
bit of background a little bit of trivia
around uh kubernetes so kubernetes was
originally a successor to a project at
Google and the original project was
Google Bor um Google Bor it does exactly
what kubernetes done does today but
kubernetes was Rewritten from the ground
up and then released as an open-source
project in 2014 so that people outside
of Google could take advant vage of the
power of kubernetes containerization
management tools and today it is managed
by the cloud native Computing foundation
and there are many many companies that
support and manage kubernetes so for
instance if you're signing up for
Microsoft aszure AWS Google Cloud all of
them will leverage kubernetes and it's
just become the the de facto tool for
managing large groups of containers so
let's kind of Step through some of the
key benefits that you'd experien from
kubernetes and so we have nine key
benefits and the first it is highly
portable and is 100% open-source code
and this means that you can actually go
ahead and contribute to this code
project if you want to through GitHub uh
the ability to scale up the solution is
incredible um what's um the the history
of kubernetes being part of a Google
project for managing the Google network
and infrastructure uh kind of really
sets the groundwork for having a
solution that is highly scalable the out
of the high scalability also comes the
need for high availability and this is
the desire to be able to have a highly
efficient and highly energized
environment that also you can really
rely on so if you're building out of
kubernetes management um environment you
know that it's going to be um available
for the solutions that you're
maintaining and it's really designed for
deployment so you can script out the
environment and actually have it as part
of your devops model model so you can
scale up and meet the demands of your
customer then what you'll find is that
the um load balancing is extremely
efficient and it allows you to
distribute the load efficiently across
your entire network so your network
remains stable and then also the tool um
allows you to manage the orchestration
of your storage so you can have local
storage such as an SSD on the hardware
that the kubernetes is M maintaining or
if the kubernetes environment is p
storage from a public Cloud such as
Azure or AWS you can actually go ahead
and make that available to your entire
system and you can inherit the security
that goes back and forth between the
cloud environments and one of the things
you'll find consistent with kubernetes
is that it is designed for a cloud first
environment um kubernetes as well is
that it's it's really a self-healing
environment so if something happens or
something fails uh kubernetes will
detect that failure and then either
restart the process kill kill the
process or replace it and then because
of that you also have automated roll
outs and roll backs uh in case you need
to be able to manage the state of the
environment and then finally uh you have
automatic bin packaging so you can
actually specify the compute power
that's being used from CPU and RAM for
each container so let's dig into the
final area which is the actual
kubernetes architecture and we're going
to cover this at a high level there's
actually another video uh that you can
that simply learn as developed which
digs deeper into the kubernetes
architecture and so the kubernetes
architecture is a cluster based
architecture and it's really about two
key areas you have the kubernetes master
which actually controls um um all of the
activities within your entire kubernetes
infrastructure and then you have nodes
um that actually are running on Linux
machines um out that are controlled by
the master so let's kind of go through
some of these um areas so we look at the
kubernetes master uh to begin with um
then we'll start with etet this is a
tool that allows for the configuration
of information and the management of
nodes within your cluster and one of the
key features that you'll find with all
of the tools that are managed within
either a the master environment or
within a node is that they are all
accessible via the API server um and
what's interesting about the API server
is that it's a restful based infrastru
structure which means that you can
actually secure each connection with SSL
um and other um security models to
ensure that your entire infrastructure
and the communication going back and
forth across your infrastructure is
tightly secured scheduler goes ahead and
actually as you'd expect actually um
manages the schedule of activities
within the actual cluster and then you
have the controller and the controller
is a Damon server that actually manages
and pushes out the instructions to all
of your nodes so uh the other tool
really are the uh the infrastructure and
you can consider them the administration
side of the master whereas controller is
the management actually pushes out all
of the controls via the API server so
let's actually dig into um one of the
actual nodes themselves and there are
three key areas of the noes one is the
do environment which actually helps and
manage and maintain the container that's
actually inside um of the node and then
you have the kuet which is responsible
for information that goes back and forth
and it's going to do most of the
conversation with the API server on the
actual health of that node and then you
have the actual kubernetes proxy which
actually runs the services actually
inside of the node so as you see all of
these infrastructures are extremely
lightweight and designed to be very
efficient and very available for your
infrastructure and so here's a quick
recap of the different tools that are
available and it really breaks down into
two key areas you have your kubernetes
master and the kubernetes node now the
kubernetes master has the instructions
of what's going to happen within your
kubernetes infrastructure and then it's
going to push out those instructure to
an indefinite number of nodes that will
allow you to be able to scale up and
scale down your solution in a dynamic
way kubernetes is an open- Source
platform used for deployment and
management of containers so let's get
started now the first step is to install
the necessary dependencies using two of
the commands first is PSE sudo app get
dat which gets all the
[Music]
updates and then you have to enter
password so this will take a few
seconds that's done the next step is to
install app transport https this is
basically used to make repositories via
https so let's go ahead with that
sudo okay that's done going ahead the
next thing we have to do is to install
the docker dependency using the command
sudo app install docker.io
you have to choose
why and this will take a few
minutes okay that's done so after
installing the docker we have to start
and enable the docker using the command
sudo system CTL start Docker and sudo
system CTL enable
Docker so now the docker is enabled so
now we are done with the first step of
installation the next step is we have to
install the necessary components for
kuties before that we have to install
the curl command because curl command is
used to send data using a URL syntax
let's install the Cur command using the
command sudo app get install curve
so you have to select Y and this will
take few
minutes so moving on the next step we
have to do is download and add key for
kubernetes installation from a URL so
let's go ahead with that Pudo
curl so this is where we get the key and
then we have to add it using pseudo app
key add and it's done so the next step
is we have to add a repository in a
certain location so before doing that we
have to change the permission of that
file so we do that using C or sudo CH
mode
[Music]
command Okay so the permission is
changed now let's go ahead and save this
file in that location first we have to
enter this command into the file this is
a
URL save
as so we have to save in this location
let's rename this to
kues do
list now let's
see okay so here it is now moving
forward now we have to check for any
updates
available in the next command we are
going to install the kubernetes
components that is cubet cub adum cucle
and kubernetes cni so let's go ahead
with
that
so this is going to take a few
minutes now that's done so the next step
we have to initialize the master node
and to do this we have to first use a
swap off command to disable the swapping
on other devices so let's do that Pudo
swap off hyper now let's go ahead with
the initialization
sudo cube adum in it this is going to
take a few
minutes okay so before going to the next
step to start the cluster we have to use
these three commands let's just copy
paste
it okay that's done so moving forward
the next step is to deploy parts using
the following
[Music]
command now the pods have been deployed
to our Network to see all the pods you
have to use the command sudu cubical get
[Music]
PS there you can see the PS that I
so we're going to break out the
presentation to the following areas
we're going to cover what kubernetes is
and why you want to be using it we're
going to introduce the actual
architecture and provide an overview of
how it contains the containers and the
other components that you wouldd have
within the architecture we also compare
kubernetes with doer swarm and one of
the things that you hear with kubernetes
almost in the same um breath is doer um
we have to be careful not to confuse
Docker from a container point of view
and Docker swarm which is a container
management tool and kubernetes is also a
container management tool so we'll
compare the two of those and then we'll
look at the hardware and software
components and then finally do a deep
dive into the architecture of kubernetes
and provide a case study of where
kubernetes has been used successfully in
the past so let's jump into this so what
actually is kubernetes so kubernetes is
a tool that was designed to actually
help manage and contain large groups of
containers it was developed originally
uh by Google out of the Google Bor um
solution and then Open Source by Google
the actual environment allows you to um
manage large groups of containers so if
you a developer in devops and you're
working with Docker then you're used to
the concept of the container and uh
you'll also know that containers and
Cloud Technologies tend to be almost
breathed in the same breath so the work
that you're doing with kubernetes is to
be able to manage large groups of
containers inside of the cloud the thing
that's great about working with
kubernetes is that it is is incredibly
flexible and allows you to have
incredibly complex applications run
efficient efficiently so let's step into
why you'd want to use kubernetes so a
couple of key points kubernetes itself
is an open- Source solution originally
developed by Google but it is available
on most Cloud platforms today so AWS
Google Cloud Microsoft Azure all of them
support kubernetes and what you'll find
is as you're setting up your
infrastructure particularly if you're
using containers you'll see that the
support for kubernetes is floated up
very very efficiently and effectively by
all three vendors it's extremely um
useful for being able to manage large
modular environments and that's really
kind of the I think one of the big
benefits of kubernetes is its modularity
is that it really breaks down uh
containers into their smaller parts and
once you do that it becomes much more
efficient uh for you as an administrator
to be able to manage that entire
environment the repr cability of
kubernetes extremely high uh you can
build out infrastructures very quickly
and um have them being able to manage
and um have containers coming on and off
um killed and created to be able to help
load balance your entire environment and
so again you we keep talking about
containers but that's really what
kubernetes is all about it's being able
to manage your applications that are
managed through containers and being
able to do that in a virtualized
infrastructure and the thing that's also
really good with kubernetes is it's
really easy to point Solutions you just
have to use a simple C call and you can
actually push out your kubernetes
infrastructure so let's have an overview
of the cetes architecture so kubernetes
is really broken up into three key areas
so you have your workstation where you
develop your commands and you push out
those commands to your master and the
master is comprised of um four key areas
um which essentially control all of your
nodes and and the node contains multiple
pods and each pod has your Docker
container built into it so consider that
you could have really almost an infinite
number of PODS sorry infinite number of
nodes being managed um by the master
environment so you have your cluster
which is a collection of servers that
maintain the Ava availability and the
compute power such as RAM CPU and disk
utilization um you have the master which
is really components that control and
schedule uh the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and and be able to
actually control and communicate back to
the master the health of that pod and
we'll get into more detail on the
architecture later in the presentation
so you know you keep hearing me talk
about um containers but they really are
the center of the work that you're doing
with kubernetes and the concept around
kubernetes and containers is really just
a natural evolution of where we've been
with internet and digital Technologies
over the last uh 10 15 years so before
kubernetes um you had tools where you
either running virtual machines or
you're running uh data centers that had
to maintain and and manage and notify
you of any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so uh you can think of
containers as that Natural Evolution
from you know uh 152 years ago you would
have written your code and posted it to
a data center uh more recently you
probably post your code to a virtual
machine and then move the virtual
machine and now you actually just work
directly into a container and everything
is self-contained and can be pushed out
to your um environment and the thing
that's great about containers they're
they're isolated environments very easy
for developers to work in them but it's
also really easy for uh operations teams
to be able to move a container into
production so let's kind of Step um back
and look at a competing product to
kubernetes which is Docker swarm now one
of the things that we have to um
remember is that Docker um containers
which are extremely popular are built by
the company Docker and made open source
and Docker actually has other products
one of those other products uh is Docker
swarm and Docker swarm is a tool that
allows you to be able to manage multiple
containers uh so if we look at some of
the uh the benefits of using Docker
swarm versus kubernetes and one of the
things that you'll find is that both
tools have strengths and weaknesses but
it's really good that they're both out
there U because it helps keep it really
kind of justifies uh the importance of
having these kind of tools so kubernetes
was designed originally from the ground
up to be autoscaling whereas Docker
swarm isn't the load balancing is
automatic on Docker swarm whereas with
kubernetes you have to manually
configure load balancing across your
nodes the installation for docker swarm
is really fast and easy I mean you can
be up and running within minutes
kubernetes takes a little bit more time
is a little bit more complicated
eventually you'll get there um I mean
it's not like it's going to take you
days and weeks but it's it is a tool
that's a when you compare the two doctor
swarms much easier to get up and running
now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's its real strength
is its ability to have strong clusters
whereas with do swarm it's cluster
strength is isn't um as strong when
compared to kubernetes now you compare
it to anything else on the market it's
really good um so this is kind of a
splitting hairs kind of comparison um
but kubernetes really does have the
advantage here if you're looking at the
two compared to each other for
scalability I mean kubernetes was
designed for by Google to scale up and
support uh Google Cloud Network
infrastructure they uh both allow you to
be able to share um storage volumes with
do you can actually do it um with um any
container with u that is managed by the
docker swarm whereas with kubernetes it
manages the storage with the Pod and a
pod can have multiple containers within
it but you can't take it down to the
level of the container interestingly uh
kubernetes does have a graphical user
interface um for being able to control
and manage uh the environment the
reality however is that you're likely to
be using terminal uh to actually make
the controls and Commands to control
your um either Docker swarm or
kubernetes and environment um it's great
that it has a a goey and to get you
started but once you're up and running
you're going to be using terminal window
for those fast quick administrative
controls that you need to make so let's
look at the hardware components for uh
kubernetes so um what's interesting is
that kubernetes is extremely light of
all the systems that we're looking at
it's extremely lightweight um it's um
allows you to have if you compare it to
like a virtual machine which is very
heavy you know um kumat is extremely
lightweight and hardly uses any
resources at all interesting enough
though is that if you are looking at the
usage of CPU it's it's better to
actually take it for um uh the cluster
as a whole rather than individual nodes
U because uh the nodes will actually
combine together to give you that whole
compute power again this is why
kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center um so you can
have persistent volumes um such as a
local SSD or you can actually attach to
a cloud data storage again kubernetes is
really designed for the cloud I would
encourage you to use cloud storage
wherever possible rather than relying on
physical storage uh the reason being is
that if you connect to cloud storage and
you need to flex your your storage the
cloud will do that for you I mean that's
just an inherent part of why you would
have cloud storage whereas if you're
connecting to physical storage you're
always restricted to the limitations of
the physical Hardware so let's um kind
of pivot and look at the software
components as compared to the hardware
components so the main part of the um
components is the actual container and
all of the software running in the
container runs on Linux so if you um
have Docker installed as a developer on
your machine it's actually running
inside of Linux um that's what makes it
so lightweight and really one of the
things that you'll find is that most
data centers and Cloud providers now are
running predominantly on Linux inside of
the um the the container itself is then
managed inside of a pod and a pod is
really just a group of containers
bundled together and um the kubernetes
scheduler and proxy server then actually
manage what um how the pods are actually
pushed out uh into your kubernetes
environment the the pods themselves can
actually then share resources both
networking and storage so pods aren't um
pushed out manually they're actually
managed through a layer of app raction
and part of their deployment and and
this is the strength of kubernetes you
use use Define your um infrastructure
and then kubernetes will then manage it
for you and there isn't that problem of
um manual management of PODS uh if you
have to manage the deployment of them
you that's simply taken away and it's
completely automated and the the final
area of software Services is on Ingress
and this is really the secure way of
being able to have communication from
outside of the cluster and passing of
information into that cluster and again
this is done securely through SSL layers
and allows you to ensure that security
is at the center of the work that you
have within your kubernetes environment
so let's dive now into the actual
architecture before we start looking at
a use case of how kubernetes is being
employed so kubernetes again is um we
looked at this uh diagram at the
beginning of the presentation and there
are really three key areas there's the
workstation where you develop your
commands and then you have the Master
environment which uh controls the uh
scheduling the communication um and the
actual um commands that you have created
and pushes those out and manages the
health of your entire node Network and
each node has uh various pods so we like
break this down uh so the master node is
the most vital component um with the
master uh you have four key controls you
have Etc controller manager schedule and
API server the cluster store Etc this
actually manases the details and values
that you've developed on your local
workstation um and then we'll work with
the out the control schedule and API
server to communicate that out those
instructions of how your infrastructure
should look like to your entire network
uh the control manager is really an API
server and again um this is all about
security so we use restful fpis um which
can be packaged in SSL to communicate
back and forth um across your pods and
the master and indeed the services
within each of them as well uh so um at
every single layer of extraction uh the
communication is secure uh the schedule
as the name would imply relase schedules
um when tasks get sent out to the actual
nodes themselves the nodes themselves
are are dumb nodes they just have uh the
applications um running on them the
master in the um is really doing all of
the work uh to make sure that your
entire network is running um efficiently
and then you have the API server which
has your rest commands and the
communication um back and forth across
your network that is secure and
efficient so your node environment is
where all the work that you do with your
containers gets pushed out too so um a a
work is really it's a combination of
containers and each container will then
logically run together on that node so
you'd have a collection of containers uh
on a node that all make logical sense to
have together uh within each node um you
have Docker and this is your isolated
environment for running your container
uh you have your cuet which is a service
for conveying information back and forth
um to the service about the actual
health of the kubernetes node itself and
then finally you have the proxy server
and the proxy server is able to manage
the nodes the volumes the the creation
of new containers um and actually helps
pass the communic the the health of the
container back up to the master to see
whether or not the container should be
uh either killed stop started or um
updated so finally let's look at see
where kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance uh increase value
and revenue into the organization so an
example of this is a company called
Black Rock uh where they actually went
through the process C of implementing
kubernetes uh so they so Black Rock had
a challenge where they need to be able
to have much more Dynamic access to
their resources uh they were running
complex installations on people's
desktops and it was just really really
difficult to be able to manage their
entire infrastructure and so they
actually then um pivoted to using cetes
and this allowed them to be able to be
much more scalable and expansive in the
management of their uh infrastructure
and as you can imagine kubernetes was
then hooked into their entire existing
system and has really become a key part
of the success that Black Rock is now
experiencing of a very stable
infrastructure um and the bottom line is
that uh Black Rock is now able to have
confidence in their infrastructure and
be able to give that confidence um as
back to their customers through the
implementation and more rapid deployment
of additional features and services so
what are orchestration tools the
application development and the life
cycle management from the time the
application is developed connecting the
source code to your continuous
integration to testing them all along
your development process and eventually
moving it down to production managing
your production servers the dependencies
that your software has the requirement
that it has in terms of the hardware the
features of fault tolerance selfhealing
capabilities autoscaling all this has
complicated over the last few years
years as a part of devops one thing that
everyone is interested in is managing
all this application dependency or life
cycle management using some or the other
kind of a tool so that's where these
orchestration tools have become very
very popular so what kind of features
that they provide is you'll have to just
let them know what are the kinds of tool
sets that is required what are the fall
tolerance mechanisms that it has to be
adopted to what kind of a self-healing
capabilities that application will have
to need and if at all there is any
autoscaling features that is required if
at all you can bake in all these
specific parameters into your tool your
orchestration tool becomes a One-Stop
shop for all your deployment and
configuration management needs that's
where these orchestration tools have
become very very popular and relevant
specifically these days when people are
more and more uh interested in adopting
devops practices all right so having
said that about orchestration tools
let's concentrate on two of these
orchestration tools specifically for
container management Docker swamp and
kubernetes many of the applications that
are written for these kind of containers
are called or kind of fall into a
category of cloud native where the
application need not know much about the
underlying infrastructure or the
platform where these applications are
running so these two along with Apache
misce there are many other container
Management systems but I'm going to pick
doer swarm and kubernetes for comparing
the features that is provided by these
two Frameworks for the sake of
comparision I picked up Doos swarm and
kubes just because of the reason that
both of them operate in the space of
container management they do something
very very similar to Containers that's
the similarity that exists between these
two orchestration tools however there's
a big difference that exist when it
comes to the types of containers that
both of them cater to and also so the
capacity of workloads they can be used
for Docker swarm is a cluster management
solution that is provided by Docker
container Docker is one of the very very
popular containers of recent times and
in case you have your applications that
is totally powered by only Docker
containers if you have a need where you
want to run a cluster of servers which
are running only Docker containers
Docker swarm should be your choice for
your orchestration tool KU on the other
hand can cater to any other types of
containers other than Docker and
including Docker as well so in case you
have your applications which have got
Docker in them which have got RK in them
which they have got Alx in them or any
other type of container kuis should be
your choice of orchestration tool a
Docker swarm can manage up to 40 50 or
60 Max uh nodes so in case your
application is totally written in Docker
containers and the load or the expected
cluster size is around 50 60 notes
Docker swamp should be your choice of
orchestration tool on the other hand
kubernetes is something that was open
sourced by Google and the kind of the
scale of operations that kubernetes can
cater to is Google scale so if in case
your applications are more than a
thousand nodes that is where kues comes
into play that's the big difference that
exist between do swarm and communities
putting those differences aside let me
compare Docker swarm and kubernetes
based upon other features which are
similar in nature the first and
important feature that kubernetes
provides is something called as
autoscaling if at all the load on your
cluster is too high and your application
is experiencing more load so kubernetes
can add new nodes to your cluster of
course you'll have to configure
kubernetes in order to have those
capabilities where it can spin up new
VMS or new nodes if at all you do that
configuration incorrectly kubernetes has
got the capacity or it has got the
feature where it can bring up a new node
and add it to the cluster on the Fly
based upon the load that exist at that
moment on similar lines if at all the
load is not too high it can identify few
of those nodes which have got less
number of uh replicas or less number of
application containers running on it it
can move them to some other node and
also scale down by deleting few nodes
from your cluster so that is the
powerfulness of autoscaling which is
provided by Ces and this unfortunately
does not exist in Docker swamp the other
feature of load balancing specifically
application load balancing so dock swarm
gives you an application Level autoload
balancing however kubernetes gives you
the flexibility of manually configuring
any other type of load balancer of your
choice for application load balancing
installation as I mentioned earlier doer
swarm is a lose CL cluster of containers
which are running on nodes it is very
easy to spin up a new node and then
connect them to a swarm and this can be
done in a very very loose coupled way
where you can create swarms of your
choice nodes are are allowed to connect
to swamps and quit or leave the swamps
on the fly so in and all the
installation is pretty easy and fast
kubernetes on the other hand the
configuration of kubernetes the way to
spin up a big cluster specifying the
size of the node how many Master notes
how many config planes that you want how
many notes that you want it's a pretty
tough thing to bring up a kubernetes
scalability kuties strength is very very
strong they are very tightly coupled and
even they have the capability where on
the cluster size things or the nodes can
be increased on the Fly based upon the
requirement on a doer swarm the cluster
strength is weak as compared to kues
worker nodes can be added to a swarm
worker notes can be asked to leave a
swarm or can be taken out of a swarm
based upon the requirement so this is
kind of a Loosely coupled architecture
for Doos swarm while Kutis the cluster
is very tightly coupled since Docker
swarm runs only Docker containers
containers find it very easy to share
data and lot of other things with each
other because they all have the same
signature they're all from the same
family so it's very easy for them to
share not just volumes but a lot of
other things however for kubernetes
since it manages containers of different
types if your application has to share
some data across different containers
there's a little bit of a complexity in
how you would want your containers to
share data also uh when it comes to
kubernetes kubernetes groups containers
in something called as pods while a pod
can have either one container that's a
preferred choice or multiple containers
and the idea of pod is like each pod can
run in any other node it's not
guaranteed that you would want to have
two or three parts to be running on the
same note that makes data sharing a
little bit of a different thing compared
to Dockers form uh GUI so there's not a
good enough uh UI tool in Dockers form
at least uh the C edition of it which
will allow you to get to know what is
running on your containers what is the
size of the containers what is the
volume of the containers and all that
stuff there are some free and
open-source tools like painer which
gives you a good visibility into your
running Docker containers however
there's nothing at a swarm level that is
provided by docker kubernetes gives you
an outof the-box dashboard which is easy
to configure and set up you can also
Club it with some metrics services and
you can also get information about the
size of the cluster that is running what
is the load on the nodes and stuff like
that all this across your cluster in a
very beautiful dashboard perspective so
that way um this is little bit of a good
UI for kubernetes while for Docker form
there isn't anything that is provided
out of the box let me spend some time in
explaining a little bit about the
kubernetes architecture what comprises
of the kubernetes cluster what resides
in a master or a control plane and what
resides in a worker node this is a very
high level uh depiction of a kues
cluster so this is the master node which
has got something called as a cluster
store a controller a schedular component
and an API server and these are the
bunch of nodes that are connected or
administered by the master node the
master node can be one 3 5 typically an
odd number this is as per the typical
cluster management thing where you would
need the master nodes to be in odd
numbers so that whenever there's any
contention in terms of what needs to be
deployed where or where to give the job
to whom and stuff like that all the
Masters would cast their vote and they
would decide on the outcome so that's
the master node and there are a bunch of
uh nodes which can be connected and
administered by the master node qctl or
the command from using using which
anybody can assign some workloads to the
cluster can also be run either on the
same node or on a separate node so qctl
is the command line tool that we will be
installing and using in order to fire up
our commands to our cluster so if at all
I have to put up a job on our cluster if
I have to give out any specific commands
to my cluster all this is done using ql
there's a bunch of rest apis which is
used by qctl and qctl will talk to the
API server and fire up the commands
specific to my cluster what runs in the
master node Master node without saying
it's the most important component of any
of the cluster if at all you're running
um cluster with only one master node if
your master node goes down there's no
other way that you know you can or any
user can talk to the different nodes in
the cluster so Master node is the most
vital component responsible for the
complete kues cluster there's always one
node that should be running as Master
node that's the bare minimal requirement
for your cluster so what are the other
components in the master node the most
important one is called etcd so this is
nothing but a data store a value of key
value pair which is stored which
contains all information about your
cluster so all the configuration details
which uh node is up which worker nodee
is down all this information is stored
in the cluster store so all the managers
would access this cluster store before
they go ahead and decide any other work
item or anything else that has to be
done specific to your cluster what's in
the controller as the name says
controller is something it's like a
demon server that is running in a
continuous loop all the time it is
responsible for controlling the set of
replicas the kind of uh workloads that
is running based upon the configuration
that the user has set in so if at all if
you are aware of something called as
replication controllers endpoint
controllers namespace controllers all
these controllers are managed by the
controller component if any user asked
for some three replicas of a particular
pod or a service to be running and if at
all any of the nodes goes down or the
Pod goes down for whatsoever reason the
controller is the one who wakes up and
assigns this particular job or the part
to some other available node by looking
up for the details using in the cluster
store that's the importance of the
control manager or the controller the
scheduler the scheduler assigns the
tasks based upon whoever is asked for
any job to be scheduled based upon a
time frame or based upon some criteria
it also tracks the working load as to
what what exactly is the load who is
running What in in the cluster and
places the workload on whoever is the
available resource at that time all
right API server this is one other
important component in in our kubernetes
cluster where how would the end end user
deploy or give out any sort of a
workload onto your cluster all the
requests come to the API server so this
is an entry point for all your um
requests that come into your cluster
anybody wants to deploy something
anybody wants to scale up a controller
anybody wants to bring down few Services
anybody wants to put in a service all
this will have to come in as a part of a
rest um API endpoint and API server is
the entry point in case you don't want
uh you know somebody to access your
cluster in case you want only specific
people or a specific users to be running
some specific workloads you can set all
those role-based access control for this
API server so this is the entry point
for anyone who wants to submit any job
to your cluster so that's a quick
overview about the master node what what
are the important components of a master
node now let's go over to what runs in a
worker or a slave node as I mentioned
earlier slave nodes are something where
the job that is submitted to your
cluster is eventually run as a pod as a
service as a container so in kues world
there's something called as a pod a pod
is nothing but a combination of a
container it is a wrapper around your
running containers so slave notes or the
worker notes typically run these parts
so that is where the whole workload
typically gets run but there are a bunch
of other components in the in the node
which also manages what runs in the Pod
who has to have access to the Pod what
is the state of the Pod is it in a
running state is it going down for some
reason and all that stuff so let's
quickly go over those components that is
there in the slim node the most
important component in my opinion that
should be on the on the Node should be
the container run time as I mentioned
earlier kuties can run any different
types of container not just Docker so in
case you want to have a node which wants
to have Docker running on it rkt running
or it LKC running on it or any other uh
container environment that is running on
it you have to ensure that the specific
container runtime environment is
available on that specific node so that
whenever a job is submitted a
description of what needs to be run as a
part of the Pod what should be the image
with with it should be powered and what
kind of a container to spin up so that's
what you know when the job is finally
assigned to this particular node it
would definitely need a container
runtime to be up and running so that
exists only if at all the container
runtime is installed and running on your
kubernetes worker note okay now that we
know what our kuberan is node can run
how would somebody assign a job to our
node that is using something called as a
cubet as the name says cubet or cubet is
a small subset which talks to the cube
API server so any node which has to run
any kind of a pod all these instructions
are passed around by the qbp server to
the Q cuet and cubet is capable of
processing whatever job that is assigned
to it and ensuring that so many parts
and so many services are spun up based
upon the requirement the last component
that exist uh in the worker node is the
cube proxy or kubernetes proxy this
plays a very very important role acting
as a load balancer and a network proxy
so what typically happens is whenever
the pods are running in nodes the pods
can be typically running in any node
there is no
um Affinity towards any node on which
these pods are running because pods or
containers are something called as
ephimeral they can run anywhere so how
would somebody reach out to these
applications that is running on some pod
which is running in one container now
and running in probably another
container another node altogether
tomorrow so that is where your
kubernetes proxy com into picture and
this component will ensure that any
container that is spun up or any pod
that is spun up it keeps track of all
these these spots and kind of connects
the end points or acts like a DNS server
so that it knows when somebody is trying
to reach out to this particular service
which is the pod on which the service is
typically running so that plays a very
very important role by acting as a load
balancer and a network proxy now that at
a very high level we know what are all
the components that make up of our
kubernetes cluster let's go ahead and
create a small cluster we'll spin up
this cluster on AWS platform finally we
get to the demo section of my tutorial
now that you guys are aware as to what
is kubernetes what is the use of
kubernetes you also know at a very very
high level what are the components of a
cluster what is a master node or a
control plane what are the components of
a master node and what should be
existing in a worker node you probably
thinking that it's going be pretty hard
to kind of set this up so let me
demystify that by running you with a
quick demo of how to set up a simple
three node kubernetes cluster using a
tool called cops and I will be doing
this whole setup on my AWS Cloud okay so
what is cops cops is nothing but a
simple tool this is an admin tool that
allows you to bring up production grade
uh kubal environments as I said setting
up a kues cluster on a bare metal is
little challenging so that is why I
would use something called as a cops and
I will use this on my cluster uh which
I'll spin up on my AWS instance so I
have an AWS account and what I'll do do
first is I will first pin up an E2
server I will power this with one of the
um Linux Amis and I will install cops on
it now this will be my starting point
from where I'm going to trigger running
up a big cluster in our case I'll try to
keep it little small I don't want too
many notes in my cluster I'm going to
have one master node and two worker
nodes this whole operation would be done
programmatically by installing this cops
uh admin tool so as you may know AWS
it's little hard for me to
programmatically run other components or
rather bring up servers it's not pretty
simple so what I would do is I will
create an IM am rule which will attach
to my E2 server so that my E2 server
gets powered uh with all the required
rule so that it can go ahead and spin up
cluster based upon my configuration that
I'm going to specify all right so once I
have my cluster set up I will also
install Cub CTL which uh you would
probably know by now is nothing but a
command line utility using which I can
kind of connect to my cluster give out
some jobs put some pots and stuff like
that so I will use ql I will also have a
pair of SSH keys so that from this
machine I would be able to successfully
get onto the master node and submit jobs
on my behalf so let me Begin by first
logging into my AWS account all the
steps that I will be following to bring
up my AWS cluster kuet cluster will be
documented and I I'll be sharing you
this document in a GitHub repository so
that in case anybody wants to try you'll
be more than happy to find all the
required instructions in one place okay
now the first thing that I would need is
to launch an AWS ec2 instance now this
is an instance where I'm going to
install cops and I will use it as a base
server from where I'm going to f some
commands which will bring up my kues
cluster so let me log into my AWS
account and um let me stick to one
particular region and let me bring up
some a one E2 instance all right so let
me launch my instance let me choose um
doesn't matter either ways but in case
you want to try it on your free tire I
would recommend choose free tire only
and choose an Ami of your choice let me
choose this instance I will make it T2
micro that is good for me configuration
instance details all right I don't want
to change anything here that looks good
maybe I will change this 30 gig that's
good for me let me add a tag to my
instance I will name this as my cops
server all right I would need a bunch of
ports to be opened up so what I'll do is
I'll create a new Security Group uh
called open to the word I would not
recommend that you do this since I don't
want to it'll take a long time for me to
specifically uh pick and choose um the
ports that has to be open for running my
cluster I don't want to do that I will
open up all the uh HTTP and https ports
so that it's quicker so I will say all
TCP I would say from anywhere just to be
on the safer side also open HTTP and
https and I will make this anywhere and
anywhere this is good for me I will say
review and launch all right so I chose a
T2 micro instance just for the sake of
easiness I opened up all the ports and
um the instant details are you've seen
what is that I've chosen all right now
important part is like I would need a
key pair I need to create a new key pair
I will call this as simply learn keys I
will download this keare and then I will
launch the instance it'll take some time
for my ac2 instance to come up in the
meanwhile what I'll do is I will convert
I have this pem file which is the pair
of keys the SSH keys with which I need
to log into my ec2 instance so I need to
convert this because I'm trying to
connect from my Windows box I would need
to convert this into a PPK file so I
have puty gen I'm going to load my set
of keys here and convert that key into a
PPK key all right open all right I would
say save hang on save Private key yes I
would say simply learn or simply learn
private private key I'll save this here
done that that's all so this is my
public key which is the PM file and this
is my PPK or my private key Now using
the private private key I will log into
my ec2 instance okay now my ec2 instance
is up and running it is running in this
uh Ohio region and this is the IP
address of my public IP address of my
machine so let me connect to it so I use
uh MOBA xterm which is nothing but an
SSH emulator you can use puty or any of
these sessions uh which allow you to do
an SSH uh this is my IP address of my
machine and uh since I've spun up an
Amazon Ami the user this is the default
user I would need to specify the private
keys for my session and this is the
private key for my session say okay I am
in my uh Amazon ec2 instance so let me
look at what are the other things that I
need to do all right so as I said I just
brought up my ec2 instance but I would
need my ec2 instance to run few things
on my behalf so that it can spin up ec2
other ec2 instances it can also talk to
an S3 bucket where I'm going to store
the instance state of my kubernetes
cluster also some sort of an Autos
scaling group because I want to spin up
more and more instances I also want to
have a private hosted Zone using my
Route 53 so I would need my ec2 instance
to have all these sort of permissions
for my server so what I will do is I
will go and create a permission a rule
in the ec2 or rather in the AWS IM Rule
and I will attach this IM rule to my E2
instance all right so let me go to my IM
am which is nothing but the identity and
uh access management I will create a
role called as possibly cops role I will
create a role this role will be used by
my ec2 so I'm going to click on ec2 I'll
say next permissions they would need a
lot of permissions uh specific
permissions to be very honest and the
permissions are all listed out here S3
ec2 and all this stuff just to keep it
pretty simple what I would do is I'll
create a role with the administrative
access all right so I don't want any
tags I will review I'll create a role
name I will say cops roll for E2 all
right so I'm going to create a role
which has got administrative access R so
I'm going to create a role so this would
ensure that my ec2 instance from which
I'm going to run my cluster would have
all the prerequisite permission so that
it can go and spin up instances talk to
S3 buckets and all that stuff all right
now let us have our running instance get
powered by this role so I will connect
this role to my running instance all
right so this is my um e to instance
that is running I'll set action attach
attach attach attach replace IM am roles
so there is no role as of now I would
want my cops role for ec2 this is the
role that I created so I want this new
rule to be assigned to my ec2 all right
great now my ec2 instance which is my
cops server has got all the required
permissions so that he can create a
cluster on my behalf great now before
this let me do a sudo yum update hyphone
y so that any of the because this is a
newly provisioned VM I just want to
ensure that all my library system
libraries are all updated so that when I
do install cob and Cube CTL and all
those things none of them will fail
because of some dependency or package
issues so I'm just running a Pudo update
iPhone y so that all the libraries are
updated okay looks like it is done so
let me go ahead and uh install cops cops
is a pretty simple installation it is
available in a particular location I
have to just copy this and ensure that I
do a curl and I install this particular
stuff all right so it is fetching uh the
cops tool it's installing for me once it
is copy down let me change the mode so
that I can execute it and then also let
me move it to my user local bin so that
it is available for me in my path okay
that's pretty much the cops installation
let me also do one other thing let me
install something called as qctl so this
is what would be you know a tool using
which I'm going to be firing my uh
commands to my cubis cluster once it
comes up okay this is a pretty smaller
executable so qctl all right so I have
ql as well as cops installed on my easy2
server now okay now what I'll do next is
I'm going to create an S3 bucket in AWS
so that the kues uh cluster state is
persisted in this bucket so let me
create a simple bucket with this name S3
bucket with this name so let me go to S3
and uh let me create this simple bucket
with this name I will just say create a
bucket okay so the simply learn.
kubernetes is the bucket that I created
so this bucket will store all
information about my cluster let me now
uh go and create a DNS entry or a DNS
zone or AWS Route 53 hosted zone for my
cluster so this is required so that you
know I can give a cluster name for my
 cluster that I'm coming up so I will
create a private hostage Zone and possib
possibly I will call call this as um
simply learn. in so this will be my name
of my private hostage Zone in case you
already have any other uh public hostage
zones in your name you can always put a
Route 53 hostage Zone specific for your
domain name since I don't have anyone
I'm just going to create a simple
private hostage zone so let me head down
to Route uh 53 click on any of these and
you'll get this hosted zones out here so
I'm going to create a hosted Zone here
I'm going to create a hosted Zone click
on create hostage Zone um this would be
simply learn. in I want to create a
public no I don't want public I want a
private hostage Zone and I'm going to
associate my VPC ID uh for this hostage
Zone since I'm going to try out all my
exercises in the Ohio region I will
associate the VPC of the Ohio region for
this all right so this is the one uh
that is specific to the Ohio region so I
will say this one I'll say create a
hostage Zone great now let me come back
to my E2 box my ec2 is all powered with
uh whatever it needs in order to uh go
ahead with the installation of my
cluster only a few things that I need to
take care of is that you know now that I
put a name for my cluster I also have an
uh S3 bucket that I configur for my
cluster I will have to ensure that I
need to put in these two configurations
as a part of my uh cluster building
activity so I will open my uh bash RC
file and I'm going to export these two
variables if at all you remember well uh
these two VAR lables are nothing but the
cluster name which is nothing but the
public or sorry the private hosted Zone
that I created and the S3 bucket where
I'm going to create or rather I'm going
to store my cluster state so let me copy
these two and open up my bash RC file
all right so I will just add these two
and copy these and Export them out as my
uh variable I'm going to save this here
now let me ensure that this gets picked
up all right I've got these two that
I've configured and I've also ensure
that this environment variables are set
now let me create a pair of SSH keys
this is to ensure that I can log into
the box that I'm going to be
provisioning as a part of my cluster SSH
hyphen keyen I don't want to give any
parameters let it go to the home
directory with the default uh name and
without any passphrase all right so I
created a bunch of my keypad now I'm all
set to go ahead and run my cops now cops
will ensure that this will take some
time for the cluster to come up but this
is exactly how I would Define my cops
command I already copied here so cops
create cluster the state of the cops
cluster will be stored in this
particular variable which is what I have
exported out the number of note count is
two note count is the worker note count
if at all I don't spec there's an
configuration for specifying the master
or the control p as well if I don't
specify that the default one is one so
I'm going to create one primary or the
master node and two uh worker nodes uh
size of my master not size of the worker
nodes uh what are the zones where I want
uh these to be created and where do I
store the cluster information and uh
okay I've already added this must count
this is actually not required but this
is the command that I'm going to fire
off so that I bring up my cluster all
right that went off U very fast but um
this actually did not create a cluster
it is just the definition of a cluster
and uh that's why this came out very
fast now that everything looks good with
uh whatever configuration that I
specified now let me go ahead and create
the cluster by saying cops update
cluster hyph iphone8 yes now this will
take some time a good 5 to 10 minutes
for it to because this will actually
start provisioning the servers and as I
mentioned earlier based upon my
configuration I'm going to come up with
one master server and uh two nodes or
the worker nodes all right so this is
the command for me to validate the
cluster and I'm going to try it out
first I'm pretty sure that it'll fail
because all my servers are not up yet it
is taking some time with the validation
everything failed but let me try to look
at my ec2 instances and U see how many
servers do I have running as of now if
you see I had only one server that had
started which was my cop server and
automatically the other three instances
one is called nodes. simply. in uh this
these two are the nodes and this is the
master node so these three got
automatically provisioned by the cops
command that I ran all right so this may
take a while for it to get validated a
good 5 to 10 minutes so what I'll do is
I'll pause the video now and I'll come
back once um the server uh the cluster
is up and running it's been a good uh 8
to 9 Minutes uh since I started my
cluster so let me validate now okay
seems good so there's a master node uh
minimum One Max one some nodes which are
nothing but the slave nodes or the
worker notes there are two of them du to
mic Chrome uh subnet so my clusters seem
to be up and running and uh my cluster
name is is simply learn. in so what I
would want to do is now let me just log
into the master and see if I can run
some pots so let me get back to my inst
installation steps here the validation
cluster is done so let me log into my
since my cluster name is simply l.n this
is the way to log into my box so let me
get into my box so if you see here uh
this is the host name of the machine
that I'm currently in if you see this
this is nothing but our discop server
this is now from here I'm trying to get
into the master box all right so if you
see the IP address has changed I was in
a different box I'm in a different box
now if I see host name you'll find a
different host name so this is 153 which
is nothing but the master node yep it's
153 this is the particular host so I'm
getting into this machine now so I
started the cluster from my cop server
here it ran and brought up three nodes
so I'm actually getting into my master
node and see if I can run some pots on
it all right so let me try cctl uh get
cluster info try Cube CTL get nodes all
right so there are three nodes here
Master node and um one master node and
two worker nodes so let me see if I have
some pods here ql get pods so there's no
pod as of now so what I'll try to do is
let me just pin up a very very simple
part just to check if my connections is
everything is correct or not correct I
have a simple um nodejs application that
I've built uh and I've got a container
for that this is already push to the
docker Hub registed is called simply
learn dockerhub and the image name that
I have here is called my node app and uh
I will use this image to power one of my
pods that I will run so this is the way
in which I'm going to launch my pod let
me show you this commands qctl run the
name for my deployment that I'm going to
run hyphen ien image and this is the
image that is going to be powering my
container so this is simply learn
dockerhub for/ my node app Hy replica
equal to I want two replicas to be
running and the port that I want to
expose this particular pod is on 8080 so
let me run this as soon as this is run
it creates something called as a
deployment all right now let me just say
um I'm not really interested in the
deployment I'm interested in checking if
at all the parts are up and running so I
will say Cub CTL get po all right so
that was pretty fast so I have two parts
that are running here these are two
replicas because I asked for two
replicas these parts are running so I
can run Cube C describe part pick up all
right I can pick up any of the Pod name
and see what is the Pod what is the
information does it contain from what is
the image that is pulling what is the
image image Name ID this is actually
running my pod which is actually
spinning up a container great so first
all good so in kubernetes the pods can
are ephemeral so they can be there at
any time cannot be at any time if I need
to expose my pod outside I will have to
create a service for my part so what
I'll do is I'll create a very very
simple uh service by exposing this
particular uh deployment before that let
me check Cube cial Get deployment so
there's a deployment that is created the
deployment name is simply learn app so
I'll will expose my deployment simply
learn app as um yeah I'll just expose
this let me see all right I'm not
specifying what type and all I don't
want to get into the complications of uh
what are the different types of exposing
the service and all that stuff so if I
don't specify anything uh it gives me
something called a cluster IP so this is
where my pod is actually exposed so let
me just check if at all this part is up
and running I'll just try a simple curl
command uh curl HTTP colon this is my
cluster IP and uh the port is 880 if at
all I hit this it's actually hitting my
application and giving me uh whatever um
I put a simple uh sis. out kind of a
thing where I'm just printing um the
container ID of whatever pod is serving
uh bigor out of so I'm actually hitting
my container and I'm getting this output
from the container so everything looks
good my container is up and running uh
so let me just go and clean it up I will
say Cub CTL uh Delete deployment simply
Lear app they should get rid of all the
parts that I created Cube CTL get all
right these parts are inter terminating
things let me just check Cube CTL get
Services there's one service I don't
want this service let me delete that CU
CTL delete service I want this all right
that's SS good so let me come back to my
host my cop server from where I'm
running so I managed to successfully
verify this part see if everything is up
and running so what I'll do is I'll just
go ahead and complete this uh demo by
going and uh getting rid of my cluster
so cops delete cluster hyphen all right
so this will ensure that it will clean
up my complete uh cluster that I created
so I had three or four running instances
if you see that all the three are
shutting down because you know the cop
server which had cops installed on it uh
is now got a mandate to go ahead and uh
shut down and clean up the whole uh
instances and all those things that are
created as a part of my deployment if
you are looking to get certified into
devops and become a kubernetes and
Docker expert have a look at wide range
of courses on devops by simply learn in
collaboration with topnotch universities
across the globe so why wait enroll the
course now from the link in the
description box this is dark so let's go
through a couple of scenarios let's do
one for kubernetes and then one for
Docker and we can actually go through
and understand what the problem specific
companies have actually had and how
they're able to use the two different
tools to solve them so our first one is
with Bose and Bose um had a large
catalog of products that kept growing
and their infrastructure had to change
so the way that they looked at that was
actually establishing two primary goals
uh to be able to allow their product
groups to be able to easier more easily
catch up to the scale of their business
so after going through um a number of
solutions they ended up coming up with a
solution of having kubernetes running
their iot platform as a service inside
of Amazon's AWS cloud service and what
you'll see with both these products is
they're very Cloud friendly but here we
have um Bose and kubernetes working
together with AWS to be able to scale up
and meet the demands of their product
catalog and so the result is that we
were able to increase the number of non
production deployments significantly by
taking the number of services from being
large bulky Services down to small micro
Services being able to handle as many as
1,50 plus deployments every year an
incredible amount of time and value has
been opened through the use of
kubernetes now let's have a look at
darker and see what a similar problem
that people would have so uh the problem
is with PayPal and PayPal um processes
something in the region of over 200
payments per second across all all of
their products and PayPal doesn't just
have PayPal they have brain tree and
venmo so the challenge um that uh PayPal
was uh really being given is that they
had different architectures which
resulted in different maintenance cycles
and different deployment times and an
overall complexity from having a decades
old architecture with PayPal through to
a modern architecture with venmo through
the use of docka payal was able to unify
the application delivery be able to
centralize the management of all of the
containers uh with one existing group
the net net result is that PayPal was
able to migrate over 700 applications
into dogger Enterprise which consists of
over 200,000 containers this ultimately
opened up a 50% increase in availability
for being able to in add in additional
time for building testing and deploying
of applications just a huge win for
PayPal now let's dig into kubernetes and
Docker so kubernetes is an open source
platform and it's designed for being
able to maintain a large number of
containers and what you're going to find
is that your argument for kubernetes
versus Docker isn't a real argument it's
kubernetes and Docker working together
so kubernetes is able to manage the
infrastructure of a containerized
environment and Docker is the number one
container App Management solution and so
with doer you're able to automate the
deployment of your applications being
able to keep them in a very lightweight
environment and being able to uh create
a nice consistent experience so that
your developers are working in the same
containers that are then also pushed out
to production so with Docker you're able
to manage multiple containers running on
the same Hardware much more efficiently
than you are with a VM environment the
productivity around Docker is extremely
high you're able to keep your
applications very isolated uh the
configuration for doca is really quick
and easy you can be up and running in
minutes with Docker once you have it
installed and running on your
development machine or inside of your
devops environment so we look at the
deployment between the two um and the
differences kubernetes is really
designed for a combination of PODS and
services in its deployment whereas with
Docker it's around about deploying
services in containers uh so the the
difference um here is that kubernetes is
going to manage the entire environment
and then and that environment consisting
of PODS and inside of a pod you're going
to have all of your containers that
you're working on and those containers
are going to control the services that
actually power the applications that are
being deployed kubernetes is by default
an autoscaling solution it has it turned
on and is always available whereas
Docker does not and that's not
surprising because Docker is a tool for
building out Solutions whereas
kubernetes is about managing your
infrastructure kubernetes is going to um
run health checks on the liveness and
Readiness of your entire environment so
not just one container but tens of
thousands of containers whereas Docker
is going to limit the health check to
the services that it's managing within
its own containers now I'm not going to
kid you kubernetes is quite hard to set
up it's it's if of the tools that you're
going to be using in your devop
environment it's it's not an easy setup
for you to use um and for this reason
you want to really take advantage of the
surfaces within Azure and other similar
Cloud environments where they actually
will do the setup for you Docker in
contrast is really easy to set up you as
I mentioned earlier you can be up and
running in a few minutes as you would
expect the fault tolerance within
kubernetes is very high and this is by
Design because the architecture of
kubernetes is built on the same
architecture that Google uses for
managing its entire Cloud infrastructure
in contrast Docker has lower fa
tolerance but that's because it's just
managing the the services within its own
containers what you'll find is that most
public Cloud providers will provide
support for both kubernetes and Docker
here we've highlighted Microsoft Azure
because they were very quick uh to jump
on and support kubernetes uh but the
reties is that today Google Amazon and
many other providers are having first
level support for kubernetes it's just
become extremely popular in a very very
short time frame the companies using
both kubernetes and Docker is vast and
every single day there are more and more
companies using it and you should be
able to look and see whether or not you
can add your own company to this list so
let's take a little scenario of a
developer and a tester before you had
the world of Docker a developer would
actually build their code and then they
send it to the tester but then the code
wouldn't work on their system the code
doesn't work on the other system due to
the differences in computer environments
so what could be the solution to this
well you could go ahead and create a
virtual machine to be the same or the
solution in both areas we think Docker
is an even better solution so let's kind
of break out what the main big
differences are between Docker and
virtual machines as you can between the
left and the right hand side both look
to be very similar what you'll see
however is that on the docker side what
you'll see as a big difference is that
the guest OS for each container has been
eliminated Docker is inherently more
lightweight but provides the same
functionality as a virtual machine so
let's step through some of the pros and
cons of a virtual machine versus Docker
so first of all a virtual machine
occupies a lot more memory space on the
host machine in contrast Docker occupies
significantly less memory space the boot
up time between both is very different
Docker just boots up faster the
performance of the docker environment is
actually better and more consistent than
the virtual machine Docker is also very
easy to set up and very easy to scale
the efficient encies therefore are much
higher with a Docker environment versus
a virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
docker environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester and as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally will step through
the different components of the docker
environment so what is devops devops is
a collaboration between the development
team and the operation team allowing you
to continuously deliver Solutions and
applications and services that both
delight and improve the efficiency of
your customers if you look at the vend
diagram that we have here on the left
hand side we have development on the
right hand side we have operation and
then there's a crossover in the middle
and that's where the devops team sits if
we look at the areas of integration
between both groups developers are
really really interested in planning
codee building and testing and
operations want to be able to
efficiently deploy operate and monitor
when you can have both groups
interacting with each other on these
seven key elements then you can have the
efficiencies of an excellent devops team
so planning and co-base we use tools
like jit and geara for building we use
Gradle and Maven testing we use selenium
the integration between Dev and Ops is
through tools such as Jenkins and then
the deployment operation is done with
tools such as Docker and Chef finally
nagas is used to monitor the entire
environment so let's step deeper into
what Docker actually is so Docker is a
tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
the container is actually a software p
package that consists of all the
dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what doger is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our ABNB
example so we have three rooms and a
kitchen and cupboard how do we resolve
this issue well we put a kitchen and
cupboard in each room we can do the same
thing for computers Docker provides the
suitable Frameworks for each different
application and since every application
has a framework with a suitable version
this space could also then be utilized
for for putting in software and
applications that Al and since every
application has its own framework and
suitable version the area that we had
previously stored for a framework can be
used for something else now we can
create a new application and this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer it's no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
and this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker demon which runs
on the server itself will then check out
the interaction and the necessary
operating system pieces needed to be
able to run the container okay so that's
just an overview of the docker engine
which is probably where you're going to
spend most of your time but there are
some other components that form the
infrastructure for Dockers let's dig
into those a little bit deeper as well
so what we're going to do now is break
out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Damon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows it's going to be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker demon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instruction on
how to build and run your Docker images
and containers if we look at the images
part of our registry here we actually
see that the image is really just a
template with the instructions used for
creating the containers which you use
within Docker the docker image is built
using a file called the docker file and
then once you've created that Docker
file you'll store that image in the
docker Hub or red Street and that allows
other people to be able to access the
same structure of of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their dependencies it's the instructions
for what your environment will look like
so you can be consistent in how that
environment is shared between multiple
developers testing units and other
people within your devops team now the
thing that's great about working with
Docker is that it's so lightweight that
you can actually run multiple Docker
containers in the same infrastructure
and share the same operating system this
is its strength it allows you to be able
to create those multiple environments
that you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use
the docker registry which is an
open-source serviz service for host Ing
and distributing the images that you
have defined you can also use Docker
itself as its own default registry and
Docker Hub now something it has to be
bear in mind though is that for publicly
shared images you may want to have your
own private images in which case you
would do that through your own registry
so once again public repositories can be
used to host the docket images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because then may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you start
creating environments that are very
specific to the solutions that you are
building and when you get to that point
you'll likely want to create a private
repository so you're not sharing that
knowledge with the world in general now
the way in which you connect with the
docker registry is through simple pull
and push commands that you run through
terminal window to be able to get the
latest information so if you want to be
able to build your own container what
you'll start doing is using the pull
commands to actually pull the image from
the docker repository and the command
line for that is fairly simple in
terminal window you would write Docker
pull and then you put in the image name
and any tags associated with that image
and use the command pools so in your
terminal window you would actually use a
simple line of command once you've
actually connected to your Docker
environment and that command will be
Docker pull with the image name and any
Associated tags around that image what
that will then do is pull the image from
the docker repository whether that's a
public repository or a private one now
in Reverse if you want to be able to
update the docker image with a new
information you do a push command where
you would take the script that you've
written about the docker container that
you define and push it to the repository
and as you can imagine the commands for
that are also fairly simple in terminal
window you would write Docker push the
image name any Associated tags and then
that would then push that image to the
docker repository again either a public
or a private repository so if we recap
the docker file creates a Docker image
that's using the build commands a dock
image then contains all the information
necessary for you to be able to execute
the project using the docker image any
user can run the code in order to create
a Docker container and once a Docker
image is built is uploaded to a registry
or to a Docker Hub where it can be
shared across your entire team and from
the docker Hub users can get access to
the docker image and build their own new
containers so the five key takeaways
here so with a virtual machine you're
able to create a virtualized environment
to run an application on an operating
system with Docker it allows you to
focus on just running the application
and doing it consistently it improves
the ability for team teams to be able to
share environments that are consistent
from Team to team it's highly productive
and it's really quick and easy to
configure the architecture of Docker is
really primarily built out of four
components of which the one that you'll
use the most is the client server
environment where as a developer you
have a client application running on
your local machine and then you connect
with a server environment where you're
getting the latest information about
that container that you're building a
solution for and then finally what we
see with the workflow improvements with
Docker is that the goal is to be able to
be more efficient to be able to be more
consistent with your development
environments and be able to push out
those environments whether it goes to a
test person to a business analyst or
anybody else on your devops team so they
have a consistent environment that looks
and acts exactly like your production
environment and can be eventually pushed
out to a production environment using
tool such as puppet or Chef so you're
creating a consistent operations
environment so before we begin let me
give you an intro to Docker so what is
Docker Docker is a tool which is used to
automate the deployment of applications
in lightweight containers so when I say
containers I mean a software package
that consists of all the dependencies
required to run the particular
application and when you deploy
containers you're basically providing
the capability for the application to
run in any kind of environment so
Dockers have multiple features some of
which are that multiple containers can
run on the same Hardware it has very
high productivity when compared to
Virtual machines it maintains isolated
applications and has a very quick and
easy configuration process so now let's
begin with the demo we'll be installing
Docker on an Ubuntu system so this is my
system I'll just open the terminal so
the first thing you can start with is
removing any Docker installation that
you probably already have present in
your system if you want to start from
scratch so this is the command to do so
sud sudo app get remove
Docker Docker engine docker.io enter
your
password and Docker is removed so now
we'll start from scratch and we'll
install Docker once again before that
I'll just clear my
screen okay so before I installed Docker
let me just ensure that all these
softwares on my system currently is in
its latest
date so suro app get
update great so that's done next thing
we'll actually install our
Docker so type in pseudo app
apt
get
install
Docker now as you can see here there's
an error that's occurred so sometimes
it's possible that due to the
environment of the machine that you're
working in this particular command does
not work in which case there's always
another command that you can start
with
just type Docker
install and that by itself will give you
the command you can use to install
Docker so as it says here sud sudo app
installer.io is a command that we will
need to execute to install
Docker and after that we'll execute the
sudo snap install Docker so sudo apt
install docker.io
first and this will install your
Docker after that's done we will have
sudo snap install Docker so snap install
Docker installs a newly created snap
package they are basically some other
dependencies for Docker that you'll have
to
install of course since this is the
installation process for the entire
darker IO it will take some
time
great so our Docker is installed the
next thing we do as I mentioned earlier
is that we need to install all the
dependency packages so the command for
that
is sudo snap install
talker enter your
password so with that we have completed
the installation process for Docker but
we'll per perform a few more stages
where we will test if the installation
has been done
right so before we move on with the
testing for Docker let's once again just
check the version that we have
installed so for that the command is
doer version and as you can see doer
version 17.1 12.1 CE has been installed
next thing we do is we pull an image
from the docker Hub so Docker run
hello
world now hello world is a Docker image
which is present on the docker Hub
Docker Hub is basically a repository
that you can find online so with this
command the docker image hello world has
been pulled onto your
system so let's see if it's actually
present on your system now the command
to check this is pseudo doer
images and as you can see here hello
world repository this is present on our
system currently so the image has been
successfully pulled onto the system and
this means that our Docker is working
now we'll try out another
command
suro doer PS minus a this displays all
the containers that you have pulled so
far so as you can see here there are
three hello world images displayed and
all of them are in exited state so I did
this demo previously too which is why
the two hello worlds which is created 2
minutes ago is also displayed here and
the first hello world which has been
created a minute ago is the one we just
did for this demo now as as you have
probably noticed that all the hello
world images over here all these
containers are in the exited state so
when you give the option for Docker PS
minus a where minus a stands for all it
displays all the containers whether they
are in exited or running state if you
want to see only those containers which
are in their running State you can
simply execute Pudo Docker
PS
suro
Docker P
yes and as you can see no container is
visible here because none of them are in
running State and with that we come to
an end of our Docker installation then
now Docker is something which is
available for most of the operating
systems different different platforms so
it supports both the Unix and the
windows platform as such so um Linux
through various commands we can do the
installation but in the case of Windows
you have to download the exe file and a
particular installer from the dockerhub
websites you can simply Google it and
you know will get a kind of link from
where you will be able to download the
package so let's go to the Chrome and uh
try to search on for the windows Str uh
particular installer you will get a link
from dockerhub you download it you get
the stable version you get the edge
version whichever version you want you
wish to download you can download it so
let's go back to the Chrome so here you
have the docker desktop for Windows so
you can go for the stable or you can go
for the edge right so you have also have
the comparison that what is the
difference between these two versions
right so um the particular Edge version
is something which is getting releases
every month and uh the um stable version
is getting the releases every quarter so
they are not doing much of the changes
to the stable version as compared to the
edge there so you just have to double
click on the installer and that will
help you to do the installation of the
process so let's get started so you just
click on the get instable version so
when you do that the uh particular
installer is going to install now it's
going to take like around 300 MB there
so that's the kind of installer which is
available so uh once the installer is
downloaded so what you can do is that
you can actually go ahead and you can uh
proceed with the doing the double click
on this installer when you double click
on that you have to proceed with some of
the steps like you know from the GUI
itself you are going to proceed with
these steps so we'll wait for 10 to 20
seconds more and then the installer will
be done and then we can do the double
click and the installation will proceed
so another thing is that uh there is a
huge difference between the installer
like for example in case of Unix the
installer is a little bit less but in
case of windows it's a gy is also
involved and there are a lot of binaries
which is available there so that's the
reason why you know the huge size is
there now it's available for free that's
for sure and it also requires the
Windows 10 professional or Enterprise
64bit there so um if you are working on
some previous uh version of operating
systems like Windows 7 and all you have
the older version called Docker toolbox
so they used to call it as like Docker
toolbox earlier but now they are calling
it as a Docker desktop with the new
Docker uh Windows 10 support as such
here so another couple of seconds and
then the installer will be done and then
we will be able to proceed with the
installation so let's see that how much
progress is there to the download so
we'll click on the downloads and here
still we have some particular
installations or some download going on
so we'll wait for some time and uh once
the installation is done then we'll go
back and uh we'll proceed with
installation so couple of seconds so
it's almost done so I'll just click on
this one you can go to the directory to
the downloads and you can double click
on that also but if you want to do the
installation you can click on this one
also and it will ask for the approval
yes or no you have to provide now once
that is done so um a desktop a kind of a
GUI component will open there so it will
start proceeding with installation so
it's asking whether you want to add the
desktop the shortcut to desktop so you
can say okay I'm going to click on okay
so it will unpack the
files all the files uh which is required
for Docker to successfully install that
is getting unpacked over here so it will
take some time to do the installation
because it's doing a lot of work here so
you can just wait for till the execution
of the installer to be completed and
once the in inster is done you can open
your command line and start working on
the
docker so taking some time to extract
the
files now it's asking us to you know do
the close and uh do the restart so once
that is done you will be able to proceed
further and you can just you know run
the command line and uh any Docker
command if you can run so that will give
you you the response whether the docker
is installed or not so you can see here
that Docker is you know something which
is installed so you can run like Docker
version you will be able to get a
version of the client when you do the
restart of the machine then at that
moment of time the docker server will
also be started and then this particular
error message will go off right now the
docker demon is not up and running
because the installation requires a
restart and when you close on this one
and go for the restart the machine will
be restarted here so this is the way
that how exactly we can go for a Docker
installation and we can go on that part
so let's looking at some of the more
advanced concepts within the docker
environment and we're going to look at
two Advanced components one is Docker
compose and the second is Docker swamp
so let's look at Docker compose Docker
compose is really designed for running
multiple containers as a single service
and it does this by running each
container in isolation but allowing the
containers to interact with each other
as was stated earlier on you would
actually write the composed environment
using yaml as the language in the files
that you would create so where would you
use something like Docker compose so an
example would be if you are running an
Apache server with my SQL database and
you need to create additional containers
to run additional services without the
need to start each one separately and
this is where you would write a set of
files using docket compos to be able to
help balance out that demand so that's
now look at Docker swarm so Docker swarm
is a service that allows you to be able
to control multiple Docker environments
within a single platform so what you
actually are looking at doing is within
your Docker swarm is we treating each
node as a Docker Damon and we're
actually having an API that's
interacting with each of those nodes
there are two types of node that you're
going to be getting comfortable working
with one is the manager node and the
second is the worker node and as you'd
expect the manager node is the one
sending out the instructions to all of
the worker nodes but there is a two-way
communication that is happening the
communication allows for the manager
node to be able to manage the
instructions and then listen to and
receive updates from the working node so
if anything happens within this
environment the mag node can react and
adjust the architecture of the work node
so it's always in sync it's really great
for large scaled environments so finally
let's go through what are some of the
basic commands you would would use
within Docker and once we've gone
through all these basic commands we'll
actually show you a demo of how you'd
actually use them as well so if we're
going to go in probably the first
command is to install Docker and so if
you have yum installed you just do yum
install Docker and you'll install Docker
onto your computer to start the docker
Damon is you want to do system CTL start
Docker the command to remove a Docker
image is Docker RMI and then the image
ID itself and that's not the image name
that's the actual alpha numeric ID
number that you want to uh grab the
command line to download a new image is
Docker pull and then the name of the
image you'd want to pull and by default
you're going to be pulling from the
docker default registry that will then
connect to your Docker Damon and
download the images from that
registry the command line to run an
image is Docker run and then the image
ID and then we have the if we wanted to
pull specifically from Docker Hub then
we would have uh Docker pull and then
the image name and colon its tag to pull
build an image from a Docker file you
would do Docker build- T and then the
image name and colon tag to shut down
the container you do Docker stop
container ID the access for running a
container is Docker exact it container
ID bash so we've gone through all the
different commands but let's actually
see how they would actually look and
we're going to go ahead and do a demo so
welcome to this demo where we're going
to go ahead and put together all of the
different commands that we outlined in
the presentation for Docker uh first is
just to list all of the docker images
that we have so we do pseudo Docker
images and we enter in our p
word and this will Now list out the
images that we've created already and we
have three images
there so let's go ahead and pull a
Docker image so to do that we'll we'll
go ahead and type pseudo
Docker and actually we don't want to do
image we want to select pull and then
the name of the image that we want to
pull which is going to be my
SQL and by default this is actually
going to go ahead and use the latest
MySQL command MySQL image that we have
so it's now going to ahead and pull this
image it's going to take a few minutes
depending on your internet connection
speed it's kind of a large file that has
to be downloaded so we'll just wait for
that to
download you see the others have
completed just waiting for this last
file to download almost there once
that's done what we're going to go ahead
and do is we'll actually uh run the dock
container and create the new con
container using the image that we just
downloaded but we have to wait for this
to download
first all right so the image has been
pulled from Docker Hub and let's go
ahead and create the new Docker
container so we're going to do pseudo
Docker run
dasd
dasp
0.0.0.0
colon
80 colon
80 and then put in my SQL callon latest
so we have the latest
version and we have our new
token and that shows our new docket
container has been
created and let's go ahead and see if
the container is running and we'll do
PSE sudo
Docker PS
to list all the running containers and
what we see is that the container is not
listed there which means it's probably
not running so let's go ahead and list
out all of the images that we have
within Docker so we can see whether it's
actually listed there so we'll do ps- a
and yes there we are we can see that we
do have our new container my SQL latest
and it was created 36 seconds ago but
it's in the exited mode so what we have
to do is we have to change that status
so it's actually
running so let's change that to running
State we'll do
pseudo
Docker
run
Dash it dash
dash
name and we can name
it
SL
SQL
my
SQL slash bin
slash
Dash and that's now going to be in the
root and we'll exit out of that and now
if we list out the docking containers we
should see it is now an active
container Soo Docker
start and then we'll start start the say
it and
then and we should now see
it there we are it's now in the running
State
excellent and we can see that it was
updated 6 seconds
ago we're going to go ahead and we're
going to clear the
screen okay now what we want to do is
remove the docker container so we're
going to do is checklist of images that
we
have and so pseudo Docker images here
are the images that we have and we have
my SQL is listed and what we want to do
is delete my SQL and to do that we're
going to type in pseudo doer rm- F image
my
SQL run that command and what we'll find
is the image uh there's no such image oh
okay so what we actually have to do is
we have to go and see that the image is
now gone it's uh been removed excellent
it's exactly what we wanted to
see and we can also delete an image by
its image ID as
well however if an image is running and
active we have to kill that image first
so we're going to go ahead and we're
going to select the image ID we'll copy
that and it's going
to we paste that it won't be able to
actually run correctly because the image
is active so what we have to do now is
stop the image and then we can kill
it so it's in the running state so we
have to
do so we do pseudo darker kill and kill
SL that will kill the container and now
we'll see that the container has
gone and now we can delete the image and
that's going to be the image gone with
image ID b boom easy
peasy okay let's go ahead on to the next
exercise which is
to so here we are we've listed all of
the uh containers and they're all gone
so let's go to the next exercise final
exercise which is to actually create a
batch image and we're going do a batch
HTTP image so let's go ahead and write
that out so it's going to be Docker
[Music]
run
dashd dash dash
name white that's going to be the name
of this HTP service-
PP 080 colon 80-
V open
quote dollar sign
PWD close
quotes colon SL
USR SL
looc slash Apache 2 slht dos
slash
httpd semicolon
2.4 we run
that our password
again so what we see is the port is
already been used so let's go ahead and
see which ports let's go see if we can
change the port or see what ports are
running so let's do pseudo images and
see which ports are being used because
it's either the the port or the name um
hasn't been put in correctly so pseudo
doer
images
PS PSE sudo doer PS d
a and yep there's Port 80
there so we'll clear the
screen
so we're going to change the container
name because I think we actually have
the wrong container name here so let's
go in and change that and we'll paste
that in and voila here we go now working
and we'll just double check and make
sure everything's working correctly so
to do that we'll go into our web browser
and we'll type in soon as Firefox opens
up type in Local
Host colon
8080 which was the the port that we
created and there we are it's a list of
all the files which shows that the
server is up and running so let's have a
look at what we have in our current
environment so today when you actually
have your standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines are now sitting within a
hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce
significantly the different elements
that you would normally have within a
virtualized environment so we're able to
get rid of the the bins and the so we're
able to get rid of the guest OS and
we're able to eliminate the hypervisor
environment and this is really important
as we actually start working and
creating environments that are
consistent because we want to be able to
make it so it's really easy and stable
for the environment that you have within
your Dev and Ops environment now
critical is getting rid of that
hypervisor element it's just a a lot of
overhead so let's have a look at a
container as an example so here we
actually have a couple of examples on
the right hand side we have different
containers we have one container running
a pachy tom cat in a with Java a second
container is running SQL server in a
microsoft.net environment the third
container is running python with mySQL
these are all running just fine within
the docker engine and sitting on top of
a host OS which could be Linux it really
could be any host OS within a consistent
infrastructure and you're able to have a
solution that can be shared easily
amongst your teams so let's have a look
at an example that you'd have today if a
company is doing a traditional Java
application so you have your developer
is working in JBoss on his system and
he's coding away and he has to get that
code over to a tester now what will
happen is that tester will then
typically in your traditional
environment then have to install J boss
on their machine and get everything
running and and hopefully set up
identically to the developer chances are
they probably won't have it exactly the
same but they'll try to get it as close
as possible and then at some point you
want to be able to test this within your
production environment so you send it
over to a system administrator who would
then also have to install JBoss on their
environment as well yeah this just seems
to be a whole lot of duplication so why
go through the problem of installing
JBoss three times and this is where it
things get really interesting because
the challenge you have today is that
it's very difficult to almost impossible
to have identical environment if you're
just installing software locally on
devices the developers probably got a
whole bunch of development software that
could be conflicting with the Jos
environment the tester has similar
testing software but probably doesn't
have all the development software and
certainly the system administrator won't
have all the tools that the developer
and tester have their own tools and so
what you want to be able to do is kind
of get away from The Challenge you have
of having to do local installations on
three different computers and in
addition what you see is that this uses
up a lot of effort because when you're
having to install software over and over
again you just keep repeating doing
really basic found ational challenges so
this is where Docker comes in and Docker
is the tool that allows you to be able
to share environments from one group to
another group without having to install
software locally on a device you install
all of the code into your Docker
container and simply share the container
so in this presentation we're going to
go through a few things we're going to
cover what Docker actually is and then
we're going to dig into the actual
architecture of Docker and kind of go
through what Docker container is and how
to create ack Docker container and then
we'll go at through the benefits of
using Docker containers and then the
commands and finalize everything out
with a brief demo so what is darker so
darker is as you'd expect because all
the software that we cover in this
series is an open-source solution and it
is a container solution that allows you
to be able to containerize all of the
necessary files and applications needed
to run the solution that you're building
so you can share it from different
people in your team whether it's a
developer tester or system administrator
and this allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of uh
two key elements there is the docker
client and then there is a rest API
connection to a Docker Damon which
actually hosts the entire entire
environment within the docker host and
the docker Damon you have your different
containers and each one has a link to a
Docker registry the docker client itself
is a rest service so as you'd expect a
rest API and that sends command line to
the docker Damon through a terminal
window or command line interface window
and we'll go through some of these demos
later on so you can actually see how you
can actually interact with Docker the
dock Damon then checks the request
against um the dock components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the actual container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so the
important part of a duckin container is
that it has has everything you need to
be able to run the application it's like
a virtualized environment it has all
your Frameworks and your libraries and
it allows the teams to be able to build
out and run exactly the right
environment that the developer intended
what's interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that's using dependencies
on other libraries or files outside of
the container because of the AR Ure it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the files
the actual folder itself is much smaller
it's very secure highly portable and the
boot up time is incredibly fast so let's
actually get into how you would actually
create a Docker container so the docker
container itself is actually built
through command line and it's built of a
file and Docker image so the actual um
Docker file is a text file that contains
all the instructions that you would need
to be able to create that Docker image
and then will actually then create all
of the project code with inside of that
image then the image becomes the item
that you would share through the dock
registry you would then use the command
line and we'll do this later on select
Docker run and then the name of the
image to be able to easily and
effectively run that image locally and
again once you've created the doc image
you can store that in the dock registry
making it available to anybody within
your network so something to bear in
mind is that Docker itself has its own
registry called Docker Hub and that is a
public registry so you can actually go
out and see other dock images that have
been created and access those images as
your own company you may want to have
your own private um repository so you
want to be able to go ahead and either
do that locally through your own
repository or you can actually get a
licensed version of do a hub where you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a docket image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command and another
one is to actually put it into your
registry which is a push command so if
you want to get a image from a Docker
registry then you want to use the pull
command and a pull command simply pulls
the image from the registry um in this
example using uh ngx as our registry and
we can actually then pull the image down
to our test environment on our local
machine so we're actually running the
container with within a Docker
application on um a local machine we're
able to then have the image run exactly
as it would in production and then you
can actually use the Run command to
actually use the docker image on your
local machine so just a you know a few
interesting tidbits about the docking
container once the container is created
a new layer is formed on top of the dock
image layer is called the container
layer each container has a separate read
right container layer and any changes
made in that docking container is then
reflected upon on the particular
container layer and if you want to
delete the container layer the container
layer also gets deleted as well so you
know why would using Docker and
containers be of benefit to you well you
know some of the things that are useful
is that containers have no external
dependencies for the applications they
run once you actually have the container
running locally it has everything it
needs to be able to run the application
so there's no having to install
additional pieces of software such as
the example we gave with J boss at the
beginning of the presentation now the
containers are really lightweight so it
makes it very easy to share the
containers amongst your teams whether
it's a developer whether it's a tester
whether it's somebody on your operations
environment it's really easy to share
those containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
light nature of your containers the
container itself also runs in isolation
uh which means that it is not impacted
by any dependencies you may have on your
own local environment so it's a
completely sandboxed environment so some
of the questions you might ask is you
know can you run multiple containers
together without the need to start each
one individually and you know what yes
you can with Dock and compose Dock and
compose allows you to run multiple
containers in a single service and again
this is a reflection on the lightweight
nature of containers within the docker
environment so we're going to end our
presentation by looking at some of the
basic commands that you'd have within
Docker so we have here on the left hand
side we have a Docker container and then
the command for each item we're actually
going to go ahead and use some of these
commands in the demo that we're going do
after this presentation you'll see that
in a moment but just you know some of
the basic commands we have are
committing the docket image into the
Container kill is a you know standard
kill command to you know terminate one
or more of the running containers so
they stop working then restart those
containers but suddenly you can look at
all the image all of the commands here
and try them out for yourself so we're
going to go ahead and start a demo of
how to use the basic commands to run
Docker so so to do this we're going to
open up terminal window or command line
depending whether you're running Linux
PC or Mac and we're going to go ahead
and the first thing we want to do is see
what our Docker image lists are so we
can go pseudo Docker images and this
will give us well first we entering our
password so let's go enter that in and
this will now gives us a list of our
Docker images and here are the docker
images have already been created in the
system and we can actually go ahead and
actually see the processes that are
actually running so I'm going to go head
and open up this window a little bit
more but this will show you the actual
processes and the containers that we
actually have and so on the far left
hand side you see under names you have
learn simply learn bore cool these are
all just different ones that we've been
working on so let's go ahead and create
a Docker image so we're going to do
pseudo
docker
run-
d-p 0.0.0 do0 callon 80 callon
80 obuntu and this will allow us to go
ahead and run
anonto image and this will run the
latest image and what we have here is a
hash number and this hash number is a
unique name that defines the container
that we've just created and we can go
ahead and we can check to make sure that
the container actually is present so
we're going to do so pseudo doer. PS and
actually show us on there so it's not in
a running state right now but that
doesn't mean that we don't have it so
let's list out all the containers that
are both running and in the exit state
so let's do sucker ps- a and this lists
all the containers that I have running
on my
machine and this shows all the ones that
have been in the running State and in
the exit State and here we see one that
we just created about a minute ago and
it's called
learn
and these are all running obuntu and
this is the one that we had created just
a few seconds
ago let's open it up and there we go so
let's change that to that new Doc
container to a running state so scroll
down and we're going to type
pseudo
[Music]
Docker
run
dasit
DH name
my um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment and we'll put in the image
name which is
Ubuntu and Das bin Dash
bash and it's now in our rout and we'll
exit out of that
so now we're going to go ahead and start
the new my Docker container so PSE sudo
Docker
start
my and we'll get the container image
which will be my Docker my
Docker return and that started that
Docker image and let's go ahead and
check against the other running Docker
images to make sure it's running
correctly so pseudo Docker
PS and there we are underneath name on
the right hand side you actually see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds quite fast so we
want to rename the container let's use
the command pseudo
Docker rename we can take another Docker
image this grab this one and we'll put
it in rename and we'll rename and we put
in the old container name which is image
and then we'll put in the new container
name and and let's call
it
purple so now the container image that
had previously been called image is now
called Purple so we do pseudo Docker
PS to list all of our Docker
images and if we scroll up and there
there we go purple how easy is that to
rename an
image and we can go ahead and use this
command if we want to stop container so
we're going to write Pudo
Docker
stop and then we have to put it in the
container
name and we'll put in my Docker the
container that we originally
created and that image has now
stop and let's go ahead and prove that
we're going to list out all the docket
images and what you see is that it's not
listed in the active images it's uh not
on the list on the far right hand side
but if we go ahead and we can list out
all of the dock images so you actually
see it's still there as an image it's
just not in an active stage it's was
known as in an exit state so here we
go and there's my Docker it's in an
exited state so that happened 27 seconds
ago so if we want to to remove a
container we can use the following
command
so pseudo
Docker
RM we
remove my Docker and that will remove it
from the exited
State and we're going to go ahead and
we're going to double check
that and
yep
yep it's not not listed there under
exited State
anymore it's
gone there we go there that's where it
used to be all right let's go
back so if we want to exit a container
in the running state so we do pseudo
kill and then the name of the
container I think one of them is called
yellow let's just check and see if
that's going to kill
it oops no I guess we don't have one
called yellow so let's find out name of
container that we actually have so PSE
sudo Docker kill oh we're going to list
out the ones that running oh okay there
we go now yellow isn't in that list so
let's take I know let's take simply
learn and so we can actually go ahead
and let's write pseudo Docker kill
simply learn and that will actually kill
an active Docker
container boom there we
go and we list out all the active
containers you can actually see now
that's the simply learn container is not
active
anymore and these are all the basic
commands for dock container if you are
looking to get certified into devops and
become a kubernetes and Docker expert
have a look at wide range of courses on
devops by simply learn in collaboration
with topnotch universities across the
globe so why wait enroll the course now
from the link in the description box so
let's dig into what a Docker swarm is so
a Docker swarm is essentially a toour
that allows you to very easily create
and schedule multiple Docker nodes so
really two or more Docker nodes and you
can have quite a large number of Docker
nodes in a single swarm uh each node
itself is actually Docker demon and that
demon is able to interact with the
docker API and have all the benefits of
being a full Docker environment one of
the other advantages you have is that
the each dock container within the swamp
can then be deployed and managed but as
a node in that entire clustered
environment so what we have here is is a
breakdown of the five key elements
within a Docker environment you have the
docker container you have the demon the
docker images and the docker client and
Docker registry so the docker demon
itself is what does all the work
interacting with the actual host
operating system to be able to manage
the docker containers so if we look here
here we have set up an environment where
we have three Docker container being run
with Docker and and what we want to be
able to do is be able to interact with
the environment because what would
happen if we actually have something
change in our environment so we have the
environment set up as a Docker swarm and
one of our containers fails what we're
able to do is use the Swarm to be able
to correct that failure so the docker
swarm manager is able to come in and
reschedule containers and as you would
imagine the actual swarm note has full
backups and full redundancy for any kind
of failures that would happen and we
would do all of this work through
command line interface so let's go
through some the features that you have
within the actual Docker swarm itself so
a key feature for Docker swarm is that
it is fully decentralized which means
that it makes it very easy for teams to
be able to access and manage the
environment as you would expect as well
is that the communication that happens
between the manager and client noes
within the swarm is highly secure and of
course this is really just a fundamental
that you should have for any kind of
solution but it is good to know that do
swarm has that built in there is also
autoload balancing within your
environment you can actually script that
into how you write out and structure
your swarm environment and that load
balancing then also allows you to then
convert that swarm environment into a
highly scalable infrastructure and then
roll back task allows to be able to roll
back environments to previous safe
environments so if say something does
get pushed out and something breaks
you're able to immediately roll back
into a safe environment so each of the
containers are pushed out and and are
controlled using services and they
actually happen to be rest Services
which make it very easy for you to be
able to integrate within your
environment and each of the different
Services contains a group of containers
of the same image Now by having having
this structure allows you to be able to
scale your application appropriate to
the demands on your service so if you
have a service that needs to have a
significantly larger number of services
for it to run you can actually scale
that appropriately that can be either
Geo or demand based one of the
requirements for setting up a Docker
swarm is you must have at least one node
deployed so the way that the
architecture is set up is that you have
a manager node and a client node and
there must be one of each for the entire
environment to be able to work
effectively so you know here we are just
jumping ahead of sales a little bit we
have two types of nodes in a Docker
swarm we have the manager node and we
have the worker node which is the client
that does the actual execution of the
tasks the manager node very similar to
other systems that we've talked about on
simply learn allows you to actually
control and manage the actual tasks that
are being executed by the worker nodes
and the worker nodes as you can imagine
and then actually execute the
instructions that the manager are
sending out to it so here we have a
situation where we can illustrate what
would happen with a manager node sending
out commands to different worker nodes
the manager is fully aware of the status
of the entire swarm environment at all
times this is because of the two-way
secure communication that's going from
the manager to the worker environment
the workers as you'd expect are
accepting tasks that are being sent from
the manager so the manager send out a
task saying that you need to be running
as a MySQL environment then the worker
node will then convert to a MySQL
environment and all of these
environments are scripted and controlled
by you as the manager the actual worker
nodes themselves actually have a client
agent and that client will then
communicate all different states of the
infrastructure back to the manager so
that anytime the manager node is in full
control of the entire swarm the manager
is the controller of this environment
and so you always want to be able to
ensure that the manager has full access
to all of the work that's happening
within the Swarm and this allows you as
the manager to be able to control your
swarm and very quickly be able to react
to any changes without having to rely on
manual installations of software and
hardware and as we covered earlier on
there is a rest API that uses the
communication over HTTP from the manager
to the worker node it's interesting to
note that it is a rest API because if
you wanted to you could actually
integrate that API into custom
applications and even then create
automated docket images to be created on
demand from third party solutions that
you may want to create so one of the
things that's a really a big Advantage
with having a swarm is that once you've
actually created the form any of the
services um that you create can be
accessed by any node of the same cluster
one of the things you do have to do
though is you have to specify what
container image that you're going to use
when you're creating a new service and
you can do that either through a
centralized Docker Hub environment or
through your own private Docker Hub
environment one of the things that's
interesting is that you can set up uh
commands and services to be either
Global or replicated a global Service
will allow you to run a service
consistently on every node within the
Swarm whereas a rep at service will only
push out functionality and tasks to
specific worker nodes within a swarm so
you may be asking yourself hey isn't a
service and task the same thing no
they're they're kind of not in the
docker world and the difference is is a
service is a description of a task or
the state whereas the actual task is the
work that needs to get done and that's
the differentiator here so what you can
do as a Docker user is you actually
create services and then you can then
Define when you want them to start as
tasks now what's interesting is that
when you do assign a task to a node that
same task cannot be assigned to another
node also what's interesting is that you
can actually have multiple managers
within a Docker swarm environment if you
do however go down this path you have to
elect one manager to be the primary
manager and the other managers to be
secondary managers in many ways those
secondary managers are really similar in
concept to worker nodes in which they
have the capability of a manager but
they are dependent on that single
primary manager to be able to provide
the right instructions and for services
and tasks to the entire swarm
environment so if we kind of recap some
of these we have a command line
interface which allows us to create and
connect via apis and that those apis
that we connect to in our swarm
environment allows us to do
orchestration via tasks and services the
the task allocation allows us to
allocate work to tasks via their IP
address which allows them to execute
them on the work and then the worker
nodes themselves have to connect to the
manager node to be able to check for
when tasks come in so that they are
keeping a consistent communication back
and forth across the entire swarm and
then the final stage is to actually
execute the tasks that have been
assigned from the manager note to the
work note so that you have a successful
execution of the solution you're looking
to build all right so with that said
let's go ahead and we're going to do a
demo so we're going to go ahead and
going to do a demo and showing how you
can run Docker swarm for this you'll
need to have both a virtual environment
running your manager and your working
environment so here we have our worker
and uh here we have our manager so we're
just going to open up terminal window
and we're going to go into the manager
for terminal window so the following
command here is used to initialize
Docker swamp I'm just going to type this
in so pseudo Docker swamp in it and we
put the the IP address for the network
we're connecting
to and we'll put in our
password and here we have now
connected and this is our manager node
and what we want to be able to highlight
is the specific token which identifies
the docker swarm environment and this is
our token right here so we're going to
copy
that because we'll need to use that for
our worker environment so here we are in
the worker environment and we'll use the
token that we just copied um as a way to
be able to connect to the manager and
connect to the dockor Swarm
environment and so PSE
sudo we'll paste in the docker swarm and
we'll join that and now here we shows
that we have joined the Swarm
environment as a
worker and now we're in the manager we
can actually show that we actually have
have that worker in the environment so
there's Docker node LS to list out all
of the items in that
environment sorry we actually did in the
wrong area so R WR pseudo doer node LS
to list out all the nodes in the Swarm
and there you are you see we've actually
connected the worker to the manager node
and both are active
and the manager is the leader and the
work of virtual box is in the
Swarm okay we're going to go ahead and
create a new service so Docker swarm
create and we're going to just change
the name to hello world for the new
service and we're going to use the
Alpine image from docket.com takes just
a moment for it to
run there we
go and there we go Services
converged
so what we have now is a new service
that has been created let's go ahead and
we'll list out the services and here we
have our new Hello World
Service and we go ahead and check the
dock and containers and we're going to
do Docker PS and here's our Alpine this
running the latest image and that's the
docker container ID
and we can see that it was just
updated and so what we're going to do is
use the same command in the worker node
and make sure everything is working over
there so go to our worker paste that
in so what we see here is there is no
image or container created because it's
not in the worker node it's it would be
in the manager node so we can go ahead
and create the new
service and we're going make the Mode
Global and that way the service is
available across all of the
Swarm so we just going to execute that
work takes a
moment
and there we are done work has been
completed and we have the two different
IDs which shows the two different
Services running and if we go back to
the worker mode and we run pseudo do
doer PS again and there we are now we
see that we have the hello world new
virtual environment has been added with
the image of Alpine and we also have the
new token ID for
that if we want to kill a node from the
swamp we can use the following command
which sud do swamp leave D- force and
that will force the node to leave the SW
so one of the questions that will come
up though is what if you want to use two
containers in a single service and and
an example of this might be like a web
server that is using a database but the
database may be in one container and the
web server may be in another container
which is very typical for your own web
and database infrastructure so very
similar um scenario but how would you do
this with Docker so by using just Docker
itself it's actually very difficult to
do this you can do but it's just very
time consuming and just not a very good
use of your productivity but with dock
compose you can actually do the whole
process quite quickly and easily and
hence the reason why we have Docker
compose to allow you to have two or more
Docker containers running in the same
environment being able to communicate as
if they were running in a production
environment so let's dig into what
Docker compose is so let's consider a
scenario that you'd have today so mintra
is is a fashion website similar to
Amazon and you go to mintra with your
web browser and on that website you
would go through a number of activities
such as logging into your account
browsing a catalog uh you'd have your
checkout application server and you go
through the process of um building out
an application behind each of these
services are different products such as
you know you have an account database
You' have a product database different
checkout processes and these would all
be run behind the scenes and each of
these can be considered a micr service
so the more microservices you build into
your environment the more complexity
you're adding and the more of value it
would be to have each of these services
in their own container but as a
developer you want to be able to jump
from one container to another container
so your login account can then be passed
onto your product catalog and then be
able to move through the whole process
so the environment that you would see
today would be small like what we're
showing right now where you'd have your
server and your database and you may
have a server running on Apache tom cat
or a SQL Server you'd have different
databases and you'd want to be able to
have each of those in their own
containerized environment so the way
that Docker compose runs is that it
works as a single service so you have
the docker compos running multiple
containers but the perception is of a
single service run the thing that's
great though is that each of those
containers will run in their in
isolation and but they can interact with
each other so unlike a normal Docker
environment where you have multiple
Docker images running completely in
isolation of each other these are images
that will run in isolation but can
interact with each other the docker
composed files are very easy to write
they're all written with a scripting
language called yaml and yaml is an xmlb
based language it actually stands for
yet another markup language but it's an
XML based language and it's very easy to
use the great thing about learning yaml
is that a lot of open- source tools in
the devops environment use yaml as the
scripting L and then finally in docket
compose as a user you can actually go in
and Trigger all your services within the
containers to start with a single
command as you can imagine this
dramatically reduces the amount of work
that you as a developer have to do so
let's take an example of if you're
running a container one with a Nix um
server and one with a redis database uh
you can create a yaml docket compose
file that actually has the instructions
on the containers needed to build out
both environments and then you can run
each of those environments separately
but have them connected as a single
service so the benefits of duck and
compos is that you have a single host um
deployment environment you can run this
all on one piece of hardware you can
have a quick and easy configuration
using yaml scripts the productivity that
you get as a developer significantly
increases because you're not having to
have time wasted trying to configure
just traditional Docker containers by
themselves you now have a way of being
able to interact with those containers
and then finally security is at the
center of all of these containers so
each of those containers are completely
isolated from each other and they are
controlled with the same level security
that you would have with a traditional
Docker image but there is a question
here around Docker compose isn't it
similar in concept to Docker swarm
because at a high level it kind of seems
like both do the same thing and the
answer is well actually no they're
different there are similarities but
there are also differences and
particularly for a developer these
differences will really start coming out
in scale as you start working on your
Solutions so let's look at Dr compose so
Dr compose will allow you to create
multiple containers on a single host and
that's the important part having a
single host which is maybe your
development PC with Docker swarm it also
allows you to create multiple containers
however you have to manage those
multiple containers on multiple hosts
which makes a lot of sense if you're
running an operations environment but
not so much if you're doing a
development environment and dock compose
is scripted with yaml which is very easy
for you to be able to control your
scripts Dr swarm doesn't have a
scripting technology like yl so it's a
little bit harder to work with so what
we have here is just some basic commands
that allow you to um get up and started
working with dock compose in our next
demo we're going to take you through how
to use all of those commands so we're
going to go ahead and do a demo on how
to use Docker compose um the first thing
you want to do is check that you have
Docker installed so what we're going to
do is we're going to open up the the
command window and we're going to typee
in Docker you'll be able to see now that
we have Docker installed U if you don't
have Docker installed go ahead and
install Docker you can go to Google
Search to install Docker and they'll
give you the instructions for that we
also have videos that we've already done
if once you have dock installed then you
want to do a Google search on Docker
compose or go to the dock. website and
select for the compose section and look
for the install Docker compose
instructions so we already have another
tab which has uh the instructions
already installed you'll see on the left
hand side there are a number of Docker
tools you want to expand the docker
compose section select install compose
and then on the right hand side you'll
have different install options for Mac
windows and Linux uh if you're
installing on those platforms make sure
you're copying uh the correct command
lines there's a couple command lines you
want to copy copy the first one over and
we paste that in and it's going to go
ahead and download all of the files for
compose it takes a while but uh we have
to get another file so while it's
downloading we'll go get the other
command line just go back and grab it
real quick was a little fast there but
we'll copy that over there was a second
command instruction and post that
command yep there's the command right
there and that will apply the
appropriate binary and once you've done
that you can actually go ahead and
validate that you have the correct
version of Docker compos installed and
for that you want to write the command
docker-compose space-- version and that
will then give you a version build once
you know that you have a version build
that means that you have everything
installed correctly on your machine so
you can see here that we have a version
number which means we have everything
installed you might see a different
version number depending on how old the
video is when you're watching it but
we're using very content at the moment
and just go ahead and clear the screen
and now what we're going to do is is
create a folder where we're actually
going to install the yo file that will
have the instructions for our docet
compos environment so we're moving the
cursor to the desktop and on the desktop
we're going to create a new folder and
uh using the MK um command and we'll
call this one dock compose f for file
and then we'll move the cursor into that
new folder by using CD command and now
we are in the folder called do compose f
um and you can go look at your desktop
and you'll see it's there and now you
want to select um pound touch
docker-compose.yml and that will
actually create the new yaml file for
you and you can use the ls list command
LS do- composed. yo and that allow you
actually now to step in and edit that
file so let's go ahead and uh hit return
and that allow us to go in and edit the
file so now I'm in the editor and and
I'm going to write some instructions um
so I'm I'm going to put a couple of
mistakes in these instructions so we can
see how do pose catches those mistakes
um so but most of this is going to be
fairly correct so you can just type this
out so we're going to start with
Services colon and we'll do web colon
and then we create image colon and we'll
do Nix and then we'll do a database
colon and we'll call the image reddis
and so what you can see because we have
two reference images we actually calling
two different images two different
containers that will be created by
Docker Hub uh so what we're going to do
is uh where we can actually show you um
how that's pulling in you can go to
dockerhub and you can actually see the
references to the images that we've just
created so go to Docker Hub which is H
hub. do.com and you'll see that uh Nix
uh reddis those are the default names
for the images so you could do uh other
images like like HTTP for Apache web
server if you wanted to there are a lot
of images in Docker Hub so suddenly have
a lot of fun building uh tools from that
environment so we're going to save those
files um in the doc image and we're just
going to do colum WQ to um exit this
screen save the text and let's go and
run our uh yaml file and you'll see the
two different error messages that will
come up so we're going to do cat Docker
Das compose
yml and here we have a display of the
text that we just wrote and now let's go
check the validity of the file that we
just created so we'll do do- compose
config and we should get an error
message and there we are error message
and the reason why we have an error
message is because the spacing is not
correct in the yo file itself so let's
go to the actual yaml file that we
created let's go through and we're going
to use uh just the file finder and you
open up the yaml file in your text
editor your favorite text editor and
here we have um the code that created it
and let use the correct um spacing so
that everything is spaced out
appropriately in the image and so this
is the spacing that you'd expect to see
a space a line between surfaces and the
first image and the first image is a tab
in and then it reference what the image
is and let's go ahead and clear the
screen and we'll see whether or not
we've got that all correct clear um
let's clear the screen and we're going
to check that everything is running
correctly and we're seeing we're still
getting an error message and the reason
why we're getting another error is that
there's still a config error uh in this
the actual yamamo file so one of the
things you have to do when you're
creating a yaml file for dock compos is
you actually have to have the
appropriate version of number in there
so what we have to do is we have to go
and find which is the appropriate
version and there are three major
versions 1 two and three depending on
the age of Docker that you have
installed on your computer uh so if you
have the very latest version of Docker
18 and newer uh you would put 3.7 as the
version number and what we're going to
do is we're going to just check on the
version that we have installed and so
just clear the screen and then we'll
just do Docker version and what we have
is our version of Docker uh
17.12 now this is the version of Docker
not Docker compos uh it's your main
Docker environment and so if we go back
to our web browser we'll actually see
that version 17 should align with
version number 3.5 yep there we are
version 3.5 so what we want to do is go
back into our yo file the very first
line that we want to put in above the
services is the version number and that
version number we're going to put in is
3.5 so we type in version version and we
put colon and then version number number
is in in quotes and you save that and
now what we can actually do is we can
actually go ahead and use the single
line instruction to run both of these uh
uh two images that we've created because
everything is configured correctly but
just one note you'll see that the
version number is actually at the bottom
of the list and that's just the way the
dock compos works it pulls out the
services and the images that you'd
create and then post the image the AV
version number at the bottom of the list
so in the presentation we talked about
how you can use one line to trigger two
or more images and so to do that we're
going to do pseudo Docker Das compose up
dasd and it's going to create the two
environments we have the web environment
and the database environment and let's
use PSE sudo Docker PS to actually list
out that all the processes that are
running right now and there we are we
actually see all of the images are
running we have the names the commands
the images and everything's up and
running and if we want to close both of
those images we can actually use a
single line of command as well and that
command will be pseudo Docker Das
compose down and that closes both of the
images and we can see that all the
databases and the web services images
have been closed and pseudo do PS will
actually list that there is nothing
wrong and that's how to use dock so uh
we are going to talk about what is a
Docker what is a Docker file what is a
syntax of Docker file and how to build a
custom Docker image with the help of
Docker file and in the end we are going
to see a particular demo related to the
docker files over here now let's talk
about what is a Docker now Docker is in
configuration management tool which we
are pretty much using to prepare the
automation of the deployment of the uh
software of the applications now in this
case we make use of the docker
containers using which we can host our
applications and within this Docker
containers we are typically hosting both
the application source code and the
defenses all together so both the dinks
are getting packaged all together into a
single unit called docka containers and
getting hosted onto the
server these containers are very
efficient in work because they are not
taking that much utilization and
resources and at the same time they can
be shared across different environments
also so the same container can be easily
deployed across different different uh
environments as
such now it helps us to maintain the
isolated application it uh helps in
having a high productivity there and it
also you know helps us to see to resolve
all the problems which is related to the
uh dependencies for your software
because we are packaging up both the
source code and the dependencies all
together in the single unit in the
single entity and that's where you will
be able to get a particular final
solution called as in Docker container
over here so Docker container is in
complete package using which you can
have like both the applications and the
dependencies bundled up together into a
single entity and that's where we call
it as a Docker
container now what exactly is an Docker
that uh you know let's see a particular
difference between the virtual machine
and the docker here now if you see on
the left hand side we have the host
operating system uh which is available
there let's say I have a laptop in this
one we have a Windows 10 now we have a
hypervisor on top of that like
hypervisor is in software which is used
to manage the virtual machines it's a
software which is available there which
takes up all the requests from the
virtual machines process it and then uh
give the response back so hypervisor
acts like a medium between the virtual
machine and the host operating system
resources so uh it's going to give that
a particular virtual environment to the
virtual machines and then we have a
virtual machines where in fact a guest
operating system is there then
application boundaries then the
application is hosted so this this guest
operating system actually takes up a lot
of utilization and these was the old
mechanism where we used to you uh follow
this mechanism for hosting our
applications so all the applications are
pretty much getting deployed in the same
manner on the form of the virtual
machine here right now if you talk about
the new method which we are talking
about right now is in Docker so here we
have host operating system but instead
of hypervisor we have now the docker
engine which is available there now this
Docker engine is something which is
making sure working in the same way that
how the hypervisor is behaving but it's
much lighter as compared to the
hypervisor so Docker is very lightweight
as compared to the hypervisor here and
here we don't have the concept of uh the
heavy duty guest operating system over
here so the guest operating systems
which we are using in case of Docker
containers are very small in size the
size of guest operating systems uh is is
in GBS in case of virtual machine but in
case of Docker containers it's pretty
much available aable in the MB size
which is there so the size of the guest
operating system in case of the virtual
machine is very small as compared to the
virtual machine and that's the biggest
uh benefit because again it will help
the containers to have a low utilization
low consumptions and uh that improves
the overall performance of the uh
overall mechanisms and how exactly the
um New method of application deployment
really helps us to go for an a
particular Advanced mechanism so that
brings on a very good and the efficient
uh way of doing the deployment of the
applications now let's talk about what
exactly is in Docker file here now uh
Docker file is a concept which is being
used as an build script so it's being
used so that we can have a build uh
script there so this will definitely
helps uh to see that how we can go for a
build of a Docker image a custom Docker
image now when we we are using the
docker file uh concept so it really
helps to uh prepare a kind of Docker
image so dock image is the end result
when you are going for the mechanism
when you're going for the docker file
buildup H so Docker file is the build
script which is available for preparing
the custom Docker containers because not
every Docker image is available on
Docker Hub so some containers you have
to use some uh images you have to use to
work on that part now Docker file is a
very simple straightforward text file
file which is available there and it's
basically used to build up custom Docker
images so Docker file is basically used
to prepare a once so when you running
the docker build so it's actually going
to build up a particular custom Docker
image and that will be given to you so
Docker uh image is the whole build
process which is being done so it's a
custom process which is available and
being done over here in this
case so you got a Docker file you run
the docker build command and that's
that's how you will be able to get a
Docker image so whatever the steps you
are having in the docker file that will
be processed one by one and as a end
result you will be able to get a
full-fledged Docker image over here in
this case right now let's talk about the
syntax of the docker file so Docker file
is equal to commands commands and the
arguments so whatever the things you
want to put up so in the docker file you
can do that and as a part of the
execution so you will be able to get an
particular end result now it's a kind of
a mechanism where we feel that how the
execution and the modifications can be
done so uh the uh comments which we are
trying to do over here is that you know
we can put up the whatever the comments
we want to put in the case of Docker
files we can do that comments is a
standard process in uh preparing any
particular file or any kind of code
because it helps us to understand that
for what reason we are preparing that
particular line then uh we have the uh
particular run statements the commands
are there so you can see that we are
running a particular command called Eco
so you using the Run attribute we are
basically going ahead and running a
particular command into our system as
part of our Docker file over here now
how to build up the docker images using
the docker file so um you know Docker
file is uh just a template which is
available there where we are putting up
that what are the different steps we
want to follow and these steps when we
follow one by one step by step so that's
where we will be able to get a
particular final Docker container over
here so Docker image is getting
converted as a Docker container and we
are creting a Docker container as an end
result now um Docker file is being built
up and uh stored in the version control
system and uh this particular Docker
files uh when we are processing So It
prepares a respective uh a specific uh
image layer over here and this image
layer is something which is uh
benefiting us that how the executions
can be done and how we can manage on
that part so that really helps us to see
that how the executions and the things
can be managed and that that can help us
to see that how the management of the
things can be performed as such so uh
Docker file is a very good concept which
is there using which you can prepare any
kind of custom Docker image and it can
be very easily handled also and it's an
fully automated mechanism you can use it
in inside the cicd pipelines for
automating the docker images right now
it's uh Docker image is a layer by layer
mechanism which is there in which we
have multiple layers each and every
layer we are doing some specific uh
entry some specific task or some you
know changes we are trying to deploy in
part of our these layers right so uh you
can use the a particular Docker build
command to perform the executions and
you will be able to get an particular
response and output will be given back
to you so Docker file is basically
helping you to see that how the
executions can be done and you will be
able to get an end result over here so
the docker image is a final result which
we are getting when we are talking about
the docker file here right so these are
all read only formats which is available
you cannot modify these directly but yes
with the help of Docker file you can do
whatever the changes you want to deploy
on these Docker
images right so uh this is a small
example where we are using a base image
as an2 18.04 so we are using the from
attribute to specify that yes we are
going to use the 21 18.04 here then we
are uh pulling a the file into the uh
directory we are transferring a
particular file into ins the docker
container and then we are running a make
Command and then CMD attribute is there
using which we can run the command on
the runtime on the docker container so
these are a very basic example but
eventually when we are done with this
Docker image we will be able to get a
four layer uh Docker image over here
because there are four steps and then
hence it will be providing you a
particular four layers in your final
Docker image over here so layer by layer
these changes will be done during the
build process and in fact using the
docker history command you can see the
layers these layers information
also right so layer one is bu2 18.04
Layer Two is pull and uh the third layer
is there where we can run the uh make
command to you know run this file which
we have pulled on and then the finally
the last one is there where we are
running like using the CMD P so the
difference between run and CMD is that
CMD will run only when you run a Docker
container so run attribute will be
executed during the docker build part
so this is uh pretty much is an way that
how we can go for the execution of a
Docker file and how you know we can go
for the uh buildup of a custom Docker
image over here so these are the some
couple of attributes which we are using
in the docker file here right now let's
talk about the entry point over here now
entry point allows you to specify a
command along with the parameters right
so uh let's say that you want to run a
particular command uh over here in the
dock container so entry point is
something which you can use to refer to
that so application is the command which
you're trying to run and then you can
give some arguments to that and argument
one is what you're trying to run over
here so entry point is very important
because that eventually helps to see
that how the execution can be done and
the implementations uh will be done over
here in this
case right so you can see like you can
run a command called Eco and then you
can use the parameters also it's not
like only the commands can be used but
you can in fact use the arguments also
now add is an attribute which is
available in the docker file which is
primly used to see to uh transfer the
files from the host machine inside the
docker image now let's see that you are
trying to prepare your own custom Docker
image apart from the dependencies you
need the source code also right so the
source code needs to be transferred
inside the docker image and for that
reason we can use the add attribute so
add attribute requires a source and
destination Source will always be the
path on which uh on the Destin on the
host machine the files are present so we
can give that and destination is the
folder which is available inside the uh
particular Docker container where we
want to store the files so you can use
the example over here just like this you
can use and you can give two entries one
is the source one is the destination and
once that is done so you will be able to
see your files being uh stored up in the
destination directory once the final
Docker image is prepared then we have en
attribute uh now ENB is something which
is used to set up some environment
variables which is required of course
for the docker image uh for the
application which is running inside the
docker image now environment variables
are something which you can hardcore
during the docker build one but using
the when you are running like when
you're creating the docker container at
that time also you can override the
these environment variables so
environment variables are very important
because uh first during the build
process you can hardcode some values but
if you really want to change them during
the runtime that also is being supported
by the environment variables so most of
the times we use it as an kind of a key
value pair just like in in Linux we use
uh we use to set up a particular
environment variable so the key value
pair is being used in case of ENB
attribute maintainer is there where we
can describe that what is the details
about the author these are the fields
which is available in the top so where
we can give the the author name and the
author you know details like email ID
and all so that we know that who is the
person who is maintaining this
particular file and we can reach out to
them in case any issues are there for
the docker file CM right so uh that is
what we have here in this one so now we
are going to see that how we can go for
a small demo on Docker file and we can
go for the implementation of this uh
particular Docker file and prepare a
custom Docker image so let's just wait
on that part so let's see some uh demo
here now in this one we have got two
examples one is the python and one is
the notas one so first of all what we
need to do is that we need to process
the python one here so this is the
sample Docker file which we have got
here for the python now all this one
requires a python file which is already
present on the server and we are going
to transfer we are going to copy this
content over here that this is the
specific Docker file content which is
available to us and all we need to do is
that we need to put this content across
the server so that we will be able to
build our custom Docker image so let's
go back to the server so here we have
have already got the python file which
is present so we have to create a Docker
file here which will have the steps
related to the docker image preparation
over here so we will be including some
uh steps over here that these are the
different steps we want to configure we
want to set up over here in this one so
python 3.6.4 is the version of the
Python which we want to configure and
then we are going to transfer some files
into slash app we are also setting up
the working directory as SL app and
ultimately we are running a command
called pip install flask to do the
installation of flask package
and then we are starting the uh specific
applications on that part so I'm going
to save this file and uh once I save
this file so all we need to do is that
we need to create a custom Docker image
right so right now there is no image
which is available over here and it's in
complete MD instance so I'm going to run
a command called Docker build hyph T
python hyphen m dot so Docker build is
the typical command but hyphen T is for
the tag that what kind of Docker image
you want to build and then we are giving
a image name and then we are specifying
dot over there now once we are done with
that so it will pick up all the things
one by one the basic Docker image is
being pulled down then some sequence of
steps is being executed so it's a
step-by-step process which is happening
over here in this case and we are then
transferring some files in the SL app
and then ultimately we are going for the
PIP install flask so it's installing
those packages and at the end the final
Docker image will be prepared and will
be given back to us so we will be able
to to get this uh Docker images over
here when we run that command we will be
able to find out that we got our custom
Docker image also available and as you
can see that the base doer image is also
available in this one but yes we have
added up like around 10 uh MB of work we
have added on top of this python uh base
Docker image here so that's how we are
able to do the processing over here in
this case in this setup so that's what
we uh can do to prepare a custom
environment of python with the help of
the docker file here now let's see that
uh we will be uh proceeding on with the
nodejs to see that if the nodejs uh kind
of setup also can be done now for nodejs
again of course we got uh the uh
specific Docker file which is present so
we can process that but let's create a
new folder over there so I'm going to
create for a new nodejs folder so that
we can create the work all together in
this one so I'm going to run a command
called npm Unit which is the command to
initialize any empty project in case of
node now you can typically bring down
some uh real code also or you can say
that I want to process an empty uh
project so you can give the
configurations you can give the custom
names but uh I want to process with
these default name itself so ultimately
it's going to uh create an entries like
uh you will be able to see like all
these uh values which we are not
providing these standards value will be
picked up for the uh specific code and
at the end you will be able to get a
package.json file created over here in
this one now you can actually bring down
the steps that how you can perform or
you can build up a custom nodejs based
Docker image now these Docker files is
giving you the capability that you can
convert a full-fledged application which
is Deployable on the virtual machine you
can actually convert them in the format
of the docker containers over here in
this one so that's what we are going to
get over here with this uh particular
setup or these mechanisms so let's go
back to the document and see that what
exactly we can copy the docker file so
we have got this Docker file over here
so all we can do is that we can copy
this one so once we copy this uh
specific file here so all we can do is
that we can transfer uh these files into
the onto the server we can convert it
into the a doer file there so that we
will be able to set up the
configurations and we can perform the
setup around that so Docker file is
again we want to create and we want to
set up on that here so let's go back to
the server so here we want to create a
Docker
file and uh we want to set up the files
inside that these are the different
files which is being available which is
being configured over here in this one
so we are running like node server.js
which is the master file so we are doing
an expose of 8 then of course we have uh
copy commands which is there then npm
install command is there and then then
package.json file is also there now what
you need to really do is that you need
to go for a server.js file which is
available in which you can configure
that how the setup needs to be done and
how the configurations should be
performed there because that's a command
that's a script which is going to be
executed within the content so let's
copy the content now I'm going to copy
this content over here which is like a
sample uh nodejs code which is available
so I'm just copy this one and transfer
it to the server so
server.js file we have to create here
just like we did in case of the Python
because this is the source we want to
transfer inside the docka container so
this is what we have got now I'm going
to uh transfer like uh I want to change
this port value from 5,000 to 8080
because that's what it's configured in
the docker file of course you can
configure 5,000 also depending on your
requirement you can do that
configurations now you have got uh these
two files over here in this one so all
you need to do is that you need to run
like Docker build hyphen nodejs hyphen
app and Dot so it will automatically
pick up like what exactly it will uh you
know go for the nodejs basic Docker
image over here so that will be
processed over here in this one so step
by step it will be able to do the
configurations so first of all it will
download the basic Docker image which is
um quite uh more in size over here in
this one once it's downloaded onto your
system then it will proceed further with
the build activity on
uh processing the docker file performing
the execution so all that stuff will be
performed with this help of this Docker
uh build process over here so it's going
to process it one by one in this
one and it's going to the next step
where it's transferring some of the
files like creating some working
directory transferring the files uh
putting up there and running the npm
install command and uh then ultimately
it will expose the port and then it will
have the uh CMD attribute configured
over there so this is basically
typically trying to portray that how a
particular Docker image can be built up
and of course once the uh build is
Success you can validate it with the
help of Docker images here so this is
the app Docker image which is been
prepared of 911 MP and uh we have got it
like a final package where the runtime
is also there and the code base is also
available now you can pretty much
experience it like with other actual
Source course like you can generate the
download some preconfigured nodejs
websites or the uh Source codes here and
you can try to run them on 3D Docker
container so that you can get the full
uh the complete idea about that how it
goes on but right now the uh these are
the basic Docker image which you can
prepare and you can build up over here
in this one so that's how it really
works on for you now let's see that what
are the different commands we have for
the docker here now the very first one
is that what are the different images
related commands which is available
there the very first one is the command
which we typically used to prepare a
custom Docker image now I can uh easily
go to the docker Hub and I can pull down
a Docker image which is available there
but let's assume that you are working on
application source code so uh you want
to prepare a Docker container for that
now that Docker container you will not
be able to have it on the docker Hub you
have to prepare on your
own right so in order to prepare your
own Docker image all you have to do is
that you have to run Docker build
command now in this case we have to
prepare a Docker file which will act
like a Docker a kind of a Docker build
script just like in case of may we have
P XML file and in case of and we have
build. XML file so here we have the
docker file which is available there and
this Docker file make sure that whatever
the steps are mentioned in this Docker
file in the build script it's going to
perform a series of steps and the build
will be done so Docker build is the
command hyphen T is the tag that what
exactly name uh you know whatever the
image you are building so what exactly
tag name you're trying to provide over
here so here we are using a tag name
called my image and then after the
column we are providing that what
version we want to go here 1.0 2.0 5.0
so whatever the version you feel that
you want to um you know refer you can do
that now this is the exact way that how
in the actual world the docker images
are getting built up by the uh
respective vendas let's say that Jenkins
is want drins wants to prepare a Docker
image so they will be preparing the
docker file in the same manner and
prepare these Docker images and publish
it to the docker Hub so all the docker
images are built up the custom Docker
images are built up with the help of
Docker file so Docker build is the
command which will process the docker
file and give you a particular Docker
image in the
end now how to list down all the docker
images so Docker list uh LS is something
which is there so this will eventually
help you to list down all the docker
images which which is listed on or which
is available into your system so that
will be uh listed out over there for you
if you feel that you want to remove a
particular image in that case you can go
for Docker image RM command and uh
whatever the image you want to remove
you can give the respective image name
and the tag name there so Docker image
armm command is there using which you
can actually see that how the image can
be deleted from the host over here in
this one right so whatever the ways
whatever the you know mechanism is there
so we are making sure that how the image
can be built up and how we can go for
the uh builtup and a particular image
can be deployed over here in this case
so in order to delete an Docker image so
you can go for Docker image RM command
to remove a Docker image completely from
the docker host so these are some couple
of commands which is available there
using which you can actually interact or
manage the docker image because in order
to prepare or in order to run a Docker
container we require a source and that
Source we are going to get it from the
docker image so now we are going to see
that how we can execute these commands
so first of all what we need to do is
that we need to see that how Docker
build works for that we require a Docker
file so I'm going to have a simple
Docker file over here in which we are
going to have a basic dock selected as
an open two after that we are going to
go for the one attribute where we are
going to do the installation of a
specific package like let's say G and
accordingly you can whatever software
you want to prepare you want to set up
you can do that and put up the commands
over here in this Docker file so I'm
going to save this file and after that
what I need to do is that I need to run
like Docker images to find out that how
many images are there there is no images
present so I'm going to build a Docker
image over here with the name of Simply
lar or whatever the uh name you want to
give you can uh give it over here in
this case it's not any kind of
restricted word so you can give the
actual Docker image name also that what
for what purpose you are preparing this
image and with that what will happen
that it will take up the values whatever
attributes you have put up into the
docker file so that will be processed
and that will be picked up and once that
uh Docker file has been processed has
been uh you know prepared you will be
able to see that yes uh a final product
or a final Docker image is finalized and
prepared over here in this one so we
just have to wait for the whole process
to complete and once my Docker image is
prepared I will be able to have a
particular setup done or achieved over
here so that's what we require over here
in this one to be
performed now once the installation is
done so you will be able to see like the
final Docker image is being prepared
over here so it says successfully built
which means that the docker image is
prepared now if you run like Docker
images you will be able to find out that
this image is available over here in
this one now you are seeing like two
images because one is like the basic
image which is being pulled on because
we are using a Docker file to do that to
process that one
there now in case you want to remove it
so all you need to do is that you need
to run like
Docker image RM and simply learn so if
you do that what will happen that your
Docker image which is related to Simply
learn will be wiped off from your system
and in case you want to do like removal
of the open to also so that also you can
do over here with the help of Docker
image
RM open to and that will be removed from
your system so your or Docker images
will be removed from your system using
the uh particular command called Docker
um image and then RM command to remove
the docker image from your system from
your host here so that's how you deal
with the different Docker images uh
commands over here in this one now let's
go back to the document now the next
section over here is the docker
container commands over here so all the
commands related to Docker containers is
what we are going to talk about over
here now the very first one is the uh
particular container so in order to
create a container so you can run the
command called Docker create and the
image because for a container you
require a Docker image that's a prequest
so once the docker image is prepared you
can just simply run Docker create
command and that will help you to create
a container onto your system now if you
want to run a particular um you want to
create a container and you want to uh
run it also at the same time so in that
situation you can go for Docker run
command the main difference between the
docker create and Docker run command is
that the docker run command is going to
uh run a container and uh it's going to
give you a response from the command but
the docker create command is going to
create only the container so once the
container is up and running so that's
only being done by the docker create
command but the docker run command is
going to actually do the uh preparation
of the container also and at the same
time you are being given as an
particular response also like you know
what a command you want to to run so
that response also you will be able to
get here and if you feel that you want
to rename an existing container so for
that you can go for the docker rename
command so existing container you can
rename it with the new container and
that's how the uh particular rename of
an existing container will be done with
onto the docker host here so Docker
rename is the command using which you
can rename an existing container and you
can get a new container over here in
this case so these are some of the
options some of the ways that how you
can interact with the containers how you
can manage them onto your Docker host
over here so you can run it you can
create it and you can rename the
containers also so now what we're going
to do is that we going to see that how
we can uh work on these particular
container commands here so first of all
I'm going to quickly pull down a Jenkins
image so that I can get a du container
out of that so it's in kind of 600
around M Docker image which is available
there now you can also have a custom
Docker image prepared using the docker
file or you can have a redem made Docker
image pulled down from the docker Hub
also so this is going to pull the latest
version of the Jenkins from the docker
Hub so that we will be able to proceeded
further on that and we will be able to
continue with the hosting so we will be
converting or running this uh specific
Docker images in the form of Docker
container so that we will be able to
have uh instance of Docker container
over here running in this one so that's
what we are trying to do here so we will
just wait for this pull activity to
complete because it's an heavy Docker
image so it may take some time to
download and once it's download you will
be able to see that
Docker images is showing a specific
Docker image of jenin now you're going
to create a container out of that so
you're going to run a command called
Docker create and Jenkins over here so
that will help us to do uh the creation
of a Docker container and once that is
done so you will be able to see like
using Docker PS command that if any
container is up and running and if you
go for Docker PS hyphen a you will be
able to see that yes a particular Docker
container is in created mode now this is
not in running mode because you see that
when I ran the docker PS command it was
not showing it in the running mode
because I just created a Docker
container here and the created sets also
shows that yes the docker container is
created but it did not ran over here so
if uh you want to run it so definitely
you can say like Docker start
and
uh you can give the container name and
uh that will help me to uh start my
container over here but of course you if
you feel that you want to directly
create a container and uh you know you
don't want to create the container only
you want to start it also so in that
case you can go for the docker run
command so Docker run command is
something which can be used instead of
uh doing the execution so you can say
like Docker run Jenkins and what it will
do it will definitely help you to get
the uh container created onto of the
Jenkins over here in this case it's not
like only creating a container but in
fact it's actually starting the
container also so you can see that it
started over here into your screen
itself that whole Jenkins gets
initialized over here in this one so
this is the way that how you can
initialize or you can create a container
now I can just do a control C over here
because it says that Jenkins is up and
running so you can start using this one
according to your requirement over here
right so I'm just doing control C so
that it can be stopped over
there right now so this is the way that
how uh typically we can create the
containers now we got like two
containers over here and uh one is up
and running and one is something which I
just closed just now over here now if
you feel that you want to rename a
container let's say like this is a
container which is available but it's
not having a meaningful name now I want
to convert it into a meaningful name
like Devore chain so that you can easily
do with the help of Docker rename
command because that will rename the
docker container name so that you will
be able to connect onto that and you can
go for the processing so that's what we
can get over here in this one so this is
how you can do the various management of
the resources over here so let's go back
to the main content now if you feel that
you want to remove all the stopped
containers because what happens that
sometimes we simply stop the container
but we do not remove it from the uh
Docker host because um we are not uh
going for the cleanup now clean cleanup
is very required because these
containers are actually taking up the
dis space also so we need to make sure
that we should be using we should be
having like minimum amount of dis space
utilization so for that what we need
really need to do is that we need to run
Docker container prune command so this
will basically help you to uh prune or
do the cleanup uh Purge of all the
containers which is running on the host
machine here and that will definitely
help us to remove all the containers
which is in stop status so that will be
simply removed over here in this one so
uh this really completely removes all
the stopped containers onto the docker
host over here in this scenario now
second one is that it creates a new
image from a container file changes now
the we what happens that normally what
uh we follow is that we use the docker
build command to do the build process
right so Docker build command is being
used in that particular way now if you
really want to go for uh preparation of
a new you know image so all you need to
do is that you need to create a new
image from a container file so uh you
can run the command called Docker
container commit command and uh you know
whatever container is there so that will
be saved or preserved as a Docker image
because sometimes we also do some ad hoc
changes or modifications on the Fly
inside the docker container by going
inside the docker container and doing
the steps so we really need to preserve
those steps and that is the reason why
we are converting a Docker container in
the form of a Docker image here so
that's what we are trying to perform
here so Docker container is something
which is uh being uh done over here in
such a way that whatever the steps
whatever the changes we are trying to do
so that needs to be performed over here
in this case so Docker container
uh commit command is being used so that
we can get a particular you know final
image as part of the container so the
container will be converted in the form
of Docker image over here and the last
one is that how we can remove an
existing container so we can go for the
docker RM command which is available
there using which you will be able to go
ahead and you can get a particular
container removed from your existing
system so Docker armm command is pretty
much there using which we can see that
how we can remove an existing container
from your system as such what exactly
now we are going to see that how we can
perform the various commands on the
containers
here so the very first command which we
want to do here is that uh you know in
order to see that how we can actually
remove all these top containers but for
this one what we really need to do is
that we need to first of all run some
couple of containers so I'm going to
quickly run a container called
Docker run hyphen D
httpd so I'm going to run a particular
web server over here and uh it should be
able to give me a proper container up
and running so I can just double check
or validate it as with the help of the
docker PS command so Docker PS command
should give me the proper confirmation
that whether the container is in up and
running over here or not so you can see
that it's up and running from last 11
seconds here now we got a command where
you know for example we have got some
stoed containers but again you have to
manually remove them one by one now if
you don't want to do that stuff if you
feel that whatever the stop containers
which is available that should be
automatically removed from your system
then we have an command called Docker
container PR now for this what we really
need to do is that we need to actually
stop this container because again it
will be only picked up like whatever the
containers which is available in the
stop Setters only those will be picked
up so I have copied the uh container
name here in this case and uh I need to
stop it so that I will be able to uh
remove it with the pr command over here
in this one and then I can run like
Docker container pront command which
will basically remove all the containers
which is in the stop status and once
that is done you will be able to see
like that is something which is
completely gone from your system so this
is a way that how it can not only take
care of these top containers but it's
actually being done so that you can
remove the multiple containers with a
single single command itself over here
so this is not something which will be
executed one by one on a different
container but yes it will definitely
take care of doing the uh changes and
Performing the
activities now another thing is that how
we can actually do the uh uh commit of a
particular Docker image let's say that
we got a Docker container which is uh
running and there are some changes like
uh some Dynamic changes which is
available there so you want to save this
uh uh particular container as a Docker
image so that whatever the change
temporary changes are there so that will
be also saved as an Docker image so all
you can do is that let's say let again
run a particular
container of httpd here so this will
help me to get a particular container
from the HTTP perspective available of
to us so Docker PS command will give me
the
option now here I have got a container
in which it's running now if I want to
save this container let's say that I
went inside a some modification did some
temporary changes I want to refer it I
want to use it so how you can do that so
for that what you really need to do
is you need to run like Docker commit
that's a command uh or you can say
Docker because you're trying to do a
Docker container commit over here so the
commit of the container is what we are
going to do so Docker container commit
and then we going to say like what is
the container so for container we will
be yes again copying uh that uh
particular container ID
and we'll say like
Custom Image colum version one so we
will be giving a custom name or a name
of a Docker image and a tag over here in
this one so this will basically save my
Docker container which is available in
this one in the format of a particular
Docker image in a format of a Docker
image over here in this case so as you
can see that almost both of them are
having the same change but the the
container which we got uh over here in
this one so this whatever the changes uh
you want to wish you want to do you can
do that and you can then put it up into
the uh form of a Docker image a final
Docker image and at last if you want to
remove it so Docker RM and container ID
now this again Docker RM will come
remove only those containers which is in
stop status so if you want to forcefully
uh remove it so you will be able to use
it with the help of hyphen f which is
the forceful remove which will remove
the container completely from your
system and once it's removed from your
system you will be able to see like
there is no container running up and
running over here in this one so Docker
PS hyphen a is not something which is up
and running over here in this one so
they completely completely got uh wiped
out from your system so that's how you
basically work on these commands on this
uh particular uh container related
commands here so let's go back to the
main
content right so these are the commands
where we can actually do the management
like how to stop the containers how to
remove the containers and how to get a
new you know darker image out of the
container here now let's see that how we
can uh list down all the running
containers if you feel that you want to
list down all the running containers
then you can go for the docker
containers LS command and that will
eventually help you to list down all the
containers which is available there
which is uh being there into your system
then Docker stop command is there to
stop a container so what ever container
which is there in the running status so
that will be stored with the help of the
docker stop command and then the restart
command is there using which you can do
a restart of a container into your
system right so once the container is
restart so whatever the application
running inside the container so that
will also be restarted over here in this
one so these are the different options
which is available using which we can
explore over here in this one somewhat
uh more commands about the container so
again I'm going to run a container over
here of httpd here so that I can show
you some couple of more commands over
here now we can run like Docker PS or
you can say like Docker container list
will be list down all all the containers
which is available to you which is being
uh you know present in this case now if
you want to do any kind of operations
like let's say if I want to stop it now
the moment you do the stop over here so
you will be able to uh stop the
container which is up and running for
you and uh you can come uh know whatever
the application is being deployed that
will also not be available to you
because that's simply uh completely gone
for you now you can see that DOA
container list is not able to show you
that particular option that whether it's
uh being shown or not because it's in
stop status so in fact you can do a you
know start or you can do a restart so
both the operations you can pretty much
do over here so that you will be able to
have a complete activity implemented so
restart is also something which is
available which can definitely help you
to see that how the things really work
and how you can go for the container U
you know uh the management over here in
this case so you can do container do
container LS command to list down what
are the containers which is available
then you can do a restart you can do a
stop also in fact start is also there
which can help you to see that how the
containers is are really available so
this is the way that how you can manage
like uh you know even the restarts and
all because this really helps you to
even do the restart or bounces to the
application which is present inside this
container so that also you will be able
to see with this particular component
here so that let's go back to the main
container and then um you know killing a
running container so if you really want
to kill a container so for that we can
go for the docker kill because Docker
stop is also going to stop the container
but if you want to forcefully kill it so
in that case you can go for the docker
kill command to do that particular kill
on the containers and then if you really
want to go inside the container and you
want to attach your terminal
so for that you can go for the docker
attach command and with this what will
happen that you can go inside the
container you can do whatever the
changes you want to perform you can do
that and you can again come back outside
that also so normally typically we use
it so that we can see some errors of uh
in the lcks which is coming up in the
containers and if any kind of issues are
coming up for the applications we should
be able to know that part now the last
one is the uh particular weight Docker
weight is there so that will block a
container and uh it will in the weight
status so Docker weight and the
container ID or the container name you
can give so with that the particular
container will go into the block status
let's see some what more uh commands
over here so we can run like Docker
container LS again to find out like what
container is available so you can see
that this particular container is
available over here in this one now if
you see like if I want to get uh
attached to this container if I want to
attach my input and output uh stream
with this container so you can run like
Docker attach and the container ID so
your terminal will be stuck over here
because it's taking up whatever the
progress which is going to be done in
the container that will be in you know a
kind of attached to your uh terminal
over here and you will be getting the
output but the moment you come out of
this like you say control C so you will
see that you know it's basically sending
the request to the container that it's
getting stopped so you will be able to
see like the docker container list is
not going to show you because the
container is in stop status so again you
have to do the restart of the container
to make sure that the container is up
and
running all right so the attach one will
help you to go uh and uh attach your
standard input and output with your
container with the running container but
when the moment you comes out you will
be able to uh disconnect or shut down
the container and again I had to restart
that container over here in this one
then again of course we have like Docker
weit then container is there so this
will basically put the uh contain
container into a block status because
all the request and all will be not sent
to the uh container and uh it will be
blocked and uh if you really want to
like uh consume or if you want to make
use of the container you will not be
able to do that so Docker weight command
will kind of block the container and you
know if you again uh try to come out of
that so again the request uh to The
Container will be uh pres resumed and
they will be able to use on that part so
Docker weight command is being used to
block the container and uh then we have
Docker attach to attach to the container
the standard input and output and lastly
is the docker kill which is going to
kill the container and you will be able
to uh see that the container is in
simply killed status so it's uh you know
it's not like a stopped over here if
it's gracefully stopped then the exit
code definitely will be zero but in this
case since it's being being killed the
particular status is 137 over here in
this one so that's a nonzero status code
so that's how where you will be able to
manage the containers on different
aspects let's go back to the main
container so these were some of the
commands which was there from the docker
uh continuous perspective let's talk
about some of the commands from the
share command perspective so how we can
share the docker images so we have a
typical pull push mechanism which is
there using which we can actually
interact with the docker Hub or Docker
registry so uh if you really want to
pull a Docker image so in that case you
can go for the docker pull command if
you want to push an image to the
registry so you can use for the docker
push command if you want to execute a
command inside a particular running
container or you want to uh execute a
command or a scrap or something like
that so you can go for the docker is a
command and with that the execution will
be done and you will be able to get a
response so that will make sure that yes
whatever the execution you're trying to
do so that will be done over here in
this mechanism now let's see that how we
can do a pull and push activity to a
Docker image so first of all the pull
command is pretty simple all you need to
do is that you need to say that okay I
want to pull a Docker image uh if let's
say that a Docker image is already
present then it's just going to say that
it's already up to date it's already
available there but if uh in some cases
you feel that the docker image is not
available and you want to pull it so all
you can do is that you can give that
Docker image name and uh that you can
give the tag also but in this case we
are using the tag as an
latest so that's the reason why even if
you provide or you don't provide it
doesn't really matter so I'm going to
give like a Docker pull my school over
here and the moment I hit on this one
what will happen that it says this a
particular Docker image is not available
into my system it's going to download it
and once it's going to be downloaded you
will be able to see that into your local
system so Docker images is something
which is going to help you with the
commands and it will give you the uh
particular ways that how the docker
image is available so I can run like
Docker images over here to see that what
are the different darker images which is
available in this manner now if you
really want to move any of the docker
image then of course you have to do a
Docker login and uh you will be doing
the uh you know the connectivity to the
docker Hub and then establish the uh
connectivity and all so what you need to
do is that you can run the ER login
command we will also see this command in
the next particular slide but right now
we are just doing it because we want to
showcase that how the push really works
so uh you can actually give
your username that uh what username you
are holding up so the password does not
shows any activity but yes still it will
let you know that whether the login is
successful or not so in this case it
says login is successful so all you need
to do is that let's say like I got this
Custom Image and I want to push it to
the docker Hub so so you need to rename
or you need to do a tag over here
because the same if you
run Docker push
Custom Image colum version one so if you
do that it will not be post into this uh
particular Docker Hub so it's going to
throw you error over here that the
access is denied so all you need to do
is that you need to do a tag
here so you have to rename it in such a
way that uh you should be able to uh use
uh add your username over there because
dockerhub will identify that if it's
there with your username then only it
will be pushed otherwise it will be
simply rejected over there so that's
what you need to do you need to rename
your Docker image uh the same Docker
image which you have you have to append
your username of the docker Hub and once
that is done so all you need to do is
that you need to just run like
Docker then push is
there then this image name and then
version one so the moment you do this
what will happen that it will start
transferring those Docker image to the
docker Hub you see this uh specifies
that yes this is going to the dockerhub
docker.io where my username is there and
this is the Custom Image which is
available there so if I refresh my page
on the dockerhub I will be able to see
like this public repo which is available
there but again this one does requir the
credentials and the login which we did
in just uh last command where we ran the
docker login command to do the login to
the portal and after that only we are
able to do the setup and the
modifications over here so that's how
you do the login and uh you are able to
connect on that now these are some of
the pull and push operations but let's
say that if you got a container which is
up and running like this is uh right now
we are not getting any container which
is available over here so you can say
like Docker
run hyund D htpd like I'm just running a
dummy container over here so that I will
be able to show you this command now if
you for some reason want to go inside
the container and want to see some
processing and want to see some content
so you can run the command called Docker
isik hyphen it and there you can give
the container ID this is nothing but the
full format of the container ID which is
given to you so you can use this one
also or you can use the short end also
short end value also you can use and in
the last we can say like bash the moment
you do this you will be going you will
be going inside the container you see
that this is the host name which is
being shown as the container ID you can
do a like psph EF command and it will uh
throw an error that the PS command is
not available because you are running
this command into the docker into the
docker container so using these kind of
commands which is not available in
container will help you to identify that
yes you are not on the host machine you
are in fact into the docker container so
that's the reason why you can do it
because for you the parts and all
everything is same the terminal is also
same so you may not be able to
differentiate in that parts so that's a
reason why it's preferred that we use
that uh commands which is going to fail
but that's okay it will give you that
okay this command is failing because I'm
into the docker container right now so
that's what we are performing over
here right and you can come out of that
but still the got Docker container is
going to be still up and running over
here in this case so the container is
not going to be down over here in this
one so that's how we manage the
container we can go inside and we can
check some informations and get some
files there
so these are some of the commands where
we can inct now in order to log to the
docker Hub because before doing the
docker post you have to login uh you
have to get a credentials and the
connectivity with Docker Hub so you can
run the docker login command where you
can see that how the execution can be
done now using the docker login command
you can interact uh with the uh
particular Docker Hub you can set up the
credentials once the uh credentials is
being set uped so what you can do is
that you can go ahead and say that okay
the uh configurations is done done and
uh we can go ahead and we can perform
the execution so Docker login is a
command where you can give the
credentials Docker info is the command
using which you can get the uh
information about the docker tool so
whatever the docker info is available
there so those things you can actually
get it over here in the case of the
docker info so Docker info is going to
give you complete information that what
exactly you have the information
available over here on this one and then
we have the docker history so using the
docker history you can the history of
the docker image that what exactly uh
the mechanisms or what are the different
image IDs are available there so uh this
is basically giving you the complete
history on how the uh Docker image is
prepared all the commands is available
there and that will help you to
eventually go and understand that how
the images can be going on and you can
manage on that part so uh these are some
of the couple of options which is
available there where we can see that
how the executions can be done and we
can can go ahead on that particular part
so Docker history is the command again
which will help you to see that how you
can go for the uh particular mechanism
and you can list on on the things over
there so Docker login was already done
previously when we tried to push the
image but again if you try to run Docker
login you will be able to do that now
Docker info is something which will give
some additional information about the
docker and uh all the informations about
the docker will be given back to you
over here in this one so the complete
Clarity uh then let's say that you got a
Docker image called uh HD now you want
to check the history of this image that
how this image is prepared what are the
different layers which is available how
many layers are there how much size is
being modified in each and every layer
so you can see that we got a layer where
we have a par changes of 7.8 38 MB but
pretty much the uh from the complete
size of this httpd The major portion is
actually of the uh this one the basic
operating system which is being
implemented and then of course there are
some packages which got deployed due to
which this uh 60 MB and 7 MB size is
coming up there so this actually gives
you the complete layerwise mechanism
that what are the informations which is
available that how the modifications
actually got modified and got changed
over here in this one so this is the
additional uh help which you can get
from the docker to manage the containers
here so these are the ways where you can
interact with the docker Hub if you
really want to create a volumes for the
containers so you can go for the docker
volume create command so Docker volume
create command is there using which you
can create the docker volumes and uh you
can pretty much do that uh setup so
Docker volume create is the command
which is available there for that
particular part it will create a volume
for you now let's see that how we can
actually work with the volume so you can
run the command called Docker volume LS
to find out at what are the different
volume switches available there now if
you really want to create a volume over
here in this one so you can say like
Docker volume create and uh you can go
for the uh volume name here now the
moment you do that what will happen that
a volume will be created and you can
validate it with the help of Docker
volume LS command that whether the
volume is being created or not over here
so this is the way that how you can
manage the volumes onto the docker host
now let's talk about some of the docker
swam commands here now Docker swam
commands is there to set up a particular
mechanism where you can see that how you
can perform a setup you can uh actually
go here and see that how the setup and
modifications can be done so Docker swam
is there where we can see that how the
execution needs to be done so uh you can
go for the command called Docker swam
init command this will actually go for
an initialization of a Docker swam
cluster and once the docker swam cluster
is initialized then you can pretty much
go there and perform whatever the
executions you want to perform you can
do that and you can execute as such over
there so Docker swam in it is a command
using which you can actually see that
how the the execution needs be done and
how we can perform the uh you know
mechanism and the changes over here so
Docker swam in it uh Hy aan advice uh
ADR address is there which helps you to
see that how the executions can be done
now joints you have the docker swam join
command which is available there now if
you go for Docker swam join command so
uh you can have a particular host so
Docker swam join command is there which
you can run on the Node machine and in
that case what we have that node will be
a part of the docker swam cluster and
then if you want to leave uh from the
swam cluster then you can go for Docker
swam leave command and it will be simply
removed from the docker swam cluster
there some of the couple of commands
which using which we can initialize or
we can uh remove from the docker swam
cluster here so Docker swam is already
being initialized or it's being created
when the moment you do the installation
of doer so toer swam in it is the
command using which you can inal inaly a
Docker container now once you inaly over
here so init after the initialization
you got a command you which you want to
add it like you want to run it onto the
Dockers famam uh
cluster it will act like a worker node
so that you will be able to deploy the
containers on top of that also if you
want to join it as a manager so you can
also get the join token again so it will
give you the proper command now if you
run like Docker node LS so you will be
able to see like what are the different
noes which is available there and for
any reason if you're not comfortable
with that and you want to reinitialize
your Docker swam cluster so all you can
do is that you can say like Docker swam
leave hyphen f and you will be left out
of the docker swam and now if you want
to run like Docker node LS you will not
be able to get that output because it's
saying that there is no uh particular
swam manager which is available so you
can run Docker spam in it or you can run
the join command to connect to that
specific node but we have already left
it and nothing is available over here so
that's a way that how you can manage
like what are the different Docker swam
commands we can typically run to manage
the docker swam cluster now let's talk
about the docker compost commands now
Docker compost commands is actually
there using which we can create multiple
containers it's not only one container
which can we can create we can create go
for multiple creations of the containers
with the help of Docker compose file so
Docker compose hyphen version is the U
command which we can give you the
version of do compos which is being used
so here we use a particular wml format
file which will help us to create the
multiple containers we Define the
services that what are the different
containers we want to create here and
then with the help of Docker hyphen
compose WL file we can create or we can
have that particular Docker compost file
set uped over here in this case so
Docker hyen compost. yml file is used to
do the setup and the modification so
that you will be able to have the
modifications done and uh you will be
able to find out a particular uh
multiple containers gets created all
together so Docker compose is the is the
way that how you can do the setup how
you are going to run the docker compose
file so in that case you're going to run
the docker hyphen compose app command
which will help you to set up like how
the compose can be done and uh once you
are done with the docker hyphen compose
app command so using this the compose
will be done and the containers will be
up and running so these are the commands
using which we can manage multiple
containers using the docker compose one
so let's run some commands related to
Docker compose so Docker iPhone compose
iPhone version will let you know that
what exactly version of Docker compose
is installed into your system so you can
run typical apt command to do the
installation so Docker hyphen compos is
the package name which you can install
so that the executables and the package
can be installed into your system onto
the server now all we need to do is that
we need to create a doer compos file so
what we need to do is that we need to
create a Docker hyphen compos do yml
file is the default name of the Y file
which we want to create here that's a
standard name so once you create this
all you need to do is like you need to
put some uh values attributes the very
first one is the version value that what
version of uh this uh particular uh
attribute you want to create here let's
say that I want to create a version two
so I will configure here that yes the
version two is what I require I want to
set up over here in this one now again
if I want to configure that what are the
different Services we want to run
because Services is the main thing which
we will be running on this machine on
this server okay so that's what we
require so then we need to put for a
particular service called Web so web
service we want to search and we want to
run and then again we will need to take
care about the spacing because that's
very important
part and uh we need to provide the
values of the image and uh all these
configurations here so that you should
be able to set up like how the uh images
and how the docker images can be built
up over here so uh web is the service
which we want to deploy and then of
course the image is the attribute which
is there for the web that yes engine is
the one which we are trying to build up
now you can give the ports configuration
also so you can say like
ports and inside that you can say like
what port mapping you want to perform so
you can say like um 80 80 or sorry 80 is
what you can attach for the engine
because it runs on 0 Port but now I have
to run like 0 Port I want to attach that
Port over here in this one so I can
specify that at Port should be available
in this one so I'm going to save this
one here and I'm going to run a command
called Docker iPhone compose up now this
will run the containers in the
interactive mode but if you feel that I
want to run in the detach mode so you
can say like hphone d also which will of
course run these ones into the detach
mode and it will be executed so it will
process your Docker compose file and
since you have used a standard name
called hyphen Docker hyphen composed. ml
so it will automatically pick that
Docker details and start creating the
container if you are looking to get
certified into devops and become a
kubernetes and Docker expert have a look
at wide range of courses on devops by
simply learn in collaboration with
topnotch universities across the globe
so why wait enroll the course now from
the link in the description box as
angular is a front-end framework we'll
be using a web server called enginex to
serve the application HTML files after
this session you will understand three
things first what is Docker and what is
it used for you will understand the
basics of Docker and what problem do ER
fixes next you'll learn how to write a
Docker file which you can think of as
the recipe Docker uses to build your
image in this session we'll be using a
multistage Ducker file to build our
application third you will learn how to
use an engine X server to serve an
angular application inside a Docker
container to follow through the examples
in this session you should have an
angular application ready and the latest
version of Docker installed on your
Workstation you can install Docker by
following instructions on
doer. first let's look at what Docker is
Docker is the most popular
containerization technology and it has
quickly become the de facto standard
when talking about
containers containerizing an application
means packaging it and all its
dependencies into a single easily
transportable and isolated
container this container will run in
exactly the same fashion
regardless of the computer it is run on
by providing this layer of consistency
Docker fixes the traditional but it
works on my machine
problem instead of Distributing just our
application we're Distributing a full
runtime environment along with our
application while not exactly correct it
might help you to think of a Docker
container as a lightweight virtual
machine inside your
computer your computer can run multiple
Docker containers at the same time
stopping and starting them individually
as
required a common problem in software
delivery is dependency
management when one application is run
on multiple development machines and
multiple server environments a small
difference in the version of an external
Library can change the functionality of
your application making it behave
differently on different
environments the beauty of Docker is
that if you build your application into
a container image and transfer the same
container image to your colleague's
computer you can be sure that the
application will function identically on
both
computers this is because the container
includes all dependencies for the
application inside it on the other hand
a Docker container should not have any
dependencies to the host it's running on
apart from Docker
itself it's important for you to
understand what images and containers
are and what's the difference between
them for the purposes of this session
you can think of a Docker image as
something that holds a file system and
some metadata in it the metadata
includes things like the name or tag of
the image and instructions for Docker
like what command to run by default when
the image is
started while the official Docker
documentation is a bit vague about the
meaning of names and tags you should
learn the basics of it every Docker
image has at least one tag and in fact
the same Docker image can have multiple
tags pointing to
it every image tag has a name part and a
version part separated by a column for
example the engine X image has multiple
tags representing different versions and
different flavors of the
image there's a special version tag
called latest which points the latest
version of the image all Docker commands
default to using the latest version tag
if no other version tag is explicitly
defined when you build an image on your
local machine or for example a build
server you can upload the image into a
Docker
registry when you configure other
machines to use the same Docker registry
you can download the same image to as
many other machines as you like there is
an official Docker registry called the
docker Hub and the model is the same as
for example GitHub you can store public
images free of charge under your account
but you will need to pay to host private
images having your images public is
great for hosting open- Source projects
but for any commercial proprietary
applications you will most likely want
to opt for hosting the images
privately if you don't want to use a
thirdparty service you can always run
your own Docker
registry my advice is that unless you're
planning on hosting a very large number
of images it's absolutely worth the cost
to spend a few dollars per month on a
hosted Docker registry such as Docker
Hub once your application ecosystem
starts to build around Docker the
registry becomes a critical piece of
infrastructure and keeping it up and
running can prove to be a substantial
piece of
work when you build or download a Docker
image to a computer and run it it
becomes a running container
if you don't tell Docker otherwise
Docker will run a pre-specified command
inside the
container as far as this command is
concerned it is running inside a Linux
server and it has access to other
commands and resources that are
available inside the same
container you can think of a container
as a Sandbox where processes running
inside the container do not have access
to the host system or other containers
unless you explicitly specify otherwise
note that the complete container runs
around a single process created by the
initial command used to start the
container if this process terminates for
any reason the container will
stop an example for our container would
be the engine X server process when you
start a container the engine ex server
process starts in it and when the engine
X server process dies or is killed the
container will stop as
well it is possible to run multiple
processes inside a single container by
for example using an init system to
spawn the processes you need but for the
purposes of this session we'll use the
concept of just a single process per
container the basic workflow with Docker
is as
follows first you will build a Docker
image either on your workstation or on a
continuous integration
server second you will push this Docker
image into a Docker registry to make it
available for other
computers third you will most likely
want to run the image on a server that
is accessible via the Internet so
clients can access whatever server
software is running inside your
container most often this is an HTTP
server and in this session we'll be
using the engine X HTTP
server in software delivery environments
that Embrace continuous delivery this
workflow is often part of a fully
automated
pipeline a pipeline like this can
trigger from committing a piece of code
into a version control system and just
automatically deploy a new version of
your application into a testing or even
production environment whenever you
change your application
code as we discussed earlier a container
is isolated from the machine it's
running on as well as any other
containers running on the same machine
unless you explicitly Define connections
between
them this is out of the scope of this
session but I recommend you spend some
time after the session to learn about
Docker networking and links between
containers in this session we'll be
running a single container that can
connect the internet to the host machine
and expose a single TCP port for the
enginex server
process building Docker images is done
with Docker files
you can think of a Docker file as a text
file containing a recipe to tell Docker
how to build an
image the basic function of a Docker
file is to be a list of instructions
that incrementally manipulate an
existing Docker image the word existing
here is important while you can
theoretically build a Docker image from
scratch in most use cases you will
actually be using another Docker image
as the base for your new
image many Frameworks and open- Source
projects publish their own Docker images
which can be used by developers to build
images on top
of in this session we'll be using a new
Docker feature called multi-stage
builds this feature is available from
Docker version 17.05 forward and it
allows you to use one base image for
building our application and another
base image for serving it this works
brilliantly for our use
case while angular is often used as part
of the full mean stack in this session
we're only concentrating on
angular I'm assuming you know angular
and how the angular command line
interface works and that you have a
package. Jon in your application
directory if you're not quite there yet
check out the excellent getting started
guide on the angular
website please note that the
functionality of angular varies wildly
between major versions however the
fundamental idea behind containerizing
an angular application is the
same our Docker file is going to have
two
stages first we'll need to build our
angular application using the NG build
command this will output the package
application in the disc directory under
our application
route in the second stage we will place
the file from the dis directory into a
directory that's served by a web server
and finally run the web server
itself this is where the multi-stage
build feature comes in handy as you
remember a Docker container only has the
software available which we've
explicitly added into
it this means that for the NG build
command to work we need to have the
angular command line interface installed
inside our Docker
container
this is something we wouldn't want on
our final Docker image because it's just
extra weight and on the other hand we
wouldn't want the web server inside the
image we're building our application
in because the angular CLI is
distributed as a node package we can use
the official node Docker image as the
base of our first Builder
stage because angular is a front end
framework it doesn't come with a server
side comp component other than the test
server shipped with the angular
CLI unfortunately this test server isn't
fit to be run on production so we'll
want to use something else to serve the
angular HTML
files in this example we will use a
robust web server called engine X to
serve the angular
application please note you could just
as well be using a server application
like Apache or liy instead I'm not going
to be diving into the specifics of
configuring engine X engine X does come
with good documentation but more
importantly it also comes with an
official Docker image we can use the
official engine X Docker image as the
base for our angular application
image the good thing is that this makes
our Docker file very short and simple
let's have a
look this is the docker file and it's
divided into two
stages the way the multi-stage build
Works in Docker is that we're actually
creating multiple Docker images with a
single Docker file but we're only
keeping the last image we've
defined this allows us to build the
application first and then copy the
resulting build artifacts into the next
Docker
image the idea behind this is that our
final Docker image won't have all the
build time dependencies in it which
makes the resulting image nice and
small in the first stage we'll use the
from directive to instruct Docker that
we want to use the node 8 image as our
base
image we're also using the as Builder
keyword so we'll be able to reference
this stage in our second stage
later the next directive is the copy
directive this command is for copying
files and directories from our local
machine to the docker
image we're giving this directive two
arguments
the location of our application code
directory and the target path within the
docker
container in this example our
application code sits in test app and we
want to place it in the directory test
app inside the
container the next directive is work
dear this tells Docker that all
following commands should be run within
the specified directory which in this
example is the application directory
Dory within the
container the next directives are the
Run directives as you might guess these
directives instruct Docker to run
commands inside the
container first we're running npm
install to install all node packages
defined in the package.json file of our
application then we're running ngg build
to package our angular application in
the dis
directory now we'll get to the second
stage which is copying the contents of
the disc directory into a directory
that's served by default by
enginex we're using the from directive
again this tells Docker to start
building another image we're defining
the from image to be an image called
engin X which is the official enginex
Docker image this image is preconfigured
to run the enginex server and serve HTML
files that are stored in a predefined
directory next we'll use the copy
directive but this time with the from
argument this tells Docker to not look
for the source path in our workstation
but instead the previous stage in this
Docker file essentially we're copying
the contents of the dis directory in the
previous Docker image into a directory
called user share engine X HTML inside
our new image
the engine X Docker image will then
serve the contents of this
directory finally we're using a
directive called expose to tell Docker
that the port 80 has a server running in
this image the port 8 is the default
HTTP Port as you might
know note that the exposed directive
alone isn't enough to actually expose
the port when the image is run you can
merely consider this as a documentation
feature and we'll still need to
explicitly map Port 80 from the
container into a port on the host
machine this enables you to connect to
the mapped port on the host machine
which will then forward the connection
into the
Container to follow this session for
your angular application create a file
with these contents and place it in The
Parent Directory of your application
code remember remember to customize the
paths in the file
accordingly make sure the name of the
file is Docker file written as one word
with the capital
D now that you've learned what the
docker file is and what one looks like
for an angular application let's learn
how to use it to containerize an example
angular
application for this we'll be using the
docker command line
interface after you've installed Docker
on your workstation you can use the
docker command line interface to build
and run
images to find the command line shell on
a Mac click on the spotlight search icon
on the top right corner of your screen
and write terminal followed by
enter while the docker command line
provides many features in this session
we'll concentrate on two the docker
build and the docker run commands first
let's navigate to the directory with our
Docker
file the command we'll be using is
Docker
build we'll be giving the command the
minus t argument which defines the tag
for the image that will be built for
this we'll be using the tag test
app the second argument for the docker
build command is the built
context this is where Docker expects to
find the docker file
we'll use a single dot to denote the
current directory and press enter to
start
building you can see Docker runs through
every directive in the docker file as a
step outputting the directive and the
output of running
it finally you'll see that Docker has
successfully built the image and tacked
it as test app later
as you might remember from before since
we didn't Define the version tag the tag
latest is used automatically great work
now let's run the image we just
built for this we'll be using the docker
run
command we'll be giving the docker run
command a few arguments first minus I
and minus t to denote that we want to
run the image in an interactive terminal
that is running the container attached
to our current terminal session so we
can see the output and interact with the
process we'll also be using the minus P
argument to map the port 8080 on our
workstation to Port 80 inside the
container as you remember the engine X
server is listening on Port 80 with this
mapping anyone connecting to port 8080
on our workstation will get forwarded to
Port 80 inside the container
finally the last argument is a tag of
the image we want to run we're using
test app colum latest now that the
container is running let's use our web
browser to navigate to port 8080 on our
local machine and we can see our angular
application being served well done you
can see that engine X by default outputs
all the requests in the standard output
of the
container after you've finished testing
press contrl C in your terminal to
terminate the engine X process and tus
kill the container you were running and
that's how you dockerize an angular
application before you go three
takeaways one using Docker makes it easy
to ensure that your application runs
consistently on different workstations
and
servers secondly angular is a front- end
framework angular needs a separate web
server software for serving it enginex
is a fast and robust Choice finally
using multi-stage Joker files enables
you to streamline your build process and
create slim Docker images why exactly
the docker networking is important let's
talk about some scenario over here so my
application is something which totally
works perfectly fine into my system and
there is no issues on there I ran it
perfectly fine and I got the response
I'm able to interact everything I'm able
to do but the same application does not
works on the other person's system now
what could be the ASO because it's
working on one machine and it's not
working on another machine now the
application does not work on another
system because there is a difference in
the computer systems maybe one system is
having a different machine and another
system is having a different machine and
you may be having some of the
differences maybe in the uh term of
Hardware maybe in the terms of software
so the differences are possible in two
different machines so how we can get a
solution that uh we don't get this kind
of problems and once the container is
prepared it should be able to run it
across different systems so doer
networking is the ideal solution for
this uh problem over here with Docker
networking the application works totally
fine and uh it's uh something which can
uh work on any system you don't have any
dependency that it will run only on a
specific machine or a particular system
so what exactly is in Docker networking
all about so Docker networking enables a
user to link a Docker container to as
many as networks you know he or she
requires so uh you can actually connect
a Docker container to a specific Network
the docker networks are used to provide
complete isolation to the docker
containers so you can have it like okay
I want to run some couple of containers
in a different network and another
containers I want to run it into a
different networks so you can do that
isolations and the whole purpose of
doing that isolation is so that we will
be able to have a proper networking and
proper executions and all that stuff so
Docker networking is a very important
concept when we talk about or when we
come up with the interaction on having
the docker containers isolated so it
resolves most of the problems okay so um
you can have like uh multiple containers
and running in same network you can have
uh different uh particular networks also
there so uh a specific Network have its
own uh attributes and then you can use
those attributes to connect to that
containers and to access that containers
but the whole idea is that the
containers are something which is uh
specifically running in a set of uh
servers or in a set of uh platform so
whenever I'm trying to look forward to
that particular container uh it has to
be there into a swim network if you
don't run it into your custom Network
then whatever the default Network
present it uh will be hosted in that
Network there so how does the docker
networking actually works so you got a
Decor file so using the docker file you
got up a custom Docker image and and
from this Docker image you have to
actually go there and uh you know put up
like the docker container so um you can
also store this Docker image to the
docker Hub so that the same Docker image
can be shared across different users and
they can also pull down their Docker
image and they can have the container
running in their systems so for the
collaborating the docker image you
require the docker Hub and the container
is the ultimate thing or object which we
need to create out of the docker image
when we go with the uh particular
process when the whole process goes on
over there so what happens that uh the
instead of the docker Hub you can use
your own private repositories also
that's something which totally relies on
to you that how exactly those things
works on so a Docker file creates a
Docker image using the build command uh
a Docker image contains all the project
source code all the dependencies U using
this Docker image you can actually run
the source code in order to get a Docker
container the moment you initialize or
run an instance of Docker image you will
be able to get a docker container you
can actually use the dockerhub for
collaborating or for sharing the docker
image so you can store the docker image
on Docker Hub and then different people
can actually go ahead and download or
pull this Docker image onto their system
and they can also run their specific
containers so what are the different
advantages we get with the docker
networking very first one is the rapid
uh deployment portability better
efficiency uh faster configuration
scalability and the security over here
so these are some of the advantages
which we get when we talk about the
docker networking here now let's talk
about the container Network model here
so what is the network model we are
looking forward from the containers
perspective now this is a kind of a
architecture here now in this one let's
see that how it actually works on now
you have a Docker engine which is a main
component which takes up all the things
and all the important components are
actually interated with each other now
the network sandbox is the one in which
you will be having your container
running up and running now every network
is having its own attributes the uh
particular different Gateway IPS the IP
addresses the range of IP address the
cidr blocks also everything can be
different to for two different networks
so the whole point is that you will be
defining okay if you don't Define that I
want to run this uh particular container
into a specific Network then definitely
that's going to run into default Network
which is which gets created the moment
you do the installation of doers but
anyhow any network is required for
running a Docker container so there are
different network drivers which is
available there so if you feel that you
want to create a network so you have to
choose a particular uh networking or the
connectivity over there and uh it really
helps us to understand that how the
configurations and uh how the different
modification and those changes can be
really done here so a lot of changes are
actually going on and uh you know
performing there and this really helps
us to understand that how the things are
being done as such over there so network
is something which is very important
from the containers perspective so the
network sandbox is an kind of a isolated
sandbox that holds the all the network
configurations of the containers so you
will be requiring a at least one Docker
container network if you want to run a
container inide in that particular
sandbox so sandbox is created when a
user requests to generate an endpoint on
the network so that's where the sandbox
get get created uh it can have several
endpoints in a network as it represents
a container's network configurations
like IP address Mac address DNS name so
every container will be having its own
unique IP address right so that can be a
particular endpoint to connect to that
container and it can have it like
different uh endpoints and different
different networks so that's always the
possibilities which is available there
now the endpoints uh uh establishes the
connectivity for the container services
with other services also so so the
moment you work on the uh particular uh
endpoints so that can be used for having
okay there is a service running in a
container so how you can do that so if
in order to connect that you require
these endpoints so that you can
establish the connectivity with the
container services whatever running
inside these Docker
containers right now the network is
something which provides the specific
underlining Network attributes like uh
the networking support to the docker
containers because these uh Network
sandbox uh boxes will definitely have a
requirement of the network so how you're
going to get that that you will be only
able to get when you go okay I will be
looking forward for the connectivity and
I can go for the communications and I
can give the hardware because although
it's a OS level virtualization but still
you require some interaction with the
hardware from the networking perspective
and then Docker engine is uh the base
engine uh which is installed on the host
machine and it's actually the main
component which makes sure that the
docker containers and the correspond ing
Docker services are always up and
running and the uh using the network
drivers you can actually create multiple
networks onto your system and you can
have those uh configurations now the
network architecture is something which
helps us to provide the connectivity
among the endpoints that Bel probably
belongs to the same network and uh it's
something which is a kind of isolated
Network so different different networks
can be isolated uh networks and we can
have the end points to them so uh there
are different ways in which we can
establish Lish the connectivity and this
is a complete Network architecture which
is there so you have a network drivers
you have the networks you have the
network sandboxes in which internally
the docker containers are running and
then these uh Network sandboxes are
having their respective end points there
to access them to establish the
connectivity now let's talk about the
network drivers now there are five type
of uh network drivers which is available
first one is the bridge host none
overlay and Mac vand so these are the
different uh particular network drivers
which is available over here uh first
one is a bridge it's a private default
Network created on the host if you go
there if you go to the installation of a
Docker this is the default driver which
gets installed onto the system onto the
host the containers link to this network
will have an internal IP address through
which we they can actually communicate
with each other on the same machine also
you can establish the connectivity you
can connect to the containers using that
private IP address but outside the
server you will not be able to have the
communication but internally you can
connect to the containers and containers
can also interact with each other they
can also communicate with each other
with this one uh the docker demon
creates the uh Docker zero so if you run
the IP conf config or if config on the
system you will be able to see a Docker
zero kind of ethernet uh Bridge uh
Network which is created using which you
can see that okay this is the network
which is created by default it's a
default installation or default
configuration which automatically
happens the moment you set up a Docker
instance so you don't have to go into
the configurations or in the setup of
there it will be already available on
that particular installation mode there
and next one is the host one so it's a
public network so when we go with the
public network it's actually uses the
host IP address and a DCP port in order
to uh interact with the service running
inside the docker container so if you
are looking forward that I want to reach
out or I want to connect to the
container but I want to use an uh IP
address which can be accessible from
outside world probably some other server
should be able to resolve that in that
situation the host Network driver comes
into the picture so it uh effectively
disables the network isolation between
the docker host and the docker
containers which means that using this
driver a user can actually run multiple
containers you know it's it's not
possible for the user to run multiple
containers on the same host because of
the fact that you are using uh the host
IP address now when we were talking
about the bridge connection at that
moment of time we were getting the
private IP addresses which was totally
isolated and totally unique so there
were actually if you create uh using the
bridge connection multiple containers so
you will see that if there are 53
containers everyone will be having a
different P private IP address but the
moment you go for the host one so in
that situation the IP address is not
going to change so you get some
restrictions okay because the
combination is of IP address and Port so
uh IP address and at8 can happen only
once it cannot happen multiple times so
that's a limitation that you cannot run
multiple containers on the same host due
to this restriction and next one is the
none so in this network driver the
docker containers will uh neither have
any access to the external network or it
will not be able to communicate with the
other containers also total isolation
and total private containers are being
done when we use this driver so if we
these options are usually used when you
want to uh disable so this is not doing
nothing but disabling the total
networking functionality for a specific
container that's what it happening over
here next one is the overlay so this one
is uh utilized for creating an internal
private Network to the docker nodes in
the docker swam cluster so overlay does
not comes up in case of a standalone
Docker installation the moment you go
with the uh particular Docker swam
cluster at that moment of time the
overlay type of network is created so
Docker uh swam is an oration tool so uh
that requires a overlay kind of uh
driver which can um you know help them
to achieve the networking in case of
cluster because in case of Docker swam
you have a master node manager node
worker nodes different different
machines are there so in that situation
overlay is the uh main component or is
the main type of uh driver which is used
and then we have the macvan so it
simplifies the communication processes
between the containers this network
assigns a MAC address to the docker
containers with this Mac address the
docker you know servers will route the
network traffic to a router so it's it's
not on the basis of IP address it's
actually on the basis of Mac address so
uh on the specific Mac address the
network the docker server will redirect
the network to a specific
router right it's usable when a user
wants to directly connect uh the
containers to the physical Network
rather than the do doer host so that's
where this communication works on so
it's a direct connectivity between the
host Network and uh the docker container
so there is no virtualization which is
happening over here so uh it's something
which is you know giving a direct access
to the networks but it's definitely on
the basis of Mac address which on which
the communication is happening now let's
talk about the networking implementation
and we'll see that how we can go for a
particular Bridge Network implementation
there so let's go back to the virtual
machine so I am connected to a
particular system here so I can check
that if the docker is available to me
and I can just run the docker version so
it says that okay the uh client and the
server component both is available there
now we are going to see that what are
the different uh containers which is
what are the different Docker containers
Network which is available there so I'm
going to run a command called Docker
Network LS now the moment you do it over
here you will see that none host and
Bridge these are the different
containers uh Network which is available
there and uh you can see their Network
IDs the names the drivers the drivers is
also very important because that's what
we are going to use here so it's a very
good one here so the default one you can
see that the bridge is uh available and
uh it's a default one whenever you
create any container by default whether
you give the container attribute or not
it's something which gets created into
the uh terms or in the uh basis of uh
the container only in the default
Network only so uh how we can get that
we can always get that part by um you
know having the understanding that what
network we are using so I'm going to run
a couple of containers or small
container and we can see that how
exactly uh the containers can be
executed and uh can be done over there
right so uh let's see so I'm going to
run a
command
Docker run hyph D
it iPhone iPhone
name Aline
one
Alpine image is what I'm going to use
and so it uh pulls down the Alpine image
that's what it happened and the same
thing I'm going to run but uh this time
I'm going to run uh Alpine 2 because uh
I want to create two
containers right so with this one what
will happen that I will be having a like
two containers available here so I can
say like Docker PS so I can see that two
containers are ring one is the Alpine
and one is the Alpine 2 right so now
what happens that we will see that how
the connectivity happens and what are
the network details so what I'm going to
do is that first of all I'm going to do
an uh inspect over there onto the uh
Bridge connectivity so that I can get
the detailed information so um this is
the name of uh the particular ones of
the network so this is the network which
is available and this is the default one
so if I scroll up you can see that this
is the name of the bridge connectivity
and uh even if you run the docker
Network LS so uh you will be able to see
that the ID also so ID also if you
compare so uh e65 that's the one which
is available now this one is something
which is using a driver called bridge
and here you can see the subnet
information now if you scroll down you
can see that two containers are
available here so I did not provide any
kind of network details but uh
definitely what happens that uh the two
containers which I deployed are
basically running inside this network
okay and you can see that uh the bridge
name Docker zero is the internet Network
which gets created uh which during the
installation of Docker gets uh
configured now you can see that uh this
is the IP address this is the IP address
here this is the MAC address and this is
also a MAC address here so uh the IP
address you can see this is a 3.0 uh do3
in the last and this is2 over here so
you can see that increase the sequence
is there so Alpine 1 was having an IP
1721 17.0.2 and uh Alpine 2 is having an
incremented IP address like a different
one as in three so you can see that
these different IP addresses are there
and uh represents if you want to access
them if you want to connect on them you
will be able to have the connectivity
accordingly so uh even if you go inside
these containers you can actually see
that what exactly
uh the connectivity you want to
establish or the connectivity you want
to perform
here right so this is the way that how
the network uh usually works and uh the
networks Bridge you are using over here
is the bridge ones which definitely
helps us to understand that how the
configurations can be done now so uh
what we are going to do over here is
that we will try to see that how exactly
uh we can uh go ahead and uh we can
connect on there and uh we can also see
that if we are are able to communicate
with each other because probably uh if I
run like Ping command over here and I'll
put the containers like
two and I'm able to reach out there is
no problem in reaching out to these IPS
because these are the private IP
addresses and I will be able to reach
out to them without any problem and
without any concerns so from the current
uh connectivity perspective there is no
problem as such and I have a connection
uh available over here but let's see uh
we'll go uh back to the container we
will see that how exactly we can talk
about we can reach out the one container
with each other so I want to connect
Alpine one with alpine 2 so how we can
do that so I'm going to use like Docker
L Pine
one okay now I got a terminal so what I
can do is that I can go for the IP and
just show so with this one what will
happen that I will be able to know the
IP address so do two is the one which is
available over here okay so this is the
IP address now I can uh simply do the uh
small ping command so I can say like
Ping google.com so that will show me
that if I have an internet connectivity
so that is great so I got a connectivity
with the internet uh over there so I
will be able to reach out any websites
or any kind of that order now uh I have
another IP so 3 is remember we were
using different IP so I can actually do
a ping also here so pink hyphen C5 five
request will put and uh three we do here
now we are trying to reach out to the
Alpine 2 over there so although we are
connecting to the Alpine 1 but we have
the connectivity with alpine 2 also so
from Alpine 1 you can connect to the
Alpine two containers so that shows that
since both of them are running in the
same container so they are having the
automatic uh connectivity and you can
easily reach out to them and without any
issues you can do that and similarly the
same thing you can do it in case of
Alpine 2 also so you get it connected to
the Alpine 2 and uh you can uh just
follow the same thing so you can connect
it with the if you go to the Alpine two
you can connect to the Alpine one also
so if you come out the containers will
be always up and running and uh you will
be able to have the uh mechanisms there
so the whole idea about these things is
that it will be basically helping you to
understand that how the executions can
be done and how we can perform the
activities as such over here so this is
the way that how we deal with the
networking and how the containers gets
created into a network and then they can
interact with each did you know the
famous Global researcher Gartner
predicts by 2023 more than 50% of the
companies will be adopting Docker
containers however a serverless
container like Docker will have a rise
in the revenue from a small base of
465.16 in the year 2020 to
944 million in the year 20 24 in this
video we will be doing a side by-side
comparison of the darker containers and
the virtual machines but before that
let's have a look at the objectives of
this lesson in this session you will
have a good understanding of what a
virtual machine is and what a Docker is
followed by that we will have a major
difference between Docker and the
virtual machines also we will discuss
some of the key differences between
Docker and virtual machines and finally
a use case on what made BBC to use
Docker so the first one what exactly is
a virtual machine it is an isolated
Computing environment that enables a
person to use an operating system via a
physical machine virtual machines
provide the functionality of a physical
computer as you can see on my screen
there is a virtual machine and this is
how a typical virtual machine looks like
now let's look at what a Docker is is
Docker is an operating system level
virtualization software platform that
enables software developers and it
administrators to create deploy and run
applications in a Docker container with
all their dependencies and a Docker
container is a lightweight software
package that includes all the
dependencies like Frameworks libraries
and manyo which are essentially required
to execute an
application this is the architecture of
Docker tool now let's move on to our
next main topic for the discussion that
is Docker versus virtual machine when it
comes to comparing the two we could say
the docker containers have much more
potential than virtual machines as you
can see on the left and right hand side
both the images look similar now let's
define the layers of virtual machine
from the bottom up let's begin with the
infrastructure infrastructure could be a
computer system system a laptop or a
virtual private server such as an Amazon
ec2 instance or as your virtual machine
or gcp virtual machine instance on the
top of the infrastructure runs the
operating system and operating system is
a software that manages hardware and
software resources for the computer
programs on your laptop this will be
likely Windows Mac OS or Linux flavored
operating systems such as one two next
comes the hypervisor a hypervisor is a
firmware that builds and runs virtual
machines there are two types of
hypervisors type one hypervisors are
hyperkit for Mac OS hyperv for Windows
and KVM for Linux operating systems and
type two hypervisors are virtual box and
vmw then we have the guest operating
systems consider an example where you
want to run two two apps on your server
in total isolation this process would
require two guest operating systems
which will be controlled by a hypervisor
virtual machines come with many
dependencies where each guest operating
system will at least occupy 512 MB of
your RAM this is worse because each
guest operating system needs its own CPU
and memory resources eventually this
will be expensive then on top of that
each guest operating system requires its
own dependencies such as binaries and
libraries since every application has
different dependencies it requires its
own set of libraries finally it's the
application this layer consist of the
source code for the application you have
built this was all about the virtual
machine on a server here's what the same
setup looks like when you're using
Docker containers here you'll notice
that there are a lot fewer layers Docker
doesn't require a bunch of massive guest
operating systems let me break it down
from the bottom up again infrastructure
and host operating systems are the same
as we discussed for the virtual machine
coming into the third layer instead of a
hypervisor Docker uses Docker engine
Docker engine or Docker is a client
server application that builds and
executes containers using Docker
components next we have our
dependencies just like on the virtual
machines here dependencies are built
into a template called Docker images as
you can see on the screen each
application is still isolated and
occupies less space now let's have a
look at the significant differences
between Docker and virtual machine let's
start with the operating system first
Docker is a container based model where
containers are software packages used
for executing an app application on any
operating
system on the other side virtual machine
is a container based model it utilizes
user space along with the kernel space
of the operating system a kernel space
is where the core of the operating
system runs and provides its services to
the user a user cannot modify this space
and user space is the portion of the
system memory in which the users
processes run Docker containers share
operating system kernels with other
applications and result in a higher
server efficiency hence Docker provides
the most substantial default isolation
capabilities among the other
configuration tools on the other hand
virtual machines do not share the
operating system also it does not
provide isolation in the host kernel in
Docker multiple workloads can run on a
single operating system but in virtual
machine each workload needs a complete
operating system or a hypervisor the
next significant difference between the
two is the performance in the case of
Docker they use the same operating
system without any additional software
like hypervisor in case of virtual
machines performance issues are a major
problem it can be due to the several
reasons like CP constraints memory
allocation Network latency and many more
running multiple virtual machines lead
to an unstable performance Docker
containers can start up quickly and
result in less put up time whereas
virtual machines do not start quickly
and lead to a PO performance now let's
talk about the next difference which is
portability with Docker containers a
developer can build an application and
store it into a Docker image later he or
she can run it across any host
environment but when it comes to Virtual
machines it has portability issues
virtual machines do not have a central
Hub like Docker does and it requires
more memory space to store data Docker
containers are smaller than virtual
machines due to which the process of
transferring files on the host file
system is easier on the other hand
dependency on the host operating system
and Hardware makes virtual machines less
portable while transferring files
virtual machines should have a copy of
the operating system
and its dependencies due to which image
size is increased and becomes a tedious
process to share
data the boot of time between the both
is very different the application in
Docker containers start without any
delay since the operating system is
already up and running on the other hand
virtual machines take much longer time
than it takes for a container to run
applications these containers were
basically designed to save time in the
deployment process of an application
whereas in Virtual machines to deploy a
single application virtual machine needs
to start an entire operating system
which would result in a full boot
process so those were the major
differences between Docker and a virtual
machine now let's discuss the minor
differences between
them Docker is not always in a running
state it stops when the stop command is
executed whereas the ver machines are
always running in the background which
will result in huge RAM consumption
talking about snapshots Docker has a lot
of snapshots a snapshot is an image that
you can upload on a private repository
to access it on another host but virtual
machines do not consist of any snapshots
in Docker images can be version
controlled just like git they have local
registry called dockerhub where users
store and distribute container images on
the other hand hand virtual machine does
not have a central Hub they are not
version controlled Docker can run
multiple containers on a system users
can connect multiple containers using
userdefined networks and shared volumes
multiple containers can be accessed on
the same machine and share the operating
system kernel with other containers each
container is isolated in user space Also
Docker containers occupy less space than
virtual machines and start
instantly virtual machine can run only a
limited number of virtual machines on a
system since each virtual machine
requires a certain amount of CPU RAM
memory and other resources your physical
systems will have less space multiple
containers can be started at a time on
the doer engine the isolation
environment allows a user to run and
deploy several containers simultaneously
on a given host virtual machines can run
only a limit limited number of virtual
machines on a system since each virtual
machine requires certain amount of CPU
RAM memory and other resources your
physical systems will have less space
now that I have told you the differences
between Docker containers and virtual
machines let me show you a real life
case study of how BCC uses Docker BCC
news is a British News Channel over 500
developers working across the globe BCC
news delivers broadcast in almost 30
different languages and with over
880,000 Daily News in English alone the
news channel ran more than 26,000 jobs
with more than 10 continuous
Integrations with sequential scheduling
the company had issues with identifying
a way to unify the coding process and
monitor the continuous integration
consistently also the existing jobs took
up to more time that is up to 60 Minutes
to schedule and perform its task docker
allowed BCC news to eliminate job weit
times and run jobs in parallel it also
gave the users the ability to work in a
more flexible CI environment where
entire code processes were unified and
stored in a single place however Docker
succeeded in spreading up the whole
continuous integration process so let's
go through and you're going to be asked
to explain what the architecture of
Docker is and Docker really is the most
popular containerization environment so
Docker uses a client server architecture
and the docker client is a service which
runs in a command line and and then the
docker Damon which is run as a rest API
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an execu package of
applications and its dependencies
together and then finally the docker
registry is a service to host and
distribute Docker images among other
users so you also be asked to provide
what are the advantages of Docker over
virtual machine and and this is
something that comes up very
consistently in fact um you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a Docker
or Docker like environment and really
the the arguments for Docker are just
absolutely fantastic you know first of
all ER does contain an occupy Docker
containers occupy significantly less
space than a virtual machine or a
dedicated machine the boot up time on
Docker is significantly faster than a VM
containers have a much better
performance as they are hosted in a
single Docker image Docker is highly
efficient and very easy to scale
particularly when you start working with
kubernetes easily portable across
multiple platforms and then finally for
space allocation docko data volumes can
be shared and reused among on multiple
containers the argument against virtual
machines is significant and particularly
if you're going into an older
environment where a company is still
using actual dedicated hardware and
haven't moved to a cloud or cloud-like
environment your Arguments for Docker
are going to be very very persuasive be
very clear on what the advantages are
for Docker over a virtual machine
because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to work with the tools
like Docker you need to be able to
effectively share with that team what
the benefits are so how do we share
Docker containers with different nodes
and in this instance what you want to be
able to do is Leverage The Power of
Docker swarm so Docker swarm is a tool
which allows the it administrators and
developers to create and manage clusters
of swarm nodes within the darker
platform and there are two elements to
the node there's the manager node and
then there's the the worker node the
manager node as you would assume manages
the entire infrastructure and the worker
node is actually the work of the agent
as it gets executed so what are the
commands to create a Docker swarm and so
here we have an example of what a
manager node would look like and once
you've created a swarm on your manager
node you can now add worker nodes to
that swarm and again when you're
stepping through this process be very
precise in the execution part that needs
to be taken to be able to effectively
create a swarm so start with manager
node and then you create a worker node
and then finally when a node is
initialized as a manag node it can
immediately create a token and that
token is used for the worker nodes and
associating the IP address with the
worker nodes question 17 how to run
multiple containers using a single
service it is possible to run multiple
containers at single service by using
Docker compose and docker compos will
actually run each of the services in
isolation so that they can interact with
each other the language used to write
out the composed files that allow you to
run the service is called yaml and yaml
stands for yet another markup language
so what is the use of a Docker file so
Docker file actually is used for
creating Docker images using the build
command so let's go through and show on
the screen what that would look like and
this would be an opportunity where if
you're actually in a technical interview
you could potentially even ask hey can I
draw on a whiteboard and show you what
the architecture for using the build
command would look like and what the
process would look like um again when
you're going through an interview
process as someone who interviews a lot
of people one of the things I really
like is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to the
Whiteboard and actually show what can
actually be done through actually
creating images on the Whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as an architect this
should be something that you are
comfortable doing and by doing it in the
interview and certainly you want to ask
permission before you actually do it but
doing this in the interview really helps
demonstrate your comfortable feelings of
working with these kind of architecture
drawings so back to the question of
creating a Docker file so we go through
and that we have a Docker file that then
goes ahead and creates the docker image
which then in turns creates the docker
container and then we are able to push
that out up to a Docker Hub and then
share that Docker Hub with everybody
else as part of the docker registry with
the whole network so what are the
differences between Docker image and
Docker containers so let's go through
the docker image so the docker images
are templates of a Docker container an
image is built using a Docker file and
it stores that Docker file in a Docker
repository or a Docker Hub and um and
you can use Docker Hub as an example and
the image layer is a readon file system
the docker container is a collection of
the runtime instances of a Docker image
and the containers are created using
Docker images and they are stored in the
docker Damon and every container is a
layer is a readr file system so you
can't replace the information you can
only append to it so while you can
actually use yaml for writing your so a
question you're going to be asked is
instead of yaml what can be an alternate
file to build Docker compose so yaml is
the one that is the default but you can
also use Json so if you are comfortable
working with Json and my that is
something that you should be get
comfortable with is you want to be able
to use that to name your files and as a
frame of reference Jason is a logical
way of being able to do value paired
matching using a JavaScript like syntax
so you're going to be asked to how to
create a Docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a MySQL Docker
container so to do that you want to be
able to build a Docker image or pull
from an existing Docker image from a
Docker repository or Hub and then you
want to be able to then use Docker to
create a new container which has MySQL
from the existing Docker image
simultaneously the layer of read write
file system is also created on top of
that image and Below at the bottom of
the screen we have what the commands
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have a Docker
registry is an open source server siiz
service used for hosting and
distributing Docker images whereas in
contrast for repositories a collection
of multiple versions of a Docker image
in a registry a user can distinguish
between docket images with their tag
names and then finally on the reg stre
Docker also has its own default registry
called Docker hub for the repository it
is a collection of multiple versions of
Docker images it is stored in a Docker
registry and it has two tbs a public and
private registry so you can actually
create your own Enterprise registry so
you're going to be asked you know what
other Cloud platforms that uh support
Docker really you know list them all and
we have listed here Amazon web services
Microsoft Azure Google Cloud Rackspace
but you could add in their IBM blue mix
could put in Red Hat really any of the
cloud service providers out there today
do support Docker it's just become an
industry standard so what is the purpose
of Expose and publish commands in Docker
so if we go through expose is an
instruction used in Docker file whereas
publish is used in Docker run command
for expose it is used to expose ports
within a Docker Network whereas with
publish is can be used used outside of a
Docker environment for expose it is a
documenting instruction used at the time
of building an image and running a
container whereas with publish it is
used as to map a host port to a running
container port for expose is the command
used in Docker whereas for publish we
use the command- P for when we're doing
our command line used in Docker and
examples of these are exposeed 8080 or
with Docker we would put in or for
publish we would do the examp Docker
run- d-p and then
0.0.0 8080 colon 80 as our command line
if you are looking to get certified into
devops and become a kubernetes and
Docker expert have a look at wide range
of courses on devops by simply learn in
collaboration with topnotch universities
across the globe so why wait enroll the
course now from the link in the
description box we're going to break up
the interview questions into three
categories it's going to be beginner
intermediate and advanced um the
beginner questions are really going to
cover about 3/4s of the questions that
we're going to ask here and then the
intermediate and advanced will finalize
out the will finish out the final
quarter and and the reason is is that
quite typically most interviews um
you'll be asked you know intermediate to
beginner level questions mainly beginner
level questions just so you can
demonstrate your understanding of
kubernetes and then there's always one
Advanced question that gets thrown out
just to make sure that you really do
know what you're talking about and
you've done at least one or two
kubernetes environment uh setups to be
able to validate your experience so
let's kind of jump into that first
section which is you know why is
kubernetes widely used and your answer
to this really comes into the fact that
kubernetes um is widely used because of
three distinct areas one is that uh
kubernetes is an open-source platform
that is readily available for the
management containers and allows you to
manage the deployment scaling and
management of those containers and so uh
the first area is that it allows you for
easy deployment of your containers the
ability to scale effectively within
kubernetes not just vertically but
horizontally for your containers and
then finally the kubernetes environment
itself was really designed to make it
easy to manage containers um in small
parts other key areas of why kubernetes
is popular is that kubernetes is the
single largest open source project out
there uh since its launch in 2014 is
just grown in Leaps and Bounds um around
that project there is just a huge
Community both an online and physical
Community there are just a large number
of online um groups that are supporting
kubernetes and allow you to get fast
access to information but in addition to
that there are also a lot of uh local
meetup groups that cover specifically
kubernetes whether as part of a devops
group or as part of a dedicated
kubernetes group the actual environment
itself is a really robust way of being
able to manage your containers
containers are becoming the most popular
way of being able to deploy cloud-based
applications kubernetes is an effective
and persistent storage environment so
kubernetes is also able to effectively
manage persistent storage whether it's
local storage as a SSD drive or a local
hard drive or whether it's cloud-based
storage and because of the way that
kubernetes is architecture it's really
designed for the cloud from the ground
up so it's a great way to get started um
in building out your Cloud um Solutions
and then finally the actual tools within
kubernetes are designed to help and
manage the health and monitoring of the
environment itself so what is Google
container engine so the Google container
engine is another open-source solution
but it's designed for um managing the
docker containers and clusters and so
kubernetes is a container management
solution and Docker is the most popular
container um Sol solution out there and
this is just a way of being able to
bring the two effectively with into each
other so that you can have Docker
containers manage the application and
then kubernetes manage the scale of your
applications as they're uh vertically
and horizontally scaled across your
network so what is the difference
between kubernetes and Docker and this
is important because kubernetes and
Docker are often dropped in the same uh
sentence and there's a lot of people
that get get confused between the two uh
so fundamentally uh kubernetes is about
uh taking pods which contain containers
which have applications in and deploying
them at scale across um a cloud Network
Docker is the container solution itself
so the actual application itself is put
into the docker container because of
this difference uh kubernetes is
designed for autoscaling is highly
available whereas Docker simply isn't
because he doesn't have to worry about
that within the container itself the
kubernetes environment will check for
livess and Readiness of the health of
the entire infrastructure where as with
Docker it's just really going to check
the services that are coming in and out
of the actual application itself within
the container kubernetes is complicated
to setup it really is just simply
complicated to setup and what you'll
find is a lot of cloud services will
actually do the setup for you whereas
Docker is very easy to set up the as you
would expect with the tolerance ratio
there is high fault tolerance with the
kubernetes whereas with Dockers low
fault tolerance and again this is
fundamental in the two different types
of strategy that KU kues and Docker have
they're both connected tightly with each
other but they both do different things
so what are the notable features of
kubernetes um one is the ability to do
container bands kubernetes always knows
where to place a container within its
entire network within the pods and with
it infrastructure the services are
managed for Security Network and storage
uh it is self monitoring so it goes in
and is able to check its own nodes for
health and the containers within the
nodes so it make sure that the
applications are always running
effectively you can do the scaling
effectively both vertically and
horizontally within kubernetes the
solution is open source so there's no
actual fee for the actual kubernetes
service itself and kubernetes allows for
storage orchestration what does it mean
by storage orchestration uh storage
orchestration is that you're are able to
um access and share uh either local
storage such as a hard drive or SSD
drive or a cloud-based drive and my
preference is always to go with
cloud-based drive because then you're
really positioning Your solution to be a
true cloud-based application so what are
the main advantages of kubernetes uh
where you get automatic roll back for
changes that go wrong the a you're able
to automate a lot of the processes that
would have been manual processes
previously uh you're able to scale the
resources both vertically and
horizontally easily and quickly and then
the these three areas um the roll bag
automating roll back uh aut automating
services and being able to scale
indirectly allows you to save money by
reducing the amount of time and people
working on your operations environment
so let's break out the architecture of
uh kubernetes so you're going to be
explain asked to explain um how is
kubernetes AR architecture structured
and there are three key areas there's
the application layer there's the
kubernetes layer and then there's the
infrastructure layer we're going to
start off with the infrastructure layer
and so the infrastructure layer is our
base layer and this is where all your
where you make your clusters for your
collections of storage and networking
resources within kubernetes and this is
where you would um have if your
databases are your data stored locally
on a hard drive or in the cloud uh your
core Network and the VMS that You' be
building out your infrastructure on
these allow you to uh group many Sheen
together into a single unit uh so that
you're able to scale up and down very
easily uh to meet customer demand so
let's talk about the middle a which is
the the kubernetes layer now so if we're
looking at our architecture for our
kubernetes layer it's really broken down
for the the core operating system the
docker um container environment and the
kubernetes code itself um and this is
where you're able to take machines and
pull them together as a cluster so that
they work within that kubernetes system
you have a master controller U within
the kubernetes that actually then will
go ahead and manage the schedule the
control manager the API server and then
you'll be able to cluster all of these
kubernetes together so that they're all
working and processing as one single
unit and then finally let's look at the
application layer where you actually
have your apps and so the application
layer and where you have your pool and
your services you know kubernetes will
use API to connect in and out of this
layer it's to run your applications
within containers and you use
instructions that are written in yam or
Json uh to actually uh set up those
containers and you actually have the
environment running on a plan that
examines these uh instructions so it
keeps the entire environment healthy API
ecosystem is used to interact with the
cluster it's so you don't have to uh do
that by yourself so make sure that
you're referencing that the apis are are
secured and connecting in and out of the
system and so you have your schedule and
control manager actually from the
kubernetes environment doing the hard
work and then the workers themselves
execute the commands that coming from
kubernetes so you will be asked to kind
of dig into some of the key areas such
as explaining the master node and
listing its components so the master
node is U comprised of four key
components there's the uh cluster store
the controller manager uh the schedule
and the API server so four key
components remember four fingers four
components and then you're going to be
asked to maybe dig deeper into what is a
cluster within kubernetes and so a
cluster is really combin comination of
resources such as machines into a single
node and then uh those nodes are pulled
together and work as a single cluster as
a whole kubernetes will work with that
whole cluster and then um once you're
actually working with that whole cluster
uh if a node fails or needs to be
removed kubernetes is able to do that
quickly and effectively so let's talk a
little bit about what are the different
types of controllers in kubernetes and
there are five types of control manager
uh you have the node control controller
the service account the token controller
the endpoint controller and the
replication controller and so the no
controller controls and handles the node
in the system the service account
enables access controls in the system
the token controller cleans up any
tokens for non-existant service accounts
the endpoint controller joins services
in pods to the endpoint objects and the
replication controller manages the Pod
life cycle so what is the role of the
cloud controller manager within this
environment and so so the role of the
cloud control manager is really to help
manage persistent storage and so
persistent storage uh is storage that's
either local storage to the physical
machine such as an SSD or hard drive or
cloud-based storage and this allows the
storage to be shared across the entire
network what is the role of the cube API
server and the cube scheduler so the
cube API server is responsible for the
establishment of communication between
the kuber netti nodes and the master
components and and it does this through
the through apis the apis themselves are
all commands that are rest based
commands and it implements an interface
which means different tools and
libraries can communicate effectively
with each other so what is the role of
the cube API server and the cube
scheduler so the cube scheder actually
does the distribution of work across the
entire infrastructure so the actual um
yaml scripts that you've written are
executed appropriately at the right time
so you're going to be asked to explain
persistent volumes in kubernetes so a
persistent volume is just a place where
you can store data again this can be
local data as in a hard drive local to
the physical machine or it can be
cloudbased data so what is a cube blate
and a cube proxy so a CU blate is a
service responsible for conveying
information to and from the control plan
itself the process and the C proxy
itself is responsible for maintaining
the work and the standards within the no
server in addition uh every node in the
cluster runs a simple Network proxy and
Q proxy routes request to the correct
container in that node in addition
you'll have some basic and primitive
load balancing and management of the
node from the proxy itself and there is
additional layers of management that get
done outside of the proxy but the proxy
is also there to help do some localized
load management so what is Cube CTL so
Cube CTL is your command line or
terminal window interface that allows
you to execute commands and later uh in
this video we'll actually go through
some of those commands etcd is the store
that configures the details and
essential values such as key value pairs
that are distributed across your entire
network it also manages the network
rules and the post forwarding of
activity so what are the disadvantages
of kubernetes well the first is it's
really hard to install and configure and
this is why Azure and many cloud
services such as Google Cloud will
actually do the installation and
configuration of kubernetes for you it
does take a long time to get up and
running with kubernetes not so much for
the actual software but your own
experience of being able to use
kubernetes it is not simple to manage
the service you actually have to have a
lot of experience um and this is why
people get paid a lot for kubernetes
skills it's a difficult environment to
get up and running and be effective in
that management again kubernetes is just
not an easy platform to learn it takes a
lot of time and effort and um one of the
disadvantages is that while kubernetes
itself is free because it is an
open-source project getting the people
who are skilled to using kubernetes is
expensive all right so those were kind
of the introductory um questions that
you can expect to be asked in your
interview the next set of questions are
really going to be on an intermediate
and advanced level and these are the
questions that will be thrown out you
know you see these as being the the
trick questions which are really there
to test that you have the skill set to
be able to implement and manage
kubernetes within a company so what
happens if a master node fails what
happens if a worker node fails how would
kubernetes manage that well kubernetes
is really designed for this kind of
scenario and so you can actually look at
it and the response would be is that the
PO itself does not get affected and
kubernetes will actually respond and fix
the uh the failed node whether it is uh
needs to be be repaired whether it needs
to be restarted or it needs to be
replaced so what is a service role in
kubernetes so a service role is an
abstraction for the pods it provides a
virtual IP address and allows clients to
connect to Containers running in the
pods using that virtual IP address and
the command that you would use to get
that we would use some of these Cube CTL
commands is quite a simple one it's Cube
CTL get services and you get a full list
then of all the services that are
available so how do I do a ro back in
deployment uh so there's a couple of
ways you can do the roll back and the
first is if you want to see the roll out
history deployment you would write Cube
C roll out history deployment and then
in square brackets deployment if you
want to just restore to the previous
roll out the command would be Cube CTL
roll out undo deployment and then in
square brackets deployment and that
would take you back to the previous uh
deployment so what is an Ingress
controller so an Ingress controller
really a is a pod that manages that
inbound traffic and a prominent feature
of that traffic is that it is secured
with an SSL termination and so what
we're going to do to now address some of
the key ways in which you can provide
API Securities on kubernetes and we have
eight of those uh so the first four are
implementing TL TLS for your security
the second would be API authentication
the third is to make the cubus um
protection the cubus um protected via
authorization mode equals web hook
command the fourth is to monitor rbac
failures the fifth is to remove those
default service account permissions
sixth would be to filter egress to Cloud
API metadata apis the seventh is to use
pod security policy to restrict
container policies and protect the node
and the eighth and final is to keep the
cube at the latest version all right
let's go into some of the more advanced
commands and so the advanced questions
that you're going to get are all going
to be uh commands that you are going to
be asked to write now my advice for you
um for these questions is to make sure
that you have some of these basic
commands memorized um they don't have to
be perfect when you're in the interview
um environment but it's good to be able
to get up to a whiteboard and actually
write out what these commands are just
so you can actually demonstrate that you
know what the commands are the
interviewer isn't going to ask you
questions on how would you uh apply
kubernetes to the company that you're
being hired for because you simply don't
have that knowledge but they do want to
be able to see clearly that you
understand the commands and terminal
window instructions that will be able to
demonstrate how you would actually go in
and access some of that information so
from the first questions you might be
asked is how do you package kuet
applications and so you would use the
package manager within kubernetes to be
able to manage and configure and deploy
applications uh into your cluster and so
here we have the the helm command that
you would use so what are init
containers and init container gets
executed before any other containers can
run on the Pod and so you want to be
able to illustrate what the init uh
instructions would look like and here we
have an example and I would again use
this example and maybe memorize it uh
for when you have this question come up
in your interview so what is the
difference between a config map and a
secret and these are more uh uh
fundamental so a config map uh just has
the instructions that are clearly
available in plain text on what the uh
cube is supposed to do whereas uh the
secrets are actually encrypted and these
are like passwords and other sensitive
data that you don't want to have um
easily available these are encrypted
information have to be decrypted to
access the content so if a node is
Tainted is there a way to schedule the
pods to that node and the answer is no
there isn't and so what you have to do
then is schedule the Pod to be uh fixed
and so that schedule the Pod to fix the
tainted node to allow for tolerations to
that and we have here the example code
that you would use to do that so how do
you deploy a feature with zero downtime
in kubernetes well kubernetes is
designed to be able to roll out um
applications with zero downtime and here
we have four examples using the rolling
update as a strategy for zero downtime
deployments in cuberes again memorize
one of these so you can actually
illustrate how you use roll out to be
able to deploy a zero downtime solution
so how do you monitor a pod that is
always running and so to do that you
would use the liveness probe to check um
the health of the application within the
Pod and see whether or not U there are
any failures or whether or not the pod
itself needs to be restarted so how do
you drain traffic from a pod during a
maintenance so again um this is a simple
command for you to remember um but to be
able to drain an application so that
it's easy for you to be able to do
maintenance on the application you would
use the cube CTL drain and then in
square brackets the node name um and
that would then uh temporarily drain uh
that um particular pod so you can
actually do maintenance on it what you
have to then and this would probably
give you bonus points if you're in an
interview is then uh reference the uncon
command so that you can uncon the actual
uh node itself so that uh the PO itself
can come back up into life um and work
as a fully functioning solution so to be
able to tie services to a pod or to a
set of PODS is that you can apply labels
to the actual pods themselves and then
use the labels as selectors to be able
to glue services to that pod and again
we have here an example of how you'd
actually do that and uh again this is
one of those where you can probably
won't be asked to uh write this out but
it'd be good to have this memorized how
would you get all pods on a node and so
the following command will actually go
ahead and bring all the pods into a
kubernetes cluster um this is quite
lengthy and you know maybe you want to
like talk to how this would be done U
but we have the instructions and how to
do that right here the important part is
to make sure that you are when you're
talking about the node name that you
reference to whoever you're interviewing
that you would what you would switch out
to U reference the actual nodes that
you're trying to pull together into the
kubernetes cluster so how do pods Mount
NFS volumes and so in these examples
here we actually Define the NFS server
and then the Pod and how You' actually
Mount two together again what you want
to be able to do is illustrate why you
would use yaml and um how you'd actually
then point to specific IP addresses with
within your kubernetes environment
within either the production or test
environment to be able to illustrate how
you would Mount out those NFS volumes so
here we wrap up cuberes and Docker
course for beginners if you like this
video consider subscribing to Simply
learn to stay updated with devops
technology if you have any queries feel
free to post them in the comment section
below and our experts will be happy to
assess you till then keep watching keep
learning staying ahead in your career
requir IR Ires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations
and delivered by industry experts choose
any of our programs and set yourself on
the path to Career Success click the
link in the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here