1
00:00:00,030 --> 00:00:02,220
so hello there and welcome to another

2
00:00:02,220 --> 00:00:04,620
tutorial my name is Tammy Bakshi and

3
00:00:04,620 --> 00:00:06,480
today we're going to be going over how

4
00:00:06,480 --> 00:00:08,790
you can do near real-time image

5
00:00:08,790 --> 00:00:11,190
stylization using a convolutional neural

6
00:00:11,190 --> 00:00:13,590
network on actually using Cora Mel

7
00:00:13,590 --> 00:00:16,320
on the new iPhone 10s using it's a

8
00:00:16,320 --> 00:00:18,600
twelve Bionic chip there have been many

9
00:00:18,600 --> 00:00:21,000
great updates in fact in order to demo

10
00:00:21,000 --> 00:00:22,350
this near-real-time

11
00:00:22,350 --> 00:00:24,810
invalidation I've got my sister as a

12
00:00:24,810 --> 00:00:26,369
test subject and you'll be seeing more

13
00:00:26,369 --> 00:00:29,250
of her later as well hi my name's Tammy

14
00:00:29,250 --> 00:00:31,050
and I'm so excited to we wonder if that

15
00:00:31,050 --> 00:00:33,000
means video showing the new iPhone

16
00:00:33,000 --> 00:00:35,610
Dennis's neural engines thank you very

17
00:00:35,610 --> 00:00:37,800
much this is gonna be great so now first

18
00:00:37,800 --> 00:00:39,600
of all what's so different about the new

19
00:00:39,600 --> 00:00:42,120
iPhone tennis the new iphone tennis is

20
00:00:42,120 --> 00:00:44,129
great because it has a lot of new

21
00:00:44,129 --> 00:00:46,260
performance enhancements well it might

22
00:00:46,260 --> 00:00:47,670
not look all that different from the

23
00:00:47,670 --> 00:00:50,460
inside it's completely architected of

24
00:00:50,460 --> 00:00:52,649
course previously with the iPhone 8 and

25
00:00:52,649 --> 00:00:54,690
iPhone 10 there was a special kind of

26
00:00:54,690 --> 00:00:56,910
ASIC that Apple implemented called

27
00:00:56,910 --> 00:00:59,070
neural engine or all I need is a 2 quart

28
00:00:59,070 --> 00:01:01,500
chip dedicated to all neural network

29
00:01:01,500 --> 00:01:03,629
processing on the iPhone that includes

30
00:01:03,629 --> 00:01:05,220
simple video compression when it comes

31
00:01:05,220 --> 00:01:07,560
to 4k video but it also includes things

32
00:01:07,560 --> 00:01:09,720
like face ID and other kinds of

33
00:01:09,720 --> 00:01:11,670
authentication and other kinds of Syrian

34
00:01:11,670 --> 00:01:13,350
machine learning work that goes on on

35
00:01:13,350 --> 00:01:16,770
your device however this year what Apple

36
00:01:16,770 --> 00:01:18,509
has done with the new iPhone 10s is

37
00:01:18,509 --> 00:01:20,189
they've taken that neural engine and

38
00:01:20,189 --> 00:01:23,030
upgraded it from 2 cores to 8 cores

39
00:01:23,030 --> 00:01:26,549
enabling it to go from 600 and 600

40
00:01:26,549 --> 00:01:28,409
billion of neural network operations per

41
00:01:28,409 --> 00:01:31,350
second on the iPhone 10 to nowadays you

42
00:01:31,350 --> 00:01:33,659
can run five trillion neural network

43
00:01:33,659 --> 00:01:36,030
operations per second on the iPhone

44
00:01:36,030 --> 00:01:38,130
techs it's really amazing what these

45
00:01:38,130 --> 00:01:39,840
performance enhancements can do of

46
00:01:39,840 --> 00:01:41,850
course there's also an extra byte of RAM

47
00:01:41,850 --> 00:01:43,740
which were some applications could be

48
00:01:43,740 --> 00:01:45,299
useful however of course if you're

49
00:01:45,299 --> 00:01:46,680
optimizing your memory usage you

50
00:01:46,680 --> 00:01:48,630
shouldn't ever need that much around for

51
00:01:48,630 --> 00:01:50,430
your application but apart from that

52
00:01:50,430 --> 00:01:53,189
there's also an allow for a lot of much

53
00:01:53,189 --> 00:01:55,409
much more intensive machine learning

54
00:01:55,409 --> 00:01:57,540
work best part is that the first time

55
00:01:57,540 --> 00:01:59,939
ever developers can actually access the

56
00:01:59,939 --> 00:02:02,549
neural engine through core ml previously

57
00:02:02,549 --> 00:02:04,500
Cornell would either run on the CPU or

58
00:02:04,500 --> 00:02:07,140
GPU but now you can actually run your

59
00:02:07,140 --> 00:02:09,629
core ml predictions directly on the

60
00:02:09,629 --> 00:02:11,370
neural engine for maximum

61
00:02:11,370 --> 00:02:13,890
and the best part is that even before

62
00:02:13,890 --> 00:02:16,830
your neural network operations ever hit

63
00:02:16,830 --> 00:02:19,050
the CPU the GP or the neural engine

64
00:02:19,050 --> 00:02:21,360
there's another custom chip developed by

65
00:02:21,360 --> 00:02:23,610
Apple that's all about actually routing

66
00:02:23,610 --> 00:02:25,920
specific operations to the correct

67
00:02:25,920 --> 00:02:28,230
computer the correct chip and so if a

68
00:02:28,230 --> 00:02:29,700
certain neural network operation is

69
00:02:29,700 --> 00:02:31,410
better to be done by other neural engine

70
00:02:31,410 --> 00:02:33,030
it will actually redirect back to the

71
00:02:33,030 --> 00:02:35,129
neural engine whereas some operations

72
00:02:35,129 --> 00:02:37,560
might actually be better done on the CPU

73
00:02:37,560 --> 00:02:39,780
or the GPU that's really great what this

74
00:02:39,780 --> 00:02:41,910
performance enhancing can do and that's

75
00:02:41,910 --> 00:02:44,750
why now you can do almost real-time

76
00:02:44,750 --> 00:02:48,269
image stylisation on the new iPhone 10s

77
00:02:48,269 --> 00:02:50,040
and that's the denim that I'd love to

78
00:02:50,040 --> 00:02:52,290
show you today as to how you can take a

79
00:02:52,290 --> 00:02:54,209
pre trained convolutional neural network

80
00:02:54,209 --> 00:02:56,220
more specifically a neural network

81
00:02:56,220 --> 00:02:58,110
trained to style images in a mosaic

82
00:02:58,110 --> 00:03:01,319
format and actually use up on the 10s

83
00:03:01,319 --> 00:03:03,930
live with a camera feed coming in from

84
00:03:03,930 --> 00:03:06,360
the avfoundation library and actually

85
00:03:06,360 --> 00:03:07,620
stylize it with the neural network

86
00:03:07,620 --> 00:03:10,799
display on screen as it's happening from

87
00:03:10,799 --> 00:03:13,140
there the walls will compare that to the

88
00:03:13,140 --> 00:03:15,090
iPhone 10 take a look at its performance

89
00:03:15,090 --> 00:03:18,060
and see how it compares and see if my

90
00:03:18,060 --> 00:03:20,190
sisters actually able to detect the

91
00:03:20,190 --> 00:03:23,310
difference and predict which video was

92
00:03:23,310 --> 00:03:25,650
shot by which phone based off the

93
00:03:25,650 --> 00:03:28,859
performance of the videos and based off

94
00:03:28,859 --> 00:03:31,319
of how smooth that style of stylization

95
00:03:31,319 --> 00:03:34,019
actually is of course in some ways there

96
00:03:34,019 --> 00:03:36,120
are more efficient methods than which

97
00:03:36,120 --> 00:03:37,739
you could do this you could record an

98
00:03:37,739 --> 00:03:40,260
actual 60 frames per second video feed

99
00:03:40,260 --> 00:03:41,790
that into the Swift program and actually

100
00:03:41,790 --> 00:03:43,349
predict on all those trains at once

101
00:03:43,349 --> 00:03:45,150
therefore using the neural and into its

102
00:03:45,150 --> 00:03:47,190
maximum capacity and getting it done in

103
00:03:47,190 --> 00:03:49,769
much better performance however there

104
00:03:49,769 --> 00:03:51,269
are also some cases where you would want

105
00:03:51,269 --> 00:03:53,549
to do it in near real-time when Apple

106
00:03:53,549 --> 00:03:55,470
actually opens up the neural engine more

107
00:03:55,470 --> 00:03:57,209
through core ml and documents and a lot

108
00:03:57,209 --> 00:03:59,130
more performance will be enhanced and of

109
00:03:59,130 --> 00:04:00,359
course you can also go ahead and

110
00:04:00,359 --> 00:04:02,819
quantize or prune weights from the

111
00:04:02,819 --> 00:04:04,680
neural network itself however for now

112
00:04:04,680 --> 00:04:06,510
let's just take a vanilla convolutional

113
00:04:06,510 --> 00:04:08,220
neural network and take a look at how

114
00:04:08,220 --> 00:04:10,139
the devil works now though let's take a

115
00:04:10,139 --> 00:04:12,870
look at a demo of how smooth you can get

116
00:04:12,870 --> 00:04:14,370
to this new real-time image

117
00:04:14,370 --> 00:04:17,039
stabilization by taking a look at how my

118
00:04:17,039 --> 00:04:19,500
phone is able to stylize a video of my

119
00:04:19,500 --> 00:04:21,930
sister in real time all right so now

120
00:04:21,930 --> 00:04:23,610
let's start up the demo and take a look

121
00:04:23,610 --> 00:04:25,380
at how it works so as you can see I've

122
00:04:25,380 --> 00:04:25,620
got

123
00:04:25,620 --> 00:04:27,060
an application on my screen over here

124
00:04:27,060 --> 00:04:29,460
called real-time style transfer I'm

125
00:04:29,460 --> 00:04:30,900
gonna click on it and immediately it

126
00:04:30,900 --> 00:04:33,389
starts live-streaming the camera and as

127
00:04:33,389 --> 00:04:35,250
you can see my sister is being in

128
00:04:35,250 --> 00:04:38,160
real-time style as a psychic and showed

129
00:04:38,160 --> 00:04:40,889
on my screen now if you just go ahead

130
00:04:40,889 --> 00:04:43,740
and wave for me perfect thank you as you

131
00:04:43,740 --> 00:04:46,139
can see while it's not in perfect

132
00:04:46,139 --> 00:04:49,080
real-time it's almost in real time and

133
00:04:49,080 --> 00:04:50,850
one of these optimizations occur when

134
00:04:50,850 --> 00:04:52,199
you go ahead and make the neural network

135
00:04:52,199 --> 00:04:53,880
more efficient you will be able to get

136
00:04:53,880 --> 00:04:57,060
to nearly real-time with this now we're

137
00:04:57,060 --> 00:04:58,530
gonna do though is we're gonna take a

138
00:04:58,530 --> 00:05:00,750
look at how the iPhone 10 handles this

139
00:05:00,750 --> 00:05:02,699
exact same situation using the exact

140
00:05:02,699 --> 00:05:04,440
same neural network and the exact same

141
00:05:04,440 --> 00:05:06,120
parameters we're gonna take a look at

142
00:05:06,120 --> 00:05:08,370
than iPhone 10 actually holds up to the

143
00:05:08,370 --> 00:05:10,199
new iPhone 10s and see which one is

144
00:05:10,199 --> 00:05:11,910
smoother and whether it's a noticeable

145
00:05:11,910 --> 00:05:14,100
difference so now what you're seeing on

146
00:05:14,100 --> 00:05:15,960
your screen right now towards the left

147
00:05:15,960 --> 00:05:18,449
is the iPhone 10 and the exact same

148
00:05:18,449 --> 00:05:19,770
application running on it

149
00:05:19,770 --> 00:05:21,090
towards the right you're seeing the

150
00:05:21,090 --> 00:05:23,280
iPhone 10 s of course the new one's

151
00:05:23,280 --> 00:05:25,380
powered by the 812 bionics the previous

152
00:05:25,380 --> 00:05:27,900
ones powered by the a 11 Bionic and so

153
00:05:27,900 --> 00:05:29,639
now my sister is going to go ahead and

154
00:05:29,639 --> 00:05:33,150
wave and as you can see you can

155
00:05:33,150 --> 00:05:35,370
immediately tell the difference in how

156
00:05:35,370 --> 00:05:37,349
smooth the new iPhone 10 s is in

157
00:05:37,349 --> 00:05:39,660
comparison to how choppy the older

158
00:05:39,660 --> 00:05:42,210
iPhone 10 is the iPhone 10 itself was a

159
00:05:42,210 --> 00:05:45,150
huge leap in technology for its time but

160
00:05:45,150 --> 00:05:46,889
they do - tennis is much much faster

161
00:05:46,889 --> 00:05:49,530
even than that now my sister does not

162
00:05:49,530 --> 00:05:52,440
know which side is which iPhone they're

163
00:05:52,440 --> 00:05:54,180
both the same size they're both the same

164
00:05:54,180 --> 00:05:56,370
parameters so what I'm gonna guy can do

165
00:05:56,370 --> 00:05:58,680
is show her both the videos and let's

166
00:05:58,680 --> 00:06:00,599
get her opinion on which one she thinks

167
00:06:00,599 --> 00:06:03,120
is the new iPhone 10s and see if it's

168
00:06:03,120 --> 00:06:05,099
actually a noticeable difference in

169
00:06:05,099 --> 00:06:08,160
quality or in smoothness let's take a

170
00:06:08,160 --> 00:06:10,310
look

171
00:06:16,680 --> 00:06:19,270
so which of these two clips do you think

172
00:06:19,270 --> 00:06:22,060
is recorded on the new iPhone tax well

173
00:06:22,060 --> 00:06:24,580
that's pretty obvious it was this one on

174
00:06:24,580 --> 00:06:26,319
the right I think that's right so you

175
00:06:26,319 --> 00:06:28,990
can tell it's much much smoother has a

176
00:06:28,990 --> 00:06:30,400
much much higher frames per second

177
00:06:30,400 --> 00:06:32,650
account at the same time you can tell

178
00:06:32,650 --> 00:06:35,280
that levels recorded with a new chip

179
00:06:35,280 --> 00:06:38,710
question for you didn't iPhone 10 come

180
00:06:38,710 --> 00:06:40,210
out last year how come there's such a

181
00:06:40,210 --> 00:06:42,310
big difference between the two it did

182
00:06:42,310 --> 00:06:44,620
come out last year but first of all that

183
00:06:44,620 --> 00:06:45,940
was the first iteration of the knurl

184
00:06:45,940 --> 00:06:47,590
engine that they had very powerful at

185
00:06:47,590 --> 00:06:50,259
the time but the new iPhone 10s has an

186
00:06:50,259 --> 00:06:51,759
even more powerful neural engine that

187
00:06:51,759 --> 00:06:54,520
can now handle many many more operations

188
00:06:54,520 --> 00:06:56,080
than the previous iPhone 10 couldn't

189
00:06:56,080 --> 00:06:57,909
there for such a big difference in

190
00:06:57,909 --> 00:07:00,039
real-time performance so that one's more

191
00:07:00,039 --> 00:07:02,620
smooth than Mac's wonder absolutely in a

192
00:07:02,620 --> 00:07:05,139
year yes in one year Apple went from

193
00:07:05,139 --> 00:07:07,419
being able to do really fast neural

194
00:07:07,419 --> 00:07:08,889
network operations to being able to do

195
00:07:08,889 --> 00:07:11,949
almost real-time image stabilization and

196
00:07:11,949 --> 00:07:13,840
one more question I used so many filters

197
00:07:13,840 --> 00:07:16,090
on social media such as snapchat and

198
00:07:16,090 --> 00:07:18,610
Instagram is this similar to that it

199
00:07:18,610 --> 00:07:21,039
looks like it it is pretty similar yes

200
00:07:21,039 --> 00:07:21,969
but those use a slightly different

201
00:07:21,969 --> 00:07:24,280
technology at least with most of the

202
00:07:24,280 --> 00:07:25,599
filters this one's actually using a

203
00:07:25,599 --> 00:07:27,340
convolutional neural network to extract

204
00:07:27,340 --> 00:07:28,840
styles from the weights of the neural

205
00:07:28,840 --> 00:07:30,610
network of certain images and to apply

206
00:07:30,610 --> 00:07:33,280
those weights to other images and

207
00:07:33,280 --> 00:07:34,270
especially because of the wonderful

208
00:07:34,270 --> 00:07:36,039
libraries that we're using like core ml

209
00:07:36,039 --> 00:07:37,930
we're not actually ruining the aspect

210
00:07:37,930 --> 00:07:39,880
ratio of the image at the same time and

211
00:07:39,880 --> 00:07:43,210
preserving that and so it's pretty

212
00:07:43,210 --> 00:07:45,370
similar to what you'd use on snapchat or

213
00:07:45,370 --> 00:07:47,080
wherever else social media platform you

214
00:07:47,080 --> 00:07:49,000
want to use but it uses a different

215
00:07:49,000 --> 00:07:51,430
technology and this would be more useful

216
00:07:51,430 --> 00:07:53,530
if I had to put up a video and not a

217
00:07:53,530 --> 00:07:55,750
patron exactly more specifically not

218
00:07:55,750 --> 00:07:57,759
necessarily the technique but the new

219
00:07:57,759 --> 00:07:59,050
iPhone is gonna be much faster

220
00:07:59,050 --> 00:08:01,810
processing larger sets of data I mean as

221
00:08:01,810 --> 00:08:03,159
I mentioned before if you wanted to do

222
00:08:03,159 --> 00:08:04,479
this more efficiently you could take an

223
00:08:04,479 --> 00:08:06,130
entire video and send up all the frames

224
00:08:06,130 --> 00:08:07,990
at once to the neural engine to maximize

225
00:08:07,990 --> 00:08:10,150
its performance or you could just do

226
00:08:10,150 --> 00:08:11,469
what we're doing here and do it in real

227
00:08:11,469 --> 00:08:13,539
time which of course really highlights

228
00:08:13,539 --> 00:08:15,759
the fastness of the single image

229
00:08:15,759 --> 00:08:18,550
performance of the new iPhone techs and

230
00:08:18,550 --> 00:08:20,409
that would be visible in a video form

231
00:08:20,409 --> 00:08:21,210
not a pigeon

232
00:08:21,210 --> 00:08:25,580
exactly that makes Thank You of course

233
00:08:25,580 --> 00:08:27,990
all right so now let's take a look at

234
00:08:27,990 --> 00:08:29,130
the code that goes behind the

235
00:08:29,130 --> 00:08:30,960
application like how it is actually

236
00:08:30,960 --> 00:08:32,490
really really simple is essentially

237
00:08:32,490 --> 00:08:35,190
taking images from a me Foundation or

238
00:08:35,190 --> 00:08:37,169
specifically from the back camera device

239
00:08:37,169 --> 00:08:39,870
it's feeding it into the ML model and

240
00:08:39,870 --> 00:08:41,459
getting the output and displaying that

241
00:08:41,459 --> 00:08:44,070
as soon as it can on the UI image view

242
00:08:44,070 --> 00:08:46,050
let's take a look at how it works first

243
00:08:46,050 --> 00:08:48,149
of all the storyboard is again very

244
00:08:48,149 --> 00:08:50,279
simple it's really just one individual

245
00:08:50,279 --> 00:08:52,260
UI element in this case a UI image view

246
00:08:52,260 --> 00:08:55,860
that takes up the whole screen in this

247
00:08:55,860 --> 00:08:57,420
case I've actually expanded that

248
00:08:57,420 --> 00:09:00,089
expanded it to be covered by the knotch

249
00:09:00,089 --> 00:09:02,640
and to expand out through the round

250
00:09:02,640 --> 00:09:04,350
corners in order to make sure that there

251
00:09:04,350 --> 00:09:07,070
isn't this weird space left like this

252
00:09:07,070 --> 00:09:09,480
just to make it take up the whole screen

253
00:09:09,480 --> 00:09:12,240
including the rounded corners and to be

254
00:09:12,240 --> 00:09:13,980
covered by the notch as well just to

255
00:09:13,980 --> 00:09:14,970
make it seem like a more immersive

256
00:09:14,970 --> 00:09:17,790
experience especially to optimize

257
00:09:17,790 --> 00:09:21,630
there's a huge screen of 10s max now

258
00:09:21,630 --> 00:09:23,610
though let's take a look at the code to

259
00:09:23,610 --> 00:09:25,170
begin with let's start off with a view

260
00:09:25,170 --> 00:09:26,579
controller there's three very simple

261
00:09:26,579 --> 00:09:28,079
imports of course we're gonna need UI

262
00:09:28,079 --> 00:09:29,700
kit for the UI view controller and other

263
00:09:29,700 --> 00:09:32,040
sorts of UI elements I would eat the

264
00:09:32,040 --> 00:09:33,959
avfoundation or the audio-video

265
00:09:33,959 --> 00:09:35,640
foundation for all the sorts of

266
00:09:35,640 --> 00:09:37,350
recording that's gonna be done by this

267
00:09:37,350 --> 00:09:39,209
application and of course we're gonna

268
00:09:39,209 --> 00:09:41,130
need core ml to do all of the different

269
00:09:41,130 --> 00:09:43,470
predictions now in this case of course

270
00:09:43,470 --> 00:09:44,880
we've just got our class called view

271
00:09:44,880 --> 00:09:48,000
controller that will conform to UI of U

272
00:09:48,000 --> 00:09:51,270
controller and well ap capture video

273
00:09:51,270 --> 00:09:53,220
data output sample buffer delegate

274
00:09:53,220 --> 00:09:55,079
that's a mouthful but essentially what

275
00:09:55,079 --> 00:09:56,910
that means is that the audio-video

276
00:09:56,910 --> 00:09:59,640
foundation library or framework will be

277
00:09:59,640 --> 00:10:02,670
able to actually provide me with with

278
00:10:02,670 --> 00:10:04,470
sample buffers and I'll talk about what

279
00:10:04,470 --> 00:10:06,860
exactly that means in just a moment

280
00:10:06,860 --> 00:10:09,329
before we continue though let's take a

281
00:10:09,329 --> 00:10:10,740
look at the different variables that the

282
00:10:10,740 --> 00:10:12,480
view controller class has first of all

283
00:10:12,480 --> 00:10:15,149
of course the image view this image view

284
00:10:15,149 --> 00:10:17,220
is going to let you display the current

285
00:10:17,220 --> 00:10:19,470
stylized image on-screen it's a UI image

286
00:10:19,470 --> 00:10:22,170
view of course the model that's actually

287
00:10:22,170 --> 00:10:25,079
going to be run predictions against in

288
00:10:25,079 --> 00:10:27,899
this case I'm using the F and s mosaic 1

289
00:10:27,899 --> 00:10:31,290
model so this model is trained by Justin

290
00:10:31,290 --> 00:10:34,050
Johnson his github use frames JC Johnson

291
00:10:34,050 --> 00:10:34,620
and he has a

292
00:10:34,620 --> 00:10:36,930
full repository called fast neural style

293
00:10:36,930 --> 00:10:38,400
there will be a link to it in the

294
00:10:38,400 --> 00:10:39,900
description below it's a wonderful

295
00:10:39,900 --> 00:10:41,130
repository with a lot of different

296
00:10:41,130 --> 00:10:42,510
models you should definitely check it

297
00:10:42,510 --> 00:10:45,480
out this will actually take in an input

298
00:10:45,480 --> 00:10:48,480
as a 720 by 720 by 3 which basically

299
00:10:48,480 --> 00:10:50,760
means a color image and will output

300
00:10:50,760 --> 00:10:53,640
another image in this case again 720 by

301
00:10:53,640 --> 00:10:56,370
720 but this is the stylized image in

302
00:10:56,370 --> 00:10:59,400
the mosaic form if we go back there are

303
00:10:59,400 --> 00:11:01,170
4 more variables here these actually

304
00:11:01,170 --> 00:11:03,030
relate to the way that Aimee foundation

305
00:11:03,030 --> 00:11:04,590
is going to let us actually capture

306
00:11:04,590 --> 00:11:07,680
video data in this case there's just one

307
00:11:07,680 --> 00:11:09,630
AV capture session and this is actually

308
00:11:09,630 --> 00:11:11,130
the session that's going to provide me

309
00:11:11,130 --> 00:11:12,090
with images

310
00:11:12,090 --> 00:11:13,980
there's the ami capture device in this

311
00:11:13,980 --> 00:11:15,870
case the rear camera is the device that

312
00:11:15,870 --> 00:11:19,050
I'm focusing on the device input which

313
00:11:19,050 --> 00:11:20,700
actually handles capturing input from

314
00:11:20,700 --> 00:11:22,860
the device and video data output which

315
00:11:22,860 --> 00:11:25,800
actually handles giving me the output

316
00:11:25,800 --> 00:11:28,170
from the video stream apart from that

317
00:11:28,170 --> 00:11:29,670
there's also one more variable called

318
00:11:29,670 --> 00:11:32,790
latest image and so essentially this AV

319
00:11:32,790 --> 00:11:34,890
capture session will continuously update

320
00:11:34,890 --> 00:11:36,660
latest image with the latest image from

321
00:11:36,660 --> 00:11:38,280
the camera feed of the neural network

322
00:11:38,280 --> 00:11:40,260
can then go ahead and query at whatever

323
00:11:40,260 --> 00:11:43,440
speed that it wants to then in the View

324
00:11:43,440 --> 00:11:44,970
today unction we've actually got a lot

325
00:11:44,970 --> 00:11:47,250
of code but it's pretty simple first of

326
00:11:47,250 --> 00:11:48,990
all I initialize the model by creating a

327
00:11:48,990 --> 00:11:51,900
new FNS mosaic 1 model meaning fast

328
00:11:51,900 --> 00:11:54,990
neural style mosaic 1 model after that I

329
00:11:54,990 --> 00:11:57,480
create a new AV capture session and set

330
00:11:57,480 --> 00:12:00,630
that to myself capture session then all

331
00:12:00,630 --> 00:12:02,910
of this code over here is essentially

332
00:12:02,910 --> 00:12:06,420
all around setting up the the

333
00:12:06,420 --> 00:12:08,370
audio-video foundation library to give

334
00:12:08,370 --> 00:12:11,760
me video output and so essentially what

335
00:12:11,760 --> 00:12:13,860
this means is towards the end over here

336
00:12:13,860 --> 00:12:16,710
you can just take a look I actually set

337
00:12:16,710 --> 00:12:20,400
the delegate for the capture session to

338
00:12:20,400 --> 00:12:22,620
self and what that lets me do and this

339
00:12:22,620 --> 00:12:24,270
is happening over here and what that

340
00:12:24,270 --> 00:12:26,850
lets me do is essentially every single

341
00:12:26,850 --> 00:12:28,590
time the capture session gets a new

342
00:12:28,590 --> 00:12:30,870
frame from the video stream it's going

343
00:12:30,870 --> 00:12:33,270
to call this function over here called

344
00:12:33,270 --> 00:12:35,310
capture output I'll talk about how that

345
00:12:35,310 --> 00:12:38,340
works in just a moment first of all

346
00:12:38,340 --> 00:12:40,050
though once I actually start running

347
00:12:40,050 --> 00:12:41,910
that capture session I then call one of

348
00:12:41,910 --> 00:12:44,520
my own functions called show stylized

349
00:12:44,520 --> 00:12:47,130
we'll get to that again just a second

350
00:12:47,130 --> 00:12:48,370
but first let's take a look

351
00:12:48,370 --> 00:12:50,170
at the capture of the function

352
00:12:50,170 --> 00:12:51,999
essentially the capture output function

353
00:12:51,999 --> 00:12:54,370
is called by the Amy Foundation library

354
00:12:54,370 --> 00:12:56,459
itself more specifically it's inherited

355
00:12:56,459 --> 00:12:59,410
by the AV capture video data output

356
00:12:59,410 --> 00:13:01,990
sample buffer delegate protocol now

357
00:13:01,990 --> 00:13:03,970
essentially what this does is it gives

358
00:13:03,970 --> 00:13:07,839
me the latest CM sample buffer which is

359
00:13:07,839 --> 00:13:09,879
essentially the current image from the

360
00:13:09,879 --> 00:13:12,040
camera stream so I just need to do a

361
00:13:12,040 --> 00:13:13,839
little bit of processing work in order

362
00:13:13,839 --> 00:13:17,410
to convert that sample buffer into a UI

363
00:13:17,410 --> 00:13:19,629
image but then once I do get this UI

364
00:13:19,629 --> 00:13:23,110
image I set self dot latest image to

365
00:13:23,110 --> 00:13:25,420
that UI image and I do this in the main

366
00:13:25,420 --> 00:13:28,329
thread to make sure no there are no

367
00:13:28,329 --> 00:13:31,329
problems with with the threads then what

368
00:13:31,329 --> 00:13:33,220
I do though is of course as this

369
00:13:33,220 --> 00:13:35,800
function is being called I then go ahead

370
00:13:35,800 --> 00:13:38,319
and run the show stylized function the

371
00:13:38,319 --> 00:13:40,600
show stylized function essentially takes

372
00:13:40,600 --> 00:13:44,319
a look at the latest image itself now if

373
00:13:44,319 --> 00:13:46,959
this latest image is not nil because of

374
00:13:46,959 --> 00:13:48,579
course towards the beginning it is nil

375
00:13:48,579 --> 00:13:51,430
over here if it is not nil however it's

376
00:13:51,430 --> 00:13:53,319
then going to go ahead and run that

377
00:13:53,319 --> 00:13:55,509
image through the neural network it's

378
00:13:55,509 --> 00:13:58,329
going to create a new UI image out of

379
00:13:58,329 --> 00:14:00,009
the output of the neural network in

380
00:14:00,009 --> 00:14:02,709
order to actually show that on the image

381
00:14:02,709 --> 00:14:05,379
view of course from there though we also

382
00:14:05,379 --> 00:14:06,819
need to make sure that the stylized

383
00:14:06,819 --> 00:14:08,829
image continuously updates with new

384
00:14:08,829 --> 00:14:11,679
camera frames so I put a little bit of a

385
00:14:11,679 --> 00:14:13,629
delay more specifically a delay of two

386
00:14:13,629 --> 00:14:16,929
thousandth of a second and after that

387
00:14:16,929 --> 00:14:19,209
delay I call this function over again

388
00:14:19,209 --> 00:14:21,069
it's gonna go ahead and show the new

389
00:14:21,069 --> 00:14:24,100
stylized image and then a mm of a second

390
00:14:24,100 --> 00:14:25,870
later it'll go ahead and do that again

391
00:14:25,870 --> 00:14:27,490
now of course you could increase this

392
00:14:27,490 --> 00:14:29,470
delay if you want to there is really no

393
00:14:29,470 --> 00:14:31,660
need to and the only reason the delay is

394
00:14:31,660 --> 00:14:33,459
there is because if there is none then

395
00:14:33,459 --> 00:14:36,730
the recurrent happens so quickly then it

396
00:14:36,730 --> 00:14:38,620
reaches its recursion limit very very

397
00:14:38,620 --> 00:14:41,230
quickly in fact we then have second of

398
00:14:41,230 --> 00:14:43,029
you start in the application and you're

399
00:14:43,029 --> 00:14:45,730
unable to do anything with it however

400
00:14:45,730 --> 00:14:48,879
the main sort of heart of the code are

401
00:14:48,879 --> 00:14:52,480
lines 118 to 122 because these lines

402
00:14:52,480 --> 00:14:54,249
actually let you take the model

403
00:14:54,249 --> 00:14:56,589
prediction convert it into an image with

404
00:14:56,589 --> 00:14:58,360
these three lines and then show that

405
00:14:58,360 --> 00:15:00,250
image on top of the image

406
00:15:00,250 --> 00:15:03,730
view of course however since core ml

407
00:15:03,730 --> 00:15:06,610
does use these pixel buffers that does

408
00:15:06,610 --> 00:15:08,650
not use UI images for input to the model

409
00:15:08,650 --> 00:15:10,450
unfortunately there is this little

410
00:15:10,450 --> 00:15:12,280
helper function that was taken from this

411
00:15:12,280 --> 00:15:14,110
block for which there is a link in the

412
00:15:14,110 --> 00:15:16,870
code as well as the description and this

413
00:15:16,870 --> 00:15:19,060
function essentially converts a UI image

414
00:15:19,060 --> 00:15:22,090
to a CV pixel buffer that can be fed

415
00:15:22,090 --> 00:15:24,700
into a machine learning model very very

416
00:15:24,700 --> 00:15:28,240
easily and so that is essentially how

417
00:15:28,240 --> 00:15:30,610
this code works of course there's a lot

418
00:15:30,610 --> 00:15:32,260
more behind the actual training of the

419
00:15:32,260 --> 00:15:34,570
neural network model but that's topic

420
00:15:34,570 --> 00:15:37,300
for another time but I do hope you

421
00:15:37,300 --> 00:15:38,830
enjoyed this tutorial that's what I had

422
00:15:38,830 --> 00:15:41,260
to cover for today as to how you can

423
00:15:41,260 --> 00:15:43,420
actually take the new iPhone 10s and its

424
00:15:43,420 --> 00:15:45,250
wonderful machine learning capabilities

425
00:15:45,250 --> 00:15:47,470
and performance and actually run

426
00:15:47,470 --> 00:15:50,920
real-time style transfer at a much much

427
00:15:50,920 --> 00:15:53,410
faster pace than you could with previous

428
00:15:53,410 --> 00:15:57,790
devices like even the iPhone 10 and so

429
00:15:57,790 --> 00:15:59,260
again I really do hope you enjoyed that

430
00:15:59,260 --> 00:16:00,910
video tutorial thank you very much

431
00:16:00,910 --> 00:16:02,860
everyone for joining today that's what I

432
00:16:02,860 --> 00:16:04,839
had for this video today if you do have

433
00:16:04,839 --> 00:16:06,220
any more questions suggestions or

434
00:16:06,220 --> 00:16:07,990
feedback please do feel free to leave

435
00:16:07,990 --> 00:16:09,460
them down in the comment section below

436
00:16:09,460 --> 00:16:11,470
as I'd love to answer any questions you

437
00:16:11,470 --> 00:16:13,930
may have and apart from that if you do

438
00:16:13,930 --> 00:16:15,220
like the video and you'd like to see

439
00:16:15,220 --> 00:16:16,930
more content just like this please do

440
00:16:16,930 --> 00:16:18,520
make sure to consider hitting the like

441
00:16:18,520 --> 00:16:20,380
button and subscribing to my channel as

442
00:16:20,380 --> 00:16:21,880
it really does help out a lot and turn

443
00:16:21,880 --> 00:16:24,190
on the turn on the Bell notifications if

444
00:16:24,190 --> 00:16:25,600
you'd like to be notified whenever I

445
00:16:25,600 --> 00:16:28,060
release new content or from that if you

446
00:16:28,060 --> 00:16:29,440
do think this video could be helpful to

447
00:16:29,440 --> 00:16:30,730
anyone else you know please do consider

448
00:16:30,730 --> 00:16:33,040
sharing it as well and apart from that

449
00:16:33,040 --> 00:16:34,839
the code will be in the description and

450
00:16:34,839 --> 00:16:36,339
that's what I had for this tutorial

451
00:16:36,339 --> 00:16:38,260
today again thank you very much everyone

452
00:16:38,260 --> 00:16:42,270
that's gonna be all for now goodbye
